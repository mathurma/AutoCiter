<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-01025</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005003</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject><subj-group><subject>Neuronal dendrites</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject><subj-group><subject>Neuronal dendrites</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neuronal plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Developmental neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Prospective Coding by Spiking Neurons</article-title>
<alt-title alt-title-type="running-head">Prospective Coding by Spiking Neurons</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Brea</surname> <given-names>Johanni</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Gaál</surname> <given-names>Alexisz Tamás</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" deceased="yes" xlink:type="simple">
<name name-style="western">
<surname>Urbanczik</surname> <given-names>Robert</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Senn</surname> <given-names>Walter</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Physiology, University of Bern, Bern, Switzerland</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>School of Computer and Communication Sciences and School of Life Sciences, Brain-Mind Institute, Ecole Polytechnique Fédérale de Lausanne, Lausanne, Switzerland</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Courant Institute of Mathematical Sciences, New York University, New York, New York, United States of America</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Center for Cognition, Learning and Memory, University of Bern, Bern, Switzerland</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Latham</surname> <given-names>Peter E.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>UCL, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: RU WS JB. Performed the experiments: JB. Analyzed the data: JB WS. Wrote the paper: JB WS ATG. Methods: JB ATG RU WS.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">johanni.brea@epfl.ch</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>6</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>24</day>
<month>6</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>6</issue>
<elocation-id>e1005003</elocation-id>
<history>
<date date-type="received">
<day>25</day>
<month>6</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>1</day>
<month>6</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Brea et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005003"/>
<abstract>
<p>Animals learn to make predictions, such as associating the sound of a bell with upcoming feeding or predicting a movement that a motor command is eliciting. How predictions are realized on the neuronal level and what plasticity rule underlies their learning is not well understood. Here we propose a biologically plausible synaptic plasticity rule to learn predictions on a single neuron level on a timescale of seconds. The learning rule allows a spiking two-compartment neuron to match its current firing rate to its own expected future discounted firing rate. For instance, if an originally neutral event is repeatedly followed by an event that elevates the firing rate of a neuron, the originally neutral event will eventually also elevate the neuron’s firing rate. The plasticity rule is a form of spike timing dependent plasticity in which a presynaptic spike followed by a postsynaptic spike leads to potentiation. Even if the plasticity window has a width of 20 milliseconds, associations on the time scale of seconds can be learned. We illustrate prospective coding with three examples: learning to predict a time varying input, learning to predict the next stimulus in a delayed paired-associate task and learning with a recurrent network to reproduce a temporally compressed version of a sequence. We discuss the potential role of the learning mechanism in classical trace conditioning. In the special case that the signal to be predicted encodes reward, the neuron learns to predict the discounted future reward and learning is closely related to the temporal difference learning algorithm TD(<italic>λ</italic>).</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Sensory inputs are often predictable. Lightning is followed by thunder, a falling object causes noise when hitting the ground, our skin gets wet when we jump into the water. Humans learn regularities like these without effort. Learned predictions allow to cover the ears in anticipation of thunder or close the eyes just before an object hits the ground and breaks into pieces. What changes in the brain when new predictions are learned? In this article, we present a mathematical model and computer simulations of the idea that the activity of a single neuron represents expected future events. Such a prospective coding can be learned in a neuron that receives input from the memory trace of a first event (e.g. lightning) and also input from the second event (e.g. thunder). Synaptic input connections from the memory trace are potentiated such that the spiking activity ramps up towards the onset of the second event. This deviates from the classical Hebbian learning that merely associates two events that are coincident in time. Learning in our model associates a current event to future events.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Swiss National Science Foundation</institution>
</funding-source>
<award-id>310030L-156863</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Senn</surname> <given-names>Walter Martin</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>The work has been supported by the Swiss National Science Foundation <ext-link ext-link-type="uri" xlink:href="http://www.snf.ch" xlink:type="simple">www.snf.ch</ext-link> (personal grant no. 310030L-156863 of WS). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="0"/>
<page-count count="25"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper. Additionally the source code of the simulations is publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jbrea/prospectiveCoding" xlink:type="simple">https://github.com/jbrea/prospectiveCoding</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Animals can learn to predict upcoming stimuli. In delayed paired-associate tasks, animals learn to respond to pairs of stimuli (e.g. images A1-B1 and A2-B2) separated by a delay. These tasks can be solved by either keeping a memory of the first stimulus (A1 or A2) during the delay period (retrospective coding) or anticipating the second stimulus (B1 or B2) during the delay period (prospective coding). Monkeys seem to use both coding schemes [<xref ref-type="bibr" rid="pcbi.1005003.ref001">1</xref>]. Recordings in the prefrontal cortex of monkeys performing a delayed paired-associate task revealed single neurons with decreasing firing rate in response to a specific first stimulus (A1 or A2) and other neurons with ramping activity in trials where a specific second stimulus (B1 or B2) is anticipated [<xref ref-type="bibr" rid="pcbi.1005003.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref002">2</xref>]. Thus, the firing rate of a neuron may encode not only past and current events, but also prospective events.</p>
<p>Learning to anticipate a future stimulus can also be observed in classical trace conditioning, where a conditioned stimulus (CS, e.g. sound of a bell) is followed after a delay by an unconditioned stimulus US (e.g. a sausage) that causes a response R (e.g. salivation) [<xref ref-type="bibr" rid="pcbi.1005003.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref004">4</xref>]. After several repetitions of this protocol, the conditioned stimulus CS can elicit response R already before the onset of the unconditioned stimulus US.</p>
<p>A common experimental finding in these examples is the slowly ramping neuronal activity prior to the predicted event. In an experiment where mice choose to lick left or right in response to a tactile cue, the neural activity in the anterior lateral motor cortex ramps up in the waiting period before the response [<xref ref-type="bibr" rid="pcbi.1005003.ref005">5</xref>]. This activity pattern implements prospective coding as it indicates whether the animal will lick left or right. Serotonergic neurons in the dorsal raphe nucleus of mice show an activity ramp in a delay period between a predictive odor cue and the availability of a sucrose reward [<xref ref-type="bibr" rid="pcbi.1005003.ref006">6</xref>]. In rats that navigate a maze towards the learned position of a chocolate milk reward, the activity of striatal neurons increases while the rat approaches the reward position [<xref ref-type="bibr" rid="pcbi.1005003.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref008">8</xref>]. In visual delayed paired associate tasks in which monkeys are trained to select a specific choice object that is associated with a previously shown cue object, increasing activity in the delay period was measured for neurons in the prefrontal cortex [<xref ref-type="bibr" rid="pcbi.1005003.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref010">10</xref>] and in the inferior temporal cortex [<xref ref-type="bibr" rid="pcbi.1005003.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref011">11</xref>].</p>
<p>It is unclear how prospective coding emerges. The cue and the associated predictable event are typically separated by an interval of some seconds. On the other hand, synaptic plasticity, that is presumably involved in learning new associations, typically requires presynaptic and postsynaptic activity to coincide in a much shorter interval. Some tens of milliseconds is, for example, the size of the ‘plasticity window’ in spike-timing dependent plasticity; no synaptic change occurs, if presynaptic and postsynaptic spike are separated by more than the size of this plasticity window [<xref ref-type="bibr" rid="pcbi.1005003.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref013">13</xref>]. This mismatch between the behavioral and the neuronal timescales begs the question how a neuronal system can learn to make predictions more than a second ahead. There are also plasticity mechanisms that can correlate pre- and postsynaptic spiking events that are separated by seconds [<xref ref-type="bibr" rid="pcbi.1005003.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref015">15</xref>]. Yet, assuming many simultaneously active afferents, it remains unclear how the behaviourally relevant pair of pre- and postsynaptic spikes can be selected out of hundreds behaviourally irrelevant pairs.</p>
<p>In normative models of synaptic plasticity, the shape of the causal part of the plasticity window matches the shape of the postsynaptic potential (PSP), if the objective is to reproduce precise spike timings [<xref ref-type="bibr" rid="pcbi.1005003.ref016">16</xref>–<xref ref-type="bibr" rid="pcbi.1005003.ref018">18</xref>]. However, if the objective is to reproduce future activity, this specific learning rule is insufficient. Yet, as we demonstrate in this article, the same plasticity rule with only a slightly wider window also allows for learning a prospective code. With this mechanism, it is possible to learn an activity ramp towards a specific event in time, or to learn predicting a time-varying signal or a sequence of activities well ahead in time. In a 2-compartment neuron model, this mechanism leads to the dendritic prediction of <italic>future</italic> somatic spiking. The mechanism stands in contrast to the work of Urbanczik &amp; Senn, where the current somatic spiking is predicted [<xref ref-type="bibr" rid="pcbi.1005003.ref018">18</xref>]. Despite this fundamental difference, the plasticity rules only differ in the width of the potentiation part of the plasticity window.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Schematic description of the learning mechanism</title>
<p>Before defining the learning rule in detail, we provide an intuitive description. In a neuron with both static synapses (green connection in <xref ref-type="fig" rid="pcbi.1005003.g001">Fig 1A and 1B</xref>) and plastic synapses (blue in <xref ref-type="fig" rid="pcbi.1005003.g001">Fig 1A and 1B</xref>), we propose a learning mechanism for the plastic synapses that relies on two basic ingredients: spike-timing dependent synaptic potentiation and balancing synaptic depression. The synaptic connections are strengthened if a presynaptic spike is followed by a postsynaptic spike within a ‘plasticity window of potentiation’ (red in <xref ref-type="fig" rid="pcbi.1005003.g001">Fig 1A and 1B</xref>). The size of this plasticity window turns out to have a strong influence on the timing of spikes that are caused by strengthened dendritic synapses. If the plasticity window has the same shape as a postsynaptic potential (PSP), learned spikes are fired at roughly the same time as target spikes [<xref ref-type="bibr" rid="pcbi.1005003.ref016">16</xref>–<xref ref-type="bibr" rid="pcbi.1005003.ref018">18</xref>]. But if the plasticity window is slightly longer than the postsynaptic potential, learned spikes tend to be fired earlier than target spikes. More precisely, because of the slightly wider plasticity window of potentiation, presynaptic spikes may elicit postsynaptic spikes through newly strengthened connections (thick blue arrow in <xref ref-type="fig" rid="pcbi.1005003.g001">Fig 1B</xref>) even before the onset of the input through static synapses. These earlier postsynaptic spikes allow to strengthen the input of presynaptic neurons that spike even earlier. We refer to this as the bootstrapping effect of predicting the own predictions. As a result, a postsynaptic activity induced by the input through static synapses will be preceded by an activity ramp produced by appropriately tuned dendritic input. The neuron learns a prospective code that predicts an upcoming event.</p>
<fig id="pcbi.1005003.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005003.g001</object-id>
<label>Fig 1</label>
<caption>
<title>A learning mechanism that leads to prospective coding.</title>
<p><bold>A</bold> The signal to be predicted (target input) originates from the green neuron and depolarizes the black neuron (gray trace) such that it spikes (black lines). The synaptic connection between a blue neuron and the black neuron is strengthened if pre- and postsynaptic spikes lie within the red plasticity window of potentiation, which is slightly broader than a typical postsynaptic potential. <bold>B</bold> Due to the strengthened connection (red circle), the black neuron spikes already before the target input arrives. Since earlier presynaptic spikes now also lie within the potentiating plasticity window, the activity of the black neuron will be anticipated earlier, giving rise to prospective coding. <bold>C</bold> A spiking neuron receives input through plastic dendritic synapses with strengths <italic>w</italic><sub><italic>i</italic></sub> and an input <italic>I</italic> through static (i.e. non-plastic) synapses. The somatic membrane potential <italic>U</italic> is well approximated by the sum of attenuated dendritic input <inline-formula id="pcbi.1005003.e001"><alternatives><graphic id="pcbi.1005003.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> and attenuated somatic input <italic>U</italic>*.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005003.g001" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec004">
<title>The 2-compartment neuron model</title>
<p>We consider a 2-compartment neuron model that captures important functional details of spiking neurons and is well suited for analytical analysis [<xref ref-type="bibr" rid="pcbi.1005003.ref018">18</xref>]. In this model (<xref ref-type="fig" rid="pcbi.1005003.g001">Fig 1C</xref>), a dendritic compartment receives input through plastic synapses with strength <italic>w</italic>. The voltage <italic>U</italic> of the somatic compartment is coupled to the dendritic voltage <italic>V</italic><sub><italic>w</italic></sub> and receives additional input <italic>I</italic> through static synapses,
<disp-formula id="pcbi.1005003.e002"><alternatives><graphic id="pcbi.1005003.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mrow><mml:mi>C</mml:mi> <mml:mspace width="0.166667em"/><mml:mover accent="true"><mml:mi>U</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mi>U</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>D</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>U</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>I</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>g</italic><sub><italic>L</italic></sub> is the leak conductance, <italic>g</italic><sub><italic>D</italic></sub> is the coupling conductance between soma and dendrite and <italic>C</italic>, the somatic capacitance. The dendritic potential <italic>V</italic><sub><italic>w</italic></sub> is given by a weighted sum of presynaptic inputs, i.e.
<disp-formula id="pcbi.1005003.e003"><alternatives><graphic id="pcbi.1005003.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mrow><mml:msub><mml:mi>V</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:msup><mml:mi>t</mml:mi> <mml:mi>f</mml:mi></mml:msup> <mml:mo>∈</mml:mo> <mml:msub><mml:mi mathvariant="script">T</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:mi>κ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:msup><mml:mi>t</mml:mi> <mml:mi>f</mml:mi></mml:msup></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
with plastic synaptic weights <italic>w</italic><sub><italic>i</italic></sub>, postsynaptic potentials PSP<sub><italic>i</italic></sub> that model the depolarization of the postsynaptic membrane potential due to the arrival of a presynaptic spikes at synapse <italic>i</italic>, set <inline-formula id="pcbi.1005003.e004"><alternatives><graphic id="pcbi.1005003.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msub><mml:mi mathvariant="script">T</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> of spike arrival times at synapse <italic>i</italic> and spike response kernel <italic>κ</italic>. Spiking of the postsynaptic neuron is modeled as an inhomogeneous Poisson process with rate <italic>φ</italic>(<italic>U</italic>).</p>
<p>We model the input with time varying excitatory and inhibitory conductances <italic>g</italic><sub><italic>E</italic></sub> and <italic>g</italic><sub><italic>I</italic></sub> proximal to the soma such that
<disp-formula id="pcbi.1005003.e005"><alternatives><graphic id="pcbi.1005003.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mrow><mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>E</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>E</mml:mi> <mml:mi>E</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>U</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>I</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>E</mml:mi> <mml:mi>I</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>U</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
as proposed by Urbanczik &amp; Senn [<xref ref-type="bibr" rid="pcbi.1005003.ref018">18</xref>].</p>
<p>For large total conductance and slowly varying input, the somatic membrane potential <italic>U</italic>(<italic>t</italic>) is well approximated (see <xref ref-type="sec" rid="sec013">Methods</xref>) by its steady state solution
<disp-formula id="pcbi.1005003.e006"><alternatives><graphic id="pcbi.1005003.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mrow><mml:mi>U</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:mi>λ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msup><mml:mi>U</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula>
where we introduced the attenuated dendritic potential
<disp-formula id="pcbi.1005003.e007"><alternatives><graphic id="pcbi.1005003.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>g</mml:mi> <mml:mi>D</mml:mi></mml:msub> <mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>V</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
the attenuated somatic input
<disp-formula id="pcbi.1005003.e008"><alternatives><graphic id="pcbi.1005003.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mrow><mml:msup><mml:mi>U</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mi mathvariant="normal">E</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>E</mml:mi> <mml:mi mathvariant="normal">E</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi mathvariant="normal">I</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>E</mml:mi> <mml:mi mathvariant="normal">I</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mtext>tot</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
and the ‘nudging’ factor
<disp-formula id="pcbi.1005003.e009"><alternatives><graphic id="pcbi.1005003.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mrow><mml:mi>λ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>D</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mtext>tot</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mspace width="4pt"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula>
with <italic>g</italic><sub>tot</sub>(<italic>t</italic>) = <italic>g</italic><sub><italic>L</italic></sub> + <italic>g</italic><sub><italic>D</italic></sub> + <italic>g</italic><sub><italic>E</italic></sub>(<italic>t</italic>) + <italic>g</italic><sub><italic>I</italic></sub>(<italic>t</italic>), to be in accordance with Urbanczik &amp; Senn [<xref ref-type="bibr" rid="pcbi.1005003.ref018">18</xref>]. The nudging factor <italic>λ</italic>(<italic>t</italic>) ∈ (0, 1] is close to 1 for small somatic input and equal to 1 if <italic>g</italic><sub><italic>E</italic></sub>(<italic>t</italic>) + <italic>g</italic><sub><italic>I</italic></sub>(<italic>t</italic>) = 0.</p>
</sec>
<sec id="sec005">
<title>Learning as dendritic prediction of the neuron’s future discounted firing rate</title>
<p>The plasticity rule we consider for the dendritic synapses can be seen as differential Hebbian in the sense that both the potentiation and depression term are a product of a post- and presynaptic term. The strength of synapse <italic>i</italic> is assumed to change continuously according to the dynamics
<disp-formula id="pcbi.1005003.e010"><alternatives><graphic id="pcbi.1005003.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>α</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>φ</mml:mi> <mml:mfenced close=")" open="("><mml:mi>U</mml:mi></mml:mfenced> <mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="("><mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:mfenced> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula>
where
<disp-formula id="pcbi.1005003.e011"><alternatives><graphic id="pcbi.1005003.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula>
is the low-pass filtered postsynaptic potential at synapse <italic>i</italic>, <italic>φ</italic>(<italic>U</italic>) and <inline-formula id="pcbi.1005003.e012"><alternatives><graphic id="pcbi.1005003.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> are the instantaneous firing rates based on the somatic potential and the attenuated dendritic potential, respectively, and <italic>η</italic> is the learning rate. The factor of potentiation <italic>α</italic> that scales the potentiation term is positive but smaller than the inverse of the largest nudging factor 1/max<sub><italic>t</italic></sub> <italic>λ</italic>(<italic>t</italic>) to prevent the unbounded growth of synaptic strengths.</p>
<p>Under the assumption of a periodic environment, rich dendritic input dynamics, constant nudging factor <italic>λ</italic> and linear <italic>φ</italic> (<xref ref-type="sec" rid="sec013">Methods</xref>), the weight dynamics in <xref ref-type="disp-formula" rid="pcbi.1005003.e010">Eq 8</xref> leads to prospective coding by making the dendritic rate <inline-formula id="pcbi.1005003.e013"><alternatives><graphic id="pcbi.1005003.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> approach the expected future discounted somatic input rate, i.e.
<disp-formula id="pcbi.1005003.e014"><alternatives><graphic id="pcbi.1005003.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mrow><mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:msub><mml:mi>τ</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub></mml:mfrac></mml:mrow></mml:msup> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>U</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula>
where the effective discount time constant <italic>τ</italic><sub>eff</sub> is given by
<disp-formula id="pcbi.1005003.e015"><alternatives><graphic id="pcbi.1005003.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>τ</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mi>α</mml:mi></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
Depending on the factor of potentiation <italic>α</italic> and the nudging factor <italic>λ</italic>, the effective time constant <italic>τ</italic><sub>eff</sub> can be much larger than the biophysical time constant <italic>τ</italic> of low-pass filtering and match behavioral timescales of seconds. In particular, if the somatic input is strong and hence <italic>λ</italic> close to 0 (close to ‘clamping’), the effective discount time constant is short, <italic>τ</italic><sub>eff</sub> ≈ <italic>τ</italic>. But when nudging is weak (<italic>λ</italic> close to 1), the synapses on the dendrite learn to predict their self-generated somatic firing rate and the effective discount time constant is extended up to <inline-formula id="pcbi.1005003.e016"><alternatives><graphic id="pcbi.1005003.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mo>≈</mml:mo> <mml:mfrac><mml:mi>τ</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. The case of weak nudging is also the case when the neuron’s somatic firing rate is roughly determined by the dendritic input, <inline-formula id="pcbi.1005003.e017"><alternatives><graphic id="pcbi.1005003.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>U</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, see <xref ref-type="disp-formula" rid="pcbi.1005003.e006">Eq 4</xref>. In particular, if after learning the somatic input is transiently silenced, the neuron’s firing rate <italic>φ</italic>(<italic>U</italic>(<italic>t</italic>)), according to <xref ref-type="disp-formula" rid="pcbi.1005003.e014">Eq 10</xref>, represents the discounted future rate of the somatic input <italic>U</italic>*(<italic>t</italic>) applied during the previous learning period, even if this was only slightly nudging the somatic potential <italic>U</italic>(<italic>t</italic>) itself.</p>
<p>Periodic inputs are unrealistic in a natural setting. But a similar result holds also in more general settings, where a neuron is occasionally exposed to correlated dendritic and somatic inputs. In this more general stochastic setting we derive the main result under the assumption that dendritic and somatic inputs depend on the state of a stationary latent Markov chain <italic>X</italic><sub>0</sub>,<italic>X</italic><sub>1</sub>, …. The dependence on a stationary latent Markov chain assures that the neuron is occasionally exposed to correlated dendritic and somatic inputs. The main result in this setting is (cf. <xref ref-type="disp-formula" rid="pcbi.1005003.e108">Eq 48</xref>)
<disp-formula id="pcbi.1005003.e018"><alternatives><graphic id="pcbi.1005003.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mrow><mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mi>α</mml:mi></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msubsup><mml:mi>γ</mml:mi> <mml:mtext>eff</mml:mtext> <mml:mi>k</mml:mi></mml:msubsup> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mfenced close="]" open="[" separators=""><mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>U</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>|</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mi>x</mml:mi></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>
where <inline-formula id="pcbi.1005003.e019"><alternatives><graphic id="pcbi.1005003.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:msub><mml:mi>γ</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>δ</mml:mi> <mml:msub><mml:mi>τ</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is a large discount factor that leads to a similar discount behavior as in the time-continuous case, if <italic>t</italic> = <italic>kδ</italic>.</p>
<p>It is important to note that in the stochastic case the dendritic rate is only informative about <italic>expected</italic> future somatic inputs. Metaphorically speaking, a neuron can learn to predict the expected win in a lottery, but obviously it cannot learn to predict single lottery draws.</p>
</sec>
<sec id="sec006">
<title>The bootstrapping effect of predicting the own predictions</title>
<p>In the limit, <italic>τ</italic> → 0 we find that <inline-formula id="pcbi.1005003.e023"><alternatives><graphic id="pcbi.1005003.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mtext>PSP</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula> and with <italic>α</italic> = 1 we recover the learning rule of Urbanczik &amp; Senn [<xref ref-type="bibr" rid="pcbi.1005003.ref018">18</xref>]. This rule adapts the dendritic synapses such that the dendritic input matches the somatic input <xref ref-type="fig" rid="pcbi.1005003.g002">Fig 2B</xref>. On the other hand, the learning rule with a slightly larger potentiation window leads to dendritic input that ramps up long before the onset of somatic input <xref ref-type="fig" rid="pcbi.1005003.g002">Fig 2C</xref>.</p>
<fig id="pcbi.1005003.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005003.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Learning activity ramps on a behavioral timescale with a biologically plausible plasticity window.</title>
<p><bold>A</bold>-<bold>B</bold> For orthogonal input patterns (exactly one presynaptic spike arrives at each synapse during 2 s) (A) and a somatic input after 1800 ms (<italic>g</italic><sub><italic>E</italic></sub> = 15 nS during green shading in B and C, <italic>g</italic><sub><italic>E</italic></sub> = 0 otherwise), the learned postsynaptic firing rate has a similar time course as the somatic input if <inline-formula id="pcbi.1005003.e020"><alternatives><graphic id="pcbi.1005003.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:mtext>PSP</mml:mtext> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> (B, lines from light gray to black: postsynaptic firing rate after 100, …, 1000 training sessions). <bold>C</bold> If <inline-formula id="pcbi.1005003.e021"><alternatives><graphic id="pcbi.1005003.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:mtext>PSP</mml:mtext> <mml:mo>≠</mml:mo> <mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> (with <italic>τ</italic> = 9 ms), the learned postsynaptic firing rate ramps up with an effective time constant of <italic>τ</italic><sub><italic>eff</italic></sub> = 600 ms towards the onset of the somatic input. The theoretical result is in good agreement with the simulation (dashed red line: <inline-formula id="pcbi.1005003.e022"><alternatives><graphic id="pcbi.1005003.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> computed by <xref ref-type="disp-formula" rid="pcbi.1005003.e014">Eq 10</xref>). During training, the 2 s long pattern of dendritic and the somatic inputs is periodically repeated.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005003.g002" xlink:type="simple"/>
</fig>
<p>By looking at Eqs <xref ref-type="disp-formula" rid="pcbi.1005003.e006">4</xref> and <xref ref-type="disp-formula" rid="pcbi.1005003.e010">8</xref> we can now obtain a better intuition for the bootstrapping effect of predicting the own predictions. If at the beginning of learning all synaptic weights <italic>w</italic><sub><italic>i</italic></sub> are zero, the dendritic potential <italic>V</italic><sub><italic>w</italic></sub> is at rest (= 0) all the time and the somatic membrane potential <italic>U</italic>(<italic>t</italic>) follows the somatic input <italic>U</italic>*(<italic>t</italic>) (see <xref ref-type="disp-formula" rid="pcbi.1005003.e006">Eq 4</xref>). In this case, the learning rule in <xref ref-type="disp-formula" rid="pcbi.1005003.e010">Eq 8</xref> contains only the potentiation term
<disp-formula id="pcbi.1005003.e024"><alternatives><graphic id="pcbi.1005003.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>α</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>U</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
In the example in <xref ref-type="fig" rid="pcbi.1005003.g002">Fig 2C</xref>, the somatic input <italic>U</italic>* and consequently <italic>φ</italic>(<italic>U</italic>*) is non-zero only after 1800 ms. Therefore, synapse <italic>i</italic> is potentiated only if a presynaptic spike arrives shortly before the onset of the somatic input. The next time a presynaptic spike arrives at synapse <italic>i</italic>, the somatic membrane potential is depolarized by the dendritic input already before the onset of the somatic input and the learning rule contains at this moment (e.g. at 1780 ms in <xref ref-type="fig" rid="pcbi.1005003.g002">Fig 2C</xref>) the terms
<disp-formula id="pcbi.1005003.e025"><alternatives><graphic id="pcbi.1005003.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>α</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>λ</mml:mi> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
These terms would cancel each other in the case of Urbanczik &amp; Senn [<xref ref-type="bibr" rid="pcbi.1005003.ref018">18</xref>] where <italic>α</italic> = <italic>λ</italic> = 1 and <inline-formula id="pcbi.1005003.e026"><alternatives><graphic id="pcbi.1005003.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. But if <inline-formula id="pcbi.1005003.e027"><alternatives><graphic id="pcbi.1005003.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> is the low-pass filtered version of the postsynaptic potential (as in <xref ref-type="fig" rid="pcbi.1005003.g002">Fig 2C</xref>) they do not cancel. Instead, synapses are potentiated, if a presynaptic spike arrives shortly before the somatic potential was depolarized due to dendritic input through already potentiated synapses. The consequence of this bootstrapping effect appears in <xref ref-type="fig" rid="pcbi.1005003.g002">Fig 2C</xref> in the gray curves. After 100 training sessions, the dendritic input starts to rise around 1200 ms, but synapses with earlier presynaptic spikes are not yet strengthened. With each further training session the dendritic input rises earlier.</p>
<p>The dendritic and the somatic inputs are deterministic periodic functions, in the example in <xref ref-type="fig" rid="pcbi.1005003.g002">Fig 2C</xref>. Therefore we can directly compare the simulation to the theoretical results of the previous section. For the interval without somatic input (0–1800 ms), where <inline-formula id="pcbi.1005003.e028"><alternatives><graphic id="pcbi.1005003.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>U</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, we find a good agreement (dashed red and thick black line in <xref ref-type="fig" rid="pcbi.1005003.g002">Fig 2C</xref>). Small differences are to be expected, because in the theoretical derivations a constant nudging factor <italic>λ</italic> is assumed and the steady-state solution of the somatic membrane potential dynamics is used (see <xref ref-type="disp-formula" rid="pcbi.1005003.e006">Eq 4</xref>). The dendritic rate <inline-formula id="pcbi.1005003.e029"><alternatives><graphic id="pcbi.1005003.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mrow><mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is only slightly below the somatic rate <italic>φ</italic>(<italic>U</italic>) in the interval with somatic input (1800–2000 ms), because the somatic input is small.</p>
</sec>
<sec id="sec007">
<title>Dependence on the dendritic input structure</title>
<p>The input pattern in <xref ref-type="fig" rid="pcbi.1005003.g002">Fig 2A</xref> is a particularly simple example of a deterministic, periodic pattern with rich enough structure. Enough structure to learn a prospective code exists also in sufficiently many randomly generated (frozen) spike trains that are deterministically repeated, if there is always at least one presynaptic spike within the duration of a <italic>PSP</italic> and the probability of repeating a nearly identical presynaptic spike pattern is low (see <xref ref-type="fig" rid="pcbi.1005003.g003">Fig 3A</xref>). We did not systematically search for the minimal number of required dendritic synapses. But for the example in <xref ref-type="fig" rid="pcbi.1005003.g003">Fig 3A</xref> we found empirically that a few hundred synapses are necessary. If the presynaptic firing frequency is only 2 Hz, we found that 1000 presynaptic neurons are enough to learn the ramp in 100 trials, whenever the learning rate is larger than in the 20 Hz case. At the end of learning, the time course of the somatic potential matches the one of the previous example (black lines in Figs <xref ref-type="fig" rid="pcbi.1005003.g002">2C</xref> and <xref ref-type="fig" rid="pcbi.1005003.g003">3A</xref>). But during learning, the time course of the somatic potential is different in the two examples (gray lines in Figs <xref ref-type="fig" rid="pcbi.1005003.g002">2C</xref> and <xref ref-type="fig" rid="pcbi.1005003.g003">3A</xref>). This is a consequence of the influence of correlations in the dendritic input. For the frozen spike trains, the presynaptic auto-correlation <inline-formula id="pcbi.1005003.e030"><alternatives><graphic id="pcbi.1005003.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo> <mml:mo>≠</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> is non-vanishing for all <italic>s</italic> and <italic>i</italic>. This causes the average firing rate to increase early during learning (<xref ref-type="fig" rid="pcbi.1005003.g003">Fig 3A</xref>; gray lines in interval 0–1500 ms in contrast to gray lines in the same interval in <xref ref-type="fig" rid="pcbi.1005003.g002">Fig 2C</xref>).</p>
<fig id="pcbi.1005003.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005003.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Dependence of learning on the type of input.</title>
<p><bold>A</bold> Learning succeeds with deterministically repeated spike trains (20 frozen spike trains out of 500, <italic>g</italic><sub><italic>E</italic></sub> as in <xref ref-type="fig" rid="pcbi.1005003.g002">Fig 2</xref>). <bold>B</bold> Learning succeeds with stochastic spiking, if the spiking rate is variable (blue shading: spiking rate, blue ticks: one sample). <bold>C</bold> The amplitude of the ramp is smaller, if the training event occurs only with 50% probability. <bold>D</bold> If the spiking rate is constant during long intervals, the input is not sufficient to learn a smooth ramp. A stepwise ramp is learned instead (<italic>g</italic><sub><italic>E</italic></sub> = 8 nS during green shading)</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005003.g003" xlink:type="simple"/>
</fig>
<p>In the examples given so far, the dendritic and the somatic inputs are deterministic, but deterministic repetitions of the exact same spike trains are unrealistic. In <xref ref-type="fig" rid="pcbi.1005003.g003">Fig 3B</xref> we consider the more realistic case of random spiking. In each trial, the spikes are sampled from an inhomogeneous Poisson process, with periodically repeating rates. The resulting activity ramp is noisier but in good agreement with the theoretical result. It is important that the rates of the Poisson process have sufficiently rich structure. In <xref ref-type="fig" rid="pcbi.1005003.g003">Fig 3D</xref> the firing rate of the Poisson process is kept constant for one third of the trial. In this case, the temporal structure is not sufficiently rich to learn a smooth ramp and a stepwise activity ramp is learned instead.</p>
<p>In <xref ref-type="fig" rid="pcbi.1005003.g003">Fig 3C</xref>, the target event occurs only with a 50% chance, i.e. the somatic input is given only in half the trials. This results in an activity ramp with smaller amplitude, which is consistent with the theoretical finding that the dendritic rate depends linearly on the average somatic input rate (see <xref ref-type="disp-formula" rid="pcbi.1005003.e018">Eq 12</xref>).</p>
</sec>
<sec id="sec008">
<title>Delayed paired-associate task</title>
<p>Prospective coding in neurons of the prefrontal cortex was observed in an experiment with monkeys performing a delayed paired-associate task [<xref ref-type="bibr" rid="pcbi.1005003.ref001">1</xref>]. In this experiment, monkeys learned to associate a visual sample to a visual target presented one second later. Our learning rule allows for learning a prospective code in such a task.</p>
<p>During training, sample A1 is always followed by target B1 after a delay of 1s, and sample A2 is followed by target B2 (<xref ref-type="fig" rid="pcbi.1005003.g004">Fig 4A</xref>). In the simulation we assume that the sample (first stimulus) leaves a memory trace in form of a spatio-temporal activity pattern that projects through dendritic synapses, while the target (second stimulus) drives somatic synapses (<xref ref-type="fig" rid="pcbi.1005003.g004">Fig 4B</xref>). In order to have sufficiently rich presynaptic activity (c.f. <xref ref-type="fig" rid="pcbi.1005003.g003">Fig 3B</xref>), the memory trace of the sample is modeled by an inhomogeneous Poisson process with sample dependent rate trajectories (<xref ref-type="fig" rid="pcbi.1005003.g004">Fig 4C</xref>), i.e. during the presentation of the first stimulus the rate trajectory of each neuron approaches a previously chosen template trajectory that depends on the sample (see <xref ref-type="sec" rid="sec013">Methods</xref>). These memory traces are inspired by liquid state machines (see <xref ref-type="sec" rid="sec012">Discussion</xref>). If a neuron receives strong somatic input only in the presence of a specific target (neurons 1 and 2 in <xref ref-type="fig" rid="pcbi.1005003.g004">Fig 4B</xref>), its firing rate ramps up exclusively in anticipation of this target (neurons 1 and 2 in <xref ref-type="fig" rid="pcbi.1005003.g004">Fig 4D</xref>). In contrast to such a ‘grandmother-cell coding’ (one neuron for one target), a set of neurons could encode the target in a distributed manner, where the target is identified by the overall activity pattern and single neurons respond differently to different target stimuli. Such a distributed code can be learned with neurons that receive somatic input of target-specific strengths (neuron 3 in <xref ref-type="fig" rid="pcbi.1005003.g004">Fig 4B</xref>; B1 stronger than B2). After learning, the amplitude of the activity ramp reflects this target specificity (neuron 3 in <xref ref-type="fig" rid="pcbi.1005003.g004">Fig 4D</xref>).</p>
<fig id="pcbi.1005003.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005003.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Prospective coding in a delayed paired-associate task.</title>
<p><bold>A</bold> In the simulation, stimulus A1 and A2 is repeatedly followed by stimulus B1 and B2, respectively, with a delay of 1s. The two pairs are chosen randomly with equal probabilities. Intertrial intervals are chosen at random uniformly between 3 s and 10 s. <bold>B</bold> The first stimulus (A1 or A2) activates a recurrent network of 2000 neurons representing a short-term memory (STM). The dynamics of the recurrent network is modeled by a stochastic process (see <xref ref-type="sec" rid="sec013">Methods</xref>). The STM is read out by 3 neurons that encode in a distributed manner the second stimulus (setting <italic>g</italic><sub><italic>E</italic></sub> = 5 nS in neuron 1 and neuron 3 during the B1 presentation, and <italic>g</italic><sub><italic>E</italic></sub> = 5 nS in neuron 2 and <italic>g</italic><sub><italic>E</italic></sub> = 2.5 nS in neuron 3 during the B2 presentation). <bold>C</bold> The time course of the firing rates of neurons in the recurrent short-term memory network depends on the first stimulus (dark blue: A1; gold: A2; spike trains of a specific STM neuron during 4 A1 trials and its estimated rates for 4 A1 and 4 A2 trials). <bold>D</bold> After learning, the firing rate of neuron 1 ramps up after stimulus A1 (gold trace), but not after stimulus A2 (blue trace). The opposite holds for neuron 2. Since neuron 3 receives more somatic input when B1 is present, the firing rate of neuron 3 ramps up to a larger value after A1 than after A2.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005003.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec009">
<title>Prospective coding of times series</title>
<p>In Figs <xref ref-type="fig" rid="pcbi.1005003.g002">2</xref> to <xref ref-type="fig" rid="pcbi.1005003.g004">4</xref> the somatic target input was silent most of the time and active only during a short interval. This simple time course of the somatic input is, however, not a requirement and learning also converges for more complex trajectories of somatic input. In general, a time varying input through (static) somatic synapses induces plasticity that advances the postsynaptic firing rate <italic>φ</italic>(<italic>U</italic>(<italic>t</italic>)) relative to the firing rate <italic>φ</italic>(<italic>U</italic>*(<italic>t</italic>)) determined by the somatic input alone. <xref ref-type="fig" rid="pcbi.1005003.g005">Fig 5A</xref> shows an example with an advancement of roughly 50ms that has been achieved with a shorter time window (∼20 ms) for synaptic potentiation. As in <xref ref-type="fig" rid="pcbi.1005003.g003">Fig 3A</xref>, the dendritic input was a periodically repeated random spike train that could also be replaced by stochastic spiking with time dependent firing rates as in <xref ref-type="fig" rid="pcbi.1005003.g003">Fig 3B</xref>.</p>
<fig id="pcbi.1005003.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005003.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Prospective coding of time-varying input.</title>
<p><bold>A</bold> Learning leads to an advancement of the postsynaptic firing rate. The dendritic input consists of spike trains of 2000 neurons (bottom; 20 shown). The somatic input is given by <italic>g</italic><sub><italic>E</italic></sub>(<italic>t</italic>) = 6(1 − sin(<italic>ωt</italic>) sin(2<italic>ωt</italic>) cos(4<italic>ωt</italic>)) nS, with <italic>ω</italic> = 2<italic>π</italic>/(2000 ms). <bold>B</bold> The correlation of the firing rate curves in A peaks at <italic>t</italic><sub>peak</sub> = 52 ms. <bold>C</bold> The advancement increases with the potentiation factor <italic>α</italic>. <bold>D</bold> With increasing potentiation factor <italic>αλ</italic> the effective discount time constant <italic>τ</italic><sub>eff</sub> becomes much larger than <italic>τ</italic> (<xref ref-type="disp-formula" rid="pcbi.1005003.e015">Eq 11</xref>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005003.g005" xlink:type="simple"/>
</fig>
<p>Since the learning rule converges to a point where the dendritic input is proportional to the future discounted somatic input (<xref ref-type="disp-formula" rid="pcbi.1005003.e014">Eq 10</xref>), the advanced sequence (black in <xref ref-type="fig" rid="pcbi.1005003.g005">Fig 5A</xref>) is not simply a forward shifted version of the somatic input (green in <xref ref-type="fig" rid="pcbi.1005003.g005">Fig 5A</xref>). This becomes clearly apparent at the center of the figure, where the somatic input is symmetric around 1000 ms, but the advanced sequence is decaying, because the somatic input has a strong dip around 1100 ms. Despite this, the advancement can be characterized by the peak time of the correlation function between <italic>φ</italic>(<italic>U</italic>(<italic>t</italic>)) and <italic>φ</italic>(<italic>U</italic>*(<italic>t</italic>)) that, as the effective discount time constant <italic>τ</italic><sub>eff</sub>, diverges with increasing potentiation factor <italic>α</italic> (<xref ref-type="fig" rid="pcbi.1005003.g005">Fig 5B–5D</xref>).</p>
<p>Time series prediction is a fundamental operation of the brain that is, for instance, involved in motor planning. In our context, the activity time course that has to be reproduced may be provided by proprioceptive feedback from muscles as somatic input <italic>U</italic>* to neurons in the primary motor cortex [<xref ref-type="bibr" rid="pcbi.1005003.ref019">19</xref>]. This feedback can be weak, delayed and sparse. The dendritic input <italic>V</italic>*, in turn, may be conveyed by a higher visual area or a premotor planning area. This dendritic input learns to predict the discounted future firing rate caused by the somatic input, and hence learns to produce the muscle activity that feeds back again as a delayed proprioceptive signal.</p>
</sec>
<sec id="sec010">
<title>Prospective coding in a recurrent neural network</title>
<p>Lastly, we consider a recurrently connected network of 200 neurons that receive external input only at the soma and no external input at the dendrites. The input at the dendrites is given by the output spikes of the network neurons, where we consider all-to-all connectivity (<xref ref-type="fig" rid="pcbi.1005003.g006">Fig 6A</xref>). In contrast to the examples in Figs <xref ref-type="fig" rid="pcbi.1005003.g002">2</xref> to <xref ref-type="fig" rid="pcbi.1005003.g005">5</xref>, there is no external control to assure the richness of the dendritic input and there are no guarantees that learning converges in the sense of <xref ref-type="disp-formula" rid="pcbi.1005003.e014">Eq 10</xref>. Still, we observe the interesting result that learning changes synaptic strengths to allow fast replay of slow experienced sequences.</p>
<fig id="pcbi.1005003.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005003.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Prospective coding in a recurrent neural network.</title>
<p><bold>A</bold> The dendritic input consists of the spike trains of other neurons within the same network (for clarity only one axon is completely drawn; in the simulation we used all to all connections). <bold>B</bold> Groups of 50 neurons receive sequential somatic input (<italic>g</italic><sub><italic>E</italic></sub> = 20 nS during green shading) of duration 100 ms. After repeatedly stimulating, the firing rate increases already prior to somatic input (see for example neurons 51–100 in the first 100 ms). Before 800 ms the last two of 300 training repetitions are shown. Afterwards, no somatic input is provided anymore except for a brief stimulation (after 2400 ms), after which the sequence is autonomously replayed at a faster speed.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005003.g006" xlink:type="simple"/>
</fig>
<p>For sequentially and periodically repeated stimulations on a slow timescale (green shading in <xref ref-type="fig" rid="pcbi.1005003.g006">Fig 6</xref>), the recurrent dendritic connections between subsequently stimulated groups of neurons are strengthened. After 300 repetitions of the same sequence, a brief initial stimulation is sufficient to evoke an activity sequence that has the same ordering as the original sequence (<xref ref-type="fig" rid="pcbi.1005003.g006">Fig 6B</xref> after 2400 ms). However, the replay dynamics can be much faster than the dynamics of the stimulation. Replay depends on the internal dynamics of the network, notably the time constants of the <italic>PSP</italic> and the membrane time constant. Due to prospective coding, the sequence becomes advanced in time while repeatedly presenting the stimuli, and due to the recurrent connectivity the advanced sequence can be recalled with a brief stimulation of the first group of neurons (<xref ref-type="fig" rid="pcbi.1005003.g006">Fig 6B</xref>). Note that there is no need to explicitly distinguish between a training and recall session. Recall differs from training only in the somatic input, which consists of a brief activation of the first group of neurons during recall and slow, sequential activation during training. The learning rule is active all the time.</p>
</sec>
<sec id="sec011">
<title>Relation to TD(<italic>λ</italic>)</title>
<p>The proposed learning mechanism of prospective coding is related to a well studied version of temporal difference (TD) learning. Using our notation for a stochastic and time discrete setting, the goal in TD learning is to estimate a value function
<disp-formula id="pcbi.1005003.e031"><alternatives><graphic id="pcbi.1005003.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mrow><mml:mi mathvariant="script">V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mi>λ</mml:mi></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msubsup><mml:mi>γ</mml:mi> <mml:mtext>TD</mml:mtext> <mml:mi>t</mml:mi></mml:msubsup> <mml:mspace width="0.166667em"/><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mfenced close="]" open="[" separators=""><mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>U</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>|</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mi>x</mml:mi></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula>
where 0 &lt; <italic>γ</italic><sub>TD</sub> &lt; 1 is a discount factor and the expectation is taken over the Markov chain <italic>X</italic><sub>0</sub>,<italic>X</italic><sub>1</sub>, …. We assume that this value function can be approximated by a linear function of the form
<disp-formula id="pcbi.1005003.e032"><alternatives><graphic id="pcbi.1005003.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e032" xlink:type="simple"/><mml:math display="block" id="M32"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script">V</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula>
where <italic>φ</italic> is linear. In TD(<italic>λ</italic>) with linear function approximation, the weights <italic>w</italic> evolve according to the learning rule [<xref ref-type="bibr" rid="pcbi.1005003.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1005003.ref022">22</xref>]
<disp-formula id="pcbi.1005003.e033"><alternatives><graphic id="pcbi.1005003.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e033" xlink:type="simple"/><mml:math display="block" id="M33"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>δ</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(17)</label></disp-formula>
with learning rate <italic>η</italic>, eligibility trace <inline-formula id="pcbi.1005003.e034"><alternatives><graphic id="pcbi.1005003.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>λ</mml:mi> <mml:mtext>TD</mml:mtext></mml:msub> <mml:msub><mml:mi>γ</mml:mi> <mml:mtext>TD</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>s</mml:mi></mml:msup> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, 0 ≤ <italic>λ</italic><sub><italic>TD</italic></sub> ≤ 1, and delta error
<disp-formula id="pcbi.1005003.e035"><alternatives><graphic id="pcbi.1005003.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mrow><mml:msub><mml:mi>δ</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mi>λ</mml:mi></mml:mrow></mml:mfrac> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>U</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>+</mml:mo> <mml:msub><mml:mi>γ</mml:mi> <mml:mtext>TD</mml:mtext></mml:msub> <mml:mspace width="0.166667em"/><mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(18)</label></disp-formula></p>
<p>This delta error is zero on average if the approximation <inline-formula id="pcbi.1005003.e036"><alternatives><graphic id="pcbi.1005003.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is equal to the value function <inline-formula id="pcbi.1005003.e037"><alternatives><graphic id="pcbi.1005003.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mrow><mml:mi mathvariant="script">V</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1005003.e031">Eq 15</xref>. Furthermore, <inline-formula id="pcbi.1005003.e038"><alternatives><graphic id="pcbi.1005003.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mrow><mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> converges to <inline-formula id="pcbi.1005003.e039"><alternatives><graphic id="pcbi.1005003.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mrow><mml:mi mathvariant="script">V</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> under the learning rule of TD(<italic>λ</italic>) in <xref ref-type="disp-formula" rid="pcbi.1005003.e033">Eq 17</xref> [<xref ref-type="bibr" rid="pcbi.1005003.ref020">20</xref>]. The discrete time version of our learning rule (<xref ref-type="disp-formula" rid="pcbi.1005003.e097">Eq 42</xref>), implemented in the 2-compartment model, converges to <xref ref-type="disp-formula" rid="pcbi.1005003.e018">Eq 12</xref> which is identical to the value function in <xref ref-type="disp-formula" rid="pcbi.1005003.e031">Eq 15</xref> if <italic>γ</italic><sub>TD</sub> = <italic>γ</italic><sub>eff</sub>. Therefore, this form of TD(<italic>λ</italic>) and our learning mechanism converge to the same value. It is also interesting to see that both methods use an eligibility trace <inline-formula id="pcbi.1005003.e040"><alternatives><graphic id="pcbi.1005003.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005003.e041"><alternatives><graphic id="pcbi.1005003.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> that are the same if <italic>λ</italic><sub>TD</sub> <italic>γ</italic><sub>TD</sub> = <italic>γ</italic>, i.e. <italic>λ</italic><sub>TD</sub> = <italic>γ</italic>/<italic>γ</italic><sub>eff</sub>. But despite the convergence to the same point and the use of the same eligibility trace, learning moves in general along different trajectories under this form of TD(<italic>λ</italic>) and the learning mechanism we propose.</p>
<p>So far we compared the learning mechanism of prospective coding to the plain TD(<italic>λ</italic>) that has access to the <italic>PSP</italic> and <italic>U</italic>*. If only access to <italic>U</italic> = <italic>λV</italic>*+<italic>U</italic>* is available, it is also possible to combine TD(<italic>λ</italic>) with the bootstrapping effect of predicting the own predictions by implementing a variant of TD(<italic>λ</italic>) in the dendritic compartment of the 2-compartment model. If the delta error is defined as
<disp-formula id="pcbi.1005003.e042"><alternatives><graphic id="pcbi.1005003.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mrow><mml:msub><mml:mi>δ</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>α</mml:mi> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>U</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(19)</label></disp-formula>
one can show that the learning rule in <xref ref-type="disp-formula" rid="pcbi.1005003.e010">Eq 8</xref> is almost identical to the TD learning rule in <xref ref-type="disp-formula" rid="pcbi.1005003.e033">Eq 17</xref> with <italic>λ</italic><sub>TD</sub> = 1 (<xref ref-type="sec" rid="sec013">Methods</xref>). In this case, the weights during learning move along similar trajectories, irrespective of whether this form of TD(1) or our learning rule is used. If this form of TD(1) were not implemented in the 2-compartment model, i.e. if the first term in the delta error in <xref ref-type="disp-formula" rid="pcbi.1005003.e042">Eq 19</xref> would be replaced by <italic>φ</italic>(<italic>U</italic>*(<italic>X</italic><sub><italic>t</italic></sub>)), the time constant of future discounting would be <italic>γ</italic> instead of <italic>γ</italic><sub>eff</sub>. But since the first term in the delta error in <xref ref-type="disp-formula" rid="pcbi.1005003.e042">Eq 19</xref> depends on the full somatic potential <italic>U</italic> = <italic>λV</italic>* + <italic>U</italic>* the bootstrapping effect of predicting the own predictions applies and the large time constant <italic>γ</italic><sub>eff</sub> arises.</p>
</sec>
</sec>
<sec id="sec012" sec-type="conclusions">
<title>Discussion</title>
<p>As a simple and biologically plausible explanation for how animals can learn to predict future events, we have proposed a local plasticity mechanism that leads to prospective coding in spiking neurons, i.e. the plastic synapses change such that the neuron’s current firing rate depends on its expected, future discounted firing rate.</p>
<p>Our model proposes a partial solution to the problem of learning associations on a behavioral timescale without using slow intrinsic processes. Even with a plasticity window that is only slightly larger than the duration of a postsynaptic potential, the effective time constant of discounting the expected future firing rate can be on the order of seconds, thanks to the bootstrapping effect of predicting the own predictions. This effect arises because already predictive inputs influence the activity of a neuron. This is captured by the 2-compartment model of Urbanczik &amp; Senn [<xref ref-type="bibr" rid="pcbi.1005003.ref018">18</xref>], where the output depends on both the dendritic (student) and the somatic (target) input.</p>
<p>For clarity, we presented the model with target input through static (i.e. non-plastic) somatic synapses and in the examples of ramping activity in Figs <xref ref-type="fig" rid="pcbi.1005003.g002">2</xref> and <xref ref-type="fig" rid="pcbi.1005003.g003">3</xref> the somatic input was non-zero only during a short period. This simple form of the target input is not a requirement. First, the learning mechanism also applies to arbitrary time courses of the somatic input, as we show in the example of time series prediction in <xref ref-type="fig" rid="pcbi.1005003.g005">Fig 5</xref>, where an advanced and smoothed version of a complex somatic input is learned. Second, the somatic synapses do not need to be static. Yet, they should change slower than the dendritic synapses in order to get a separation of plasticity timescales. And third, the target input could also arrive at another dendritic branch instead of the soma (see generality of the results in <xref ref-type="sec" rid="sec013">Methods</xref>).</p>
<p>We focused solely on learning temporal associations and neglected important aspects of learning in animals. However, the proposed learning mechanism can easily be extended to include, for example, a weighting based on behavioral relevance. In the delayed paired-associate task, our model learns the associations between sample and target irrespective of the behavioral relevance of this association. In animal training, however, reward or punishment is crucial; for example the monkeys in the study of Rainer et al. [<xref ref-type="bibr" rid="pcbi.1005003.ref001">1</xref>] received juice rewards. The learning rate in our learning mechanism is a free parameter that could incorporate a weighting by behavioral relevance. Biophysically, a neuromodulator like dopamine could implement this modulation of the learning rate. It is also possible to postpone the weight update in <xref ref-type="disp-formula" rid="pcbi.1005003.e002">Eq 1</xref> and use reward modulated eligibility traces instead (see e.g. [<xref ref-type="bibr" rid="pcbi.1005003.ref023">23</xref>–<xref ref-type="bibr" rid="pcbi.1005003.ref025">25</xref>] for theory and [<xref ref-type="bibr" rid="pcbi.1005003.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref026">26</xref>] for experiments).</p>
<p>The proposed learning mechanism could also be involved in classical trace conditioning, where the first stimulus (CS) is separated from the second stimulus (US) and the response (R) by a delay period, similar to the situation in the delayed paired-associate task. Let us assume that neuron 1 in <xref ref-type="fig" rid="pcbi.1005003.g004">Fig 4</xref> is involved in initiating response R (e.g. salivation). If the unconditioned stimulus causes somatic input to this neuron and a memory trace of the conditioned stimulus arrives at the dendritic synapses, our learning mechanism would lead to ramping activity and salivation prior to the onset of the unconditioned stimulus that originally triggered the salivation. To our knowledge, there is no conclusive experimental data to support or discard the hypothesis that prospective coding is involved in classical trace conditioning. In the cited studies on ramping activity [<xref ref-type="bibr" rid="pcbi.1005003.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref006">6</xref>–<xref ref-type="bibr" rid="pcbi.1005003.ref011">11</xref>], the animals were actively engaged in a task (operant conditioning). It is unlikely, however, that the ramping activity is merely a side-effect of movement preparation, since Rainer et al. [<xref ref-type="bibr" rid="pcbi.1005003.ref001">1</xref>] found it to be stimulus-specific but not action-specific.</p>
<p>In our model of delayed paired-associate tasks, activity ramps rely on temporally structured input from short-term memory neurons. The usage of these short-term memory neurons is motivated by the observation that hippocampal activity is needed to overcome the temporal discontiguity in trace conditioning [<xref ref-type="bibr" rid="pcbi.1005003.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref027">27</xref>]. We modeled the dynamics of the recurrent short-term memory network with a stochastic process. The parameter choice of this stochastic process is inspired by the widespread experimental observation that stimulus onset quenches the neural variability [<xref ref-type="bibr" rid="pcbi.1005003.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref029">29</xref>]. It should also be possible to model the memory traces with “dynamical attractors” in recurrent networks of rate neurons [<xref ref-type="bibr" rid="pcbi.1005003.ref030">30</xref>] or with long and stable transient dynamics in balanced networks [<xref ref-type="bibr" rid="pcbi.1005003.ref031">31</xref>]. Since these memory traces are not the main focus of this study we generated them in a simpler way with the stochastic process, which still feels more natural than the delay-line like traces used in a study on trace conditioning [<xref ref-type="bibr" rid="pcbi.1005003.ref032">32</xref>].</p>
<p>In recurrent neural networks the learning rule of prospective coding allows fast replay of slow input sequences (<xref ref-type="fig" rid="pcbi.1005003.g006">Fig 6</xref>). Fast replay could be valuable for planning, where it is important to quickly assess the likely successors of a given state. The same fast replay of a previously induced slower activity sequence was also observed in the rat primary visual cortex [<xref ref-type="bibr" rid="pcbi.1005003.ref033">33</xref>] and it is as well studied as compressed hippocampal replay of a spatial sequence [<xref ref-type="bibr" rid="pcbi.1005003.ref034">34</xref>]. In rats these replay events can be observed minutes or hours after the spatial experience. In contrast, the simple form of the plasticity rule in <xref ref-type="disp-formula" rid="pcbi.1005003.e010">Eq 8</xref> does not have any consolidation properties and ongoing pre- and postsynaptic activity would quickly change the learned weight patterns and thus overwrite the memories. It is, however, straightforward to extend the plasticity model by a consolidation mechanism. In the three state consolidation model of Ziegler et al. [<xref ref-type="bibr" rid="pcbi.1005003.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref036">36</xref>], early long-term potentiation (LTP) is induced by a triplet rule [<xref ref-type="bibr" rid="pcbi.1005003.ref037">37</xref>]. Replacing the triplet rule by the plasticity rule in <xref ref-type="disp-formula" rid="pcbi.1005003.e010">Eq 8</xref> would endow the learning rule of prospective coding with a consolidation mechanism. Such a consolidation mechanism would allow to replay sequences a long time after the training session.</p>
<p>Aiming at a better understanding of biological implementations of prediction learning, our model allows to speculate about physiological realizations of the model variables. Similar to previously proposed plasticity rules [<xref ref-type="bibr" rid="pcbi.1005003.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref018">18</xref>], our learning mechanism depends on the postsynaptic firing rate <italic>φ</italic>(<italic>U</italic>), a function of the dendritic potential <inline-formula id="pcbi.1005003.e043"><alternatives><graphic id="pcbi.1005003.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, the postsynaptic potential <italic>PSP</italic> and, as a new ingredient compared to previous propositions [<xref ref-type="bibr" rid="pcbi.1005003.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref018">18</xref>]: a low-pass filtered version of the postsynaptic potential <inline-formula id="pcbi.1005003.e044"><alternatives><graphic id="pcbi.1005003.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. A plasticity window that is slightly larger than the duration of a postsynaptic potential is in agreement with experimentally measured plasticity window sizes [<xref ref-type="bibr" rid="pcbi.1005003.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref038">38</xref>]. In particular, an increased level of dopamine was observed to expand the effective time window of potentiation to at least ∼45 ms [<xref ref-type="bibr" rid="pcbi.1005003.ref038">38</xref>]. Importantly, even with a plasticity window on this timescale, predictions can be learned on a timescale of seconds due to the bootstrapping effect of predicting the own predictions.</p>
<p>We have shown that the proposed learning mechanism is closely related to temporal difference learning with eligibility traces TD(<italic>λ</italic>). As discussed in the previous paragraph, a local biological implementation of our learning rule seems straightforward. In contrast, it seems more challenging to locally implement the delta error of TD learning. Potjans et al. and Kolodziejski et al. propose a local implementation that depends either on differential Hebbian plasticity [<xref ref-type="bibr" rid="pcbi.1005003.ref039">39</xref>] or on two postsynaptic activity traces with different time constants to approximate the difference in the delta error [<xref ref-type="bibr" rid="pcbi.1005003.ref040">40</xref>]. Both methods require a gating mechanism that allows plasticity only shortly after the onset of a new state and they require transition intervals between states of fixed duration. Furthermore, “state neurons” are only highly active when the agent is in a certain state, which requires the segmentation of the sensory input stream into discrete states. The learning rule we propose does not require these strong assumptions.</p>
<p>Frémaux et al. [<xref ref-type="bibr" rid="pcbi.1005003.ref041">41</xref>] speculate about a non-local implementation of TD learning with spiking neurons, where the TD error is represented by the firing rate of dopaminergic neurons that receive input from three groups of neurons that encode reward, value function and derivative of the value function. In the simulations, however, Frémaux et al. did not use the proposed network implementation of the TD error and they mention that it remains to be seen whether such a circuit can effectively be used to compute a useful TD error. A non-local implementation of the TD error appears compelling in a actor-critic setting, since the actor and the critic can be learned with the same TD signal. However, if the task is to predict more than a scalar quantity like reward, it seems inefficient to use a non-local implementation of the TD error for each quantity to be predicted. Already in our simple example of prospective coding in a recurrent neural network, four TD error networks would be needed in such a non-local implementation.</p>
<p>Generally, associating temporally separated events requires some memory of the first event until the second event is present. Possible neural implementations of this memory rely on long spiking activity traces or on long synaptic eligibility traces. Our model of the delayed paired-associate task relies on long spiking activity traces. The short-term memory network can be seen as a liquid state machine [<xref ref-type="bibr" rid="pcbi.1005003.ref042">42</xref>] or echo state machine [<xref ref-type="bibr" rid="pcbi.1005003.ref043">43</xref>] and the ramping activity is learned as readout from this activity traces. Alternatively, the activity trace could be represented by slowly, exponentially decaying spiking activity after strong stimulation of a cell [<xref ref-type="bibr" rid="pcbi.1005003.ref044">44</xref>]. This proposition, however, fails to explain the experimentally observed activity ramps prior to predictable events [<xref ref-type="bibr" rid="pcbi.1005003.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref006">6</xref>–<xref ref-type="bibr" rid="pcbi.1005003.ref011">11</xref>]</p>
<p>The origin of the ramping activity observed in experiments is not yet fully understood. An alternative to our proposition can be found in recurrent neural network dynamics, where slowly ramping or decaying activity arises with appropriately tuned synaptic weights [<xref ref-type="bibr" rid="pcbi.1005003.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005003.ref025">25</xref>]. In a reinforcement learning setting the time constant of the ramp can be learned by adjusting the recurrent weights with reward modulated Hebbian plasticity [<xref ref-type="bibr" rid="pcbi.1005003.ref045">45</xref>]. Data analysis of recordings in the macaque lateral intraparietal area revealed yet another candidate explanation: single neuron activity profiles could follow a step-like time course, while the averaged activity is a ramp, if the steps occur at different points in time [<xref ref-type="bibr" rid="pcbi.1005003.ref046">46</xref>].</p>
<p>Despite the formal link of our prospective coding algorithm to TD learning, the learning we consider is purely supervised on the level of the neuron. Yet, the same learning rule can also be used to explain conditioning experiments. Instead of the multiplicative modulation by a global reward signal, the reward signal could directly nudge the somatic compartment of the neurons and act as a teaching signal. But the learning rule would also allow for combining the somatic nudging signal with an additional modulatory factor, and nudging and modulatory signals could even be sparse and interleaved. For instance, the rule may explain the simultaneous shaping of predictive motor circuitries by sensory feedback and reward [<xref ref-type="bibr" rid="pcbi.1005003.ref005">5</xref>]. Fluctuating somatic inputs may cause behavioral variations and feedback signals may gate dendritic plasticity such that only rewarded fluctuations act as a target signal for prospective coding. It is also possible to adapt the somatic input connections directly with reinforcement learning, and a ramping activity could arise from learning a prospective code with stimulus-dependent dendritic input.</p>
<p>Since reward is an intrinsic component in animal training, we acquired an advanced knowledge about the neuronal bases of reward prediction. But predictions are not restricted to reward, and predicting the identity of stimuli yields more versatile information. We speculate that prospective coding is more abundant than previously thought and, as we showed, it could easily be implemented on the level of an individual neuron. This view is also consistent with the recently observed future-predicting encoding in the retina [<xref ref-type="bibr" rid="pcbi.1005003.ref047">47</xref>]. To this end, a potentiation window slightly larger than a PSP, together with the bootstrapping effect of predicting the own predictions, is a parsimonious mechanism for learning prospective codes by neurons. A characterisitics of these neurons is that their current firing rate matches their own expected future discounted rate.</p>
</sec>
<sec id="sec013" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec014">
<title>Parameters of the neuron model</title>
<p>The spike response kernel <italic>κ</italic> in <xref ref-type="disp-formula" rid="pcbi.1005003.e003">Eq 2</xref> is given by
<disp-formula id="pcbi.1005003.e045"><alternatives><graphic id="pcbi.1005003.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e045" xlink:type="simple"/><mml:math display="block" id="M45"><mml:mrow><mml:mi>κ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>c</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>H</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi> <mml:mo>/</mml:mo> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi> <mml:mo>/</mml:mo> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="4pt"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(20)</label></disp-formula>
with Heaviside function <italic>H</italic>(<italic>t</italic>) = 0 if <italic>t</italic> &lt; 0 and <italic>H</italic>(<italic>t</italic>) = 1 otherwise, <italic>τ</italic><sub><italic>m</italic></sub> = 10 ms, <italic>τ</italic><sub><italic>s</italic></sub> = 10/3 ms and <inline-formula id="pcbi.1005003.e046"><alternatives><graphic id="pcbi.1005003.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:msup><mml:mi>c</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>∞</mml:mi></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>H</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi> <mml:mo>/</mml:mo> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi> <mml:mo>/</mml:mo> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>We set the somatic capacitance <italic>C</italic> = 1 nF, the leak conductance <italic>g</italic><sub><italic>L</italic></sub> = 100 nS, the coupling conductance <italic>g</italic><sub><italic>D</italic></sub> = 1.8 <italic>μS</italic>, and the excitatory and inhibitory reversal potential <italic>E</italic><sub><italic>E</italic></sub> = 14/3 and <italic>E</italic><sub><italic>I</italic></sub> = -1/3, respectively. The description of the excitatory conductance <italic>g</italic><sub><italic>E</italic></sub>(<italic>t</italic>) is given in the figure captions. The inhibitory nudging conductance <italic>g</italic><sub><italic>I</italic></sub>(<italic>t</italic>) was equal to 0 except for simulations with <inline-formula id="pcbi.1005003.e047"><alternatives><graphic id="pcbi.1005003.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:mrow><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mtext>PSP</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula> in <xref ref-type="fig" rid="pcbi.1005003.g002">Fig 2</xref>, where <italic>g</italic><sub><italic>I</italic></sub>(<italic>t</italic>) = 4<italic>g</italic><sub><italic>E</italic></sub>(<italic>t</italic>). The resting potential is 0 for both, the dendritic potential <italic>V</italic><sub><italic>w</italic></sub> and the somatic potential <italic>U</italic>. If one takes our unitless resting potential of 0 to correspond to -70 mV, and a potential of 1 to correspond to -55 mV, the above choices for <italic>E</italic><sub><italic>E</italic></sub> and <italic>E</italic><sub><italic>I</italic></sub> correspond to reversal potentials of 0 mV (excitation) and -75 mV (inhibition).</p>
<p>The instantaneous firing rate of the neuron is assumed to depend on the somatic membrane potential through function <italic>φ</italic>(<italic>U</italic>), which in the simulations has the form
<disp-formula id="pcbi.1005003.e048"><alternatives><graphic id="pcbi.1005003.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e048" xlink:type="simple"/><mml:math display="block" id="M48"><mml:mrow><mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>U</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo><mml:mfenced close="" open="{" separators=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mi>φ</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mspace width="4.pt"/><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>U</mml:mi> <mml:mo>&gt;</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mspace width="4.pt"/><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>U</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>φ</mml:mi> <mml:mtext>max</mml:mtext></mml:msub> <mml:mo>·</mml:mo> <mml:mi>U</mml:mi></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mtext>otherwise,</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(21)</label></disp-formula>
with <italic>φ</italic><sub>max</sub> = 0.06 kHz. In simulations with spiking, the firing rate multiplied by the simulation time step serves as instantaneous rate of an inhomogeneous Bernoulli process.</p>
</sec>
<sec id="sec015">
<title>Steady state solution of the somatic potential dynamics</title>
<p>For slowly enough changing <italic>I</italic><sub>tot</sub>(<italic>t</italic>) and <italic>g</italic><sub>tot</sub>(<italic>t</italic>), <italic>U</italic>(<italic>t</italic>) is well approximated by <italic>I</italic><sub>tot</sub>(<italic>t</italic>)/<italic>g</italic><sub>tot</sub>(<italic>t</italic>). To see this, we use the ansatz <italic>U</italic>(<italic>t</italic>) = <italic>I</italic><sub>tot</sub>(<italic>t</italic>)/<italic>g</italic><sub>tot</sub>(<italic>t</italic>) + <italic>ϵ</italic>(<italic>t</italic>) in <xref ref-type="disp-formula" rid="pcbi.1005003.e002">Eq 1</xref> and find
<disp-formula id="pcbi.1005003.e049"><alternatives><graphic id="pcbi.1005003.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e049" xlink:type="simple"/><mml:math display="block" id="M49"><mml:mrow><mml:mover accent="true"><mml:mi>ϵ</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:msub><mml:mi>g</mml:mi> <mml:mtext>tot</mml:mtext></mml:msub> <mml:mi>C</mml:mi></mml:mfrac> <mml:mi>ϵ</mml:mi> <mml:mo>-</mml:mo> <mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mtext>tot</mml:mtext></mml:msub> <mml:msub><mml:mi>g</mml:mi> <mml:mtext>tot</mml:mtext></mml:msub></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mtext>tot</mml:mtext></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mtext>tot</mml:mtext></mml:msub></mml:mrow> <mml:msubsup><mml:mi>g</mml:mi> <mml:mtext>tot</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(22)</label></disp-formula>
which leads to the conclusion that the error <italic>ϵ</italic> is small if <inline-formula id="pcbi.1005003.e050"><alternatives><graphic id="pcbi.1005003.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:mrow><mml:mo>|</mml:mo> <mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mtext>tot</mml:mtext></mml:msub> <mml:msub><mml:mi>g</mml:mi> <mml:mtext>tot</mml:mtext></mml:msub></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mtext>tot</mml:mtext></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mtext>tot</mml:mtext></mml:msub></mml:mrow> <mml:msubsup><mml:mi>g</mml:mi> <mml:mtext>tot</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:mo>|</mml:mo> <mml:mo>≪</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> during at least an interval of approximate duration <inline-formula id="pcbi.1005003.e051"><alternatives><graphic id="pcbi.1005003.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:mfrac><mml:mi>C</mml:mi> <mml:msub><mml:mi>g</mml:mi> <mml:mtext>tot</mml:mtext></mml:msub></mml:mfrac></mml:math></alternatives></inline-formula>.</p>
<p>Under these assumptions we write
<disp-formula id="pcbi.1005003.e052"><alternatives><graphic id="pcbi.1005003.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e052" xlink:type="simple"/><mml:math display="block" id="M52"><mml:mrow><mml:mi>U</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mtext>tot</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mtext>tot</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mi>λ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msup><mml:mi>U</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(23)</label></disp-formula>
where we introduce the ‘nudging’ factor <inline-formula id="pcbi.1005003.e053"><alternatives><graphic id="pcbi.1005003.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:mrow><mml:mi>λ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>D</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mtext>tot</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, the attenuated dendritic potential <inline-formula id="pcbi.1005003.e054"><alternatives><graphic id="pcbi.1005003.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>g</mml:mi> <mml:mi>D</mml:mi></mml:msub> <mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>V</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and the attenuated somatic input <inline-formula id="pcbi.1005003.e055"><alternatives><graphic id="pcbi.1005003.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:mrow><mml:msup><mml:mi>U</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mi mathvariant="normal">E</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>E</mml:mi> <mml:mi mathvariant="normal">E</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi mathvariant="normal">I</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>E</mml:mi> <mml:mi mathvariant="normal">I</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mtext>tot</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec016">
<title>Generality of the results</title>
<p>Our main results are robust to variations of the model. For example, the target input <italic>I</italic> could be given by the input through static synapses on another dendritic branch instead of synapses at the soma, i.e. <italic>I</italic>(<italic>t</italic>) = <italic>g</italic><sub><italic>D</italic></sub>(<italic>V</italic><sub><italic>s</italic></sub>(<italic>t</italic>) − <italic>U</italic>). In this case, the nudging factor becomes <inline-formula id="pcbi.1005003.e056"><alternatives><graphic id="pcbi.1005003.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:mrow><mml:mi>λ</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>D</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:msub><mml:mi>g</mml:mi> <mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> and is constant in time.</p>
<p>Modifying the depression term of the learning rule has an effect on the effective time scale <italic>τ</italic><sub>eff</sub>, but large effective time constants are achievable in any case. If the depression term in <xref ref-type="disp-formula" rid="pcbi.1005003.e010">Eq 8</xref> would be replaced by <inline-formula id="pcbi.1005003.e057"><alternatives><graphic id="pcbi.1005003.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:mrow><mml:mo>-</mml:mo> <mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>λ</mml:mi> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, the effective time constant would be <italic>τ</italic><sub>eff</sub> ≈ <italic>τ</italic>/(1 − <italic>α</italic>), i.e. <italic>τ</italic><sub>eff</sub> would be independent of <italic>λ</italic> but still diverge when <italic>α</italic> → 1. Similarly, for a depression term given by −<italic>φ</italic>(<italic>V</italic><sub><italic>w</italic></sub>)PSP<sub><italic>i</italic></sub>, the effective time constant would be <italic>τ</italic><sub>eff</sub> ≈ <italic>τ</italic>/(1 − <italic>λ</italic><sub>2</sub> <italic>α</italic>), with <italic>λ</italic><sub>2</sub> = <italic>g</italic><sub><italic>D</italic></sub>/<italic>g</italic><sub>tot</sub>.</p>
<p>In the current writing of the learning rule, <xref ref-type="disp-formula" rid="pcbi.1005003.e010">Eq 8</xref>, the postsynaptic term arises as instantaneous firing rate <italic>φ</italic>(<italic>U</italic>). But this rate could also be replaced by a postsynaptic sample spike train <italic>S</italic>(<italic>t</italic>) that averages out to this same rate, 〈<italic>S</italic>(<italic>t</italic>)〉 = <italic>φ</italic>(<italic>U</italic>(<italic>t</italic>)). Since learning becomes slower by this sampling, we run our simulations in the form of <xref ref-type="disp-formula" rid="pcbi.1005003.e010">Eq 8</xref>.</p>
</sec>
<sec id="sec017">
<title>Dynamics of short-term memory neurons in the delayed paired associate task</title>
<p>For each STM neuron <italic>i</italic> we first choose template rate trajectories <inline-formula id="pcbi.1005003.e058"><alternatives><graphic id="pcbi.1005003.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mn>1</mml:mn></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> for stimulus A1 and <inline-formula id="pcbi.1005003.e059"><alternatives><graphic id="pcbi.1005003.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> for stimulus A2 by sampling from a mean-zero Ornstein-Uhlenbeck process
<disp-formula id="pcbi.1005003.e060"><alternatives><graphic id="pcbi.1005003.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e060" xlink:type="simple"/><mml:math display="block" id="M60"><mml:mrow><mml:mi>d</mml:mi> <mml:msubsup><mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>s</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msubsup><mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>s</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mi>d</mml:mi> <mml:mi>W</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(24)</label></disp-formula>
where <italic>W</italic> is a Wiener process, 1/<italic>θ</italic><sub>1</sub> = 1000 ms, <italic>σ</italic><sub>1</sub> = 1 and <italic>s</italic> ∈ {1, 2}. Actual rate trajectories <italic>r</italic><sub><italic>i</italic></sub>(<italic>t</italic>) were sampled from a process with trial dependent mean and time dependent variance, i.e.
<disp-formula id="pcbi.1005003.e061"><alternatives><graphic id="pcbi.1005003.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e061" xlink:type="simple"/><mml:math display="block" id="M61"><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msup><mml:mi>μ</mml:mi> <mml:mi>s</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>σ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mi>W</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(25)</label></disp-formula>
where 1/<italic>θ</italic><sub>2</sub> = 100 ms,
<disp-formula id="pcbi.1005003.e062"><alternatives><graphic id="pcbi.1005003.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e062" xlink:type="simple"/><mml:math display="block" id="M62"><mml:mrow><mml:msup><mml:mi>μ</mml:mi> <mml:mi>s</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>σ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>s</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>σ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives> <label>(26)</label></disp-formula>
and <italic>σ</italic><sup>2</sup>(<italic>t</italic>) = 1 if <italic>t</italic> &lt; 0 s or <italic>t</italic> &gt; 3 s, <italic>σ</italic><sup>2</sup>(<italic>t</italic>) = 0.1 otherwise. This assures that in each trial the rate trajectories approach the template trajectories during the presentation of the sample. In between trials, the rate trajectories are independent of the template trajectories. Spike times are determined by sampling from an inhomogeneous Bernoulli process with rate <italic>φ</italic>(<italic>r</italic><sub><italic>i</italic></sub>(<italic>t</italic>))Δ<italic>t</italic>, where Δ<italic>t</italic> is the simulation time step.</p>
</sec>
<sec id="sec018">
<title>Simulation details</title>
<p>The differential equations were integrated with the Euler forward method with step size 0.1 ms. We choose the learning rate <italic>η</italic> = 0.5 in all simulations except for the simulation in <xref ref-type="fig" rid="pcbi.1005003.g002">Fig 2B and 2C</xref> where <italic>η</italic> = 50, since the presynaptic firing rate is low. All simulations are written in C. The plots are generated with Mathematica. The source code is publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jbrea/prospectiveCoding" xlink:type="simple">https://github.com/jbrea/prospectiveCoding</ext-link>.</p>
</sec>
<sec id="sec019">
<title>Stationary point of learning for periodic environments</title>
<p>We assume a stationary environment and rich dendritic input dynamics, such that the dendritic inputs can potentially be predictive of the somatic input. There are different ways to model stationarity of the environment. One way is to restrict the inputs to depend on a stationary latent Markov chain. We consider this case in detail in the next section. Here, to present the main ideas in a mathematically simple form, we look at the artificial case, where stationarity enters through deterministic and periodic functions PSP<sub><italic>i</italic></sub>(<italic>t</italic>) and <italic>U</italic>*(<italic>t</italic>) with period <italic>T</italic>. Under this assumption, learning is at a stationary point when the changes of the weights in <xref ref-type="disp-formula" rid="pcbi.1005003.e010">Eq 8</xref> integrated over one period vanish, i.e.
<disp-formula id="pcbi.1005003.e063"><alternatives><graphic id="pcbi.1005003.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e063" xlink:type="simple"/><mml:math display="block" id="M63"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:mfenced close=")" open="(" separators=""><mml:mi>α</mml:mi> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>U</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(27)</label></disp-formula>
Using the definition of <inline-formula id="pcbi.1005003.e064"><alternatives><graphic id="pcbi.1005003.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1005003.e011">Eq 9</xref> we find
<disp-formula id="pcbi.1005003.e065"><alternatives><graphic id="pcbi.1005003.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e065" xlink:type="simple"/><mml:math display="block" id="M65"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:mfenced close=")" open="(" separators=""><mml:mi>α</mml:mi> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>U</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(28)</label></disp-formula> <disp-formula id="pcbi.1005003.e066"><alternatives><graphic id="pcbi.1005003.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e066" xlink:type="simple"/><mml:math display="block" id="M66"><mml:mrow><mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mi>α</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>U</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mfenced> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(29)</label></disp-formula>
where <xref ref-type="disp-formula" rid="pcbi.1005003.e066">Eq 29</xref> is obtained by changing the order of integration, changing the integration variable <italic>t</italic> to <italic>t</italic> + <italic>s</italic> and using <inline-formula id="pcbi.1005003.e067"><alternatives><graphic id="pcbi.1005003.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e067" xlink:type="simple"/><mml:math display="inline" id="M67"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>s</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, which holds for any <italic>T</italic>-periodic function <italic>f</italic>(<italic>t</italic>). The puzzling transition from an integral that depends on the past values of PSP<sub><italic>i</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1005003.e065">Eq 28</xref> to an integral that depends on the future values of <italic>U</italic> in <xref ref-type="disp-formula" rid="pcbi.1005003.e066">Eq 29</xref> is a result of the assumed stationarity of the environment, which here is expressed in the periodicity of the functions PSP<sub><italic>i</italic></sub>(<italic>t</italic>) and <italic>U</italic>*(<italic>t</italic>). <xref ref-type="disp-formula" rid="pcbi.1005003.e066">Eq 29</xref> holds for all synapses <italic>i</italic>, if
<disp-formula id="pcbi.1005003.e068"><alternatives><graphic id="pcbi.1005003.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e068" xlink:type="simple"/><mml:math display="block" id="M68"><mml:mrow><mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>U</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(30)</label></disp-formula>
Strictly, <xref ref-type="disp-formula" rid="pcbi.1005003.e068">Eq 30</xref> follows from <xref ref-type="disp-formula" rid="pcbi.1005003.e066">Eq 29</xref> only if the inputs PSP<sub><italic>i</italic></sub>(<italic>t</italic>) span the space of square integrable, <italic>T</italic>-periodic functions. In actual implementations the number of synapses is limited, but we find empirically that <xref ref-type="disp-formula" rid="pcbi.1005003.e068">Eq 30</xref> holds approximately at the stationary point if, loosely speaking, the inputs PSP<sub><italic>i</italic></sub>(<italic>t</italic>) at individual synapses are sufficiently rich and different from each other.</p>
<p>The right-hand side of <xref ref-type="disp-formula" rid="pcbi.1005003.e068">Eq 30</xref> also depends on the dendritic potential <inline-formula id="pcbi.1005003.e069"><alternatives><graphic id="pcbi.1005003.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>, since the membrane potential <italic>U</italic> depends both on the dendritic input <inline-formula id="pcbi.1005003.e070"><alternatives><graphic id="pcbi.1005003.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e070" xlink:type="simple"/><mml:math display="inline" id="M70"><mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> and the somatic input <italic>U</italic>* (see <xref ref-type="disp-formula" rid="pcbi.1005003.e006">Eq 4</xref>). Assuming a linear transfer function <italic>φ</italic>, <xref ref-type="disp-formula" rid="pcbi.1005003.e068">Eq 30</xref> becomes
<disp-formula id="pcbi.1005003.e071"><alternatives><graphic id="pcbi.1005003.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e071" xlink:type="simple"/><mml:math display="block" id="M71"><mml:mrow><mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup> <mml:mfenced close=")" open="(" separators=""><mml:mi>λ</mml:mi> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>+</mml:mo> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>U</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(31)</label></disp-formula>
With a Fourier transform and assuming a constant nudging factor <italic>λ</italic> we can solve this equation for <inline-formula id="pcbi.1005003.e072"><alternatives><graphic id="pcbi.1005003.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e072" xlink:type="simple"/><mml:math display="inline" id="M72"><mml:mrow><mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>The Fourier coefficients <inline-formula id="pcbi.1005003.e073"><alternatives><graphic id="pcbi.1005003.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e073" xlink:type="simple"/><mml:math display="inline" id="M73"><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1005003.e074"><alternatives><graphic id="pcbi.1005003.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e074" xlink:type="simple"/><mml:math display="inline" id="M74"><mml:mrow><mml:mi>k</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, of the <italic>T</italic>-periodic function <inline-formula id="pcbi.1005003.e075"><alternatives><graphic id="pcbi.1005003.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e075" xlink:type="simple"/><mml:math display="inline" id="M75"><mml:mrow><mml:mi>h</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> are given by
<disp-formula id="pcbi.1005003.e076"><alternatives><graphic id="pcbi.1005003.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e076" xlink:type="simple"/><mml:math display="block" id="M76"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mn>0</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>i</mml:mi> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mi>k</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac></mml:mrow></mml:msup> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mn>0</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>i</mml:mi> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mi>k</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac></mml:mrow></mml:msup> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mi>k</mml:mi> <mml:mi>s</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac></mml:mrow></mml:msup> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(32)</label></disp-formula> <disp-formula id="pcbi.1005003.e077"><alternatives><graphic id="pcbi.1005003.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e077" xlink:type="simple"/><mml:math display="block" id="M77"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>i</mml:mi> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(33)</label></disp-formula> <disp-formula id="pcbi.1005003.e078"><alternatives><graphic id="pcbi.1005003.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e078" xlink:type="simple"/><mml:math display="block" id="M78"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mo>-</mml:mo> <mml:mi>i</mml:mi> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(34)</label></disp-formula>
where, in the first line, we changed the order of integration, changed the variable <italic>t</italic> to <italic>t</italic> − <italic>s</italic> and used the periodicity of the integrand to obtain <inline-formula id="pcbi.1005003.e079"><alternatives><graphic id="pcbi.1005003.e079g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e079" xlink:type="simple"/><mml:math display="inline" id="M79"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>s</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>i</mml:mi> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mi>k</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac></mml:mrow></mml:msup> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>i</mml:mi> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mi>k</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac></mml:mrow></mml:msup> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. In the second line we introduced the Fourier coefficients <inline-formula id="pcbi.1005003.e080"><alternatives><graphic id="pcbi.1005003.e080g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e080" xlink:type="simple"/><mml:math display="inline" id="M80"><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub></mml:math></alternatives></inline-formula>.</p>
<p>With <inline-formula id="pcbi.1005003.e081"><alternatives><graphic id="pcbi.1005003.e081g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e081" xlink:type="simple"/><mml:math display="inline" id="M81"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <italic>g</italic>(<italic>t</italic>) = <italic>φ</italic>(<italic>U</italic>*(<italic>t</italic>)) we rewrite <xref ref-type="disp-formula" rid="pcbi.1005003.e071">Eq 31</xref> <disp-formula id="pcbi.1005003.e082"><alternatives><graphic id="pcbi.1005003.e082g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e082" xlink:type="simple"/><mml:math display="block" id="M82"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup> <mml:mfenced close=")" open="(" separators=""><mml:mi>λ</mml:mi> <mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>g</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(35)</label></disp-formula>
and Fourier transform both sides to obtain
<disp-formula id="pcbi.1005003.e083"><alternatives><graphic id="pcbi.1005003.e083g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e083" xlink:type="simple"/><mml:math display="block" id="M83"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mfenced close=")" open="(" separators=""><mml:mi>λ</mml:mi> <mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mo>-</mml:mo> <mml:mi>i</mml:mi> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(36)</label></disp-formula>
Solving for <inline-formula id="pcbi.1005003.e084"><alternatives><graphic id="pcbi.1005003.e084g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e084" xlink:type="simple"/><mml:math display="inline" id="M84"><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub></mml:math></alternatives></inline-formula> leads to
<disp-formula id="pcbi.1005003.e085"><alternatives><graphic id="pcbi.1005003.e085g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e085" xlink:type="simple"/><mml:math display="block" id="M85"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mspace width="0.166667em"/><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mo>-</mml:mo> <mml:mi>i</mml:mi> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac></mml:mrow></mml:mfrac> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mi>α</mml:mi> <mml:mi>λ</mml:mi></mml:mrow> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mo>-</mml:mo> <mml:mi>i</mml:mi> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(37)</label></disp-formula> <disp-formula id="pcbi.1005003.e086"><alternatives><graphic id="pcbi.1005003.e086g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e086" xlink:type="simple"/><mml:math display="block" id="M86"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mspace width="0.166667em"/><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mi>λ</mml:mi></mml:mrow> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mo>-</mml:mo> <mml:mi>i</mml:mi> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(38)</label></disp-formula>
This equation has the same structure as <xref ref-type="disp-formula" rid="pcbi.1005003.e078">Eq 34</xref>. With the inverse Fourier transform and assuming <italic>αλ</italic> &lt; 1 we find <xref ref-type="disp-formula" rid="pcbi.1005003.e014">Eq 10</xref>, i.e.
<disp-formula id="pcbi.1005003.e087"><alternatives><graphic id="pcbi.1005003.e087g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e087" xlink:type="simple"/><mml:math display="block" id="M87"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:msub><mml:mi>τ</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub></mml:mfrac></mml:mrow></mml:msup> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(39)</label></disp-formula>
where <inline-formula id="pcbi.1005003.e088"><alternatives><graphic id="pcbi.1005003.e088g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e088" xlink:type="simple"/><mml:math display="inline" id="M88"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>τ</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mi>λ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec020">
<title>Convergence of learning in stationary stochastic environments</title>
<p>We formalize the notion of a stationary environment by introducing a stationary latent Markov chain and restricting the dendritic input PSP<sub><italic>i</italic></sub>(<italic>t</italic>) = PSP<sub><italic>i</italic></sub>(<italic>X</italic><sub><italic>t</italic></sub>) and the somatic input <italic>U</italic>*(<italic>t</italic>) = <italic>U</italic>*(<italic>X</italic><sub><italic>t</italic></sub>) to depend on the state <italic>X</italic><sub><italic>t</italic></sub> of the Markov chain. An alternative way to formalize the notion of stationarity would be to define stationary dynamics of the dendritic inputs and define the correlation between dendritic and somatic input. As it is always possible to reformulate the stationary dendritic input dynamics and the correlation between dendritic and somatic input in terms of a stationary latent Markov chain—with potentially large state space—we stick to the description with a latent Markov chain.</p>
<p>Formally, for time <inline-formula id="pcbi.1005003.e089"><alternatives><graphic id="pcbi.1005003.e089g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e089" xlink:type="simple"/><mml:math display="inline" id="M89"><mml:mrow><mml:mi>t</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, states <italic>X</italic><sub><italic>t</italic></sub> in a finite set <inline-formula id="pcbi.1005003.e090"><alternatives><graphic id="pcbi.1005003.e090g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e090" xlink:type="simple"/><mml:math display="inline" id="M90"><mml:mrow><mml:mi mathvariant="script">X</mml:mi> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> evolve according to a stationary, irreducible Markov chain with transition probabilities <italic>T</italic>(<italic>s</italic><sub><italic>i</italic></sub>,<italic>s</italic><sub><italic>j</italic></sub>) = <italic>Pr</italic>(<italic>X</italic><sub><italic>t</italic>+1</sub> = <italic>s</italic><sub><italic>j</italic></sub>|<italic>X</italic><sub><italic>t</italic></sub> = <italic>s</italic><sub><italic>i</italic></sub>) and stationary distribution <italic>π</italic>(<italic>s</italic><sub><italic>i</italic></sub>) = <italic>Pr</italic>(<italic>X</italic><sub><italic>t</italic></sub> = <italic>s</italic><sub><italic>i</italic></sub>).</p>
<p>Note that the case of deterministic periodic input is readily formulated in terms of a stationary latent Markov chain that cycles deterministically through the state space, e.g. <italic>T</italic>(<italic>s</italic><sub><italic>i</italic></sub>, <italic>s</italic><sub><italic>j</italic></sub>) = 1 if <italic>j</italic> = <italic>i</italic> + 1 or <italic>j</italic> = 1 and <italic>i</italic> = <italic>N</italic> and <italic>T</italic>(<italic>s</italic><sub><italic>i</italic></sub>, <italic>s</italic><sub><italic>j</italic></sub>) = 0 otherwise. Functions that depend only on the state of the Markov chain are thus cyclic with period <italic>N</italic>, e.g. PSP<sub><italic>i</italic></sub>(<italic>X</italic><sub><italic>t</italic></sub>) = PSP<sub><italic>i</italic></sub>(<italic>X</italic><sub><italic>t</italic>+<italic>N</italic></sub>).</p>
<p>In order to switch to matrix notation in the rest of this section, we introduce the following terms:</p>
<list list-type="bullet">
<list-item>
<p>Discounting operator <inline-formula id="pcbi.1005003.e091"><alternatives><graphic id="pcbi.1005003.e091g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e091" xlink:type="simple"/><mml:math display="inline" id="M91"><mml:mrow><mml:mi>A</mml:mi> <mml:mo>=</mml:mo> <mml:mi>α</mml:mi> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:msup><mml:mi>γ</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:msup><mml:mi>T</mml:mi> <mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, with transition matrix <italic>T</italic> and discount factor <italic>γ</italic> ∈ [0, 1).</p>
</list-item>
<list-item>
<p>Postsynaptic potentials <bold>b</bold><sub><italic>i</italic></sub> = (PSP<sub><italic>i</italic></sub>(<italic>s</italic><sub>1</sub>), …,PSP<sub><italic>i</italic></sub>(<italic>s</italic><sub><italic>N</italic></sub>))′ for each synapse <italic>i</italic>.</p>
</list-item>
<list-item>
<p>Matrix of postsynaptic potentials <italic>B</italic> = [<bold>b</bold><sub>1</sub> <bold>b</bold><sub>2</sub>⋯<bold>b</bold><sub><italic>S</italic></sub>], where <italic>S</italic> is the number of synapses.</p>
</list-item>
<list-item>
<p>Postsynaptic firing rates <bold>r</bold><sub><italic>U</italic></sub> = (<italic>φ</italic>(<italic>U</italic>(<italic>s</italic><sub>1</sub>)), …, <italic>φ</italic>(<italic>U</italic>(<italic>s</italic><sub><italic>N</italic></sub>)))′.</p>
</list-item>
<list-item>
<p>Dendritic rates <inline-formula id="pcbi.1005003.e092"><alternatives><graphic id="pcbi.1005003.e092g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e092" xlink:type="simple"/><mml:math display="inline" id="M92"><mml:mrow><mml:msub><mml:mtext>r</mml:mtext><mml:mi>V</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>w</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>w</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</list-item>
<list-item>
<p>Somatic input rates <bold>r</bold><sub><italic>I</italic></sub> = (<italic>φ</italic>(<italic>U</italic>*(<italic>s</italic><sub>1</sub>)), …, <italic>φ</italic>(<italic>U</italic>*(<italic>s</italic><sub><italic>N</italic></sub>)))′.</p>
</list-item>
<list-item>
<p>Expected future discounted firing rate <inline-formula id="pcbi.1005003.e093"><alternatives><graphic id="pcbi.1005003.e093g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e093" xlink:type="simple"/><mml:math display="inline" id="M93"><mml:mrow><mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>U</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>α</mml:mi> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:msup><mml:mi>γ</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:mi>U</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>X</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mi>x</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</list-item>
<list-item>
<p>Expected low-pass filtered postsynaptic potential <inline-formula id="pcbi.1005003.e094"><alternatives><graphic id="pcbi.1005003.e094g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e094" xlink:type="simple"/><mml:math display="inline" id="M94"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:msup><mml:mi>γ</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mtext>PSP</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>X</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mi>x</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</list-item>
</list>
<p>In the following we sketch the proof for the equivalents of Eqs <xref ref-type="disp-formula" rid="pcbi.1005003.e068">30</xref>, <xref ref-type="disp-formula" rid="pcbi.1005003.e018">12</xref> and <xref ref-type="disp-formula" rid="pcbi.1005003.e015">11</xref> in the Markov chain setting. We will make use of the following basic facts about conditional expectations:
<disp-formula id="pcbi.1005003.e095"><alternatives><graphic id="pcbi.1005003.e095g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e095" xlink:type="simple"/><mml:math display="block" id="M95"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mfenced close="]" open="[" separators=""><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>X</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mi>x</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:munderover> <mml:mi>T</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>T</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mi mathvariant="bold">f</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(40)</label></disp-formula> <disp-formula id="pcbi.1005003.e096"><alternatives><graphic id="pcbi.1005003.e096g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e096" xlink:type="simple"/><mml:math display="block" id="M96"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mfenced close="]" open="[" separators=""><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>X</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mi>x</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>π</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:munder> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>π</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>Pr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>π</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">f</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>Π</mml:mo> <mml:msup><mml:mi>T</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(41)</label></disp-formula>
where <italic>t</italic> &gt; 0, <italic>T</italic><sup><italic>t</italic></sup> denotes the matrix power of <italic>T</italic>, Π = diag(<italic>π</italic>(<italic>s</italic><sub>1</sub>),<italic>π</italic>(<italic>s</italic><sub>2</sub>), …, <italic>π</italic>(<italic>s</italic><sub><italic>N</italic></sub>)) is the diagonal “stationary distribution matrix”, column vector <bold>f</bold> = (<italic>f</italic>(<italic>s</italic><sub>1</sub>),<italic>f</italic>(<italic>s</italic><sub>2</sub>), …, <italic>f</italic>(<italic>s</italic><sub><italic>N</italic></sub>))′ and row vector <bold>f</bold>′, the transposed of <bold>f</bold>.</p>
<sec id="sec021">
<title>1. At the fixed point of learning the dendritic rate is proportional to the expected future discounted firing rate (cf. <xref ref-type="disp-formula" rid="pcbi.1005003.e068">Eq 30</xref>)</title>
<p>In discrete time the learning rule in <xref ref-type="disp-formula" rid="pcbi.1005003.e010">Eq 8</xref> becomes
<disp-formula id="pcbi.1005003.e097"><alternatives><graphic id="pcbi.1005003.e097g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e097" xlink:type="simple"/><mml:math display="block" id="M97"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>α</mml:mi> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>U</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mfenced> <mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(42)</label></disp-formula>
with <inline-formula id="pcbi.1005003.e098"><alternatives><graphic id="pcbi.1005003.e098g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e098" xlink:type="simple"/><mml:math display="inline" id="M98"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:msup><mml:mi>γ</mml:mi> <mml:mi>s</mml:mi></mml:msup> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. While Δ<italic>w</italic><sub><italic>t</italic>,<italic>i</italic></sub> is a stochastic variable in general, we will discuss in the following only the corresponding ordinary differential equation (ODE) of the mean
<disp-formula id="pcbi.1005003.e099"><alternatives><graphic id="pcbi.1005003.e099g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e099" xlink:type="simple"/><mml:math display="block" id="M99"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">X</mml:mi></mml:mrow></mml:munder> <mml:mi>π</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mfenced close=")" open="(" separators=""><mml:mi>α</mml:mi> <mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>U</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(43)</label></disp-formula>
where <inline-formula id="pcbi.1005003.e100"><alternatives><graphic id="pcbi.1005003.e100g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e100" xlink:type="simple"/><mml:math display="inline" id="M100"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:msup><mml:mi>γ</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mtext>PSP</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>X</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mi>x</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the expected low-pass filtered postsynaptic potential. This ODE has the same fixed point and convergence behavior as the dynamics in <xref ref-type="disp-formula" rid="pcbi.1005003.e097">Eq 42</xref> under mild assumptions [<xref ref-type="bibr" rid="pcbi.1005003.ref048">48</xref>].</p>
<p>As in Eqs <xref ref-type="disp-formula" rid="pcbi.1005003.e065">28</xref> and <xref ref-type="disp-formula" rid="pcbi.1005003.e066">29</xref>, we are going to show now that we can rewrite the dynamics of the mean synaptic weight in terms of the future discounted firing rate <italic>F</italic>(<italic>x</italic>) instead of the expected low-pass filtered postsynaptic potential <inline-formula id="pcbi.1005003.e101"><alternatives><graphic id="pcbi.1005003.e101g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e101" xlink:type="simple"/><mml:math display="inline" id="M101"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, i.e.
<disp-formula id="pcbi.1005003.e102"><alternatives><graphic id="pcbi.1005003.e102g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e102" xlink:type="simple"/><mml:math display="block" id="M102"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>x</mml:mi></mml:munder> <mml:mi>π</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mfenced close=")" open="(" separators=""><mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mfenced> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(44)</label></disp-formula></p>
<p>This result is a consequence of the assumed stationarity of the Markov chain. It can be found by focusing on the potentiation term in the learning rule in <xref ref-type="disp-formula" rid="pcbi.1005003.e099">Eq 43</xref> and using <xref ref-type="disp-formula" rid="pcbi.1005003.e096">Eq 41</xref> and the notation introduced in the last paragraph, in particular,
<disp-formula id="pcbi.1005003.e103"><alternatives><graphic id="pcbi.1005003.e103g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e103" xlink:type="simple"/><mml:math display="block" id="M103"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>γ</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mfenced close="]" open="[" separators=""><mml:mtext>PSP</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>X</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mi>x</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>π</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mfenced close=")" open="(" separators=""><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>γ</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:msup><mml:mi mathvariant="bold">b</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>Π</mml:mo> <mml:msup><mml:mi>T</mml:mi> <mml:mi>t</mml:mi></mml:msup></mml:mfenced> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>π</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo> <mml:mi>α</mml:mi></mml:mrow></mml:mfrac> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">b</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>Π</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(45)</label></disp-formula>
which leads to
<disp-formula id="pcbi.1005003.e104"><alternatives><graphic id="pcbi.1005003.e104g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e104" xlink:type="simple"/><mml:math display="block" id="M104"><mml:mrow><mml:munder><mml:mo>∑</mml:mo> <mml:mi>x</mml:mi></mml:munder> <mml:mi>π</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>α</mml:mi> <mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>U</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">b</mml:mi> <mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>Π</mml:mo> <mml:mi>A</mml:mi> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>U</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>x</mml:mi></mml:munder> <mml:mi>π</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>U</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>x</mml:mi></mml:munder> <mml:mi>π</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(46)</label></disp-formula>
Using this equality in <xref ref-type="disp-formula" rid="pcbi.1005003.e099">Eq 43</xref> leads to <xref ref-type="disp-formula" rid="pcbi.1005003.e102">Eq 44</xref>.</p>
<p>Assuming a trivial kernel for <italic>B</italic>Π, i.e. <italic>B</italic>Π<bold>x</bold> = <bold>0</bold> ⇔ <bold>x</bold> = <bold>0</bold>, we find by looking at <xref ref-type="disp-formula" rid="pcbi.1005003.e102">Eq 44</xref> that
<disp-formula id="pcbi.1005003.e105"><alternatives><graphic id="pcbi.1005003.e105g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e105" xlink:type="simple"/><mml:math display="block" id="M105"><mml:mrow><mml:mo>∀</mml:mo> <mml:mi>i</mml:mi> <mml:mo>:</mml:mo> <mml:mspace width="0.166667em"/><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>⇔</mml:mo> <mml:mo>∀</mml:mo> <mml:mi>x</mml:mi> <mml:mo>:</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(47)</label></disp-formula>
which is analogous to the statement in <xref ref-type="disp-formula" rid="pcbi.1005003.e068">Eq 30</xref>. The assumption of a trivial kernel of <italic>B</italic>Π implies that the map PSP(<italic>s</italic><sub><italic>i</italic></sub>) from the state space of the latent Markov chain to dendritic inputs is one-to-one. This assumption is analogous to the statement that the dendritic inputs PSP<sub><italic>i</italic></sub>(<italic>t</italic>) at individual synapses are sufficiently rich and different from each other (see <xref ref-type="sec" rid="sec002">Results</xref> after <xref ref-type="disp-formula" rid="pcbi.1005003.e068">Eq 30</xref>).</p>
</sec>
<sec id="sec022">
<title>2. At the fixed point of learning the dendritic rate that is proportional to the expected future discounted somatic input rate, but with a longer discount time constant (cf. <xref ref-type="disp-formula" rid="pcbi.1005003.e018">Eq 12</xref>)</title>
<p>Since the future discounted firing rate <italic>F</italic>(<italic>x</italic>) depends on <inline-formula id="pcbi.1005003.e106"><alternatives><graphic id="pcbi.1005003.e106g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e106" xlink:type="simple"/><mml:math display="inline" id="M106"><mml:mrow><mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, it is not trivial to solve <xref ref-type="disp-formula" rid="pcbi.1005003.e105">Eq 47</xref> for <inline-formula id="pcbi.1005003.e107"><alternatives><graphic id="pcbi.1005003.e107g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e107" xlink:type="simple"/><mml:math display="inline" id="M107"><mml:mrow><mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Similar as in the result section in Eqs <xref ref-type="disp-formula" rid="pcbi.1005003.e071">31</xref> and <xref ref-type="disp-formula" rid="pcbi.1005003.e014">10</xref>, however, we can show for a linear <italic>φ</italic> and constant <italic>λ</italic>(<italic>s</italic><sub><italic>i</italic></sub>) = <italic>λ</italic> that
<disp-formula id="pcbi.1005003.e108"><alternatives><graphic id="pcbi.1005003.e108g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e108" xlink:type="simple"/><mml:math display="block" id="M108"><mml:mrow><mml:mi>φ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mi>w</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mi>α</mml:mi></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msubsup><mml:mi>γ</mml:mi> <mml:mtext>eff</mml:mtext> <mml:mi>t</mml:mi></mml:msubsup> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mfenced close="]" open="[" separators=""><mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>U</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>|</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mi>x</mml:mi></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(48)</label></disp-formula>
with <inline-formula id="pcbi.1005003.e109"><alternatives><graphic id="pcbi.1005003.e109g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e109" xlink:type="simple"/><mml:math display="inline" id="M109"><mml:mrow><mml:msub><mml:mi>γ</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>γ</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. Indeed, assuming linear <italic>φ</italic> we can rewrite <xref ref-type="disp-formula" rid="pcbi.1005003.e105">Eq 47</xref> in vector notation and solve for <bold>r</bold><sub><italic>V</italic></sub> to obtain
<disp-formula id="pcbi.1005003.e110"><alternatives><graphic id="pcbi.1005003.e110g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e110" xlink:type="simple"/><mml:math display="block" id="M110"><mml:mrow><mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>V</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>A</mml:mi> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>U</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>λ</mml:mi> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>V</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>I</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(49)</label></disp-formula> <disp-formula id="pcbi.1005003.e111"><alternatives><graphic id="pcbi.1005003.e111g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e111" xlink:type="simple"/><mml:math display="block" id="M111"><mml:mrow><mml:mo>⇒</mml:mo> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>V</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mi>A</mml:mi> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>I</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>λ</mml:mi> <mml:mi>s</mml:mi></mml:msup> <mml:msup><mml:mi>A</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mspace width="0.166667em"/><mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>I</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(50)</label></disp-formula>
where we have assumed that <italic>λα</italic> &lt; 1 − <italic>γ</italic> such that the series converges. Powers of <italic>A</italic> evaluate to
<disp-formula id="pcbi.1005003.e112"><alternatives><graphic id="pcbi.1005003.e112g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e112" xlink:type="simple"/><mml:math display="block" id="M112"><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:munderover><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:munderover><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>t</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msup><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:math></alternatives> <label>(51)</label></disp-formula>
and thus we can rewrite <xref ref-type="disp-formula" rid="pcbi.1005003.e111">Eq 50</xref> to get,
<disp-formula id="pcbi.1005003.e113"><alternatives><graphic id="pcbi.1005003.e113g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e113" xlink:type="simple"/><mml:math display="block" id="M113"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>r</mml:mi></mml:mstyle><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:munderover><mml:mrow><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:munderover><mml:mrow><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>t</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>︸</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msup><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msup><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>r</mml:mi></mml:mstyle><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>α</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:msup><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>r</mml:mi></mml:mstyle><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(52)</label></disp-formula>
which proves the claim in <xref ref-type="disp-formula" rid="pcbi.1005003.e108">Eq 48</xref>. The effective time constant <inline-formula id="pcbi.1005003.e114"><alternatives><graphic id="pcbi.1005003.e114g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e114" xlink:type="simple"/><mml:math display="inline" id="M114"><mml:mrow><mml:msub><mml:mi>γ</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>γ</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> can be much larger than <italic>γ</italic>. In fact, for <italic>α</italic> → (1 − <italic>γ</italic>)/<italic>λ</italic> we find <italic>γ</italic><sub>eff</sub> → 1.</p>
</sec>
<sec id="sec023">
<title>Remarks</title>
<list list-type="order">
<list-item>
<p>For affine <italic>φ</italic>(<italic>u</italic>) = <italic>a</italic> ⋅ <italic>u</italic> + <italic>c</italic> the equivalent of <xref ref-type="disp-formula" rid="pcbi.1005003.e111">Eq 50</xref> is
<disp-formula id="pcbi.1005003.e115"><alternatives><graphic id="pcbi.1005003.e115g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e115" xlink:type="simple"/><mml:math display="block" id="M115"><mml:mrow><mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>V</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mi>A</mml:mi></mml:mfenced> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mi>A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>I</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(53)</label></disp-formula>
In a first order approximation, the stationary <bold>r</bold><sub><italic>V</italic></sub> for a non-linear <italic>φ</italic> is thus a translated version of the stationary solution for a linear <italic>φ</italic>.</p>
</list-item>
<list-item>
<p>For input <bold>r</bold><sub><italic>I</italic></sub> = 0 and linear <italic>φ</italic> we find that at the stationary point of learning <bold>r</bold><sub><italic>V</italic></sub> = 0. Thus we expect that learned weights decay again, once input <bold>r</bold><sub><italic>I</italic></sub> is removed.</p>
</list-item>
</list>
</sec>
<sec id="sec024">
<title>3. Convergence of learning</title>
<p>For linear <italic>φ</italic>(<italic>u</italic>) = <italic>ϕ</italic> ⋅ <italic>u</italic> with <italic>ϕ</italic> &gt; 0, we have <bold>r</bold><sub><italic>V</italic></sub> = <italic>ϕB</italic> <bold>w</bold> and thus the learning rule in <xref ref-type="disp-formula" rid="pcbi.1005003.e099">Eq 43</xref> can be written in vector notation as
<disp-formula id="pcbi.1005003.e116"><alternatives><graphic id="pcbi.1005003.e116g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e116" xlink:type="simple"/><mml:math display="block" id="M116"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:msup><mml:mi>B</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>Π</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mi>A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Λ</mml:mo> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>V</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>I</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>V</mml:mi></mml:msub></mml:mfenced> <mml:mo>=</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mi>η</mml:mi> <mml:mi>ϕ</mml:mi> <mml:msup><mml:mi>B</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>Π</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mi>A</mml:mi> <mml:mo>Λ</mml:mo> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mfenced> <mml:mi>B</mml:mi></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mo>=</mml:mo> <mml:mi>X</mml:mi></mml:mrow></mml:munder> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>+</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mi>η</mml:mi> <mml:msup><mml:mi>B</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>Π</mml:mo> <mml:mi>A</mml:mi> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>I</mml:mi></mml:msub></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mo>=</mml:mo> <mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:munder> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(54)</label></disp-formula>
where we introduced the diagonal ‘nudging matrix’ Λ = diag(<italic>λ</italic>(<italic>s</italic><sub>1</sub>),…,<italic>λ</italic>(<italic>s</italic><sub><italic>N</italic></sub>)).</p>
<p>With <bold>w</bold>* = −<italic>X</italic><sup>+</sup> <bold>c</bold> + (1 − <italic>X</italic><sup>+</sup> <italic>X</italic>)<bold>w</bold> the orthogonal projection of <bold>w</bold> onto <italic>W</italic>* = {<bold>w</bold>|<italic>X</italic> <bold>w</bold> = −<bold>c</bold>}, where <italic>X</italic><sup>+</sup> denotes the Moore-Penrose pseudoinverse of <italic>X</italic>, we are going to show that <inline-formula id="pcbi.1005003.e117"><alternatives><graphic id="pcbi.1005003.e117g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e117" xlink:type="simple"/><mml:math display="inline" id="M117"><mml:mrow><mml:mi>L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>-</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is a Lyapunov function of the dynamics in <xref ref-type="disp-formula" rid="pcbi.1005003.e116">Eq 54</xref>. With <bold>y</bold> = <bold>w</bold> − <bold>w</bold>* and <inline-formula id="pcbi.1005003.e118"><alternatives><graphic id="pcbi.1005003.e118g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e118" xlink:type="simple"/><mml:math display="inline" id="M118"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi> <mml:mo>=</mml:mo> <mml:msqrt><mml:mrow><mml:mi>η</mml:mi> <mml:mi>ϕ</mml:mi></mml:mrow></mml:msqrt> <mml:mi>B</mml:mi> <mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, the temporal evolution of <italic>L</italic> is given by
<disp-formula id="pcbi.1005003.e119"><alternatives><graphic id="pcbi.1005003.e119g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e119" xlink:type="simple"/><mml:math display="block" id="M119"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mover><mml:mi>L</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>∇</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mover><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>y</mml:mi></mml:mstyle><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>y</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>c</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>y</mml:mi></mml:mstyle><mml:mo>′</mml:mo></mml:msup><mml:mi>X</mml:mi><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mi>ϕ</mml:mi><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>y</mml:mi></mml:mstyle><mml:mo>′</mml:mo></mml:msup><mml:msup><mml:mi>B</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>Π</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>Λ</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>B</mml:mi><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo>〈</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>Λ</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mo>〉</mml:mo><mml:mo>−</mml:mo><mml:mo>〈</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mo>〉</mml:mo><mml:mo>≤</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where we defined the scalar product 〈<bold>x</bold>,<bold>y</bold>〉 = <bold>x</bold>′ Π <bold>y</bold> and the inequality follows since both <italic>A</italic> and Λ are contracting maps, i.e. <inline-formula id="pcbi.1005003.e120"><alternatives><graphic id="pcbi.1005003.e120g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e120" xlink:type="simple"/><mml:math display="inline" id="M120"><mml:mrow><mml:msqrt><mml:mrow><mml:mo>〈</mml:mo> <mml:mi>A</mml:mi> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>,</mml:mo> <mml:mi>A</mml:mi> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>〉</mml:mo></mml:mrow></mml:msqrt> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>∥</mml:mo> <mml:mi>A</mml:mi> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>∥</mml:mo></mml:mrow> <mml:mo>≤</mml:mo> <mml:mrow><mml:mo>∥</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>∥</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and therefore
<disp-formula id="pcbi.1005003.e121"><alternatives><graphic id="pcbi.1005003.e121g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e121" xlink:type="simple"/><mml:math display="block" id="M121"><mml:mrow><mml:mo>〈</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>,</mml:mo> <mml:mi>A</mml:mi> <mml:mo>Λ</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>〉</mml:mo> <mml:mo>≤</mml:mo> <mml:mo>∥</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>∥</mml:mo> <mml:mo>∥</mml:mo> <mml:mi>A</mml:mi> <mml:mo>Λ</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>∥</mml:mo> <mml:mo>≤</mml:mo> <mml:mo>∥</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>∥</mml:mo> <mml:mo>∥</mml:mo> <mml:mo>Λ</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>∥</mml:mo> <mml:mo>≤</mml:mo> <mml:mo>∥</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>∥</mml:mo> <mml:mo>∥</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>∥</mml:mo> <mml:mo>=</mml:mo> <mml:mo>〈</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>〉</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(55)</label></disp-formula> Λ is contracting because it is diagonal with entries between 0 and 1 and <italic>A</italic> is contracting because
<disp-formula id="pcbi.1005003.e122"><alternatives><graphic id="pcbi.1005003.e122g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e122" xlink:type="simple"/><mml:math display="block" id="M122"><mml:mrow><mml:msup><mml:mrow><mml:mo>∥</mml:mo> <mml:mi>A</mml:mi> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:mi>π</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:munder><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:munder> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>z</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:mo>≤</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:mi>π</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:munder><mml:munder accentunder="true"><mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:msubsup><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mfenced> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mo>&lt;</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:munder> <mml:mfenced close=")" open="(" separators=""><mml:munder><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:munder> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mfenced> <mml:mo>≤</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:mi>π</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:munder> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>∥</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(56)</label></disp-formula>
where we used the facts that 0 ≤ <italic>A</italic><sub><italic>ij</italic></sub> &lt; 1 and the row sums of <italic>A</italic> are equal to <inline-formula id="pcbi.1005003.e123"><alternatives><graphic id="pcbi.1005003.e123g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e123" xlink:type="simple"/><mml:math display="inline" id="M123"><mml:mfrac><mml:mi>α</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula> and therefore <inline-formula id="pcbi.1005003.e124"><alternatives><graphic id="pcbi.1005003.e124g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e124" xlink:type="simple"/><mml:math display="inline" id="M124"><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:msubsup><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>≤</mml:mo> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>&lt;</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
</sec>
<sec id="sec025">
<title>Relation between the learning rule in <xref ref-type="disp-formula" rid="pcbi.1005003.e097">Eq 42</xref> and TD(1)</title>
<p>For <italic>λ</italic><sub>TD</sub> = 1 and therefore <inline-formula id="pcbi.1005003.e125"><alternatives><graphic id="pcbi.1005003.e125g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e125" xlink:type="simple"/><mml:math display="inline" id="M125"><mml:mrow><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, we can rewrite <xref ref-type="disp-formula" rid="pcbi.1005003.e033">Eq 17</xref> by expanding the delta error in <xref ref-type="disp-formula" rid="pcbi.1005003.e042">Eq 19</xref> and using the identity <inline-formula id="pcbi.1005003.e126"><alternatives><graphic id="pcbi.1005003.e126g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e126" xlink:type="simple"/><mml:math display="inline" id="M126"><mml:mrow><mml:mi>γ</mml:mi> <mml:mspace width="0.166667em"/><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mtext>PSP</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> to find
<disp-formula id="pcbi.1005003.e127"><alternatives><graphic id="pcbi.1005003.e127g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e127" xlink:type="simple"/><mml:math display="block" id="M127"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mtext>PSP</mml:mtext></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mtext>PSP</mml:mtext></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mtext>PSP</mml:mtext></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext>PS</mml:mtext><mml:msub><mml:mtext>P</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mtext>PSP</mml:mtext></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mtext>PSP</mml:mtext></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>PS</mml:mtext><mml:msub><mml:mtext>P</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(57)</label></disp-formula>
<disp-formula id="pcbi.1005003.e128"><alternatives><graphic id="pcbi.1005003.e128g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e128" xlink:type="simple"/><mml:math display="block" id="M128"><mml:mrow><mml:mo>+</mml:mo> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mi>φ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>V</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mspace width="0.166667em"/><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(58)</label></disp-formula>
With small parameter updates in each time step, the terms in <xref ref-type="disp-formula" rid="pcbi.1005003.e128">Eq 58</xref> approximately cancel each other when summing over subsequent terms: <inline-formula id="pcbi.1005003.e129"><alternatives><graphic id="pcbi.1005003.e129g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e129" xlink:type="simple"/><mml:math display="inline" id="M129"><mml:mrow><mml:msub><mml:mi>δ</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> contributes <inline-formula id="pcbi.1005003.e130"><alternatives><graphic id="pcbi.1005003.e130g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e130" xlink:type="simple"/><mml:math display="inline" id="M130"><mml:mrow><mml:mo>+</mml:mo> <mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005003.e131"><alternatives><graphic id="pcbi.1005003.e131g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e131" xlink:type="simple"/><mml:math display="inline" id="M131"><mml:mrow><mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mspace width="0.166667em"/><mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> contributes <inline-formula id="pcbi.1005003.e132"><alternatives><graphic id="pcbi.1005003.e132g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005003.e132" xlink:type="simple"/><mml:math display="inline" id="M132"><mml:mrow><mml:mo>-</mml:mo> <mml:mi>φ</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>PSP</mml:mtext> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. What remains are the terms in <xref ref-type="disp-formula" rid="pcbi.1005003.e127">Eq 57</xref>, which resemble the terms in the learning rule in <xref ref-type="disp-formula" rid="pcbi.1005003.e097">Eq 42</xref>.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>We thank Christian Pozzorini, Laureline Logiaco and Kristin Völk for valuable comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005003.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rainer</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Rao</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name> (<year>1999</year>) <article-title>Prospective Coding for Objects in Primate Prefrontal Cortex</article-title>. <source>The Journal of Neuroscience</source> <volume>19</volume>: <fpage>5493</fpage>–<lpage>5505</lpage>. <object-id pub-id-type="pmid">10377358</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Reutimann</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Yakovlev</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name> (<year>2004</year>) <article-title>Climbing Neuronal Activity as an Event-Based Cortical Representation of Time</article-title>. <source>Journal of Neuroscience</source> <volume>24</volume>: <fpage>3295</fpage>–<lpage>3303</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4098-03.2004" xlink:type="simple">10.1523/JNEUROSCI.4098-03.2004</ext-link></comment> <object-id pub-id-type="pmid">15056709</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref003">
<label>3</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Pavlov</surname> <given-names>IP</given-names></name> (<year>1927</year>). <source>Conditioned reflexes: an investigation of the physiological activity of the cerebral cortex</source>. <publisher-name>Oxford Univ. Press</publisher-name></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bangasser</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Waxler</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Santollo</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Shors</surname> <given-names>TJ</given-names></name> (<year>2006</year>) <article-title>Trace conditioning and the hippocampus: the importance of contiguity</article-title>. <source>The Journal of Neuroscience</source> <volume>26</volume>: <fpage>8702</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1742-06.2006" xlink:type="simple">10.1523/JNEUROSCI.1742-06.2006</ext-link></comment> <object-id pub-id-type="pmid">16928858</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>Tw</given-names></name>, <name name-style="western"><surname>Guo</surname> <given-names>ZV</given-names></name>, <name name-style="western"><surname>Gerfen</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Svoboda</surname> <given-names>K</given-names></name> (<year>2016</year>) <article-title>A motor cortex circuit for motor planning and movement</article-title>. <source>Nature</source> <volume>519</volume>: <fpage>51</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature14178" xlink:type="simple">10.1038/nature14178</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>Y</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>Dorsal raphe neurons signal reward through 5-HT and glutamate</article-title>. <source>Neuron</source> <volume>81</volume>: <fpage>1360</fpage>–<lpage>74</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2014.02.010" xlink:type="simple">10.1016/j.neuron.2014.02.010</ext-link></comment> <object-id pub-id-type="pmid">24656254</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Howe</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Tierney</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Sandberg</surname> <given-names>SG</given-names></name>, <name name-style="western"><surname>Phillips</surname> <given-names>PEM</given-names></name>, <name name-style="western"><surname>Graybiel</surname> <given-names>AM</given-names></name> (<year>2013</year>) <article-title>Prolonged dopamine signalling in striatum signals proximity and value of distant rewards</article-title>. <source>Nature</source> <volume>500</volume>: <fpage>575</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature12475" xlink:type="simple">10.1038/nature12475</ext-link></comment> <object-id pub-id-type="pmid">23913271</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van der Meer</surname> <given-names>MAA</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name> (<year>2011</year>) <article-title>Theta phase precession in rat ventral striatum links place and reward information</article-title>. <source>The Journal of Neuroscience</source> <volume>31</volume>: <fpage>2843</fpage>–<lpage>2854</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4869-10.2011" xlink:type="simple">10.1523/JNEUROSCI.4869-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21414906</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Quintana</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Fuster</surname> <given-names>JM</given-names></name> (<year>1999</year>) <article-title>From perception to action: Temporal integrative functions of prefrontal and parietal neurons</article-title>. <source>Cerebral Cortex</source> <volume>9</volume>: <fpage>213</fpage>–<lpage>221</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/9.3.213" xlink:type="simple">10.1093/cercor/9.3.213</ext-link></comment> <object-id pub-id-type="pmid">10355901</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name>, <name name-style="western"><surname>Erickson</surname> <given-names>Ca</given-names></name>, <name name-style="western"><surname>Desimone</surname> <given-names>R</given-names></name> (<year>1996</year>) <article-title>Neural mechanisms of visual working memory in prefrontal cortex of the macaque</article-title>. <source>The Journal of Neuroscience</source> <volume>16</volume>: <fpage>5154</fpage>–<lpage>5167</lpage>. <object-id pub-id-type="pmid">8756444</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sakai</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Miyashita</surname> <given-names>Y</given-names></name> (<year>1991</year>) <article-title>Neural organization for the long-term memory of paired associates</article-title>. <source>Nature</source> <volume>354</volume>: <fpage>152</fpage>–<lpage>155</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/354152a0" xlink:type="simple">10.1038/354152a0</ext-link></comment> <object-id pub-id-type="pmid">1944594</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lübke</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Frotscher</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sakmann</surname> <given-names>B</given-names></name> (<year>1997</year>) <article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs</article-title>. <source>Science</source> <volume>275</volume>: <fpage>213</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.275.5297.213" xlink:type="simple">10.1126/science.275.5297.213</ext-link></comment> <object-id pub-id-type="pmid">8985014</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bi</surname> <given-names>GQ</given-names></name>, <name name-style="western"><surname>Poo</surname> <given-names>M</given-names></name> (<year>2001</year>) <article-title>Synaptic modification by correlated activity: Hebb’s postulate revisited</article-title>. <source>Annu Rev Neurosci</source> <volume>24</volume>: <fpage>139</fpage>–<lpage>66</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.24.1.139" xlink:type="simple">10.1146/annurev.neuro.24.1.139</ext-link></comment> <object-id pub-id-type="pmid">11283308</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cichon</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Gan</surname> <given-names>W</given-names></name> (<year>2015</year>) <article-title>Branch-specific dendritic Ca2+ spikes cause persistent synaptic plasticity</article-title>. <source>Nature</source> <volume>520</volume>: <fpage>180</fpage>–<lpage>5</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature14251" xlink:type="simple">10.1038/nature14251</ext-link></comment> <object-id pub-id-type="pmid">25822789</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yagishita</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hayashi-Takagi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ellis-Davies</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Urakubo</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Ishii</surname> <given-names>S</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>A critical time window for dopamine actions on the structural plasticity of dendritic spines</article-title>. <source>Science</source> <volume>345</volume>: <fpage>1616</fpage>–<lpage>1620</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1255514" xlink:type="simple">10.1126/science.1255514</ext-link></comment> <object-id pub-id-type="pmid">25258080</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pfister</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Toyoizumi</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Barber</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name> (<year>2006</year>) <article-title>Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning</article-title>. <source>Neural Computation</source> <volume>18</volume>: <fpage>1318</fpage>–<lpage>1348</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2006.18.6.1318" xlink:type="simple">10.1162/neco.2006.18.6.1318</ext-link></comment> <object-id pub-id-type="pmid">16764506</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brea</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Pfister</surname> <given-names>JP</given-names></name> (<year>2013</year>) <article-title>Matching recall and storage in sequence learning with spiking neural networks</article-title>. <source>The Journal of Neuroscience</source> <volume>33</volume>: <fpage>9565</fpage>–<lpage>75</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4098-12.2013" xlink:type="simple">10.1523/JNEUROSCI.4098-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23739954</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Urbanczik</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name> (<year>2014</year>) <article-title>Learning by the Dendritic Prediction of Somatic Spiking</article-title>. <source>Neuron</source> <volume>81</volume>: <fpage>521</fpage>–<lpage>528</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2013.11.030" xlink:type="simple">10.1016/j.neuron.2013.11.030</ext-link></comment> <object-id pub-id-type="pmid">24507189</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wise</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Boussaoud</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Caminiti</surname> <given-names>R</given-names></name> (<year>1997</year>) <article-title>Premotor and parietal cortex: corticocortical connectivity and combinatorial computations</article-title>. <source>Annu Rev Neurosci</source> <volume>20</volume>: <fpage>25</fpage>–<lpage>42</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.20.1.25" xlink:type="simple">10.1146/annurev.neuro.20.1.25</ext-link></comment> <object-id pub-id-type="pmid">9056706</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jaakkola</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Jordan</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>S</given-names></name> (<year>1994</year>) <article-title>On the Convergence of Stochastic Iterative Dynamic Programming Algorithms</article-title>. <source>Neural Computation</source> <volume>6</volume>: <fpage>1185</fpage>–<lpage>1201</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1994.6.6.1185" xlink:type="simple">10.1162/neco.1994.6.6.1185</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name> (<year>1992</year>) <article-title>The convergence of TD(<italic>λ</italic>) for general <italic>λ</italic></article-title>. <source>Machine Learning</source> <volume>8</volume>: <fpage>341</fpage>–<lpage>362</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00992701" xlink:type="simple">10.1007/BF00992701</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name> (<year>1988</year>) <article-title>Learning to Predict by the Methods of Temporal Differences</article-title>. <source>Machine Learning</source> <volume>3</volume>: <fpage>9</fpage>–<lpage>44</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1022633531479" xlink:type="simple">10.1023/A:1022633531479</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Urbanczik</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name> (<year>2009</year>) <article-title>Reinforcement learning in populations of spiking neurons</article-title>. <source>Nature Neuroscience</source> <volume>12</volume>: <fpage>250</fpage>–<lpage>2</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2264" xlink:type="simple">10.1038/nn.2264</ext-link></comment> <object-id pub-id-type="pmid">19219040</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friedrich</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Urbanczik</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name> (<year>2011</year>) <article-title>Spatio-temporal credit assignment in neuronal population learning</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1002092</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002092" xlink:type="simple">10.1371/journal.pcbi.1002092</ext-link></comment> <object-id pub-id-type="pmid">21738460</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gavornik</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Shuler</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Loewenstein</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bear</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shouval</surname> <given-names>HZ</given-names></name> (<year>2009</year>) <article-title>Learning reward timing in cortex through reward dependent expression of synaptic plasticity. Supporting Material</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>106</volume>: <fpage>6826</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0901835106" xlink:type="simple">10.1073/pnas.0901835106</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>He</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Huertas</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tie</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Hell</surname> <given-names>J</given-names></name>, <etal>et al</etal>. (<year>2015</year>) <article-title>Distinct Eligibility Traces for LTP and LTD in Cortical Synapses</article-title>. <source>Neuron</source>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2015.09.037" xlink:type="simple">10.1016/j.neuron.2015.09.037</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Solomon</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Vander Schaaf</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>RF</given-names></name>, <name name-style="western"><surname>Weisz</surname> <given-names>DJ</given-names></name> (<year>1986</year>) <article-title>Hippocampus and trace conditioning of the rabbit’s classically conditioned nictitating membrane response</article-title>. <source>Behavioral Neuroscience</source> <volume>100</volume>: <fpage>729</fpage>–<lpage>744</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0735-7044.100.5.729" xlink:type="simple">10.1037/0735-7044.100.5.729</ext-link></comment> <object-id pub-id-type="pmid">3778636</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Churchland</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Sugrue</surname> <given-names>LP</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>MR</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Stimulus onset quenches neural variability: a widespread cortical phenomenon</article-title>. <source>Nature neuroscience</source> <volume>13</volume>: <fpage>369</fpage>–<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2501" xlink:type="simple">10.1038/nn.2501</ext-link></comment> <object-id pub-id-type="pmid">20173745</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Churchland</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name> (<year>2012</year>) <article-title>Two layers of neural variability</article-title>. <source>Nature Neuroscience</source> <volume>15</volume>: <fpage>1472</fpage>–<lpage>1474</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3247" xlink:type="simple">10.1038/nn.3247</ext-link></comment> <object-id pub-id-type="pmid">23103992</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Laje</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Buonomano</surname> <given-names>DV</given-names></name> (<year>2013</year>) <article-title>Robust timing and motor patterns by taming chaos in recurrent neural networks</article-title>. <source>Nature Neuroscience</source> <volume>16</volume>: <fpage>925</fpage>–<lpage>933</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3405" xlink:type="simple">10.1038/nn.3405</ext-link></comment> <object-id pub-id-type="pmid">23708144</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hennequin</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name> (<year>2014</year>) <article-title>Optimal Control of Transient Dynamics in Balanced Networks Supports Generation of Complex Movements</article-title>. <source>Neuron</source> <volume>82</volume>: <fpage>1394</fpage>–<lpage>1406</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2014.04.045" xlink:type="simple">10.1016/j.neuron.2014.04.045</ext-link></comment> <object-id pub-id-type="pmid">24945778</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref032">
<label>32</label>
<mixed-citation publication-type="other" xlink:type="simple">Ludvig EEA, Sutton RRS, Verbeek E, Kehoe EJ (2009) A computational model of hippocampal function in trace conditioning. In: Advances in Neural Information Processing Systems 21, Curran Associates, Inc. pp. 993–1000.</mixed-citation>
</ref>
<ref id="pcbi.1005003.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Xu</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Jiang</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Poo</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Dan</surname> <given-names>Y</given-names></name> (<year>2012</year>) <article-title>Activity recall in a visual cortical ensemble</article-title>. <source>Nature Neuroscience</source> <volume>15</volume>: <fpage>449</fpage>–<lpage>455</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3036" xlink:type="simple">10.1038/nn.3036</ext-link></comment> <object-id pub-id-type="pmid">22267160</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Davidson</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Kloosterman</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>Ma</given-names></name> (<year>2009</year>) <article-title>Hippocampal Replay of Extended Experience</article-title>. <source>Neuron</source> <volume>63</volume>: <fpage>497</fpage>–<lpage>507</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.07.027" xlink:type="simple">10.1016/j.neuron.2009.07.027</ext-link></comment> <object-id pub-id-type="pmid">19709631</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ziegler</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Zenke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Kastner</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name> (<year>2015</year>) <article-title>Synaptic Consolidation: From Synapses to Behavioral Modeling</article-title>. <source>Journal of Neuroscience</source> <volume>35</volume>: <fpage>1319</fpage>–<lpage>1334</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3989-14.2015" xlink:type="simple">10.1523/JNEUROSCI.3989-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25609644</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Clopath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Ziegler</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Vasilaki</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Büsing</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name> (<year>2008</year>) <article-title>Tag-Trigger-Consolidation: A Model of Early and Late Long-Term-Potentiation and Depression</article-title>. <source>PLoS Computational Biology</source> <volume>4</volume>: <fpage>e1000248</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000248" xlink:type="simple">10.1371/journal.pcbi.1000248</ext-link></comment> <object-id pub-id-type="pmid">19112486</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pfister</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name> (<year>2006</year>) <article-title>Triplets of spikes in a model of spike timing-dependent plasticity</article-title>. <source>The Journal of Neuroscience</source> <volume>26</volume>: <fpage>9673</fpage>–<lpage>82</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1425-06.2006" xlink:type="simple">10.1523/JNEUROSCI.1425-06.2006</ext-link></comment> <object-id pub-id-type="pmid">16988038</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhang</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>PM</given-names></name>, <name name-style="western"><surname>Bi</surname> <given-names>GQ</given-names></name> (<year>2009</year>) <article-title>Gain in sensitivity and loss in temporal contrast of STDP by dopaminergic modulation at hippocampal synapses</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>106</volume>: <fpage>13028</fpage>–<lpage>13033</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0900546106" xlink:type="simple">10.1073/pnas.0900546106</ext-link></comment> <object-id pub-id-type="pmid">19620735</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kolodziejski</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Porr</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Wörgötter</surname> <given-names>F</given-names></name> (<year>2009</year>) <article-title>On the asymptotic equivalence between differential Hebbian and temporal difference learning</article-title>. <source>Neural Computation</source> <volume>21</volume>: <fpage>1173</fpage>–<lpage>1202</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2008.04-08-750" xlink:type="simple">10.1162/neco.2008.04-08-750</ext-link></comment> <object-id pub-id-type="pmid">19018698</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Potjans</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Morrison</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Diesmann</surname> <given-names>M</given-names></name> (<year>2009</year>) <article-title>A Spiking Neural Network Model of an Actor-Critic Agent</article-title>. <source>Neural Computation</source> <volume>21</volume>: <fpage>301</fpage>–<lpage>339</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2008.08-07-593" xlink:type="simple">10.1162/neco.2008.08-07-593</ext-link></comment> <object-id pub-id-type="pmid">19196231</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Frémaux</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Sprekeler</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name> (<year>2013</year>) <article-title>Reinforcement learning using a continuous time actor-critic framework with spiking neurons</article-title>. <source>PLoS computational biology</source> <volume>9</volume>: <fpage>e1003024</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003024" xlink:type="simple">10.1371/journal.pcbi.1003024</ext-link></comment> <object-id pub-id-type="pmid">23592970</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Natschläger</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name> (<year>2002</year>) <article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title>. <source>Neural Computation</source> <volume>14</volume>: <fpage>2531</fpage>–<lpage>60</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976602760407955" xlink:type="simple">10.1162/089976602760407955</ext-link></comment> <object-id pub-id-type="pmid">12433288</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jaeger</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Haas</surname> <given-names>H</given-names></name> (<year>2004</year>) <article-title>Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication</article-title>. <source>Science</source> <volume>304</volume>: <fpage>78</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1091277" xlink:type="simple">10.1126/science.1091277</ext-link></comment> <object-id pub-id-type="pmid">15064413</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Drew</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name> (<year>2006</year>) <article-title>Extending the effects of spike-timing-dependent plasticity to behavioral timescales</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>103</volume>: <fpage>8876</fpage>–<lpage>81</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0600676103" xlink:type="simple">10.1073/pnas.0600676103</ext-link></comment> <object-id pub-id-type="pmid">16731625</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huertas</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Shuler</surname> <given-names>XMGH</given-names></name>, <name name-style="western"><surname>Shouval</surname> <given-names>HZ</given-names></name> (<year>2015</year>) <source>A Simple Network Architecture Accounts for Diverse Reward Time Responses in Primary Visual Cortex</source> <volume>35</volume>: <fpage>12659</fpage>–<lpage>12672</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005003.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Latimer</surname> <given-names>KW</given-names></name>, <name name-style="western"><surname>Yates</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>MLR</given-names></name>, <name name-style="western"><surname>Huk</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name> (<year>2015</year>) <article-title>Single-trial spike trains in parietal cortex reveal discrete steps during decision-making</article-title>. <source>Science</source> <volume>349</volume>: <fpage>184</fpage>–<lpage>187</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.aaa4056" xlink:type="simple">10.1126/science.aaa4056</ext-link></comment> <object-id pub-id-type="pmid">26160947</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Palmer</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Marre</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name> (<year>2015</year>) <article-title>Predictive information in a sensory population</article-title>. <source>PNAS</source> <volume>112</volume>:<fpage>6908</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1506855112" xlink:type="simple">10.1073/pnas.1506855112</ext-link></comment> <object-id pub-id-type="pmid">26038544</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005003.ref048">
<label>48</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Kushner</surname> <given-names>HJ</given-names></name>, <name name-style="western"><surname>Yin</surname> <given-names>GG</given-names></name> (<year>2003</year>) <source>Stochastic Approximation and Recursive Algorithms and Applications</source>, <volume>volume 35</volume>. <publisher-name>Springer Science &amp; Business Media</publisher-name>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>