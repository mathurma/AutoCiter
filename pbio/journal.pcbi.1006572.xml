<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00439</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006572</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Geometry</subject><subj-group><subject>Ellipses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject><subj-group><subject>Normal distribution</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Research assessment</subject><subj-group><subject>Research validity</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical noise</subject><subj-group><subject>Gaussian noise</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Comparing Bayesian and non-Bayesian accounts of human confidence reports</article-title>
<alt-title alt-title-type="running-head">Computational models of human confidence</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4720-7861</contrib-id>
<name name-style="western">
<surname>Adler</surname> <given-names>William T.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9835-9083</contrib-id>
<name name-style="western">
<surname>Ma</surname> <given-names>Wei Ji</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Center for Neural Science, New York University, New York, NY, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Psychology, New York University, New York, NY, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gershman</surname> <given-names>Samuel J.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Harvard University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">will@wtadler.com</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>11</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="epub">
<day>13</day>
<month>11</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>11</issue>
<elocation-id>e1006572</elocation-id>
<history>
<date date-type="received">
<day>16</day>
<month>3</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>11</day>
<month>10</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Adler, Ma</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006572"/>
<abstract>
<p>Humans can meaningfully report their confidence in a perceptual or cognitive decision. It is widely believed that these reports reflect the Bayesian probability that the decision is correct, but this hypothesis has not been rigorously tested against non-Bayesian alternatives. We use two perceptual categorization tasks in which Bayesian confidence reporting requires subjects to take sensory uncertainty into account in a specific way. We find that subjects do take sensory uncertainty into account when reporting confidence, suggesting that brain areas involved in reporting confidence can access low-level representations of sensory uncertainty, a prerequisite of Bayesian inference. However, behavior is not fully consistent with the Bayesian hypothesis and is better described by simple heuristic models that use uncertainty in a non-Bayesian way. Both conclusions are robust to changes in the uncertainty manipulation, task, response modality, model comparison metric, and additional flexibility in the Bayesian model. Our results suggest that adhering to a rational account of confidence behavior may require incorporating implementational constraints.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Humans are able to report a sense of confidence in decisions that we make. It is widely hypothesized that confidence reflects the computed probability that a decision is accurate; however, this hypothesis has not been fully explored. We use several human behavioral experiments to test a variety of models that may be considered to be distinct hypotheses about the computational underpinnings of confidence. We find that reported confidence does not appear to reflect the probability that a decision is correct, but instead emerges from a heuristic approximation of this probability.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
<institution>National Science Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>DGE-1342536</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4720-7861</contrib-id>
<name name-style="western">
<surname>Adler</surname> <given-names>William T.</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1342536 (WTA), <ext-link ext-link-type="uri" xlink:href="https://www.nsfgrfp.org/" xlink:type="simple">https://www.nsfgrfp.org/</ext-link>. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="12"/>
<table-count count="0"/>
<page-count count="34"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-11-27</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data and code used for running experiments, model fitting, and plotting is available on a GitHub repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/wtadler/confidence" xlink:type="simple">https://github.com/wtadler/confidence</ext-link>. We have also used Zenodo to assign a DOI to the repository: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1422804" xlink:type="simple">10.5281/zenodo.1422804</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>People often have a sense of a level of confidence about their decisions. Such a “feeling of knowing” [<xref ref-type="bibr" rid="pcbi.1006572.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref002">2</xref>] may serve to improve performance in subsequent decisions [<xref ref-type="bibr" rid="pcbi.1006572.ref003">3</xref>], learning [<xref ref-type="bibr" rid="pcbi.1006572.ref001">1</xref>], and group decision-making [<xref ref-type="bibr" rid="pcbi.1006572.ref004">4</xref>]. Much recent work has focused on identifying brain regions and neural mechanisms responsible for the computation of confidence in humans [<xref ref-type="bibr" rid="pcbi.1006572.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1006572.ref007">7</xref>], nonhuman primates [<xref ref-type="bibr" rid="pcbi.1006572.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1006572.ref010">10</xref>], and rodents [<xref ref-type="bibr" rid="pcbi.1006572.ref011">11</xref>]. In the search for the neural correlates of confidence, the leading premise has been that confidence is Bayesian, i.e., the observer’s estimated probability that a choice is correct [<xref ref-type="bibr" rid="pcbi.1006572.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1006572.ref014">14</xref>]. In human studies, however, naïve subjects can give a meaningful answer when you ask them to rate their confidence about a decision [<xref ref-type="bibr" rid="pcbi.1006572.ref015">15</xref>]; thus, “confidence” intrinsically means something to people, and it is not a foregone conclusion that this intrinsic sense corresponds to the Bayesian definition. Therefore, we regard the above “definition” as a testable hypothesis about the way the brain computes explicit confidence reports; we use Bayesian decision theory to formalize this hypothesis.</p>
<p>Bayesian decision theory provides a general and often quantitatively accurate account of perceptual decisions in a wide variety of tasks [<xref ref-type="bibr" rid="pcbi.1006572.ref016">16</xref>–<xref ref-type="bibr" rid="pcbi.1006572.ref018">18</xref>]. According to this theory, the decision-maker combines knowledge about the statistical structure of the world with the present sensory input to compute a posterior probability distribution over possible states of the world. In principle, a confidence report might be derived from the same posterior distribution; this is the hypothesis described above, which we will call the Bayesian confidence hypothesis (BCH). The main goal of this paper is to test that hypothesis. Recent studies have attempted to test the BCH [<xref ref-type="bibr" rid="pcbi.1006572.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref020">20</xref>] but, because of their experimental designs, are unable to meaningfully distinguish the Bayesian model from any other model of confidence.</p>
<p>Recent work has proposed possible qualitative signatures of Bayesian confidence [<xref ref-type="bibr" rid="pcbi.1006572.ref021">21</xref>]. However, the observation (or lack thereof) of these signatures provides an uncertain amount of evidence in favor of (or against) the Bayesian model, and the signatures are therefore not useful for determining which computations underlie confidence reports [<xref ref-type="bibr" rid="pcbi.1006572.ref022">22</xref>]. To objectively and quantitatively determine whether confidence ratings appear to be Bayesian, we use a formal model comparison approach. We test the predictions of the BCH as we vary the quality of the sensory evidence and the task structure within individuals. We compare Bayesian models against a variety of alternative models, something that is important for the epistemological standing of Bayesian claims [<xref ref-type="bibr" rid="pcbi.1006572.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref024">24</xref>]. We find that the BCH qualitatively describes human behavior but that quantitatively, even the most flexible Bayesian model is outperformed by models that take uncertainty into account in a non-Bayesian way.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Experiment 1</title>
<p>During each session, each subject completed two orientation categorization tasks, Tasks A and B. On each trial, a category <italic>C</italic> was selected randomly (both categories were equally probable), and a stimulus <italic>s</italic> was drawn from the corresponding stimulus distribution and displayed. The subject categorized the stimulus and simultaneously reported their confidence on a 4-point scale, with a single button press (<xref ref-type="fig" rid="pcbi.1006572.g001">Fig 1a</xref>). Using a single button press for choice and confidence may prevent post-choice influences on the confidence judgment ([<xref ref-type="bibr" rid="pcbi.1006572.ref025">25</xref>], but see [<xref ref-type="bibr" rid="pcbi.1006572.ref026">26</xref>]) and emphasized that confidence should reflect the observer’s perception rather than a preceding motor response. The categories were defined by normal distributions on orientation, which differed by task (<xref ref-type="fig" rid="pcbi.1006572.g001">Fig 1b</xref>). In Task A, the distributions had different means (±<italic>μ</italic><sub><italic>C</italic></sub>) and the same standard deviation (<italic>σ</italic><sub><italic>C</italic></sub>); leftward-tilting stimuli were more likely to be from category 1. Variants of Task A are common in decision-making studies [<xref ref-type="bibr" rid="pcbi.1006572.ref027">27</xref>]. In Task B, the distributions had the same mean (0°) and different standard deviations (<italic>σ</italic><sub>1</sub>, <italic>σ</italic><sub>2</sub>); stimuli around the horizontal were more likely to be from category 1. Variants of Task B are less common [<xref ref-type="bibr" rid="pcbi.1006572.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1006572.ref030">30</xref>] but have some properties of perceptual organization tasks; for example, a subject may have to detect when a stimulus belongs to a narrow category (e.g., in which two line segments are collinear) that is embedded in a a broader category (e.g., in which two line segments are unrelated).</p>
<fig id="pcbi.1006572.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006572.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Task design.</title>
<p>(<bold>a</bold>) Schematic of a test block trial. After stimulus offset, subjects reported category and confidence level with a single button press. (<bold>b</bold>) Stimulus distributions for Tasks A and B. (<bold>c</bold>) Examples of low and high reliability stimuli. Six (out of eleven) subjects saw drifting Gabors, and five subjects saw ellipses. (<bold>d</bold>) Generative model. (<bold>e</bold>) Example measurement distributions at different reliability levels. In all models (except Linear Neural), the measurement is assumed to be drawn from a Gaussian distribution centered on the true stimulus, with s.d. dependent on reliability.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.g001" xlink:type="simple"/>
</fig>
<p>Subjects were highly trained on the categories; during training, we only used highest-reliability stimuli, and we provided trial-to-trial category correctness feedback. Subjects were then tested with 6 different reliability levels, which were chosen randomly on each trial. During testing, correctness feedback was withheld to avoid the possibility that confidence simply reflects a learned mapping between stimulus orientation and reliability and the probability of being correct [<xref ref-type="bibr" rid="pcbi.1006572.ref030">30</xref>–<xref ref-type="bibr" rid="pcbi.1006572.ref033">33</xref>].</p>
<p>Because we are interested in subjects’ intrinsic computation of confidence, we did not instruct or incentivize them to assign probability ranges to each button (e.g., by using a scoring rule [<xref ref-type="bibr" rid="pcbi.1006572.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1006572.ref036">36</xref>]). If we had, we would have essentially been training subjects to use a specific model of confidence.</p>
<p>To ensure that our results were independent of stimulus type, we used two kinds of stimuli. Some subjects saw oriented drifting Gabors; for these subjects, stimulus reliability was manipulated through contrast. Other subjects saw oriented ellipses; for these subjects, stimulus reliability was manipulated through ellipse elongation (<xref ref-type="fig" rid="pcbi.1006572.g001">Fig 1c</xref>). We found no major differences in model rankings between Gabor and ellipse subjects, therefore we will make no distinctions between the groups.</p>
<p>For modeling purposes, we assume that the observer’s internal representation of the stimulus is a noisy measurement <italic>x</italic>, drawn from a Gaussian distribution with mean <italic>s</italic> and s.d. <italic>σ</italic> (<xref ref-type="fig" rid="pcbi.1006572.g001">Fig 1d and 1e</xref>). In the model, <italic>σ</italic> (i.e., uncertainty) is a fitted function of stimulus reliability.</p>
</sec>
<sec id="sec004">
<title>Bayesian model</title>
<p>A Bayes-optimal observer uses knowledge of the generative model to make a decision that maximizes the probability of being correct. Here, when the measurement on a given trial is <italic>x</italic>, this strategy amounts to choosing the category <italic>C</italic> for which the posterior probability <italic>p</italic>(<italic>C</italic> ∣ <italic>x</italic>) is highest. This is equivalent to reporting category 1 when the log posterior ratio, <inline-formula id="pcbi.1006572.e001"><alternatives><graphic id="pcbi.1006572.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mi>d</mml:mi> <mml:mo>=</mml:mo> <mml:mtext>log</mml:mtext> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>∣</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn> <mml:mo>∣</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, is positive.</p>
<p>In Task A, <italic>d</italic> is <inline-formula id="pcbi.1006572.e002"><alternatives><graphic id="pcbi.1006572.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:msub><mml:mi>d</mml:mi> <mml:mtext>A</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>x</mml:mi> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>C</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>C</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. Therefore, the ideal observer reports category 1 when <italic>x</italic> is positive; this is the structure of many psychophysical tasks [<xref ref-type="bibr" rid="pcbi.1006572.ref037">37</xref>]. In Task B, however, <italic>d</italic> is <inline-formula id="pcbi.1006572.e003"><alternatives><graphic id="pcbi.1006572.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msub><mml:mi>d</mml:mi> <mml:mtext>B</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mtext>log</mml:mtext> <mml:mfrac><mml:mrow><mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mrow><mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:msup><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>; the observer needs both <italic>x</italic> and <italic>σ</italic> in order to make an optimal decision.</p>
<p>From the point of view of the observer, <italic>σ</italic> is the trial-to-trial level of sensory uncertainty associated with the measurement [<xref ref-type="bibr" rid="pcbi.1006572.ref038">38</xref>]. In a minor variation of the optimal observer, we allow for the possibility that the observer’s prior belief over category, <italic>p</italic>(<italic>C</italic>), is different from the true value of (0.5, 0.5); this adds a constant to <italic>d</italic><sub>A</sub> and <italic>d</italic><sub>B</sub>.</p>
<p>We introduce the Bayesian confidence hypothesis (BCH), stating that confidence reports depend on the internal representation of the stimulus (here <italic>x</italic>) only via <italic>d</italic>. In the BCH, the observer chooses a response by comparing <italic>d</italic> to a set of category and confidence boundaries. For example, whenever <italic>d</italic> falls within a certain range, the observer presses the “medium-low confidence, category 2” button. The BCH is thus an extension of the choice model described above, wherein the value of <italic>d</italic> is used to compute confidence as well as chosen category. There is another way of thinking about this. Bayesian models assume that subjects compute <italic>d</italic> in order to make an optimal choice. Assuming people compute <italic>d</italic> at all, are they able to use it to report confidence as well? We refer to the Bayesian model here as simply “Bayes.” We also tested several more constrained versions of this model.</p>
<p>The observer’s decision can be summarized as a mapping from a combination of a measurement and an uncertainty level (<italic>x</italic>, <italic>σ</italic>) to a response that indicates both category and confidence. We can visualize this mapping as in <xref ref-type="fig" rid="pcbi.1006572.g002">Fig 2</xref>, first column. It is clear that the pattern of decision boundaries in the BCH is qualitatively very different between Task A and Task B. In Task A, the decision boundaries are quadratic functions of uncertainty; confidence decreases monotonically with uncertainty and increases with the distance of the measurement from 0. In Task B, the decision boundaries are neither linear nor quadratic.</p>
<fig id="pcbi.1006572.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006572.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Decision rules/mappings in four models.</title>
<p>Each model corresponds to a different mapping from a measurement and uncertainty level to a category and confidence response. Colors correspond to category and confidence response, as in <xref ref-type="fig" rid="pcbi.1006572.g001">Fig 1a</xref>. Plots were generated using parameter values that were roughly similar to those found after fitting subject data but were chosen primarily to illustrate the different features of the models.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Alternative models</title>
<p>At first glance, it seems obvious that sensory uncertainty is relevant to the computation of confidence. However, this is by no means a given; in fact, a prominent proposal is that confidence is based on the distance between the measurement and the decision boundary, without any role for sensory uncertainty [<xref ref-type="bibr" rid="pcbi.1006572.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref039">39</xref>]. Therefore, we tested a model (Fixed) in which the response is a function of the measurement alone (equivalent to a maximum likelihood estimate of the stimulus orientation), and not of the uncertainty of that measurement (<xref ref-type="fig" rid="pcbi.1006572.g002">Fig 2</xref>, second column).</p>
<p>We also tested heuristic models in which the subject uses their knowledge of their sensory uncertainty but does not compute a posterior distribution over category. We have previously classified such models as <italic>probabilistic non-Bayesian</italic> [<xref ref-type="bibr" rid="pcbi.1006572.ref040">40</xref>]. In the Orientation Estimation model, subjects base their response on a maximum a posteriori estimate of orientation (rather than category), using the mixture of the two stimulus distributions as a prior distribution. In the Linear Neural model, subjects base their response on a linear function of the output of a hypothetical population of neurons.</p>
<p>We derived two additional probabilistic non-Bayesian models, Lin and Quad, from the observation that the Bayesian decision criteria are an approximately linear function of uncertainty in some measurement regimes and approximately quadratic in others. These models are able to produce approximately Bayesian behavior without actually performing any computation of the posterior. In Lin and Quad, subjects base their response on a linear or a quadratic function of <italic>x</italic> and <italic>σ</italic>, respectively. A comparison of the Lin and Quad columns to the Bayes column in <xref ref-type="fig" rid="pcbi.1006572.g002">Fig 2</xref> demonstrates that Lin and Quad can approximate the Bayesian mapping from (<italic>x</italic>, <italic>σ</italic>) to response despite not being based on the Bayesian decision variable. All of the models we tested were variants of the six models described so far (Bayes, Fixed, Orientation Estimation, Linear Neural, Lin, Quad).</p>
<p>Each trial consists of the experimentally determined orientation and reliability level and the subject’s category and confidence response (an integer between 1 and 8). This is a very rich data set, which we summarize in <xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3</xref>. We find the following effects: performance and confidence increase as a function of reliability (<xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3a, 3b, 3h and 3i</xref>), and high-confidence reports are less frequent than low-confidence reports (<xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3e and 3f</xref>). Note <xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3c and 3d</xref> especially; this is the projection of the data that we will use to demonstrate model fits for the rest of this paper. We use this projection because the vertical axis (mean button press) most closely approximates the form of the raw data. Additionally, because our models are differentiated by how they use uncertainty, it is informative to plot how response changes as a function of reliability, in addition to category and task.</p>
<fig id="pcbi.1006572.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006572.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Behavioral data and fits from best model (Quad), experiment 1.</title>
<p>Error bars represent ±1 s.e.m. across 11 subjects. Shaded regions represent ±1 s.e.m. on model fits. (<bold>a,b</bold>) Proportion “category 1” reports as a function of stimulus reliability and true category. (<bold>c,d</bold>) Mean button press as a function of stimulus reliability and true category. (<bold>e,f</bold>) Normalized histogram of confidence reports for both true categories. (<bold>g</bold>) Proportion correct category reports as a function of confidence report and task. (<bold>h,i</bold>) Mean confidence as a function of stimulus reliability and correctness. (<bold>j,k</bold>) Mean confidence as a function of stimulus orientation and reliability. (<bold>l,m</bold>) Proportion “category 1” reports as a function of stimulus orientation and reliability. (<bold>n,o</bold>) Mean button press as a function of stimulus orientation and reliability. (<bold>c,d,n,o</bold>) Vertical axis label colors correspond to button presses, as in <xref ref-type="fig" rid="pcbi.1006572.g001">Fig 1a</xref>. (<bold>l–o</bold>) For clarity, only 3 of 6 reliability levels are shown, although models were fit to all reliability levels.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.g003" xlink:type="simple"/>
</fig>
<p>Recently, a measure of the degree of association between accuracy and confidence, meta-<italic>d</italic>′, has been developed [<xref ref-type="bibr" rid="pcbi.1006572.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref042">42</xref>]. While it can be useful for characterizing individual differences, we do not include it in our analyses or display it in <xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3</xref>. That is because one strength of our experimental design is that we parametrically vary stimulus strength and stimulus reliability; this differs from papers in which meta-<italic>d</italic>′ plays a central role because, in those papers, the stimulus is often only a binary category.</p>
</sec>
<sec id="sec006">
<title>Model comparison</title>
<p>We used Markov Chain Monte Carlo (MCMC) sampling to fit models to raw individual-subject data. To account for overfitting, we compared models using leave-one-out cross-validated log likelihood scores (LOO) computed with the full posteriors obtained through MCMC [<xref ref-type="bibr" rid="pcbi.1006572.ref043">43</xref>]. A model recovery analysis ensured that our models are meaningfully distinguishable (<xref ref-type="sec" rid="sec023">Methods</xref>). Unless otherwise noted, models were fit jointly to Task A and B category and confidence responses.</p>
<sec id="sec007">
<title>Use of sensory uncertainty</title>
<p>We first compared Bayes to the Fixed model, in which the observer does not take trial-to-trial sensory uncertainty into account (<xref ref-type="fig" rid="pcbi.1006572.g004">Fig 4</xref>). Fixed provides a poor fit to the data, indicating that observers use not only a point estimate of their measurement, but also their uncertainty about that measurement. Bayes outperforms Fixed by a summed LOO difference (median and 95% CI of bootstrapped sums across subjects) of 2265 [498, 4253]. For the rest of this paper, we will report model comparison results using this format.</p>
<fig id="pcbi.1006572.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006572.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Model fits and model comparison for models Fixed and Bayes.</title>
<p>Bayes provides a better fit, but both models have large deviations from the data. Left and middle columns: model fits to mean button press as a function of reliability, true category, and task. Error bars represent ±1 s.e.m. across 11 subjects. Shaded regions represent ±1 s.e.m. on model fits, with each model on a separate row. Right column: LOO model comparison. Bars represent individual subject LOO scores for Bayes, relative to Fixed. Negative (leftward) values indicate that, for that subject, Bayes had a higher (better) LOO score than Fixed. Blue lines and shaded regions represent, respectively, medians and 95% CI of bootstrapped mean LOO differences across subjects. These values are equal to the summed LOO differences reported in the text divided by the number of subjects. Although we plot data as a function of the true category here, the model only takes in measurement and reliability as an input; it is not free to treat individual trials from each true category differently.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.g004" xlink:type="simple"/>
</fig>
<p>Although Bayes fits better than Fixed, it still shows systematic deviations from the data, especially at high reliabilities. (Because we fit our models to all of the raw data and because boundary parameters are shared across all reliability levels, the fit to high-reliability trials is constrained by the fit to low-reliability trials).</p>
</sec>
<sec id="sec008">
<title>Noisy log posterior ratio</title>
<p>To see if we could improve Bayes’s fit, we tried a version that included decision noise, i.e. noise on the log posterior ratio <italic>d</italic>. We assumed that this noise takes the form of additive zero-mean Gaussian noise with s.d. <italic>σ</italic><sub><italic>d</italic></sub>. This is almost equivalent to the probability of a response being a logistic (softmax) function of <italic>d</italic> [<xref ref-type="bibr" rid="pcbi.1006572.ref044">44</xref>]. Adding <italic>d</italic> noise improves the Bayesian model fit by 804 [510, 1134] (<xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1 Table</xref>).</p>
<p>For the rest of the reported fits to behavior, we will only consider this version of Bayes with <italic>d</italic> noise, and will refer to this model as Bayes-<italic>d</italic>N. We will refer to Bayes-<italic>d</italic>N, Fixed, Orientation Estimation, Linear Neural, Lin, and Quad, when fitted jointly to category and confidence data from Tasks A and B, as our core models.</p>
</sec>
<sec id="sec009">
<title>Heuristic models</title>
<p>Orientation Estimation performs worse than Bayes-<italic>d</italic>N by 2041 [385, 3623] (<xref ref-type="fig" rid="pcbi.1006572.g005">Fig 5</xref>, second row). The intuition for one way that this model fails is as follows: at low levels of reliability, the MAP estimate is heavily influenced by the prior and tends to be very close to the prior mean (0°). This explains why, in Task B, there is a bias towards reporting “high confidence, category 1” at low reliability. Linear Neural performs about as well as Bayes-<italic>d</italic>N, with summed LOO differences of 1188 [-588, 2704], and the fits to the summary statistics are qualitatively poor (<xref ref-type="fig" rid="pcbi.1006572.g005">Fig 5</xref>, third row).</p>
<fig id="pcbi.1006572.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006572.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Model fits and model comparison for Bayes-<italic>d</italic>N and heuristic models.</title>
<p>In both tasks, Bayes-<italic>d</italic>N fails to describe the data at high reliabilities; Lin and Quad provides a good fit at most reliabilities. Left and middle columns: as in <xref ref-type="fig" rid="pcbi.1006572.g004">Fig 4</xref>. Right column: bars represent individual subject LOO scores for each model, relative to Bayes-<italic>d</italic>N. Negative (leftward) values indicate that, for that subject, the model in the corresponding row had a higher (better) LOO score than Bayes-<italic>d</italic>N. Blue lines and shaded regions: as in <xref ref-type="fig" rid="pcbi.1006572.g004">Fig 4</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.g005" xlink:type="simple"/>
</fig>
<p>Finally, Lin and Quad outperform Bayes-<italic>d</italic>N by 1398 [571, 2644] and 1167 [858, 2698], respectively. Both models provide qualitatively better fits, especially at high reliabilities (compare <xref ref-type="fig" rid="pcbi.1006572.g005">Fig 5</xref>, first row, to <xref ref-type="fig" rid="pcbi.1006572.g005">Fig 5</xref>, fourth and fifth rows), and strongly tilted orientations (compare <xref ref-type="supplementary-material" rid="pcbi.1006572.s010">S10n and S10o Fig</xref> to <xref ref-type="supplementary-material" rid="pcbi.1006572.s014">S14n and S14o Fig</xref> and <xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3n and 3o</xref>).</p>
<p>We summarize the performance of our core models in <xref ref-type="fig" rid="pcbi.1006572.g006">Fig 6</xref>. Noting that a LOO difference of more than 5 is considered to be very strong evidence [<xref ref-type="bibr" rid="pcbi.1006572.ref045">45</xref>], the heuristic models Lin and Quad perform much better than Bayes-<italic>d</italic>N. Furthermore, we can decisively rule out Fixed. We will now describe variants of our core models.</p>
<fig id="pcbi.1006572.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006572.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Comparison of core models, experiment 1.</title>
<p>Models were fit jointly to Task A and B category and confidence responses. Blue lines and shaded regions represent, respectively, medians and 95% CI of bootstrapped summed LOO differences across subjects. LOO differences for these and other models are shown in <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1a Fig</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.g006" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec010">
<title>Non-parametric relationship between reliability and <italic>σ</italic></title>
<p>One potential criticism of our fitting procedure is that we assumed a parameterized relationship between reliability and <italic>σ</italic>. To see if our results were dependent on that assumption, we modified the models such that <italic>σ</italic> was non-parametric (i.e., there was a free parameter for <italic>σ</italic> at each level of reliability). With this feature added to our core models, Quad still fits better than Bayes-<italic>d</italic>N by 1676 [839, 2730] and it fits better than Fixed by 6097 [4323, 7901] (<xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1 Table</xref>). This feature improved Quad’s performance by 325 [141, 535]. For the rest of this paper, we will only report the fits of Bayes-<italic>d</italic>N, the best-fitting non-Bayesian model, and Fixed. See supplementary figures and tables for all other model fits.</p>
</sec>
<sec id="sec011">
<title>Incorrect assumptions about the generative model</title>
<p>Suboptimal behavior can be produced by optimal inference using incorrect generative models, a phenomenon known as “model mismatch” [<xref ref-type="bibr" rid="pcbi.1006572.ref046">46</xref>–<xref ref-type="bibr" rid="pcbi.1006572.ref048">48</xref>]. Up to now, Bayes-<italic>d</italic>N has assumed that observers have accurate knowledge of the parameters of the generative model. To test whether this assumption prevents Bayes-<italic>d</italic>N from fitting the data well, we tested a series of Bayesian models in which the observer has inaccurate knowledge of the generative model.</p>
<p>Bayes-<italic>d</italic>N assumed that, because subjects were well trained, they knew the true values of <italic>σ</italic><sub><italic>C</italic></sub>, <italic>σ</italic><sub>1</sub>, and <italic>σ</italic><sub>2</sub>, the standard deviations of the stimulus distributions. We tested a model in which these values were free parameters, rather than fixed to the true value. We would expect these free parameters to improve the fit of Bayes-<italic>d</italic>N in the case where subjects were not trained enough to sufficiently learn the stimulus distributions. This feature improves Bayes-<italic>d</italic>N’s fit by 908 [318, 1661], but it still underperforms Quad by 768 [399, 1144] (<xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1 Table</xref>).</p>
<p>Previous models also assumed that subjects had accurate knowledge of their own measurement noise; their perceptual uncertainty, used in the computation of <italic>d</italic>, was identical to their measurement noise, used to generate measurements <italic>x</italic>, with both uncertainty and measurement noise equal to <italic>σ</italic>. We tested models in which we fit <italic>σ</italic><sub>measurement</sub> and <italic>σ</italic><sub>inference</sub> as two independent functions of reliability [<xref ref-type="bibr" rid="pcbi.1006572.ref046">46</xref>]. This feature improves Bayes-<italic>d</italic>N’s fit by 1310 [580, 2175], but it still underperforms Quad by 362 [162, 602] (<xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1 Table</xref>).</p>
</sec>
<sec id="sec012">
<title>Weighted average of precision and perceived probability of being correct</title>
<p>A recent paper ([<xref ref-type="bibr" rid="pcbi.1006572.ref049">49</xref>]; although see [<xref ref-type="bibr" rid="pcbi.1006572.ref022">22</xref>]) proposed that confidence is a weighted average of a function of variance, such as <inline-formula id="pcbi.1006572.e004"><alternatives><graphic id="pcbi.1006572.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:math></alternatives></inline-formula>, and the perceived probability of being correct (incidentally, under a non-Bayesian decision rule). We tested such a model (using a Bayesian decision rule), which fits better than Fixed by 3059 [758, 5528] but still underperforms Lin by 3478 [2211, 5020] (<xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1 Table</xref>).</p>
</sec>
<sec id="sec013">
<title>Separate fits to Tasks A and B</title>
<p>In order to determine whether model rankings were primarily due to differences in one of the two tasks, we fit our models to each task individually. In Task A, Quad fits better than Bayes-<italic>d</italic>N by 581 [278, 938], and better than Fixed by 3534 [2529, 4552] (<xref ref-type="supplementary-material" rid="pcbi.1006572.s002">S2 Fig</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006572.s017">S2 Table</xref>). In Task B, Quad fits better than Bayes-<italic>d</italic>N by 978 [406, 1756] and fits better than Fixed by 3234 [2099, 4390] (<xref ref-type="supplementary-material" rid="pcbi.1006572.s003">S3 Fig</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006572.s018">S3 Table</xref>). [<xref ref-type="bibr" rid="pcbi.1006572.ref041">41</xref>]</p>
</sec>
<sec id="sec014">
<title>Fits to category choice data only</title>
<p>In order to see whether our results were peculiar to combined category and confidence responses, we fit our models to the category choices only. Lin fits better than Bayes-<italic>d</italic>N by 595 [311, 927] and fits better than Fixed by 1690 [976, 2534] (<xref ref-type="supplementary-material" rid="pcbi.1006572.s004">S4 Fig</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006572.s019">S4 Table</xref>).</p>
</sec>
<sec id="sec015">
<title>Fits to Task B only, with noise parameters fitted from Task A</title>
<p>To confirm that the fitted values of sensory uncertainty in the probabilistic models are meaningful, we treated Task A as an independent experiment to measure subjects’ sensory noise. The category choice data from Task A can be used to determine the four uncertainty parameters. We fit Fixed with a decision boundary of 0° (equivalent to a Bayesian choice model with no prior), using maximum likelihood estimation. We fixed these parameters and used them to fit our models to Task B category and confidence responses. Lin fits better than Bayes-<italic>d</italic>N by 1773 [451, 2845] and fits better than Fixed by 5016 [3090, 6727] (<xref ref-type="supplementary-material" rid="pcbi.1006572.s005">S5 Fig</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006572.s020">S5 Table</xref>).</p>
</sec>
<sec id="sec016">
<title>Separate category and confidence responses (experiment 2)</title>
<p>There has been some recent debate as to whether it is more appropriate to collect choice and confidence with a single motor response (as described above) or with separate responses [<xref ref-type="bibr" rid="pcbi.1006572.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref050">50</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref051">51</xref>]. Aitchison et al. [<xref ref-type="bibr" rid="pcbi.1006572.ref019">19</xref>] found that confidence appears more Bayesian when subjects use separate responses. To confirm this, we ran a second experiment in which subjects chose a category by pressing one of two buttons, then reported confidence by pressing one of four buttons. Aitchison et al. [<xref ref-type="bibr" rid="pcbi.1006572.ref019">19</xref>] also provided correctness feedback on every trial; in order to ensure that we could compare our results to theirs, we also provided correctness feedback in this experiment, even though this manipulation was not of primary interest. After fitting our core models, our results did not differ substantially from experiment 1: Lin fits better than Bayes-<italic>d</italic>N by 396 [186, 622] and fits better than Fixed by 2095 [1344, 2889] (<xref ref-type="supplementary-material" rid="pcbi.1006572.s006">S6 Fig</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006572.s021">S6 Table</xref>).</p>
</sec>
<sec id="sec017">
<title>Task B only (experiment 3)</title>
<p>It is possible that subjects behave suboptimally when they have to do multiple tasks in a session; in other words, perhaps one task “corrupts” the other. To explore this possibility, we ran an an experiment in which subjects completed Task B only. We chose Task B over Task A for this experiment because Task B has the desirable characteristic that uncertainty is required for optimal categorization. Quad fits better than Bayes-<italic>d</italic>N by 1361 [777, 2022] and fits better than Fixed by 7326 [4905, 9955] (<xref ref-type="supplementary-material" rid="pcbi.1006572.s007">S7 Fig</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006572.s022">S7 Table</xref>). In experiments 2 and 3, subjects only saw drifting Gabors; we did not use ellipses.</p>
<p>We also fit only the choice data, and found that Lin fits about as well as Bayes-<italic>d</italic>N, with summed LOO differences of 117 [-76, 436] and fits better than Fixed by 1084 [619, 1675] (<xref ref-type="supplementary-material" rid="pcbi.1006572.s008">S8 Fig</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006572.s023">S8 Table</xref>). This approximately replicates our previously published results [<xref ref-type="bibr" rid="pcbi.1006572.ref030">30</xref>].</p>
</sec>
<sec id="sec018">
<title>Model comparison metric</title>
<p>None of our model comparison results depend on our choice of metric: in all three experiments, model rankings changed negligibly if we used AIC, BIC, AICc, or WAIC instead of LOO.</p>
</sec>
</sec>
</sec>
<sec id="sec019" sec-type="conclusions">
<title>Discussion</title>
<p>Although people can report subjective feelings of confidence, the computations that produce this feeling are not well understood. It has been proposed that confidence is the observer’s computed posterior probability that a decision is correct [<xref ref-type="bibr" rid="pcbi.1006572.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1006572.ref014">14</xref>]. However, this hypothesis has not been fully tested. We carried out a strong test of human confidence reports, using overlapping categories [<xref ref-type="bibr" rid="pcbi.1006572.ref052">52</xref>], withholding feedback on testing trials, and varying experimental components such as task, stimulus type, and stimulus reliability [<xref ref-type="bibr" rid="pcbi.1006572.ref032">32</xref>]. We used model comparison to investigate the computational underpinnings of confidence, fitting a total of 75 models from 6 distinct model families.</p>
<p>Our first finding is that, like the optimal observer, subjects use knowledge of their sensory uncertainty when reporting confidence in a categorical decision; models in which the observer ignores their sensory uncertainty provide a poor fit to the data (<xref ref-type="fig" rid="pcbi.1006572.g004">Fig 4</xref>). Our second finding is that subjects do not appear to use knowledge of their sensory uncertainty in a way that is fully consistent with the Bayesian confidence hypothesis. Instead, heuristic models that approximate Bayesian computation—but do not compute a posterior probability over category—outperform the Bayesian models in two tasks (<xref ref-type="fig" rid="pcbi.1006572.g005">Fig 5</xref>, compare top row to bottom two rows). This result continued to hold after we relaxed assumptions about the relationship between reliability and noise, and about the subject’s knowledge of the generating model. We accounted for the fact that our models had different amounts of flexibility by using a wide array of model comparison metrics and by showing that our models are meaningfully distinguishable.</p>
<sec id="sec020">
<title>Limitations</title>
<p>Our study has several limitations. For instance, because of our short presentation time, we cannot say much about how our results generalize to tasks that require integration of evidence over time [<xref ref-type="bibr" rid="pcbi.1006572.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref053">53</xref>–<xref ref-type="bibr" rid="pcbi.1006572.ref055">55</xref>]. Additionally, because our stimuli are very low-level, we cannot say much about high-level stimuli like faces [<xref ref-type="bibr" rid="pcbi.1006572.ref056">56</xref>]. Also, we only considered explicit confidence ratings, which differ from the implicit confidence that can be gathered from humans (e.g., by presenting two tasks and asking the subject to choose which one they feel more confident about completing correctly [<xref ref-type="bibr" rid="pcbi.1006572.ref057">57</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref058">58</xref>]) or from nonhuman animals [<xref ref-type="bibr" rid="pcbi.1006572.ref013">13</xref>] (e.g., by measuring how frequently they decline to make a difficult choice [<xref ref-type="bibr" rid="pcbi.1006572.ref008">8</xref>], or how long they will wait for a reward [<xref ref-type="bibr" rid="pcbi.1006572.ref011">11</xref>]). It is possible that implicit confidence might be more Bayesian than explicit confidence; Barthelmé and Mamassian [<xref ref-type="bibr" rid="pcbi.1006572.ref058">58</xref>] conduct an implicit confidence experiment and rule out some heuristic models. However, their experimental task is substantially different from the one presented here. In their experiment, the stimulus feature of interest (orientation) only takes on two values rather than varying parametrically, so it requires a different class of heuristic models. Future studies of the difference between implicit and explicit confidence should use experiments that are able to distinguish the models presented here, which has not been done.</p>
</sec>
<sec id="sec021">
<title>Other investigations of deviations from Bayesian confidence</title>
<p>Like the present study, Aitchison et al. [<xref ref-type="bibr" rid="pcbi.1006572.ref019">19</xref>] found evidence that confidence reports may emerge from heuristic computations. However, they sampled stimuli from only a small region of their two-dimensional space, where model predictions may not vary greatly. Therefore, their stimulus set did not allow for the models to be strongly distinguished. Furthermore, although they tested for <italic>Bayesian</italic> computation, they did not test for <italic>probabilistic</italic> computation (whether observers take sensory uncertainty into account on a trial-to-trial basis [<xref ref-type="bibr" rid="pcbi.1006572.ref040">40</xref>]) as we do here. Such a test requires that the experimenter vary the reliability, not only the value, of the stimulus feature of interest.</p>
<p>Navajas et al. [<xref ref-type="bibr" rid="pcbi.1006572.ref049">49</xref>] suggested that confidence reports are best described as a weighted average of precision and the probability of being correct. However, their model uses the estimated probability of being correct under a non-Bayesian decision rule [<xref ref-type="bibr" rid="pcbi.1006572.ref022">22</xref>]. They did not show the fit of a Bayesian model, and therefore their study does not constitute a true test of whether confidence is Bayesian. Here, we tested and rejected the hypothesis that confidence is a weighted average of precision and the posterior probability of being correct under a Bayesian decision rule.</p>
<p>Sanders et al. [<xref ref-type="bibr" rid="pcbi.1006572.ref020">20</xref>] reported that confidence has a “statistical” nature. However, their experiment was unable to determine whether confidence is Bayesian or not [<xref ref-type="bibr" rid="pcbi.1006572.ref017">17</xref>], because the stimuli varied along only one dimension. Aitchison et al. [<xref ref-type="bibr" rid="pcbi.1006572.ref019">19</xref>] note that, to distinguish models of confidence, the experimenter must use stimuli that are characterized by two dimensions (e.g., contrast and orientation as in this experiment, or contrast and crowding as in Barthelmé and Mamassian [<xref ref-type="bibr" rid="pcbi.1006572.ref058">58</xref>]). This is because, when fitting models that map from an internal variable to an integer confidence rating, it is impossible to distinguish between two internal variables that are monotonically related (in the case of Sanders et al. [<xref ref-type="bibr" rid="pcbi.1006572.ref020">20</xref>], the measurement and the posterior probability of being correct). Therefore, the only alternative model proposed by Sanders et al. [<xref ref-type="bibr" rid="pcbi.1006572.ref020">20</xref>] is based on reaction time, rather than on the presented stimuli.</p>
<p>In detection and coarse discrimination tasks, Lau, Rahnev, and colleagues report that subjects overestimate their confidence in the periphery and for unattended stimuli. The authors have proposed a signal detection theory model in which high eccentricity or lower attention induces higher noise, and the confidence criterion may not change at all [<xref ref-type="bibr" rid="pcbi.1006572.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref059">59</xref>–<xref ref-type="bibr" rid="pcbi.1006572.ref063">63</xref>]. As a result, more probability mass will “spill over” the criterion to the high-confidence regime. How do these findings relate to ours? At a qualitative level, they are consistent in that confidence does not seem Bayesian. However, in detection and coarse discrimination tasks, it is not possible to distinguish between fixed-criterion and probabilistic models [<xref ref-type="bibr" rid="pcbi.1006572.ref064">64</xref>], and their data cannot be used to infer that the criterion is fixed. The paradigms in the present paper are able to distinguish such models because of the parametric manipulation of orientation, the stimulus feature of interest; indeed, we find strong evidence against the Fixed criterion model. It remains to be seen whether claims that confidence can be systematically dissociated from perceptual performance [<xref ref-type="bibr" rid="pcbi.1006572.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref065">65</xref>–<xref ref-type="bibr" rid="pcbi.1006572.ref069">69</xref>] are consistent with the account presented here, in which the brain adjusts confidence criteria based on uncertainty but in a non-Bayesian manner.</p>
<p>Another form of non-Bayesian confidence ratings is the recent proposal that, in confidence judgments, only the “positive evidence” in favor of the chosen option matters, instead of the “balance of evidence” between two options [<xref ref-type="bibr" rid="pcbi.1006572.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref070">70</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref071">71</xref>]. In our tasks, this form of suboptimality would entail that confidence is derived from the (log) likelihood of the chosen category, instead of from the (log) likelihood ratio. This does not seem consistent with our data; for example, the likelihood of category 2 decreases as the absolute value of the stimulus and, correspondingly, the measurement, increases (<xref ref-type="fig" rid="pcbi.1006572.g007">Fig 7b</xref>). However, confidence for a category 2 decision steadily increases with the absolute value of the stimulus (<xref ref-type="fig" rid="pcbi.1006572.g007">Fig 7a</xref>). More work is needed to understand whether alternative models could explain the “positive evidence” data, and if not, what causes the difference with our results.</p>
<fig id="pcbi.1006572.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006572.g007</object-id>
<label>Fig 7</label>
<caption>
<title/>
<p>(<bold>a</bold>) In experiment 1, Task B, on trials in which the subject chose category 2, mean confidence increases with the absolute value of stimulus orientation. (<bold>b</bold>) The “positive evidence” in favor of category 2, however, decreases with the absolute value of stimulus orientation. This plot depicts the category-conditioned stimulus distribution <italic>p</italic>(<italic>s</italic> ∣ <italic>C</italic> = 2); positive evidence in this experiment is equivalent to the likelihood <italic>p</italic>(<italic>x</italic> ∣ <italic>C</italic> = 2), which is just <italic>p</italic>(<italic>s</italic> ∣ <italic>C</italic> = 2) convolved with the subject’s measurement noise.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec022">
<title>Status of Bayesian models</title>
<p>What do our findings tell us about the neural basis of confidence? Previous studies have found that neural activity in some brain areas (e.g., human medial temporal lobe [<xref ref-type="bibr" rid="pcbi.1006572.ref007">7</xref>] and prefrontal cortex [<xref ref-type="bibr" rid="pcbi.1006572.ref072">72</xref>], monkey lateral intraparietal cortex [<xref ref-type="bibr" rid="pcbi.1006572.ref008">8</xref>] and pulvinar [<xref ref-type="bibr" rid="pcbi.1006572.ref010">10</xref>], rodent orbitofrontal cortex [<xref ref-type="bibr" rid="pcbi.1006572.ref011">11</xref>]) is associated with behavioral indicators of confidence, and/or with the distance of a stimulus to a decision boundary. However, such studies mostly used stimuli that vary along a single dimension (e.g., net retinal dot motion energy, mixture of two odors). Because measurement is indistinguishable from the probability of being correct in these classes of tasks, neural activity associated with confidence may represent either the measurement or the probability of being correct [<xref ref-type="bibr" rid="pcbi.1006572.ref019">19</xref>]. In addition to the recommendation of Aitchison et al. [<xref ref-type="bibr" rid="pcbi.1006572.ref019">19</xref>] to distinguish between these possibilities by varying stimuli along two dimensions, we recommend fitting both Bayesian and non-Bayesian probabilistic models to behavior. In view of the relatively poor performance of the Bayesian models in the present study, the proposal [<xref ref-type="bibr" rid="pcbi.1006572.ref012">12</xref>] to correlate behavior and neural activity with predictions of the Bayesian confidence model should be viewed with skepticism.</p>
<p>Our results raise general issues about the status of Bayesian models as descriptions of behavior. First, because it is impossible to exhaustively test all models that might be considered “Bayesian,” we cannot rule out the entire class of models. However, we have tried to alleviate this issue as much as possible by testing a large number of Bayesian models—far more than the number of Bayesian and non-Bayesian models tested in other studies of confidence. Second, Bayesian models are often held in favor for their generalizability; one can determine the performance-maximizing strategy for any task. Although generalizability indeed makes Bayesian models attractive and powerful, we do not believe that this property should override a bad fit.</p>
<p>One could take two different views of our heuristic model results. The first view is that the heuristics should be taken seriously as principled models [<xref ref-type="bibr" rid="pcbi.1006572.ref073">73</xref>]; here, the challenge is to demonstrate that they describe behavior in a variety of tasks and can be motivated based on underlying principles. The second view is that these are descriptive models simply meant to demonstrate that a simple model can provide a good fit to the data; here, the heuristics are benchmarks for more principled models, and the challenge is to find a principled model that fits the data as well as the heuristics. We lean towards the second view and interpret our results as demonstrating that the purest form of the Bayesian confidence hypothesis does not describe human confidence reports particularly well.</p>
<p>However, one might still conclude, after examining the fits of the Bayesian model, that the behavior is “approximately Bayesian” rather than “non-Bayesian.” As written, this is a semantic distinction because it relies on one’s definition of “approximate.” However, it can be turned into a more meaningful question: Are the differences between human behavior and Bayesian models accounted for by an unknown principle, such as an ecologically relevant objective function that includes both task performance and biological constraints?</p>
<p>Although there are benefits associated with veridical explicit representations of confidence [<xref ref-type="bibr" rid="pcbi.1006572.ref074">74</xref>–<xref ref-type="bibr" rid="pcbi.1006572.ref076">76</xref>], there are also neural constraints that may give rise to non-Bayesian behavior [<xref ref-type="bibr" rid="pcbi.1006572.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref024">24</xref>]. Such constraints include the kinds of operations that neurons can perform, the high energy cost of spiking [<xref ref-type="bibr" rid="pcbi.1006572.ref077">77</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref078">78</xref>], and the cost of neural wiring length [<xref ref-type="bibr" rid="pcbi.1006572.ref079">79</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref080">80</xref>]. A search for ecologically rational constraints on Bayesian computation benefits from a positive characterization of the deviations from Bayesian computation, in the form of heuristic models such as Lin and Quad. Specifically, one could define neural networks with various combinations of constraints, and train them as if they were psychophysical subjects in our tasks. After training, one could fit behavioral models to them; this approach has already shown that the output from such neural networks is sometimes best described by heuristic models [<xref ref-type="bibr" rid="pcbi.1006572.ref081">81</xref>]. Using model ranking as a measure of similarity, one could determine which network architecture and training procedure produces confidence behavior that is most similar to that of humans. This could reveal which constraints are responsible for the specific deviations from Bayesian computation that we have observed.</p>
</sec>
</sec>
<sec id="sec023" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec024">
<title>Ethics statement</title>
<p>The experiments were approved by the University Committee on Activities Involving Human Subjects of New York University. Informed consent was given by each subject before the experiment.</p>
</sec>
<sec id="sec025">
<title>Experiment 1</title>
<sec id="sec026">
<title>Subjects</title>
<p>11 subjects (2 male), aged 20–42, participated in the experiment. Subjects received $10 per 40-60 minute session, plus a completion bonus of $15. All subjects were naïve to the purpose of the experiment. No subjects were fellow scientists.</p>
</sec>
<sec id="sec027">
<title>Apparatus and stimuli</title>
<p><italic>Apparatus</italic>. Subjects were seated in a dark room, at a viewing distance of 32 cm from the screen, with their chin in a chinrest. Stimuli were presented on a gamma-corrected 60 Hz 9.7-inch 2048-by-1536 display. The display (LG LP097QX1-SPA2) was the same as that used in the 2013 iPad Air (Apple); we chose it for its high pixel density (264 pixels/inch). The display was connected to a Windows desktop PC using the Psychophysics Toolbox extensions [<xref ref-type="bibr" rid="pcbi.1006572.ref082">82</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref083">83</xref>] for MATLAB (Mathworks).</p>
<p><italic>Stimuli</italic>. The background was mid-level gray (199 cd/m<sup>2</sup>). The stimulus was either a drifting Gabor (Subjects 3, 6, 8, 9, 10, and 11) or an ellipse (Subjects 1, 2, 4, 5, and 7). The Gabor had a peak luminance of 398 cd/m<sup>2</sup> at 100% contrast, a spatial frequency of 0.5 cycles per degrees of visual angle (dva), a speed of 6 cycles per second, a Gaussian envelope with a standard deviation of 1.2 dva, and a randomized starting phase. Each ellipse had a total area of 2.4 dva<sup>2</sup>, and was black (0.01 cd/m<sup>2</sup>). We varied the contrast of the Gabor and the elongation (eccentricity) of the ellipse.</p>
<p><italic>Categories</italic>. In Task A, stimulus orientations were drawn from Gaussian distributions with means <italic>μ</italic><sub>1</sub> = −4° (category 1) and <italic>μ</italic><sub>2</sub> = 4° (category 2) and standard deviations <italic>σ</italic><sub>1</sub> = <italic>σ</italic><sub>2</sub> = 5°. In Task B, stimulus orientations were drawn from Gaussian distributions with means <italic>μ</italic><sub>1</sub> = <italic>μ</italic><sub>2</sub> = 0°, and standard deviations <italic>σ</italic><sub>1</sub> = 3° (category 1) and <italic>σ</italic><sub>2</sub> = 12° (category 2) (<xref ref-type="fig" rid="pcbi.1006572.g001">Fig 1b</xref>). We chose these category means and standard deviations such that the accuracy of an optimal observer would be around 80%.</p>
</sec>
<sec id="sec028">
<title>Procedure</title>
<p>Each subject completed 5 sessions. Each session consisted of two parts; the subject did Task A in the first part, followed by Task B in the second part, or vice versa (chosen randomly each session). Each part started with instruction and was followed by alternating blocks of 96 category training trials and 144 testing trials, for a total of three blocks of each type, with a block of 24 confidence training trials immediately after the first category training block. Combining all sessions and both tasks, each subject completed 2880 category training trials, 240 confidence training trials, and 4320 testing trials; we did not analyze category training or confidence training trials.</p>
<p><italic>Instruction</italic>. At the start of each part of a session, subjects were shown 30 (72 in the first session) exemplar stimuli from each category. Additionally, we provided them with a printed graphic similar to <xref ref-type="fig" rid="pcbi.1006572.g001">Fig 1b</xref>, and explained how the stimuli were generated from distributions. We answered any questions.</p>
<p><italic>Category training</italic>. To ensure that subjects knew the stimulus distributions well, we gave them extensive category training. Each trial proceeded as follows (<xref ref-type="fig" rid="pcbi.1006572.g001">Fig 1a</xref>): Subjects fixated on a central cross for 1 s. Category 1 or category 2 was selected with equal probability. The stimulus orientation was drawn from the corresponding stimulus distribution (<xref ref-type="fig" rid="pcbi.1006572.g001">Fig 1b</xref>). Gabors had 100% contrast, and ellipses had 0.95 eccentricity (elongation). The stimulus appeared at fixation for 300 ms, replacing the fixation cross. Subjects were asked to report category 1 or category 2 by pressing a button with their left or right index finger, respectively. Subjects were able to respond immediately after the offset of the stimulus, at which point verbal correctness feedback was displayed for 1.1 s. The fixation cross then reappeared.</p>
<p><italic>Confidence training</italic>. To familiarize subjects with the button mappings, they completed a short confidence training block at the start of every task. We told subjects that in this block, it would be harder to tell what the stimulus orientation was, there would be no correctness feedback, and they would be reporting their confidence on each trial in addition to their category choice. We provided them with a printed graphic similar to the buttons pictured in <xref ref-type="fig" rid="pcbi.1006572.g001">Fig 1a</xref>, indicating that they had to press one of eight buttons to indicate both category choice and confidence level, the latter on a 4-point scale. The confidence levels were labeled as “very high,” “somewhat high,” “somewhat low,” and “very low.” Gabors had 0.4%, 0.8%, 1.7%, 3.3%, 6.7%, or 13.5% contrast, and ellipses had 0.15, 0.28, 0.41, 0.54, 0.67, or 0.8 eccentricity, chosen randomly with equal probability on each trial (<xref ref-type="fig" rid="pcbi.1006572.g001">Fig 1c</xref>). Stimuli were only displayed for 50 ms. Trial-to-trial feedback consisted only of a message telling them which category and confidence level they had reported. Other than these changes, the trial procedure was the same as in category training.</p>
<p>Subjects were not instructed to use the full range of confidence reports [<xref ref-type="bibr" rid="pcbi.1006572.ref020">20</xref>], as that might have biased them away from reporting what felt most natural. Instead, they were simply asked to be “as accurate as possible in reporting their confidence” on each trial.</p>
<p><italic>Testing</italic>. The trial procedure in testing blocks was the same as in confidence training blocks, except that trial-to-trial feedback was completely withheld. At the end of each block, subjects were required to take at least a 30 s break. During the break, they were shown the percentage of trials that they had correctly categorized. Subjects were also shown a list of the top 10 block scores (across all subjects, indicated by initials) for the task they had just done. This was intended to motivate subjects to score highly, and to reassure them that their scores were normal, since it is rare to score above 80% on a block.</p>
</sec>
<sec id="sec029">
<title>Descriptive statistics</title>
<p>Since our models do not include any learning effects, we wanted to ensure that task performance was stable. For all tasks and experiments, we found no evidence that performance changed significantly as a function of the number of trials. For each experiment and task (the 5 lines in <xref ref-type="fig" rid="pcbi.1006572.g008">Fig 8</xref>), we fit a logistic regression to the binary correctness data for each subject, obtaining a set of slope coefficients. We then used a t-test to determine whether these sets of coefficients differed significantly from zero. In no group did the slopes differ significantly from zero; across all 5 groupings the minimum <italic>p</italic>-value was 0.077 (Task A, experiment 2), which would not be significant even before correcting for multiple comparisons.</p>
<fig id="pcbi.1006572.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006572.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Performance as a function of number of trials, for both tasks and for all experiments.</title>
<p>Performance was computed as a moving average over test trials (200 trials wide). Shaded regions represent ±1 s.e.m. over subjects. Performance did not change significantly over the course of each experiment.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.g008" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec030">
<title>Experiment 1</title>
<p>The following statistical differences were assessed using repeated-measures ANOVA.</p>
<p>In Task A, there was a significant effect of true category on category choice (<italic>F</italic><sub>1,10</sub> = 285, <italic>p</italic> &lt; 10<sup>−7</sup>). There was no main effect of reliability, which took 6 levels of contrast or ellipse elongation, on category choice (<italic>F</italic><sub>5,50</sub> = 0.27, <italic>p</italic> = 0.88). In other words, subjects were not significantly biased to respond with a particular category at low reliabilities. There was a significant interaction between reliability and true category, which is to be expected (<italic>F</italic><sub>5,50</sub> = 59.6, <italic>p</italic> &lt; 10<sup>−15</sup>) (<xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3a</xref>).</p>
<p>In Task B, there was again a significant effect of true category on category choice (<italic>F</italic><sub>1,10</sub> = 78.3, <italic>p</italic> &lt; 10<sup>−5</sup>). There was no main effect of reliability (<italic>F</italic><sub>5,50</sub> = 2.93, <italic>p</italic> = 0.051). There was again a significant interaction between reliability and true category (<italic>F</italic><sub>5,50</sub> = 28, <italic>p</italic> &lt; 10<sup>−12</sup>) (<xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3b</xref>).</p>
<p>In Task A, there was a significant effect of true category on response (<italic>F</italic><sub>1,10</sub> = 136, <italic>p</italic> &lt; 10<sup>−6</sup>). There was no main effect of reliability (<italic>F</italic><sub>5,50</sub> = 0.61, <italic>p</italic> = 0.642). There was a significant interaction between reliability and true category (<italic>F</italic><sub>5,50</sub> = 58.7, <italic>p</italic> &lt; 10<sup>−13</sup>) (<xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3c</xref>).</p>
<p>In Task B, there was a significant effect of true category on response (<italic>F</italic><sub>1,10</sub> = 54.2, <italic>p</italic> &lt; 10<sup>−6</sup>). There was a significant effect of reliability (<italic>F</italic><sub>5,50</sub> = 4.84, <italic>p</italic> = 0.0128). There was a significant interaction between reliability and true category (<italic>F</italic><sub>5,50</sub> = 29.2, <italic>p</italic> &lt; 10<sup>−8</sup>) (<xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3d</xref>).</p>
<p>In Task A, there was a main effect of confidence on the proportion of reports (<italic>F</italic><sub>3,30</sub> = 7.75, <italic>p</italic> &lt; 10<sup>−3</sup>); low-confidence reports were more frequent than high-confidence reports. There was no significant effect of true category (<italic>F</italic><sub>1,10</sub> = 0.784, <italic>p</italic> = 0.397) and no interaction between confidence and category on proportion of responses (<italic>F</italic><sub>3,30</sub> = 1.45, <italic>p</italic> = 0.25) (<xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3e</xref>).</p>
<p>In Task B, there was a main effect of confidence on the proportion of reports (<italic>F</italic><sub>3,30</sub> = 4.36, <italic>p</italic> = 0.012). There was no significant effect of category (<italic>F</italic><sub>1,10</sub> = 0.22, <italic>p</italic> = 0.64), although there was an interaction between confidence and category (<italic>F</italic><sub>3,30</sub> = 8.37, <italic>p</italic> = 0.003). This is likely because for task B, category 2 has a higher proportion of “easy” stimuli (<xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3f</xref>).</p>
<p>In both tasks, reported confidence had a significant effect on performance (<italic>F</italic><sub>3,30</sub> = 36.9, <italic>p</italic> &lt; 10<sup>−3</sup>). Task also had a significant effect on performance (<italic>F</italic><sub>1,10</sub> = 20.1, <italic>p</italic> = 0.001); although we chose the category parameters such that the performance of the optimal observer is matched, subjects were significantly better at Task A. There was no interaction between task and confidence (<italic>F</italic><sub>3,30</sub> = 0.878, <italic>p</italic> = 0.436) (<xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3g</xref>).</p>
<p>
<xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3l and 3m</xref> shows psychometric choice curves for both tasks, at all 6 levels of reliability. Each point represents roughly the same number of trials.</p>
<p>
<xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3n and 3o</xref> shows a similar set of psychometric curves. These curves differ from <xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3l and 3m</xref> in that they represent the mean button press rather than mean category choice.</p>
<p>In Task A (<xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3l and 3n</xref>), mean category choice and mean button press depend monotonically on orientation, with a slope that increases with reliability. In Task B (<xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3m and 3o</xref>), the mean category choice and mean button press tends towards category 1 when stimulus orientation is near horizontal, and tends towards category 2 when orientation is strongly tilted; this reflects the stimulus distributions.</p>
<p><italic>Effect of stimulus type on results: Gabor vs. ellipse</italic>. Since some subjects only saw Gabors and some only saw ellipses, we used Spearman’s rank correlation coefficient to measure the similarity of the two groups’ model rankings. Spearman’s rank correlation coefficient between Gabor and ellipse subjects for the summed LOO scores of the model groupings in <xref ref-type="fig" rid="pcbi.1006572.g006">Fig 6</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1 Fig</xref> was 0.952 and 0.944, respectively (a value of 1 would indicate identical rankings). In both model groupings, the identities of the lowest- and highest-ranked models were the same for both Gabor and ellipse subjects. This indicates that the choice of stimulus type did not have a systematic effect on model rankings.</p>
</sec>
</sec>
<sec id="sec031">
<title>Experiment 2: Separate category and confidence responses and testing feedback</title>
<p>This control experiment was identical to experiment 1 except for the following modifications:</p>
<list list-type="bullet">
<list-item>
<p>Subjects first reported choice by pressing one of two buttons with their left hand, and then reported confidence by pressing one of four buttons with their right hand.</p>
</list-item>
<list-item>
<p>Subjects reported confidence in category training blocks, and received correctness feedback after reporting confidence.</p>
</list-item>
<list-item>
<p>There were no confidence training blocks.</p>
</list-item>
<list-item>
<p>In testing blocks, subjects received correctness feedback after each trial.</p>
</list-item>
<list-item>
<p>Subjects completed a total of 3240 testing trials.</p>
</list-item>
<list-item>
<p>8 subjects (0 male), aged 19–23, participated. None were participants in experiment 1, and again, none were fellow scientists.</p>
</list-item>
<list-item>
<p>Drifting Gabors were used; no subjects saw ellipses.</p>
</list-item>
</list>
</sec>
<sec id="sec032">
<title>Experiment 3: Task B only</title>
<p>This experiment was identical to experiment 1 except for the following modifications:</p>
<list list-type="bullet">
<list-item>
<p>Subjects completed blocks of Task B only.</p>
</list-item>
<list-item>
<p>Subjects completed a total of 3240 testing trials.</p>
</list-item>
<list-item>
<p>15 subjects (8 male), aged 19–30, participated. None were participants in experiments 1 or 2.</p>
</list-item>
<list-item>
<p>Drifting Gabors were used; no subjects saw ellipses.</p>
</list-item>
</list>
</sec>
<sec id="sec033">
<title>Modeling</title>
<sec id="sec034">
<title>Measurement noise</title>
<p>For models (such as our core models) where the relationship between reliability (i.e., contrast or ellipse eccentricity) and noise was parametric, we assumed a power law relationship between reliability <italic>c</italic> and measurement noise variance <italic>σ</italic><sup>2</sup>: <italic>σ</italic><sup>2</sup>(<italic>c</italic>) = <italic>γ</italic> + <italic>αc</italic><sup>−<italic>β</italic></sup>. We have previously [<xref ref-type="bibr" rid="pcbi.1006572.ref030">30</xref>] used this power law relationship because it encompasses a large family of monotonically decreasing relationships using only three parameters. The relationship is also consistent with a form of the Naka-Rushton function [<xref ref-type="bibr" rid="pcbi.1006572.ref084">84</xref>, <xref ref-type="bibr" rid="pcbi.1006572.ref085">85</xref>] commonly used to describe the mapping from reliability to neural gain <italic>g</italic>: <inline-formula id="pcbi.1006572.e005"><alternatives><graphic id="pcbi.1006572.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mi>g</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>γ</mml:mi> <mml:msup><mml:mi>c</mml:mi> <mml:mi>β</mml:mi></mml:msup></mml:mrow> <mml:mrow><mml:msup><mml:mi>c</mml:mi> <mml:mi>β</mml:mi></mml:msup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. The power law relationship then holds under the assumption that measurement noise variance is inversely proportional to gain [<xref ref-type="bibr" rid="pcbi.1006572.ref086">86</xref>].</p>
<p>For all models except the Bayesian model with additive precision, we assumed additive orientation-dependent noise in the form of a rectified 2-cycle sinusoid, accounting for the finding that measurement noise is higher at non-cardinal orientations [<xref ref-type="bibr" rid="pcbi.1006572.ref087">87</xref>]. The measurement noise s.d. comes out to
<disp-formula id="pcbi.1006572.e006"><alternatives><graphic id="pcbi.1006572.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mrow><mml:mi>σ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>,</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msqrt><mml:mrow><mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:msup><mml:mi>c</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>β</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msqrt> <mml:mo>+</mml:mo> <mml:mi>ψ</mml:mi> <mml:mrow><mml:mo>|</mml:mo><mml:mtext>sin</mml:mtext> <mml:mfrac><mml:mrow><mml:mi>π</mml:mi> <mml:mi>s</mml:mi></mml:mrow> <mml:mn>90</mml:mn></mml:mfrac><mml:mo>|</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula></p>
</sec>
<sec id="sec035">
<title>Response probability</title>
<p>We coded all responses as <italic>r</italic> ∈ {1, 2, …, 8}, with each value indicating category and confidence. For all models except the Linear Neural model, the probability of a single trial <italic>i</italic> is equal to the probability mass of the measurement distribution <inline-formula id="pcbi.1006572.e007"><alternatives><graphic id="pcbi.1006572.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>∣</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> (i.e., a normal distribution over <italic>x</italic> with mean <italic>s</italic><sub><italic>i</italic></sub> and variance <inline-formula id="pcbi.1006572.e008"><alternatives><graphic id="pcbi.1006572.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>) in a range corresponding to the subject’s response <italic>r</italic><sub><italic>i</italic></sub>. Because we only use a small range of orientations, we can safely approximate measurement noise as a normal distribution rather than a Von Mises distribution. We find the boundaries <inline-formula id="pcbi.1006572.e009"><alternatives><graphic id="pcbi.1006572.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> in measurement space, as defined by the fitting model and parameters <italic>θ</italic>, and then compute the probability mass of the measurement distribution between the boundaries. For Task A, this quantity is
<disp-formula id="pcbi.1006572.e010"><alternatives><graphic id="pcbi.1006572.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:msub><mml:mi>b</mml:mi> <mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow> <mml:msub><mml:mi>b</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub></mml:msubsup> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow/><mml:mspace width="-0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>b</italic><sub>0</sub> = −∞° and <italic>b</italic><sub>8</sub> = ∞°. For Task B, this quantity is
<disp-formula id="pcbi.1006572.e011"><alternatives><graphic id="pcbi.1006572.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>∣</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow/><mml:mspace width="-0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>x</mml:mi> <mml:mo>+</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:msub><mml:mi>b</mml:mi> <mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow> <mml:msub><mml:mi>b</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub></mml:msubsup> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow/><mml:mspace width="-0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
where <italic>b</italic><sub>0</sub> = 0° and <italic>b</italic><sub>8</sub> = ∞°.</p>
<p>To obtain the log likelihood of the dataset, given a model with parameters <italic>θ</italic>, we compute the sum of the log probability for every trial <italic>i</italic>, where <italic>t</italic> is the total number of trials:
<disp-formula id="pcbi.1006572.e012"><alternatives><graphic id="pcbi.1006572.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mrow><mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mtext>data</mml:mtext> <mml:mo>∣</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:munderover> <mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>∣</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:munderover> <mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:msub><mml:mi>p</mml:mi> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>∣</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula></p>
</sec>
<sec id="sec036">
<title>Model specification</title>
<p><italic>Bayesian</italic>. Derivation of <italic>d</italic><sub>A</sub> and <italic>d</italic><sub>B</sub>: The log posterior ratio <italic>d</italic> is equivalent to the log likelihood ratio plus the log prior ratio:
<disp-formula id="pcbi.1006572.e013"><alternatives><graphic id="pcbi.1006572.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mrow><mml:mi>d</mml:mi> <mml:mo>=</mml:mo> <mml:mtext>log</mml:mtext> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>∣</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn> <mml:mo>∣</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mtext>log</mml:mtext> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mtext>log</mml:mtext> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula></p>
<p>To get <italic>d</italic><sub>A</sub> and <italic>d</italic><sub>B</sub>, we need to find the task-specific expressions for <italic>p</italic>(<italic>x</italic> ∣ <italic>C</italic>). The observer knows that the measurement <italic>x</italic> is caused by the stimulus <italic>s</italic>, but has no knowledge of <italic>s</italic>. Therefore, the optimal observer marginalizes over <italic>s</italic>:
<disp-formula id="pcbi.1006572.e014"><alternatives><graphic id="pcbi.1006572.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo> <mml:mrow/><mml:mspace width="-0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>s</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
We substitute the expressions for the noise distribution and the stimulus distribution, and evaluate the integral:
<disp-formula id="pcbi.1006572.e015"><alternatives><graphic id="pcbi.1006572.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>∫</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>C</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>C</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow/><mml:mspace width="-0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>C</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>C</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula>
Plugging the task- and category-specific <italic>μ</italic><sub><italic>C</italic></sub> and <italic>σ</italic><sub><italic>C</italic></sub> into <xref ref-type="disp-formula" rid="pcbi.1006572.e015">Eq (7)</xref>, and substituting the resulting expression back into <xref ref-type="disp-formula" rid="pcbi.1006572.e013">Eq (5)</xref>, we get:
<disp-formula id="pcbi.1006572.e016"><alternatives><graphic id="pcbi.1006572.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:msub><mml:mi>d</mml:mi> <mml:mtext>A</mml:mtext></mml:msub> <mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>x</mml:mi> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mtext>log</mml:mtext> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula>
<disp-formula id="pcbi.1006572.e017"><alternatives><graphic id="pcbi.1006572.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:msub><mml:mi>d</mml:mi> <mml:mtext>B</mml:mtext></mml:msub> <mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mtext>log</mml:mtext> <mml:mfrac><mml:mrow><mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mrow><mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:msup><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mtext>log</mml:mtext> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula></p>
<p>The 8 possible category and confidence responses are determined by comparing the log posterior ratio <italic>d</italic> to a set of decision boundaries <bold>k</bold> = (<italic>k</italic><sub>0</sub>, <italic>k</italic><sub>1</sub>, …, <italic>k</italic><sub>8</sub>). <italic>k</italic><sub>4</sub> is equal to the log prior ratio <inline-formula id="pcbi.1006572.e018"><alternatives><graphic id="pcbi.1006572.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:mtext>log</mml:mtext> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, which functions as the boundary on <italic>d</italic> between the 4 category 1 responses and the 4 category 2 responses; <italic>k</italic><sub>4</sub> is the only boundary parameter in models of category choice (and not confidence). <italic>k</italic><sub>0</sub> is fixed at −∞ and <italic>k</italic><sub>8</sub> is fixed at ∞. In all models, the observer chooses category 1 when <italic>d</italic> is positive.</p>
<p>Because the decision boundaries are free parameters, our models effectively include a large family of possible cost functions. A different cost function would be equivalent to a rescaling of the confidence boundaries <bold>k</bold>. To see this, it is probably easiest to consider category choice alone; there, asymmetric costs for getting either category wrong would translate into a different value of <italic>k</italic><sub>4</sub>, the category decision boundary (i.e., the observer’s prior over category). For us, this boundary (like all other boundaries) is a free parameter.</p>
<p>The posterior probability of category 1 can be written as as <inline-formula id="pcbi.1006572.e019"><alternatives><graphic id="pcbi.1006572.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>∣</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mtext>exp</mml:mtext> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>d</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>Levels of strength: The Bayesian model is unique in that it is possible to formulate a principled version with relatively few boundary parameters. In principle, it is possible that such a model could perform better than more flexible models, if those models are overfitting. We formulated several levels of strength of the BCH, with weaker versions having fewer assumptions and more sets of mappings between the posterior probability of being correct and the confidence report (<xref ref-type="fig" rid="pcbi.1006572.g009">Fig 9</xref>). In the <italic>ultrastrong BCH</italic>, confidence is a function solely of the posterior probability of the chosen category. In the <italic>strong BCH</italic>, it is additionally a function of the current task.</p>
<fig id="pcbi.1006572.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006572.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Distributions of posterior probabilities of being correct, with confidence criteria for Bayesian models with three different levels of strength.</title>
<p>Solid lines represent the distributions of posterior probabilities for each category and task in the absence of measurement noise and sensory uncertainty. Dashed lines represent confidence criteria, generated from the mean of subject 4’s posterior distribution over parameters. Each model has a different number of sets of mappings between posterior probability and confidence report. In Bayes<sub>Ultrastrong</sub>, there is one set of mappings. In Bayes<sub>Strong</sub>, there is one set for Task A, and another for Task B. In Bayes<sub>Weak</sub>, as in the non-Bayesian models, there is one set for Task A, and one set for each reported category in Task B. Plots were generated from the mean of subject 4’s posterior distribution over parameters as in <xref ref-type="fig" rid="pcbi.1006572.g002">Fig 2</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.g009" xlink:type="simple"/>
</fig>
<p>Most studies cannot distinguish between the ultrastrong and strong BCH because they test subjects in only one task. Furthermore, the weak BCH is only justifiable in tasks where the categories have different distributions of the posterior probability of being correct; the subject may then rescale their mappings between the posterior and their confidence. Here, one can see that Task B has this feature by observing that, in the bottom row of <xref ref-type="fig" rid="pcbi.1006572.g009">Fig 9</xref>, the distributions of posterior probabilities are different for the two categories). Most experimental tasks are like Task A, where the distributions are identical. We compared Bayesian models (Bayes<sub>Ultrastrong</sub>, Bayes<sub>Strong</sub>) corresponding to each of these versions of the BCH.</p>
<p>In Bayes<sub>Ultrastrong</sub>, <bold>k</bold> is symmetric across <italic>k</italic><sub>4</sub>: <italic>k</italic><sub>4+<italic>j</italic></sub> − <italic>k</italic><sub>4</sub> = <italic>k</italic><sub>4</sub> − <italic>k</italic><sub>4−<italic>j</italic></sub> for <italic>j</italic> ∈ {1, 2, 3}. Furthermore, in Bayes<sub>Ultrastrong</sub>, <bold>k</bold><sub>A</sub> = <bold>k</bold><sub>B</sub>. So Bayes<sub>Ultrastrong</sub> has a total of 4 free boundary parameters: <italic>k</italic><sub>1</sub>, <italic>k</italic><sub>2</sub>, <italic>k</italic><sub>3</sub>, <italic>k</italic><sub>4</sub>. Bayes<sub>Ultrastrong</sub> consists of the observer determining the response by comparing <italic>d</italic><sub>A</sub> and <italic>d</italic><sub>B</sub> to a single symmetric set of boundaries (<xref ref-type="fig" rid="pcbi.1006572.g009">Fig 9</xref>, left column).</p>
<p>Bayes<sub>Strong</sub> is identical to Bayes<sub>Ultrastrong</sub> except that <bold>k</bold><sub><bold>A</bold></sub> is allowed to differ from <bold>k</bold><sub><bold>B</bold></sub>. So Bayes<sub>Strong</sub> has a total of 8 free boundary parameters: <italic>k</italic><sub>1A</sub>, <italic>k</italic><sub>2A</sub>, <italic>k</italic><sub>3A</sub>, <italic>k</italic><sub>4A</sub>, <italic>k</italic><sub>1B</sub>, <italic>k</italic><sub>2B</sub>, <italic>k</italic><sub>3B</sub>, <italic>k</italic><sub>4B</sub>. Bayes<sub>Strong</sub> consists of the observer determining the response by comparing <italic>d</italic><sub>A</sub> to a symmetric set of boundaries, and <italic>d</italic><sub>B</sub> to a different symmetric set of boundaries (<xref ref-type="fig" rid="pcbi.1006572.g009">Fig 9</xref>, middle column).</p>
<p>Bayes<sub>Weak</sub> is identical to Bayes<sub>Strong</sub> except that symmetry is not enforced for <bold>k</bold><sub><bold>B</bold></sub>. So Bayes<sub>Weak</sub> has a total of 11 free boundary parameters: <italic>k</italic><sub>1A</sub>, <italic>k</italic><sub>2A</sub>, <italic>k</italic><sub>3A</sub>, <italic>k</italic><sub>4A</sub>, <italic>k</italic><sub>1B</sub>, <italic>k</italic><sub>2B</sub>, <italic>k</italic><sub>3B</sub>, <italic>k</italic><sub>4B</sub>, <italic>k</italic><sub>5B</sub>, <italic>k</italic><sub>6B</sub>, <italic>k</italic><sub>7B</sub>. Bayes<sub>Weak</sub> consists of the observer comparing <italic>d</italic><sub>A</sub> to a symmetric set of boundaries, and <italic>d</italic><sub>B</sub> to a different non-symmetric set of boundaries (<xref ref-type="fig" rid="pcbi.1006572.g009">Fig 9</xref>, right column).</p>
<p>We did not include Bayes<sub>Strong</sub> and Bayes<sub>Ultrastrong</sub> in the core models reported in the main text, because Bayes<sub>Weak</sub> provided a much better fit to the data. Because it was not necessary in the main text to distinguish the three strengths of Bayesian models, we refer to Bayes<sub>Weak</sub> there simply as Bayes. However, we do include Bayes<sub>Strong</sub> and Bayes<sub>Ultrastrong</sub> in our model recovery analysis (described below) and in our supplemental model comparison tables.</p>
<p>Decision boundaries: In the Bayesian models without <italic>d</italic> noise, we translate boundary parameters <bold>k</bold> to measurement boundaries <bold>b</bold> corresponding to fitted noise levels <italic>σ</italic>. To do this, we use the parameters <bold>k</bold> as the left-hand side of Eqs <xref ref-type="disp-formula" rid="pcbi.1006572.e016">(8)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006572.e017">(9)</xref> and solve for <italic>x</italic> at the fitted levels of <italic>σ</italic>. These values were used as the measurement boundaries <bold>b</bold>(<italic>σ</italic>).</p>
<p>In the Bayesian models with <italic>d</italic> noise, we assume that, for each trial, there is an added Gaussian noise term on <italic>d</italic>, <italic>η</italic><sub><italic>d</italic></sub> ∼ <italic>p</italic>(<italic>η</italic><sub><italic>d</italic></sub>), where <inline-formula id="pcbi.1006572.e020"><alternatives><graphic id="pcbi.1006572.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>η</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and <italic>σ</italic><sub><italic>d</italic></sub> is a free parameter. We pre-computed 101 evenly spaced draws of <italic>η</italic><sub><italic>d</italic></sub> and their corresponding probability densities <italic>p</italic>(<italic>η</italic><sub><italic>d</italic></sub>). We used Eqs <xref ref-type="disp-formula" rid="pcbi.1006572.e016">(8)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006572.e017">(9)</xref> to compute a lookup table containing the values of <italic>d</italic> as a function of <italic>x</italic>, <italic>σ</italic>, and <italic>η</italic><sub><italic>d</italic></sub>. We then used linear interpolation to find sets of measurement boundaries <bold>b</bold>(<italic>σ</italic>) corresponding to each draw of <italic>η</italic><sub><italic>d</italic></sub> [<xref ref-type="bibr" rid="pcbi.1006572.ref046">46</xref>]. We then computed 101 response probabilities for each trial, one for each draw of <italic>η</italic><sub><italic>d</italic></sub>, and computed the weighted average according to <italic>p</italic>(<italic>η</italic><sub><italic>d</italic></sub>).</p>
<p><italic>Probability correct with additive precision</italic>. We tested a model in which the decision variable was a weighted mixture of precision (equivalent in this case to the Fisher information of the measurement variable <italic>x</italic>) and the perceived probability of being correct [<xref ref-type="bibr" rid="pcbi.1006572.ref049">49</xref>]. In this model, the decision variable is <inline-formula id="pcbi.1006572.e021"><alternatives><graphic id="pcbi.1006572.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:mfrac><mml:mi>ω</mml:mi> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mtext>exp</mml:mtext> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mo>|</mml:mo> <mml:mi>d</mml:mi> <mml:mo>|</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>ω</italic> is a free parameter. To find the measurement boundaries <bold>b</bold>(<italic>σ</italic>), we substituted Eqs <xref ref-type="disp-formula" rid="pcbi.1006572.e016">(8)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006572.e017">(9)</xref> for <italic>d</italic>, and set the whole value equal to parameters <bold>k</bold>, solving for <italic>x</italic> at the fitted levels of <italic>σ</italic>. This model can be considered a hybrid Bayesian-heuristic model. Like Bayes<sub>Ultrastrong</sub>, it has 4 free boundary parameters. Although the model is a hybrid Bayesian-heuristic model, not a strictly Bayesian one, we refer to it as Bayes<sub>Ultrastrong</sub> + precision in <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1 Fig</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1 Table</xref>.</p>
<p><italic>Fixed</italic>. In Fixed, the observer compares the measurement to a set of boundaries that are not dependent on <italic>σ</italic>. We fit free parameters <bold>k</bold>, and use measurement boundaries <italic>b</italic><sub><italic>r</italic></sub> = <italic>k</italic><sub><italic>r</italic></sub>.</p>
<p><italic>Lin and Quad</italic>. In Lin and Quad, the observer compares the measurement to a set of boundaries that are linear or quadratic functions of <italic>σ</italic>. We fit free parameters <bold>k</bold> and <bold>m</bold>, and use measurement boundaries <italic>b</italic><sub><italic>r</italic></sub>(<italic>σ</italic>) = <italic>k</italic><sub><italic>r</italic></sub> + <italic>m</italic><sub><italic>r</italic></sub><italic>σ</italic> (Lin) or <italic>b</italic><sub><italic>r</italic></sub>(<italic>σ</italic>) = <italic>k</italic><sub><italic>r</italic></sub> + <italic>m</italic><sub><italic>r</italic></sub><italic>σ</italic><sup>2</sup> (Quad).</p>
<p>Lin and Quad are each a supermodel of Fixed. In other words, there are parameter settings where Lin and Quad are equivalent to Fixed (although our model comparison methods ensure that the models are still distinguishable, see “Model recovery” section). Additionally, in Task A, Quad is a supermodel of the Bayesian models without <italic>d</italic> noise.</p>
<p><italic>Orientation estimation</italic>. In Orientation Estimation, the observer uses the mixture of the two stimulus distributions as a prior distribution to compute a maximum a posteriori estimate of the stimulus:
<disp-formula id="pcbi.1006572.e022"><alternatives><graphic id="pcbi.1006572.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>=</mml:mo> <mml:munder><mml:mtext>argmax</mml:mtext> <mml:mi>s</mml:mi></mml:munder> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula>
<disp-formula id="pcbi.1006572.e023"><alternatives><graphic id="pcbi.1006572.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mrow><mml:mo>=</mml:mo> <mml:munder><mml:mtext>argmax</mml:mtext> <mml:mi>s</mml:mi></mml:munder> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
<disp-formula id="pcbi.1006572.e024"><alternatives><graphic id="pcbi.1006572.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mrow><mml:mo>=</mml:mo> <mml:munder><mml:mtext>argmax</mml:mtext> <mml:mi>s</mml:mi></mml:munder> <mml:mo>[</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>
The observer then compares <inline-formula id="pcbi.1006572.e025"><alternatives><graphic id="pcbi.1006572.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> to a set of boundaries <bold>k</bold> to determine category and confidence response.</p>
<p><italic>Decision boundaries</italic>. To find the decision boundaries in measurement space, we used <italic>gmm1max_n2_fast</italic> from Luigi Acerbi’s gmm1 (<ext-link ext-link-type="uri" xlink:href="https://github.com/lacerbi/gmm1" xlink:type="simple">github.com/lacerbi/gmm1</ext-link>) 1-D Gaussian mixture model toolbox to solve <xref ref-type="disp-formula" rid="pcbi.1006572.e024">Eq (12)</xref>, computing a lookup table containing the value of <inline-formula id="pcbi.1006572.e026"><alternatives><graphic id="pcbi.1006572.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> as a function of <italic>x</italic> and <italic>σ</italic> [<xref ref-type="bibr" rid="pcbi.1006572.ref046">46</xref>]. We then found, using linear interpolation, the values of <italic>x</italic> corresponding to <italic>σ</italic> and the free parameters <bold>k</bold>. These values were used as the measurement boundaries <bold>b</bold>(<italic>σ</italic>).</p>
<p><italic>Linear neural</italic>. In this section, <bold>r</bold> refers to neural activity, not button responses. This model is different from all other models in that the generative model does not include measurement <italic>x</italic>. The model can be derived as follows.</p>
<p>All neurons have Gaussian tuning curves with variance <inline-formula id="pcbi.1006572.e027"><alternatives><graphic id="pcbi.1006572.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>TC</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> and gain <inline-formula id="pcbi.1006572.e028"><alternatives><graphic id="pcbi.1006572.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mi>g</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. Tuning curve means are contained in the vector of preferred stimuli <inline-formula id="pcbi.1006572.e029"><alternatives><graphic id="pcbi.1006572.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. The number of spikes in the population is <inline-formula id="pcbi.1006572.e030"><alternatives><graphic id="pcbi.1006572.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi> <mml:mo>∼</mml:mo> <mml:mtext>Poisson</mml:mtext> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>TC</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Neural weights are a linear function of the preferred stimuli: <inline-formula id="pcbi.1006572.e031"><alternatives><graphic id="pcbi.1006572.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>On each trial, we get some quantity that is a weighted sum of each neuron’s activity, <italic>z</italic> = <bold>w</bold> ⋅ <bold>r</bold>.<inline-formula id="pcbi.1006572.e032"><alternatives><graphic id="pcbi.1006572.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:mtext mathvariant="double-struck">E</mml:mtext> <mml:mo>[</mml:mo> <mml:mi>z</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>s</mml:mi> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>·</mml:mo> <mml:mtext mathvariant="double-struck">E</mml:mtext> <mml:mo>[</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>s</mml:mi> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mi>g</mml:mi> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mtext>exp</mml:mtext> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>TC</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>Rather than sum over all neurons, we assume an infinite number of neurons uniformly spanning all possible preferred stimuli <inline-formula id="pcbi.1006572.e033"><alternatives><graphic id="pcbi.1006572.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. This allows us to replace the sum with an integral. The expected value of <italic>z</italic> is <inline-formula id="pcbi.1006572.e034"><alternatives><graphic id="pcbi.1006572.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mrow><mml:mi>a</mml:mi> <mml:mi>g</mml:mi> <mml:mo>∫</mml:mo> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mtext>exp</mml:mtext> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>TC</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mrow/><mml:mspace width="-0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mi>g</mml:mi> <mml:mi>s</mml:mi> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>TC</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>. The variance of <italic>z</italic> is <inline-formula id="pcbi.1006572.e035"><alternatives><graphic id="pcbi.1006572.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:msubsup><mml:mi>w</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:msub><mml:mi>f</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mi>g</mml:mi> <mml:mo>∫</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msup> <mml:mtext>exp</mml:mtext> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>TC</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mrow/><mml:mspace width="-0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mi>g</mml:mi> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>TC</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>TC</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>s</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>Now that we have the mean and variance of <italic>z</italic>, we assume that <italic>z</italic> is normally distributed. This is equivalent to assuming that there are a high number of spikes, because the Poisson distribution approximates the normal distribution as the rate parameter becomes high. To compute response probability, we fit neural activity boundaries <bold>k</bold>, and replace Eqs <xref ref-type="disp-formula" rid="pcbi.1006572.e010">(2)</xref> or <xref ref-type="disp-formula" rid="pcbi.1006572.e011">(3)</xref> with
<disp-formula id="pcbi.1006572.e036"><alternatives><graphic id="pcbi.1006572.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>∣</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:msub><mml:mi>k</mml:mi> <mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow> <mml:msub><mml:mi>k</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub></mml:msubsup> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>;</mml:mo> <mml:mi>a</mml:mi> <mml:mi>g</mml:mi> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>TC</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mi>g</mml:mi> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>TC</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>TC</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow/><mml:mspace width="-0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>z</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula></p>
<p><italic>Lapse rates</italic>. In confidence and category models, we fit three different types of lapse rate. On each trial, there is some fitted probability of:</p>
<list list-type="bullet">
<list-item>
<p>A “full lapse” in which the category report is random, and confidence report is chosen from a distribution over the four levels defined by λ<sub>1</sub>, the probability of a “very low confidence” response, and λ<sub>4</sub>, the probability of a “very high confidence” response, with linear interpolation for the two intermediate levels.</p>
</list-item>
<list-item>
<p>A “confidence lapse” λ<sub>confidence</sub> in which the category report is chosen normally, but the confidence report is chosen from a uniform distribution over the four levels.</p>
</list-item>
<list-item>
<p>A “repeat lapse” λ<sub>repeat</sub> in which the category and confidence response is simply repeated from the previous trial.</p>
</list-item>
</list>
<p>In category choice models, we fit a standard category lapse rate λ, as well as the above “repeat lapse” λ<sub>repeat</sub>.</p>
<p><italic>Parameterization</italic>. Because of tradeoffs when directly fitting parameters <italic>γ</italic>, <italic>α</italic>, <italic>β</italic>, we re-parameterized <xref ref-type="disp-formula" rid="pcbi.1006572.e006">Eq (1)</xref> as
<disp-formula id="pcbi.1006572.e037"><alternatives><graphic id="pcbi.1006572.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mrow><mml:mi>σ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>,</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msqrt><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>L</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>L</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>H</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>c</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>β</mml:mi></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>c</mml:mi> <mml:mtext>L</mml:mtext> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>β</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msubsup><mml:mi>c</mml:mi> <mml:mtext>L</mml:mtext> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>β</mml:mi></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>c</mml:mi> <mml:mtext>H</mml:mtext> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>β</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt> <mml:mo>+</mml:mo> <mml:mi>ψ</mml:mi> <mml:mrow><mml:mo>|</mml:mo><mml:mtext>sin</mml:mtext> <mml:mfrac><mml:mrow><mml:mi>π</mml:mi> <mml:mi>s</mml:mi></mml:mrow> <mml:mn>90</mml:mn></mml:mfrac><mml:mo>|</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
where <italic>c</italic><sub>L</sub> and <italic>c</italic><sub>H</sub> were the values of the lowest and highest reliabilities used. This way, <italic>σ</italic><sub>L</sub> and <italic>σ</italic><sub>H</sub> were free parameters that determined the s.d. of the measurement distributions for the lowest and highest reliabilities, and <italic>β</italic> was a free parameter determining the curvature of the function between the two reliabilities. For models where the relationship between reliability and noise was non-parametric, the first term in <xref ref-type="disp-formula" rid="pcbi.1006572.e006">Eq (1)</xref> was replaced with free s.d. parameters (<italic>σ</italic><sub>rel. 1</sub>, …, <italic>σ</italic><sub>rel. 6</sub>) corresponding to each of the six reliability levels.</p>
<p>For models where subjects had incorrect knowledge about their measurement noise, we fit two sets of uncertainty-related parameters. One set was for the generative measurement noise (used in Eqs <xref ref-type="disp-formula" rid="pcbi.1006572.e010">(2)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006572.e011">(3)</xref>), and the other set was for the subject’s belief about their noise, e.g., their sensory uncertainty (used in Eqs <xref ref-type="disp-formula" rid="pcbi.1006572.e016">(8)</xref>, <xref ref-type="disp-formula" rid="pcbi.1006572.e017">(9)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006572.e024">(12)</xref>).</p>
<p>All parameters that defined the width of a distribution (e.g., <italic>σ</italic><sub>L</sub>, <italic>σ</italic><sub>H</sub>, <italic>σ</italic><sub><italic>d</italic></sub>, <italic>σ</italic><sub>rel. 1</sub>, …) were sampled in log-space and exponentiated during the computation of the log likelihood. See <xref ref-type="supplementary-material" rid="pcbi.1006572.s024">S9 Table</xref> for a complete list of each model’s parameters.</p>
</sec>
<sec id="sec037">
<title>Model fitting</title>
<p>Rather than find a maximum likelihood estimate of the parameters, we sampled from the posterior distribution over parameters, <italic>p</italic>(<italic>θ</italic> ∣ data); this has the advantage of maintaining a measure of uncertainty about the parameters, which can be used both for model comparison and for plotting model fits. We used the log posterior
<disp-formula id="pcbi.1006572.e038"><alternatives><graphic id="pcbi.1006572.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e038" xlink:type="simple"/><mml:math display="block" id="M38"><mml:mrow><mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>∣</mml:mo> <mml:mtext>data</mml:mtext> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mtext>data</mml:mtext> <mml:mo>∣</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mtext>constant</mml:mtext> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula>
where log <italic>p</italic>(data ∣ <italic>θ</italic>) is given in <xref ref-type="disp-formula" rid="pcbi.1006572.e012">Eq (4)</xref>. We assumed a factorized prior over each parameter <italic>j</italic>:
<disp-formula id="pcbi.1006572.e039"><alternatives><graphic id="pcbi.1006572.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e039" xlink:type="simple"/><mml:math display="block" id="M39"><mml:mrow><mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula>
where <italic>j</italic> is the parameter index and <italic>n</italic> is the number of parameters. We took uniform (or, for parameters that were standard deviations, log-uniform) priors over reasonable, sufficiently large ranges [<xref ref-type="bibr" rid="pcbi.1006572.ref046">46</xref>], which we chose before fitting any models.</p>
<p>We sampled from the probability distribution using a Markov Chain Monte Carlo (MCMC) method, slice sampling [<xref ref-type="bibr" rid="pcbi.1006572.ref088">88</xref>]. For each model and dataset combination, we ran between 4 and 7 parallel chains with random starting points. For each chain, we took 40,000 to 600,000 total samples (depending on model computational time) from the posterior distribution over parameters. We discarded the first third of the samples and kept 6,667 of the remaining samples, evenly spaced to reduce autocorrelation. All samples with log posteriors more than 40 below the maximum log posterior were discarded. Marginal probability distributions of the sample log likelihoods were visually checked for convergence across chains. In total we had 842 model and dataset combinations, with a median of 26,668 kept samples (IQR = 13,334).</p>
<p>After sampling, we conducted a visual check to confirm that our parameter ranges were sufficiently large. For each model, we plotted the posterior distribution over parameter values for each subject; an example plot is shown in <xref ref-type="fig" rid="pcbi.1006572.g010">Fig 10</xref>. Visual checks of these plots confirmed that the distributions are unimodal and roughly Gaussian. Visual checks also confirmed that the parameter distributions are well-contained within the chosen parameter ranges, except for the distributions of:</p>
<list list-type="bullet">
<list-item>
<p>Lapse rate parameters, which tend to mass around 0, where they are necessarily bounded.</p>
</list-item>
<list-item>
<p>Log noise parameters, which have a large negative range where they are effectively at zero noise.</p>
</list-item>
<list-item>
<p>Upper confidence boundary parameters, which become small for subjects who frequently report “high confidence,” or large for subjects who frequently do.</p>
</list-item>
</list>
<fig id="pcbi.1006572.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006572.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Posterior distributions over parameter values for an example model.</title>
<p>Each subplot represents a parameter of the model. Each colored histogram represents the sampled posterior distribution for a parameter and a subject in experiment 1, with colors consistent for each subject. The limits of the x-axis indicates the allowable range for each parameter. Black triangles indicate the overall mean parameter value.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.g010" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec038">
<title>Model comparison</title>
<p><italic>Model groupings</italic>. We used 8 groupings of model-subject combinations where it made sense to consider the models as being on equal footing for the purpose of model comparison. The model-subject combinations were grouped by: experiment (which corresponded to subject population), data type (category response only vs. category and confidence response), task type (Task A, B, or both fit jointly). The 8 groupings correspond to <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1</xref> to <xref ref-type="supplementary-material" rid="pcbi.1006572.s008">S8</xref> Figs and <xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1</xref> to <xref ref-type="supplementary-material" rid="pcbi.1006572.s023">S8</xref> Tables.</p>
<p><italic>Metric choice</italic>. A more complex model is likely to fit a dataset better than a simpler model, even if only by chance. Since we are interested in our models’ predictive accuracy for unobserved data, it is important to choose a metric for model comparison that takes the complexity of the model into account, avoiding the problem of overfitting. Roughly speaking, there are two ways to compare models: information criteria and cross-validation.</p>
<p>Most information criteria (such as AIC, BIC, and AICc) are based on a point estimate for <italic>θ</italic>, typically <italic>θ</italic><sub>MLE</sub>, the <italic>θ</italic> that maximizes the log likelihood of the dataset (<xref ref-type="disp-formula" rid="pcbi.1006572.e012">Eq (4)</xref>). For instance, AIC adds a correction for the number of parameters <italic>n</italic> to the log likelihood of the dataset: <inline-formula id="pcbi.1006572.e040"><alternatives><graphic id="pcbi.1006572.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mrow><mml:mtext>AIC</mml:mtext> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:msubsup> <mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>∣</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mtext>MLE</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>WAIC is a more Bayesian approach to information criteria that adds a correction for the effective number of parameters [<xref ref-type="bibr" rid="pcbi.1006572.ref089">89</xref>]. Because WAIC is based on samples from the full posterior of <italic>θ</italic> (<xref ref-type="disp-formula" rid="pcbi.1006572.e038">Eq (15)</xref>, typically sampled via MCMC), it takes into account the model’s uncertainty landscape.</p>
<p>Although information criteria are computationally convenient, they are based on asymptotic results and assumptions about the data that may not always hold [<xref ref-type="bibr" rid="pcbi.1006572.ref089">89</xref>]. An alternative way to estimate predictive accuracy for unobserved data is to cross-validate, fitting the model to training data and evaluating the fit on held out data. Leave-one-out cross-validation is the most thorough way to cross-validate, but is very computationally intensive; it requires that you fit your model <italic>t</italic> times, where <italic>t</italic> is the number of trials. Here we use a method (PSIS-LOO, referred to here simply as LOO) proposed by Vehtari et al. [<xref ref-type="bibr" rid="pcbi.1006572.ref043">43</xref>] for approximating leave-one-out cross-validation that, like WAIC, uses samples from the full posterior of <italic>θ</italic>:
<disp-formula id="pcbi.1006572.e041"><alternatives><graphic id="pcbi.1006572.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e041" xlink:type="simple"/><mml:math display="block" id="M41"><mml:mrow><mml:mtext>LOO</mml:mtext> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:munderover> <mml:mtext>log</mml:mtext> <mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>u</mml:mi></mml:msub> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>u</mml:mi></mml:mrow></mml:msub> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>∣</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>u</mml:mi></mml:msub> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(17)</label></disp-formula>
where <italic>θ</italic><sub><italic>u</italic></sub> is the <italic>u</italic>-th sampled set of parameters, and <italic>w</italic><sub><italic>i</italic>,<italic>u</italic></sub> is the importance weight of trial <italic>i</italic> for sample <italic>u</italic>. Pareto smoothed importance sampling provides an accurate and reliable estimate of the weights. LOO is currently the most accurate approximation of leave-one-out cross-validation [<xref ref-type="bibr" rid="pcbi.1006572.ref090">90</xref>]. Conveniently, it has a natural diagnostic that allows the user to know when the metric may be inaccurate [<xref ref-type="bibr" rid="pcbi.1006572.ref043">43</xref>]; we used that diagnostic and confirmed that our use of the metric is justified.</p>
<p>We determined that our results were not dependent on our choice of metric. We computed AIC, BIC, AICc, WAIC, and LOO for all models in the 8 model groupings, multiplying the information criteria by <inline-formula id="pcbi.1006572.e042"><alternatives><graphic id="pcbi.1006572.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> to match the scale of LOO. For AIC, BIC, and AICc, we used the parameter sample with the highest log likelihood as our estimate of <italic>θ</italic><sub>MLE</sub>. Then we computed Spearman’s rank correlation coefficient for every possible pairwise comparison of model comparison metrics for all model and dataset combinations, producing 80 total values (8 model groupings × 10 possible pairwise comparisons of model comparison metrics). All values were greater than 0.998, indicating that, had we used an information criterion instead of LOO, we would not have changed our conclusions. Furthermore, there are no model groupings in which the identities of the lowest- and highest-ranked models are dependent on the choice of metric. The agreement of these metrics strengthens our confidence in our conclusions.</p>
<p><italic>Metric aggregation</italic>. Summed LOO differences: In all figures where we present model comparison results (e.g., <xref ref-type="fig" rid="pcbi.1006572.g005">Fig 5</xref>, right column), we aggregate LOO scores by the following procedure. Choose a reference model (usually the one with the lowest mean LOO score across subjects). Subtract all LOO scores from the corresponding subject’s score for the reference model; this converts all scores to a LOO “difference from reference” score, with higher scores being worse. Repeat the following standard bootstrap procedure 10,000 times: Choose randomly, with replacement, a group of datasets equal to the total number of unique datasets, and take the sum over subjects of their “difference from reference” scores for each model. Plots indicate the median and 95% CI of these bootstrapped summed “difference from reference” scores. This approach implicitly assumes that all data was generated from the same model.</p>
<p>To confirm that our sample size was large enough to trust our bootstrapped confidence intervals, we bootstrapped our bootstrapping procedure to see how the confidence intervals were affected by the number of subjects <italic>N</italic>. For an example pair of models that we might be interested in comparing, we took the 11 LOO differences between the models, one for each subject in experiment 1. For each <italic>N</italic> between 2 and 11, we took 50 subsamples of our subject LOO differences with replacement; this is akin to running the experiment 50 times for each <italic>N</italic>. For each subsample, we conducted the above bootstrap procedure, which give us a median and 95% CI on the mean of differences. We then plot the mean of these values, with error bars indicating ±1 s.d., at each <italic>N</italic> (<xref ref-type="fig" rid="pcbi.1006572.g011">Fig 11a</xref>). A visual check indicates that the confidence interval appears to converge at about <italic>N</italic> = 9. This indicates that our bootstrapped confidence intervals are trustworthy.</p>
<fig id="pcbi.1006572.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006572.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Example analysis of a bootstrapped confidence interval.</title>
<p>(<bold>a</bold>) Uncertainty estimates for bootstrapped confidence intervals, as a function of the number of subjects included. Blue line represents the median bootstrapped mean of LOO differences, and black lines indicate the lower and upper bounds of the 95% CI. Error bars represent ±1 s.d. (<bold>b</bold>) For comparison to <bold>a</bold>, the standard style of plot used to show model comparison results (e.g., <xref ref-type="fig" rid="pcbi.1006572.g004">Fig 4</xref>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.g011" xlink:type="simple"/>
</fig>
<p>Group level Bayesian model selection: We also used LOO scores to compute two metrics that allow for model heterogeneity across the group. The first metric was “protected exceedance probability,” the posterior probability that one model occurs more frequently than any other model in the set [<xref ref-type="bibr" rid="pcbi.1006572.ref091">91</xref>], above and beyond chance (e.g., <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1b Fig</xref>). The second was the expected posterior probability that a model generated the data of a randomly chosen dataset [<xref ref-type="bibr" rid="pcbi.1006572.ref092">92</xref>] (e.g., <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1c Fig</xref>). The latter metric assumes a uniform prior over models, which is a function of the total number of datasets. We used the SPM12 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm" xlink:type="simple">www.fil.ion.ucl.ac.uk/spm</ext-link>) software package to compute these metrics.</p>
<p>In all but one of the 8 model groupings, all three methods of metric aggregation identify the same overall best model. For example, in <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1 Fig</xref>, one model (Quad + non-param. <italic>σ</italic>) has the lowest summed LOO differences, the highest protected exceedance probability, and the highest expected posterior probability.</p>
</sec>
<sec id="sec039">
<title>Visualization of model fits</title>
<p>Model fits were plotted by bootstrapping synthetic group datasets with the following procedure: For each task, model, and subject, we generated 20 synthetic datasets, each using a different set of parameters sampled, without replacement, from the posterior distribution of parameters. Each synthetic dataset was generated using the same stimuli as the ones presented to the real subject. We randomly selected a number of synthetic datasets equal to the number of subjects to create a synthetic group dataset. For each synthetic group dataset, we computed the mean output (e.g., button press, confidence, performance) per bin. We then repeated this 1,000 times and computed the mean and standard deviation of the mean output per bin across all 1,000 synthetic group datasets, which we then plotted as the shaded regions. Therefore, shaded regions represent the mean ±1 s.e.m. of synthetic group datasets.</p>
<p>For plots with orientation on the horizontal axis (e.g., <xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3j–3o</xref>), stimulus orientation was binned according to quantiles of the task-dependent stimulus distributions so that each point consisted of roughly the same number of trials. For each task, we took the overall stimulus distribution <inline-formula id="pcbi.1006572.e043"><alternatives><graphic id="pcbi.1006572.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006572.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and found bin edges such that the probability mass of <italic>p</italic>(<italic>s</italic>) was the same in each bin. We then plotted the binned data with linear spacing on the horizontal axis.</p>
</sec>
<sec id="sec040">
<title>Model recovery</title>
<p>We performed a model recovery analysis [<xref ref-type="bibr" rid="pcbi.1006572.ref093">93</xref>] to test our ability to distinguish our 6 core models, as well as the 2 stronger versions of the Bayesian model. We generated synthetic datasets from each of the 8 models for both Tasks A and B, using the same sets of stimuli that were originally randomly generated for each of the 11 subjects. To ensure that the statistics of the generated responses were similar to those of the subjects, we generated responses to these stimuli from 4 of the randomly chosen parameter estimates obtained via MCMC sampling for each subject and model. In total, we generated 352 datasets (8 generating models × 11 subjects × 4 datasets). We then fit all 8 models to every dataset, using maximum likelihood estimation (MLE) of parameters by an interior-point constrained optimization (MATLAB’s <italic>fmincon</italic>), and computed AIC scores from the resulting fits.</p>
<p>We found that the true generating model was the best-fitting model, on average, in all cases (<xref ref-type="fig" rid="pcbi.1006572.g012">Fig 12</xref>). Overall, AIC “selected” the correct model (i.e., AIC scores were lowest for the model that generated the data) for 86.6% of the datasets, indicating that our models are distinguishable.</p>
<fig id="pcbi.1006572.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006572.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Model recovery analysis.</title>
<p>Shade represents the difference between the mean AIC score (across datasets) for each fitted model and for the one with the lowest mean AIC score. White squares indicate the model that had the lowest mean AIC score when fitted to data generated from each model. The squares on the diagonal indicate that the true generating model was the best-fitting model, on average, in all cases.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.g012" xlink:type="simple"/>
</fig>
<p>Ideally, we would have evaluated our model recovery fits using LOO, as we evaluated the fits to human data. However, LOO can only be obtained when fitting with MCMC sampling, which takes orders of magnitudes longer than fitting with MLE. It would be impossible to fit all 352 synthetic datasets in a short amount of time using the procedure and sampling quality standards described above (i.e., a large number of samples, across multiple converged chains). Furthermore, we do not believe that our model recovery is dependent on how the models are fit and the fits are evaluated; we found that AIC and LOO scores gave us near-identical model rankings for data from real subjects.</p>
</sec>
</sec>
</sec>
<sec id="sec041">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006572.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Model comparison, experiment 1.</title>
<p>Models were fit jointly to Task A and B category and confidence responses. (a) Medians and 95% CI of bootstrapped sums of LOO differences, relative to the best model. Higher values indicate worse fits. (<bold>b</bold>) The protected exceedance probability, i.e., the posterior probability that a model occurs more frequently than the others [<xref ref-type="bibr" rid="pcbi.1006572.ref091">91</xref>]. (<bold>c</bold>) The expected posterior probability that a model generated the data of a randomly chosen subject [<xref ref-type="bibr" rid="pcbi.1006572.ref092">92</xref>].</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Model comparison, experiment 1.</title>
<p>Models were fit to Task A category and confidence responses. See <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1 Fig</xref> caption.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Model comparison, experiment 1.</title>
<p>Models were fit to Task B category and confidence responses. See <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1 Fig</xref> caption.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Model comparison, experiment 1.</title>
<p>Models were fit jointly to Task A and B category choices. See <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1 Fig</xref> caption.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Model comparison, experiment 1.</title>
<p>Noise parameters were fit to Task A category choices and then fixed during the fitting of Task B category and confidence responses. See <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1 Fig</xref> caption.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Model comparison, experiment 2.</title>
<p>Models were fit jointly to Task A and B category and confidence responses. See <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1 Fig</xref> caption.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s007" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Model comparison, experiment 3.</title>
<p>Models were fit to Task B category and confidence responses. See <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1 Fig</xref> caption.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s008" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>Model comparison, experiment 3.</title>
<p>Models were fit to Task B category choices. See <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1 Fig</xref> caption.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s009" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s009" xlink:type="simple">
<label>S9 Fig</label>
<caption>
<title>Model fits and model comparison for the three strengths of the Bayesian model, as in <xref ref-type="fig" rid="pcbi.1006572.g005">Fig 5</xref>.</title>
<p>In the main text, Bayes<sub>Weak</sub>-<italic>d</italic>N is referred to simply as Bayes.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s010" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s010" xlink:type="simple">
<label>S10 Fig</label>
<caption>
<title>Bayes<sub>Weak</sub>-<italic>d</italic>N fits, as in <xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3</xref>.</title>
<p>In the main text, Bayes<sub>Weak</sub>-<italic>d</italic>N is referred to simply as Bayes.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s011" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s011" xlink:type="simple">
<label>S11 Fig</label>
<caption>
<title>Fixed fits, as in <xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3</xref>.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s012" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s012" xlink:type="simple">
<label>S12 Fig</label>
<caption>
<title>Orientation Estimation fits, as in <xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3</xref>.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s013" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s013" xlink:type="simple">
<label>S13 Fig</label>
<caption>
<title>Linear Neural fits, as in <xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3</xref>.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s014" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s014" xlink:type="simple">
<label>S14 Fig</label>
<caption>
<title>Lin fits, as in <xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3</xref>.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s015" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s015" xlink:type="simple">
<label>S15 Fig</label>
<caption>
<title>Quad fits, as in <xref ref-type="fig" rid="pcbi.1006572.g003">Fig 3</xref>, but for data in experiment 2.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s016" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s016" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Cross comparison of all models in <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1 Fig</xref>.</title>
<p>Cells indicate medians and 95% CI of bootstrapped summed LOO score differences. A negative median indicates that the model in the corresponding row had a higher score (better fit) than the model in the corresponding column.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s017" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s017" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Cross comparison of all models in <xref ref-type="supplementary-material" rid="pcbi.1006572.s002">S2 Fig</xref>.</title>
<p>See <xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1 Table</xref> caption.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s018" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s018" xlink:type="simple">
<label>S3 Table</label>
<caption>
<title>Cross comparison of all models in <xref ref-type="supplementary-material" rid="pcbi.1006572.s003">S3 Fig</xref>.</title>
<p>See <xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1 Table</xref> caption.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s019" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s019" xlink:type="simple">
<label>S4 Table</label>
<caption>
<title>Cross comparison of all models in <xref ref-type="supplementary-material" rid="pcbi.1006572.s004">S4 Fig</xref>.</title>
<p>See <xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1 Table</xref> caption.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s020" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s020" xlink:type="simple">
<label>S5 Table</label>
<caption>
<title>Cross comparison of all models in <xref ref-type="supplementary-material" rid="pcbi.1006572.s005">S5 Fig</xref>.</title>
<p>See <xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1 Table</xref> caption.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s021" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s021" xlink:type="simple">
<label>S6 Table</label>
<caption>
<title>Cross comparison of all models in <xref ref-type="supplementary-material" rid="pcbi.1006572.s006">S6 Fig</xref>.</title>
<p>See <xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1 Table</xref> caption.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s022" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s022" xlink:type="simple">
<label>S7 Table</label>
<caption>
<title>Cross comparison of all models in <xref ref-type="supplementary-material" rid="pcbi.1006572.s007">S7 Fig</xref>.</title>
<p>See <xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1 Table</xref> caption.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s023" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s023" xlink:type="simple">
<label>S8 Table</label>
<caption>
<title>Cross comparison of all models in <xref ref-type="supplementary-material" rid="pcbi.1006572.s008">S8 Fig</xref>.</title>
<p>See <xref ref-type="supplementary-material" rid="pcbi.1006572.s016">S1 Table</xref> caption.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006572.s024" mimetype="application/vnd.ms-excel" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006572.s024" xlink:type="simple">
<label>S9 Table</label>
<caption>
<title>List of parameters for each model.</title>
<p>Each sheet corresponds with the sets of models pictured in <xref ref-type="supplementary-material" rid="pcbi.1006572.s001">S1 Fig</xref> and S1; <xref ref-type="supplementary-material" rid="pcbi.1006572.s002">S2 Fig</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006572.s017">S2 Table</xref>; and so on.</p>
<p>(XLS)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors would like to thank Luigi Acerbi for helpful ideas and tools related to model fitting and model comparison. We would also like to thank Luigi Acerbi, Rachel N. Denison, Andra Mihali, A. Emin Orhan, Bas van Opheusden, and Aspen H. Yoo for helpful conversations and comments about the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006572.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Meyniel</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Sigman</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Mainen</surname> <given-names>ZF</given-names></name>. <article-title>Confidence as Bayesian probability: From neural origins to behavior</article-title>. <source>Neuron</source>. <year>2015</year> <month>Oct</month>;<volume>88</volume>(<issue>1</issue>):<fpage>78</fpage>–<lpage>92</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2015.09.039" xlink:type="simple">10.1016/j.neuron.2015.09.039</ext-link></comment> <object-id pub-id-type="pmid">26447574</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brown</surname> <given-names>AS</given-names></name>. <article-title>A review of the tip-of-the-tongue experience</article-title>. <source>Psychol Bull</source>. <year>1991</year> <month>Mar</month>;<volume>109</volume>(<issue>2</issue>):<fpage>204</fpage>–<lpage>223</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-2909.109.2.204" xlink:type="simple">10.1037/0033-2909.109.2.204</ext-link></comment> <object-id pub-id-type="pmid">2034750</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Persaud</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>McLeod</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cowey</surname> <given-names>A</given-names></name>. <article-title>Post-decision wagering objectively measures awareness</article-title>. <source>Nat Neurosci</source>. <year>2007</year> <month>Jan</month>;<volume>10</volume>(<issue>2</issue>):<fpage>257</fpage>–<lpage>261</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1840" xlink:type="simple">10.1038/nn1840</ext-link></comment> <object-id pub-id-type="pmid">17237774</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bahrami</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Olsen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Roepstorff</surname> <given-names>A</given-names></name>. <article-title>Optimally interacting minds</article-title>. <source>Science</source>. <year>2010</year>;<volume>329</volume>(<issue>5995</issue>):<fpage>1081</fpage>–<lpage>1085</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1185718" xlink:type="simple">10.1126/science.1185718</ext-link></comment> <object-id pub-id-type="pmid">20798320</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fleming</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Weil</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Nagy</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Rees</surname> <given-names>G</given-names></name>. <article-title>Relating introspective accuracy to individual differences in brain structure</article-title>. <source>Science</source>. <year>2010</year> <month>Sep</month>;<volume>329</volume>(<issue>5998</issue>):<fpage>1541</fpage>–<lpage>1543</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1191883" xlink:type="simple">10.1126/science.1191883</ext-link></comment> <object-id pub-id-type="pmid">20847276</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fleming</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>The neural basis of metacognitive ability</article-title>. <source>Phil Trans R Soc B</source>. <year>2012</year> <month>May</month>;<volume>367</volume>(<issue>1594</issue>):<fpage>1338</fpage>–<lpage>1349</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2011.0417" xlink:type="simple">10.1098/rstb.2011.0417</ext-link></comment> <object-id pub-id-type="pmid">22492751</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rutishauser</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Ye</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Koroma</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tudusciuc</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Ross</surname> <given-names>IB</given-names></name>, <name name-style="western"><surname>Chung</surname> <given-names>JM</given-names></name>, <etal>et al</etal>. <article-title>Representation of retrieval confidence by single neurons in the human medial temporal lobe</article-title>. <source>Nat Neurosci</source>. <year>2015</year> <month>Jun</month>;<volume>18</volume>(<issue>7</issue>):<fpage>1041</fpage>–<lpage>1050</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4041" xlink:type="simple">10.1038/nn.4041</ext-link></comment> <object-id pub-id-type="pmid">26053402</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>. <article-title>Representation of confidence associated with a decision by neurons in the parietal cortex</article-title>. <source>Science</source>. <year>2009</year> <month>May</month>;<volume>324</volume>(<issue>5928</issue>):<fpage>759</fpage>–<lpage>764</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1169405" xlink:type="simple">10.1126/science.1169405</ext-link></comment> <object-id pub-id-type="pmid">19423820</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fetsch</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>. <article-title>Effects of cortical microstimulation on confidence in a perceptual decision</article-title>. <source>Neuron</source>. <year>2014</year> <month>Aug</month>;<volume>83</volume>(<issue>4</issue>):<fpage>797</fpage>–<lpage>804</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.07.011" xlink:type="simple">10.1016/j.neuron.2014.07.011</ext-link></comment> <object-id pub-id-type="pmid">25123306</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Komura</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Nikkuni</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hirashima</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Uetake</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Miyamoto</surname> <given-names>A</given-names></name>. <article-title>Responses of pulvinar neurons reflect a subject’s confidence in visual categorization</article-title>. <source>Nat Neurosci</source>. <year>2013</year> <month>Jun</month>;<volume>16</volume>(<issue>6</issue>):<fpage>749</fpage>–<lpage>755</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3393" xlink:type="simple">10.1038/nn.3393</ext-link></comment> <object-id pub-id-type="pmid">23666179</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kepecs</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Uchida</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Zariwala</surname> <given-names>HA</given-names></name>, <name name-style="western"><surname>Mainen</surname> <given-names>ZF</given-names></name>. <article-title>Neural correlates, computation and behavioural impact of decision confidence</article-title>. <source>Nature</source>. <year>2008</year> <month>Sep</month>;<volume>455</volume>(<issue>7210</issue>):<fpage>227</fpage>–<lpage>231</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature07200" xlink:type="simple">10.1038/nature07200</ext-link></comment> <object-id pub-id-type="pmid">18690210</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Drugowitsch</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kepecs</surname> <given-names>A</given-names></name>. <article-title>Confidence and certainty: Distinct probabilistic quantities for different goals</article-title>. <source>Nat Neurosci</source>. <year>2016</year> <month>Feb</month>;<volume>19</volume>(<issue>3</issue>):<fpage>366</fpage>–<lpage>374</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4240" xlink:type="simple">10.1038/nn.4240</ext-link></comment> <object-id pub-id-type="pmid">26906503</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kepecs</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mainen</surname> <given-names>ZF</given-names></name>. <article-title>A computational framework for the study of confidence in humans and animals</article-title>. <source>Phil Trans R Soc B</source>. <year>2012</year> <month>May</month>;<volume>367</volume>(<issue>1594</issue>):<fpage>1322</fpage>–<lpage>1337</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2012.0037" xlink:type="simple">10.1098/rstb.2012.0037</ext-link></comment> <object-id pub-id-type="pmid">22492750</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Drugowitsch</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Moreno-Bote</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Relation between belief and performance in perceptual decision making</article-title>. <source>PLoS ONE</source>. <year>2014</year> <month>May</month>;<volume>9</volume>(<issue>5</issue>):<fpage>e96511</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0096511" xlink:type="simple">10.1371/journal.pone.0096511</ext-link></comment> <object-id pub-id-type="pmid">24816801</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Peirce</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Jastrow</surname> <given-names>J</given-names></name>. <article-title>On small differences in sensation</article-title>. <source>Memoirs of the National Academy of Sciences</source>. <year>1884</year>;<volume>3</volume>:<fpage>73</fpage>–<lpage>83</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref016">
<label>16</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Knill</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Richards</surname> <given-names>W</given-names></name>. <source>Perception as Bayesian inference</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>; <year>1996</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Jazayeri</surname> <given-names>M</given-names></name>. <article-title>Neural coding of uncertainty and probability</article-title>. <source>Annu Rev Neurosci</source>. <year>2014</year>;<volume>37</volume>(<issue>1</issue>):<fpage>205</fpage>–<lpage>220</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-neuro-071013-014017" xlink:type="simple">10.1146/annurev-neuro-071013-014017</ext-link></comment> <object-id pub-id-type="pmid">25032495</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Körding</surname> <given-names>K</given-names></name>. <article-title>Decision theory: What “should” the nervous system do?</article-title> <source>Science</source>. <year>2007</year> <month>Oct</month>;<volume>318</volume>(<issue>5850</issue>):<fpage>606</fpage>–<lpage>610</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1142998" xlink:type="simple">10.1126/science.1142998</ext-link></comment> <object-id pub-id-type="pmid">17962554</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Aitchison</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bang</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Bahrami</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>. <article-title>Doubly Bayesian analysis of confidence in perceptual decision-making</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year> <month>Oct</month>;<volume>11</volume>(<issue>10</issue>):<fpage>e1004519</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004519" xlink:type="simple">10.1371/journal.pcbi.1004519</ext-link></comment> <object-id pub-id-type="pmid">26517475</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sanders</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Hangya</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kepecs</surname> <given-names>A</given-names></name>. <article-title>Signatures of a statistical computation in the human sense of confidence</article-title>. <source>Neuron</source>. <year>2016</year> <month>May</month>;<volume>90</volume>(<issue>3</issue>):<fpage>499</fpage>–<lpage>506</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.03.025" xlink:type="simple">10.1016/j.neuron.2016.03.025</ext-link></comment> <object-id pub-id-type="pmid">27151640</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hangya</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Sanders</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Kepecs</surname> <given-names>A</given-names></name>. <article-title>A mathematical framework for statistical decision confidence</article-title>. <source>Neural Computation</source>. <year>2016</year> <month>Sep</month>;<volume>28</volume>(<issue>9</issue>):<fpage>1840</fpage>–<lpage>1858</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00864" xlink:type="simple">10.1162/NECO_a_00864</ext-link></comment> <object-id pub-id-type="pmid">27391683</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Adler</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>. <article-title>Limitations of proposed signatures of Bayesian confidence</article-title>. <source>Neur Comp</source>. <year>2018</year>;. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco_a_01141" xlink:type="simple">10.1162/neco_a_01141</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bowers</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>CJ</given-names></name>. <article-title>Bayesian just-so stories in psychology and neuroscience</article-title>. <source>Psychol Bull</source>. <year>2012</year>;<volume>138</volume>(<issue>3</issue>):<fpage>389</fpage>–<lpage>414</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0026450" xlink:type="simple">10.1037/a0026450</ext-link></comment> <object-id pub-id-type="pmid">22545686</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jones</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Love</surname> <given-names>BC</given-names></name>. <article-title>Bayesian Fundamentalism or Enlightenment? On the explanatory status and theoretical contributions of Bayesian models of cognition</article-title>. <source>Behav Brain Sci</source>. <year>2011</year> <month>Aug</month>;<volume>34</volume>(<issue>4</issue>):<fpage>169</fpage>–<lpage>188</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X10003134" xlink:type="simple">10.1017/S0140525X10003134</ext-link></comment> <object-id pub-id-type="pmid">21864419</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Navajas</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bahrami</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>. <article-title>Post-decisional accounts of biases in confidence</article-title>. <source>Curr Opin Behav Sci</source>. <year>2016</year>;<volume>11</volume>:<fpage>55</fpage>–<lpage>60</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cobeha.2016.05.005" xlink:type="simple">10.1016/j.cobeha.2016.05.005</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pleskac</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Busemeyer</surname> <given-names>JR</given-names></name>. <article-title>Two-stage dynamic signal detection: A theory of choice, decision time, and confidence</article-title>. <source>Psychol Rev</source>. <year>2010</year> <month>Jul</month>;<volume>117</volume>(<issue>3</issue>):<fpage>864</fpage>–<lpage>901</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0019737" xlink:type="simple">10.1037/a0019737</ext-link></comment> <object-id pub-id-type="pmid">20658856</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Britten</surname> <given-names>KH</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>. <article-title>The analysis of visual motion: A comparison of neuronal and psychophysical performance</article-title>. <source>J Neurosci</source>. <year>1992</year> <month>Dec</month>;<volume>12</volume>(<issue>12</issue>):<fpage>4745</fpage>–<lpage>4765</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.12-12-04745.1992" xlink:type="simple">10.1523/JNEUROSCI.12-12-04745.1992</ext-link></comment> <object-id pub-id-type="pmid">1464765</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Knill</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Kersten</surname> <given-names>D</given-names></name>. <article-title>Object classification for human and ideal observers</article-title>. <source>Vis Res</source>. <year>1995</year> <month>Feb</month>;<volume>35</volume>(<issue>4</issue>):<fpage>549</fpage>–<lpage>568</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0042-6989(94)00150-K" xlink:type="simple">10.1016/0042-6989(94)00150-K</ext-link></comment> <object-id pub-id-type="pmid">7900295</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sanborn</surname> <given-names>AN</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Shiffrin</surname> <given-names>RM</given-names></name>. <article-title>Uncovering mental representations with Markov chain Monte Carlo</article-title>. <source>Cogn Psychol</source>. <year>2010</year> <month>Mar</month>;<volume>60</volume>(<issue>2</issue>):<fpage>63</fpage>–<lpage>106</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cogpsych.2009.07.001" xlink:type="simple">10.1016/j.cogpsych.2009.07.001</ext-link></comment> <object-id pub-id-type="pmid">19703686</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Qamar</surname> <given-names>AT</given-names></name>, <name name-style="western"><surname>Cotton</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>George</surname> <given-names>RG</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Prezhdo</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Laudano</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Trial-to-trial, uncertainty-based adjustment of decision boundaries in visual categorization</article-title>. <source>PNAS</source>. <year>2013</year> <month>Dec</month>;<volume>110</volume>(<issue>50</issue>):<fpage>20332</fpage>–<lpage>20337</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1219756110" xlink:type="simple">10.1073/pnas.1219756110</ext-link></comment> <object-id pub-id-type="pmid">24272938</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maniscalco</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Peters</surname> <given-names>MAK</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>H</given-names></name>. <article-title>Heuristic use of perceptual evidence leads to dissociation between performance and metacognitive sensitivity</article-title>. <source>Atten Percept Psychophys</source>. <year>2016</year> <month>Jan</month>;<volume>78</volume>(<issue>3</issue>):<fpage>923</fpage>–<lpage>937</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13414-016-1059-x" xlink:type="simple">10.3758/s13414-016-1059-x</ext-link></comment> <object-id pub-id-type="pmid">26791233</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Mamassian</surname> <given-names>P</given-names></name>. <article-title>Bayesian decision theory as a model of human visual perception: Testing Bayesian transfer</article-title>. <source>Vis Neurosci</source>. <year>2009</year> <month>Jan</month>;<volume>26</volume>(<issue>1</issue>):<fpage>147</fpage>–<lpage>155</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0952523808080905" xlink:type="simple">10.1017/S0952523808080905</ext-link></comment> <object-id pub-id-type="pmid">19193251</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Körding</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>Bayesian integration in sensorimotor learning</article-title>. <source>Nature</source>. <year>2004</year> <month>Jan</month>;<volume>427</volume>(<issue>6971</issue>):<fpage>244</fpage>–<lpage>247</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature02169" xlink:type="simple">10.1038/nature02169</ext-link></comment> <object-id pub-id-type="pmid">14724638</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Massoni</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gajdos</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Vergnaud</surname> <given-names>JC</given-names></name>. <article-title>Confidence measurement in the light of signal detection theory</article-title>. <source>Front Psychol</source>. <year>2014</year>;<volume>5</volume>(<issue>325</issue>):<fpage>1455</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2014.01455" xlink:type="simple">10.3389/fpsyg.2014.01455</ext-link></comment> <object-id pub-id-type="pmid">25566135</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brier</surname> <given-names>GW</given-names></name>. <article-title>Verification of forecasts expressed in terms of probability</article-title>. <source>Monthly Weather Review</source>. <year>1950</year>;<volume>78</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>3</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1175/1520-0493(1950)078&lt;0001:VOFEIT&gt;2.0.CO;2" xlink:type="simple">10.1175/1520-0493(1950)078&lt;0001:VOFEIT&gt;2.0.CO;2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gneiting</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Raftery</surname> <given-names>AE</given-names></name>. <article-title>Strictly proper scoring rules, prediction, and estimation</article-title>. <source>J Am Stat Assoc</source>. <year>2007</year> <month>Mar</month>;<volume>102</volume>(<issue>477</issue>):<fpage>359</fpage>–<lpage>378</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1198/016214506000001437" xlink:type="simple">10.1198/016214506000001437</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref037">
<label>37</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Green</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Swets</surname> <given-names>JA</given-names></name>. <source>Signal detection theory and psychophysics</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>1966</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>. <article-title>Signal detection theory, uncertainty, and Poisson-like population codes</article-title>. <source>Vis Res</source>. <year>2010</year> <month>Oct</month>;<volume>50</volume>(<issue>22</issue>):<fpage>2308</fpage>–<lpage>2319</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2010.08.035" xlink:type="simple">10.1016/j.visres.2010.08.035</ext-link></comment> <object-id pub-id-type="pmid">20828581</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rahnev</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Maniscalco</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Graves</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>de Lange</surname> <given-names>FP</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>H</given-names></name>. <article-title>Attention induces conservative subjective biases in visual perception</article-title>. <source>Nat Neurosci</source>. <year>2011</year> <month>Oct</month>;<volume>14</volume>(<issue>12</issue>):<fpage>1513</fpage>–<lpage>1515</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2948" xlink:type="simple">10.1038/nn.2948</ext-link></comment> <object-id pub-id-type="pmid">22019729</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>. <article-title>Organizing probabilistic models of perception</article-title>. <source>Trends Cogn Sci</source>. <year>2012</year>;<volume>16</volume>(<issue>10</issue>):<fpage>511</fpage>–<lpage>518</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2012.08.010" xlink:type="simple">10.1016/j.tics.2012.08.010</ext-link></comment> <object-id pub-id-type="pmid">22981359</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maniscalco</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>H</given-names></name>. <article-title>A signal detection theoretic approach for estimating metacognitive sensitivity from confidence ratings</article-title>. <source>Conscious Cogn</source>. <year>2012</year> <month>Mar</month>;<volume>21</volume>(<issue>1</issue>):<fpage>422</fpage>–<lpage>430</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.concog.2011.09.021" xlink:type="simple">10.1016/j.concog.2011.09.021</ext-link></comment> <object-id pub-id-type="pmid">22071269</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fleming</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>HC</given-names></name>. <article-title>How to measure metacognition</article-title>. <source>Front Hum Neurosci</source>. <year>2014</year>;<volume>8</volume>:<fpage>443</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2014.00443" xlink:type="simple">10.3389/fnhum.2014.00443</ext-link></comment> <object-id pub-id-type="pmid">25076880</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref043">
<label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Vehtari A, Gelman A, Gabry J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. arXiv. 2015 Jul;.</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Keshvari</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>van den Berg</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>. <article-title>Probabilistic computation in human perception under variability in encoding precision</article-title>. <source>PLoS ONE</source>. <year>2012</year> <month>Jun</month>;<volume>7</volume>(<issue>6</issue>):<fpage>e40216</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0040216" xlink:type="simple">10.1371/journal.pone.0040216</ext-link></comment> <object-id pub-id-type="pmid">22768258</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kass</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Raftery</surname> <given-names>AE</given-names></name>. <article-title>Bayes factors</article-title>. <source>J Am Stat Assoc</source>. <year>1995</year>;<volume>90</volume>(<issue>430</issue>):<fpage>773</fpage>–<lpage>795</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1995.10476572" xlink:type="simple">10.1080/01621459.1995.10476572</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Acerbi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Vijayakumar</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>On the origins of suboptimality in human probabilistic inference</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year> <month>Jun</month>;<volume>10</volume>(<issue>6</issue>):<fpage>e1003661</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003661" xlink:type="simple">10.1371/journal.pcbi.1003661</ext-link></comment> <object-id pub-id-type="pmid">24945142</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Pitkow</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Not noisy, just wrong: The role of suboptimal inference in behavioral variability</article-title>. <source>Neuron</source>. <year>2012</year> <month>Apr</month>;<volume>74</volume>(<issue>1</issue>):<fpage>30</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2012.03.016" xlink:type="simple">10.1016/j.neuron.2012.03.016</ext-link></comment> <object-id pub-id-type="pmid">22500627</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref048">
<label>48</label>
<mixed-citation publication-type="other" xlink:type="simple">Orhan AE, Jacobs RA. Are performance limitations in visual short-term memory tasks due to capacity limitations of model mismatch? arXiv. 2014 Jul;.</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Navajas</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hindocha</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Foda</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Keramati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Bahrami</surname> <given-names>B</given-names></name>. <article-title>The idiosyncratic nature of confidence</article-title>. <source>Nat Hum Behav</source>. <year>2017</year> <month>Sep</month>;<volume>11</volume>(<issue>11</issue>):<fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Corthell</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>. <article-title>Choice certainty is informed by both evidence and decision time</article-title>. <source>Neuron</source>. <year>2014</year> <month>Dec</month>;<volume>84</volume>(<issue>6</issue>):<fpage>1329</fpage>–<lpage>1342</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.12.015" xlink:type="simple">10.1016/j.neuron.2014.12.015</ext-link></comment> <object-id pub-id-type="pmid">25521381</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wilimzig</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tsuchiya</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Fahle</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Einhäuser</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>. <article-title>Spatial attention increases performance but not subjective confidence in a discrimination task</article-title>. <source>Journal of Vision</source>. <year>2008</year> <month>May</month>;<volume>8</volume>(<issue>5</issue>):<fpage>7.1</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/8.5.7" xlink:type="simple">10.1167/8.5.7</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Palminteri</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wyart</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Koechlin</surname> <given-names>E</given-names></name>. <article-title>The importance of falsification in computational cognitive modeling</article-title>. <source>Trends Cogn Sci</source>. <year>2017</year> <month>Jun</month>;<volume>21</volume>(<issue>6</issue>):<fpage>425</fpage>–<lpage>433</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2017.03.011" xlink:type="simple">10.1016/j.tics.2017.03.011</ext-link></comment> <object-id pub-id-type="pmid">28476348</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zylberberg</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Barttfeld</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sigman</surname> <given-names>M</given-names></name>. <article-title>The construction of confidence in a perceptual decision</article-title>. <source>Frontiers in Integrative Neuroscience</source>. <year>2012</year>;<volume>6</volume>:<fpage>79</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnint.2012.00079" xlink:type="simple">10.3389/fnint.2012.00079</ext-link></comment> <object-id pub-id-type="pmid">23049504</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref054">
<label>54</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Vickers</surname> <given-names>DD</given-names></name>. <source>Decision processes in visual perception</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Academic Press</publisher-name>; <year>1979</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van den Berg</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Anandalingam</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Zylberberg</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>A common mechanism underlies changes of mind about decisions and confidence</article-title>. <source>eLife</source>. <year>2016</year> <month>Feb</month>;<day>5</day>:<fpage>e12192</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.12192" xlink:type="simple">10.7554/eLife.12192</ext-link></comment> <object-id pub-id-type="pmid">26829590</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Peters</surname> <given-names>MAK</given-names></name>, <name name-style="western"><surname>Thesen</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ko</surname> <given-names>YD</given-names></name>, <name name-style="western"><surname>Maniscalco</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Carlson</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Davidson</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Perceptual confidence neglects decision-incongruent evidence in the brain</article-title>. <source>Nat Hum Behav</source>. <year>2017</year>;<volume>1</volume>(<issue>0139</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41562-017-0139" xlink:type="simple">10.1038/s41562-017-0139</ext-link></comment> <object-id pub-id-type="pmid">29130070</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barthelmé</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mamassian</surname> <given-names>P</given-names></name>. <article-title>Evaluation of objective uncertainty in the visual system</article-title>. <source>PLoS Comput Biol</source>. <year>2009</year> <month>Sep</month>;<volume>5</volume>(<issue>9</issue>):<fpage>e1000504</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000504" xlink:type="simple">10.1371/journal.pcbi.1000504</ext-link></comment> <object-id pub-id-type="pmid">19750003</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref058">
<label>58</label>
<mixed-citation publication-type="other" xlink:type="simple">Barthelmé S, Mamassian P. Flexible mechanisms underlie the evaluation of visual confidence. In: PNAS; 2010.</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rahnev</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Maniscalco</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Luber</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lisanby</surname> <given-names>SH</given-names></name>. <article-title>Direct injection of noise to the visual cortex decreases accuracy but increases decision confidence</article-title>. <source>J Neurophysiol</source>. <year>2012</year> <month>Mar</month>;<volume>107</volume>(<issue>6</issue>):<fpage>1556</fpage>–<lpage>1563</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00985.2011" xlink:type="simple">10.1152/jn.00985.2011</ext-link></comment> <object-id pub-id-type="pmid">22170965</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bressler</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Silver</surname> <given-names>MA</given-names></name>. <source>Spatial attention improves reliability of fMRI retinotopic mapping signals in occipital and parietal cortex</source>. <year>2010</year> <month>Aug</month>;p. <fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pestilli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Carrasco</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Gardner</surname> <given-names>JL</given-names></name>. <article-title>Attentional enhancement via selection and pooling of early sensory responses in human visual cortex</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>72</volume>(<issue>5</issue>):<fpage>832</fpage>–<lpage>846</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.09.025" xlink:type="simple">10.1016/j.neuron.2011.09.025</ext-link></comment> <object-id pub-id-type="pmid">22153378</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wyart</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Nobre</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Summerfield</surname> <given-names>C</given-names></name>. <article-title>Dissociable prior influences of signal probability and relevance on visual contrast sensitivity</article-title>. <source>PNAS</source>. <year>2012</year>;<volume>109</volume>(<issue>9</issue>):<fpage>3593</fpage>–<lpage>3598</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1120118109" xlink:type="simple">10.1073/pnas.1120118109</ext-link></comment> <object-id pub-id-type="pmid">22331901</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Solovey</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Graney</surname> <given-names>GG</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>H</given-names></name>. <article-title>A decisional account of subjective inflation of visual perception at the periphery</article-title>. <source>Atten Percept Psychophys</source>. <year>2014</year>;<volume>77</volume>(<issue>1</issue>):<fpage>258</fpage>–<lpage>271</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13414-014-0769-1" xlink:type="simple">10.3758/s13414-014-0769-1</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Denison</surname> <given-names>RN</given-names></name>, <name name-style="western"><surname>Adler</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Carrasco</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>. <article-title>Humans flexibly incorporate attention-dependent uncertainty into perceptual decisions and confidence</article-title>. <source>PNAS</source>. <year>2018</year>;. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1717720115" xlink:type="simple">10.1073/pnas.1717720115</ext-link></comment> <object-id pub-id-type="pmid">30297430</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cortese</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Amano</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Koizumi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kawato</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>H</given-names></name>. <article-title>Multivoxel neurofeedback selectively modulates confidence without changing perceptual performance</article-title>. <source>Nat Commun</source>. <year>2016</year> <month>Dec</month>;<volume>7</volume>:<fpage>1</fpage>–<lpage>18</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms13669" xlink:type="simple">10.1038/ncomms13669</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref066">
<label>66</label>
<mixed-citation publication-type="other" xlink:type="simple">Koizumi A, Maniscalco B, Lau H. Does metacognitive awareness facilitate cognitive control? 2014 Jun;p. 1–22.</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref067">
<label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rounis</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Maniscalco</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Rothwell</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>E</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>H</given-names></name>. <article-title>Theta-burst transcranial magnetic stimulation to the prefrontal cortex impairs metacognitive visual awareness</article-title>. <source>Cogn Neurosci</source>. <year>2010</year>;<volume>1</volume>:<fpage>165</fpage>–<lpage>175</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/17588921003632529" xlink:type="simple">10.1080/17588921003632529</ext-link></comment> <object-id pub-id-type="pmid">24168333</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref068">
<label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Simons</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Peers</surname> <given-names>PV</given-names></name>, <name name-style="western"><surname>Mazuz</surname> <given-names>YS</given-names></name>, <name name-style="western"><surname>Berryhill</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Olson</surname> <given-names>IR</given-names></name>. <article-title>Dissociation between memory accuracy and memory confidence following bilateral parietal lesions</article-title>. <source>Cereb Cortex</source>. <year>2010</year>;<volume>20</volume>:<fpage>479</fpage>–<lpage>485</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhp116" xlink:type="simple">10.1093/cercor/bhp116</ext-link></comment> <object-id pub-id-type="pmid">19542474</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref069">
<label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lau</surname> <given-names>HC</given-names></name>, <name name-style="western"><surname>Passingham</surname> <given-names>RE</given-names></name>. <article-title>Relative blindsight in normal observers and the neural correlate of visual consciousness</article-title>. <source>PNAS</source>. <year>2006</year>;<volume>103</volume>:<fpage>18763</fpage>–<lpage>18768</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0607716103" xlink:type="simple">10.1073/pnas.0607716103</ext-link></comment> <object-id pub-id-type="pmid">17124173</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Koizumi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Maniscalco</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>H</given-names></name>. <article-title>Does perceptual confidence facilitate cognitive control?</article-title> <source>Atten Percept Psychophys</source>. <year>2015</year>;<volume>77</volume>:<fpage>1295</fpage>–<lpage>1306</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13414-015-0843-3" xlink:type="simple">10.3758/s13414-015-0843-3</ext-link></comment> <object-id pub-id-type="pmid">25737256</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref071">
<label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Samaha</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Barrett</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Sheldon</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Larocque</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Postle</surname> <given-names>BR</given-names></name>. <article-title>Dissociating perceptual confidence from discrimination accuracy reveals no influence of metacognitive awareness on working memory</article-title>. <source>Front Psychol</source>. <year>2016</year>;<volume>7</volume>(<issue>851</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2016.00851" xlink:type="simple">10.3389/fpsyg.2016.00851</ext-link></comment> <object-id pub-id-type="pmid">27375529</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref072">
<label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fleming</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Huijgen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Prefrontal contributions to metacognition in perceptual decision making</article-title>. <source>J Neurosci</source>. <year>2012</year> <month>May</month>;<volume>32</volume>(<issue>18</issue>):<fpage>6117</fpage>–<lpage>6125</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.6489-11.2012" xlink:type="simple">10.1523/JNEUROSCI.6489-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22553018</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref073">
<label>73</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Gigerenzer</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Hertwig</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Pachur</surname> <given-names>T</given-names></name>. <source>Heuristics: The foundations of adaptive behavior</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2011</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref074">
<label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Folke</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Jacobsen</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fleming</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>De Martino</surname> <given-names>B</given-names></name>. <article-title>Explicit representation of confidence informs future value-based decisions</article-title>. <source>Nat Hum Behav</source>. <year>2016</year> <month>Nov</month>;<volume>1</volume>(<issue>2</issue>).</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref075">
<label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bahrami</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Olsen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Bang</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Roepstorff</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rees</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Frith</surname> <given-names>C</given-names></name>. <article-title>What failure in collective decision-making tells us about metacognition</article-title>. <source>Phil Trans R Soc B</source>. <year>2012</year> <month>May</month>;<volume>367</volume>(<issue>1594</issue>):<fpage>1350</fpage>–<lpage>1365</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2011.0420" xlink:type="simple">10.1098/rstb.2011.0420</ext-link></comment> <object-id pub-id-type="pmid">22492752</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref076">
<label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bang</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Fusaroli</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Tylén</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Olsen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>JYF</given-names></name>, <etal>et al</etal>. <article-title>Does interaction matter? Testing whether a confidence heuristic can replace interaction in collective decision-making</article-title>. <source>Conscious Cogn</source>. <year>2014</year> <month>May</month>;<volume>26</volume>:<fpage>13</fpage>–<lpage>23</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.concog.2014.02.002" xlink:type="simple">10.1016/j.concog.2014.02.002</ext-link></comment> <object-id pub-id-type="pmid">24650632</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref077">
<label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lennie</surname> <given-names>P</given-names></name>. <article-title>The cost of cortical computation</article-title>. <source>Current Biology</source>. <year>2003</year> <month>Mar</month>;<volume>13</volume>(<issue>6</issue>):<fpage>493</fpage>–<lpage>497</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0960-9822(03)00135-0" xlink:type="simple">10.1016/S0960-9822(03)00135-0</ext-link></comment> <object-id pub-id-type="pmid">12646132</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref078">
<label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Attwell</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Laughlin</surname> <given-names>SB</given-names></name>. <article-title>An energy budget for signaling in the grey matter of the brain</article-title>. <source>J Cereb Blood Flow Metab</source>. <year>2001</year> <month>Oct</month>;<volume>21</volume>(<issue>10</issue>):<fpage>1133</fpage>–<lpage>1145</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1097/00004647-200110000-00001" xlink:type="simple">10.1097/00004647-200110000-00001</ext-link></comment> <object-id pub-id-type="pmid">11598490</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref079">
<label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Koulakov</surname> <given-names>AA</given-names></name>. <article-title>Maps in the brain: What can we learn from them?</article-title> <source>Annu Rev Neurosci</source>. <year>2004</year>;<volume>27</volume>:<fpage>369</fpage>–<lpage>392</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.neuro.27.070203.144226" xlink:type="simple">10.1146/annurev.neuro.27.070203.144226</ext-link></comment> <object-id pub-id-type="pmid">15217337</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref080">
<label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Clune</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mouret</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Lipson</surname> <given-names>H</given-names></name>. <article-title>The evolutionary origins of modularity</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source>. <year>2013</year> <month>Jan</month>;<volume>280</volume>:<fpage>20122863</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rspb.2012.2863" xlink:type="simple">10.1098/rspb.2012.2863</ext-link></comment> <object-id pub-id-type="pmid">23363632</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref081">
<label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Orhan</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>. <article-title>Efficient probabilistic inference in generic neural networks trained with non-probabilistic feedback</article-title>. <source>Nat Commun</source>. <year>2017</year> <month>Jul</month>;<volume>8</volume>(<issue>1</issue>):<fpage>138</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-017-00181-8" xlink:type="simple">10.1038/s41467-017-00181-8</ext-link></comment> <object-id pub-id-type="pmid">28743932</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref082">
<label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pelli</surname> <given-names>DG</given-names></name>. <article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title>. <source>Spat Vis</source>. <year>1997</year>;<volume>10</volume>(<issue>4</issue>):<fpage>437</fpage>–<lpage>442</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1163/156856897X00366" xlink:type="simple">10.1163/156856897X00366</ext-link></comment> <object-id pub-id-type="pmid">9176953</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref083">
<label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brainard</surname> <given-names>DH</given-names></name>. <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source>. <year>1997</year>;<volume>10</volume>(<issue>4</issue>):<fpage>433</fpage>–<lpage>436</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1163/156856897X00357" xlink:type="simple">10.1163/156856897X00357</ext-link></comment> <object-id pub-id-type="pmid">9176952</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref084">
<label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Naka</surname> <given-names>KI</given-names></name>, <name name-style="western"><surname>Rushton</surname> <given-names>WA</given-names></name>. <article-title>S-potentials from luminosity units in the retina of fish (Cyprinidae)</article-title>. <source>Journal of Physiology</source>. <year>1966</year> <month>Aug</month>;<volume>185</volume>(<issue>3</issue>):<fpage>587</fpage>–<lpage>599</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1113/jphysiol.1966.sp008001" xlink:type="simple">10.1113/jphysiol.1966.sp008001</ext-link></comment> <object-id pub-id-type="pmid">5918060</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref085">
<label>85</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>DiMattina</surname> <given-names>C</given-names></name>. <article-title>Comparing models of contrast gain using psychophysical experiments</article-title>. <source>Journal of Vision</source>. <year>2016</year> <month>Jul</month>;<volume>16</volume>(<issue>9</issue>):<fpage>1</fpage>–<lpage>18</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/16.9.1" xlink:type="simple">10.1167/16.9.1</ext-link></comment> <object-id pub-id-type="pmid">27380470</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref086">
<label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Bayesian inference with probabilistic population codes</article-title>. <source>Nat Neurosci</source>. <year>2006</year> <month>Nov</month>;<volume>9</volume>(<issue>11</issue>):<fpage>1432</fpage>–<lpage>1438</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1790" xlink:type="simple">10.1038/nn1790</ext-link></comment> <object-id pub-id-type="pmid">17057707</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref087">
<label>87</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Girshick</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Cardinal rules: Visual orientation perception reflects knowledge of environmental statistics</article-title>. <source>Nat Neurosci</source>. <year>2011</year> <month>Jun</month>;<volume>14</volume>(<issue>7</issue>):<fpage>926</fpage>–<lpage>932</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2831" xlink:type="simple">10.1038/nn.2831</ext-link></comment> <object-id pub-id-type="pmid">21642976</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref088">
<label>88</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Neal</surname> <given-names>RM</given-names></name>. <source>Slice sampling</source>. <year>2003</year>;<volume>31</volume>(<issue>3</issue>):<fpage>705</fpage>–<lpage>767</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref089">
<label>89</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hwang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Vehtari</surname> <given-names>A</given-names></name>. <article-title>Understanding predictive information criteria for Bayesian models</article-title>. <source>Statistics and Computing</source>. <year>2013</year>;<volume>24</volume>(<issue>6</issue>):<fpage>997</fpage>–<lpage>1016</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11222-013-9416-2" xlink:type="simple">10.1007/s11222-013-9416-2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref090">
<label>90</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Acerbi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Dokka</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Angelaki</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>. <article-title>Bayesian comparison of explicit and implicit causal inference strategies in multisensory heading perception</article-title>. <source>bioRxiv</source>. <year>2017</year> <month>Jun</month>;.</mixed-citation>
</ref>
<ref id="pcbi.1006572.ref091">
<label>91</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rigoux</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>. <article-title>Bayesian model selection for group studies—Revisited</article-title>. <source>NeuroImage</source>. <year>2014</year> <month>Jan</month>;<volume>84</volume>:<fpage>971</fpage>–<lpage>985</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2013.08.065" xlink:type="simple">10.1016/j.neuroimage.2013.08.065</ext-link></comment> <object-id pub-id-type="pmid">24018303</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref092">
<label>92</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>WD</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Moran</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Bayesian model selection for group studies</article-title>. <source>NeuroImage</source>. <year>2009</year> <month>Jul</month>;<volume>46</volume>(<issue>4</issue>):<fpage>1004</fpage>–<lpage>1017</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2009.03.025" xlink:type="simple">10.1016/j.neuroimage.2009.03.025</ext-link></comment> <object-id pub-id-type="pmid">19306932</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006572.ref093">
<label>93</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van den Berg</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Awh</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>. <article-title>Factorial comparison of working memory models</article-title>. <source>Psychol Rev</source>. <year>2014</year> <month>Jan</month>;<volume>121</volume>(<issue>1</issue>):<fpage>124</fpage>–<lpage>149</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0035234" xlink:type="simple">10.1037/a0035234</ext-link></comment> <object-id pub-id-type="pmid">24490791</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>