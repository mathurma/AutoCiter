<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="article-commentary" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="publisher">pbio</journal-id><journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id><journal-id journal-id-type="pmc">plosbiol</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Biology</journal-title></journal-title-group><issn pub-type="epub">1545-7885</issn><issn pub-type="ppub">1544-9173</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="doi">10.1371/journal.pbio.0030026</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Synopsis</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Computational Biology</subject>
          <subject>Biophysics</subject>
          <subject>Neuroscience</subject>
        </subj-group>
        <subj-group subj-group-type="System Taxonomy">
          <subject>Insects</subject>
        </subj-group>
      </article-categories><title-group><article-title>Computation Provides a Virtual Recording of Auditory Signaling</article-title><alt-title alt-title-type="running-head">Synopsis</alt-title></title-group><pub-date pub-type="ppub">
        <month>1</month>
        <year>2005</year>
      </pub-date><pub-date pub-type="epub">
        <day>4</day>
        <month>1</month>
        <year>2005</year>
      </pub-date><volume>3</volume><issue>1</issue><elocation-id>e26</elocation-id><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2005</copyright-year><copyright-holder>Public Library of Science</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><related-article page="e8" related-article-type="companion" vol="3" xlink:href="info:doi/10.1371/journal.pbio.0030008" xlink:title="research article" xlink:type="simple">
				<article-title>Disentangling Sub-Millisecond Processes within an Auditory Transduction Chain</article-title>
			</related-article></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title/>
      <p>A small rodent rustles through a field in the still night, making just enough noise to betray its location to a circling barn owl. A female frog sits on the bank of a pond amid a cacophony of courting bullfrogs, immune to the mating calls of all but her own species. Thanks to a sophisticated sensory processing system, animals can cut through a vast array of ambient auditory stimuli to extract meaningful information that allows them to tell where a sound came from, for example, or whether they should respond to a particular mating call.</p>
      <p>An acoustic stimulus arrives at the ear as sound energy in the form of air pressure fluctuations. The sound signal triggers oscillations in mechanical resonators such as the eardrum and hair sensilla. These oscillations convert sound energy into mechanical energy, opening ion channels in auditory receptor cells and producing electrical currents that change the neuron's membrane potential. This, in turn, produces the action potential that carries the sound signal to the brain. This multistep signal transduction process takes less than a millisecond, but exactly how it occurs at this time scale remains obscure. Direct measurements of the individual steps can't be made without destroying the mechanical structure; consequently, most measurements are taken downstream of the mechanical oscillations at locations like the auditory nerve. Likewise, the temporal resolution of most stimulus–response trials is far too imprecise to analyze processing at the sub-millisecond level.</p>
      <p>Given these experimental limitations, Tim Gollisch and Andreas Herz turned to computational methods and showed that it's possible to reveal the individual steps of complex signal processing by analyzing the output activity alone. Using grasshopper auditory receptors as models, the authors identified the individual signal-processing steps from eardrum vibrations to electrical potential within a sub-millisecond time frame and propose a model for auditory signaling.</p>
      <p>The crucial step in their study is the search for those sets of inputs (stimuli) that would yield a given <italic>fixed</italic> output (response). To get the parameters to describe the final output, the authors generated a sound stimulus (two short clicks) and recorded axon responses of receptor neurons in a grasshopper auditory nerve. From these recordings, they defined the fixed output as the probability of a receptor neuron firing a single action potential. They then asked how the various parameters, which were associated with different time scales, could produce the same predefined firing probability.<xref ref-type="fig" rid="pbio-0030026-g001"/></p>
      <fig id="pbio-0030026-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.0030026.g001</object-id>
        <caption>
          <title>A schematic representation of auditory signaling</title>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0030026.g001" xlink:type="simple"/>
      </fig>
      <p>By varying the stimulus parameters and comparing the obtained values within their mathematical framework—and making certain assumptions, for example, that the steps signal through a “feedforward” process—they could then tease out the individual processing steps that contribute to the desired output within the required time frame. With this approach, Gollisch and Herz disentangled individual steps of two consecutive integration processes—which they conclude are the mechanical resonance of the eardrum and the electrical integration of the receptor neuron—down to the microsecond level. Surprisingly, this fine temporal resolution is achieved even though the neuron's action potentials jitter by about one millisecond.</p>
      <p>Thus, using just the final output, this approach can extract the temporal details of the individual processes that contribute to the chain of auditory transduction events. While this method is best-suited for deconstructing unidirectional pathways, the authors suggest it could also help separate “feedforward” from feedback signaling components, especially when feedback is triggered by the final steps. But since many sensory systems share the same basic signal-processing steps, this method is likely applicable to a broad range of problems.</p>
    </sec>
  </body>
</article>