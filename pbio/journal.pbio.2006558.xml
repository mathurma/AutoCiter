<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id>
<journal-title-group>
<journal-title>PLOS Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pbio.2006558</article-id>
<article-id pub-id-type="publisher-id">pbio.2006558</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Speech signal processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Magnetoencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Magnetoencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Magnetoencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Acoustic signals</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Audio signal processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Motor cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Motor cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject><subj-group><subject>Visual signals</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject><subj-group><subject>Visual signals</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject><subj-group><subject>Visual signals</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Representational interactions during audiovisual speech entrainment: Redundancy in left posterior superior temporal gyrus and synergy in left motor cortex</article-title>
<alt-title alt-title-type="running-head">Neural decomposition of information in audiovisual speech</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7527-8280</contrib-id>
<name name-style="western">
<surname>Park</surname>
<given-names>Hyojin</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ince</surname>
<given-names>Robin A. A.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Schyns</surname>
<given-names>Philippe G.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Thut</surname>
<given-names>Gregor</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Gross</surname>
<given-names>Joachim</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>School of Psychology, Centre for Human Brain Health (CHBH), University of Birmingham, Birmingham, United Kingdom</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Institute of Neuroscience and Psychology, University of Glasgow, Glasgow, United Kingdom</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Institute for Biomagnetism and Biosignalanalysis, University of Muenster, Muenster, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bizley</surname>
<given-names>Jennifer</given-names>
</name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University College London, United Kingdom of Great Britain and Northern Ireland</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">h.park@bham.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>6</day>
<month>8</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>8</month>
<year>2018</year>
</pub-date>
<volume>16</volume>
<issue>8</issue>
<elocation-id>e2006558</elocation-id>
<history>
<date date-type="received">
<day>4</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>24</day>
<month>7</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Park et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pbio.2006558"/>
<abstract>
<p>Integration of multimodal sensory information is fundamental to many aspects of human behavior, but the neural mechanisms underlying these processes remain mysterious. For example, during face-to-face communication, we know that the brain integrates dynamic auditory and visual inputs, but we do not yet understand where and how such integration mechanisms support speech comprehension. Here, we quantify representational interactions between dynamic audio and visual speech signals and show that different brain regions exhibit different types of representational interaction. With a novel information theoretic measure, we found that theta (3–7 Hz) oscillations in the posterior superior temporal gyrus/sulcus (pSTG/S) represent auditory and visual inputs redundantly (i.e., represent common features of the two), whereas the same oscillations in left motor and inferior temporal cortex represent the inputs synergistically (i.e., the instantaneous relationship between audio and visual inputs is also represented). Importantly, redundant coding in the left pSTG/S and synergistic coding in the left motor cortex predict behavior—i.e., speech comprehension performance. Our findings therefore demonstrate that processes classically described as integration can have different statistical properties and may reflect distinct mechanisms that occur in different brain regions to support audiovisual speech comprehension.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Combining different sources of information is fundamental to many aspects of behavior, from our ability to pick up a ringing mobile phone to communicating with a friend in a busy environment. Here, we have studied the integration of auditory and visual speech information. Our work demonstrates that integration relies upon two different representational interactions. One system conveys redundant information by representing information that is common to both auditory and visual modalities. The other system, which is supported by a different brain area, represents synergistic information by conveying greater information than the linear summation of individual auditory and visual information. Further, we show that these mechanisms are related to behavioral performance. This novel insight opens new ways to enhance our understanding of the mechanisms underlying multi-modal information integration, a fundamental aspect of brain function. These fresh insights have been achieved by applying to brain imaging data a recently developed methodology called the partial information decomposition. This methodology also provides a novel and principled way to quantify the interactions between representations of multiple stimulus features in the brain.</p>
</abstract>
<funding-group>
<funding-statement>Wellcome Trust Senior Investigator Award <ext-link ext-link-type="uri" xlink:href="https://wellcome.ac.uk/" xlink:type="simple">https://wellcome.ac.uk/</ext-link> (grant number 098433) to JG. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Wellcome Trust Senior Investigator Award <ext-link ext-link-type="uri" xlink:href="https://wellcome.ac.uk/" xlink:type="simple">https://wellcome.ac.uk/</ext-link> (grant number 098434) to GT. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Wellcome Trust Senior Investigator Award <ext-link ext-link-type="uri" xlink:href="https://wellcome.ac.uk/" xlink:type="simple">https://wellcome.ac.uk/</ext-link> (grant number 107802) to PGS. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="0"/>
<page-count count="26"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-08-16</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data files are available from the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/hpcj8/" xlink:type="simple">https://osf.io/hpcj8/</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>While engaged in a conversation, we effortlessly integrate auditory and visual speech information into a unified perception. Such integration of multisensory information is a key aspect of audiovisual speech processing that has been extensively studied [<xref ref-type="bibr" rid="pbio.2006558.ref001">1</xref>–<xref ref-type="bibr" rid="pbio.2006558.ref004">4</xref>]. Studies of multisensory integration have demonstrated that, in face-to-face conversation, especially in adverse conditions, observing lip movements of the speaker can improve speech comprehension [<xref ref-type="bibr" rid="pbio.2006558.ref004">4</xref>–<xref ref-type="bibr" rid="pbio.2006558.ref007">7</xref>]. In fact, some people’s ability to perform lip reading demonstrates that lip movements during speech contain considerable information to understand speech without corresponding auditory information [<xref ref-type="bibr" rid="pbio.2006558.ref001">1</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref008">8</xref>], even though auditory information is essential to understand speech accurately [<xref ref-type="bibr" rid="pbio.2006558.ref009">9</xref>].</p>
<p>Turning to the brain, we know that specific regions are involved in audiovisual integration. Specifically, the superior temporal gyrus/sulcus (STG/S) responds to integration of auditory and visual stimuli, and its disruption leads to reduced McGurk fusion [<xref ref-type="bibr" rid="pbio.2006558.ref0010">10</xref>–<xref ref-type="bibr" rid="pbio.2006558.ref014">14</xref>]. However, these classic studies present two shortcomings. First, their experimental designs typically contrasted two conditions: unisensory (i.e., audio or visual cues) and multisensory (congruent or incongruent audio and visual cues). However, such contrast does not dissociate effects of integration per se from those arising from differences in stimulation complexity (i.e., one or two sources) that could modulate attention, cognitive load, and even arousal. A second shortcoming is that previous studies typically investigated (changes of) regional activation and not information integration between audiovisual stimuli and brain signals. Here, we address these two shortcomings and study the specific mechanisms of audiovisual integration from brain oscillations. We used a novel methodology (speech-brain entrainment) and novel information theoretic measures (the partial information decomposition [PID] [<xref ref-type="bibr" rid="pbio.2006558.ref015">15</xref>]) to quantify the interactions between audiovisual stimuli and dynamic brain signals.</p>
<p>Our methodology of speech-brain entrainment builds on recent studies suggesting that rhythmic components in brain activity that are temporally aligned to salient features in speech—most notably the syllable rate [<xref ref-type="bibr" rid="pbio.2006558.ref005">5</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref006">6</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref016">16</xref>–<xref ref-type="bibr" rid="pbio.2006558.ref018">18</xref>]—facilitate processing of both the auditory and visual speech inputs. The main advantage of speech-brain entrainment is that it replaces unspecific measures of activation with measures that directly quantify the coupling between the components of continuous speech (e.g., syllable rate) and frequency-specific brain activity, thereby tapping more directly into the brain mechanisms of speech segmentation and coding [<xref ref-type="bibr" rid="pbio.2006558.ref017">17</xref>].</p>
<p>In the present study, we used a recently developed information theoretic framework called PID (see <xref ref-type="fig" rid="pbio.2006558.g001">Fig 1A</xref> and <xref ref-type="sec" rid="sec009">Materials and methods</xref> for details) [<xref ref-type="bibr" rid="pbio.2006558.ref015">15</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref019">19</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref020">20</xref>]. We consider a three-variable system with a target variable M (here magnetoencephalography [MEG]) and two predictor variables A and V (here audio and visual speech signals), with both A and V conveying information about the target M. Conceptually, the redundancy is related to whether the information conveyed by A and V is the same or different. If the variables are fully redundant, then this means either alone is enough to convey all the information about M (i.e., obtain an optimal prediction of M), and adding observation of the second modality has no benefit for predicting the MEG signal M. The concept of synergy is related to whether A and V convey more information when observed simultaneously, so the prediction of M is enhanced by simultaneous observation of the values of A and V [<xref ref-type="bibr" rid="pbio.2006558.ref015">15</xref>]. This means M also represents the instantaneous relationship between A and V. For example, if M is given by the difference between A and V at each sample, then observing either A or V alone tells little about the value of M, but observing them together completely determines it. The PID provides a methodology to rigorously quantify both redundancy and synergy, as well as the unique information in each modality. Unique information is the prediction of the MEG that can be obtained from observing A alone but that is not redundantly available from observing V.</p>
<fig id="pbio.2006558.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006558.g001</object-id>
<label>Fig 1</label>
<caption>
<title>PID of audiovisual speech processing in the brain.</title>
<p>(A) Information structure of multisensory audio and visual inputs (sound envelope and lip movement signal) predicting brain response (MEG signal). Ellipses indicate total mutual information I(MEG;A,V), mutual information I(MEG;A), and mutual information I(MEG;V); and the four distinct regions indicate unique information of auditory speech I<sub>uni</sub>(MEG;A), unique information of visual speech I<sub>uni</sub>(MEG;V), redundancy I<sub>red</sub>(MEG;A,V), and synergy I<sub>syn</sub>(MEG;A,V). See Materials and methods for details. See also Ince [<xref ref-type="bibr" rid="pbio.2006558.ref015">15</xref>], Barrett [<xref ref-type="bibr" rid="pbio.2006558.ref021">21</xref>], and Wibral and colleagues [<xref ref-type="bibr" rid="pbio.2006558.ref022">22</xref>] for general aspects of the PID analysis. (B) Unique information of visual speech and auditory speech was compared to determine the dominant modality in different areas (see <xref ref-type="supplementary-material" rid="pbio.2006558.s001">S1 Fig</xref> for more details). Stronger unique information for auditory speech was found in bilateral auditory, temporal, and inferior frontal areas, and stronger unique information for visual speech was found in bilateral visual cortex (<italic>P</italic> &lt; 0.05, FDR corrected). The underlying data for this figure are available from the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/hpcj8/" xlink:type="simple">https://osf.io/hpcj8/</ext-link>). <italic>Figure modified from [<xref ref-type="bibr" rid="pbio.2006558.ref015">15</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref021">21</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref022">22</xref>] to illustrate the relationship between stimuli in the present study.</italic> FDR, false discovery rate; MEG, magnetoencephalography; PID, partial information decomposition.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006558.g001" xlink:type="simple"/>
</fig>
<p>The PID framework therefore addresses a perennial question in multisensory processing: the extent to which each sensory modality contributes uniquely to sensory representation in the brain versus how the representation of different modalities interact (e.g., audio and visual). The PID provides a principled approach to investigate different cross-modal representational interactions (redundant and synergistic) in the human brain during naturalistic audiovisual speech processing—that is, to understand how neural representations of dynamic auditory and visual speech signals interact in the brain to form a unified perception.</p>
<p>Specifically, we recorded brain activity using MEG while participants attended to continuous audiovisual speech to entrain brain activity. We applied the PID to reveal where and how speech-entrained brain activity in different regions reflects different types of auditory and visual integration. In the first experimental condition, we used naturalistic audiovisual speech for which attention to visual speech was not critical (“All congruent” condition). In the second condition, we added a second interfering auditory stimulus that was incongruent to the congruent audiovisual stimuli (“AV congruent” condition), requiring attention to visual speech to suppress the competing additional incongruent auditory input. In the third condition, both auditory stimuli were not congruent to visual stimulus (“All incongruent”). This allows us to see how the congruence of audiovisual stimuli changes integration. We contrasted measures of redundant and synergistic cross-modal interactions between the conditions to reveal differential effects of attention and congruence on multisensory integration mechanisms and behavioral performance.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>We first studied PID in an “All congruent” condition (diotic presentation of speech with matching video) to understand multisensory representational interactions in the brain during processing of natural audiovisual speech. We used mutual information (MI) to quantify the overall dependence between the full multisensory dynamic stimulus time course (broadband speech amplitude envelope and lip area for auditory and visual modalities, respectively) and the recorded brain activity. To determine the dominant modality in each brain area, we statistically compared the auditory unique information to visual unique information across subjects. Note that here, auditory unique information is unique in the context of our PID analysis. Specifically, it quantifies information about the MEG response, which is available only from the auditory speech envelope and not from the visual lip area. The same is true for unique visual information. The analysis revealed stronger visual entrainment in bilateral visual cortex and stronger auditory entrainment in bilateral auditory, temporal, and inferior frontal areas (paired two-sided <italic>t</italic> test, df: 43, <italic>P</italic> &lt; 0.05, false discovery rate [FDR] corrected; <xref ref-type="fig" rid="pbio.2006558.g001">Fig 1B</xref>).</p>
<p>To identify a frequency band at which auditory and visual speech signals show significant dependencies, we computed MI between both signals and compared it to MI between nonmatching auditory and visual speech signals for frequencies from 0 to 20 Hz. Here, we used all the talks in the present study to delineate the spectral profile of dependencies between matching or nonmatching auditory and visual speech signals. As expected, only matching audiovisual speech signals show significant MI peaking at 5 Hz (<xref ref-type="fig" rid="pbio.2006558.g002">Fig 2A</xref>), consistent with previous results based on coherence measure (see Fig 2C in [<xref ref-type="bibr" rid="pbio.2006558.ref005">5</xref>]). Based on this, we focus our further analysis on the 3–7 Hz frequency band (5 ± 2 Hz) in the following analyses (Figs <xref ref-type="fig" rid="pbio.2006558.g003">3</xref>–<xref ref-type="fig" rid="pbio.2006558.g005">5</xref>). This frequency range is known to correspond to the syllable rate in continuous speech [<xref ref-type="bibr" rid="pbio.2006558.ref016">16</xref>] and within which amplitude envelope of speech is known to reliably entrain auditory brain activity [<xref ref-type="bibr" rid="pbio.2006558.ref018">18</xref>–<xref ref-type="bibr" rid="pbio.2006558.ref023">23</xref>].</p>
<fig id="pbio.2006558.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006558.g002</object-id>
<label>Fig 2</label>
<caption>
<title>MI between auditory and visual speech signals.</title>
<p>(A) To investigate PID in “AV congruent” condition, first MI between auditory speech and visual speech signals was computed separately for matching and nonmatching signals. MI for matching auditory-visual speech signals shows a peak around 5 Hz (red line), whereas MI for nonmatching signals is flat (blue line). The underlying data for this figure are available from the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/hpcj8/" xlink:type="simple">https://osf.io/hpcj8/</ext-link>). (B) Analysis of PID is shown for “AV congruent” condition in which both matching and nonmatching auditory-visual speech signals are present on the same brain response (MEG data). Two external speech signals (auditory speech envelope and lip movement signal) and brain signals were used in the PID computation. Each signal was band-pass filtered, followed by Hilbert transform. MEG, magnetoencephalography; MI, mutual information; PID, partial information decomposition.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006558.g002" xlink:type="simple"/>
</fig>
<fig id="pbio.2006558.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006558.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Redundancy and synergy revealed by PID for attention-modulated speech processing (“AV congruent” condition).</title>
<p>Redundant and synergistic information of matching audiovisual speech signals in the brain compared to nonmatching signals are shown. Each map (matching or nonmatching in each information map) was firstly yielded to regression analysis using speech comprehension and then transformed to standard Z maps and subtracted. (A) Redundant information is localized in left auditory and superior and middle temporal cortices. (B) Synergistic information is found in left motor and bilateral visual areas (<italic>Z</italic>-difference map at <italic>P</italic> &lt; 0.005). The underlying data for this figure are available from the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/hpcj8/" xlink:type="simple">https://osf.io/hpcj8/</ext-link>). PID, partial information decomposition.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006558.g003" xlink:type="simple"/>
</fig>
<fig id="pbio.2006558.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006558.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Redundancy and synergy in congruence effect.</title>
<p>Comparison between conditions of matching versus nonmatching audiovisual speech signals in “AV congruent” condition entails both attention and congruence effects. To separate this effect, we additionally analyzed contrast for congruence (“AV congruent” &gt; “All incongruent”) first. (A) Redundancy for congruence effect is observed in left inferior frontal region and pSTG/S and right posterior middle temporal cortex (<italic>Z</italic>-difference map at <italic>P</italic> &lt; 0.005). (B) Synergistic information for congruence effect is found in superior part of somatosensory and parietal cortices in left hemisphere (<italic>Z</italic>-difference map at <italic>P</italic> &lt; 0.005). The underlying data for this figure are available from the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/hpcj8/" xlink:type="simple">https://osf.io/hpcj8/</ext-link>). pSTG/S, posterior superior temporal gyrus/sulcus.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006558.g004" xlink:type="simple"/>
</fig>
<fig id="pbio.2006558.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006558.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Redundancy and synergy in attention effect.</title>
<p>Redundancy and synergy in attention (“AV congruent” &gt; “All congruent”) are analyzed. Further, to explore whether this effect is specific to “AV congruent” condition (not because of decreased information in “All congruent” condition), we extracted raw values of each information map at the local maximum voxel and correlated it with speech comprehension accuracy across subjects. (A) Redundancy for attention effect was observed in left auditory and temporal (superior and middle temporal cortices and pSTG/S) areas and right inferior frontal and superior temporal cortex (<italic>Z</italic>-difference map at <italic>P</italic> &lt; 0.005). (B) Synergistic information for attention effect was localized in left motor cortex, inferior temporal cortex, and parieto-occipital areas (<italic>Z</italic>-difference map at <italic>P</italic> &lt; 0.005). (C) Redundancy at the left posterior superior temporal region in “AV congruent” condition was found to be positively correlated with speech comprehension accuracy (<italic>R</italic> = 0.43, <italic>P</italic> = 0.003). However, this redundant representation was not found for left motor cortex where synergistic information was represented (<italic>R</italic> = 0.21, <italic>P</italic> = 0.18). (D) Synergy at the left motor cortex in “AV congruent” condition was also positively correlated with speech comprehension accuracy across subjects (<italic>R</italic> = 0.34, <italic>P</italic> = 0.02). Likewise, synergistic representation was not found to be related to comprehension in the left posterior superior temporal region where redundant information was represented (<italic>R</italic> = 0.04, <italic>P</italic> = 0.81). This finding suggests that redundant information in the left posterior superior temporal region and synergistic information in the left motor cortex in a challenging audiovisual speech condition support better speech comprehension. The underlying data for this figure are available from the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/hpcj8/" xlink:type="simple">https://osf.io/hpcj8/</ext-link>). N.S., not significant; pSTG/S, posterior superior temporal gyrus/sulcus.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006558.g005" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>Redundancy in left posterior superior temporal gyrus/sulcus (pSTG/S) and synergy in left motor cortex</title>
<p>Next, we investigated how multimodal representational interactions are modulated by attention and congruence in continuous audiovisual speech. Here, we focus on an “AV congruent” condition in which a congruent audiovisual stimulus pair is presented monaurally together with an interfering nonmatching auditory speech stimulus to the other ear (<xref ref-type="fig" rid="pbio.2006558.g002">Fig 2B</xref>). This condition is of particular interest because visual speech (lip movement) is used to disambiguate the two competing auditory speech signals. Furthermore, it is ideally suited for our analysis because we can directly contrast representational interactions quantified with the PID in matching and nonmatching audiovisual speech signals in the same data set (see <xref ref-type="fig" rid="pbio.2006558.g002">Fig 2B</xref>).</p>
<p><xref ref-type="fig" rid="pbio.2006558.g003">Fig 3</xref> shows corrected group statistics for the contrast of matching and nonmatching audiovisual speech in the “AV congruent” condition. Redundant information is significantly stronger in left auditory and superior and middle temporal cortices (<xref ref-type="fig" rid="pbio.2006558.g003">Fig 3A</xref>; Z-difference map at <italic>P</italic> &lt; 0.005) for matching compared to nonmatching audiovisual speech. In contrast, significantly higher synergistic information for matching compared to nonmatching audiovisual speech is found in left motor and bilateral visual areas spreading along dorsal and ventral stream regions of speech processing [<xref ref-type="bibr" rid="pbio.2006558.ref024">24</xref>] (<xref ref-type="fig" rid="pbio.2006558.g003">Fig 3B</xref>; Z-difference map at <italic>P</italic> &lt; 0.005). Next, we tested attention and congruence effects separately because the contrast of matching versus nonmatching audiovisual speech confounds both effects.</p>
<p>First, the congruence effect (“AV congruent” &gt; “All incongruent”) shows higher redundant information in left inferior frontal region (BA 44/45) and posterior superior temporal gyrus and right posterior middle temporal cortex (<xref ref-type="fig" rid="pbio.2006558.g004">Fig 4A</xref>; Z-difference map at <italic>P</italic> &lt; 0.005) and higher synergistic information in superior part of somatosensory and parietal cortices in left hemisphere (<xref ref-type="fig" rid="pbio.2006558.g004">Fig 4B</xref>; Z-difference map at <italic>P</italic> &lt; 0.005).</p>
<p>The attention effect (“AV congruent” &gt; “All congruent”) shows higher redundant information in left auditory and temporal (superior, middle, and inferior temporal cortices and pSTG/S) areas and right inferior frontal and superior temporal cortex (<xref ref-type="fig" rid="pbio.2006558.g005">Fig 5A</xref>; <italic>Z</italic>-difference map at <italic>P</italic> &lt; 0.005). Higher synergistic information was localized in left motor cortex, inferior temporal cortex, and parieto-occipital areas (<xref ref-type="fig" rid="pbio.2006558.g005">Fig 5B</xref>; <italic>Z</italic>-difference map at <italic>P</italic> &lt; 0.005).</p>
<p>In summary, theta-band activity in left pSTG/S represents redundant information about audiovisual speech significantly more strongly in experimental conditions with higher attention and congruence. In contrast, synergistic information in the left motor cortex is more prominent in conditions requiring increased attention. Therefore, the increased relevance of visual speech in the “AV congruent” condition leads to increased redundancy in left pSTG/S and increased synergy in left motor cortex. This differential effect on representational interactions may reflect different integration mechanisms operating in the different areas. For detailed local maps of interaction between predictors (auditory and visual speech signals) and target (MEG response), see <xref ref-type="supplementary-material" rid="pbio.2006558.s003">S3 Fig</xref>.</p>
</sec>
<sec id="sec004">
<title>Performance scales with redundancy in left pSTG/S and synergy in left motor cortex</title>
<p>Next, we investigated if the differential pattern of redundancy and synergy is of behavioral relevance in our most important condition—"AV congruent"—in which visual speech is particularly informative. To this end, we extracted raw values of redundancy for the location showing strongest redundancy in the left pSTG/S in <xref ref-type="fig" rid="pbio.2006558.g005">Fig 5A</xref> and synergy for the location showing strongest synergy in the left motor cortex in <xref ref-type="fig" rid="pbio.2006558.g005">Fig 5B</xref> for “AV congruent” condition. After normalization with surrogate data (see <xref ref-type="sec" rid="sec009">Materials and methods</xref> section), we computed correlation with performance measures (comprehension accuracy) across participants. Both redundancy in left pSTG/S (<italic>R</italic> = 0.43, <italic>P</italic> = 0.003; <xref ref-type="fig" rid="pbio.2006558.g005">Fig 5C</xref>) and synergy in left motor cortex (<italic>R</italic> = 0.34, <italic>P</italic> = 0.02; <xref ref-type="fig" rid="pbio.2006558.g005">Fig 5D</xref>) are significantly correlated with comprehension accuracy. These results suggest that the redundancy in left pSTG/S and synergy in left motor cortex under challenging conditions (i.e., in the presence of distracting speech) are related to perceptual mechanisms underlying comprehension.</p>
</sec>
</sec>
<sec id="sec005" sec-type="conclusions">
<title>Discussion</title>
<p>In this study, we investigated how multisensory audiovisual speech rhythms are represented in the brain and how they are integrated for speech comprehension. We propose to study multisensory integration using information theory for the following reasons: First, by directly quantifying dynamic encoded representation of speech stimuli, our results are more clearly relevant to information-processing mechanisms than are differences in activation between blocks of stimulus conditions. Second, cross-modal interactions can be quantified directly within a naturalistic multimodal presentation without requiring contrasts between multimodal and unimodal conditions (e.g., AV &gt; A + V). Third, the PID provides measures of representational interactions that address questions that are not available with other statistical approaches (particularly synergy; <xref ref-type="fig" rid="pbio.2006558.g001">Fig 1A</xref>) [<xref ref-type="bibr" rid="pbio.2006558.ref015">15</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref021">21</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref022">22</xref>].</p>
<p>We found that left posterior superior temporal region represents speech information that is common to both auditory and visual modalities (redundant), while left motor cortex represents information about the instantaneous relationship between audio and visual speech (synergistic). These results are obtained from low-frequency theta rhythm (3–7 Hz) signals corresponding to syllable rate. Importantly, redundancy in pSTG/S and synergy in left motor cortex predict behavioral performance—speech comprehension accuracy—across participants.</p>
<p>A critical hallmark of multisensory integration in general, and audiovisual integration in particular, is the behavioral advantage conveyed by both stimulus modalities as compared to each single modality. Here, we have shown that this process may rely on at least two different mechanisms in two different brain areas, reflected in different representational interaction profiles revealed with information theoretic synergy and redundancy.</p>
<sec id="sec006">
<title>What do redundancy and synergy mean? Linking to audiovisual integration in functional magnetic resonance imaging (fMRI) studies</title>
<p>In fMRI studies, audiovisual speech integration has been studied using experimental conditions that manipulate the stimulus modalities presented (e.g., [<xref ref-type="bibr" rid="pbio.2006558.ref013">13</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref025">25</xref>]). Changes in blood oxygen level–dependent (BOLD) responses elicited by congruent audiovisual stimuli (AV) have been compared to auditory-only (AO), visual-only (VO), their sum (AO + VO), or their conjunction (AO ∩ VO). Greater activation for the congruent audiovisual condition (AV) compared to others has been interpreted as a signature of audiovisual speech integration. Comparison to auditory-only (AO) activation or visual-only (VO) activation has been regarded as a less conservative criterion for integration, since even if auditory and visual stimuli caused independent BOLD activity that combined linearly, this contrast would reveal an effect. To address this, comparison to the summation of the unimodal activations (AO + VO) has been used to demonstrate supra-additive activation, which is more suggestive of a cross-modal integration process. Rather than overall activation while the stimulus is present, the information theoretic approach instead focuses on quantifying the degree to which the changing speech time course is encoded or represented in the neural signals. The MI calculated here is an effect size for the ongoing entrainment of the MEG time course by the time varying speech—i.e., it quantifies the strength of the representation of dynamic audiovisual speech in the neural activity. While the basic expression on which our redundancy measure is based (Materials and methods, <xref ref-type="disp-formula" rid="pbio.2006558.e001">Eq 1</xref>) looks similar to an activation contrast (e.g., sum versus conjunction), it is important to keep in mind that this is about the strength of the dynamic low-frequency entrainment in each modality, not simply overall activation contrasts between conditions as in the classic fMRI approach.</p>
<p>The PID can quantify the representational interactions between multiple sensory signals and the associated brain response in a single experimental condition in which both sensory modalities are simultaneously present. In the PID framework, the unique contributions of a single (e.g., auditory) sensory modality to brain activity are directly quantified when both are present, instead of relying on the statistical contrast between modalities presented independently. Furthermore, the PID method allows the quantification of both redundant and synergistic interactions. In the context of audiovisual integration, both types of interaction can be seen as integration effects. Redundant information refers to quantification of overlapping information content of the predictor variables (auditory and visual speech signals), and synergistic information refers to additional information gained from simultaneous observation of two predictor variables compared to observation of one. Both of these types of interaction quantify multimodal stimulus representation that cannot be uniquely attributed to one of the two modalities. Redundant representation cannot be uniquely attributed, since that part of the brain response could be predicted from either of the stimulus modalities. Synergistic representation cannot be uniquely attributed, since that part of the brain response could only be predicted from simultaneous observation of both modalities and not from either one alone.</p>
<p>Note that these statistical interactions are quite different from interaction terms in a linear regression analysis, which would indicate the (linear) functional relationship between one stimulus modality and the MEG response is modulated by the value of the other stimulus modality. MI is an effect size that can be interpreted, because of its symmetry, from both an encoding and decoding perspective. From an encoding perspective, MI is a measure of how much an observer’s predictive model for possible MEG activity values changes when a specific auditory speech value is observed 100 ms prior. It quantifies the improvement in predictive performance of such an observer when making an optimal guess based on the auditory speech signal they see, over the guess they would make based on overall MEG activity without observing a stimulus value. From this perspective, redundancy quantifies the overlapping or common predictions that would be made by two Bayesian optimal observers, one predicting based on the auditory signal and the other the visual. Synergy is an increase in predictive power when both signals are obtained simultaneously. That is, it is possible to obtain a better prediction of the MEG with simultaneous knowledge of the specific combination of A and V observed than it is from combining only the predictions of the previous two unimodal observers. From considering the local plots (i.e., the values that are summed to obtain the final expectation value) in <xref ref-type="supplementary-material" rid="pbio.2006558.s003">S3 Fig</xref>, we can see that a better prediction of the MEG in left motor cortex is made from the joint multimodal input in the case in which the MEG signal is high (above median), and the auditory and visual signals are in opposite ranges (e.g., high/low or low/high).</p>
<p>Existing techniques like representational similarity analysis (RSA) [<xref ref-type="bibr" rid="pbio.2006558.ref026">26</xref>] and cross-decoding [<xref ref-type="bibr" rid="pbio.2006558.ref027">27</xref>] can address the same conceptual problem as redundancy but from the angle of similarity of representations on average rather than specific overlapping Bayesian predictive information content within individual samples, which the information theoretic framework provides. Techniques exploiting decoding in different conditions can show the degree to which multimodal representations are similar to unimodal representations [<xref ref-type="bibr" rid="pbio.2006558.ref028">28</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref029">29</xref>] and whether there is an improvement in performance when the representation is learned in the multimodal condition. However, PID is explicitly a trivariate analysis considering two explicit quantified stimulus features and the brain signal. The information theoretic definition of synergy means there is enhanced prediction of neural responses from simultaneous multimodal stimuli compared to independent predictions combined from each modality (but still presented together). This differs from typical multimodal versus unimodal contrasts, even those involving decoding, because it explicitly considers the effect of continuous naturalistic variation in both stimulus modalities on the recorded signal.</p>
</sec>
<sec id="sec007">
<title>Left posterior superior temporal region extracts common features from auditory and visual speech rhythms</title>
<p>Posterior superior temporal region (pSTG/S) has been implicated in audiovisual speech integration area by functional [<xref ref-type="bibr" rid="pbio.2006558.ref030">30</xref>–<xref ref-type="bibr" rid="pbio.2006558.ref032">32</xref>] and anatomical [<xref ref-type="bibr" rid="pbio.2006558.ref033">33</xref>] neuroimaging. A typical finding in fMRI studies is that pSTG/S shows stronger activation for audiovisual (AV) compared to auditory-only (AO) and/or visual-only (VO) conditions. This was confirmed by a combined fMRI-transcranial magnetic stimulation (TMS) study in which the likelihood of McGurk fusion was reduced when TMS was applied individually to fMRI-localized posterior superior temporal sulcus (pSTS), suggesting a critical role of pSTS in auditory-visual integration [<xref ref-type="bibr" rid="pbio.2006558.ref014">14</xref>].</p>
<p>The redundant information in the same left superior temporal region in this study matches this notion that this region processes shared information from both modalities. We found this region not only in the congruence effect (“AV congruent” &gt; “All incongruent”; <xref ref-type="fig" rid="pbio.2006558.g004">Fig 4A</xref>) but also in the attention effect (“AV congruent” &gt; “All congruent”; <xref ref-type="fig" rid="pbio.2006558.g005">Fig 5A</xref>).</p>
</sec>
<sec id="sec008">
<title>Left motor cortex activity reflects synergistic information in audiovisual speech processing</title>
<p>We found the left motor cortex shows increased synergy for the matching versus nonmatching audio stimuli of “AV congruent” condition (<xref ref-type="fig" rid="pbio.2006558.g003">Fig 3B</xref>). However, further analysis optimized for effects of attention and congruence revealed slightly different areas—with the area that shows strongest synergy change with attention (<xref ref-type="fig" rid="pbio.2006558.g005">Fig 5B</xref>; BA6) located more lateral and anterior compared to the area identified in the congruence (<xref ref-type="fig" rid="pbio.2006558.g004">Fig 4B</xref>). Previous studies have demonstrated increased phase locking of left motor cortex activity to frequency-tagged stimuli during auditory spatial attention [<xref ref-type="bibr" rid="pbio.2006558.ref034">34</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref035">35</xref>]. We extend these findings by demonstrating attention-mediated synergistic interactions of auditory and visual representations in left motor cortex.</p>
<p>The motor region in the attention contrast is consistent with the area in our previous study that showed entrainment to lip movements during continuous speech that correlated with speech comprehension [<xref ref-type="bibr" rid="pbio.2006558.ref005">5</xref>]. In another study, we identified this area as the source of top-down modulation of activity in the left auditory cortex [<xref ref-type="bibr" rid="pbio.2006558.ref023">23</xref>]. The definition of synergistic information in our context refers to more information gained from the simultaneous observation of auditory and visual speech compared to the observation of each alone. When it comes to the attention effect (“AV congruent” &gt; “All congruent”), “AV congruent” condition requires paying more attention to auditory and visual speech than the “All congruent” condition does, even though the speech signals to be attended match the visual stimulus in both conditions. Thus, this synergy effect in the left motor cortex can be explained by a net attention effect at the same level of stimulus congruence. This effect is likely driven by stronger attention to visual speech, which is informative for the disambiguation of the two competing auditory speech streams [<xref ref-type="bibr" rid="pbio.2006558.ref005">5</xref>]. This notion is plausible because it is supported by directional information analysis that shows that the left motor cortex better predicts upcoming visual speech in the “AV congruent” condition, in which attention to visual speech is crucial (<xref ref-type="supplementary-material" rid="pbio.2006558.s002">S2B and S2D Fig</xref>).</p>
<p>However, a number of open questions in need of further investigation still remain. First, the auditory speech envelope and lip area information used in our analysis only capture part of the rich audiovisual information that is available to interlocutors in a real-life conversation. Other, currently unaccounted features might even be correlated across modalities (e.g., a different visual feature that is correlated with the auditory envelope). Since our analysis is restricted to these two features, it is possible that with a richer feature set for each modality, the unique information obtained from each would be reduced. In addition, the auditory speech signal is available at a much higher temporal resolution compared to the lip area signal, leading to a potential bias in the information content of both signals. Since the analysis of speech-brain coupling is a relatively new research field, we envisage methodological developments that will capture more aspects of the rich audiovisual signals. But in the context of our analysis that is focused on syllable components in speech, it seems reasonable to use these two signals that are known to contain clear representations of syllable-related frequencies [<xref ref-type="bibr" rid="pbio.2006558.ref018">18</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref036">36</xref>].</p>
<p>Second, it should be noted that we computed PID measures on the speech signals and 100 ms shifted MEG signal as in previous analyses [<xref ref-type="bibr" rid="pbio.2006558.ref005">5</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref018">18</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref023">23</xref>] to compensate for delays between stimulus presentation and main cortical responses. We have confirmed that this (on average) maximizes speech-brain coupling. However, different aspects of multisensory integration likely occur at different latencies, especially in higher-order brain areas. This highly interesting but complex question is beyond the scope of the present study but will hopefully be addressed within a similar framework in future studies.</p>
<p>Third, while an unambiguous proof is missing, we believe that converging evidence suggests that participants attended visual speech more in “AV congruent” condition than in the other conditions. Indeed, it seems very unlikely that participants did not attend to visual speech after being explicitly instructed to attend (especially because visual speech provided important task-relevant information in the presence of a distracting auditory input). The converging evidence is based on behavioral performance, eye tracking results, and previous studies. Previous research indicates that the availability of visual speech information improves speech intelligibility under difficult listening conditions [<xref ref-type="bibr" rid="pbio.2006558.ref001">1</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref006">6</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref037">37</xref>]. The “AV congruent” condition was clearly more difficult compared to the “All congruent” condition because of the presence of an interfering auditory stimulus. One could argue that participants could accomplish the task by simply using auditory spatial attention. However, our behavioral data (see Fig 1B in [<xref ref-type="bibr" rid="pbio.2006558.ref005">5</xref>]) argue against this interpretation. If participants had ignored the visual stimulus and only used auditory spatial attention, then we would expect to see the same behavioral performance between “AV congruent” and “All incongruent” conditions. In both cases, two different auditory stimuli were presented, and only relying on auditory information would lead to the same behavioral performance. Instead, we find a significant difference in behavioral performance between both conditions. The availability of the congruent visual stimulus (in the “AV congruent” condition) resulted in a significant increase of behavioral performance (compared to “All incongruent” condition) to the extent that it reached the performance for the “All congruent” condition (no significant difference between “All congruent” and “AV congruent” conditions measured by comprehension accuracy; mean ± s.e.m; 85.0% ± 1.66% for “All congruent,” 83.40% ± 1.73% for “AV congruent” condition). This is strong evidence that participants actually made use of the visual information. In addition, this is also supported by eye fixation on the speaker’s lip movement, as shown in <xref ref-type="supplementary-material" rid="pbio.2006558.s005">S5 Fig</xref>.</p>
<p>In summary, we demonstrate how information theoretic tools can provide a new perspective on audiovisual integration, by explicitly quantifying both redundant and synergistic cross-modal representational interactions. This reveals two distinct profiles of audiovisual integration that are supported by different brain areas (left motor cortex and left pSTG/S) and are differentially recruited under different listening conditions.</p>
</sec>
</sec>
<sec id="sec009" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec010">
<title>Participants</title>
<p>Data from 44 subjects were analyzed (26 females; age range: 18–30 y; mean age: 20.54 ± 2.58 y). Another analysis of these data was presented in a previous report [<xref ref-type="bibr" rid="pbio.2006558.ref005">5</xref>]. All subjects were healthy, right-handed (confirmed by Edinburgh Handedness Inventory [<xref ref-type="bibr" rid="pbio.2006558.ref038">38</xref>]), and had normal or corrected-to-normal vision and normal hearing (confirmed by 2 hearing tests using research applications on an iPad: uHear [Unitron Hearing Limited] and Hearing-Check [RNID]). None of the participants had a history of developmental, psychological, or neurological disorders. They all provided informed written consent before the experiment and received monetary compensation for their participation. The study was approved by the local ethics committee (CSE01321; College of Science and Engineering, University of Glasgow) and conducted in accordance with the ethical guidelines in the Declaration of Helsinki.</p>
</sec>
<sec id="sec011">
<title>Stimuli and experiment</title>
<p>We used audiovisual video clips of a professional male speaker talking continuously (7–9 min), which were used in our previous study [<xref ref-type="bibr" rid="pbio.2006558.ref005">5</xref>]. Since in some conditions (“AV congruent,” “All incongruent” conditions) the auditory speeches are delivered dichotically, to ensure that there are no differences other than talks themselves in those conditions, we made all the videos with the same male speaker. The talks were originally taken from TED talks (<ext-link ext-link-type="uri" xlink:href="http://www.ted.com/talks/" xlink:type="simple">www.ted.com/talks/</ext-link>) and edited to be appropriate to the stimuli we used (e.g., editing words referring to visual materials, the gender of the speaker, etc.).</p>
<p>High-quality audiovisual video clips were filmed by a professional filming company, with sampling rate of 48 kHz for audio and 25 frames per second (fps) for video in 1,920 × 1,080 pixels.</p>
<p>In order to validate stimuli, 11 videos were rated by 33 participants (19 females; aged 18–31 y; mean age: 22.27 ± 2.64 y) in terms of arousal, familiarity, valence, complexity, significance (informativeness), agreement (persuasiveness), concreteness, self-relatedness, and level of understanding, using Likert scale [<xref ref-type="bibr" rid="pbio.2006558.ref039">39</xref>] 1–5 (for an example of concreteness, 1: very abstract, 2: abstract, 3: neither abstract nor concrete, 4: concrete, 5: very concrete). Eight talks were finally selected for the MEG experiment by excluding talks with mean scores of 1 and 5.</p>
<p>Questionnaires for each talk were validated in a separate behavioral study (16 subjects; 13 females; aged 18–23 y; mean age: 19.88 ± 1.71 y). These questionnaires are designed to assess the level of speech comprehension. Each questionnaire consists of 10 questions about a given talk to test general comprehension (e.g., “What is the speaker’s job?”) and were validated in terms of accuracy (the same level of difficulty), response time, and the length (word count).</p>
<p>Experimental conditions used in this study were “All congruent,” “All incongruent,” and “AV congruent.” In each condition (7–9 min), 1 video recording was presented, and 2 (matching or nonmatching) auditory recordings were presented to the left and the right ear, respectively. Half of the 44 participants attended to speech in the left ear, and the other half attended to speech in the right ear.</p>
<p>The “All congruent” condition is a natural audiovisual speech condition in which auditory stimuli to both ears and visual stimuli are congruent (V1A1A1; the first A denotes talk presented to the left ear, and the second A denotes talk presented to the right ear; the number refers to the identity of the talks). The “All incongruent” condition has three different stimulus streams from three different videos, and participants are instructed to attend to auditory information presented to one ear (V1A2A3). The “AV congruent” condition consists of one auditory stimulus matching the visual information, and the speech presented to the other ear serves as a distractor. Participants attend to the talk that matches visual information (V1A1A2 for left ear attention group, V1A2A1 for right ear attention group). Each condition represents one experimental block, and the order of conditions was counterbalanced across subjects.</p>
<p>Participants were instructed to fixate on the speaker’s lip throughout the presentation in all experimental conditions, and we monitored the eye gaze using an eye tracker. Furthermore, we explained the importance of eye fixation on the speaker’s lip movement during the instruction session. They were also informed that for this reason, their eye movement and gaze behavior would be monitored using an eye tracker (see eye tracker data analysis in <xref ref-type="supplementary-material" rid="pbio.2006558.s005">S5 Fig</xref>).</p>
<p>A fixation cross (either yellow or blue color) was overlaid on the speaker’s lip during the whole video for mainly two reasons: (1) to help maintain eye fixation on the speaker’s lip movement and (2) to indicate the auditory stimulus to pay attention to (left or right ear; e.g., “If the color of fixation cross is yellow, please attend to left ear speech”). The color was counterbalanced across subjects (for half of participants, yellow indicates attention to the left ear speech; for another half, attention to the right ear speech). This configuration was kept the same for all experimental conditions to ensure the same video display other than the experimental manipulations we aimed at. However, in “All congruent” condition (natural audiovisual speech), in which 1 auditory stream is presented diotically, attention cannot be directed to left or right ear, so participants were instructed to ignore the color of the fixation cross and just to attend the auditory stimuli naturally. In addition, to prevent stimulus-specific effects, we used 2 sets of stimuli consisting of different combinations of audiovisual talks. These 2 sets were randomized across participants (set 1 for half of participants, set 2 for the other half). For example, talks for “All congruent” condition in set 1 were talks for “AV congruent” condition in set 2.</p>
<p>There was no significant difference in comprehension accuracy between left and right ear attention groups (two-sample <italic>t</italic> test, df: 42, <italic>P</italic> &gt; 0.05). In this study, we pooled across both groups for data analysis so that attentional effects for a particular side (e.g., left or right) are expected to cancel out.</p>
<p>For the recombination and editing of audiovisual talks, we used Final Cut Pro X (Apple, Cupertino, CA). The stimuli were presented with Psychtoolbox [<xref ref-type="bibr" rid="pbio.2006558.ref040">40</xref>] in MATLAB (MathWorks, Natick, MA). Visual stimuli were delivered with a resolution of 1,280 × 720 pixels at 25 fps (mp4 format). Auditory stimuli were delivered at a 48 kHz sampling rate via a sound pressure transducer through 2 five-meter-long plastic tubes terminating in plastic insert earpieces.</p>
<p>A comprehension questionnaire was administered about the attended speech separately for each condition.</p>
</sec>
<sec id="sec012">
<title>Data acquisition</title>
<p>Cortical neuromagnetic signals were recorded using a 248 magnetometers whole-head MEG system (MAGNES 3600 WH, 4-D Neuroimaging) in a magnetically shielded room. The MEG signals were sampled at 1,017 Hz and were denoised with information from the reference sensors using the denoise_pca function in FieldTrip toolbox [<xref ref-type="bibr" rid="pbio.2006558.ref041">41</xref>]. Bad sensors were excluded by visual inspection, and electrooculographic (EOG) and electrocardiographic (ECG) artifacts were eliminated using independent component analysis (ICA). An eye tracker (EyeLink 1000, SR Research) was used to examine participants’ eye gaze and movements to ensure that they fixated on the speaker’s lip movements.</p>
<p>Structural T1-weighted MRIs of each participant were acquired at 3 T Siemens Trio Tim scanner (Siemens, Erlangen, Germany) with the following parameters: 1.0 × 1.0 × 1.0 mm<sup>3</sup> voxels; 192 sagittal slices; field of view (FOV): 256 × 256 matrix.</p>
</sec>
<sec id="sec013">
<title>Data analysis</title>
<p>Information theoretic quantities were estimated with the Gaussian-Copula Mutual Information (GCMI) method [<xref ref-type="bibr" rid="pbio.2006558.ref042">42</xref>] (<ext-link ext-link-type="uri" xlink:href="https://github.com/robince/gcmi" xlink:type="simple">https://github.com/robince/gcmi</ext-link>). PID analysis was performed with the GCMI approach in combination with an open source PID implementation in MATLAB, which implements the PID [<xref ref-type="bibr" rid="pbio.2006558.ref019">19</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref020">20</xref>] with a redundancy measure based on common change in local surprisal [<xref ref-type="bibr" rid="pbio.2006558.ref015">15</xref>] (<ext-link ext-link-type="uri" xlink:href="https://github.com/robince/partial-info-decomp" xlink:type="simple">https://github.com/robince/partial-info-decomp</ext-link>). For statistics and visualization, we used the FieldTrip Toolbox [<xref ref-type="bibr" rid="pbio.2006558.ref041">41</xref>] and in-house MATLAB codes. We followed the suggested guidelines [<xref ref-type="bibr" rid="pbio.2006558.ref043">43</xref>] for MEG studies.</p>
<sec id="sec014">
<title>MEG-MRI coregistration</title>
<p>Structural MR images of each participant were coregistered to the MEG coordinate system using a semiautomatic procedure. Anatomical landmarks (nasion, bilateral preauricular points) were identified before the MEG recording and also manually identified in the individual’s MR images. Based on these landmarks, both MEG and MRI coordinate systems were initially aligned. Subsequently, numerical optimization was achieved by using the ICP algorithm [<xref ref-type="bibr" rid="pbio.2006558.ref044">44</xref>].</p>
</sec>
<sec id="sec015">
<title>Source localization</title>
<p>A head model was created for each individual from their structural MRI using normalization and segmentation routines in FieldTrip and SPM8. Leadfield computation was performed based on a single-shell volume conductor model [<xref ref-type="bibr" rid="pbio.2006558.ref045">45</xref>] using an 8 mm grid defined on the template provided by MNI (Montreal Neurological Institute). The template grid was linearly transformed into individual head space for spatial normalization. Cross-spectral density matrices were computed using fast Fourier transform on 1 s segments of data after applying multitaper (±2 Hz frequency smoothing [<xref ref-type="bibr" rid="pbio.2006558.ref046">46</xref>]). Source localization was performed using DICS beamforming algorithm [<xref ref-type="bibr" rid="pbio.2006558.ref047">47</xref>], and beamformer coefficients were computed sequentially for all frequencies from 1 to 20 Hz for the dominant source direction in all voxels with a regularization of 7% of the mean across eigenvalues of the cross-spectral density matrix.</p>
</sec>
<sec id="sec016">
<title>Auditory speech signal processing</title>
<p>The amplitude envelope of auditory speech signals was computed following the approach reported in [<xref ref-type="bibr" rid="pbio.2006558.ref036">36</xref>]. We constructed 8 frequency bands in the range 100–10,000 Hz to be equidistant on the cochlear map [<xref ref-type="bibr" rid="pbio.2006558.ref048">48</xref>]. The auditory sound speech signals were band-pass filtered in these bands using a fourth-order forward and reverse Butterworth filter. Then, Hilbert transform was applied to obtain amplitude envelopes for each band of signal. These signals were then averaged across bands and resulted in a wideband amplitude envelope. For further analysis, signals were downsampled to 250 Hz.</p>
</sec>
<sec id="sec017">
<title>Visual speech signal processing</title>
<p>A lip movement signal was computed using an in-house MATLAB script. We first extracted the outline lip contour of the speaker for each frame of the movie stimuli. From the lip contour outline, we computed the frame-by-frame lip area (area within lip contour). This signal was resampled at 250 Hz to match the sampling rate of the preprocessed MEG signal and auditory sound envelope signal. We reported the first demonstration of visual speech entrainment using this lip movement signal [<xref ref-type="bibr" rid="pbio.2006558.ref005">5</xref>].</p>
</sec>
<sec id="sec018">
<title>Estimating MI and other information theoretic quantities: Shannon’s information theory [<xref ref-type="bibr" rid="pbio.2006558.ref049">49</xref>]</title>
<p>Information theory was originally developed to study man-made communication systems; however, it also provides a theoretical framework for practical statistical analysis. It has become popular for the analysis of complex systems in a range of fields and has been successfully applied in neuroscience to spike trains [<xref ref-type="bibr" rid="pbio.2006558.ref050">50</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref051">51</xref>], LFPs [<xref ref-type="bibr" rid="pbio.2006558.ref052">52</xref>–<xref ref-type="bibr" rid="pbio.2006558.ref053">53</xref>], EEG [<xref ref-type="bibr" rid="pbio.2006558.ref054">54</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref055">55</xref>], and MEG time series data [<xref ref-type="bibr" rid="pbio.2006558.ref018">18</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref023">23</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref056">56</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref057">57</xref>]. MI is a measure of statistical dependence between two variables, with a meaningful effect size measured in bits (see [<xref ref-type="bibr" rid="pbio.2006558.ref042">42</xref>] for a review). MI of 1 bit corresponds to a reduction of uncertainty about one variable of a factor 2 after observation of another variable. Here, we estimate MI and other quantities using GCMI [<xref ref-type="bibr" rid="pbio.2006558.ref042">42</xref>]. This provides a robust, semiparametric lower bound estimator of MI by combining the statistical theory of copulas with the closed-form solution for the entropy of Gaussian variables. Crucially, this method performs well for higher dimensional responses as required for measuring three-way statistical interactions and allows estimation over circular variables, like phase.</p>
</sec>
<sec id="sec019">
<title>MI between auditory and visual speech signals</title>
<p>Following the GCMI method [<xref ref-type="bibr" rid="pbio.2006558.ref042">42</xref>], we normalized the complex spectrum by its amplitude to obtain a 2D representation of the phase as points lying on the unit circle. We then rank-normalized the real and imaginary parts of this normalized spectrum separately and used the multivariate GCMI estimator to quantify the dependence between these two 2D signals. This gives a lower bound estimate of the MI between the phases of the two signals.</p>
<p>To determine the frequency of interest for the main analysis (PID), we computed MI between auditory (A) and visual (V) speech signals for the matching AV and nonmatching AV signals from all the stimuli we used. As shown in <xref ref-type="fig" rid="pbio.2006558.g002">Fig 2A</xref>, there was no relationship between nonmatching auditory and visual stimuli, but there was a frequency-dependent relationship for matching stimuli peaking in the band 3–7 Hz. This is consistent with previous results using coherence measure [<xref ref-type="bibr" rid="pbio.2006558.ref005">5</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref036">36</xref>]. This frequency band corresponds to the syllable rate and is known to show robust phase coupling between speech and brain signals.</p>
</sec>
<sec id="sec020">
<title>PID theory</title>
<p>We seek to study the relationships between the neural representations of auditory (here amplitude envelope) and visual (here dynamic lip area) stimuli during natural speech. MI can quantify entrainment of the MEG signal by either or both of these stimuli but cannot address the relationship between the two entrained representations—their representational interactions. The existence of significant auditory entrainment revealed with MI demonstrates that an observer who saw a section of auditory stimulus would be able to, on average, make some prediction about the MEG activity recorded after presentation of that stimulus (this is precisely what is quantified by MI). Visual MI reveals the same for the lip area. However, a natural question is then whether these two stimulus modalities provide the same information about the MEG or provide different information. If an observer saw the auditory stimulus and made a corresponding prediction for the MEG activity, would that prediction be improved by observation of the concurrent visual stimulus, or would all the information about the likely MEG response available in the visual stimulus already be available from the related auditory stimulus? Alternatively, would an observer who saw both modalities together perhaps be able to make a better prediction of the MEG, on average, than would be possible if the modalities were not observed simultaneously?</p>
<p>This is conceptually the same question that is addressed with techniques such as RSA [<xref ref-type="bibr" rid="pbio.2006558.ref026">26</xref>] or cross-decoding [<xref ref-type="bibr" rid="pbio.2006558.ref027">27</xref>]. RSA determines similar representations by comparing the pairwise similarity structure in responses evoked by a stimulus set usually consisting of many exemplars with hierarchical categorical structure. If the pattern of pairwise relationships between stimulus-evoked responses is similar between two brain areas, it indicates there is a similarity in how the stimulus ensemble is represented. Cross-decoding works by training a classification or regression algorithm in one experimental condition or time region and then testing its performance in another experimental region or time region. If it performs above chance on the test set, this demonstrates some aspect of the representation in the data that the algorithm learned in the training phase is preserved in the second situation. Both these techniques address the same conceptual issue of representational similarity, which is measured with redundancy in the information theoretic framework, but have specific experimental design constraints and are usually used to compare different neural responses (recorded from different regions or time periods or with different experimental modalities). The information theoretic approach is more flexible and can be applied both to simple binary experimental conditions as well as continuous valued dynamic features extracted from complex naturalistic stimuli, such as those we consider here. Further, it allows us to study representational interactions between stimulus features (not only neural responses) and provides the ability to quantify synergistic as well as redundant interactions.</p>
<p>We can address this question with information theory through a quantity called “Interaction Information” [<xref ref-type="bibr" rid="pbio.2006558.ref015">15</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref058">58</xref>], which is defined as follows:
<disp-formula id="pbio.2006558.e001">
<alternatives>
<graphic id="pbio.2006558.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2006558.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>;</mml:mo><mml:mi>A</mml:mi><mml:mo>;</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>;</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>;</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula></p>
<p>This quantifies the difference between the MI when the two modalities are observed together and the sum of the MI when each modality is considered alone.</p>
<p>Note that this is equivalent, with opposite sign, to a quantity called co-information. Considering co-information and thinking of information quantifying the size of sets that can be visualized in a Venn diagram, the terms I(MEG;A) + I(MEG;V) count the overall contribution of both variables but with any overlapping region counted twice. The term I(MEG;[A,V]) counts any overlapping region once and the nonoverlapping regions once each. So when subtracting the latter from the former, all that remains is the size of the overlapping region. This interpretation crucially depends on the property that MI is additive for independent variables, a property that is not shared by variance-based measures.</p>
<p>If the co-information overlap is positive, or equivalently interaction information (<xref ref-type="disp-formula" rid="pbio.2006558.e001">Eq 1</xref>) is negative, this indicates a redundant, or shared, representation. Some of what is learned about the neural response from the visual stimulus is already obtained from observation of the auditory stimulus. If the interaction information is positive, this indicates a synergistic representation. The two stimuli provide a better prediction when they are considered together than would be expected from observing each individually.</p>
<p>Interaction information is the difference between synergy and redundancy [<xref ref-type="bibr" rid="pbio.2006558.ref019">19</xref>] and therefore measures a net effect. It is possible to have zero interaction information, even in the presence of strong redundant and synergistic interactions (for example, over different ranges of the stimulus space) that cancel out in the net value. The methodological problem of fully separating redundancy and synergy has recently been addressed with the development of a framework called the PID [<xref ref-type="bibr" rid="pbio.2006558.ref019">19</xref>–<xref ref-type="bibr" rid="pbio.2006558.ref022">22</xref>]. This provides a mathematical framework to obtain decomposition of MI into unique redundant and synergistic components. The PID requires a measure of information redundancy.</p>
<p>Here, we measure redundancy using a recently proposed method based on pointwise common change in surprisal; Iccs [<xref ref-type="bibr" rid="pbio.2006558.ref015">15</xref>]. We use capital M,A,V to denote the MEG, auditory, and visual speech signals, respectively, and lower case letters to denote individual values of the same. Then, Iccs is defined as
<disp-formula id="pbio.2006558.e002">
<alternatives>
<graphic id="pbio.2006558.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2006558.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>ccs</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo>;</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:munder><mml:mo>∫</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mtext>ccs</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mtext>ccs</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mspace width="4pt"/><mml:mtext>if</mml:mtext><mml:mspace width="2pt"/><mml:mtext>sgn</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>sgn</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>sgn</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>sgn</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where
<disp-formula id="pbio.2006558.e003">
<alternatives>
<graphic id="pbio.2006558.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2006558.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfrac bevelled="true"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfrac bevelled="true"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>a</mml:mi><mml:mo>;</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfrac bevelled="true"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>a</mml:mi><mml:mo>;</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula></p>
<p>Iccs exploits the additivity of local or pointwise information values (denoted with lower case <italic>i</italic>) to calculate, for each specific value of the variables considered, the overlap in pointwise information about the MEG signal that is shared between the auditory and visual speech signals. This is calculated using the local interaction information (the negative of which is called co-information and denoted c). The expectation of this pointwise overlap is then taken, in the same way MI is the expectation of pointwise values, but because of the sign conditions, only pointwise terms that unambiguously correspond to a redundant interaction are included. Full details of the measure are given in [<xref ref-type="bibr" rid="pbio.2006558.ref015">15</xref>]. This is the only redundancy measure that corresponds to an intuitive notion of overlapping information content and is defined for more than two variables and for continuous systems. We use it here in a continuous Gaussian formulation together with the rank-normalization approach of GCMI. As there is no closed-form expression for Iccs in the case of Gaussian variables, we use Monte Carlo numerical integration. Note that the calculation requires a surrogate joint distribution over all three variables. Here, we use
<disp-formula id="pbio.2006558.e004">
<alternatives>
<graphic id="pbio.2006558.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2006558.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="true">˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>arg</mml:mtext><mml:mspace width="2pt"/><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mo>Δ</mml:mo><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mo>Δ</mml:mo><mml:mi>P</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>∈</mml:mo><mml:mo>Δ</mml:mo><mml:mo>:</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula></p>
<p>This is the maximum entropy distribution that constrains the marginal distribution of each modality speech signal and MEG. The resulting redundancy measure is therefore invariant to the marginal dependence between auditory and visual signals (which differs here between conditions).</p>
<p>With a measure of redundant information in hand, the PID framework allows us to separate the redundant and synergistic contributions to the interaction information, as well as the unique information in each modality (<xref ref-type="fig" rid="pbio.2006558.g001">Fig 1</xref>). For clarity, we restate the interpretation of these terms in this experimental context.</p>
<list list-type="bullet">
<list-item><p>Unique Information I<sub>uni</sub>(MEG;A): This quantifies that part of the MEG activity that can be explained or predicted only from the auditory speech envelope.</p></list-item>
<list-item><p>Unique Information I<sub>uni</sub>(MEG;V): This quantifies that part of the MEG activity that can be explained or predicted only from the visual lip area.</p></list-item>
<list-item><p>Redundancy I<sub>red</sub>(MEG;A,V): This quantifies the information about the MEG signal that is common to or shared between the two modalities. Alternatively, this quantifies the representation in the MEG of the variations that are common to both signals.</p></list-item>
<list-item><p>Synergy I<sub>syn</sub>(MEG;A,V): This quantifies the extra information that arises when both modalities are considered together. It indicates that prediction of the MEG response is improved by considering the dynamic relationship between the two stimuli, over and above what could be obtained from considering them individually.</p></list-item>
</list>
<p>Conceptually, the redundancy is related to whether the information conveyed by A and V in individual samples is the same or different. If the variables are fully redundant, then this means either alone is enough to convey all the information about M (i.e., obtain an optimal prediction of M), and adding observation of the second modality has no benefit for prediction. The concept of synergy is related to whether A and V convey more information when observed simultaneously, so the prediction of M is enhanced by simultaneous observation of the values of A and V [<xref ref-type="bibr" rid="pbio.2006558.ref015">15</xref>]. For example, if M is given by the difference between A and V at each sample, then observing either A or V alone tells little about the value of M, but observing them together completely determines it.</p>
</sec>
<sec id="sec021">
<title>PID analysis</title>
<p>For brain signals, frequency-specific brain activation time series were computed by applying the beamformer coefficients to the MEG data filtered in the same frequency band (fourth-order Butterworth filter, forward and reverse, center frequency ±2 Hz). The auditory and visual speech signals were filtered in the same frequency band (5 ± 2 Hz; 3–7 Hz) after we checked the dependencies between matching or nonmatching auditory and visual speeches used in the present study (<xref ref-type="fig" rid="pbio.2006558.g002">Fig 2A</xref>). MEG signals were shifted by 100 ms as in previous studies [<xref ref-type="bibr" rid="pbio.2006558.ref005">5</xref>, <xref ref-type="bibr" rid="pbio.2006558.ref018">18</xref>] to compensate for delays between stimulus presentation and cortical responses. Then, each map of PID was computed using these auditory and visual speech signals and source-localized brain signal for each voxel and each frequency band.</p>
<p>As described above (MI between auditory and visual speech signals), the complex spectra obtained from the Hilbert transform were amplitude normalized, and the real and imaginary parts were each rank-normalized. The covariance matrix of the full 6-dimensional signal space was then computed, which completely describes the Gaussian-Copula dependence between the variables. The PID was applied with redundancy measured by pointwise common change in surprisal (Iccs) [<xref ref-type="bibr" rid="pbio.2006558.ref015">15</xref>] for Gaussian variables as described above.</p>
<p>This calculation was performed independently for each voxel, resulting in volumetric maps for the four PID terms (redundant information, unique information of auditory speech, unique information of visual speech, synergistic information) for each frequency band in each individual. This computation was performed for all experimental conditions: “All congruent,” “All incongruent,” and “AV congruent.”</p>
<p>In addition, surrogate maps were created by computing the same decomposed information maps between brain signals and time-shifted speech signals for each of the four experimental conditions in each individual. Visual speech signals were shifted for 30 s, and auditory speech signals were shifted for 60 s. These surrogate data provide an estimate of each information map that can be expected by chance for each condition. These surrogate data are not used to create a null distribution but to estimate analysis bias at the group level. The surrogate data are used in analysis for Figs <xref ref-type="fig" rid="pbio.2006558.g001">1B–1D</xref>, <xref ref-type="fig" rid="pbio.2006558.g005">5C and 5D</xref>, and <xref ref-type="supplementary-material" rid="pbio.2006558.s001">S1</xref>, <xref ref-type="supplementary-material" rid="pbio.2006558.s002">S2</xref> and <xref ref-type="supplementary-material" rid="pbio.2006558.s004">S4</xref> Figs.</p>
</sec>
<sec id="sec022">
<title>Statistics</title>
<p>Group statistics was performed on the data of all 44 participants in FieldTrip. First, individual volumetric maps for each calculation (MI, Unique Information, Redundancy, Synergy) were smoothed with a 10 mm Gaussian kernel. Then, they were subjected to dependent <italic>t</italic> statistics using nonparametric randomization (Monte Carlo randomization) for comparisons between experimental conditions or to surrogate data. Results are reported after multiple comparison correction was performed using FDR [<xref ref-type="bibr" rid="pbio.2006558.ref059">59</xref>].</p>
<p>For the maps relating synergistic and redundant PID relevant for behavior, each information map (unique, redundant, and synergistic map) was subjected to regression analysis. In the regression analysis, we detected brain regions that were positively correlated to comprehension accuracy using nonparametric randomization (Monte Carlo randomization). Then, regression <italic>t</italic> maps were converted to standard <italic>Z</italic>-map (<italic>Z</italic>-transformation) and subtracted between conditions (<italic>P</italic> &lt; 0.005).</p>
</sec>
</sec>
</sec>
<sec id="sec023">
<title>Supporting information</title>
<supplementary-material id="pbio.2006558.s001" mimetype="application/x-7z-compressed" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006558.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Neural decomposition of natural audiovisual speech (‘All congruent’ condition).</title>
<p>In order to understand multisensory representational interactions in the brain during processing of natural audiovisual speech, we first define characteristics of decomposed information in ‘All congruent’ condition. To observe overall patterns in each information map, we first normalized each information map by time-shifted surrogate map at individual level, then averaged across subjects. <bold>(A)</bold> This multisensory audiovisual MI (total mutual information I(MEG;A,V)) includes unique unimodal as well as redundant and synergistic multisensory effects, which we can separate with the PID. The total mutual information map shows multimodal stimulus entrainment in bilateral auditory/temporal areas and to lesser extent in visual cortex. <bold>(B)</bold> Auditory unique information I(MEG;A) is present in bilateral auditory areas, where it accounts for a large proportion of the total mutual information I(MEG;A,V). <bold>(C)</bold> Visual unique information I(MEG;V) is present in both visual and auditory areas, but overall visual entrainment is weaker than auditory entrainment. <bold>(D)</bold> We suspect that the auditory-temporal involvement in the visual unique information might due to the correlation between auditory and visual speech signals (<xref ref-type="fig" rid="pbio.2006558.g002">Fig 2A</xref>), so we computed visual unique information in ‘All incongruent’ condition where auditory and visual speech do not match. As expected, it is only present in visual areas. Please note that these figures represent grand averages and not statistical maps. To further investigate each information of PID, we used predefined ROI maps from SPM Anatomy Toolbox (version 2.1) [1 in <xref ref-type="supplementary-material" rid="pbio.2006558.s006">S1 References</xref>] and Automated Anatomical Labeling (AAL) [2 in <xref ref-type="supplementary-material" rid="pbio.2006558.s006">S1 References</xref>]. SPM Anatomy Toolbox provides probabilistic cytoarchitectonic maps which provides stereotaxic information on the location and variability of cortical areas in the MNI (Montreal Neurological Institute) space. AAL maps provide anatomical parcellation of the spatially normalized single-subject high-resolution T1 of MNI space. Both ROI toolbox provide complementary ROI maps to each other, so that we used both toolboxes. We were interested in each decomposed PID profile in natural speech condition in four main areas: Auditory/Temporal, Visual, Motor/Sensory, Language-related areas. Auditory/Temporal area (purple) includes primary auditory cortex (TE 1.0, TE 1.1, TE 1.2), higher auditory cortex (TE 3), superior temporal gyrus (STG), superior temporal pole (STG p), middle temporal gyrus (MTG), middle temporal pole (MTG p), inferior temporal gyrus (ITG). Visual area (blue) includes BA17/V1 (hOC1), BA18/V2 (hOC2), dorsal extrastriate cortex (hOC3d/hOC4d), ventral extrastriate cortex (hOC3v/hOC4v), lateral occipital cortex (hOc4la, hOc4lp), V5/MT+ area (hOc5). Motor/Sensory area (brown) includes Areas 4a and 4p, supplementary motor area (SMA), and primary somatosensory cortex Areas 1, 2, 3a, 3b. Language-related area (red) includes BA44, BA45, Inferior frontal opercular part (Inf Oper), Inferior frontal triangular part (Tri Oper), Rolandic operculum (Rol Oper), supramarginal gyrus (SMG), and angular gyrus (AG). We first transformed the dimension of each ROI map to the dimension of our source space data, then we extracted each information (unique unimodal information for auditory and visual speech, redundancy and synergy) of bandpass-filtered (low frequencies 1–7 Hz) phase data from each ROI and then each information value was averaged within the ROI. This was performed for ‘All congruent’ condition and time-shifted surrogate data. Each information data was averaged across all subjects after subtracted by surrogate data within individual (mean ± s.e.m). Data shown per each hemisphere (LH, RH). Statistics compared to the time-shifted surrogate data was also performed and shown with asterisk in each bar when it is significant (paired two-sided <italic>t</italic>-test; df: 43; <italic>P</italic> &lt; 0.05). First for unimodal unique information (UI-A, UI-V), as expected, auditory unique information <bold>(E, F)</bold> showed strong unique information in primary auditory cortices while visual unique information <bold>(G, H)</bold> showed strong unique information in visual cortices as well as auditory/temporal areas (uncorrected statistics). This auditory/temporal representation of visual unique information is interesting (see also C) and might be due to the correlated features of audiovisual speech signals when they are congruent (<xref ref-type="fig" rid="pbio.2006558.g002">Fig 2A</xref>) as in the ‘All congruent’ condition. This is plausible because when we analysed visual unique information for ‘All incongruent’ condition where auditory and visual speeches are incongruent, it only showed visual areas (D). There have been studies on the auditory representation of visual speech demonstrating auditory cortical activation during silent lipreading [3 in <xref ref-type="supplementary-material" rid="pbio.2006558.s006">S1 References</xref>], and other studies showing primary auditory [4, 5 in <xref ref-type="supplementary-material" rid="pbio.2006558.s006">S1 References</xref>] or auditory/temporal association cortices [6–8 in <xref ref-type="supplementary-material" rid="pbio.2006558.s006">S1 References</xref>] when audiovisual stimuli are congruent. The underlying mechanisms should be further elucidated, but this is likely due to feedback processes in the brain related to multisensory representation of congruent (thus correlated) audiovisual speech [9, 10 in <xref ref-type="supplementary-material" rid="pbio.2006558.s006">S1 References</xref>]. Next, for Redundancy <bold>(I, J)</bold> and Synergy <bold>(K, L)</bold>, overall redundant information in both hemispheres is strong. This was expected considering the same audiovisual inputs in this condition (‘All congruent’). In auditory/temporal areas, both redundant and synergistic information were significantly different from time-shifted surrogate data. This pattern is the same in both hemispheres. However, in visual areas, only redundant information is significant when compared to time-shifted surrogate data whereas nearly none of synergistic information in visual cortices remain significant. In motor/sensory and language-related areas, redundant information is strongly significant in both hemispheres. Synergistic information in inferior frontal regions was also significant with more left-lateralized pattern. It should be noted that this pattern arises when perceived speech is natural unlike when task is challenging as in ‘AV congruent’ condition (the main analysis). The underlying data for this figure are available from the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/hpcj8/" xlink:type="simple">https://osf.io/hpcj8/</ext-link>).</p>
<p>(7Z)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006558.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006558.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Left pSTG and left motor cortex differentially predict visual speech.</title>
<p>A potential benefit of speech-entrained brain activity is the facilitation of temporal prediction of upcoming speech. We therefore investigated to what extent the different integration mechanisms in pSTG and motor cortex (reflected by differences in redundancy versus synergy) lead to differences in prediction. Since informativeness changed most strongly for the visual (speech) input signal (informative for ‘AV congruent’, less informative for ‘All congruent’), we expected strongest prediction effects for visual speech.</p>
<p><bold>Delayed Mutual Information analysis.</bold> We used Delayed Mutual Information to investigate to what extent brain areas predict upcoming auditory or visual speech. Delayed mutual information refers to mutual information between two signals offset with different delays. If there is significant MI between brain activity at one time, and the speech signal at a later time, this shows that brain activity contains information about the future of the speech signal. Directed Information or Transfer Entropy [11, 12 in <xref ref-type="supplementary-material" rid="pbio.2006558.s006">S1 References</xref>], is based on the same principle but additionally conditions out the past of the speech signal, to ensure the delayed interaction is providing new information over and above that available in the past of the stimulus. Here, since the delayed MI peaks are clear and well isolated from the 0 lag we present the simpler measure, but transfer entropy calculations revealed similar effects (results not shown). By means of delayed MI, we investigated prediction mechanism between theta phase in each brain area and later theta phase in visual speech. We tested delays from 0 ms to 500 ms in steps of 20 ms and then averaged the values across these delays. Interestingly, prediction of visual speech varied in both brain areas between conditions, but in different ways. Left pSTG predicts visual speech stronger in ‘All congruent’ and ‘AV congruent’ conditions than in incongruent condition (<bold>A</bold>; <italic>t</italic> = 2.99, <italic>P</italic> = 0.004 in ‘All congruent’ &gt; ‘All incongruent’; <italic>t</italic> = 2.32, <italic>P</italic> = 0.02 in ‘AV congruent’ &gt; ‘All incongruent’). Left motor cortex predicts visual speech stronger for ‘AV congruent’ than ‘All congruent’ (<bold>B</bold>; attention effect; <italic>t</italic> = 2.24, <italic>P</italic> = 0.03). When we unfolded these patterns in the temporal domain more interesting pattern emerged. The prediction mechanism in left pSTG operates in shorter temporal delays of 150–300 ms (<bold>C</bold>; <italic>P</italic> &lt; 0.05), but left motor cortex is involved in longer temporal delays of 350 ms and above (<bold>D</bold>; <italic>P</italic> &lt; 0.05). These findings suggest that left pSTG is mostly sensitive to congruent audiovisual speech (as demonstrated by redundancy in <xref ref-type="fig" rid="pbio.2006558.g004">Fig 4</xref>) and best predicts visual speech when congruent audiovisual speech is available in the absence of distracting input. This happens fast at shorter delays with visual speech. However, this pattern is different for left motor cortex. Here, we see better prediction in the ‘AV congruent’ condition, when visual speech information is informative, attended and useful to resolve a challenging listening task. Thus it has rather slow temporal dynamics at delays greater than 350 ms. The prediction of auditory speech was not different between conditions. This is expected because the level of auditory attention is similar across conditions. Overall, this suggests that integration mechanisms in left pSTG are optimized for congruent audiovisual speech. This is consistent with results in Figs <xref ref-type="fig" rid="pbio.2006558.g004">4</xref> and <xref ref-type="fig" rid="pbio.2006558.g005">5</xref> that show prominent redundancy in left pSTG. Left motor cortex instead seems to play an important role when greater attentional efforts are required and potential conflicts need to be resolved (as is the case for ‘AV congruent’ condition). The underlying data for this figure are available from the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/hpcj8/" xlink:type="simple">https://osf.io/hpcj8/</ext-link>).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006558.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006558.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Local maps of redundancy and synergy in searching for mechanisms of AV speech interaction.</title>
<p>This figure presents more detailed results from an analysis of interactions between auditory and visual speech signals as predictors of MEG signal in left superior temporal gyrus (pSTG) and left motor cortex. The selection of regions of interest was based on the results presented in Figs <xref ref-type="fig" rid="pbio.2006558.g003">3</xref>–<xref ref-type="fig" rid="pbio.2006558.g005">5</xref> that left pSTG features largely redundant interactions whereas left motor areas show predominantly synergistic interactions. As described in the Methods section, we estimate information quantities using Gaussian-Copula Mutual Information (GCMI) [<xref ref-type="bibr" rid="pbio.2006558.ref042">42</xref>] which provides a robust semi-parametric lower bound estimator of mutual information, by combining the statistical theory of copulas with the closed form solution for the entropy of Gaussian variables. Crucially, this method performs well for higher dimensional responses as required for measuring three-way statistical interactions and allows estimation over circular variables like phase. Complex spectra from Hilbert-transformed signals of auditory speech, visual speech and each brain region were amplitude normalized, and the real and imaginary parts were rank-normalized. The covariance matrix describing Gaussian-Copula dependence was computed. Information and PID values are expectations over the space of joint values. To get more insight into the mechanisms underlying the information theoretic quantification, we can directly visualise the values which are summed in the expectation, often called local values [14 in <xref ref-type="supplementary-material" rid="pbio.2006558.s006">S1 References</xref>]. While the main analysis involved 2D Hilbert transformed signals, for ease of visualisation we here consider just the 1D bandpass filtered signal. Joint MI can be written as: <inline-formula id="pbio.2006558.e005"><alternatives><graphic id="pbio.2006558.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2006558.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>;</mml:mo><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="2pt"/><mml:mstyle displaystyle="true"><mml:mrow><mml:msub><mml:mo>∭</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mspace width="2pt"/><mml:mi>d</mml:mi><mml:mi>v</mml:mi><mml:mspace width="2pt"/><mml:mi>d</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula> where <inline-formula id="pbio.2006558.e006"><alternatives><graphic id="pbio.2006558.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2006558.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="2pt"/><mml:msub><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mfrac bevelled="true"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> is the local information. We plot here the combined term <italic>pi</italic>, the local information quantity multipled by the probability of those values. This breaks down the overall quantity into the individual values which are integrated over the space. After copula normalization of a signal 0 corresponds to the median amplitude, so positive values on the x- and y-axis correspond to amplitude values above the median for auditory and visual speech signals, respectively. Here we study Mutual Information (MI) between auditory and visual speech for both regions of interest (upper row; <bold>A, B, E, F</bold>) and redundancy for left pSTG and synergy for left motor cortex (bottom row) averaged across all participants. In the ‘L pSTG’ plot, when MEG response &lt; 0 <bold>(C)</bold>, the redundancy comes from above median values of both auditory and visual speech. This shows that when both auditory and visual speech signals are high (above median), they redundantly suggest that MEG response is below the median value in the band. However, when both auditory and visual speech signals are below their median values, they redundantly suggest an above median MEG response <bold>(D)</bold>. The ‘L Motor Cortex’ plot shows that when auditory and visual signals have opposite signs (i.e. above median value in one signal occurring with a below median value of the other signal, red diagonal nodes) <bold>(G,H)</bold> they synergistically inform about a co-occurring MEG response value. That is, knowing that A is high and V is low together (or know that V is high and A is low together), provides a better prediction of a specific MEG response value than would be expected if the evidence was combined independently. Here the negative node (blue) indicates a negative synergistic contribution to mutual information (sometimes called misinformation). Above/below median values can be interpreted as loud/quiet auditory speech and large/small lip movement, so low sound amplitude combined with small lip movement can produce larger response in the left pSTG whereas high sound amplitude combined with large lip movement can produce smaller response in the left pSTG. This seems highly plausible mechanism given the left pSTG’s role in AV integration that it has greater involvement when both speech are not physically strong enough. However, synergy shows a different pattern that regardless of high or low MEG response in the left motor cortex, the combination of low sound amplitude and large lip movement, or combination of high sound amplitude and small lip movement can produce synergistic information. This suggests synergistic information arises when the interaction comes out of unbalanced features of predictors that are complementary to each other. The underlying data for this figure are available from the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/hpcj8/" xlink:type="simple">https://osf.io/hpcj8/</ext-link>).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006558.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006558.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Synergy between brain regions predictive of visual speech.</title>
<p>To better understand the integration mechanism of audiovisual speech processing observed in redundant and synergistic interaction between multisensory speech signals predictive of brain activity, we computed PID differently in which redundant and synergistic interaction between brain regions predictive of speech signals (auditory or visual).</p>
<p><bold>Selection of brain regions.</bold> We selected eight brain regions to test a predictive mechanism, i.e., does MEG activity predict upcoming speech. Regions were selected from the maximum coordinates of contrast for attention and congruence effects shown in Figs <xref ref-type="fig" rid="pbio.2006558.g004">4</xref> and <xref ref-type="fig" rid="pbio.2006558.g005">5</xref>. The abbreviation used in the nodes in the Figures, MNI coordinates, Talairach coordinates [15 in <xref ref-type="supplementary-material" rid="pbio.2006558.s006">S1 References</xref>] and Brodmann area (BA) are shown in the parenthesis: Left auditory cortex (A1; MNI = [–36–24 8]; TAL = [-35.6–22.9 8.5]; BA 41/22), left visual cortex (V1; MNI = [–28–88–8]; TAL = [-27.7–85.6–2.5]; BA 18), left posterior superior temporal gyrus (pSTG; MNI = [–60–24 0]; TAL = [-59.4–23.3 1.2]; BA 21/22), left motor cortex (M1; MNI = [–44 0 64]; TAL = [-43.6 2.9 58.8]; BA 6), left supplementary motor area (SMA; MNI = [–4 0 48]; TAL = [-4.0 2.2 44.1]; BA 6), left inferior frontal gyrus (IFG; MNI = [–64 16 24]; TAL = [-63.4 16.6 21.3]; BA 44/45), right inferior frontal gyrus (IFG; MNI = [60 8 16]; TAL = [59.4 8.5 14.3]; BA 44), left precuneus (Prec; MNI = [–4–72 64]; TAL = [-4–66.8 62.3]; BA 7).</p>
<p><bold>Partial Information Decomposition (PID) analysis predictive of speech.</bold> The PID analysis described above was computed to investigate cross-modal AV representational interactions in an individual brain region. But both PID and interaction information can be applied also to consider representational interactions between two brain regions to a single stimulus feature (as RSA is normally applied). To understand representational interactions between brain regions predictive of speech signals, we computed PID values with activity from two brain regions as the predictor variables and a unimodal speech signal (auditory or visual) as a target variable. For this, we used the eight brain regions (see above) and computed the PID for each pair of eight regions predictive of visual speech or auditory speech. This resulted in 28 pairwise computations (n(n-1)/2). Here, redundant information between two brain regions means they both provide the same prediction of the upcoming speech signal. A synergistic interaction demonstrates that the particular dynamic relationship between the neural activity in the two regions is itself predictive of speech, in a way that the direct recorded MEG in each region alone is not. The signals at the maximum coordinates were extracted from each region, PID was computed for each pair of brain regions predictive of auditory or visual speech signals. Each condition was compared to time-shifted surrogate data and between conditions. We found interesting results for synergistic interaction (but not for redundant information) between brain regions on visual speech for attention effect (‘AV congruent’ &gt; ‘All congruent’). <bold>(A)</bold> ‘AV congruent’ vs. surrogate data. Synergistic interaction between IFG (L)–M1, IFG (L)–SMA, A1–V1, A1–IFG (R), pSTG–IFG (R), pSTG–V1 were significant when predictive of visual speech. <bold>(B)</bold> ‘All congruent’ vs. surrogate data. Synergistic interaction between A1–V1, pSTG–V1 were shown to be predictive of visual speech. <bold>(C)</bold> ‘AV congruent’ vs. ‘All congruent’ (attention). When these two conditions were compared directly, synergistic interaction between IFG (L)–M1, IFG (L)–SMA, IFG (L)–A1, IFG (L)–Precuneus, SMA–Precuneus were observed to be predictive of visual speech (paired two-sided <italic>t</italic>-test; <italic>P</italic> &lt; 0.05). However, we could not find any significant interaction (either redundancy or synergy) between these regions predictive of auditory speech. These results suggest that synergistic information interaction between the regions centering around left inferior frontal gyrus (BA44/BA45) and motor areas, which matches dorsal stream in speech processing [<xref ref-type="bibr" rid="pbio.2006558.ref024">24</xref>], plays important role in attention to speech particularly visual speech when the task is challenging. The underlying data for this figure are available from the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/hpcj8/" xlink:type="simple">https://osf.io/hpcj8/</ext-link>).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006558.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006558.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Attention to visual speech revealed by analysis of eye fixation.</title>
<p>Participants were carefully instructed to fixate on the speaker’s mouth in all experimental conditions and we monitored participants’ eye movement using an eye tracker (see <xref ref-type="sec" rid="sec009">Materials and methods</xref>) to ensure that they fixate on the speaker’s lip movements. In order to investigate attention to visual speech, we analysed the simultaneously recorded eye tracking data. <bold>(A)</bold> We first constructed 2D-histograms of fixation position throughout the recording sessions (while participants viewed the speaker’s face) for each participant and each experimental condition. These histograms support the fact that participants followed instructions and fixated on the speaker’s mouth. Compliance with the instructions means that the visual information was available to them and it seems unlikely that this information was not used in the case of an interfering auditory stimulus (‘AV congruent’ condition). <bold>(B)</bold> We further analysed the 2D distribution of fixations by fitting Gaussian functions along the horizontal and vertical dimension for each participant and each experimental condition. Statistical comparison (<italic>t</italic>-test) of the width of the Gaussian function for horizontal dimension revealed a significant difference between conditions with congruent auditory and visual stimuli compared to incongruent auditory and visual stimuli (paired two sided <italic>t</italic>-test, df: 43; ‘AV congruent’ vs. ‘All incongruent’: <italic>t</italic> = -2.94, <italic>P</italic> = 0.005; ‘All congruent’ vs. ‘All incongruent’: <italic>t</italic> = -2.39, <italic>P</italic> = 0.02). Congruent AV stimuli had a significantly lower width (more narrow distribution) indicating a more focussed fixation on the mouth compared to incongruent AV stimuli (where visual stimulus was not informative). This result is not an unambiguous proof but it suggests that the informative visual information is attended and used by the participant. The underlying data for this figure are available from the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/hpcj8/" xlink:type="simple">https://osf.io/hpcj8/</ext-link>).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006558.s006" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006558.s006" xlink:type="simple">
<label>S1 References</label>
<caption>
<title>Reference list for Supporting Information.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Edwin Robertson for valuable comments on an earlier version of the manuscript.</p>
</ack>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item><term>BOLD</term>
<def><p>blood oxygen level–dependent</p></def>
</def-item>
<def-item><term>ECG</term>
<def><p>electrocardiographic</p></def>
</def-item>
<def-item><term>EOG</term>
<def><p>electrooculographic</p></def>
</def-item>
<def-item><term>FDR</term>
<def><p>false discovery rate</p></def>
</def-item>
<def-item><term>fMRI</term>
<def><p>functional magnetic resonance imaging</p></def>
</def-item>
<def-item><term>FOV</term>
<def><p>field of view</p></def>
</def-item>
<def-item><term>fps</term>
<def><p>frame per second</p></def>
</def-item>
<def-item><term>GCMI</term>
<def><p>Gaussian-Copula Mutual Information</p></def>
</def-item>
<def-item><term>ICA</term>
<def><p>independent component analysis</p></def>
</def-item>
<def-item><term>MEG</term>
<def><p>magnetoencephalography</p></def>
</def-item>
<def-item><term>MI</term>
<def><p>mutual information</p></def>
</def-item>
<def-item><term>MNI</term>
<def><p>Montreal Neurological Institute</p></def>
</def-item>
<def-item><term>PID</term>
<def><p>partial information decomposition</p></def>
</def-item>
<def-item><term>pSTG/S</term>
<def><p>posterior superior temporal gyrus/sulcus</p></def>
</def-item>
<def-item><term>pSTS</term>
<def><p>posterior superior temporal sulcus</p></def>
</def-item>
<def-item><term>RSA</term>
<def><p>representational similarity analysis</p></def>
</def-item>
<def-item><term>STG/S</term>
<def><p>superior temporal gyrus/sulcus</p></def>
</def-item>
<def-item><term>TMS</term>
<def><p>transcranial magnetic stimulation</p></def>
</def-item>
</def-list>
</glossary>
<ref-list>
<title>References</title>
<ref id="pbio.2006558.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sumby</surname> <given-names>WH</given-names></name>, <name name-style="western"><surname>Pollack</surname> <given-names>I</given-names></name>. <article-title>Visual Contribution to Speech Intelligibility in Noise</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1954</year>;<volume>26</volume>(<issue>2</issue>).</mixed-citation></ref>
<ref id="pbio.2006558.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McGurk</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>MacDonald</surname> <given-names>J</given-names></name>. <article-title>Hearing lips and seeing voices</article-title>. <source>Nature</source>. <year>1976</year>;<volume>264</volume>(<issue>5588</issue>):<fpage>746</fpage>–<lpage>8</lpage>. <object-id pub-id-type="pmid">1012311</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grant</surname> <given-names>KW</given-names></name>, <name name-style="western"><surname>Seitz</surname> <given-names>PF</given-names></name>. <article-title>The use of visible speech cues for improving auditory detection of spoken sentences</article-title>. <source>J Acoust Soc Am</source>. <year>2000</year>;<volume>108</volume>(<issue>3 Pt 1</issue>):<fpage>1197</fpage>–<lpage>208</lpage>. <object-id pub-id-type="pmid">11008820</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Wassenhove</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Grant</surname> <given-names>KW</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>. <article-title>Visual speech speeds up the neural processing of auditory speech</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2005</year>;<volume>102</volume>(<issue>4</issue>):<fpage>1181</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0408949102" xlink:type="simple">10.1073/pnas.0408949102</ext-link></comment> <object-id pub-id-type="pmid">15647358</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Park</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Kayser</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Thut</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>. <article-title>Lip movements entrain the observers’ low-frequency brain oscillations to facilitate speech intelligibility</article-title>. <source>Elife</source>. <year>2016</year>;<volume>5</volume>:<fpage>e14521</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.14521" xlink:type="simple">10.7554/eLife.14521</ext-link></comment> <object-id pub-id-type="pmid">27146891</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zion Golumbic</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Cogan</surname> <given-names>GB</given-names></name>, <name name-style="western"><surname>Schroeder</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>. <article-title>Visual input enhances selective speech envelope tracking in auditory cortex at a "cocktail party"</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>(<issue>4</issue>):<fpage>1417</fpage>–<lpage>26</lpage>. Epub 2013/01/25. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3675-12.2013" xlink:type="simple">10.1523/JNEUROSCI.3675-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23345218</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Giordano</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Ince</surname> <given-names>RAA</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schyns</surname> <given-names>PG</given-names></name>, <name name-style="western"><surname>Panzeri</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kayser</surname> <given-names>C</given-names></name>. <article-title>Contributions of local speech encoding and functional connectivity to audio-visual speech perception</article-title>. <source>Elife</source>. <year>2017</year>;<volume>6</volume>.  <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.24763" xlink:type="simple">10.7554/eLife.24763</ext-link></comment> <object-id pub-id-type="pmid">28590903</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Skipper</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Devlin</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Lametti</surname> <given-names>DR</given-names></name>. <article-title>The hearing ear is always found close to the speaking tongue: Review of the role of the motor system in speech perception</article-title>. <source>Brain Lang</source>. <year>2016</year>;<volume>164</volume>:<fpage>77</fpage>–<lpage>105</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bandl.2016.10.004" xlink:type="simple">10.1016/j.bandl.2016.10.004</ext-link></comment> <object-id pub-id-type="pmid">27821280</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Feld</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Sommers</surname> <given-names>MS</given-names></name>. <article-title>Lipreading, processing speed, and working memory in younger and older adults</article-title>. <source>J Speech Lang Hear Res</source>. <year>2009</year>;<volume>52</volume>(<issue>6</issue>):<fpage>1555</fpage>–<lpage>65</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1044/1092-4388(2009/08-0137)" xlink:type="simple">10.1044/1092-4388(2009/08-0137)</ext-link></comment> <object-id pub-id-type="pmid">19717657</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref0010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Calvert</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Campbell</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Brammer</surname> <given-names>MJ</given-names></name>. <article-title>Evidence from functional magnetic resonance imaging of crossmodal binding in the human heteromodal cortex</article-title>. <source>Curr Biol</source>. <year>2000</year>;<volume>10</volume>(<issue>11</issue>):<fpage>649</fpage>–<lpage>57</lpage>. <object-id pub-id-type="pmid">10837246</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miller</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>D’Esposito</surname> <given-names>M</given-names></name>. <article-title>Perceptual fusion and stimulus coincidence in the cross-modal integration of speech</article-title>. <source>J Neurosci</source>. <year>2005</year>;<volume>25</volume>(<issue>25</issue>):<fpage>5884</fpage>–<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0896-05.2005" xlink:type="simple">10.1523/JNEUROSCI.0896-05.2005</ext-link></comment> <object-id pub-id-type="pmid">15976077</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sekiyama</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kanno</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Miura</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sugita</surname> <given-names>Y</given-names></name>. <article-title>Auditory-visual speech perception examined by fMRI and PET</article-title>. <source>Neurosci Res</source>. <year>2003</year>;<volume>47</volume>(<issue>3</issue>):<fpage>277</fpage>–<lpage>87</lpage>. <object-id pub-id-type="pmid">14568109</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Matchin</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Groulx</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Hickok</surname> <given-names>G</given-names></name>. <article-title>Audiovisual speech integration does not rely on the motor system: evidence from articulatory suppression, the McGurk effect, and fMRI</article-title>. <source>J Cogn Neurosci</source>. <year>2014</year>;<volume>26</volume>(<issue>3</issue>):<fpage>606</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_00515" xlink:type="simple">10.1162/jocn_a_00515</ext-link></comment> <object-id pub-id-type="pmid">24236768</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beauchamp</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Nath</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Pasalar</surname> <given-names>S</given-names></name>. <article-title>fMRI-Guided transcranial magnetic stimulation reveals that the superior temporal sulcus is a cortical locus of the McGurk effect</article-title>. <source>J Neurosci</source>. <year>2010</year>;<volume>30</volume>(<issue>7</issue>):<fpage>2414</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4865-09.2010" xlink:type="simple">10.1523/JNEUROSCI.4865-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20164324</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ince</surname> <given-names>RAA</given-names></name>. <article-title>Measuring multivariate redundant information with pointwise common change in surprisal</article-title>. <source>Entropy</source>. <year>2017</year>;<volume>19</volume>(<issue>7</issue>):<fpage>318</fpage>. Epub 29 June 2017. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3390/e19070318" xlink:type="simple">10.3390/e19070318</ext-link></comment></mixed-citation></ref>
<ref id="pbio.2006558.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ding</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Melloni</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Tian</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>. <article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title>. <source>Nat Neurosci</source>. <year>2016</year>;<volume>19</volume>(<issue>1</issue>):<fpage>158</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4186" xlink:type="simple">10.1038/nn.4186</ext-link></comment> <object-id pub-id-type="pmid">26642090</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Giraud</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>. <article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>(<issue>4</issue>):<fpage>511</fpage>–<lpage>7</lpage>. Epub 2012/03/20. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3063" xlink:type="simple">10.1038/nn.3063</ext-link></comment> <object-id pub-id-type="pmid">22426255</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hoogenboom</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Thut</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Schyns</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Panzeri</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Belin</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Speech rhythms and multiplexed oscillatory sensory coding in the human brain</article-title>. <source>PLoS Biol</source>. <year>2013</year>;<volume>11</volume>(<issue>12</issue>):<fpage>e1001752</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1001752" xlink:type="simple">10.1371/journal.pbio.1001752</ext-link></comment> <object-id pub-id-type="pmid">24391472</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref019"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Williams PL, Beer RD. Nonnegative Decomposition of Multivariate Information. arXiv:10042515v1. 2010. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1004.2515" xlink:type="simple">https://arxiv.org/abs/1004.2515</ext-link></mixed-citation></ref>
<ref id="pbio.2006558.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Timme</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Alford</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Flecker</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Beggs</surname> <given-names>JM</given-names></name>. <article-title>Synergy, redundancy, and multivariate information measures: an experimentalist’s perspective</article-title>. <source>Journal of computational neuroscience</source>. <year>2014</year>;<volume>36</volume>(<issue>2</issue>):<fpage>119</fpage>–<lpage>40</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10827-013-0458-4" xlink:type="simple">10.1007/s10827-013-0458-4</ext-link></comment> <object-id pub-id-type="pmid">23820856</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barrett</surname> <given-names>AB</given-names></name>. <article-title>Exploration of synergistic and redundant information sharing in static and dynamical Gaussian systems</article-title>. <source>Phys Rev E Stat Nonlin Soft Matter Phys</source>. <year>2015</year>;<volume>91</volume>(<issue>5</issue>):<fpage>052802</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.91.052802" xlink:type="simple">10.1103/PhysRevE.91.052802</ext-link></comment> <object-id pub-id-type="pmid">26066207</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wibral</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Priesemann</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Kay</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Lizier</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Phillips</surname> <given-names>WA</given-names></name>. <article-title>Partial information decomposition as a unified approach to the specification of neural goal functions</article-title>. <source>Brain Cogn</source>. <year>2017</year>;<volume>112</volume>:<fpage>25</fpage>–<lpage>38</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bandc.2015.09.004" xlink:type="simple">10.1016/j.bandc.2015.09.004</ext-link></comment> <object-id pub-id-type="pmid">26475739</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Park</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Ince</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Schyns</surname> <given-names>PG</given-names></name>, <name name-style="western"><surname>Thut</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>. <article-title>Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners</article-title>. <source>Curr Biol</source>. <year>2015</year>;<volume>25</volume>(<issue>12</issue>):<fpage>1649</fpage>–<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2015.04.049" xlink:type="simple">10.1016/j.cub.2015.04.049</ext-link></comment> <object-id pub-id-type="pmid">26028433</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hickok</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>. <article-title>The cortical organization of speech processing</article-title>. <source>Nature reviews Neuroscience</source>. <year>2007</year>;<volume>8</volume>(<issue>5</issue>):<fpage>393</fpage>–<lpage>402</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn2113" xlink:type="simple">10.1038/nrn2113</ext-link></comment> <object-id pub-id-type="pmid">17431404</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beauchamp</surname> <given-names>MS</given-names></name>. <article-title>Statistical criteria in FMRI studies of multisensory integration</article-title>. <source>Neuroinformatics</source>. <year>2005</year>;<volume>3</volume>(<issue>2</issue>):<fpage>93</fpage>–<lpage>113</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1385/NI:3:2:093" xlink:type="simple">10.1385/NI:3:2:093</ext-link></comment> <object-id pub-id-type="pmid">15988040</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>P</given-names></name>. <article-title>Representational similarity analysis—connecting the branches of systems neuroscience</article-title>. <source>Front Syst Neurosci</source>. <year>2008</year>;<volume>2</volume>:<fpage>4</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/neuro.06.004.2008" xlink:type="simple">10.3389/neuro.06.004.2008</ext-link></comment> <object-id pub-id-type="pmid">19104670</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>King</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>. <article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title>. <source>Trends in cognitive sciences</source>. <year>2014</year>;<volume>18</volume>(<issue>4</issue>):<fpage>203</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2014.01.002" xlink:type="simple">10.1016/j.tics.2014.01.002</ext-link></comment> <object-id pub-id-type="pmid">24593982</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Crosse</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Butler</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Lalor</surname> <given-names>EC</given-names></name>. <article-title>Congruent Visual Speech Enhances Cortical Entrainment to Continuous Auditory Speech in Noise-Free Conditions</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>(<issue>42</issue>):<fpage>14195</fpage>–<lpage>204</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1829-15.2015" xlink:type="simple">10.1523/JNEUROSCI.1829-15.2015</ext-link></comment> <object-id pub-id-type="pmid">26490860</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Crosse</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Di Liberto</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Lalor</surname> <given-names>EC</given-names></name>. <article-title>Eye Can Hear Clearly Now: Inverse Effectiveness in Natural Audiovisual Speech Processing Relies on Long-Term Crossmodal Temporal Integration</article-title>. <source>J Neurosci</source>. <year>2016</year>;<volume>36</volume>(<issue>38</issue>):<fpage>9888</fpage>–<lpage>95</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1396-16.2016" xlink:type="simple">10.1523/JNEUROSCI.1396-16.2016</ext-link></comment> <object-id pub-id-type="pmid">27656026</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Campbell</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>MacSweeney</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Surguladze</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Calvert</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>McGuire</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Suckling</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Cortical substrates for the perception of face actions: an fMRI study of the specificity of activation for seen speech and for meaningless lower-face acts (gurning)</article-title>. <source>Brain Res Cogn Brain Res</source>. <year>2001</year>;<volume>12</volume>(<issue>2</issue>):<fpage>233</fpage>–<lpage>43</lpage>. <object-id pub-id-type="pmid">11587893</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beauchamp</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Argall</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>A</given-names></name>. <article-title>Integration of auditory and visual information about objects in superior temporal sulcus</article-title>. <source>Neuron</source>. <year>2004</year>;<volume>41</volume>(<issue>5</issue>):<fpage>809</fpage>–<lpage>23</lpage>. <object-id pub-id-type="pmid">15003179</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Callan</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Munhall</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kroos</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Callan</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Vatikiotis-Bateson</surname> <given-names>E</given-names></name>. <article-title>Multisensory integration sites identified by perception of spatial wavelet filtered visual speech gesture information</article-title>. <source>J Cogn Neurosci</source>. <year>2004</year>;<volume>16</volume>(<issue>5</issue>):<fpage>805</fpage>–<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089892904970771" xlink:type="simple">10.1162/089892904970771</ext-link></comment> <object-id pub-id-type="pmid">15200708</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seltzer</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pandya</surname> <given-names>DN</given-names></name>. <article-title>Afferent cortical connections and architectonics of the superior temporal sulcus and surrounding cortex in the rhesus monkey</article-title>. <source>Brain research</source>. <year>1978</year>;<volume>149</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>24</lpage>. Epub 1978/06/23. <object-id pub-id-type="pmid">418850</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bharadwaj</surname> <given-names>HM</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name>. <article-title>Measuring auditory selective attention using frequency tagging</article-title>. <source>Front Integr Neurosci</source>. <year>2014</year>;<volume>8</volume>:<fpage>6</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnint.2014.00006" xlink:type="simple">10.3389/fnint.2014.00006</ext-link></comment> <object-id pub-id-type="pmid">24550794</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Rajaram</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Xia</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bharadwaj</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Larson</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Hamalainen</surname> <given-names>MS</given-names></name>, <etal>et al</etal>. <article-title>Auditory selective attention reveals preparatory activity in different cortical regions for selection based on source location and source pitch</article-title>. <source>Front Neurosci</source>. <year>2012</year>;<volume>6</volume>:<fpage>190</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnins.2012.00190" xlink:type="simple">10.3389/fnins.2012.00190</ext-link></comment> <object-id pub-id-type="pmid">23335874</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chandrasekaran</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Trubanova</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Stillittano</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Caplier</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ghazanfar</surname> <given-names>AA</given-names></name>. <article-title>The natural statistics of audiovisual speech</article-title>. <source>PLoS Comput Biol</source>. <year>2009</year>;<volume>5</volume>(<issue>7</issue>):<fpage>e1000436</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000436" xlink:type="simple">10.1371/journal.pcbi.1000436</ext-link></comment> <object-id pub-id-type="pmid">19609344</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Berthommier</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Savariaux</surname> <given-names>C</given-names></name>. <article-title>Seeing to hear better: evidence for early audio-visual interactions in speech identification</article-title>. <source>Cognition</source>. <year>2004</year>;<volume>93</volume>(<issue>2</issue>):<fpage>B69</fpage>–<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cognition.2004.01.006" xlink:type="simple">10.1016/j.cognition.2004.01.006</ext-link></comment> <object-id pub-id-type="pmid">15147940</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oldfield</surname> <given-names>RC</given-names></name>. <article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title>. <source>Neuropsychologia</source>. <year>1971</year>;<volume>9</volume>(<issue>1</issue>):<fpage>97</fpage>–<lpage>113</lpage>. <object-id pub-id-type="pmid">5146491</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Likert</surname> <given-names>R</given-names></name>. <article-title>A technique for the measurement of attitudes</article-title>. <source>Archives of Psychology</source>. <year>1932</year>;<volume>22</volume>(<issue>140</issue>):<fpage>1</fpage>–<lpage>55</lpage>.</mixed-citation></ref>
<ref id="pbio.2006558.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brainard</surname> <given-names>DH</given-names></name>. <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial vision</source>. <year>1997</year>;<volume>10</volume>(<issue>4</issue>):<fpage>433</fpage>–<lpage>6</lpage>. <object-id pub-id-type="pmid">9176952</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oostenveld</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Fries</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Maris</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Schoffelen</surname> <given-names>JM</given-names></name>. <article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source>Computational intelligence and neuroscience</source>. <year>2011</year>;<volume>2011</volume>:<fpage>156869</fpage>. Epub 2011/01/22. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1155/2011/156869" xlink:type="simple">10.1155/2011/156869</ext-link></comment> <object-id pub-id-type="pmid">21253357</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ince</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Giordano</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Kayser</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Rousselet</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schyns</surname> <given-names>PG</given-names></name>. <article-title>A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula</article-title>. <source>Hum Brain Mapp</source>. <year>2017</year>;<volume>38</volume>(<issue>3</issue>):<fpage>1541</fpage>–<lpage>73</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hbm.23471" xlink:type="simple">10.1002/hbm.23471</ext-link></comment> <object-id pub-id-type="pmid">27860095</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Baillet</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Barnes</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Henson</surname> <given-names>RN</given-names></name>, <name name-style="western"><surname>Hillebrand</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Jensen</surname> <given-names>O</given-names></name>, <etal>et al</etal>. <article-title>Good practice for conducting and reporting MEG research</article-title>. <source>NeuroImage</source>. <year>2013</year>;<volume>65</volume>:<fpage>349</fpage>–<lpage>63</lpage>. Epub 2012/10/11. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2012.10.001" xlink:type="simple">10.1016/j.neuroimage.2012.10.001</ext-link></comment> <object-id pub-id-type="pmid">23046981</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref044"><label>44</label><mixed-citation publication-type="other" xlink:type="simple">Besl PJ, McKay ND. A method for registration of 3-D shapes. IEEE T Pattern Anal. 1992:239–56.</mixed-citation></ref>
<ref id="pbio.2006558.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nolte</surname> <given-names>G</given-names></name>. <article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors</article-title>. <source>Physics in medicine and biology</source>. <year>2003</year>;<volume>48</volume>(<issue>22</issue>):<fpage>3637</fpage>–<lpage>52</lpage>. <object-id pub-id-type="pmid">14680264</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref046"><label>46</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Percival</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Walden</surname> <given-names>AT</given-names></name>. <source>Spectral analysis for physical applications</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>; <year>1993</year>.</mixed-citation></ref>
<ref id="pbio.2006558.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kujala</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hamalainen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Timmermann</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Schnitzler</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Salmelin</surname> <given-names>R</given-names></name>. <article-title>Dynamic imaging of coherent sources: Studying neural interactions in the human brain</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2001</year>;<volume>98</volume>(<issue>2</issue>):<fpage>694</fpage>–<lpage>9</lpage>. Epub 2001/02/24. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.98.2.694" xlink:type="simple">10.1073/pnas.98.2.694</ext-link></comment> <object-id pub-id-type="pmid">11209067</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>ZM</given-names></name>, <name name-style="western"><surname>Delgutte</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>. <article-title>Chimaeric sounds reveal dichotomies in auditory perception</article-title>. <source>Nature</source>. <year>2002</year>;<volume>416</volume>(<issue>6876</issue>):<fpage>87</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/416087a" xlink:type="simple">10.1038/416087a</ext-link></comment> <object-id pub-id-type="pmid">11882898</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shannon</surname> <given-names>CE</given-names></name>. <article-title>The mathematical theory of communication</article-title>. <source>Bell Syst Tech J</source>. <year>1948</year>;<volume>27</volume>:<fpage>379</fpage>.</mixed-citation></ref>
<ref id="pbio.2006558.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Strong</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>de Ruyter van Steveninck</surname> <given-names>RR</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Koberle</surname> <given-names>R</given-names></name>. <article-title>On the application of information theory to neural spike trains</article-title>. <source>Pac Symp Biocomput</source>. <year>1998</year>:<fpage>621</fpage>–<lpage>32</lpage>. <object-id pub-id-type="pmid">9697217</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Borst</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>. <article-title>Information theory and neural coding</article-title>. <source>Nat Neurosci</source>. <year>1999</year>;<volume>2</volume>(<issue>11</issue>):<fpage>947</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/14731" xlink:type="simple">10.1038/14731</ext-link></comment> <object-id pub-id-type="pmid">10526332</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Montemurro</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Rasch</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Murayama</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>, <name name-style="western"><surname>Panzeri</surname> <given-names>S</given-names></name>. <article-title>Phase-of-firing coding of natural visual stimuli in primary visual cortex</article-title>. <source>Current biology: CB</source>. <year>2008</year>;<volume>18</volume>(<issue>5</issue>):<fpage>375</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2008.02.023" xlink:type="simple">10.1016/j.cub.2008.02.023</ext-link></comment> <object-id pub-id-type="pmid">18328702</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rubino</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Robbins</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Hatsopoulos</surname> <given-names>NG</given-names></name>. <article-title>Propagating waves mediate information transfer in the motor cortex</article-title>. <source>Nat Neurosci</source>. <year>2006</year>;<volume>9</volume>(<issue>12</issue>):<fpage>1549</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1802" xlink:type="simple">10.1038/nn1802</ext-link></comment> <object-id pub-id-type="pmid">17115042</object-id>.</mixed-citation></ref>
<ref id="pbio.2006558.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ince</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Jaworska</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Panzeri</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>van Rijsbergen</surname> <given-names>NJ</given-names></name>, <name name-style="western"><surname>Rousselet</surname> <given-names>GA</given-names></name>, <etal>et al</etal>. <article-title>The Deceptively Simple N170 Reflects Network Information Processing Mechanisms Involving Visual Feature Coding and Transfer Across Hemispheres</article-title>. <source>Cereb Cortex</source>. <year>2016</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhw196" xlink:type="simple">10.1093/cercor/bhw196</ext-link></comment> <object-id pub-id-type="pmid">27550865</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schyns</surname> <given-names>PG</given-names></name>, <name name-style="western"><surname>Thut</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>. <article-title>Cracking the code of oscillatory activity</article-title>. <source>PLoS Biol</source>. <year>2011</year>;<volume>9</volume>(<issue>5</issue>):<fpage>e1001064</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1001064" xlink:type="simple">10.1371/journal.pbio.1001064</ext-link></comment> <object-id pub-id-type="pmid">21610856</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ince</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>van Rijsbergen</surname> <given-names>NJ</given-names></name>, <name name-style="western"><surname>Thut</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Rousselet</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Panzeri</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Tracing the Flow of Perceptual Features in an Algorithmic Brain Network</article-title>. <source>Sci Rep</source>. <year>2015</year>;<volume>5</volume>:<fpage>17681</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep17681" xlink:type="simple">10.1038/srep17681</ext-link></comment> <object-id pub-id-type="pmid">26635299</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayser</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Ince</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kayser</surname> <given-names>C.</given-names></name> <article-title>Irregular Speech Rate Dissociates Auditory Cortical Entrainment, Evoked Responses, and Frontal Alpha</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>(<issue>44</issue>):<fpage>14691</fpage>–<lpage>14701</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2243-15.2015" xlink:type="simple">10.1523/JNEUROSCI.2243-15.2015</ext-link></comment> <object-id pub-id-type="pmid">26538641</object-id></mixed-citation></ref>
<ref id="pbio.2006558.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McGill</surname> <given-names>WJ</given-names></name>. <article-title>Multivariate information transmission</article-title>. <source>Psychometrika</source>. <year>1954</year>;<volume>19</volume>(<issue>2</issue>):<fpage>97</fpage>–<lpage>116</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF02289159" xlink:type="simple">10.1007/BF02289159</ext-link></comment></mixed-citation></ref>
<ref id="pbio.2006558.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Genovese</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Lazar</surname> <given-names>NA</given-names></name>, <name name-style="western"><surname>Nichols</surname> <given-names>T</given-names></name>. <article-title>Thresholding of statistical maps in functional neuroimaging using the false discovery rate</article-title>. <source>NeuroImage</source>. <year>2002</year>;<volume>15</volume>(<issue>4</issue>):<fpage>870</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1006/nimg.2001.1037" xlink:type="simple">10.1006/nimg.2001.1037</ext-link></comment> <object-id pub-id-type="pmid">11906227</object-id>.</mixed-citation></ref>
</ref-list>
</back>
</article>