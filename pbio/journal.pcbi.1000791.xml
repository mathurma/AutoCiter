<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">10-PLCB-RA-1648R2</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000791</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Neuroscience/Cognitive Neuroscience</subject></subj-group></article-categories><title-group><article-title>Influence of Low-Level Stimulus Features, Task Dependent Factors, and Spatial Biases on Overt Visual Attention</article-title><alt-title alt-title-type="running-head">Influences on Overt Visual Attention</alt-title></title-group><contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Kollmorgen</surname><given-names>Sepp</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Nortmann</surname><given-names>Nora</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Schröder</surname><given-names>Sylvia</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>König</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Institute of Neurobiopsychology, University of Osnabrück, Osnabrück, Germany</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">sschroed@ini.phys.ethz.ch</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: SK NN SS PK. Performed the experiments: SK NN SS. Analyzed the data: SK NN SS. Contributed reagents/materials/analysis tools: SK NN SS PK. Wrote the paper: SK NN SS PK.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>5</month><year>2010</year></pub-date><pub-date pub-type="epub"><day>20</day><month>5</month><year>2010</year></pub-date><volume>6</volume><issue>5</issue><elocation-id>e1000791</elocation-id><history>
<date date-type="received"><day>12</day><month>1</month><year>2010</year></date>
<date date-type="accepted"><day>21</day><month>4</month><year>2010</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2010</copyright-year><copyright-holder>Kollmorgen et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>Visual attention is thought to be driven by the interplay between low-level visual features and task dependent information content of local image regions, as well as by spatial viewing biases. Though dependent on experimental paradigms and model assumptions, this idea has given rise to varying claims that either bottom-up or top-down mechanisms dominate visual attention. To contribute toward a resolution of this discussion, here we quantify the influence of these factors and their relative importance in a set of classification tasks. Our stimuli consist of individual image patches (bubbles). For each bubble we derive three measures: a measure of salience based on low-level stimulus features, a measure of salience based on the task dependent information content derived from our subjects' classification responses and a measure of salience based on spatial viewing biases. Furthermore, we measure the empirical salience of each bubble based on our subjects' measured eye gazes thus characterizing the overt visual attention each bubble receives. A multivariate linear model relates the three salience measures to overt visual attention. It reveals that all three salience measures contribute significantly. The effect of spatial viewing biases is highest and rather constant in different tasks. The contribution of task dependent information is a close runner-up. Specifically, in a standardized task of judging facial expressions it scores highly. The contribution of low-level features is, on average, somewhat lower. However, in a prototypical search task, without an available template, it makes a strong contribution on par with the two other measures. Finally, the contributions of the three factors are only slightly redundant, and the semi-partial correlation coefficients are only slightly lower than the coefficients for full correlations. These data provide evidence that all three measures make significant and independent contributions and that none can be neglected in a model of human overt visual attention.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>In our lifetime we make about 5 billion eye movements. Yet our knowledge about what determines where we look at is surprisingly sketchy. Some traditional approaches assume that gaze is guided by simple image properties like local contrast (low-level features). Recent arguments emphasize the influence of tasks (high-level features) and motor constraints (spatial bias). The relative importance of these factors is still a topic of debate. In this study, subjects view and classify natural scenery and faces while their eye movements are recorded. The stimuli are composed of small image patches. For each of these patches we derive a measure for low-level features and spatial bias. Utilizing the subjects' classification responses, we additionally derive a measure reflecting the information content of a patch with respect to the classification task (high-level features). We show that the effect of spatial bias is highest, that high-level features are a close runner-up, and that low-level features have, on average, a smaller influence. Remarkably, the different contributions are mostly independent. Hence, all three measures contribute to the guidance of eye movements and have to be considered in a model of human visual attention.</p>
</abstract><funding-group><funding-statement>This work was supported by the EU research project Synthetic Forager, FP7 ICT-2007.2.1 #217148 (<ext-link ext-link-type="uri" xlink:href="http://cordis.europa.eu" xlink:type="simple">http://cordis.europa.eu</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="20"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>In daily life, eye movements center parts of a scene on the human fovea several times a second <xref ref-type="bibr" rid="pcbi.1000791-Buswell1">[1]</xref>. The part of the visual field falling onto the fovea is represented with the highest spatial acuity and, compared to the periphery, receives disproportionately more cortical processing resources <xref ref-type="bibr" rid="pcbi.1000791-Tootell1">[2]</xref>. The selection process is an important aspect of attention, and it has a profound impact on our perception <xref ref-type="bibr" rid="pcbi.1000791-Rizzolatti1">[3]</xref>. The selection of fixation points is governed by several factors. First, goal-driven, top-down mechanisms adapt eye movements to the specific task <xref ref-type="bibr" rid="pcbi.1000791-Land1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Yarbus1">[5]</xref>. Second, bottom-up mechanisms that consider only sensory-driven aspects, such as local image features <xref ref-type="bibr" rid="pcbi.1000791-Koch1">[6]</xref>, contribute to the fixation selection process. Third, characteristics inherent to the visual apparatus, such as the spatial bias to the center region <xref ref-type="bibr" rid="pcbi.1000791-Tatler1">[7]</xref> and geometric properties of saccades <xref ref-type="bibr" rid="pcbi.1000791-Brockmann1">[8]</xref>, are widely acknowledged to influence the selection of fixation points. However, the relative roles and the interaction of these mechanisms are not understood, and a quantitative understanding of the principles of fixation selection is still lacking.</p>
<p>Attention models designed to cope with the complexities of natural conditions are usually based on a so-called salience map <xref ref-type="bibr" rid="pcbi.1000791-Koch1">[6]</xref>. Filtering the input image with kernels reminiscent of early visual processing generates feature maps at various spatial scales. These are then combined into a single salience map, which encodes the probability that an image region will be attended <xref ref-type="bibr" rid="pcbi.1000791-Itti1">[9]</xref>. In principle, the selection of features for such models is unconstrained. First implementations were designed to explain covert attention in experiments involving artificial stimuli and based on a repertoire of simple features. Present models slowly move towards a more complete list of relevant features <xref ref-type="bibr" rid="pcbi.1000791-Wolfe1">[10]</xref> and include more and more features (Betz T, Kietzmann TC, Wilming N, König P (in press). Investigating task dependent top-down effects on overt visual attention. J Vis). Furthermore, they introduce probabilistic and decision theoretic concepts <xref ref-type="bibr" rid="pcbi.1000791-Gao1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Zhang1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Itti2">[13]</xref>. Salience maps predict, to some extent, fixations in complex scenes <xref ref-type="bibr" rid="pcbi.1000791-Li1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Parkhurst1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Peters1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Privitera1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Tatler2">[18]</xref> for humans as well as for monkeys <xref ref-type="bibr" rid="pcbi.1000791-Einhuser1">[19]</xref>. The critical phrase “to some extent” is at the center of an intense debate. Is it possible to refine models based on stimulus dependent salience to model overt attention as well as intersubject variability allows?</p>
<p>A major concern is that even if features of the salience map, such as luminance contrast, are good correlates of fixation probability, they do not necessarily drive attention causally <xref ref-type="bibr" rid="pcbi.1000791-Tatler1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Carmi1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Einhuser2">[21]</xref>, but are contingent on higher-order statistics <xref ref-type="bibr" rid="pcbi.1000791-Einhuser3">[22]</xref>. These issues have raised considerable skepticism regarding models based purely on low-level image features.</p>
<p>For these reasons, there is consensus that viable models of human attention should not rely solely on stimulus properties. Specifically, eye movements are influenced by spatial constraints and properties of the oculomotor system. A wide range of studies has demonstrated a preponderance of small amplitude saccades <xref ref-type="bibr" rid="pcbi.1000791-Bahill1">[23]</xref>. Furthermore, under typical lab conditions observers have a central bias—i.e., a tendency to fixate preferentially close to the center of photographs of natural scenes, in excess of behavior under truly natural conditions <xref ref-type="bibr" rid="pcbi.1000791-Tatler3">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Schumann1">[25]</xref>. Furthermore, the recent years have seen a major advance in our understanding of scene layout. Including such information, which was automatically generated by machine learning algorithms, leads to a very high prediction accuracy in a search task for pedestrians <xref ref-type="bibr" rid="pcbi.1000791-Ehinger1">[26]</xref>. Furthermore, recent work demonstrates that spatial properties might have a large influence on overt attention <xref ref-type="bibr" rid="pcbi.1000791-Tatler4">[27]</xref>. While it is clear that these spatial factors contribute to the selection of fixation points, there is as yet no quantification of their general influence.</p>
<p>That the task context influences eye movements has long been observed <xref ref-type="bibr" rid="pcbi.1000791-Buswell1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Yarbus1">[5]</xref>. In a study utilizing a variety of tasks—including abstract interpretations, such as the judgment of social status—the task was found to strikingly modify observed fixation patterns <xref ref-type="bibr" rid="pcbi.1000791-Yarbus1">[5]</xref>. Also the complex activities of daily living reveal the task dependence of human eye movements <xref ref-type="bibr" rid="pcbi.1000791-Land2">[28]</xref>. Models for visual attention based solely on low-level visual features fail to capture the effects of the task context. Several extensions to existing and also new models have been proposed to address that shortcoming <xref ref-type="bibr" rid="pcbi.1000791-Navalpakkam1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Torralba1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Tsotsos1">[31]</xref>. An elegant information theoretic approach combines visual appearance, spatial information and high-level information further improving prediction accuracy (Kanan CM, Cottrell GW (2010). Robust classification of objects, faces, and flowers using natural image statistics. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR-2010)).</p>
<p>It was suggested early on that in a specific task context, the information content of an image patch defines its salience <xref ref-type="bibr" rid="pcbi.1000791-Antes1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Mackworth1">[33]</xref>. This proposal has triggered bottom-up driven models of attention incorporating a decision theoretic approach <xref ref-type="bibr" rid="pcbi.1000791-Gao1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Zhang1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Itti2">[13]</xref>. Hence the information content of a patch may be viewed as a task dependent, high-level feature. This view is suited to reconciling stimulus-driven models and task-centered models. Recent studies emphasizing the importance of objects in overt attention are compatible with this view <xref ref-type="bibr" rid="pcbi.1000791-Einhuser4">[34]</xref>. However, information content is determined either intuitively or based on a direct subjective rating. Furthermore, there is presently no general algorithm available that would reliably label objects in a visual scene. Instead studies rely again on ratings of human subjects (<ext-link ext-link-type="uri" xlink:href="http://labelme.csail.mit.edu/" xlink:type="simple">http://labelme.csail.mit.edu/</ext-link>). An explicit quantification of the contribution of task dependent factors relative to feature-based factors is still missing.</p>
<p>In summary, it is widely acknowledged that image features, the task of the observer, and the properties of the oculomotor system contribute to the selection of fixation points. Still, in the absence of quantitative data on the relative weight of the different factors, settling the issue of how exactly each of these contributes towards overt attention is not possible. In the present study, we attempt a step in this direction: we quantify the relative contribution of stimulus properties, task dependence, and oculomotor constraints to the selection of fixation points. We capture stimulus dependent properties by an analysis of low-level image features. Task dependent factors are captured by the information content of discrete parts of the stimulus in well defined tasks. The influence of oculomotor constraints is taken into account by a generative model including typical saccadic parameters and the central bias. With this approach we obtain scalars for each of these three factors for each image region, allowing us to quantify their independent contributions to human eye movements.</p>
<p>To quantify the three different types of influences we sample non-overlapping image patches (bubbles) from forest scenes and face images. These isolated patches are shown in different configurations in combination with a classification task. This design is inspired by Gosselin and Schyns, who have introduced the bubble paradigm to measure which parts of an image are used by the observer to solve a classification task <xref ref-type="bibr" rid="pcbi.1000791-Gosselin1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Gosselin2">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Schyns1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Vinette1">[38]</xref>. The technique applies two-dimensional Gaussian filters to isolate locations of visual cues, which are called bubbles. These are then presented in varying combinations, revealing only a limited controlled subset of the image content in combination with a classification task. Based on the observers' responses, Gosselin and Schyns derived a map revealing the regions of an image that are relevant for a specific classification task <xref ref-type="bibr" rid="pcbi.1000791-Gosselin1">[35]</xref>. We use the bubble technique in combination with an eye-tracking experiment to obtain measures of different contributions to overt attention. Each bubble is treated as an independent unit. Utilizing recorded eye movements, responses in the classification task, feature analysis of the image patches, and baseline data taken from a free viewing eye-tracking study, we compute four measures: the stimulus dependent measure captures low-level feature contrast and is based on the luminance and texture distribution within each bubble. The task-related measure ignores image features, but quantifies how much information a bubble contains in the context of a specific classification task. Additional high-level factors, e.g. emotional and attentional state, might be relevant. We tried to keep these constant as much as possible. This quantification does of course not capture all possible top-down effects discussed in the literature as a classification task provides a particular context. The third measure, describing the spatial characteristics of eye movements, builds on a baseline study and takes into consideration the global fixation bias and geometrical properties of saccades. By evaluating the eye-tracking data of the main study, we obtain the fourth measure that captures the empirical salience of each bubble.</p>
<p>In comparison to full field stimuli, our bubble stimuli consist of a manageable number of discrete perceptual units. Using discrete units allows us to assign a single value for each of the measures to each bubble. In particular, describing the task dependent information of a bubble using the degree of agreement between subjects with respect to a classification task requires individual pieces of visual information. It is not clear how a measurement of local information content could be achieved using full field stimuli only. Accordingly, the problem of measuring local information is exactly the one addressed when the bubbles technique was first established <xref ref-type="bibr" rid="pcbi.1000791-Gosselin1">[35]</xref>.</p>
<p>Having acquired the four measures for each bubble, we finally use linear multivariate regression to quantify the overall and the individual, i.e., non-redundant, contributions of the task- dependent, feature-based, and spatial-based factors influencing attention.</p>
</sec><sec id="s2">
<title>Results</title>
<p>In this study, subjects had to classify visual stimuli based either on face images or on forest scenes. We employed a total of four different tasks. Face stimuli had to be classified either according to <bold><italic>gender</italic></bold> or according to facial <bold><italic>expression</italic></bold>, with the stimulus <bold>classes</bold> <italic>happy</italic>, <italic>sad</italic>, <italic>fearful</italic>, or <italic>disgusted</italic>. For the stimuli based on forest scenes, one task (<bold><italic>space</italic></bold>) was to decide whether the scenery was <italic>close and narrow</italic> – when the image was a close-up or displayed a closed environment – or whether it was <italic>wide and open</italic>. The other task was to judge the presence of indicators of human <bold><italic>influence</italic></bold> such as houses, roads, paths, trunks of trees. Stimulus presentations lasted for three seconds during which the subjects' eye movements were recorded. The majority of the stimuli were composed of 1 to 5 bubbles placed on a gray background. Half of the stimuli consisted of bubbles originating from the same full field image (condition <bold><italic>same</italic></bold>), whereas 15% of the stimuli combined bubbles from different full field images belonging to the same stimulus class (condition <bold><italic>congruent</italic></bold>). Another 15% of the stimuli were composed of bubbles originating from full field images of different classes (condition <bold><italic>incongruent</italic></bold>). To control for position effects, we also showed stimuli (16%) in which the positions of the bubbles are shuffled (condition <bold><italic>permuted</italic></bold>). The remaining 4% of the stimuli were full field images, which we used to confirm that subjects agreed on the classes of the images underlying the bubble stimuli. The bubbles themselves were constructed from square image patches with a side length of 6 visual degrees. To each patch, we applied a space-variant filter to imitate the retinal resolution when fixating the center of the bubble and a Gaussian mask to avoid visible edges.</p>
<p>75 subjects took part in this study, each performed 280 trials. We used a total of 2061 gray-scale stimuli for all subjects. This resulted in 21000 trials, recorded with 131935 fixations.</p>
<sec id="s2a">
<title>Bubbles are Treated as Units</title>
<p>In a first step, we investigated viewing behavior relative to bubbles. Subjects made, on average, 6.2 fixations in each trial where bubbles were presented. Of these, 94% were no more than 3 visual degrees distant from the closest bubble center and thus were located well inside a bubble. Three percent were located at the screen center and can be attributed to anticipation of the decision screen that followed each trial. The remaining 3% were scattered across the screen. Hence, the fixations were rare in the space between bubbles and were clearly targeted at bubbles.</p>
<p>We designed the bubbles in such a way that maximal and complete information is available when subjects fixated the center of the bubble (see <xref ref-type="sec" rid="s4">Methods</xref>). Hence subjects would not gain anything by scanning different positions within the same bubble. This, however, does not necessarily prevent them from doing so. We tested this to confirm that bubbles were indeed treated as perceptual units. Of the total number of fixations that targeted bubbles, 60% originated from outside the respective bubble (“first fixations”). The remaining 40% were due to saccades within a bubble (“subsequent fixations”). The distributions of distances to bubble centers for these two groups of fixations were significantly different (p&lt;0.01, KS-test, <xref ref-type="fig" rid="pcbi-1000791-g001">Figure 1</xref>). The median of the distances to the closest bubble center was 1.05° for “first fixations” and larger than the median of 0.91° for “subsequent fixations”. For pairs of first and subsequent fixations, the subsequent fixation was, on average, 0.16° closer to the bubble center. Additionally, both distributions were more sharply peaked than the distribution that would have resulted if fixations had been sampled from the Gaussian mask used for bubble construction (p&lt;0.01 in both cases, KS-test, see <xref ref-type="fig" rid="pcbi-1000791-g001">Figure 1</xref>). Altogether, the fixation data do not indicate that individual bubbles were scanned for information, but suggest that participants targeted bubble centers and made small corrective saccades towards bubble centers when landing off-center. The data is hence consistent with the assumption that bubbles were treated as perceptual units.</p>
<fig id="pcbi-1000791-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.g001</object-id><label>Figure 1</label><caption>
<title>Fixations on bubbles.</title>
<p>(A) An example trajectory recorded during the experiment. The fixation labeled with zero is the first fixation in that trial, which was excluded from analyses. (B) Distributions of distances between fixation locations and the closest bubble center for “first fixations” into a bubble (median 1.05°) and “subsequent fixations” within the same bubble (median 0.91°). For comparison, the distribution that would result if all fixations were sampled from the Gaussian window used to construct the bubbles (median 1.18°) is also given.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.g001" xlink:type="simple"/></fig>
<p>Building on the property that bubbles are treated as units, we derive a measure characterizing the empirical salience of a complete bubble. It is based on the fixation counts of a bubble in specific stimulus configurations (see below). In the above example of <xref ref-type="fig" rid="pcbi-1000791-g001">Figure 1</xref> these fixation counts amount to 3, 3, 0, and 0 for bubbles A, B, C, and D, respectively. These counts are then averaged over subjects.</p>
</sec><sec id="s2b">
<title>Information of Different Bubbles is Integrated</title>
<p>The average classification performance for the original images (full field stimuli) was 94%, as measured by the fraction of responses that were correct with respect to the image class established in pre-experiments. For the four tasks average performance was 87%, 94%, 99% and 94% (<italic>expression</italic>, <italic>gender</italic>, <italic>influence</italic>, and <italic>space</italic>). When comparing the different tasks, please note that in <italic>expression</italic> the chance level is 25% and in all others 50%. The high-level of performance indicates that participants understood the tasks and had a shared interpretation of stimulus classes.</p>
<p>In order to be independent of predefined “correct” responses in the following analyses, we use the more general measure of <italic>stimulus information</italic>. It is defined by the maximal possible entropy of the distribution of responses minus the entropy of the actual response distribution (see <xref ref-type="sec" rid="s4">Methods</xref>). When all subjects agree on the classification of a stimulus, that stimulus contains maximal information with respect to the classification tasks. When their response distribution is flat, the stimulus contains no information. In the case of <italic>expression</italic>, with 4 choices the <italic>stimulus information</italic> ranges from 0 to 2 bit, in the other tasks from 0 to 1 bit. <italic>Stimulus information</italic> thus captures the degree of consensus from the subjects classifying the stimuli.</p>
<p>Next we investigated stimulus information in the reduced stimuli composed of bubbles. Presenting bubble stimuli composed of bubbles taken from the same base image (condition <italic>same</italic>) yielded average stimulus information of 1.18 bit, 0.66 bit, 0.74 bit, and 0.54 bit for the four tasks <italic>expression</italic>, <italic>gender</italic>, <italic>influence</italic>, and <italic>space</italic>, respectively. Presenting stimuli composed of bubbles taken from different images of the same response class (condition <italic>congruent</italic>) average stimulus information changed to 1.31 bit, 0.62 bit, 0.67 bit, and 0.61 bit (<italic>expression</italic>, <italic>gender</italic>, <italic>influence</italic>, and <italic>space</italic>). In contrast, in presenting stimuli composed of bubbles taken from images of different response classes (condition <italic>incongruent</italic>) it dropped to 1.12 bit, 0.55 bit, 0.58 bit, and 0.34 bit. These data demonstrate that stimulus information is far from complete and that no ceiling effects are to be expected.</p>
<p>To address the integration of information we analyze stimulus information as a function of the number of bubbles (<xref ref-type="fig" rid="pcbi-1000791-g002">Figure 2</xref>). First, we compare measured stimulus information in the <italic>same</italic> condition with estimates of a probabilistic model of information integration (see <xref ref-type="sec" rid="s4">Methods</xref>). The model, which we denote as <bold>p-model</bold>, integrates the response distributions of individual bubbles to estimate the stimulus response distribution and is presented here as a hypothesis. In the following, we only test plausibility of the p-model; we give a more detailed account in the <xref ref-type="sec" rid="s3">Discussion</xref>. Stimulus information is computed from the entropy of the stimulus response distribution as described above. The p-model assumes independence of the information in different bubbles and integrates the information optimally. To predict stimulus information as a function of the number of bubbles, a sample of the bubbles, which were presented on their own, is selected. Then the respective response distributions of these stimuli are integrated using the p-model. This procedure is repeated 1000 times for each number of bubbles and each task. The resulting average information values are compared to the empirically found information values of the stimuli containing the respective numbers of bubbles (<xref ref-type="fig" rid="pcbi-1000791-g002">Figure 2</xref>). The selection of single bubble stimuli for integration is done independent of image class. In the <italic>expression</italic> task, which uses face stimuli, we observe a pronounced surplus of experimentally observed average stimulus information (green line) compared to the prediction of the p-model (dashed black line). This higher-than-expected stimulus information indicates a violation of the assumption of independence of the information in different bubbles and is investigated below. In the gender task, which also uses face stimuli, at four and five bubbles a surplus of measured information is observed as well. Due to the larger variance of these two data points it does not reach significance. Stimulus information in the <italic>influence</italic> task, which uses natural scenes, is well predicted by the p-model, and no significant deviation of estimate and data could be detected (p&gt;0.05, bootstrapped confidence intervals). For <italic>space</italic>, stimulus information is a little, but significantly, smaller than predicted by the p-model (p&lt;0.05, bootstrapped confidence intervals). In this condition, the integration strategy of the subjects does not quite reach optimal performance. These data suggest that the p-model provides a reasonable description of the information integration. The mentioned deviations are further investigated below.</p>
<fig id="pcbi-1000791-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.g002</object-id><label>Figure 2</label><caption>
<title>Stimulus information versus number of bubbles for the four tasks.</title>
<p>Stimulus information estimated using the p-model is plotted for all four tasks (black dashed line). This is contrasted with the measured stimulus information in the <italic>same</italic> condition (green line) and in the <italic>congruent</italic> condition (red line). The blue line marks the measurements that result if the positions of bubble stimuli of the <italic>same</italic> condition are shuffled (<italic>same</italic>, <italic>permuted</italic>). The colored stars mark significant differences (p&lt;0.05, bootstrapped confidence intervals) between the curve belonging to the respective condition and the p-model estimate. For visibility, the 95% confidence interval is marked by error bars only for condition <italic>same</italic>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.g002" xlink:type="simple"/></fig>
<p>Now, we investigate the integration of information for the different conditions. We compare stimulus conditions <italic>same</italic> and <italic>congruent</italic>. In contrast to the former, the latter is composed of bubbles that originate from different full field images of the same response class. Data obtained in <italic>same</italic> and <italic>congruent</italic> conditions give rise to nearly identical values of stimulus information in all tasks, and their difference is never significant (p&gt;0.05, bootstrapped confidence intervals, <xref ref-type="fig" rid="pcbi-1000791-g002">Figure 2</xref> green and red lines). Specifically, this includes the large deviation from the prediction of the p-model in the <italic>expression</italic> task. This indicates that the information of bubbles is integrated in the same way, irrespective of whether the bubbles originate from the <italic>same</italic> or different <italic>congruent</italic> full field stimuli.</p>
<p>To further elucidate the cause for the deviations of the data from the p-model estimates, we consider the interaction of bubble information and spatial location. For this purpose, we employ permuted stimuli. These are composed of bubbles placed at positions not matching their location in the respective full field stimuli (see <xref ref-type="sec" rid="s4">Methods</xref>). In all tasks, including the <italic>expression</italic> task, the stimulus information in this <italic>permuted</italic> condition is well predicted by the p-model (<xref ref-type="fig" rid="pcbi-1000791-g002">Figure 2A</xref>, blue line). For the face stimuli, this, together with the large differences between the p-model and the <italic>same</italic> and the <italic>congruent</italic> condition for high numbers of bubbles, demonstrates that the subjects' integration of information is influenced by bubble locations. This can be understood intuitively if one assumes that bubbles at certain locations (e.g., mouth) are given more weight, irrespective of the actual content of the bubble (e.g., smile). Indeed, the main result for permuted stimuli is an improved fit by the p-model. On the other hand, position effects are not a likely cause for the deviations in the space task. There, the <italic>permuted</italic> and <italic>same</italic> conditions show no pronounced differences. The stimulus information for both is slightly below that of the p-model.</p>
<p>To test more directly whether bubble position and arrangement have an influence on information integration in the tasks <italic>gender</italic>, <italic>influence</italic>, and <italic>space</italic>, we performed an additional test and considered the differences between the response distributions of normal stimuli and their permuted versions. To specify whether these differences reflect a significant effect of permutation, we investigate whether the differences are consistent with the assumption that the responses for normal and permuted stimuli are sampled from the same stimulus answer distribution, independent of bubble arrangement. As the overwhelming majority (98.6%) of the differences between permuted and non permuted stimuli is located within the 95% confidence region of the zero hypothesis, no significant effect of permutation could be detected. It must be noted, however, that the test power is limited by the small number of trials using permuted stimuli.</p>
<p>We arrive at the conclusion that the p-model provides a good description of integration of information for face stimuli in the permuted condition and for forest scenes in all conditions. In the <italic>same</italic> and the <italic>congruent</italic> condition, face stimuli consisting of many bubbles are processed using additional configural information <xref ref-type="bibr" rid="pcbi.1000791-Calder1">[39]</xref>.</p>
</sec><sec id="s2c">
<title>Three Different Saliences of Bubbles and their Relation to Fixation Behavior</title>
<p>Now we address the relative contributions to fixation behavior of the stimulus dependent salience, task dependent information, and geometric properties of the stimuli. First, to address the stimulus dependent salience, we consider the low-level visual information of luminance contrast and texture contrast. These features are presumably processed in a bottom-up manner and have been used in other studies before. Second, to address the task dependent information, we consider the measure of bubble information, which captures the contribution of a bubble to the classification responses of subjects (see <xref ref-type="sec" rid="s4">Methods</xref>). Third, to address the geometric properties, we investigate whether a simple generative model of fixation behavior that is based on the spatial arrangement of bubbles, central fixation bias, and geometrical constraints of average saccadic length and direction is informative with respect to the frequencies of fixation of different bubbles. Finally, these three components are used to explain the empirical distribution of fixations on bubbles, represented by empirical saliences. The measure of empirical salience is a context independent measure that represents how often a bubble is fixated relative to any other bubble. To obtain a measure which is independent of the specific stimulus context (instead of values for each stimulus) we combined the data from all stimuli and computed the best linear fit (see <xref ref-type="sec" rid="s4">Methods</xref>). With this measure in turn the actual averaged fixation counts on the individual stimuli can be reconstructed with an average accuracy of 94.4%. Hence the empirical salience gives a faithful description of the fixation probability of a bubble in all stimulus configurations. The three former components and their relation with empirical salience are now considered in turn.</p>
</sec><sec id="s2d">
<title>Correlation of Low-Level Stimulus Features with Empirical Salience</title>
<p>In agreement with a large body of previous research <xref ref-type="bibr" rid="pcbi.1000791-Gao1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Zhang1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Einhuser2">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Ak1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Parkhurst2">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Reinagel1">[42]</xref>, we characterize low-level visual information contained in a bubble by its luminance and texture contrast. We estimate the contribution to fixation behavior by considering fixation probability conditioned on these feature contrasts. This allows determining the correlation of local features, as used in common stimulus-driven models of overt attention, with the empirical salience of bubbles.</p>
<p>The luminance and texture contrast of bubbles are determined by standard procedures (see <xref ref-type="sec" rid="s4">Methods</xref>). To infer the conditional fixation probability, we recur to a previous study where gaze movements on full field images have been recorded, and the conditional probability to fixate a location given its feature values was determined empirically <xref ref-type="bibr" rid="pcbi.1000791-Schumann2">[43]</xref>. Here we use the same procedure and the results of the previous study to convert both luminance contrasts and texture contrasts into fixation probabilities. To obtain a model that incorporates both, we combine the resulting probabilities, assuming independence of the contributions of the two feature contrasts.</p>
<p><xref ref-type="fig" rid="pcbi-1000791-g003">Figure 3A</xref> shows an example stimulus from the <italic>expression</italic> task with the individual bubbles labeled with their stimulus dependent salience. Bubble A, located on the right eye and eyebrow, contains high luminance and texture contrasts. This is mapped to a high value of the stimulus dependent salience (see <xref ref-type="sec" rid="s4">Methods</xref>). Relative to the other bubbles of the <italic>expression</italic> task, bubble A has a high stimulus dependent salience and a high empirical salience, placing it in the upper right-hand corner of the scatter plot of stimulus dependent salience vs. empirical salience (<xref ref-type="fig" rid="pcbi-1000791-g003">Figure 3B</xref>). Bubble B, centered on the upper lip, has a lower stimulus dependent salience, but is looked at slightly more often than bubble A, placing it in the upper left-hand corner of the scatter plot. Bubble C, showing hair, has the strongest stimulus dependent salience of all four bubbles, but is only rarely looked at, placing it in the lower right corner of the plot. Bubble D, also showing hair, has very low stimulus dependent and empirical salience, placing it in the lower left corner of the plot. In this specific example, stimulus dependent salience and empirical salience appear unrelated.</p>
<fig id="pcbi-1000791-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.g003</object-id><label>Figure 3</label><caption>
<title>Relationship between stimulus dependent and empirical salience.</title>
<p>(A) Example stimulus from the expression task with bubbles labeled by their stimulus dependent salience. (B) Scatter plot of stimulus dependent vs. empirical salience for the <italic>expression</italic> task. The positions of the bubbles from the example stimulus are marked by colored dots. The correlation coefficient r is given as a figure inset. (C) Correlation coefficients for all four tasks (E – <italic>expression</italic>, G – <italic>gender</italic>, I – <italic>influence</italic>, S – <italic>space</italic>). One star marks a significant correlation (p&lt;0.05, t-test); two stars mark a highly significant correlation (p&lt;0.01, t-test).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.g003" xlink:type="simple"/></fig>
<p>To determine the predictive power of the feature-driven model, we correlate the predicted fixation probabilities for individual bubbles with their empirical salience (both log transformed, see <xref ref-type="sec" rid="s4">Methods</xref>). <xref ref-type="fig" rid="pcbi-1000791-g003">Figure 3B</xref> shows a scatter plot of stimulus dependent salience of all bubbles in the <italic>expression</italic> task versus their empirical salience. It shows a weak, albeit not significant, correlation (p&gt;0.05, t-test). Similarly, no significant correlation is observed for the <italic>space</italic> task (<xref ref-type="fig" rid="pcbi-1000791-g003">Figure 3C</xref>). In the remaining two tasks, <italic>gender</italic> and <italic>influence</italic>, we do observe a significant correlation. This shows that the strength of the correlation of low-level features with selected fixation points varies as a function of the task for photographs of faces as well as of natural environments.</p>
</sec><sec id="s2e">
<title>Correlation of Bubble Information with Fixated Bubbles</title>
<p>We take the contribution of a bubble to <italic>stimulus information</italic> as a surrogate for high-level information. We estimate bubble information for all bubbles that were shown in isolation or in combinations by assuming that the information of individual bubbles in a stimulus is integrated according to the p-model. Under this assumption, bubble information can be estimated in a global fit that maximizes the agreement between the subjects' responses to all stimuli and the alleged information contained in each single bubble (see <xref ref-type="sec" rid="s4">Methods</xref>). This global fit estimates the information contained in each bubble, including those that were shown in isolation.</p>
<p>As a model of information integration we use the p-model introduced above. The results of the global fit based on the p-model may be viewed as a high-level feature specific to the context of the current task. <xref ref-type="fig" rid="pcbi-1000791-g004">Figure 4A</xref> shows an example of a stimulus of the <italic>expression</italic> task where the total measured stimulus information is 2 bit. The individual bubbles are labeled with their fitted response distributions and bubble information values. The global fit gives the information content as 0.40, 1.84, 0.13, and 0.01 bit for bubbles A, B, C, and D, respectively. In turn, estimating the stimulus information by the p-model results in 1.97 bit. This is close to the measured stimulus information with an error of 0.03 bit. Over all the bubble stimuli, the mean errors of stimulus information predicted from the fitted bubble answer distributions are 0.32, 0.20, 0.26, and 0.24 bit for the tasks <italic>expression</italic>, <italic>gender</italic>, <italic>influence</italic>, and <italic>space</italic>, respectively. For comparison, we computed the errors that would be expected if the predictions by the p-model were the true underlying response distributions of the stimuli (see <xref ref-type="supplementary-material" rid="pcbi.1000791.s001">Text S1 B</xref>). In that case, the subjects sample their responses from the predicted response distributions and the resulting average errors serve as lower bounds for the expected errors. The resulting errors are 0.29, 0.16, 0.16, and 0.18 bit (<italic>expression</italic>, <italic>gender</italic>, <italic>influence</italic> and <italic>space</italic>). This implies that the deviation from the p-model stays within a factor of 2 of the theoretical lower limit and is consistent with the observation above that the p-model faithfully describes the dependence of stimulus information on the number of bubbles (<xref ref-type="fig" rid="pcbi-1000791-g002">Figure 2</xref>). Hence, bubble information is reliably estimated by the global fit with the p-model.</p>
<fig id="pcbi-1000791-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.g004</object-id><label>Figure 4</label><caption>
<title>Bubble information.</title>
<p>(A) Example stimulus from the <italic>expression</italic> task where the individual bubbles are labeled by their fitted response distributions and the corresponding bubble information. The four numbers above the black line give the response probabilities for the classes “disgusted,” “happy,” “fearful,” and “sad.” The bold number below the line gives the bubble information (in bit). For the whole stimulus, the measured response distribution and stimulus information (in bit) is given in the lower right corner. (B) The distribution of bubble information for the <italic>expression</italic> task. The bubble information of the four bubbles of the example stimulus is marked by colored dots. (C) The distribution of bubble information for the other three tasks <italic>gender</italic>, <italic>influence</italic> and <italic>space</italic>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.g004" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1000791-g004">Figure 4B and C</xref> show the frequencies of bubble information for the four tasks. The majority of bubbles have low bubble information values. Only a few have very high information. Bubble information varies over the whole possible range in all four tasks.</p>
<p>We now investigate the correlation between bubble information and empirical salience (both log transformed, see <xref ref-type="sec" rid="s4">Methods</xref>). <xref ref-type="fig" rid="pcbi-1000791-g005">Figure 5A</xref> shows the example stimulus with the individual bubbles labeled by their bubble information, and <xref ref-type="fig" rid="pcbi-1000791-g005">Figure 5B</xref> shows a scatter plot of bubble information and empirical salience for the <italic>expression</italic> task. Bubble A, located on the right eye, is somewhat informative and looked at very often, placing it in the upper right corner of the plot. Bubble B, located on the smiling mouth, is much more informative than A but looked at only slightly more often, placing it in the upper right corner of the plot, to the right of bubble A. Bubble C, showing hair, has less information than A and B but is still somewhat informative. It is looked at less often than A and B. Bubble D, finally, has almost no information and is also looked at very seldom. In this specific example, bubble information and empirical salience are closely related.</p>
<fig id="pcbi-1000791-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.g005</object-id><label>Figure 5</label><caption>
<title>Relationship of task dependent and empirical salience.</title>
<p>(A) Example stimulus of the <italic>expression</italic> task with individual bubbles labeled by their bubble information. (B) Scatter plot of bubble information and empirical salience for the <italic>expression</italic> task. The positions of the example bubbles are marked by colored dots. The correlation coefficient r is given as a figure inset. (C) Correlation coefficients for all four tasks. Two stars mark highly significant correlations (p&lt;0.01, t-test).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.g005" xlink:type="simple"/></fig>
<p>Investigating the complete set of bubbles, we find that for the <italic>expression</italic> task the correlation of bubble information and empirical salience is highly significant (p&lt;0.01, t-test). Although there is a noticeable drop in correlation for the tasks <italic>gender</italic>, <italic>influence</italic>, and <italic>space</italic>; all are highly significant (p&lt;0.01, t-test) as well (<xref ref-type="fig" rid="pcbi-1000791-g005">Figure 5C</xref>). Hence there are strong correlations between bubble information and empirical salience in all four tasks.</p>
</sec><sec id="s2f">
<title>Correlation of Spatial Arrangement with Fixated Bubbles</title>
<p>We use a generative model to predict the empirical salience of a bubble independent of its visual content, but given its location. The generative model, as defined in the <xref ref-type="sec" rid="s4">Methods</xref> section, predicts gaze trajectories on a stimulus given the initial fixation spot and the spatial arrangement of bubbles. Please note that the spatial arrangement of the bubbles alone does not contain information on the frequency of fixations on different bubbles. The model takes into account the central bias of fixations and geometric constraints on the length and direction of saccades. It does not incorporate an explicit inhibition of return (see <xref ref-type="sec" rid="s3">Discussion</xref>). Both the central bias of fixations and the geometric constraints on saccades are grand averages over a large number of full field stimuli from many different categories (see <xref ref-type="sec" rid="s4">Methods</xref>). The model generates fixation sequences on bubble stimuli. From these sequences the average probabilities of fixating individual bubbles on a stimulus are computed. These only locally valid values are now transformed to a global scale in the same way as the relative frequencies of fixations made by actual subjects were transformed into empirical saliences (see <xref ref-type="sec" rid="s4">Methods</xref>). We now consider the correlation of this spatial bias salience with empirical salience (both log transformed). <xref ref-type="fig" rid="pcbi-1000791-g006">Figure 6A</xref> shows an example of a stimulus from the <italic>expression</italic> task where the individual bubbles are labeled with their spatial bias saliences, and <xref ref-type="fig" rid="pcbi-1000791-g006">Figure 6B</xref> shows a scatter plot of spatial bias salience versus empirical salience. Bubbles A and B are looked at very often and have relatively high spatial bias saliences, which is probably due to the fact that they are close to the center of the stimulus and close to each other. Bubbles C and D, which are farther away from the center and have lower spatial bias saliences, are looked at much more rarely. In this specific example, spatial bias and empirical salience are closely related.</p>
<fig id="pcbi-1000791-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.g006</object-id><label>Figure 6</label><caption>
<title>Relationship of spatial bias and empirical salience.</title>
<p>(A) Example stimulus of the <italic>expression</italic> task with individual bubbles labeled by their spatial bias salience. (B) Scatter plot of spatial bias and empirical salience for the <italic>expression</italic> task. The positions of the example bubbles are marked by colored dots. The correlation coefficient r is given as a figure inset. (C) Correlation coefficients for all tasks. Two stars mark highly significant correlations (p&lt;0.01, t-test).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.g006" xlink:type="simple"/></fig>
<p>For all bubbles of the <italic>expression</italic> task, the correlation between spatial bias salience and empirical salience is highly significant. For the other three tasks, the correlation is highly significant as well (<xref ref-type="fig" rid="pcbi-1000791-g006">Figure 6C</xref>). The correlation of empirical salience with the prediction based on spatial properties is of comparable strength in all four tasks.</p>
</sec><sec id="s2g">
<title>Partializing the Information in Low-Level Stimulus Features, Bubble Information, and Spatial Arrangement</title>
<p>For a combined view we compare the values of all three predictor variables and empirical salience for the example stimulus (<xref ref-type="fig" rid="pcbi-1000791-g007">Figure 7</xref>). Gathering the information from <xref ref-type="fig" rid="pcbi-1000791-g003">Figure 3</xref>, <xref ref-type="fig" rid="pcbi-1000791-g005">5</xref>, and <xref ref-type="fig" rid="pcbi-1000791-g006">6</xref> reveals bubble information as the best predictor (e.g., the order of the bubbles according to bubble information is the same as according to empirical salience), followed by the spatial bias and the stimulus dependent salience. This example is reasonably representative for the <italic>expression</italic> task. In other tasks the contribution of stimulus dependent salience, bubble information, and spatial bias salience is more balanced. However, the individual correlations of empirical salience with the three predictors do not address how much the effects of one of these predictor variables are already addressed by another, because of correlations between individual predictors. In the following we address this question, which is crucial for the investigation of the causal role of the individual predictors.</p>
<fig id="pcbi-1000791-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.g007</object-id><label>Figure 7</label><caption>
<title>Relationship between empirical salience, stimulus dependent salience, bubble information and spatial bias salience for an example stimulus.</title>
<p>The example stimulus from the <italic>expression</italic> task is given on the left. The four values characterizing each bubble are shown on their respective scales (right panel). The range of spanned values for each variable is mapped to the same interval for comparison. The colors code for the identity of the different bubbles.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.g007" xlink:type="simple"/></fig>
<p>We employ a multivariate linear model to predict empirical salience from the joint set of all the predictors. We analyze how well a linear combination of the stimulus dependent salience, bubble information, and spatial bias salience of each bubble can explain the attention it attracts, as reflected by the empirical salience values. As in the pair-wise correlations, we use the log transform of each predictor variable and correlate with the log transform of empirical salience. The model structure is as follows:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.e001" xlink:type="simple"/></disp-formula>To address correlations between individual predictor variables, we use semi-partial correlations, which correlate one predictor with empirical salience while controlling for the effect of all other predictors (compare <xref ref-type="sec" rid="s4">Methods</xref>).</p>
<p><xref ref-type="table" rid="pcbi-1000791-t001">Table 1</xref> gives the results of this correlation analysis for the four tasks, and <xref ref-type="fig" rid="pcbi-1000791-g008">Figure 8</xref> summarizes these results visually. The multivariate regression coefficient (<italic>R</italic>) is highly significant (p&lt;0.01, F-test) for all four tasks, but varies considerably across tasks. For <italic>expression</italic>, the multivariate correlation is highly significant, with 48% of variance explained. Bubble information is the best individual predictor, the pair-wise correlation being highly significant. The individual predictive power of spatial bias salience is smaller, but the pair-wise correlation is still highly significant. Stimulus dependent salience, on the other hand, does not significantly correlate with empirical salience. These results indicate that subjects have much information about where to expect informative bubbles <italic>a priori</italic> and that their attention is guided by this task dependent knowledge. This is exactly what one would expect of a system that is specialized in effectively recognizing facial <italic>expression</italic>. It is clearly inconsistent with a purely bottom-up driven account of overt attention. For the <italic>gender</italic> task, the multivariate correlation coefficient is smaller than for <italic>expression</italic>, but still highly significant, with 27% of variance explained. Spatial bias salience and bubble information have almost the same pair-wise correlation coefficient, both correlations being highly significant. In contrast to the <italic>expression</italic> task, the pair-wise correlation of the stimulus dependent salience is also significant. For the <italic>influence</italic> task, the multivariate correlation is also highly significant, with 36% of variance explained. Again all three predictors show significant, even highly significant, pair-wise correlations. Spatial bias salience has the highest correlation coefficient, followed by bubble information and stimulus dependent salience, the latter two being almost identical. For the <italic>space</italic> task, the multivariate correlation coefficient is smallest, but still highly significant, with 25% of variance explained. Spatial bias salience is the best predictor, followed by bubble information. Both these pair-wise correlations are highly significant. In contrast to <italic>influence</italic>, the correlation coefficient of stimulus dependent salience is very small and not significant.</p>
<fig id="pcbi-1000791-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.g008</object-id><label>Figure 8</label><caption>
<title>Influence of the three factors on empirical salience.</title>
<p>The multivariate regression results are given for all four tasks <italic>expression</italic> (E), <italic>gender</italic> (G), <italic>influence</italic> (I), and <italic>space</italic> (S). The height of each bar depicts the R<sup>2</sup> value; each shaded area represents the squared semi-partial correlation coefficient, which reflects the unique contribution of the respective factor. The white area in each bar represents the amount of variability of empirical salience that can explained by more than one factor.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.g008" xlink:type="simple"/></fig><table-wrap id="pcbi-1000791-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.t001</object-id><label>Table 1</label><caption>
<title>Results of the multivariate regression.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000791-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Task</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Stimulus dependent salience</td>
<td align="left" colspan="1" rowspan="1">Bubble Information</td>
<td align="left" colspan="1" rowspan="1">Spatial bias</td>
<td align="left" colspan="1" rowspan="1">All together</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1"><bold>Expression</bold></td>
<td align="left" colspan="1" rowspan="1">Correlation coefficient <italic>r</italic></td>
<td align="left" colspan="1" rowspan="1"><bold>0.130</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>0.631**</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>0.437**</bold></td>
<td align="left" colspan="1" rowspan="1"/>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Semi-partial correlation coefficient <italic>sr</italic></td>
<td align="left" colspan="1" rowspan="1">0.091</td>
<td align="left" colspan="1" rowspan="1">0.527**</td>
<td align="left" colspan="1" rowspan="1">0.252*</td>
<td align="left" colspan="1" rowspan="1"/>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Multivariate regression</td>
<td align="left" colspan="3" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">R<sup>2</sup> = 0.476**</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><bold>Gender</bold></td>
<td align="left" colspan="1" rowspan="1">Correlation coefficient <italic>r</italic></td>
<td align="left" colspan="1" rowspan="1"><bold>0.235*</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>0.326**</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>0.362**</bold></td>
<td align="left" colspan="1" rowspan="1"/>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Semi-partial correlation coefficient <italic>sr</italic></td>
<td align="left" colspan="1" rowspan="1">0.213*</td>
<td align="left" colspan="1" rowspan="1">0.304**</td>
<td align="left" colspan="1" rowspan="1">0.329**</td>
<td align="left" colspan="1" rowspan="1"/>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Multivariate regression</td>
<td align="left" colspan="3" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">R<sup>2</sup> = 0.269**</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><bold>Influence</bold></td>
<td align="left" colspan="1" rowspan="1">Correlation coefficient <italic>r</italic></td>
<td align="left" colspan="1" rowspan="1"><bold>0.324**</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>0.345**</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>0.456**</bold></td>
<td align="left" colspan="1" rowspan="1"/>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Semi-partial correlation coefficient <italic>sr</italic></td>
<td align="left" colspan="1" rowspan="1">0.324**</td>
<td align="left" colspan="1" rowspan="1">0.155</td>
<td align="left" colspan="1" rowspan="1">0.406**</td>
<td align="left" colspan="1" rowspan="1"/>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Multivariate regression</td>
<td align="left" colspan="3" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">R<sup>2</sup> = 0.360**</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><bold>Space</bold></td>
<td align="left" colspan="1" rowspan="1">Correlation coefficient <italic>r</italic></td>
<td align="left" colspan="1" rowspan="1"><bold>0.067</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>0.290**</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>0.412**</bold></td>
<td align="left" colspan="1" rowspan="1"/>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Semi-partial correlation coefficient <italic>sr</italic></td>
<td align="left" colspan="1" rowspan="1">0.055</td>
<td align="left" colspan="1" rowspan="1">0.243*</td>
<td align="left" colspan="1" rowspan="1">0.401**</td>
<td align="left" colspan="1" rowspan="1"/>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Multivariate regression</td>
<td align="left" colspan="3" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">R<sup>2</sup> = 0.245**</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt101"><label/><p>Pair-wise regression coefficients and semi-partial regression coefficients for the different predictors are given for each task. The total variance of empirical salience that is explained by all three factors is given in the last column. One star marks significant correlations (p&lt;0.05); two stars mark highly significant correlations (p&lt;0.01).</p></fn></table-wrap-foot></table-wrap>
<p>The previous observations on the relative predictive power of individual predictors in the different tasks are also supported by the semi-partial correlation analysis. The only exceptions are a rather large decrease from the pair-wise to the semi-partial correlation for bubble information in the <italic>influence</italic> task, reflecting a rather small unique influence of bubble information on empirical salience, as well as a noticeable decrease of the semi-partial correlation coefficient compared to the pair-wise correlation coefficient for the spatial bias based predictor in the <italic>expression</italic> task.</p>
<p>On the level of individual predictors, we make several observations. Spatial bias salience shows a strong and stable contribution in all four tasks. The unique contribution of bubble information is strong as well, but varies considerably over tasks. In the case of <italic>influence</italic>, it is not even significant. Stimulus dependent salience is the weakest predictor of the three, but shows significant correlations in <italic>gender</italic> and <italic>influence</italic>. Each single predictor shows significant normal and semi-partial correlations in at least some of the tasks. Furthermore, the relative contributions of the predictors, in terms of both uncontrolled and semi-partial correlations, vary considerably over tasks. Hence the contribution of the three different factors is dependent on the task, and none can be generally dismissed in an explanation for guidance of overt attention.</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<p>In this study, we quantify and compare the influence of low-level stimulus features, task dependent features, and spatial biases on overt visual attention. The major achievement is a direct and quantitative comparison of the individual influences of these factors on fixation behavior in a single study. The experimental approach builds on the bubble paradigm as introduced by Gosselin and Schyns <xref ref-type="bibr" rid="pcbi.1000791-Gosselin1">[35]</xref>. It makes use of visual stimuli composed of small image patches, called bubbles, based on face images and forest scenes. Subjects classified stimuli according to facial expression and gender, or according to scenic openness and human influence, respectively. The subjects' eye movements show that bubbles are not scanned for information and verify our assumption that bubbles are treated as perceptual units. To each bubble, we assigned an empirical salience that adequately represents the fixation probability of the bubble. We further quantitatively assessed three factors that are thought to influence visual attention: first, stimulus dependent salience reflecting the probability of fixating a bubble given its luminance and texture contrast; second, bubble information reflecting how much information a bubble contains with respect to the classification task; and third, spatial bias salience reflecting the fixation probability given the location of the bubble. Bubble information was estimated based on the subjects' classification responses to stimuli composed of one or several bubbles using a model of information integration. We showed that this model is a reasonable approximation of the integration. Interestingly, we found that information of individual bubbles is integrated even if bubbles originate from different images of the same class and independent from their spatial arrangement in the case of forest scenes.</p>
<p>Having measured the three factors bubble information, stimulus dependent salience, and spatial bias salience, we then quantified how well they predict fixation behavior. We found that a substantial portion of variance of empirical salience could be explained by all three factors combined, although the share of variance explained varies across tasks. Pair-wise correlations between empirical salience and each of the factors indicate clear differences between the three factors. Empirical salience shows high correlations with spatial bias throughout all four tasks, whereas both the correlations with stimulus dependent salience and bubble information vary strongly with tasks. Stimulus dependent salience is the weakest predictor, but reaches significant levels in the <italic>gender</italic> and <italic>influence</italic> tasks. Bubble information is the best predictor in the <italic>expression</italic> task but for the other tasks it reaches slightly lower correlations with empirical salience than does spatial bias. Surprisingly, the semi-partial correlation coefficients, which reflect the unique contributions of each predictor controlling for the influences of the other factors, are only slightly lower than the pair-wise correlation coefficients. This indicates that all three factors act almost independently on visual attention. In summary, we find that all factors contribute, but that the absolute and relative strength of contribution depends on the task.</p>
<p>We now look into the potentially critical issues and shortcomings of our paradigm. These fall into two overall categories. First, we discuss the validity of our different measures. Second, we analyze how much the results obtained using our bubble paradigm generalize to more natural conditions.</p>
<sec id="s3a">
<title>Validity of Bubble Measures</title>
<sec id="s3a1">
<title>Empirical Salience</title>
<p>One basic assumption of the present approach is that the empirical saliences of different bubbles are independent from each other — i.e. the empirical salience of a bubble is not influenced by any other bubble on the same stimulus. An indicator of a violation of this assumption would be a change of the ratio of fixations falling onto two bubbles when other bubbles were presented simultaneously. We tested whether empirical salience values can predict the average number of fixations made by the subjects onto each bubble in all stimuli. The test resulted in very small errors showing that our assumption of independence between bubbles with respect to empirical salience is not violated.</p>
</sec><sec id="s3a2">
<title>Stimulus dependent salience</title>
<p>We characterized stimulus dependent salience as the conditional probability of fixating on an image patch given its local luminance and texture contrast for several reasons. First, these two low-level features were shown to correlate with fixation behavior in many previous studies <xref ref-type="bibr" rid="pcbi.1000791-Einhuser2">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Ak1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Parkhurst2">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Reinagel1">[42]</xref>. Hence, the present study can be compared directly with this previous work. Second, in an independent study, we observed that the strength of influence of different low-level features on overt visual attention is highly correlated over image categories and tasks (Betz T, Kietzmann TC, Wilming N, König P (in press). Investigating task dependent top-down effects on overt visual attention. J Vis). Hence, the potential benefit of additional features appears small. Third, as a control we compared stimulus dependent salience with a measure of salience obtained by a publicly available model, often used as a baseline <xref ref-type="bibr" rid="pcbi.1000791-Itti1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Walther1">[44]</xref>. Indeed, the correlation of the two sets of saliences is high and in all tasks in the range of 0.4–0.7. Furthermore, the correlation of salience according to the model by Itti and Koch with empirical salience of bubbles is not qualitatively different from the data presented here. Fourth, previous studies showed that luminance contrast influences the response of area V1, but not the response of higher areas <xref ref-type="bibr" rid="pcbi.1000791-Avidan1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Rolls1">[46]</xref>. These results indicate that luminance contrast is a good measure for the relevance of stimulus dependent signals in early visual cortex and justifies the term “low-level”. Fifth, another recent study claims that stimulus dependent salience is well described by luminance contrast without the need to introduce more complex kernels <xref ref-type="bibr" rid="pcbi.1000791-Kienzle1">[47]</xref>. Sixth, texture contrast, which is defined as second-order luminance contrast, is usually considered a low-level feature in that sense as well and triggered some debate in the literature <xref ref-type="bibr" rid="pcbi.1000791-Ak1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Parkhurst2">[41]</xref>. For these reasons we decided to base our characterization of low-level contributions on luminance and texture contrast.</p>
</sec><sec id="s3a3">
<title>Spatial bias salience</title>
<p>We characterized spatial bias salience through a generative model of fixation behavior. The model takes into account the central bias of fixations (0<sup>th</sup> order) and geometric constraints on the length and direction of saccades (1<sup>st</sup> order). While the location of a particular fixation has an influence on the next fixation, we do not model higher order dependencies. Specifically, we do not account for inhibition of return, which would be a 2<sup>nd</sup> order relation of direction and length of saccades. Inhibition of return is characterized as a small delay of saccades that return to the location of a previous fixation. As the current investigation is not concerned with these dynamic aspects, it is not of relevance here. Furthermore, recent studies report that inhibition of return might actually not change viewing strategy for complex scenes <xref ref-type="bibr" rid="pcbi.1000791-Hooge1">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Smith1">[49]</xref>.</p>
</sec><sec id="s3a4">
<title>Bubble information</title>
<p>Estimation of bubble information is based on the complete data set and involves a specific model of information integration. Both issues are considered in turn. In principle it would have been possible to estimate bubble information directly from stimuli presenting single bubbles only. This approach comes, however, with several disadvantages. First, the presentation of only single bubbles as stimuli is rather inefficient. To get reliable estimates of bubble information, each single bubble stimulus would have to be shown much more often. Since a participant cannot respond to the same single bubble stimulus twice and should not see individual bubbles too often, many more participants would be needed. Additionally, the responses on stimuli with several bubbles would be left unused, further diminishing efficiency. Given that in the present study 75 subjects were investigated, more than in any of the eye tracking studies cited above, this issue of efficiency quickly gets prohibitive. Second, using qualitatively different stimuli for computing empirical salience and bubble information potentially introduces systematic biases. For example, the difficulty of the classification task is increased considerably on single bubbles compared to stimuli with several bubbles. This might lead to performance near chance level, which in turn could cause subjects to lose motivation and concentration. Third, for the purpose of the present study our interest is focused on an estimate of bubble information in the context of the stimulus. In the event that estimates of information of isolated bubbles and bubbles in more complex context diverge (e.g. a systematic increase or decrease), the latter would be the relevant measure as it matches the viewing conditions during the task. These reasons further grow our confidence in the validity of the applied methods.</p>
<p>Several models of information integration are conceivable. The mode of information integration is an important topic in its own right and a complete treatment is beyond the scope of the present paper. We assume a probabilistic integration model but also considered two other models of information integration: first a local model that captures stimulus information by the maximally informative bubble, second a global model that differs from the probabilistic model by capturing contra factual evidence for the different choice possibilities. Compared to the p-model these models both show lower performance (see <xref ref-type="supplementary-material" rid="pcbi.1000791.s001">Text S1 D</xref>). Furthermore, under the assumption of the p-model being the true model of information integration, the estimates for bubble information resulting from the global fitting procedure are unbiased and have moderate variance (see <xref ref-type="supplementary-material" rid="pcbi.1000791.s001">Text S1 C</xref> and <xref ref-type="supplementary-material" rid="pcbi.1000791.s002">Figure S1</xref>). This indicates that the predictions based on the p-model are generally good estimates of bubble information.</p>
<p>In conclusion, although we did not show that a probabilistic model for information integration is the true or optimal model we demonstrated that the estimates for stimulus information obtained through it are robust and consistent with the majority of the data. The influence of configural information in face stimuli has been described before and does not pose a problem in the current context. The question of which is the optimal model of information integration is left to be answered by future research.</p>
<p>For these reasons we decided to show stimuli with varying numbers of bubbles in a homogeneous set and to employ the information integration model and global fitting procedure. In so doing we assess the two behavioral measures, bubble information and empirical salience, from the same subjects during the same experimental trials and make optimal use of experimental data to improve the signal to noise level.</p>
</sec><sec id="s3a5">
<title>Effects of bubble position on information integration</title>
<p>The discrepancy between stimulus information in bubble stimuli of condition <italic>same</italic> versus <italic>permuted</italic> in task <italic>expression</italic> (see <xref ref-type="fig" rid="pcbi-1000791-g002">Figure 2</xref>) could have several causes. The faces are similarly positioned in all stimuli so that the location of the bubbles hints at which bubbles contain relevant information: subjects might know a priori where informative regions, e.g. the eyes or the mouth, are located and select fixation targets accordingly. Furthermore, faces are special perceptual stimuli. Specific brain areas are devoted to the processing of face stimuli, and identification can be completely disrupted by reversing a face image <xref ref-type="bibr" rid="pcbi.1000791-Kanwisher1">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Freiwald1">[51]</xref>. Position effects could, therefore, play a more important role for the classification of face images than for the classification of forest scenes <xref ref-type="bibr" rid="pcbi.1000791-Calder1">[39]</xref>. Indeed, a major effect of permutations in the <italic>expression</italic> task is a largely improved fit of the p-model. This indicates that, once the standardized positioning is violated, different bubbles are treated as independent pieces of information, enabling the “normal” mode of information integration. The effect of bubble position is less pronounced in the <italic>gender</italic> task. For the <italic>gender</italic> stimuli, supposedly more regions contain information and the correlation between bubble position and bubble information is weaker. In summary, our data indicate that position effects have some influence in face stimuli, but less so in the forest scenes.</p>
</sec></sec><sec id="s3b">
<title>Generalization to Full Scenes</title>
<p>Do the observed correlations between empirical salience, on the one side, and stimulus dependent salience, bubble information, and spatial bias salience, on the other side generalize to full field images? This is a variation of the eternal question where to place the balance between complex natural conditions and well controlled laboratory stimuli. Here, the answer depends critically on whether the four measures we employ are preserved on full field stimuli. For example, it is decisive whether the empirical salience of image patches measured on full field stimuli is comparable to the empirical salience measured on bubble stimuli. In the same way, bubble information, stimulus dependent salience and spatial bias salience need to be preserved. If the four measures that characterize a bubble were preserved when the bubble is embedded in a full field stimulus then the relationship between the measures, in particular the correlations between them, would be preserved as well and our results should generalize to full scene viewing. We consider this question for each of the measures in turn.</p>
<p>Stimulus based salience, as we defined it, is just dependent on a local image patch. It is thus preserved for full field stimuli. Bubble information measures how much information with respect to a task is contained within a single bubble. The amount of information contained appears largely independent of bubble context and thus only depends on the image patch itself. Spatial bias salience, as we define it, is based on global fixation and saccade biases assessed from a large variety of full field stimuli. Hence, the effect of spatial bias should be largely independent of whether an image patch is embedded into a full field or bubble stimulus. The question of whether the measure of empirical salience is preserved on full field stimuli is more intricate. The observer may very well fixate image regions in the bubble stimuli that would never draw her attention given the complete image. We tested this by correlating empirical salience of bubbles with the fixation densities of the full field images containing those bubbles (r = 0.79, r = 0.75, r = 0.55, r = 0.32 for <italic>expression</italic>, <italic>gender</italic>, <italic>influence</italic>, and <italic>space</italic>, respectively; p&lt;0.01 in all cases). Since empirical salience of individual bubbles is well preserved on full field stimuli, we expect that our findings generalize to full scene viewing.</p>
<p>Previously, it was debated whether the informative regions uncovered by Gosselin and Schyns' bubble paradigm <xref ref-type="bibr" rid="pcbi.1000791-Gosselin1">[35]</xref> are valid for full scene viewing as well. Murray and Gold argue that the bubble stimuli change the information integration strategy employed by the observer <xref ref-type="bibr" rid="pcbi.1000791-Murray1">[52]</xref>. A former study showed that observers used different stimulus regions to identify faces, depending on which regions were covered by Gaussian white noise (Schwartz O, Bayer HM, Pelli DG (1998). Features, frequencies, and facial expressions [ARVO abstract #825]. Investigative Ophthalmology and Visual Science, 39(4)). It is conceivable that for full field images, which include redundant features, observers normally base their classification decision on only one or two of these features. The bubble stimuli force the observers to use different features on different trials, because only small fragments of the stimulus are shown on any given trial (Gosselin and Schyns argue, however, that these concerns are unfounded <xref ref-type="bibr" rid="pcbi.1000791-Gosselin3">[53]</xref>). These potential problems are not relevant for our study since we do not claim that certain bubbles would be used by the observers to solve the classification task on full fields, whereas other bubbles would not. Instead, we quantify the information of each single bubble, i.e., how well the task can be solved given only this bubble. By using the information integration model, we actually incorporate the observer's strategy to use different image regions, depending on which regions are shown. Hence, our measure of task dependent information is not invalidated by the use of bubble stimuli.</p>
<p>In summary, we consider the present experimental paradigm a most sensible compromise, balancing between the complexities of natural conditions and well controlled laboratory stimuli, and suitable for the questions addressed.</p>
</sec><sec id="s3c">
<title>Relationship of Low-Level and High-Level Features to Bottom-Up and Top-Down Neural Signals</title>
<p>One of the most debated issues concerning overt visual attention is the role of bottom-up and top-down signals on a neural level. This issue is not integral part of the results of the current study. In the present study we discuss the influence of stimulus dependent salience and bubble information. Stimulus dependent salience translates directly to low-level stimulus features and to some degree, these features can be identified with bottom-up signals. It has been shown that neurons in V1 are sensitive to these features <xref ref-type="bibr" rid="pcbi.1000791-Avidan1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Rolls1">[46]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Hubel1">[54]</xref>. To reach relevant motor centers and influence eye movements, these signals have to traverse the hierarchy of the visual system <xref ref-type="bibr" rid="pcbi.1000791-Felleman1">[55]</xref>. This may be viewed as a bottom-up process. The second measure, bubble information, relates to high-level features of the visual stimulus interpreted in a specific context. Considering complex response properties in high-level brain areas, these are a natural place to extract such information <xref ref-type="bibr" rid="pcbi.1000791-Tanaka1">[56]</xref>. Again, in view of abundant connectivity, it is plausible that such information is sent down to lower areas of the hierarchy in a top-down manner. However, receptive field properties of neurons in V1 are complex, and non-classic surround effects are far from understood <xref ref-type="bibr" rid="pcbi.1000791-Olshausen1">[57]</xref>. Furthermore, it has been proposed that essential characteristics of a salience map are already captured in the response properties of V1 neurons <xref ref-type="bibr" rid="pcbi.1000791-Li1">[14]</xref>. For that reason we are cautious using the terms <italic>top-down</italic> and <italic>bottom-up signaling</italic>, and we took care not to make unwarranted speculations about the site of the integration of the observed contributions of low-level and high-level stimulus features.</p>
</sec><sec id="s3d">
<title>A Unified Theory of Overt Visual Attention</title>
<p>Many low-level image features were suggested to play an important role for the guidance of visual attention <xref ref-type="bibr" rid="pcbi.1000791-Itti1">[9]</xref>. When compared to random image locations, fixated regions of natural and artificial images are characterized by higher decorrelation of intensities of nearby image points <xref ref-type="bibr" rid="pcbi.1000791-Reinagel1">[42]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Parkhurst3">[58]</xref>, higher luminance contrast <xref ref-type="bibr" rid="pcbi.1000791-Einhuser1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Parkhurst2">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Reinagel1">[42]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Parkhurst3">[58]</xref>, texture contrast <xref ref-type="bibr" rid="pcbi.1000791-Einhuser1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Parkhurst2">[41]</xref>, color contrast <xref ref-type="bibr" rid="pcbi.1000791-Frey1">[59]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Frey2">[60]</xref>, orientation contrast <xref ref-type="bibr" rid="pcbi.1000791-Parkhurst1">[15]</xref>, flicker and motion contrast <xref ref-type="bibr" rid="pcbi.1000791-Carmi1">[20]</xref>, strong statistical dependencies between frequency components of different orientation like curved lines (Saal H, Nortmann N, Krüger N and König P (2006) Salient image regions as a guide for useful visual features. IEEE AICS), edges <xref ref-type="bibr" rid="pcbi.1000791-Tatler2">[18]</xref>, occlusions or isolated spots <xref ref-type="bibr" rid="pcbi.1000791-Krieger1">[61]</xref>, and disparity <xref ref-type="bibr" rid="pcbi.1000791-Jansen1">[62]</xref>. These effects, however, appear to be relatively weak <xref ref-type="bibr" rid="pcbi.1000791-Tatler2">[18]</xref>, and another study reports that locations of extremes of luminance intensity, luminance contrast, high spatial frequency content, and edge density do not match with locations of fixations <xref ref-type="bibr" rid="pcbi.1000791-Mannan1">[63]</xref>. Yet another study puts forward contradicting evidence in favor of the role of high spatial frequency content <xref ref-type="bibr" rid="pcbi.1000791-Baddeley1">[64]</xref>. The strength of these effects was found to vary with image type <xref ref-type="bibr" rid="pcbi.1000791-Parkhurst1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Ak1">[40]</xref>. Still, the idea is that with increasing complexity of the features investigated a faithful description of human overt visual attention can be reached.</p>
<p>This line of research has come under attack from two sides. On the one hand, Kienzle and colleagues show that much of the observed correlation of selected fixation points in a free viewing task on gray-scale images of natural scenes can be captured by an extremely simple center surround mechanism <xref ref-type="bibr" rid="pcbi.1000791-Kienzle1">[47]</xref>. On the other hand, recent studies found that high-level features play an important role in overt visual attention and act more strongly on fixation behavior than low-level features when subjects engage in visual search tasks <xref ref-type="bibr" rid="pcbi.1000791-Chen1">[65]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Einhuser5">[66]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Underwood1">[67]</xref>. In more natural settings, task and context have a strong impact on eye movements as well <xref ref-type="bibr" rid="pcbi.1000791-Rothkopf1">[68]</xref>. Also models of visual attention that employ top-down processing were successfully applied to visual search tasks <xref ref-type="bibr" rid="pcbi.1000791-Tsotsos1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Hamker1">[69]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Navalpakkam2">[70]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Rao1">[71]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Turano1">[72]</xref>. Recent work tries to combine low-level and high-level cues <xref ref-type="bibr" rid="pcbi.1000791-Cristino1">[73]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Vincent1">[74]</xref>. The latter study specifically investigates the salience of light sources (very high luminance contrast) in natural scenes at dawn and dusk. They show that high-level features and spatial biases make the largest contribution in a mixture model, which is in line with the results reported here. However, in the work by Vincent et al. <xref ref-type="bibr" rid="pcbi.1000791-Vincent1">[74]</xref> the definition of high-level features like foreground/background contains a subjective component and might correlate strongly with low-level features like disparity. Indeed, we could recently demonstrate that disparity has a strong influence on the selection of fixation points in stereoscopic presentation <xref ref-type="bibr" rid="pcbi.1000791-Jansen1">[62]</xref>, close regions being viewed earlier than far regions. Furthermore, about 40% of this effect survives in 2D presentation. This highlights the problem to define objectively low-level and high-level cues and to analyze their independent contribution to the guidance of gaze movements. Some experimental studies assessed the informativeness of image regions by subjective ratings <xref ref-type="bibr" rid="pcbi.1000791-Antes1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Mackworth1">[33]</xref>; or they made use of identified informative regions of face images for different tasks <xref ref-type="bibr" rid="pcbi.1000791-Malcolm1">[75]</xref>. In agreement with our data, these investigations show that fixation patterns vary for different tasks even if the visual input is identical — i.e., that high-level features like task dependent information have an influence on attention, and that more informative regions are fixated upon more often than less informative ones. The advantage of our approach is that it enables us to quantitatively measure task dependent information in an objective way. Another study presents an information theoretic approach to the combination of different cues <xref ref-type="bibr" rid="pcbi.1000791-Kanan1">[76]</xref>. They demonstrate that the model clearly outperforms models with pure bottom-up architectures. Furthermore, Ehinger and colleagues give a highly informative comparison with current contextual guidance models <xref ref-type="bibr" rid="pcbi.1000791-Ehinger1">[26]</xref>. Our results are in line with these studies. Averaged over all the tasks investigated, high-level features contribute more than low-level features. Some experimental studies assessed the informativeness of image regions by subjective ratings <xref ref-type="bibr" rid="pcbi.1000791-Antes1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Mackworth1">[33]</xref>; or they made use of identified informative regions of face images for different tasks <xref ref-type="bibr" rid="pcbi.1000791-Malcolm1">[75]</xref>. In agreement with our data, these investigations show that fixation patterns vary for different tasks even if the visual input is identical — i.e., that high-level features like task dependent information have an influence on attention, and that more informative regions are fixated upon more often than less informative ones. The advantage of our approach is that it enables us to quantitatively measure task dependent information in an objective way.</p>
<p>One center issue of the debate about low-level and high-level features is whether, and to what degree, they have a causal role versus pure correlative effects. A study on images whose luminance contrast was locally modified shows that fixations are attracted by increases as well as decreases of luminance contrast, but that the effect within the region of normal variance of luminance contrast is small <xref ref-type="bibr" rid="pcbi.1000791-Einhuser2">[21]</xref>. Furthermore, these observations cannot be explained by induced changes in texture contrast <xref ref-type="bibr" rid="pcbi.1000791-Ak1">[40]</xref>. This argues against a causal effect, but in favor of a pure correlative effect of luminance contrast in a free viewing task on natural stimuli. Our present results agree with the aforementioned studies inasmuch as the low-level factors exhibit, on average, weak effects on fixation behavior. However, our analysis of the correlation of empirical salience with the three predictors uncovers a surprising fact. The semi-partial correlations are only a little smaller than the full correlations. This indicates little redundancy of the three predictors — i.e. low-level features are not coincident correlations of high-level features in many tasks. This argues that none of the predictors can be neglected, but that a true integration is to be achieved. This is very much in the spirit of recent proposals, putting the problem of overt attention in a Bayesian framework <xref ref-type="bibr" rid="pcbi.1000791-Gao1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Zhang1">[12]</xref>.</p>
<p>Concerning the role of spatial biases on visual attention, it was pointed out that the spatial bias towards the screen center has to be taken into account when studying the effect of image features on selection of fixation points <xref ref-type="bibr" rid="pcbi.1000791-Tatler1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Mannan1">[63]</xref>. Furthermore, some work has been done on the statistical properties of saccade length and directions. Human saccades can be modeled as a Levy flight with a heavy-tailed distribution <xref ref-type="bibr" rid="pcbi.1000791-Brockmann1">[8]</xref> and it can be shown that under certain assumptions such a distribution leads to optimal scanning behavior. Research on higher order correlations, i.e. dependencies of selected fixation points within a trajectory, is still rare <xref ref-type="bibr" rid="pcbi.1000791-Tatler5">[77]</xref>. Given our current knowledge of spatial properties, a comparison of several models of fixation behavior revealed that the best performance is obtained from a strategy combining top-down information and spatial bias, which, however, was defined as the restriction of fixations to one side of the image <xref ref-type="bibr" rid="pcbi.1000791-Turano1">[72]</xref>. Our results support this view, showing a surprisingly high correlation between spatial bias and visual attention. This effect is strong and consistent in all tasks tested. This contrasts with the emphasis on low-level and high-level features in current models of visual attention. Forthcoming models should put the spatial properties of eye movements on an equal footing with other factors.</p>
<p>The present study contributes to focusing discussions of models of attention on quantitatively testable properties. Low-level stimulus features, task dependent information content, and spatial viewing biases jointly explain a substantial fraction of the variation of empirical salience — i.e., a unifying theory of visual attention will have large predictive power. Furthermore, each of the three factors contributes significantly. A unified theory of overt visual attention has to account for all of them.</p>
</sec></sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4a">
<title>Ethics Statement</title>
<p>All subjects were informed about the experimental procedure, the eye-tracking device, and their right to withdraw from the experiment at any time. However, they were initially kept naïve as to the purpose of the experiment and were debriefed after the experiment. All participants consented in writing to take part in the experiment and to allow scientific usage of the recorded data. The experimental procedure conformed to the Declaration of Helsinki and national guidelines.</p>
</sec><sec id="s4b">
<title>Participants</title>
<p>75 student volunteers participated in the experiment (39 female, 36 male). Their ages ranged from 18 to 41, with a mean of 24.2 years. All had normal or corrected-to-normal vision, which was confirmed by a vision test with Landolt rings. Participation was voluntary, and participants either were granted extra course credits or received monetary compensation for their participation.</p>
</sec><sec id="s4c">
<title>Apparatus and Recording</title>
<p>Participants' eye movements were recorded with the head-mounted Eyelink II eye-tracking system and the Eyelink II software package (SR Research, Ltd., Mississauga, Ontario, Canada). Monocular eye-position data were sampled with infrared-based tracking only, using a sampling rate of 250 Hz. The saccade classification of the Eyelink system is based on velocity and acceleration. A saccade starts if an initial acceleration threshold of 8000°/s<sup>2</sup> is exceeded and a distance of at least 0.1° is covered with a minimal velocity of 30°/s. Fixation points are then defined by the samples in between two saccades. Stimuli were presented on a 21-inch Samsung Syncmaster 1100 DF 2004 (Samsung Electronics Co. Ltd., Korea) CRT monitor at a distance of 80 cm from the subject, using a display resolution of 1024×786 pixels and a refresh rate of 120 Hz. These settings resulted in a spatial resolution of 33 pixels per degree of visual angle. No headrest was used.</p>
</sec><sec id="s4d">
<title>Stimuli</title>
<p>All stimuli were based on gray-scale face images <xref ref-type="bibr" rid="pcbi.1000791-Tottenham1">[78]</xref> and forest scenes (the forest scene photographs were used with permission from W. Einhäuser and P. König <xref ref-type="bibr" rid="pcbi.1000791-Einhuser2">[21]</xref>). Photographs used for the construction of stimuli were selected on the basis of pre-experiments (forest scenes: Steinwender J (2005) Context dependency of overt attention in natural scenes. Bachelor's thesis, University of Osnabrück; faces: pre-experiment, data not shown). Face images had to be classified in different <bold>tasks</bold> (see below) according to gender (<bold><italic>gender</italic></bold>) and expression (<bold><italic>expression</italic></bold>), forest images according to scenic openness (<bold><italic>space</italic></bold>) and human influence (<bold><italic>influence</italic></bold>). Only photographs that were evaluated consistently by all participants of the pre-experiments were included in the present study. These responses defined the different <bold>classes</bold> used below during stimulus construction. We selected a total of 24 photographs of faces and 36 photographs of forest scenes. The stimulus sets were balanced in the context of each of the four tasks. In 4% of all trials, stimuli were photographs shown in full field condition (<xref ref-type="fig" rid="pcbi-1000791-g009">Figure 9</xref>). Although these full fields were shown during the main experiment to control for changes in classification, their main purpose was to serve as a basis for the creation of bubble stimuli. In 96% of the trials, bubble stimuli constructed from the same basic set of photographs were presented. These were created in three steps. First, 6.0° square patches were selected from the available full field photographs. Second, the image patches were space-variant filtered, imitating the retinal resolution as a function of eccentricity, and masked by a Gaussian envelope. Third, these bubbles were recombined and placed on an equiluminant gray background in different ways to create a variety of bubble stimuli. A total of 2061 gray-scale stimuli were used.</p>
<fig id="pcbi-1000791-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.g009</object-id><label>Figure 9</label><caption>
<title>The different stimulus classes.</title>
<p>Subjects had to classify faces and forest scenes according to four tasks (<italic>expression</italic>, <italic>gender</italic>, <italic>influence</italic>, and <italic>space</italic>). For the forest scenes, the different response possibilities are given above the example stimuli. The stimuli are shown as full fields and are used for bubble stimuli construction. For copy right reasons, we cannot show the face stimuli here but we refer the reader to Tottenham et al. <xref ref-type="bibr" rid="pcbi.1000791-Tottenham1">[78]</xref>. The face stimuli are taken from the “NimStim” stimulus set.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.g009" xlink:type="simple"/></fig>
<p>The selection of image patches from full fields was governed by the following criteria: first, we selected image patches from locations where the fixation density obtained in the pre-experiments was very low or very high. This way of selecting patch positions yields a set of patches with diverse empirical saliences. Second, since bubbles should be independent units of information, they must not overlap. Third, for each bubble on a particular full field stimulus, there should be bubbles on other full field stimuli that occupy the same position. This constraint allowed controlling for position effects when combining bubbles from different full fields. Ideally, some of these bubbles on other full fields should be close to minima and some to maxima of their respective fixation distribution. We used a randomized algorithm to generate an appropriate selection. Since the aligned geometry of the face stimuli made it impossible to fully satisfy the latter constraint, a residual set of bubbles for the face stimuli was selected by hand. The resulting distribution of bubble centers for the <italic>expression</italic> task is shown in <xref ref-type="fig" rid="pcbi-1000791-g010">Figure 10A</xref>.</p>
<fig id="pcbi-1000791-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.g010</object-id><label>Figure 10</label><caption>
<title>Bubble stimuli.</title>
<p>(A) Distribution of bubble positions for the <italic>expression</italic> task. (B) A single bubble based on a patch of 6 visual degrees from a full field face stimulus. The patch was filtered using an eccentricity dependent frequency filter simulating the drop of spatial acuity and a Gaussian mask to avoid edge effects. (C) Different types of bubble stimuli were generated. Stimuli of the <italic>same</italic> condition are built from patches of the same image. Stimuli of the <italic>congruent</italic> and <italic>incongruent</italic> condition are built from patches of different images of the same class or of different classes, respectively. <italic>Permuted</italic> stimuli were created for each of the three conditions by shuffling the positions of bubbles.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.g010" xlink:type="simple"/></fig>
<p>The selected image patches were first filtered using an eccentricity-dependent frequency filter that simulates the decline of visual acuity towards the edges of the visual field as resulting from the non-uniform distribution of photoreceptors on the human retina <xref ref-type="bibr" rid="pcbi.1000791-Sere1">[79]</xref>. This approach ensures that all information present in a bubble can be gained by fixating the bubble center and that scanning bubbles is inefficient. To prevent potential artifacts resulting from sharp apertures, the space-variant filtered patches were masked using an isotropic Gaussian window with a standard deviation of 1.0°. This made the bubbles blend inconspicuously into the gray background. An example is shown in <xref ref-type="fig" rid="pcbi-1000791-g010">Figure 10B</xref>.</p>
<p>The final bubble stimuli were created by combining bubbles. In a small fraction, individual bubbles were shown (12%). The remaining stimuli were composed of two (42%), three (26%), four (14%) or five bubbles (2%). Combining several bubbles, depending on their full field stimulus of origin, allows different <bold>conditions</bold> (<xref ref-type="fig" rid="pcbi-1000791-g010">Figure 10C</xref>). <bold>Same</bold> stimuli (50% of all stimuli, including single bubbles) were composed entirely of bubbles from the same full field image. <bold>Congruent</bold> stimuli (15%) were composed of bubbles from different full fields that were classified in the same way during the pre-experiments (they belong to the same class). <bold>Incongruent</bold> stimuli (15%) were composed of bubbles from full fields of different classes. <bold>Permutations</bold> (16%) were created by shuffling the positions of the bubbles. The final stimulus set was created using a randomized algorithm that optimized the set with respect to the constraint that each individual bubble should appear in the same number of stimuli.</p>
</sec><sec id="s4e">
<title>Classification Tasks</title>
<p>During the experiment, participants classified visual stimuli in four different tasks. In the first task, participants tagged stimuli according to the facial <italic>expression</italic> of the actors into the classes “happy,” “sad,” “fearful,” or “disgusted.” Similarly they classified <italic>gender</italic> into “male” or “female.” For the <italic>space</italic> task, participants were asked to choose between “close and narrow” or “wide and open.” They were instructed to respond “close and narrow” if the image was a close-up or if it would not be possible to leave the scene—for example, if leaves and branches were blocking the view. They were told to respond “open and wide” if it was possible to look far ahead. For the <italic>influence</italic> task, we asked participants to look for indicators of human influence such as houses, roads and paths, trunks of trees, fences, and hewn stones and to classify the stimuli into either “present” or “absent.” The wording of the instructions was the same for all participants.</p>
</sec><sec id="s4f">
<title>Procedure</title>
<p>A complete experimental session lasted approximately one hour. It was divided into four blocks, one for each of the four classification tasks. Face stimuli and forest scene blocks were presented alternately. In the beginning of the experiment, participants were instructed about the procedure, and example bubble stimuli were shown. They were directed to classify the stimuli by pressing numbers on the keyboard's keypad and to take their best guess in cases where they were not sure about the stimulus' class.</p>
<p>Before the beginning of each block, the eye tracker was calibrated, and task and answer choices for that block were explained and exemplified. Each block consisted of 70 trials that were presented in constrained random order (see below). Each trial began with the presentation of a fixation cross in the middle of the screen. Whenever the fixation of the cross indicated a notable decline in tracking quality, the eye tracker was recalibrated. This ensured that the mean tracking error for at least one eye was always lower than 0.4°. If the cross was fixated properly, the conductor of the experiment triggered the stimulus presentation. We excluded the very first fixation from all subsequent analysis, as it directly reflects the preceding fixation of the fixation cross. The trial lasted for 3 seconds and was followed by the answer screen, which stayed on until participants responded by using the keyboard. There was no time limit for the decision. Before the next trial started, visual feedback of the participant's response was given to minimize classification errors due to typos (<xref ref-type="fig" rid="pcbi-1000791-g011">Figure 11</xref>).</p>
<fig id="pcbi-1000791-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.g011</object-id><label>Figure 11</label><caption>
<title>Experimental procedure.</title>
<p>Each trial began with the presentation of a fixation point used for drift correction. Subsequently, the stimulus was presented for 3 seconds. The response screen was displayed until the subject responded to the classification task by pressing one of the indicated keys. The subject's choice was then shown as feedback.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.g011" xlink:type="simple"/></fig>
<p>The stimuli shown to each participant and their order were selected by a randomizing algorithm that respected the following constraints: for each participant, each stimulus was shown at most once; each bubble was presented at most four times; and stimuli with the same bubble were not shown in direct succession. Furthermore, on average, each stimulus should be shown to 8 participants, and the variation in the number of participants that have seen a particular stimulus should be as small as possible.</p>
</sec><sec id="s4g">
<title>Data Analysis</title>
<p>In the following, we first define a measure for the empirical salience of bubbles as quantified by fixation probability. Then we derive measures for the spatial bias, and the stimulus dependent and task dependent effects. These three measures will be used to investigate the relative contributions to the empirical salience of stimuli. All three measures put the bubbles in a global order.</p>
<sec id="s4g1">
<title>Empirical salience</title>
<p>To obtain a global quantification of empirical salience, we assume that on any stimulus <italic>S</italic>, the ratio between the number of fixations at a bubble <italic>A</italic>, <italic>F<sub>S</sub>(A)</italic>, and the number of fixations at another bubble <italic>B</italic>, <italic>F<sub>S</sub>(B)</italic>, is independent of the context in which both are presented. This implies<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.e002" xlink:type="simple"/><label>(1)</label></disp-formula>for any stimulus <italic>S</italic> with bubbles <italic>A</italic> and <italic>B</italic>, where <italic>E<sub>A</sub></italic> and <italic>E<sub>B</sub></italic> are global measures of <italic>empirical salience</italic>, which are independent of stimulus context. From this, it follows that the equation<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.e003" xlink:type="simple"/><label>(2)</label></disp-formula>holds for any stimulus <italic>S</italic> and any bubble <italic>A</italic>. Because every stimulus was presented to several subjects, we have, in fact, several left-hand sides of this equation. We average them for each stimulus and bubble. Next, the resulting equations are grouped into a linear system, and we compute the <italic>empirical salience</italic> as the best approximate solution. We eliminate one degree of freedom by imposing a scale, demanding that all empirical saliences sum to one.</p>
</sec><sec id="s4g2">
<title>Stimulus dependent salience</title>
<p>To characterize the bottom-up contribution to fixation behavior, we use a feature-based salience model. It models the conditional probability of fixating a location of an image, given a set of local low-level image features. Here we consider luminance contrast and texture contrast as features.</p>
<p>Luminance contrast is defined as the standard deviation of the luminance intensity in an image patch, normalized by the mean intensity of the entire image <xref ref-type="bibr" rid="pcbi.1000791-Einhuser1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Reinagel1">[42]</xref>. We calculate it using circular patches weighted by a Gaussian window, <italic>G</italic>, in close analogy to the computation of a bubble. Formally, the luminance contrast of a pixel, <italic>LC(x)</italic>, is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.e004" xlink:type="simple"/><label>(3)</label></disp-formula>where <italic>I(x)</italic> is the map of luminance intensity at each pixel, Δ is the displacement relative to the center of the bubble, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000791.e005" xlink:type="simple"/></inline-formula> is the smooth luminance map obtained by a convolution with a Gaussian of the same size as the Gaussian used in bubble construction. Please note that the normalization deviates from the definition given by Reinagel and Zador <xref ref-type="bibr" rid="pcbi.1000791-Reinagel1">[42]</xref> and Einhäuser et al. <xref ref-type="bibr" rid="pcbi.1000791-Einhuser1">[19]</xref>. In these previous studies luminance contrast was normalized in each individual image. Here, however, the bubble stimuli show only a limited aperture of the respective full field stimulus. Hence varying normalization of bubble stimuli, due to not visible differences in the respective full field stimuli would make contrast values incomparable. Furthermore, in conditions <italic>congruent</italic> and <italic>incongruent</italic>, several different full field stimuli contribute. There is no obvious generalization of an image-specific normalization procedure to these conditions. For these reasons we follow the suggestion of Zhang et al. <xref ref-type="bibr" rid="pcbi.1000791-Zhang1">[12]</xref> and normalize luminance contrast by the mean luminance contrast over all the images of one task (<italic>I<sub>task</sub></italic>). This is based on the assumption that the influence of a bubble's contrast on the viewing behavior depends on the whole range of contrast values appearing in the images of one category. <xref ref-type="fig" rid="pcbi-1000791-g012">Figure 12A</xref> shows a luminance contrast map of one of our full field stimuli.</p>
<fig id="pcbi-1000791-g012" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.g012</object-id><label>Figure 12</label><caption>
<title>Computation of stimulus dependent salience.</title>
<p>For each bubble, stimulus dependent salience was computed by considering the luminance and texture contrast map of the embedding full field (A and C). Luminance and texture contrast at the location of the bubble (marked by red circles for one example bubble) are then mapped to fixation probabilities (red dots). These mappings (B and D) map luminance and texture contrast bins (see text) to fixation probabilities and were obtained in a baseline study using a large number of stimuli from different categories. The resulting fixation probabilities based on luminance and texture contrast were multiplied yielding the stimulus dependent salience.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.g012" xlink:type="simple"/></fig>
<p>Texture contrast is defined as the standard deviation of the luminance contrast values in an image patch, normalized by the mean luminance contrast of the entire image <xref ref-type="bibr" rid="pcbi.1000791-Einhuser1">[19]</xref>. Formally,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.e006" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000791.e007" xlink:type="simple"/></inline-formula> is the map of the Gaussian weighted mean luminance contrasts. Analogous to luminance contrast, we normalize by the mean luminance contrast over all images in one task. The luminance contrast map, <italic>LC</italic>, used for the computation of the texture contrast, is calculated with a Gaussian window of a quarter of the size of a bubble. For the subsequent computation of texture contrasts, the same Gaussian window, <italic>G</italic>, as for bubble creation is used. The luminance contrast and texture contrast of a single bubble are defined as the contrast values at the center of the bubble.</p>
<p>Based on the feature contrasts of each bubble, we now derive a scalar describing the stimulus dependent contribution to fixation probability (<xref ref-type="fig" rid="pcbi-1000791-g012">Figure 12</xref>). In a previous study we investigated the relation of luminance contrast and texture contrast with fixation probability in natural stimuli <xref ref-type="bibr" rid="pcbi.1000791-Ak1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1000791-Schumann2">[43]</xref>. From the observed distribution of selected fixation points and the image statistics, we used Bayes' rule to determine the conditional probability to fixate a given location. Importantly, the data were well described by a model assuming independent contributions of luminance contrast and texture contrast. Here we use this mapping, which originates from an independently obtained data set, to predict fixation probability based on the luminance contrast and texture contrast of the bubble stimuli.</p>
<p>For computational efficiency and optimal usage of data we bin the luminance contrast and texture contrast values of each image. We chose 20 bins with boundaries so that the number of available image locations falling into each bin is constant. Next, the probability of a feature value (luminance contrast or texture contrast) occurring at a fixated location was calculated. Then priors on the image features and fixation locations are computed. The priors on the image features are constant due to the equilibration of the distribution. The priors for the fixation locations were estimated for each image category. Both the feature and fixation location priors were corrected for the spatial viewing biases to obtain a measure based purely on low-level image features. The probability of fixating a location, given its local features, was then estimated using Bayes' rule. Finally, the stimulus dependent salience value of each bubble was calculated as the product of the fixation probabilities based on luminance and texture contrast.</p>
</sec><sec id="s4g3">
<title>Spatial bias salience</title>
<p>As a next step we investigated to what degree the fixation of bubbles can be predicted by a spatial bias towards the screen center <xref ref-type="bibr" rid="pcbi.1000791-Tatler1">[7]</xref> and the statistics of saccade length and orientation <xref ref-type="bibr" rid="pcbi.1000791-Bahill1">[23]</xref>. <xref ref-type="fig" rid="pcbi-1000791-g013">Figure 13</xref> shows the structure of a generative model based on bubble positions and on the parameters of the Gaussian window used for bubble construction (bubble masks), global fixation statistics (central bias), and saccade statistics. Using the specific bubble locations as input to the model is necessary to account for the strong fixation preference towards bubbles found in the experimental data, the very purpose of using bubbles. The fixation and saccade bias maps are derived from empirical data recorded in a previous study of our laboratory using the same experimental (Walter A (2006) Baseline Study on Overt Visual Attention. Bachelor's thesis, University of Osnabrück. Walter showed images of urban scenes/man-made objects, natural images, fractals, and pink noise images under a free viewing condition to 27 participants. We pooled over all her data from all of these categories.). The fixation bias map contains the distribution of fixations in absolute (screen) coordinates; the coordinates of fixations relative to their preceding fixations form the saccade bias map. For each trial, both maps are computed and convolved with a Gaussian kernel, with a standard deviation of 0.5° and then normalized to integral of one. Finally, we average across trials weighting each trial equally independent of the number of fixations made.</p>
<fig id="pcbi-1000791-g013" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000791.g013</object-id><label>Figure 13</label><caption>
<title>Simulation of fixation trajectories based on spatial biases.</title>
<p>Spatial bias salience was computed from simulated fixation trajectories based on the central bias of fixations, saccade statistics, and bubble positions. Given the current fixation location, the next fixation is generated by, first, multiplying the central bias map (A) with the bubble position map (B). Second, the resulting intermediate map (C) is multiplied with the probability distribution over saccade vectors (D) centered at the current fixation. The next fixation is then sampled from the resulting map (E). For example, assuming a fixation of the upper left bubble in panel C, the multiplication (indicated by the white coordinate frame) of the intermediate map (C) and saccade statistics (D) results in the depicted next fixation map (E). Repeating this sampling procedure resulted in the simulated fixation trajectory.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.g013" xlink:type="simple"/></fig>
<p>Based on the three maps, we simulate gaze trajectories of 75 virtual participants in 280 trials each in close analogy to the actual experiments. For each simulated trial, the global stimulus independent fixation map and the stimulus specific bubble mask are combined by point-wise multiplication. This combination results in an intermediate map of the spatial bias specific for the position of the bubbles in the stimulus considered (<xref ref-type="fig" rid="pcbi-1000791-g013">Figure 13C</xref>). Next, the saccade bias map is combined with the intermediate map by first aligning the center of the saccade map with the last fixation location (or the screen center for the first fixation within a trial), then multiplying both maps point-wise and normalizing the result to integral one (<xref ref-type="fig" rid="pcbi-1000791-g013">Figure 13E</xref>). The next simulated fixation is then randomly drawn from that probability distribution. This procedure is repeated until as many simulated fixations are drawn for the simulated trial as were made in the corresponding original trial.</p>
<p>From the simulated data we obtain a scalar measure for the fixation probability of each bubble, independent of the task and the spatial structure of the respective full field images. Instead, this spatial bias salience is based solely on the spatial position of the bubbles and the global properties of fixation points and saccades.</p>
</sec><sec id="s4g4">
<title>Bubble information</title>
<p>To characterize task dependent influences on fixation behavior, we derive a scalar measure for the information a bubble contains with respect to a classification task. First, we assume that each individual bubble is associated with a probability distribution that captures how likely the subjects are to decide for each stimulus class (<italic>response distribution</italic>). If this distribution is flat, the bubble contains no information relevant for classification, and performance of subjects viewing only this bubble would be at chance level. If one of its components is one and all others are zero, then the bubble contains maximal information. This is captured by the entropy of a bubble's response distribution. If <italic>I(B)</italic> denotes the information content and <italic>P<sub>R</sub>(B)</italic> denotes the response distribution of bubble <italic>B</italic>, with entropy <italic>E(P<sub>R</sub>(B))</italic>, then<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.e008" xlink:type="simple"/><label>(5)</label></disp-formula>where <italic>E<sub>max</sub></italic> denotes the maximal entropy that can occur for probability distributions like <italic>P<sub>R</sub>(B)</italic> and depends only on the number of degrees of freedom of <italic>P<sub>R</sub>(B)</italic>. For tasks <italic>expression</italic>, <italic>gender</italic>, <italic>influence</italic>, and <italic>space</italic>, <italic>E<sub>max</sub></italic> is 2, 1, 1, and 1, respectively.</p>
<p>Second, along the same lines we assume that our participants' responses to a stimulus <italic>S</italic> are independent and identically distributed according to the response distribution of the stimulus. In the case of a stimulus <italic>S</italic> composed of a single bubble <italic>B</italic> the distribution of observed answers is an estimate of <italic>P<sub>R</sub>(B)</italic>. To estimate the empirical saliences of single bubbles from measured classification responses to stimuli composed of several bubbles, we need to make an assumption on how the response distributions for single bubbles are related to the joint response distribution of a stimulus containing those bubbles. Here, we assume optimal probabilistic integration of the independent response distributions of single bubbles (<bold>p-model</bold>). We describe the response distribution <italic>P<sub>R</sub>(S)</italic> of a stimulus <italic>S = {B<sub>1</sub>, …, B<sub>n</sub>}</italic> by the function <italic>Z</italic> operating on the individual response distributions <italic>P<sub>R</sub>(B<sub>1</sub>), …, P<sub>R</sub>(B<sub>n</sub>)</italic>.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.e009" xlink:type="simple"/><label>(6)</label></disp-formula>We call <italic>Z</italic> the information integration function. It integrates the response distributions of single bubbles independent of the bubbles' absolute position or their relative arrangement. Furthermore, it does not relate to the visual content of the bubbles. It is defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.e010" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000791.e011" xlink:type="simple"/></inline-formula>, with the summation over different stimulus classes <italic>d</italic>, is the appropriate normalization. <italic>Z</italic> is formally derived by writing the probability for a stimulus <italic>S = {B<sub>1</sub>, …, B<sub>n</sub>}</italic> to be of class <italic>c</italic>, <italic>P<sub>R</sub>(S)[c]</italic>, in terms of the corresponding probabilities for the individual bubbles in <italic>S</italic> to be of class <italic>c</italic>, <italic>P<sub>R</sub>(B<sub>i</sub>)[c]</italic> under the assumption that the individual bubbles are independent.</p>
<p>For each stimulus, we can formulate an equation like (6). Hence for each task, we can formulate as many equations like (6) as there are stimuli in that task. These equations operate on response distributions. Each equation can, however, be transformed into a set of scalar equations by considering the different components of the response distributions (probabilities for the different classes) separately. This yields 1791, 600, 588, and 585 equations for the tasks <italic>expression</italic>, <italic>gender</italic>, <italic>influence</italic> and <italic>space</italic>, respectively (<italic>expression</italic> has four instead of two response possibilities, yielding more scalar equations). This contrasts with 282, 94, 88, and 89 free parameters in the four tasks, equaling the number of bubbles used for stimulus construction in these tasks, multiplied by the number of possible responses minus 1. We solve this over a determined system of non-linear equations by a maximum likelihood method. Details of this fitting procedure are given in <xref ref-type="supplementary-material" rid="pcbi.1000791.s001">Text S1 A</xref>. Finally, we determine estimated bubble information from the estimated response distributions of single bubbles according to equation (5).</p>
</sec><sec id="s4g5">
<title>Correlation analysis</title>
<p>We employ pair-wise correlation analyses (Pearson's correlations) to address the net effect of individual predictor variables. To address how well a linear combination of the stimulus dependent salience, the bubble information, and the spatial bias salience of each bubble can explain the attention it attracts, as reflected by the empirical salience values, we employ a multivariate model. Finally, to correlate one predictor with empirical salience while controlling for the effect of all other predictors, we use semi-partial correlations. For example, when we are interested in the correlation of bubble information and empirical salience controlled for the influence of stimulus-based salience and spatial bias salience, we consider the residuals of a multivariate correlation (with intersection) of stimulus-based salience and spatial bias salience with bubble information. These residuals are the differences between the prediction of the multivariate model and the actual bubble information values. We now correlate these residuals with empirical salience. The result is called the semi-partial correlation coefficient of bubble information and empirical salience.</p>
<p>For all, the simple pair-wise correlation analysis, the multivariate correlation and the semi-partial correlation analysis, we used the log transform of the predictor variables and the log transform of empirical salience. This standard practice <xref ref-type="bibr" rid="pcbi.1000791-Tabachnick1">[80]</xref> has the main effect of making the distributions of the individual variables more normal.</p>
</sec></sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1000791.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.s001" xlink:type="simple"><label>Text S1</label><caption>
<p>Description of the computation of bubble information and of other models of information integration.</p>
<p>(0.30 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000791.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000791.s002" xlink:type="simple"><label>Figure S1</label><caption>
<p>Distributions of bubble entropy estimates for different initial bubble entropies (see text) for the expression task. Each plot accumulates the data for all initial bubble entropies in a small interval around the displayed value. The distributions were obtained from simulations (see text).</p>
<p>(0.20 MB TIF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We want to thank Tobias Krieger and Sven Dähne for conducting several of the eye tracking experiments. We furthermore thank Frank Schumann, Alper Açık and Selim Onat for their input on stimulus dependent salience models, and Jasmin Steinwender who conducted one of the pre-experiments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1000791-Buswell1"><label>1</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Buswell</surname><given-names>GT</given-names></name>
</person-group>             <year>1935</year>             <source>How people look at pictures. A study of the psychology of perception in art</source>             <publisher-loc>Chicago</publisher-loc>             <publisher-name>University of Chicago Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000791-Tootell1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tootell</surname><given-names>RB</given-names></name>
<name name-style="western"><surname>Silverman</surname><given-names>MS</given-names></name>
<name name-style="western"><surname>Switkes</surname><given-names>E</given-names></name>
<name name-style="western"><surname>De Valois</surname><given-names>RL</given-names></name>
</person-group>             <year>1982</year>             <article-title>Deoxyglucose analysis of retinotopic organization in primate striate cortex.</article-title>             <source>Science</source>             <volume>218</volume>             <fpage>902</fpage>             <lpage>904</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Rizzolatti1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rizzolatti</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Riggio</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Dascola</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Umilta</surname><given-names>C</given-names></name>
</person-group>             <year>1987</year>             <article-title>Reorienting attention across the horizontal and vertical meridians: evidence in favor of a premotor theory of attention.</article-title>             <source>Neuropsychologia</source>             <volume>25</volume>             <fpage>31</fpage>             <lpage>40</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Land1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Land</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Mennie</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Rusted</surname><given-names>J</given-names></name>
</person-group>             <year>1999</year>             <article-title>The roles of vision and eye movements in the control of activities of daily living.</article-title>             <source>Perception</source>             <volume>28</volume>             <fpage>1311</fpage>             <lpage>1328</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Yarbus1"><label>5</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Yarbus</surname><given-names>AL</given-names></name>
</person-group>             <year>1967</year>             <source>Eye movements and vision</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Plenum</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000791-Koch1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Ullman</surname><given-names>S</given-names></name>
</person-group>             <year>1985</year>             <article-title>Shifts in selective visual attention: towards the underlying neural circuitry.</article-title>             <source>Hum Neurobiol</source>             <volume>4</volume>             <fpage>219</fpage>             <lpage>227</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Tatler1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tatler</surname><given-names>BW</given-names></name>
</person-group>             <year>2007</year>             <article-title>The central fixation bias in scene viewing: selecting an optimal viewing position independently of motor biases and image feature distributions.</article-title>             <source>J Vis</source>             <volume>7</volume>             <fpage>1</fpage>             <lpage>17</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Brockmann1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Brockmann</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Geisel</surname><given-names>T</given-names></name>
</person-group>             <year>1999</year>             <article-title>Are human scanpaths Levy flights?</article-title>             <source>Artificial Neural Networks</source>             <volume>1</volume>             <fpage>263</fpage>             <lpage>268</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Itti1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Itti</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>
</person-group>             <year>2001</year>             <article-title>Computational modelling of visual attention.</article-title>             <source>Nat Rev Neurosci</source>             <volume>2</volume>             <fpage>194</fpage>             <lpage>203</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Wolfe1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wolfe</surname><given-names>JM</given-names></name>
<name name-style="western"><surname>Horowitz</surname><given-names>TS</given-names></name>
</person-group>             <year>2004</year>             <article-title>What attributes guide the deployment of visual attention and how do they do it?</article-title>             <source>Nat Rev Neurosci</source>             <volume>5</volume>             <fpage>495</fpage>             <lpage>501</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Gao1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gao</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Vasconcelos</surname><given-names>N</given-names></name>
</person-group>             <year>2009</year>             <article-title>Decision-theoretic saliency: computational principles, biological plausibility, and implications for neurophysiology and psychophysics.</article-title>             <source>Neural Comput</source>             <volume>21</volume>             <fpage>239</fpage>             <lpage>271</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Zhang1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zhang</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Tong</surname><given-names>MH</given-names></name>
<name name-style="western"><surname>Marks</surname><given-names>TK</given-names></name>
<name name-style="western"><surname>Shan</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Cottrell</surname><given-names>GW</given-names></name>
</person-group>             <year>2008</year>             <article-title>SUN: A Bayesian framework for saliency using natural statistics.</article-title>             <source>J Vis</source>             <volume>8</volume>             <fpage>32 31</fpage>             <lpage>20</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Itti2"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Itti</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Baldi</surname><given-names>P</given-names></name>
</person-group>             <year>2009</year>             <article-title>Bayesian surprise attracts human attention.</article-title>             <source>Vision Res</source>             <volume>49</volume>             <fpage>1295</fpage>             <lpage>1306</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Li1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Li</surname><given-names>Z</given-names></name>
</person-group>             <year>2002</year>             <article-title>A saliency map in primary visual cortex.</article-title>             <source>Trends Cogn Sci</source>             <volume>6</volume>             <fpage>9</fpage>             <lpage>16</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Parkhurst1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Parkhurst</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Law</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Niebur</surname><given-names>E</given-names></name>
</person-group>             <year>2002</year>             <article-title>Modeling the role of salience in the allocation of overt visual attention.</article-title>             <source>Vision Res</source>             <volume>42</volume>             <fpage>107</fpage>             <lpage>123</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Peters1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Peters</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>Iyer</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Itti</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>
</person-group>             <year>2005</year>             <article-title>Components of bottom-up gaze allocation in natural images.</article-title>             <source>Vision Res</source>             <volume>45</volume>             <fpage>2397</fpage>             <lpage>2416</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Privitera1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Privitera</surname><given-names>CM</given-names></name>
<name name-style="western"><surname>Stark</surname><given-names>LW</given-names></name>
</person-group>             <year>2000</year>             <article-title>Algorithms for Defining Visual Regions-of-Interest: Comparison with Eye Fixations.</article-title>             <source>IEEE Trans Pattern Anal Mach Intell</source>             <volume>22</volume>             <fpage>970</fpage>             <lpage>982</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Tatler2"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tatler</surname><given-names>BW</given-names></name>
<name name-style="western"><surname>Baddeley</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>Gilchrist</surname><given-names>ID</given-names></name>
</person-group>             <year>2005</year>             <article-title>Visual correlates of fixation selection: effects of scale and time.</article-title>             <source>Vision Res</source>             <volume>45</volume>             <fpage>643</fpage>             <lpage>659</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Einhuser1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Einhäuser</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Kruse</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Hoffmann</surname><given-names>KP</given-names></name>
<name name-style="western"><surname>König</surname><given-names>P</given-names></name>
</person-group>             <year>2006</year>             <article-title>Differences of monkey and human overt attention under natural conditions.</article-title>             <source>Vision Res</source>             <volume>46</volume>             <fpage>1194</fpage>             <lpage>1209</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Carmi1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Carmi</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Itti</surname><given-names>L</given-names></name>
</person-group>             <year>2006</year>             <article-title>Visual causes versus correlates of attentional selection in dynamic scenes.</article-title>             <source>Vision Res</source>             <volume>46</volume>             <fpage>4333</fpage>             <lpage>4345</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Einhuser2"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Einhäuser</surname><given-names>W</given-names></name>
<name name-style="western"><surname>König</surname><given-names>P</given-names></name>
</person-group>             <year>2003</year>             <article-title>Does luminance-contrast contribute to a saliency map for overt visual attention?</article-title>             <source>Eur J Neurosci</source>             <volume>17</volume>             <fpage>1089</fpage>             <lpage>1097</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Einhuser3"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Einhäuser</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Rutishauser</surname><given-names>U</given-names></name>
<name name-style="western"><surname>Frady</surname><given-names>EP</given-names></name>
<name name-style="western"><surname>Nadler</surname><given-names>S</given-names></name>
<name name-style="western"><surname>König</surname><given-names>P</given-names></name>
<etal/></person-group>             <year>2006</year>             <article-title>The relation of phase noise and luminance contrast to overt attention in complex visual stimuli.</article-title>             <source>J Vis</source>             <volume>6</volume>             <fpage>1148</fpage>             <lpage>1158</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Bahill1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bahill</surname><given-names>AT</given-names></name>
<name name-style="western"><surname>Adler</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Stark</surname><given-names>L</given-names></name>
</person-group>             <year>1975</year>             <article-title>Most naturally occurring human saccades have magnitudes of 15 degrees or less.</article-title>             <source>Invest Ophthalmol</source>             <volume>14</volume>             <fpage>468</fpage>             <lpage>469</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Tatler3"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tatler</surname><given-names>BW</given-names></name>
<name name-style="western"><surname>Baddeley</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>Vincent</surname><given-names>BT</given-names></name>
</person-group>             <year>2006</year>             <article-title>The long and the short of it: spatial statistics at fixation vary with saccade amplitude and task.</article-title>             <source>Vision Res</source>             <volume>46</volume>             <fpage>1857</fpage>             <lpage>1862</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Schumann1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schumann</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Einhäuser-Treyer</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Vockeroth</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Bartl</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Schneider</surname><given-names>E</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>Salient features in gaze-aligned recordings of human visual input during free exploration of natural environments.</article-title>             <source>J Vis</source>             <volume>8</volume>             <fpage>12 11</fpage>             <lpage>17</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Ehinger1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ehinger</surname><given-names>KA</given-names></name>
<name name-style="western"><surname>Hidalgo-Sotelo</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Torralba</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name>
</person-group>             <year>2009</year>             <article-title>Modeling Search for People in 900 Scenes: A combined source model of eye guidance.</article-title>             <source>Vis cogn</source>             <volume>17</volume>             <fpage>945</fpage>             <lpage>978</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Tatler4"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tatler</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Vincent</surname><given-names>B</given-names></name>
</person-group>             <year>2009</year>             <article-title>The prominence of behavioural biases in eye guidance.</article-title>             <source>Visual Cognition</source>             <volume>99999</volume>             <fpage>1</fpage>             <lpage>26</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Land2"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Land</surname><given-names>MF</given-names></name>
<name name-style="western"><surname>Hayhoe</surname><given-names>M</given-names></name>
</person-group>             <year>2001</year>             <article-title>In what ways do eye movements contribute to everyday activities?</article-title>             <source>Vision Res</source>             <volume>41</volume>             <fpage>3559</fpage>             <lpage>3565</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Navalpakkam1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Navalpakkam</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Itti</surname><given-names>L</given-names></name>
</person-group>             <year>2007</year>             <article-title>Search goal tunes visual features optimally.</article-title>             <source>Neuron</source>             <volume>53</volume>             <fpage>605</fpage>             <lpage>617</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Torralba1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Torralba</surname><given-names>A</given-names></name>
</person-group>             <year>2003</year>             <article-title>Modeling global scene factors in attention.</article-title>             <source>J Opt Soc Am A Opt Image Sci Vis</source>             <volume>20</volume>             <fpage>1407</fpage>             <lpage>1418</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Tsotsos1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tsotsos</surname><given-names>JK</given-names></name>
<name name-style="western"><surname>Culhane</surname><given-names>SM</given-names></name>
<name name-style="western"><surname>Wai</surname><given-names>WYK</given-names></name>
<name name-style="western"><surname>Yuzhong</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Davis</surname><given-names>N</given-names></name>
<etal/></person-group>             <year>1995</year>             <article-title>Modeling visual attention via selective tuning.</article-title>             <source>Artificial Intelligence</source>             <volume>78</volume>             <fpage>507</fpage>             <lpage>545</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Antes1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Antes</surname><given-names>JR</given-names></name>
</person-group>             <year>1974</year>             <article-title>The time course of picture viewing.</article-title>             <source>J Exp Psychol</source>             <volume>103</volume>             <fpage>62</fpage>             <lpage>70</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Mackworth1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mackworth</surname><given-names>NH</given-names></name>
<name name-style="western"><surname>Morandi</surname><given-names>AJ</given-names></name>
</person-group>             <year>1967</year>             <article-title>The gaze selects information details within pictures.</article-title>             <source>Perception and Psychophysics</source>             <volume>11</volume>             <fpage>547</fpage>             <lpage>551</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Einhuser4"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Einhäuser</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Spain</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Perona</surname><given-names>P</given-names></name>
</person-group>             <year>2008</year>             <article-title>Objects predict fixations better than early saliency.</article-title>             <source>J Vis</source>             <volume>8</volume>             <fpage>18 11</fpage>             <lpage>26</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Gosselin1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gosselin</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Schyns</surname><given-names>PG</given-names></name>
</person-group>             <year>2001</year>             <article-title>Bubbles: a technique to reveal the use of information in recognition tasks.</article-title>             <source>Vision Res</source>             <volume>41</volume>             <fpage>2261</fpage>             <lpage>2271</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Gosselin2"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gosselin</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Schyns</surname><given-names>PG</given-names></name>
</person-group>             <year>2002</year>             <article-title>RAP: a new framework for visual categorization.</article-title>             <source>Trends Cogn Sci</source>             <volume>6</volume>             <fpage>70</fpage>             <lpage>77</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Schyns1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schyns</surname><given-names>PG</given-names></name>
<name name-style="western"><surname>Bonnar</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Gosselin</surname><given-names>F</given-names></name>
</person-group>             <year>2002</year>             <article-title>Show me the features! Understanding recognition from the use of visual information.</article-title>             <source>Psychol Sci</source>             <volume>13</volume>             <fpage>402</fpage>             <lpage>409</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Vinette1"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Vinette</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Gosselin</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Schyns</surname><given-names>PG</given-names></name>
</person-group>             <year>2004</year>             <article-title>Spatio-temporal dynamics of face recognition in a flash: it's in the eyes.</article-title>             <source>Cogn Sci</source>             <volume>28</volume>             <fpage>289</fpage>             <lpage>301</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Calder1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Calder</surname><given-names>AJ</given-names></name>
<name name-style="western"><surname>Young</surname><given-names>AW</given-names></name>
<name name-style="western"><surname>Keane</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Dean</surname><given-names>M</given-names></name>
</person-group>             <year>2000</year>             <article-title>Configural information in facial expression perception.</article-title>             <source>J Exp Psychol Hum Percept Perform</source>             <volume>26</volume>             <fpage>527</fpage>             <lpage>551</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Ak1"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Açık</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Onat</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Schumann</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Einhäuser</surname><given-names>W</given-names></name>
<name name-style="western"><surname>König</surname><given-names>P</given-names></name>
</person-group>             <year>2009</year>             <article-title>Effects of luminance contrast and its modifications on fixation behavior during free viewing of images from different categories.</article-title>             <source>Vision Res</source>             <volume>49</volume>             <fpage>1541</fpage>             <lpage>1553</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Parkhurst2"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Parkhurst</surname><given-names>DJ</given-names></name>
<name name-style="western"><surname>Niebur</surname><given-names>E</given-names></name>
</person-group>             <year>2004</year>             <article-title>Texture contrast attracts overt visual attention in natural scenes.</article-title>             <source>Eur J Neurosci</source>             <volume>19</volume>             <fpage>783</fpage>             <lpage>789</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Reinagel1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Reinagel</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Zador</surname><given-names>AM</given-names></name>
</person-group>             <year>1999</year>             <article-title>Natural scene statistics at the centre of gaze.</article-title>             <source>Network</source>             <volume>10</volume>             <fpage>341</fpage>             <lpage>350</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Schumann2"><label>43</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schumann</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Açık</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Onat</surname><given-names>S</given-names></name>
<name name-style="western"><surname>König</surname><given-names>P</given-names></name>
</person-group>             <year>2007</year>             <article-title>Integration of different features in guiding eye-movements.</article-title>             <comment>Proceedings of the 7th Meeting of the German Neuroscience Society/31th Göttingen Neurobiology Conference</comment>          </element-citation></ref>
<ref id="pcbi.1000791-Walther1"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Walther</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>
</person-group>             <year>2006</year>             <article-title>Modeling attention to salient proto-objects.</article-title>             <source>Neural Networks</source>             <volume>19</volume>             <fpage>1395</fpage>             <lpage>1407</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Avidan1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Avidan</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Harel</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Hendler</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Ben-Bashat</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Zohary</surname><given-names>E</given-names></name>
<etal/></person-group>             <year>2002</year>             <article-title>Contrast sensitivity in human visual areas and its relationship to object recognition.</article-title>             <source>J Neurophysiol</source>             <volume>87</volume>             <fpage>3102</fpage>             <lpage>3116</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Rolls1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name>
<name name-style="western"><surname>Baylis</surname><given-names>GC</given-names></name>
</person-group>             <year>1986</year>             <article-title>Size and contrast have only small effects on the responses to faces of neurons in the cortex of the superior temporal sulcus of the monkey.</article-title>             <source>Exp Brain Res</source>             <volume>65</volume>             <fpage>38</fpage>             <lpage>48</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Kienzle1"><label>47</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kienzle</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Franz</surname><given-names>MO</given-names></name>
<name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Wichmann</surname><given-names>FA</given-names></name>
</person-group>             <year>2009</year>             <article-title>Center-surround patterns emerge as optimal predictors for human saccade targets.</article-title>             <source>J Vis</source>             <volume>9</volume>             <fpage>1</fpage>             <lpage>15</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Hooge1"><label>48</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hooge</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Over</surname><given-names>E</given-names></name>
<name name-style="western"><surname>van Wezel</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Frens</surname><given-names>M</given-names></name>
</person-group>             <year>2005</year>             <article-title>Inhibition of return is not a foraging facilitator in saccadic search and free viewing.</article-title>             <source>Vision research</source>             <volume>45</volume>             <fpage>1901</fpage>             <lpage>1908</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Smith1"><label>49</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Smith</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Henderson</surname><given-names>J</given-names></name>
</person-group>             <year>2009</year>             <article-title>Facilitation of return during scene viewing.</article-title>             <source>Visual Cognition</source>             <volume>17</volume>             <fpage>1083</fpage>             <lpage>1108</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Kanwisher1"><label>50</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kanwisher</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Yovel</surname><given-names>G</given-names></name>
</person-group>             <year>2006</year>             <article-title>The fusiform face area: a cortical region specialized for the perception of faces.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>361</volume>             <fpage>2109</fpage>             <lpage>2128</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Freiwald1"><label>51</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Freiwald</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Tsao</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Livingstone</surname><given-names>M</given-names></name>
</person-group>             <year>2009</year>             <article-title>A face feature space in the macaque temporal lobe.</article-title>             <source>Nature neuroscience</source>          </element-citation></ref>
<ref id="pcbi.1000791-Murray1"><label>52</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Murray</surname><given-names>RF</given-names></name>
<name name-style="western"><surname>Gold</surname><given-names>JM</given-names></name>
</person-group>             <year>2004</year>             <article-title>Troubles with bubbles.</article-title>             <source>Vision Res</source>             <volume>44</volume>             <fpage>461</fpage>             <lpage>470</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Gosselin3"><label>53</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gosselin</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Schyns</surname><given-names>PG</given-names></name>
</person-group>             <year>2004</year>             <article-title>No troubles with bubbles: a reply to Murray and Gold.</article-title>             <source>Vision Res</source>             <volume>44</volume>             <fpage>471</fpage>             <lpage>477; discussion 479–482</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Hubel1"><label>54</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hubel</surname><given-names>DH</given-names></name>
<name name-style="western"><surname>Wiesel</surname><given-names>TN</given-names></name>
</person-group>             <year>1961</year>             <article-title>Integrative action in the cat's lateral geniculate body.</article-title>             <source>J Physiol</source>             <volume>155</volume>             <fpage>385</fpage>             <lpage>398</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Felleman1"><label>55</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Felleman</surname><given-names>DJ</given-names></name>
<name name-style="western"><surname>Van Essen</surname><given-names>DC</given-names></name>
</person-group>             <year>1991</year>             <article-title>Distributed hierarchical processing in the primate cerebral cortex.</article-title>             <source>Cereb Cortex</source>             <volume>1</volume>             <fpage>1</fpage>             <lpage>47</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Tanaka1"><label>56</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tanaka</surname><given-names>K</given-names></name>
</person-group>             <year>1996</year>             <article-title>Inferotemporal cortex and object vision.</article-title>             <source>Annu Rev Neurosci</source>             <volume>19</volume>             <fpage>109</fpage>             <lpage>139</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Olshausen1"><label>57</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name>
<name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name>
</person-group>             <year>2005</year>             <article-title>How close are we to understanding v1?</article-title>             <source>Neural Comput</source>             <volume>17</volume>             <fpage>1665</fpage>             <lpage>1699</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Parkhurst3"><label>58</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Parkhurst</surname><given-names>DJ</given-names></name>
<name name-style="western"><surname>Niebur</surname><given-names>E</given-names></name>
</person-group>             <year>2003</year>             <article-title>Scene content selected by active vision.</article-title>             <source>Spat Vis</source>             <volume>16</volume>             <fpage>125</fpage>             <lpage>154</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Frey1"><label>59</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Frey</surname><given-names>HP</given-names></name>
<name name-style="western"><surname>Honey</surname><given-names>C</given-names></name>
<name name-style="western"><surname>König</surname><given-names>P</given-names></name>
</person-group>             <year>2008</year>             <article-title>What's color got to do with it? The influence of color on visual attention in different categories.</article-title>             <source>J Vis</source>             <volume>8</volume>             <fpage>6 1</fpage>             <lpage>17</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Frey2"><label>60</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Frey</surname><given-names>HP</given-names></name>
<name name-style="western"><surname>König</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Einhäuser</surname><given-names>W</given-names></name>
</person-group>             <year>2007</year>             <article-title>The role of first- and second-order stimulus features for human overt attention.</article-title>             <source>Percept Psychophys</source>             <volume>69</volume>             <fpage>153</fpage>             <lpage>161</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Krieger1"><label>61</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Krieger</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Rentschler</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Hauske</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Schill</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Zetzsche</surname><given-names>C</given-names></name>
</person-group>             <year>2000</year>             <article-title>Object and scene analysis by saccadic eye-movements: an investigation with higher-order statistics.</article-title>             <source>Spat Vis</source>             <volume>13</volume>             <fpage>201</fpage>             <lpage>214</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Jansen1"><label>62</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Jansen</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Onat</surname><given-names>S</given-names></name>
<name name-style="western"><surname>König</surname><given-names>P</given-names></name>
</person-group>             <year>2009</year>             <article-title>Influence of disparity on fixation and saccades in free viewing of natural scenes.</article-title>             <source>J Vis</source>             <volume>9</volume>             <fpage>29 21</fpage>             <lpage>19</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Mannan1"><label>63</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mannan</surname><given-names>SK</given-names></name>
<name name-style="western"><surname>Ruddock</surname><given-names>KH</given-names></name>
<name name-style="western"><surname>Wooding</surname><given-names>DS</given-names></name>
</person-group>             <year>1996</year>             <article-title>The relationship between the locations of spatial features and those of fixations made during visual examination of briefly presented images.</article-title>             <source>Spat Vis</source>             <volume>10</volume>             <fpage>165</fpage>             <lpage>188</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Baddeley1"><label>64</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Baddeley</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>Tatler</surname><given-names>BW</given-names></name>
</person-group>             <year>2006</year>             <article-title>High frequency edges (but not contrast) predict where we fixate: A Bayesian system identification analysis.</article-title>             <source>Vision Res</source>             <volume>46</volume>             <fpage>2824</fpage>             <lpage>2833</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Chen1"><label>65</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chen</surname><given-names>X</given-names></name>
<name name-style="western"><surname>Zelinsky</surname><given-names>GJ</given-names></name>
</person-group>             <year>2006</year>             <article-title>Real-world visual search is dominated by top-down guidance.</article-title>             <source>Vision Res</source>             <volume>46</volume>             <fpage>4118</fpage>             <lpage>4133</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Einhuser5"><label>66</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Einhäuser</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Rutishauser</surname><given-names>U</given-names></name>
<name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>
</person-group>             <year>2008</year>             <article-title>Task-demands can immediately reverse the effects of sensory-driven saliency in complex visual stimuli.</article-title>             <source>J Vis</source>             <volume>8</volume>             <fpage>2 1</fpage>             <lpage>19</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Underwood1"><label>67</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Underwood</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Foulsham</surname><given-names>T</given-names></name>
<name name-style="western"><surname>van Loon</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Humphreys</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Bloyce</surname><given-names>J</given-names></name>
</person-group>             <year>2006</year>             <article-title>Eye movements during scene inspection: A test of the saliency map hypothesis.</article-title>             <source>Eur J Cogn Psychol</source>             <volume>18</volume>             <fpage>321</fpage>             <lpage>342</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Rothkopf1"><label>68</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rothkopf</surname><given-names>CA</given-names></name>
<name name-style="western"><surname>Ballard</surname><given-names>DH</given-names></name>
<name name-style="western"><surname>Hayhoe</surname><given-names>MM</given-names></name>
</person-group>             <year>2007</year>             <article-title>Task and context determine where you look.</article-title>             <source>J Vis</source>             <volume>7</volume>             <fpage>16 11</fpage>             <lpage>20</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Hamker1"><label>69</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hamker</surname><given-names>FH</given-names></name>
</person-group>             <year>2004</year>             <article-title>A dynamic model of how feature cues guide spatial attention.</article-title>             <source>Vision Res</source>             <volume>44</volume>             <fpage>501</fpage>             <lpage>521</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Navalpakkam2"><label>70</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Navalpakkam</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Itti</surname><given-names>L</given-names></name>
</person-group>             <year>2005</year>             <article-title>Modeling the influence of task on attention.</article-title>             <source>Vision Res</source>             <volume>45</volume>             <fpage>205</fpage>             <lpage>231</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Rao1"><label>71</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rao</surname><given-names>RP</given-names></name>
<name name-style="western"><surname>Zelinsky</surname><given-names>GJ</given-names></name>
<name name-style="western"><surname>Hayhoe</surname><given-names>MM</given-names></name>
<name name-style="western"><surname>Ballard</surname><given-names>DH</given-names></name>
</person-group>             <year>2002</year>             <article-title>Eye movements in iconic visual search.</article-title>             <source>Vision Res</source>             <volume>42</volume>             <fpage>1447</fpage>             <lpage>1463</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Turano1"><label>72</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Turano</surname><given-names>KA</given-names></name>
<name name-style="western"><surname>Geruschat</surname><given-names>DR</given-names></name>
<name name-style="western"><surname>Baker</surname><given-names>FH</given-names></name>
</person-group>             <year>2003</year>             <article-title>Oculomotor strategies for the direction of gaze tested with a real-world activity.</article-title>             <source>Vision Res</source>             <volume>43</volume>             <fpage>333</fpage>             <lpage>346</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Cristino1"><label>73</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cristino</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Baddeley</surname><given-names>R</given-names></name>
</person-group>             <year>2009</year>             <article-title>The nature of the visual representations involved in eye movements when walking down the street.</article-title>             <source>Visual Cognition</source>             <volume>17</volume>             <fpage>880</fpage>             <lpage>903</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Vincent1"><label>74</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Vincent</surname><given-names>BT</given-names></name>
<name name-style="western"><surname>Baddeley</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Correani</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Troscianko</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Leonards</surname><given-names>U</given-names></name>
</person-group>             <year>2009</year>             <article-title>Do we look at lights? Using mixture modelling to distinguish between low- and high-level factors in natural image viewing.</article-title>             <source>Visual Cognition</source>             <volume>17</volume>             <fpage>856</fpage>             <lpage>879</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Malcolm1"><label>75</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Malcolm</surname><given-names>GL</given-names></name>
<name name-style="western"><surname>Lanyon</surname><given-names>LJ</given-names></name>
<name name-style="western"><surname>Fugard</surname><given-names>AJ</given-names></name>
<name name-style="western"><surname>Barton</surname><given-names>JJ</given-names></name>
</person-group>             <year>2008</year>             <article-title>Scan patterns during the processing of facial expression versus identity: an exploration of task-driven and stimulus-driven effects.</article-title>             <source>J Vis</source>             <volume>8</volume>             <fpage>2 1</fpage>             <lpage>9</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Kanan1"><label>76</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kanan</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Tong</surname><given-names>MH</given-names></name>
<name name-style="western"><surname>Zhang</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Cottrell</surname><given-names>GW</given-names></name>
</person-group>             <year>2009</year>             <article-title>SUN: Top-down saliency using natural statistics.</article-title>             <source>Visual Cognition</source>             <volume>17</volume>             <fpage>979</fpage>             <lpage>1003</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Tatler5"><label>77</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tatler</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Vincent</surname><given-names>B</given-names></name>
</person-group>             <year>2008</year>             <article-title>Systematic tendencies in scene viewing.</article-title>             <source>Journal of Eye Movement Research</source>             <volume>2</volume>             <fpage>5</fpage>          </element-citation></ref>
<ref id="pcbi.1000791-Tottenham1"><label>78</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tottenham</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Tanaka</surname><given-names>JW</given-names></name>
<name name-style="western"><surname>Leon</surname><given-names>AC</given-names></name>
<name name-style="western"><surname>McCarry</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Nurse</surname><given-names>M</given-names></name>
<etal/></person-group>             <year>2009</year>             <article-title>The NimStim set of facial expressions: judgments from untrained research participants.</article-title>             <source>Psychiatry Res</source>             <volume>168</volume>             <fpage>242</fpage>             <lpage>249</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Sere1"><label>79</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sere</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Marendaz</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Herault</surname><given-names>J</given-names></name>
</person-group>             <year>2000</year>             <article-title>Nonhomogeneous resolution of images of natural scenes.</article-title>             <source>Perception</source>             <volume>29</volume>             <fpage>1403</fpage>             <lpage>1412</lpage>          </element-citation></ref>
<ref id="pcbi.1000791-Tabachnick1"><label>80</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tabachnick</surname><given-names>BG</given-names></name>
<name name-style="western"><surname>Fidell</surname><given-names>LS</given-names></name>
</person-group>             <year>2007</year>             <source>Using Multivariate Statistics</source>             <publisher-name>Pearson Education, Inc</publisher-name>          </element-citation></ref>
</ref-list>

</back>
</article>