<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-01833</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002448</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Circuit models</subject>
                <subject>Single neuron function</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Learning and memory</subject>
              <subject>Neural networks</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Storage of Correlated Patterns in Standard and Bistable Purkinje Cell Models</article-title><alt-title alt-title-type="running-head">Storage of Correlated Patterns in Purkinje Models</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Clopath</surname>
            <given-names>Claudia</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Nadal</surname>
            <given-names>Jean-Pierre</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Brunel</surname>
            <given-names>Nicolas</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Laboratory of Neurophysics and Physiology, CNRS and Université Paris Descartes, Paris, France</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Laboratoire de Physique Statistique (CNRS, ENS, UPMC, Univ. Paris Diderot), Ecole Normale Supérieure, Paris, France</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Centre d'Analyse et de Mathématique Sociales (CNRS, EHESS), Ecole des Hautes Etudes en Sciences Sociales, Paris, France</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">claudia.clopath@parisdescartes.fr</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: CC NB. Performed the experiments: CC NB. Analyzed the data: CC NB. Contributed reagents/materials/analysis tools: CC JPN NB. Wrote the paper: CC JPN NB.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>4</month>
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>26</day>
        <month>4</month>
        <year>2012</year>
      </pub-date><volume>8</volume><issue>4</issue><elocation-id>e1002448</elocation-id><history>
        <date date-type="received">
          <day>6</day>
          <month>12</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>13</day>
          <month>2</month>
          <year>2012</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2012</copyright-year><copyright-holder>Clopath et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>The cerebellum has long been considered to undergo supervised learning, with climbing fibers acting as a ‘teaching’ or ‘error’ signal. Purkinje cells (PCs), the sole output of the cerebellar cortex, have been considered as analogs of perceptrons storing input/output associations. In support of this hypothesis, a recent study found that the distribution of synaptic weights of a perceptron at maximal capacity is in striking agreement with experimental data in adult rats. However, the calculation was performed using random uncorrelated inputs and outputs. This is a clearly unrealistic assumption since sensory inputs and motor outputs carry a substantial degree of temporal correlations. In this paper, we consider a binary output neuron with a large number of inputs, which is required to store associations between temporally correlated sequences of binary inputs and outputs, modelled as Markov chains. Storage capacity is found to increase with both input and output correlations, and diverges in the limit where both go to unity. We also investigate the capacity of a bistable output unit, since PCs have been shown to be bistable in some experimental conditions. Bistability is shown to enhance storage capacity whenever the output correlation is stronger than the input correlation. Distribution of synaptic weights at maximal capacity is shown to be independent on correlations, and is also unaffected by the presence of bistability.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>The cerebellum is one of the main brain structures involved in motor learning. Classical theories of cerebellar function assign a crucial role to Purkinje cells (PCs), that are assumed to perform as simple perceptrons. In these theories, PCs should learn to provide an appropriate motor output, given a particular input, encoded by the granule cell (GC) network. This learning is assumed to occur through modifications of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e001" xlink:type="simple"/></inline-formula> synapses, under the control of the climbing fiber input to PCs, which is supposed to carry an error signal. In this paper, we compute storage capacity and distribution of weights in the presence of temporal correlations in inputs and outputs, which are unavoidable in sensory inputs and motor outputs. Furthermore, we study how bistability in the PCs affects capacity and distribution of weights. We find that (1) capacity increases monotonically with both input and output correlations; (2) bistability increases storage capacity, when the output correlation is larger than the input correlation; (3) the distribution of weights at maximal capacity is independent of the degree of temporal correlations, as well as the nature of the output unit (mono or bistable) and is in striking agreement with experimental data.</p>
      </abstract><funding-group><funding-statement>This work has been supported by the French National Research Agency, the ANR (grant ANR-08-SYSC-005). NB and JPN are CNRS members. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="10"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>The cerebellum is heavily involved in learning tasks that requires precise spatio-temporal sequences, such as grasping, precise eye movement, etc. It has long been thought <xref ref-type="bibr" rid="pcbi.1002448-Marr1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Albus1">[2]</xref> that the particular form of learning at work in this structure is supervised learning, whereby the neural system adapts its synaptic weights to reproduce a desired input-output relationship, thanks to an error signal. As such, the cerebellum would be one of the main structures of the central nervous system involved in supervised learning <xref ref-type="bibr" rid="pcbi.1002448-Doya1">[3]</xref>. More precisely, it has been proposed <xref ref-type="bibr" rid="pcbi.1002448-Marr1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Albus1">[2]</xref> that each Purkinje cell (PC) may be seen as a single layer perceptron <xref ref-type="bibr" rid="pcbi.1002448-Rosenblatt1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Minsky1">[5]</xref> - a single binary output neuron, with its <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e002" xlink:type="simple"/></inline-formula> input synapses (see <xref ref-type="fig" rid="pcbi-1002448-g001">Figure 1</xref>). Indeed, the PCs, the sole output of the cerebellar cortex, receive two types of excitatory synaptic inputs: individually weak synaptic inputs from a large number (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e003" xlink:type="simple"/></inline-formula>) of Granule cells (GCs), through the Parallel Fibers (PFs); and a single, very strong input from the inferior olive, through the so-called Climbing Fiber (CF). This strong input is thought to represent the ‘error signal’ similarly to a perceptron - indeed, CF firing rates are in some conditions modulated by the error made by an animal <xref ref-type="bibr" rid="pcbi.1002448-Soetedjo1">[6]</xref>, and it has been shown in vitro that CF activity affects synaptic plasticity <xref ref-type="bibr" rid="pcbi.1002448-Ito1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Jorntell1">[8]</xref>.</p>
      <fig id="pcbi-1002448-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002448.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Simplified model of Purkinje cell.</title>
          <p>A. Simplified sketch of the cerebellar cortex circuit. GC stands for Granule cell, PC for Purkinje cell, PF for Parallel fiber, CF for Climbing fiber. B. Perceptron model: the input layer is composed of GCs, the output unit is the PC. CF represents the error signal. C. Bistable output. If the previous output is 0, the input current needs to be larger than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e004" xlink:type="simple"/></inline-formula> to switch the output to 1. If the previous output is 1, the input current needs to be below <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e005" xlink:type="simple"/></inline-formula> to switch the output to 0.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.g001" xlink:type="simple"/>
      </fig>
      <p>On the theoretical side, a particularly well studied problem is the one of learning random input-output associations by the perceptron. The maximal storage capacity (maximal number of random associations that can be learned per input synapse, in the large <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e006" xlink:type="simple"/></inline-formula> limit) has been computed by several methods <xref ref-type="bibr" rid="pcbi.1002448-Cover1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Gardner1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Hertz1">[11]</xref>. For binary input-output units, unconstrained synaptic weight, and random unbiased associations, the maximal capacity is 2, i.e. the number of associations that can be stored is two times the number of inputs. If synaptic weights are sign-constrained, as one expects in real neurons, the capacity is divided by a factor 2 and becomes equal to 1 <xref ref-type="bibr" rid="pcbi.1002448-Amit1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Kanter1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Nadal1">[14]</xref>. The capacity has also been computed in the presence of robustness constraints, biased associations, and other constraints on synaptic weights <xref ref-type="bibr" rid="pcbi.1002448-Gardner1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Gutfreund1">[15]</xref>. Distributions of synaptic weights at the maximal capacity can also be computed. At maximal capacity, the distribution is a Gaussian when weights are unconstrained, while sign constraints lead to truncated Gaussian distributions, together with a delta function at zero weight synapses <xref ref-type="bibr" rid="pcbi.1002448-Kohler1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Brunel1">[17]</xref>. Brunel et al. <xref ref-type="bibr" rid="pcbi.1002448-Brunel1">[17]</xref> showed that the distribution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e007" xlink:type="simple"/></inline-formula> synaptic weights is in very good agreement with the analytically computed distribution for a perceptron close to maximal capacity, giving further support to the idea that PCs are similar to perceptrons.</p>
      <p>The study of Brunel et al. <xref ref-type="bibr" rid="pcbi.1002448-Brunel1">[17]</xref> considered for simplicity uncorrelated input-output associations. In the case of the cerebellum, the assumption of uncorrelated inputs and outputs is clearly unrealistic, as any naturalistic sensory input or sequence of motor commands will carry a substantial degree of temporal correlations. Moreover, under some conditions, PC dynamics seem to be consistent with a bistable device <xref ref-type="bibr" rid="pcbi.1002448-Yartsev1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Fernandez1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Loewenstein1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Genet1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Williams1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Oldfield1">[23]</xref>. The consequences of temporal correlations, as well as the presence of bistability on the learning capacity of the model remain however to be clarified.</p>
      <p>In this paper, we study the capacity and optimal connectivity in a perceptron network storing correlated input-output associations. More precisely, we study (i) a standard binary perceptron, whose task is to learn a sequence of associations with an arbitrary level of temporal correlations in the inputs and outputs; (ii) a bistable perceptron, again subjected to a correlated sequence of associations. We show that the capacity (maximal number of associations in a learnable sequence) is independent of the correlations in the output if the inputs are not correlated. If the inputs are temporally correlated, the capacity grows with output correlation. The capacity diverges in the limit when both correlations become close to unity. The weight distribution is shown to be independent of the degree of correlation, both in the input and output. It is also found that adding a bistability range increases capacity when the output correlation is larger than the input correlation. The optimal width of the bistability range increases with output correlation. Finally, we show that in order to reach maximal capacity, the error signal (CF) has to change the state of the output unit (PC) in addition to its synapses, consistent with experimental data <xref ref-type="bibr" rid="pcbi.1002448-Loewenstein1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Yartsev1">[18]</xref>.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>Binary perceptron with correlations</title>
        <p>In this section, we investigate storage of associations between temporally correlated input and output sequences. The maximal capacity is defined as the maximal length of a sequence that can be learned per input synapse, or in other words the maximal number of associations composing the sequence. We study a simple Markov chain model for generating the sequences. The sequence to be learned is composed of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e008" xlink:type="simple"/></inline-formula> patterns, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e009" xlink:type="simple"/></inline-formula>. A pattern is given by the state of input cell <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e010" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e011" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e012" xlink:type="simple"/></inline-formula> (Granule cell) and the state of the target output sequence, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e013" xlink:type="simple"/></inline-formula> (Purkinje cell, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e014" xlink:type="simple"/></inline-formula> for target). The patterns are presented always in the same order. For the first pattern in the sequence, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e015" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e016" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e017" xlink:type="simple"/></inline-formula> is the input coding level, i.e. the probability that the granule cell is active in a given pattern. For the following patterns, we have<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e018" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e019" xlink:type="simple"/></inline-formula> measures the correlation between successive input patterns. Note that different input neurons are not correlated. The target outputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e020" xlink:type="simple"/></inline-formula> are generated similarly but with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e021" xlink:type="simple"/></inline-formula> and correlation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e022" xlink:type="simple"/></inline-formula>. In most of the paper we chose <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e023" xlink:type="simple"/></inline-formula>, unless stated otherwise.</p>
        <p>In the perceptron, the output is obtained though a comparison of a weighted sum of the inputs to a threshold,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e024" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e025" xlink:type="simple"/></inline-formula> are the synaptic weights and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e026" xlink:type="simple"/></inline-formula> is the threshold. The Heaviside function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e027" xlink:type="simple"/></inline-formula> is 1 if the argument positive and zero otherwise.</p>
        <p>Correlations defined by Equation 1 make calculations using the replica method <xref ref-type="bibr" rid="pcbi.1002448-Gardner1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Gutfreund1">[15]</xref> extremely involved. The only case in which calculations can be performed easily is with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e028" xlink:type="simple"/></inline-formula>. In this case, one can show that both capacity and distribution of weights are independent of the output correlation. In the more general case, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e029" xlink:type="simple"/></inline-formula>, we resort to numerical simulation.</p>
        <p>For numerical simulations, we chose the variant of the perceptron algorithm used in Brunel et al. <xref ref-type="bibr" rid="pcbi.1002448-Brunel1">[17]</xref>. Namely, the threshold being fixed, the weights are modified according to the standard perceptron learning rule, i.e.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e030" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e031" xlink:type="simple"/></inline-formula> is the learning rate, except that the weights have a lower hard bound at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e032" xlink:type="simple"/></inline-formula>.</p>
        <p>This rule can be shown to be guaranteed to converge to a solution, provided the solution exists, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e033" xlink:type="simple"/></inline-formula> is small enough (see Methods). Interestingly, the rule is in agreement with basic experimental protocols leading to plasticity in slice experiments <xref ref-type="bibr" rid="pcbi.1002448-Jorntell1">[8]</xref>. Indeed, LTD is induced when the CF and the PF are simultaneously active (CF firing more than its average firing rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e034" xlink:type="simple"/></inline-formula>) and LTP when PF fires alone (meaning that CF does not fire, i.e. below <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e035" xlink:type="simple"/></inline-formula>). The plasticity can be written as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e036" xlink:type="simple"/></inline-formula>. It was used to model cerebellar learning in tasks such as the Vestibulo-Ocular Reflex (VOR) adaptation <xref ref-type="bibr" rid="pcbi.1002448-Dean1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Porrill1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Lepora1">[26]</xref>. This learning rule can easily be mapped to the perceptron learning rule as the CF is thought to signal the error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e037" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002448-Marr1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Albus1">[2]</xref>.</p>
        <p><xref ref-type="fig" rid="pcbi-1002448-g002">Figure 2</xref> shows the capacity and distribution of synaptic weights of a binary perceptron storing associations of correlated input/output sequences, for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e038" xlink:type="simple"/></inline-formula>. If the inputs are uncorrelated, the maximal capacity is independent of the output correlation and is equal to 1, as shown analytically (<xref ref-type="fig" rid="pcbi-1002448-g002">Figure 2B</xref>, blue line). This can be understood easily since the classification problem would not change after reshuffling the pattern index <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e039" xlink:type="simple"/></inline-formula>. Second, we find numerically that the capacity is also constant and equal to 1 for uncorrelated inputs and correlated outputs (<xref ref-type="fig" rid="pcbi-1002448-g002">Figure 2C</xref>, blue line). This means that if the output is temporally uncorrelated, temporal correlation in the input does not affect the number of associations the system can learn. However, if the inputs are correlated, the capacity increases with output correlation. We find that the capacity can be well fitted by the function<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e040" xlink:type="simple"/><label>(4)</label></disp-formula>with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e041" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e042" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e043" xlink:type="simple"/></inline-formula>. The intuitive reason is that if the patterns are highly correlated, they become more similar to one another, and thus it is easier to learn them.</p>
        <fig id="pcbi-1002448-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002448.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Standard perceptron storing correlated input/output sequences.</title>
            <p>A. Maximal capacity as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e044" xlink:type="simple"/></inline-formula> (dot: simulation, error bar: standard deviation, line: fit with a function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e045" xlink:type="simple"/></inline-formula>). Inset. Same but plotted as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e046" xlink:type="simple"/></inline-formula> in a loglog scale. B. Maximal capacity as a function of the output correlation for different input correlations. C. Maximal capacity as a function of the input correlation for different output correlations. D. Weight distribution after learning at the maximal capacity for the case of uncorrelated input and output (blue: simulation, green: theory). The theoretical fraction of silent synapses is 0.5. The rest of the distribution is a truncated Gaussian with zero mean and standard deviation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e047" xlink:type="simple"/></inline-formula>. E. Fraction of silent synapses as a function of the output correlation for different input correlations. The theoretical value is 0.5 (green). F. Variance of the weight distribution normalized by the mean synaptic weight, fitted by a truncated Gaussian. The theoretical value is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e048" xlink:type="simple"/></inline-formula> (green). In all simulations, the perceptron has <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e049" xlink:type="simple"/></inline-formula> inputs and the simulations were averaged over 10 trials. The coding level is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e050" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.g002" xlink:type="simple"/>
        </fig>
        <p>Simulations (<xref ref-type="fig" rid="pcbi-1002448-g002">Figure 2E–F</xref>) indicate that the weight distribution at maximal capacity is a truncated Gaussian with 50% of silent synapses, independent of the level of both input and output correlations. This finite fraction of silent synapses is due to the constraint that synapses cannot become negative. During the learning process, some synapses tend to go up, others tend to go down. Some would tend to go to negative values, but become stuck at zero. As one reaches the maximal capacity, a finite fraction of these synapses ends up exactly at zero, while the remaining synapses are distributed according to a truncated Gaussian <xref ref-type="bibr" rid="pcbi.1002448-Brunel1">[17]</xref>.</p>
        <p>We have so far focused on the case <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e051" xlink:type="simple"/></inline-formula>. This is at odds with available data on the activity of granule cells and Purkinje cells in vivo, that shows consistently high firing rates in Purkinje cells, while granule cells tend to fire at much lower rates <xref ref-type="bibr" rid="pcbi.1002448-Chadderton1">[27]</xref>. In <xref ref-type="fig" rid="pcbi-1002448-g003">Figure 3</xref> therefore, we show how the capacity and the number of silent synapses depend on the input and output coding levels. We find that the capacity is independent on the input coding level, but strongly depends on the output coding level, for any correlation level. The capacity increases if the output coding level decreases, and diverges in the limit of a sparse output coding level <xref ref-type="bibr" rid="pcbi.1002448-Gardner1">[10]</xref>. For example, when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e052" xlink:type="simple"/></inline-formula>, the capacity is approximately doubled compared to the case <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e053" xlink:type="simple"/></inline-formula>. Interestingly, the capacity is well fitted by a function which is a product between two terms, one which depends only on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e054" xlink:type="simple"/></inline-formula>, the other only on correlations, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e055" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e056" xlink:type="simple"/></inline-formula> is given by Equation 4. The number of silent synapses is found to be independent on input and output coding levels (<xref ref-type="fig" rid="pcbi-1002448-g003">Figure 3B</xref>), and is therefore independent on all statistical parameters characterizing the associations.</p>
        <fig id="pcbi-1002448-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002448.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Standard perceptron storing correlated input/output sequences with various input and output coding levels as well as robustness parameters.</title>
            <p>A–B. Dependence on coding levels. A. Maximal capacity as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e057" xlink:type="simple"/></inline-formula> for different coding levels (symbols: simulations, error bar: standard deviation). The blue curve shows the fit used for <xref ref-type="fig" rid="pcbi-1002448-g002">Figure 2</xref>, Equation 4. The black curve shows the blue line multiplied by the capacity for uncorrelated patterns with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e058" xlink:type="simple"/></inline-formula>. No robustness constraint is considered. B. Fraction of silent synapses as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e059" xlink:type="simple"/></inline-formula> for different coding levels, no robustness constraint. The theoretical value is 0.5 (green). C–D. Dependence on the robustness parameter. C. Capacity as a function of same input and output correlation for different robustness parameters (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e060" xlink:type="simple"/></inline-formula>). D. Fraction of silent synapses as a function of same input and output correlation for different robustness parameters (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e061" xlink:type="simple"/></inline-formula>). In all simulations, the perceptron has <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e062" xlink:type="simple"/></inline-formula> inputs. Means and standard deviations were computed from 10 independent samples.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.g003" xlink:type="simple"/>
        </fig>
        <p>Experimentally, the fraction of silent synapses was estimated to be about 80% <xref ref-type="bibr" rid="pcbi.1002448-Isope1">[28]</xref>. The fraction of silent synapses is 50% when no robustness constraints are imposed on learning, but it increases if a robustness constraint is introduced <xref ref-type="bibr" rid="pcbi.1002448-Brunel1">[17]</xref>. The robustness parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e063" xlink:type="simple"/></inline-formula> is defined in the following way: for a robust classification, we now need <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e064" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e065" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e066" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e067" xlink:type="simple"/></inline-formula>. In <xref ref-type="fig" rid="pcbi-1002448-g003">Figure 3 C–D</xref>, we show, consistent with previous studies with uncorrelated patterns <xref ref-type="bibr" rid="pcbi.1002448-Gardner1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Brunel1">[17]</xref>, that the capacity decreases when the robustness constraint increases, whereas the fraction of silent synapses increases. Note that for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e068" xlink:type="simple"/></inline-formula>, the capacity can no longer be expressed as a simple product of the capacity for uncorrelated patterns, times <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e069" xlink:type="simple"/></inline-formula>. The increase in capacity as the input and output correlations increase is relatively less pronounced than for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e070" xlink:type="simple"/></inline-formula>. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e071" xlink:type="simple"/></inline-formula>, 80% of silent synapses are found <xref ref-type="bibr" rid="pcbi.1002448-Brunel1">[17]</xref>, consistent with the experimental estimate <xref ref-type="bibr" rid="pcbi.1002448-Isope1">[28]</xref>. This fraction is again independent on both input and output correlation, as shown in <xref ref-type="fig" rid="pcbi-1002448-g003">Figure 3D</xref>.</p>
      </sec>
      <sec id="s2b">
        <title>Bistable perceptron</title>
        <sec id="s2b1">
          <title>Bistable perceptron with correlations in the output and uncorrelated inputs</title>
          <p>In <italic>in vivo</italic> experiments, PCs undergo under some conditions transitions between so-called up and down states. These up and down states are thought to be a manifestation of an intrinsic bistability of the PCs <xref ref-type="bibr" rid="pcbi.1002448-Yartsev1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Loewenstein1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Williams1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Oldfield1">[23]</xref> but see <xref ref-type="bibr" rid="pcbi.1002448-Schonewille1">[29]</xref>. The computational advantage of bistability in PCs remains however an open question. We argue here that bistable PCs can serve to increase memory storage if the correlation in the output is larger than the correlation in the input. More precisely, we use a binary perceptron where the output is bistable, i.e. it depends on past history: to switch the cell from 0 to 1, the input current should be larger than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e072" xlink:type="simple"/></inline-formula>, while to switch it from 1 to 0, it should be smaller than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e073" xlink:type="simple"/></inline-formula>. Hence, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e074" xlink:type="simple"/></inline-formula> is the size of the bistable range (see <xref ref-type="fig" rid="pcbi-1002448-g001">Figure 1B</xref>). For the patterns to be learned, we now need to find synaptic weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e075" xlink:type="simple"/></inline-formula> such that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e076" xlink:type="simple"/><label>(5)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e077" xlink:type="simple"/><label>(6)</label></disp-formula></p>
          <p>To investigate how the capacity depends on temporal correlations in the output, we consider sequences of patterns generated from a Markov chain as defined in the previous section, Equation 1.</p>
          <p>The analytical calculation for correlated output and uncorrelated inputs (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e078" xlink:type="simple"/></inline-formula>) is described in the Method section in detail. Both capacity and distribution of synaptic weight are computed using the replica method <xref ref-type="bibr" rid="pcbi.1002448-Gardner1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Gutfreund1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Kohler1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Brunel1">[17]</xref>. The results are shown in <xref ref-type="fig" rid="pcbi-1002448-g004">Figure 4</xref>. For a given value of output correlation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e079" xlink:type="simple"/></inline-formula>, there is an optimal bistable range that maximizes the capacity. When correlations are present in the output, the probability that the state of the cell remains unchanged from one pattern to the next is higher than the probability that it changes. Bistability tends to favor stability of the output in its previous state, and thus makes it easier for the system to learn such input/output associations.</p>
          <fig id="pcbi-1002448-g004" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002448.g004</object-id>
            <label>Figure 4</label>
            <caption>
              <title>Bistable perceptron with correlated output, but no input correlation.</title>
              <p>A. Maximal capacity as a function of the bistability range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e080" xlink:type="simple"/></inline-formula>, for different values of the output correlation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e081" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e082" xlink:type="simple"/></inline-formula> (line: theoretical results, symbols: simulations, error bars: standard deviation). The grey line shows the results of simulations with a different learning rule where the CF does not change the state of the PC, called the “no state switching” rule (NSS)(see Methods section). B. Fraction of silent synapses as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e083" xlink:type="simple"/></inline-formula> for different values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e084" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e085" xlink:type="simple"/></inline-formula>. C. Maximal capacity as a function of the bistability range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e086" xlink:type="simple"/></inline-formula>, for different values of coding level and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e087" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e088" xlink:type="simple"/></inline-formula> is defined as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e089" xlink:type="simple"/></inline-formula> (see Methods). For the simulations, the network is composed of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e090" xlink:type="simple"/></inline-formula> inputs. Means and standard deviations were computed from 10 independent samples.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.g004" xlink:type="simple"/>
          </fig>
          <p><xref ref-type="fig" rid="pcbi-1002448-g004">Figure 4</xref> also shows that the maximal capacity at the optimal bistable range grows with output correlation. Furthermore, the optimal bistable range also grows with output correlations - so that if the target outputs are highly correlated, the best strategy is to have a large bistabile range. Conversely, the optimal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e091" xlink:type="simple"/></inline-formula> is equal to zero for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e092" xlink:type="simple"/></inline-formula>. The weight distribution has the same stereotypical form as in the standard case with a large number of silent synapses. Interestingly, for any output correlation, the fraction of silent synapses is constant and equal to 50% at the optimal bistable range (see <xref ref-type="fig" rid="pcbi-1002448-g004">Figure 4B</xref>). Here no robustness constraint is considered.</p>
          <p><xref ref-type="fig" rid="pcbi-1002448-g004">Figure 4C</xref> shows how the capacity depends on input and output coding levels. As expected, the capacity is increased when the output coding level decreases. Interestingly, for a fixed bistable range, the capacity also depends on the input coding level. The optimal bistable range increases when the input coding level decreases. However, the capacity at the optimal bistable range is independent on the input coding level.</p>
          <p>We then numerically confirm the theoretical results using a perceptron learning algorithm (<xref ref-type="fig" rid="pcbi-1002448-g004">Figure 4</xref>). The learning rule is defined as previously (Equation 3, with the same constraints on the weights and threshold). However, here the error signal not only influences the weight change but also the state of the output. The output therefore switches to match the target output if there is an error after each pattern presented. Then, when the next pattern is presented, the output depends on the previous pattern which is guaranteed to be correct (see Method section for details).</p>
          <p>If the CF does not change the state of the PCs, the simulations does not reach maximal capacity (<xref ref-type="fig" rid="pcbi-1002448-g004">Figure 4A</xref>, grey dashed line). The intuitive reason is that, if the current PC state is wrong, the next state is going to be wrongly influenced by the wrong current state due to bistability.</p>
        </sec>
        <sec id="s2b2">
          <title>Bistable perceptron with input/output correlations</title>
          <p>In this section, we simulate numerically the bistable perceptron with correlated input and output (<xref ref-type="fig" rid="pcbi-1002448-g005">Figure 5</xref>). When correlation in the input increases, the optimal bistable range decreases. Intuitively, temporal correlations in the input will automatically produce temporal correlations in the output. Therefore, if the correlation in the input is stronger, a smaller bistability is needed. Additionally, when correlation in the input is higher than the correlation in the output, the maximal capacity is maximized without bistability. Capacity is therefore enhanced through bistability only if the correlation in the output is larger than the correlation in the input. Again, this is understood by the fact that bistability introduces naturally more correlations in the output than what is in the input.</p>
          <fig id="pcbi-1002448-g005" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002448.g005</object-id>
            <label>Figure 5</label>
            <caption>
              <title>Bistable perceptron with correlated input/output.</title>
              <p>A. Capacity as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e093" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e094" xlink:type="simple"/></inline-formula> and different <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e095" xlink:type="simple"/></inline-formula>. B. Fraction of silent synapses at the maximal capacity as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e096" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e097" xlink:type="simple"/></inline-formula> and for different <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e098" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e099" xlink:type="simple"/></inline-formula> is defined as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e100" xlink:type="simple"/></inline-formula>. The network is composed of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e101" xlink:type="simple"/></inline-formula> inputs with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e102" xlink:type="simple"/></inline-formula>. Simulations were repeated 10 times (error bar: standard deviation).</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.g005" xlink:type="simple"/>
          </fig>
        </sec>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>In this paper, we reconsidered the problem of learning random input-output associations in a perceptron with excitatory weights, considered as a model for cerebellar Purkinje cells. We computed the storage capacity, and distribution of synaptic weights, in two distinct models that are subjected to correlated input-output associations, described as Markov chains: a standard binary perceptron; and a bistable perceptron.</p>
      <p>We find that the maximal capacity increases monotonically when both input and output correlations are increased. The capacity diverges in the limit when both go to unity. This divergence of the capacity is reminiscent of the divergence of the capacity of perceptrons storing uncorrelated input-output associations in the limit when the output coding level <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e103" xlink:type="simple"/></inline-formula> goes to one <xref ref-type="bibr" rid="pcbi.1002448-Gardner1">[10]</xref>. In the bistable perceptron, we find that the capacity is optimal for a non-zero bistable range, whenever the output correlation is larger than the input correlation. This result can be understood intuitively by the fact that bistability will automatically generate additional temporal correlations in the output of a neuron. A bistable neuron is therefore better equipped to learn such input/output associations, compared to a standard perceptron.</p>
      <p>Interestingly, Purkinje cells are known to exhibit bistability in vitro <xref ref-type="bibr" rid="pcbi.1002448-Williams1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Loewenstein1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Fernandez1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Oldfield1">[23]</xref> and their dynamics in vivo has been shown to be compatible with a bistable unit, at least under some conditions <xref ref-type="bibr" rid="pcbi.1002448-Loewenstein1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Yartsev1">[18]</xref> (but see <xref ref-type="bibr" rid="pcbi.1002448-Schonewille1">[29]</xref>). Our results suggest that this bistable behavior might help Purkinje cells to achieve a higher capacity. We further speculate that different areas of the cerebellum might use cells with different degrees of bistability, depending on the temporal correlations imposed upon these areas. Our results also suggests that to optimally use bistability, a learning rule leading to optimal capacity should implement a mechanism that switches the state of the neuron in the case of an error. This switching mechanism fits perfectly with the properties of the climbing fiber (CF) input. Indeed, CF inputs (the putative error signal in PCs) have been able to switch Purkinje cells both from the down to the up state, and from the up to the down state <xref ref-type="bibr" rid="pcbi.1002448-Loewenstein1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Yartsev1">[18]</xref>.</p>
      <p>We also found that the distribution of synaptic weights at the maximal capacity is independent on the degree of correlations in the input and output, for both standard and bistable perceptrons. It is also independent on the input and output coding levels. This distribution is composed of a finite fraction of zero-weight (silent) synapses, and a truncated Gaussian distribution for positive weights. As shown in <xref ref-type="bibr" rid="pcbi.1002448-Brunel1">[17]</xref>, such a distribution fits very well data from paired recordings in cerebellar slices <xref ref-type="bibr" rid="pcbi.1002448-Isope1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Brunel1">[17]</xref>. Our results suggest that such a distribution might be a universal property of neural systems storing information with excitatory synapses, close to maximal capacity <xref ref-type="bibr" rid="pcbi.1002448-Barbour1">[30]</xref>.</p>
      <p>The learning algorithm that we used is in good qualitative agreement with standard protocols used to induce plasticity in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e104" xlink:type="simple"/></inline-formula> synapses. This algorithm can be proved to converge to a solution of the learning problem, provided such a solution exists (see appendix). For the algorithm to converge, changes induced by an individual pattern must be extremely small (of the order of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e105" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e106" xlink:type="simple"/></inline-formula> is the number of inputs). It is unclear whether such small changes can be induced at this synapse. If individual synaptic changes are not small, then maximal capacity will not be reached with such an algorithm. It would be interesting to investigate the capacity of algorithms in which synaptic changes are of order 1, rather than of the order of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e107" xlink:type="simple"/></inline-formula>.</p>
      <p>We have focused on the GC<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e108" xlink:type="simple"/></inline-formula>PC feedforward network. Many other sites of plasticity have been identified in the cerebellum, including in the deep cerebellar and medial vestibular nuclei, and in interneurons of the molecular layer that provide feedforward inhibition to PCs (see e.g. <xref ref-type="bibr" rid="pcbi.1002448-Hansel1">[31]</xref>). It remains to be investigated how interactions between these different plasticity sites allows the cerebellum to optimize its learning capabilities.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Variant of the perceptron algorithm for positive weights, fixed threshold and 0,1 units: Proof of convergence</title>
        <p>The conditions for storing associations can be expressed as,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e109" xlink:type="simple"/></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e110" xlink:type="simple"/><label>(7)</label></disp-formula>Defining<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e111" xlink:type="simple"/><label>(8)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e112" xlink:type="simple"/><label>(9)</label></disp-formula>equation (7) can be rewritten as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e113" xlink:type="simple"/><label>(10)</label></disp-formula></p>
        <p>The constraint on the weights are<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e114" xlink:type="simple"/><label>(11)</label></disp-formula></p>
        <p>One can write the perceptron algorithm with sign constraint as:</p>
        <list list-type="bullet">
          <list-item>
            <p>(0) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e115" xlink:type="simple"/></inline-formula>; start with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e116" xlink:type="simple"/></inline-formula>;</p>
          </list-item>
          <list-item>
            <p>(1) pick a pattern <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e117" xlink:type="simple"/></inline-formula> at random; if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e118" xlink:type="simple"/></inline-formula> then, for each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e119" xlink:type="simple"/></inline-formula>,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e120" xlink:type="simple"/><label>(12)</label></disp-formula>Increase <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e121" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e122" xlink:type="simple"/></inline-formula> if a change have been made (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e123" xlink:type="simple"/></inline-formula>). This means that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e124" xlink:type="simple"/></inline-formula> measures the number of presented patterns for which changes had to be made, rather than the total number of presented patterns.</p>
          </list-item>
          <list-item>
            <p>Go to (1)</p>
          </list-item>
        </list>
        <p>The principle of the proof of convergence is as follows. Let us suppose that there exists a solution to the learning task with positive weights. In other words, we assume there exists a set of weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e125" xlink:type="simple"/></inline-formula> and a stability parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e126" xlink:type="simple"/></inline-formula> such that for every <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e127" xlink:type="simple"/></inline-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e128" xlink:type="simple"/><label>(13)</label></disp-formula>is satisfied.</p>
        <p>As in the standard case (with unconstrained weights), one computes the cosine of the angle between the weight vectors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e129" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e130" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e131" xlink:type="simple"/><label>(14)</label></disp-formula>One shows that this quantity increases monotonically with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e132" xlink:type="simple"/></inline-formula>, so that it becomes larger than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e133" xlink:type="simple"/></inline-formula>, which is not possible: hence after some finite value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e134" xlink:type="simple"/></inline-formula> there is no pattern for which a learning step has to be made.</p>
        <p>We write <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e135" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e136" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e137" xlink:type="simple"/></inline-formula> according to (12), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e138" xlink:type="simple"/></inline-formula> being the pattern learnt at step <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e139" xlink:type="simple"/></inline-formula>,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e140" xlink:type="simple"/><label>(15)</label></disp-formula>This can be rewritten as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e141" xlink:type="simple"/><label>(16)</label></disp-formula>where the last term in the r.h.s. is specific to the learning of patterns for which the desired output is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e142" xlink:type="simple"/></inline-formula>.</p>
        <p>From the hypothesis that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e143" xlink:type="simple"/></inline-formula> is a solution, one has<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e144" xlink:type="simple"/></disp-formula>so that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e145" xlink:type="simple"/><label>(17)</label></disp-formula></p>
        <p>One proceeds similarly for the norm:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e146" xlink:type="simple"/></disp-formula><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e147" xlink:type="simple"/></inline-formula> being either <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e148" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e149" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e150" xlink:type="simple"/></inline-formula> in the later case, one has <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e151" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e152" xlink:type="simple"/></inline-formula> is the maximal fraction of active inputs.</p>
        <p>To get a bound on the scalar product <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e153" xlink:type="simple"/></inline-formula> one proceeds as in Equation 16,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e154" xlink:type="simple"/></disp-formula>This leads to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e155" xlink:type="simple"/><label>(18)</label></disp-formula></p>
        <p>Since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e156" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e157" xlink:type="simple"/></inline-formula> is smaller than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e158" xlink:type="simple"/></inline-formula> in the sum over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e159" xlink:type="simple"/></inline-formula>,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e160" xlink:type="simple"/><label>(19)</label></disp-formula>From Equation 18, we have<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e161" xlink:type="simple"/><label>(20)</label></disp-formula>Making use of this inequality, one gets from Equation 17 the bound<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e162" xlink:type="simple"/><label>(21)</label></disp-formula>In the sum over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e163" xlink:type="simple"/></inline-formula>, one has <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e164" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e165" xlink:type="simple"/></inline-formula>, and a contribution only from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e166" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e167" xlink:type="simple"/></inline-formula>. Hence a crude lower bound on this sum is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e168" xlink:type="simple"/></disp-formula>Putting everything together, we find<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e169" xlink:type="simple"/><label>(22)</label></disp-formula>If we choose <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e170" xlink:type="simple"/></inline-formula> small enough, that is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e171" xlink:type="simple"/><label>(23)</label></disp-formula>then the right hand side of Equation 22 becomes larger than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e172" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e173" xlink:type="simple"/></inline-formula> larger than<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e174" xlink:type="simple"/></disp-formula>This means that the algorithm converges after a number of learning steps smaller than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e175" xlink:type="simple"/></inline-formula>.</p>
        <p>Note that this proof of convergence of the sign-constrained perceptron is distinct from the one of Amit et al. <xref ref-type="bibr" rid="pcbi.1002448-Amit1">[12]</xref>. Amit et al.consider <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e176" xlink:type="simple"/></inline-formula> input and output units, and a threshold which is zero. In our case, the units are 0,1, and the threshold is strictly positive. This imposes a constraint on the learning rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e177" xlink:type="simple"/></inline-formula>, which is not present in the case of Amit et al. <xref ref-type="bibr" rid="pcbi.1002448-Amit1">[12]</xref>.</p>
      </sec>
      <sec id="s4b">
        <title>Calculation of the capacity of a bistable perceptron</title>
        <p>The capacity is defined as the maximal number of random associations that can be learned per input synapse. The capacity of a perceptron with bistable output, where the target output is correlated and the inputs are uncorrelated, can be computed analytically, using the replica method <xref ref-type="bibr" rid="pcbi.1002448-Gardner1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002448-Gutfreund1">[15]</xref>. The calculation of weight distribution can also be computed with the same method. Both calculations are similar to the calculations described in the supplementary information of Brunel et al. <xref ref-type="bibr" rid="pcbi.1002448-Brunel1">[17]</xref> (called BSI in the following). The idea, introduced by Elizabeth Gardner <xref ref-type="bibr" rid="pcbi.1002448-Gardner1">[10]</xref>, is to consider the space of all possible couplings. In this space, only a subspace of weights satisfy the constraints imposed by learning. These constraints are<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e178" xlink:type="simple"/><label>(24)</label></disp-formula>where we have introduce a robustness parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e179" xlink:type="simple"/></inline-formula> (set to zero in all the <xref ref-type="sec" rid="s2">results</xref> section). The probabilities of the four distinct sets of pairs of successive outputs are<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e180" xlink:type="simple"/><label>(25)</label></disp-formula>Note that in the large <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e181" xlink:type="simple"/></inline-formula> limit, if we take <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e182" xlink:type="simple"/></inline-formula>, the synaptic weights need to scale as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e183" xlink:type="simple"/></inline-formula>, while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e184" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e185" xlink:type="simple"/></inline-formula> both have to scale as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e186" xlink:type="simple"/></inline-formula>. We therefore define <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e187" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e188" xlink:type="simple"/></inline-formula>.</p>
        <p>The ‘typical’ volume of the subspace of weights satisfying Equations 24 can then be computed, as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e189" xlink:type="simple"/></inline-formula>. The maximal capacity is obtained as the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e190" xlink:type="simple"/></inline-formula> for which the typical volume vanishes. This is done using the replica method. This method consists in calculating the average volume of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e191" xlink:type="simple"/></inline-formula> independent replicas of the system (average here means average over the distribution of the stored patterns),<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e192" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e193" xlink:type="simple"/></inline-formula> is the stability of pattern <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e194" xlink:type="simple"/></inline-formula> in replica <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e195" xlink:type="simple"/></inline-formula>, defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e196" xlink:type="simple"/></disp-formula>and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e197" xlink:type="simple"/></inline-formula> is the Heaviside function that imposes constraints (Equations 24).</p>
        <p>The calculation follows a standard procedure. One first introduces integral representations for the Heaviside functions, which allows to average over the patterns. Then, one introduces order parameters<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e198" xlink:type="simple"/><label>(26)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e199" xlink:type="simple"/><label>(27)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e200" xlink:type="simple"/><label>(28)</label></disp-formula>together with conjugate parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e201" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e202" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e203" xlink:type="simple"/></inline-formula>. We then use a replica-symmetric ansatz (all the order parameters are taken to be independent of replica index <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e204" xlink:type="simple"/></inline-formula>), perform the limit <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e205" xlink:type="simple"/></inline-formula> and obtain<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e206" xlink:type="simple"/><label>(29)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e207" xlink:type="simple"/><label>(30)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e208" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e209" xlink:type="simple"/></inline-formula>, while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e210" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e211" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e212" xlink:type="simple"/></inline-formula> is the Gaussian measure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e213" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e214" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e215" xlink:type="simple"/></inline-formula>.</p>
        <p>In the large <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e216" xlink:type="simple"/></inline-formula> limit, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e217" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e218" xlink:type="simple"/></inline-formula>. In that limit, we rewrite<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e219" xlink:type="simple"/><label>(31)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e220" xlink:type="simple"/><label>(32)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e221" xlink:type="simple"/><label>(33)</label></disp-formula>Saddle point equations give in that limit<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e222" xlink:type="simple"/><label>(34)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e223" xlink:type="simple"/><label>(35)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e224" xlink:type="simple"/><label>(36)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e225" xlink:type="simple"/><label>(37)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e226" xlink:type="simple"/><label>(38)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e227" xlink:type="simple"/><label>(39)</label></disp-formula>where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e228" xlink:type="simple"/><label>(40)</label></disp-formula>These equations can be solved to numerically to obtain all quantities of interest.</p>
        <p>Finally, the equation for the distribution of synaptic weights for the bistable perceptron is identical to the one for the standard perceptron, i.e. at maximal capacity<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e229" xlink:type="simple"/><label>(41)</label></disp-formula>where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e230" xlink:type="simple"/><label>(42)</label></disp-formula>In particular the fraction of zero weight synapses is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002448.e231" xlink:type="simple"/></inline-formula>.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <p>We would like to thank Boris Barbour, Antonin Blot, Mariano Casado, Vincent Hakim and Clément Lena for fruitful discussions.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002448-Marr1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Marr</surname><given-names>D</given-names></name></person-group>             <year>1969</year>             <article-title>A theory of cerebellar cortex.</article-title>             <source>J Physiol (Lond)</source>             <volume>202</volume>             <fpage>437</fpage>             <lpage>470</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Albus1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Albus</surname><given-names>J</given-names></name></person-group>             <year>1971</year>             <article-title>A theory of cerebellar function.</article-title>             <source>J Mathematical Biosciences</source>             <volume>10</volume>             <fpage>25</fpage>             <lpage>61</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Doya1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name></person-group>             <year>2000</year>             <article-title>Complementary roles of basal ganglia and cerebellum in learning and motor control.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>10</volume>             <fpage>732</fpage>             <lpage>739</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Rosenblatt1">
        <label>4</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rosenblatt</surname><given-names>F</given-names></name></person-group>             <year>1962</year>             <source>Principles of neurodynamics</source>             <publisher-loc>Washington</publisher-loc>             <publisher-name>Spartan books</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Minsky1">
        <label>5</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Minsky</surname><given-names>ML</given-names></name><name name-style="western"><surname>Papert</surname><given-names>SA</given-names></name></person-group>             <year>1969</year>             <source>Perceptrons: An Introduction to Computational Geometry</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Soetedjo1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Soetedjo</surname><given-names>R</given-names></name><name name-style="western"><surname>Kojima</surname><given-names>Y</given-names></name><name name-style="western"><surname>Fuchs</surname><given-names>AF</given-names></name></person-group>             <year>2008</year>             <article-title>Complex spike activity in the oculomotor vermis of the cerebellum: a vectorial error signal for saccade motor learning?</article-title>             <source>J Neurophysiol</source>             <volume>100</volume>             <fpage>1949</fpage>             <lpage>1966</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Ito1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ito</surname><given-names>M</given-names></name></person-group>             <year>1989</year>             <article-title>Long-term depression.</article-title>             <source>Annu Rev Neurosci</source>             <volume>12</volume>             <fpage>85</fpage>             <lpage>102</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Jorntell1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jorntell</surname><given-names>H</given-names></name><name name-style="western"><surname>Hansel</surname><given-names>C</given-names></name></person-group>             <year>2006</year>             <article-title>Synaptic memories upside down: bidirectional plasticity at cerebellar parallel fiber-Purkinje cell synapses.</article-title>             <source>Neuron</source>             <volume>52</volume>             <fpage>227</fpage>             <lpage>238</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Cover1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cover</surname><given-names>T</given-names></name></person-group>             <year>1965</year>             <article-title>Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition.</article-title>             <source>IEEE Trans Electron Comput</source>             <volume>14</volume>             <fpage>326</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Gardner1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gardner</surname><given-names>E</given-names></name></person-group>             <year>1988</year>             <article-title>The phase space of interactions in neural network models.</article-title>             <source>J Phys A</source>             <volume>21</volume>             <fpage>257</fpage>             <lpage>270</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Hertz1">
        <label>11</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hertz</surname><given-names>J</given-names></name><name name-style="western"><surname>Krogh</surname><given-names>A</given-names></name><name name-style="western"><surname>Palmer</surname><given-names>RG</given-names></name></person-group>             <year>1991</year>             <source>Introduction to the Theory of Neural Com-putation</source>             <publisher-loc>Redwood City, CA</publisher-loc>             <publisher-name>Addison-Wesley</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Amit1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Amit</surname><given-names>D</given-names></name><name name-style="western"><surname>Wong</surname><given-names>K</given-names></name><name name-style="western"><surname>Campbell</surname><given-names>C</given-names></name></person-group>             <year>1989</year>             <article-title>Perceptron learning with sign-constrained weights.</article-title>             <source>J Phys A Math Gen</source>             <volume>22</volume>             <fpage>2039</fpage>             <lpage>2045</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Kanter1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kanter</surname><given-names>I</given-names></name><name name-style="western"><surname>Eisenstein</surname><given-names>E</given-names></name></person-group>             <year>1990</year>             <article-title>On the capacity per synapse.</article-title>             <source>J Phys A Math Gen</source>             <volume>23</volume>             <fpage>L93i</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Nadal1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name></person-group>             <year>1990</year>             <article-title>On the storage capacity with sign-constrained synaptic couplings.</article-title>             <source>Network</source>             <volume>1</volume>             <fpage>463</fpage>             <lpage>466</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Gutfreund1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gutfreund</surname><given-names>H</given-names></name><name name-style="western"><surname>Stein</surname><given-names>Y</given-names></name></person-group>             <year>2613–2630</year>             <article-title>Capacity of neural networks with discrete synaptic couplings.</article-title>             <source>J Phys A Math Gen</source>             <volume>23</volume>             <fpage>1990</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Kohler1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kohler</surname><given-names>H</given-names></name><name name-style="western"><surname>Wildmaier</surname><given-names>D</given-names></name></person-group>             <year>1991</year>             <article-title>Sign-constrained linear learning and diluting in neural networks.</article-title>             <source>J Phys A Math Gen</source>             <volume>24</volume>             <fpage>L245</fpage>             <lpage>L502</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Brunel1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name><name name-style="western"><surname>Hakim</surname><given-names>V</given-names></name><name name-style="western"><surname>Isope</surname><given-names>P</given-names></name><name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name><name name-style="western"><surname>Barbour</surname><given-names>B</given-names></name></person-group>             <year>2004</year>             <article-title>Optimal information storage and the distribution of synaptic weights: Perceptron versus purkinje cell.</article-title>             <source>Neuron</source>             <volume>43</volume>             <fpage>745</fpage>             <lpage>757</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Yartsev1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yartsev</surname><given-names>MM</given-names></name><name name-style="western"><surname>Givon-Mayo</surname><given-names>R</given-names></name><name name-style="western"><surname>Maller</surname><given-names>M</given-names></name><name name-style="western"><surname>Donchin</surname><given-names>O</given-names></name></person-group>             <year>2009</year>             <article-title>Pausing purkinje cells in the cerebellum of the awake cat.</article-title>             <source>Front Syst Neurosci</source>             <volume>3</volume>             <fpage>2</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Fernandez1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fernandez</surname><given-names>FR</given-names></name><name name-style="western"><surname>Engbers</surname><given-names>JD</given-names></name><name name-style="western"><surname>Turner</surname><given-names>RW</given-names></name></person-group>             <year>2007</year>             <article-title>Firing dynamics of cerebellar purkinje cells.</article-title>             <source>J Neurophysiol</source>             <volume>98</volume>             <fpage>278</fpage>             <lpage>294</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Loewenstein1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name><name name-style="western"><surname>Mahon</surname><given-names>S</given-names></name><name name-style="western"><surname>Chadderton</surname><given-names>P</given-names></name><name name-style="western"><surname>Kitamura</surname><given-names>K</given-names></name><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name><etal/></person-group>             <year>2005</year>             <article-title>Bistability of cerebellar Purkinje cells modulated by sensory stimulation.</article-title>             <source>Nat Neurosci</source>             <volume>8</volume>             <fpage>202</fpage>             <lpage>211</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Genet1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Genet</surname><given-names>S</given-names></name><name name-style="western"><surname>Delord</surname><given-names>B</given-names></name></person-group>             <year>2002</year>             <article-title>A biophysical model of nonlinear dynamics underlying plateau potentials and calcium spikes in purkinje cell dendrites.</article-title>             <source>J Neurophysiol</source>             <volume>88</volume>             <fpage>2430</fpage>             <lpage>2444</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Williams1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Williams</surname><given-names>SR</given-names></name><name name-style="western"><surname>Christensen</surname><given-names>SR</given-names></name><name name-style="western"><surname>Stuart</surname><given-names>GJ</given-names></name><name name-style="western"><surname>Hausser</surname><given-names>M</given-names></name></person-group>             <year>2002</year>             <article-title>Membrane potential bistability is controlled by the hyperpolarization-activated current I(H) in rat cere-bellar Purkinje neurons in vitro.</article-title>             <source>J Physiol (Lond)</source>             <volume>539</volume>             <fpage>469</fpage>             <lpage>483</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Oldfield1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Oldfield</surname><given-names>CS</given-names></name><name name-style="western"><surname>Marty</surname><given-names>A</given-names></name><name name-style="western"><surname>Stell</surname><given-names>BM</given-names></name></person-group>             <year>2010</year>             <article-title>Interneurons of the cerebellar cortex toggle Purkinje cells between up and down states.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>107</volume>             <fpage>13153</fpage>             <lpage>13158</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Dean1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dean</surname><given-names>P</given-names></name><name name-style="western"><surname>Porrill</surname><given-names>J</given-names></name><name name-style="western"><surname>Stone</surname><given-names>JV</given-names></name></person-group>             <year>2002</year>             <article-title>Decorrelation control by the cerebellum achieves oculomotor plant compensation in simulated vestibulo-ocular reflex.</article-title>             <source>Proc Biol Sci</source>             <volume>269</volume>             <fpage>1895</fpage>             <lpage>1904</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Porrill1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Porrill</surname><given-names>J</given-names></name><name name-style="western"><surname>Dean</surname><given-names>P</given-names></name></person-group>             <year>2007</year>             <article-title>Cerebellar motor learning: when is cortical plasticity not enough?</article-title>             <source>PLoS Comput Biol</source>             <volume>3</volume>             <fpage>1935</fpage>             <lpage>1950</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Lepora1">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lepora</surname><given-names>NF</given-names></name><name name-style="western"><surname>Porrill</surname><given-names>J</given-names></name><name name-style="western"><surname>Yeo</surname><given-names>CH</given-names></name><name name-style="western"><surname>Dean</surname><given-names>P</given-names></name></person-group>             <year>2010</year>             <article-title>Sensory prediction or motor control? Application of marr-albus type models of cerebellar function to classical conditioning.</article-title>             <source>Front Comput Neurosci</source>             <volume>4</volume>             <fpage>140</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Chadderton1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chadderton</surname><given-names>P</given-names></name><name name-style="western"><surname>Margrie</surname><given-names>TW</given-names></name><name name-style="western"><surname>Hausser</surname><given-names>M</given-names></name></person-group>             <year>2004</year>             <article-title>Integration of quanta in cerebellar granule cells during sensory processing.</article-title>             <source>Nature</source>             <volume>428</volume>             <fpage>856</fpage>             <lpage>860</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Isope1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Isope</surname><given-names>P</given-names></name><name name-style="western"><surname>Barbour</surname><given-names>B</given-names></name></person-group>             <year>2002</year>             <article-title>Properties of unitary Granule cell to Purkinje cell synapses in adult rat cerebellar slices.</article-title>             <source>J Neurosci</source>             <volume>22</volume>             <fpage>9668</fpage>             <lpage>9678</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Schonewille1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schonewille</surname><given-names>M</given-names></name><name name-style="western"><surname>Khosrovani</surname><given-names>S</given-names></name><name name-style="western"><surname>Winkelman</surname><given-names>BH</given-names></name><name name-style="western"><surname>Hoebeek</surname><given-names>FE</given-names></name><name name-style="western"><surname>De Jeu</surname><given-names>MT</given-names></name><etal/></person-group>             <year>2006</year>             <article-title>Purkinje cells in awake behaving animals operate at the upstate membrane potential.</article-title>             <source>Nat Neurosci</source>             <volume>9</volume>             <fpage>459</fpage>             <lpage>461</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Barbour1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barbour</surname><given-names>B</given-names></name><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name><name name-style="western"><surname>Hakim</surname><given-names>V</given-names></name><name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name></person-group>             <year>2007</year>             <article-title>What can we learn from synaptic weight distributions?</article-title>             <source>Trends Neurosci</source>             <volume>30</volume>             <fpage>622</fpage>             <lpage>629</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002448-Hansel1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hansel</surname><given-names>C</given-names></name><name name-style="western"><surname>Linden</surname><given-names>DJ</given-names></name><name name-style="western"><surname>D'Angelo</surname><given-names>E</given-names></name></person-group>             <year>2001</year>             <article-title>Beyond parallel fiber LTD: the diversity of synaptic and non-synaptic plasticity in the cerebellum.</article-title>             <source>Nat Neurosci</source>             <volume>4</volume>             <fpage>467</fpage>             <lpage>475</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>