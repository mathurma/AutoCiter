<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">10-PLCB-RA-2252R2</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1001003</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology/Computational Neuroscience</subject><subject>Neuroscience/Theoretical Neuroscience</subject><subject>Neuroscience/Psychology</subject></subj-group></article-categories><title-group><article-title>Structure Learning in Human Sequential Decision-Making</article-title><alt-title alt-title-type="running-head">Structure Learning in Decision-Making</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Acuña</surname><given-names>Daniel E.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Schrater</surname><given-names>Paul</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Department of Computer Science and Engineering, University of Minnesota, Minneapolis, Minnesota, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Department of Psychology, University of Minnesota, Minneapolis, Minnesota, United States of America</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Behrens</surname><given-names>Tim</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">John Radcliffe Hospital, United Kingdom</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">acuna002@umn.edu</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: DEA PS. Performed the experiments: DEA. Analyzed the data: DEA PS. Wrote the paper: DEA PS.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>12</month><year>2010</year></pub-date><pub-date pub-type="epub"><day>2</day><month>12</month><year>2010</year></pub-date><volume>6</volume><issue>12</issue><elocation-id>e1001003</elocation-id><history>
<date date-type="received"><day>17</day><month>5</month><year>2010</year></date>
<date date-type="accepted"><day>20</day><month>10</month><year>2010</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2010</copyright-year><copyright-holder>Acuña, Schrater</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>Studies of sequential decision-making in humans frequently find suboptimal performance relative to an ideal actor that has perfect knowledge of the model of how rewards and events are generated in the environment. Rather than being suboptimal, we argue that the learning problem humans face is more complex, in that it also involves learning the structure of reward generation in the environment. We formulate the problem of structure learning in sequential decision tasks using Bayesian reinforcement learning, and show that learning the generative model for rewards qualitatively changes the behavior of an optimal learning agent. To test whether people exhibit structure learning, we performed experiments involving a mixture of one-armed and two-armed bandit reward models, where structure learning produces many of the qualitative behaviors deemed suboptimal in previous studies. Our results demonstrate humans can perform structure learning in a near-optimal manner.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>Every decision-making experiment has a structure that specifies how rewards are obtained, which is usually explained to the subject at the beginning of the experiment. Participants frequently fail to act as if they understand the experimental structure, even in tasks as simple as determining which of two biased coins they should choose to maximize the number of trials that produce “heads”. We hypothesize that participants' behavior is not driven by top-down instructions—rather, participants must learn through experience how the rewards are generated. We formalize this hypothesis using a fully rational optimal Bayesian reinforcement learning approach that models optimal structure learning in sequential decision making. In an experimental test of structure learning in humans, we show that humans learn reward structure from experience in a near optimal manner. Our results demonstrate that behavior purported to show that humans are error-prone and suboptimal decision makers can result from an optimal learning approach. Our findings provide a compelling new family of rational hypotheses for behavior previously deemed irrational, including under- and over-exploration.</p>
</abstract><funding-group><funding-statement>This research was supported by the Office of Naval Research Grant N00014-07-1-0937. DEA was partially funded by the National Institutes of Health (NIH) Neuro-physical-computational Sciences (NPCS) Graduate Training Fellowship (1R90 DK71500-04), CONICYT-FIC-World Bank Fellowship (05-DOCFIC-BANCO-01) and the Center for Cognitive Sciences of the University of Minnesota. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="12"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>From a squirrel deciding where to bury its nuts to a scientist selecting the next experiment, all decision-making organisms must balance exploration of alternatives against exploitation of known options in developing action plans. Finding a balance is equivalent to knowing when you can profit from learning about new options and knowing when you know enough. However, determining when exploration is profitable is itself a decision problem that requires understanding or learning about the statistical structure of the environment. Theoretical work on optimal exploration <xref ref-type="bibr" rid="pcbi.1001003-Bellman1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Gittins1">[2]</xref> shows that assessing the long-term value of exploration involves integrating the predicted informational value of exploration with primary reward. Predicting the value of future information requires having a model of the reward generation process for the domain.</p>
<p>The structure learning problem may be present in tasks with as few as two options. Suppose, for example, that you interact with the environment by choosing one of the two options at discrete choice points and that the option chosen generates a stochastic binary reward. As a rational agent, your aim is to maximize the total reward from the environment, but the difficulty is that the rate of reward for each option is unknown and must be learned. In this simple setting, there may be several hypothesis about how the reward generation process works—how actions, observations and unknowns are <italic>structurally</italic> “connected.” We propose three kinds of structures that capture several versions of sequential decision-making tasks available in the literature. The first structure has temporal dependency between the present probability of reward and the past probability of reward, investigated in the context of <italic>Multi-Armed Bandit problems</italic> <xref ref-type="bibr" rid="pcbi.1001003-Whittle1">[3]</xref>–<xref ref-type="bibr" rid="pcbi.1001003-Yi1">[5]</xref>. When this dependency involves a random walk, the environment becomes non-stationary and a rational agent will discount both past reward observations <xref ref-type="bibr" rid="pcbi.1001003-Yu1">[6]</xref> and potential future reward (equivalent to discounting) and it will exhibit a higher learning rate in the sense of a greater dependence on recent reward information. In the second structure, reward probabilities can be affected by actions. For example, choosing an option may temporarily decrease the reward probability. Different kinds of action-reward probability contingencies can produce a range of different rational responses, from probability matching (foraging) to maximization. The third structure is <italic>reward coupling</italic> and is the primary focus of this paper.</p>
<p>To illustrate what structure learning entails, <xref ref-type="fig" rid="pcbi-1001003-g001">Fig. 1A</xref> shows a probabilistic graphical model representing the possible relationships between variables for a typical sequential decision task with two outcomes. In the graph, nodes represent unknown or observable quantities and links represent statistical contingencies between them. The unknown probabilities of reward at a given time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e001" xlink:type="simple"/></inline-formula> for both option 1 and 2 are represented by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e002" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e003" xlink:type="simple"/></inline-formula>, respectively. Taking action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e004" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e005" xlink:type="simple"/></inline-formula> produces a reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e006" xlink:type="simple"/></inline-formula> that can be either 0 (failure) or 1 (success). Learning the success probabilities must be balanced with the desire to maximize expected future reward. Different assumptions about the connectivity (structure) between variables produce a surprising range of rational responses. One of those structures is <italic>temporal dependency</italic> (see <xref ref-type="fig" rid="pcbi-1001003-g001">Fig. 1B</xref>) between success probabilities. In this case, rather than being fixed, the success probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e007" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e008" xlink:type="simple"/></inline-formula> depend on past values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e009" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e010" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1001003-Whittle1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Daw1">[4]</xref>. The second structure includes an effect of actions on reward probabilities (see <xref ref-type="fig" rid="pcbi-1001003-g001">Fig. 1C</xref>). Different kinds of action-reward probability contingencies can produce a range of different rational responses, from matching to maximization <xref ref-type="bibr" rid="pcbi.1001003-Sakai1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Sakai2">[8]</xref>. <xref ref-type="fig" rid="pcbi-1001003-g001">Fig. 1D</xref> illustrates <italic>Reward coupling</italic> which determines whether the reward probabilities are related to each other. For example, options may be probabilistically coupled so that if one option is “good” the other must be “bad”. This type of structure has profound consequences on exploratory and exploitative behavior.</p>
<fig id="pcbi-1001003-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001003.g001</object-id><label>Figure 1</label><caption>
<title>Different structures in sequential decision-making.</title>
<p><bold>A</bold>) General structure. Arcs highlighted denote <bold>B</bold>) temporal dependency between success probabilities, <bold>C</bold>) action-dependent reward state leading to different optimality principles—from foraging to maximization and <bold>D</bold>) reward coupling affecting exploration vs. exploitation demands.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.g001" xlink:type="simple"/></fig>
<p>To illustrate <italic>reward coupling</italic>, imagine you are serving a ball in tennis against an opponent who almost always adopts the same position near the center of the court. How do you choose whether you serve left or right? Assume the defender must anticipate and make its choice to defend left or right before it sees your serve. Clearly you should take advantage of the previous history of successful and unsuccessful serves against this opponent to try to exploit any weakness, but how you should make use of this history depends on what you can learn from your choices. For example, if you last served left and failed, can you infer it would have been better to serve right? The answer depends critically on the way options are probabilistically related. The outcomes of an anticipatory defender are probabilistically coupled - its probability of selecting left is one minus its probability of selecting right (similar to a coin flip). For coupled outcomes, what can be learned on each trial is independent of your actions and no active exploration is needed.</p>
<p>Imagine instead you throw a ball at one of two targets: left or right—with the goal of determining which target is easier to hit. In this case, you can infer little from a failure on the left target about your success on the right. The options are independent, which means that observing one option tells you little or nothing about the other. Exploration is then necessary for learning, and your choices impact what can be learned. Thus, the kind of probabilistic dependence between options determines whether passive (action independent) or active learning strategies are needed.</p>
<p>An organism with initial ignorance about the environment will not have a model of the probabilistic coupling, and thus will not know the value of exploration. But how can it know what kind of probabilistic dependence is present?</p>
<p>In this work, we investigate the possibility that people learn models of reward generation using rational analysis. From a rational perspective, actions should be selected both to increase reward and to provide information about the reward generation process. Probabilistic methods for learning dependencies between variables are termed structure learning or causal learning, and has been an active topic within the machine learning community. We argue that structure learning plays a major role in human sequential decision making. Because structure denotes the statistical relationships between entities and events, it forms the basis for generating future predictions, and it enables model-based approaches to reinforcement learning.</p>
<p>Using model-based (Bayesian) reinforcement learning <xref ref-type="bibr" rid="pcbi.1001003-Kaelbling1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1001003-Poupart1">[12]</xref> optimal exploration can be extended to handle uncertainty across a set of plausible reward generation models. In one formulation we follow here, latent parameters on model structure are treated as a hidden state, such that the algorithm tries to find values of the hidden state that maximize expected discounted reward. In essence, at the beginning of a set of tasks, we assume there is initial uncertainty over a parametric family of structures—causal models of reward generation. The learning of this causal structure is then incorporated into acting. This is a natural extension of causal induction (predictive of behavior in simpler tasks <xref ref-type="bibr" rid="pcbi.1001003-Tenenbaum1">[13]</xref>) to sequential experimentation.</p>
<p>To maximize the differences that uncertainty about the causal relationships between options would produce, we exposed subjects to one of two possible models that represent two extremes in the exploration– exploitation trade-off in a slot-machine gambling environment, where the probabilistic coupling between the payoffs between machines must be learned. Using Bayesian RL to generate an optimal exploratory agent for this environment, we show that optimal actions with reward model uncertainty include exploratory actions that are specific to model learning, and exhibit patterns that would be considered over- and under- exploration for an agent without reward model uncertainty. We demonstrate that humans are able to learn the probabilistic coupling structure for this environment, and that they exhibit exploratory choice behavior predicted by reward model learning.</p>
</sec><sec id="s2">
<title>Results</title>
<p>Participants made decisions in a set of 32 two-option tasks, each terminating stochastically, with an average of 48 trials. For each task, an option produced an stochastic binary reward with a fixed probability that had to be estimated by the participant. Participants were asked to maximized their reward gathered for the whole experiment and were compensated in proportion to the total reward.</p>
<p>Formally, the choice of option 1 or 2 transitions the agent into that state, and generates an observable binary reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e011" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e012" xlink:type="simple"/></inline-formula>, respectively. The reward distributions are initially unknown but remain constant within a task, which ends stochastically with a probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e013" xlink:type="simple"/></inline-formula>. At the end of each task the reward distributions are reset. The tasks are analogous to playing slot machines in a casino. There are two slot machines. The state of the environment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e014" xlink:type="simple"/></inline-formula> represents which of the slot machines is active. Actions involve selecting which of the machines to activate (pull the slot machine lever), and active machines generate binary rewards probabilistically.</p>
<p>To experimentally test how well humans can learn the probabilistic coupling structure of an environment, we used two environments with different reward structure designed to generate clear differences in decisions and exploratory behavior. In the first environment, which we term <italic>independent</italic>, the reward distributions for each machine are independent. In the second environment, called <italic>coupled</italic>, the two reward distributions are coupled by sharing a common cause: when one option gives reward, the other will not. The optimal policies for these environments generate exploratory behavior that span the range of possibilities, from independent where exploration is necessary to coupled, where exploration is superfluous. An agent with uncertainty about whether the environment is coupled or independent will need to learn both the coupling structure and the reward values of the options.</p>
<p>The environments were presented as two distinctive “blocks” of tasks. Each block was presented as a “game room” and machines in that game room had a unique color (blue in one room and yellow in the other). Unknown to the subjects, however, the first block of 16 tasks corresponded to one reward structure and the second block of 16 tasks corresponded to other reward structure.</p>
<p>We argue that it would be unreasonable for participants to assume a reward structure beforehand. They, instead, have to perform an estimation of this structure through a block of tasks while jointly learning the reward rates within the task. To predict human decisions in the task, we develop a normative model that makes decisions while actively gathering evidence about both task structure and the rewards available at each option and compare its performance both to other normative models that assume a fixed task structure and to model-free RL based on Q-learning with soft-max action selection.</p>
<sec id="s2a">
<title>Structure learning model with Bayesian reinforcement learning</title>
<p>In general, structure learning involves estimating the underlying dependency structure between variables. Such learning has been formulated as a probabilistic inference problem, where inference is performed over a family of hypothesized dependencies. Within machine learning, it is common to represent these dependencies using graphical models, in which nodes are variables and conditional dependencies between variables can be represented as edges.</p>
<p>More specifically, a graphical model conveys knowledge on how a joint probability distribution can be factored into multiple known conditional probabilities. For example, in <xref ref-type="fig" rid="pcbi-1001003-g002">Fig. 2A</xref>, and ignoring all the plates, the edge from node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e015" xlink:type="simple"/></inline-formula> to node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e016" xlink:type="simple"/></inline-formula> would indicate that the joint probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e017" xlink:type="simple"/></inline-formula> can be equivalently written as the product of two known distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e018" xlink:type="simple"/></inline-formula>. Additionally, a plate is a shorthand notation for replicating variables inside it while sharing conditional relationships and distribution functions. For example, the node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e019" xlink:type="simple"/></inline-formula> inside the plate with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e020" xlink:type="simple"/></inline-formula> means that there are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e021" xlink:type="simple"/></inline-formula> variables (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e022" xlink:type="simple"/></inline-formula>) that have the same known distribution function. The node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e023" xlink:type="simple"/></inline-formula> is inside a plate with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e024" xlink:type="simple"/></inline-formula> and inside the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e025" xlink:type="simple"/></inline-formula> plate, which indicates—quite compactly—that the total set of nodes is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e026" xlink:type="simple"/></inline-formula> for each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e027" xlink:type="simple"/></inline-formula>. Finally, the conditional probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e028" xlink:type="simple"/></inline-formula>, for any <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e029" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e030" xlink:type="simple"/></inline-formula>, correspond to the same distribution function.</p>
<fig id="pcbi-1001003-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001003.g002</object-id><label>Figure 2</label><caption>
<title>Graphical models of reward generation.</title>
<p>The agent faces <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e031" xlink:type="simple"/></inline-formula> tasks, each comprising a random number <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e032" xlink:type="simple"/></inline-formula> of choices. <bold>A</bold>) Rewarding options are independent. <bold>B</bold>) Rewarding options are coupled within a task. <bold>C</bold>) Mixture of tasks. Rewarding options may be independent or coupled. The node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e033" xlink:type="simple"/></inline-formula> acts as a “XOR” switch between coupled and independent structure.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.g002" xlink:type="simple"/></fig>
<p>A variety of Machine Learning methods have been developed to perform structure learning in graphical models (e.g., <xref ref-type="bibr" rid="pcbi.1001003-Heckerman1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Neapolitan1">[15]</xref>), and these have provided a compelling account of human causal inference and learning in cognitive tasks <xref ref-type="bibr" rid="pcbi.1001003-Tenenbaum1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Tenenbaum2">[16]</xref>. Below we show human structure learning in a sequential decision-making task. However, formulating the structure learning problem within sequential decision making is significantly more difficult, requiring a combination of probabilistic inference with reinforcement learning commonly called Bayesian reinforcement learning.</p>
<p>Bayesian reinforcement learning (BRL) can be used to describe an agent that learns the structure of rewards in the environment while performing optimal action selection that balances exploration and exploitation. Agents interact with a stochastic environment by performing an action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e034" xlink:type="simple"/></inline-formula> that affect the state of the environment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e035" xlink:type="simple"/></inline-formula> by transitioning to a new state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e036" xlink:type="simple"/></inline-formula> with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e037" xlink:type="simple"/></inline-formula>. Rewards are received with a probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e038" xlink:type="simple"/></inline-formula> that depends on the action and the outcome of the action. For the agents we are interested in describing, the goal is to maximize the reward accumulated across participation in a set of tasks which end stochastically with a probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e039" xlink:type="simple"/></inline-formula>. The optimal BRL agent schedules actions that maximize the expected reward received during the task: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e040" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e041" xlink:type="simple"/></inline-formula> is the reward to be received immediately, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e042" xlink:type="simple"/></inline-formula> the reward received next, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e043" xlink:type="simple"/></inline-formula> the reward received two steps into the future, and so on, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e044" xlink:type="simple"/></inline-formula> is current model of the environment. In standard model-based reinforcement learning, the agent uses a probabilistic model of reward sources and environment to compute this expectation. In BRL, the agent does not know either the reward sources and environment precisely, but rather generates <italic>beliefs</italic> over a family of possible models.</p>
<p>After each observation, the belief distribution is updated using Bayesian inference. By considering the set of possible future observations, this belief updating can be used to “look ahead” to predict future rewards that can be achieved from different plans of action. The <italic>value of a belief</italic> can be found using the Bellman equation <xref ref-type="bibr" rid="pcbi.1001003-Bellman2">[17]</xref><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e045" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e046" xlink:type="simple"/></inline-formula> represents the belief “update” by Bayes' rule<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e047" xlink:type="simple"/><label>(2)</label></disp-formula></p>
<p>In the context of reinforcement learning, a policy is a prescription of what action should be taken at a particular state. One of the key ideas in BRL is that the optimal policy can be described as a mapping from belief states to actions. In particular, an optimal policy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e048" xlink:type="simple"/></inline-formula> can be recovered by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e049" xlink:type="simple"/><label>(3)</label></disp-formula></p>
<p>We specialized this framework to model structure learning in sequential decision experiments (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for more details). For the BRL agent with structure learning, uncertainty about reward dynamics and contingencies can be modeled by including within the belief state not only reward probabilities, but also the possibility of independent or coupled structure. Maximizing the expected reward over this belief state yields the optimal balance of exploration and exploitation, resulting in action selection that seeks to simultaneously maximize (1) immediate expected rewards, (2) information about reward dynamics and (3) information about task structure.</p>
<p><xref ref-type="fig" rid="pcbi-1001003-g002">Fig. 2A</xref> represents a graphical model for the generation of rewards in an independent environment. Rewards <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e050" xlink:type="simple"/></inline-formula> are samples from Bernoulli distributions with separate Beta prior distributed reward probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e051" xlink:type="simple"/></inline-formula> for each option. The belief state about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e052" xlink:type="simple"/></inline-formula> is summarized by the counts of the number of successes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e053" xlink:type="simple"/></inline-formula> and failures <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e054" xlink:type="simple"/></inline-formula> for each option. <xref ref-type="fig" rid="pcbi-1001003-g002">Fig. 2B</xref> shows a graphical model for a coupled environment. Coupling is represented as a “shared” probability of reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e055" xlink:type="simple"/></inline-formula> from which the rewards <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e056" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e057" xlink:type="simple"/></inline-formula> are sampled. However, the probability of reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e058" xlink:type="simple"/></inline-formula> follows a Bernoulli distribution with parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e059" xlink:type="simple"/></inline-formula> whereas <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e060" xlink:type="simple"/></inline-formula> follows a Bernoulli distribution with parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e061" xlink:type="simple"/></inline-formula>.</p>
<p>To model learning coupling structure, we introduce a hidden binary state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e062" xlink:type="simple"/></inline-formula>, representing whether the options are independent or coupled in the environment. Uncertainty about the coupling structure generates a mixture between the independent and coupled environment models. <xref ref-type="fig" rid="pcbi-1001003-g002">Fig. 2C</xref> shows the full graphical model that incorporates uncertainty about the environment structure. It is a mixture model of the independent and coupled environments (<xref ref-type="fig" rid="pcbi-1001003-g002">Fig. 2A and B</xref>.) The parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e063" xlink:type="simple"/></inline-formula> switches between a coupled environment for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e064" xlink:type="simple"/></inline-formula> and an independent environment for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e065" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details.). Structure uncertainty is captured by a Bernoulli distribution on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e066" xlink:type="simple"/></inline-formula> with parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e067" xlink:type="simple"/></inline-formula>, which will change solely based on the rewards observed.</p>
<p>Without uncertainty, the optimal decision-making strategies for both the independent and coupled environments are well-known and relatively simple. The optimal policy for a coupled environment is purely exploitative—it simply chooses the option with the greater number of successes (including failures of the other option as successes) because the reward observed in one option tells us <italic>everything</italic> the reward that would have been received in the other option. Optimal action selection for an independent environment, however, involves balancing the exploration–exploitation trade-off. Exploration is required because choosing one option does not provide information about the other. The optimal policy for an independent environment involves computing a quality index for each option, called the Gittins index <xref ref-type="bibr" rid="pcbi.1001003-Gittins2">[18]</xref>, and selecting the highest quality option. The Gittins index computes the maximum expected reward per unit discounted time for each option, and is the result of the following optimization problem:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e068" xlink:type="simple"/></disp-formula>With uncertainty, optimal action selection depends on the belief that the environment is coupled, as captured by the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e069" xlink:type="simple"/></inline-formula>. In the <xref ref-type="sec" rid="s4">methods</xref> section, we show that the optimal policy for structure learning can be expressed as a mixture of the optimal policies for the independent and coupled environments. For all the models, the optimal policy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e070" xlink:type="simple"/></inline-formula> is a function of the observed counts of successes, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e071" xlink:type="simple"/></inline-formula>, and failures, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e072" xlink:type="simple"/></inline-formula>, for each option, and priors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e073" xlink:type="simple"/></inline-formula>.</p>
<p>To illustrate the behavior of the structure learning model, we expose the model to a sequence of tasks. The model is placed in either a coupled or independent environment (<xref ref-type="fig" rid="pcbi-1001003-g003">Fig. 3A &amp; B</xref>). Every 50 trials the reward probabilities on the options are randomly reset, but the type of environment stays fixed. For both environments, the structure learning model learns the environment type, as expressed by the convergence of the posterior distribution on the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e074" xlink:type="simple"/></inline-formula> parameter to its true value. For the parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e075" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e076" xlink:type="simple"/></inline-formula>, the marginal probability is indicated by the color, with brighter indicating higher relative probability mass. The structure learning model quickly learns in both environments, although it is frequently easier to detect an independent environment—whenever both options are significantly above or below chance, the coupled structure can be quickly ruled out. Once there is high certainty on the structure (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e077" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e078" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e079" xlink:type="simple"/></inline-formula> is the data), beliefs are concentrated on the parameters that matter for that structure—<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e080" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e081" xlink:type="simple"/></inline-formula> becomes concentrated on the reward probabilities of each option in the independent environment, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e082" xlink:type="simple"/></inline-formula> becomes uniform in the coupled environment.</p>
<fig id="pcbi-1001003-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001003.g003</object-id><label>Figure 3</label><caption>
<title>Learning simulation of structure learning model.</title>
<p>Four tasks of 50 trials each are sequentially shown to the structure learning model. Priors were <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e083" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e084" xlink:type="simple"/></inline-formula>. Marginal beliefs on reward probabilities (brightness indicates relative probability mass), probability of coupling and expected reward are shown as functions of time. <bold>A</bold>) Simulation on Independent Environment <bold>B</bold>) Simulation on Coupled Environment.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.g003" xlink:type="simple"/></fig>
<p>The effect of structure uncertainty on the behavior of the structure learning model is evident by looking at the expected reward. For action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e085" xlink:type="simple"/></inline-formula>, this expected reward is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e086" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e087" xlink:type="simple"/></inline-formula> is the posterior probability on the structure given the data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e088" xlink:type="simple"/></inline-formula> represented by the counts <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e089" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e090" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e091" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e092" xlink:type="simple"/></inline-formula>. If the probability that the structure is coupled is high (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e093" xlink:type="simple"/></inline-formula>), then the expected reward accrues regardless of which action is chosen. If the probability that the structure is independent (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e094" xlink:type="simple"/></inline-formula>) is high, then the expected reward depends only on the option chosen. Thus the belief about coupling gates the need for exploration. In an independent model, there is a value attached to choosing the option with less evidence even if the current evidence suggests it has a lower probability of success. The expected reward for action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e095" xlink:type="simple"/></inline-formula> is similarly<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e096" xlink:type="simple"/></disp-formula>In <xref ref-type="fig" rid="pcbi-1001003-g004">Fig. 4</xref>, we perform a simulation that shows how the structure learning model described can behave as a independent or coupled model depending on the uncertainty about coupling belief. We purposely chose evidence values for which the independent model would pick one option while the coupled model would pick the other. When a curve dips below 0, it means that the learning model would choose option 1, and when it does above 0, it would pick option 2. Note that the structure learning model can sometimes behave as a coupled or independent model depending on the uncertainty about the structure. This difference between the structure learning model vs. fixed models will play an important role later when we show that people change their policy in accord with structure learning.</p>
<fig id="pcbi-1001003-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001003.g004</object-id><label>Figure 4</label><caption>
<title>Effect of task uncertainty on exploration– exploitation of structure learning model.</title>
<p>The data available for the options are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e097" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e098" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e099" xlink:type="simple"/></inline-formula> and discount factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e100" xlink:type="simple"/></inline-formula> is 0.98, all values fixed for the simulation. The number of failures for option two (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e101" xlink:type="simple"/></inline-formula>) is varied from 1 through 3. Under these conditions, the independent would always choose option 1 whereas the coupled model would always choose option 2. However, the structure learning model switches between these two The graph shows the difference in values between the option 2 and 1 as a function of the task uncertainty.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.g004" xlink:type="simple"/></fig></sec><sec id="s2b">
<title>Model comparison</title>
<p>To quantify structure learning in participant's decisions, we compared the predictions of the structure learning model with models that capture the decisions expected from knowledge of structure in the absence of learning (fixed independent and coupled structure). Additionally, we used Q-learning algorithm <xref ref-type="bibr" rid="pcbi.1001003-Watkins1">[19]</xref> with a soft-max action selection <xref ref-type="bibr" rid="pcbi.1001003-Sutton1">[20]</xref> as a base model. Q-learning is a model-free RL method that does not model the reward probabilities or structure, rather it estimates the value of an action by compiling over experienced outcomes (called Temporal Difference learning). However, Q-learning does not balance exploration and exploitation in a principled way, but rather performs heuristic explorations based on random actions. It is proven to estimate the optimal value of an action after infinitely many observations for every action and state <xref ref-type="bibr" rid="pcbi.1001003-Watkins1">[19]</xref>. The temporal difference aspect of Q-learning as well as the exploratory interpretation of the soft-max rule have been shown to correlate with brain activity <xref ref-type="bibr" rid="pcbi.1001003-Daw1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Schultz1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Schultz2">[22]</xref>.</p>
<p>Fitting the models to all the response data, we find that the structure learning model prediction rate (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e102" xlink:type="simple"/></inline-formula>) is better than the coupled model prediction rate (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e103" xlink:type="simple"/></inline-formula>), exact binomial test <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e104" xlink:type="simple"/></inline-formula>, better than the fixed independent model prediction rate (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e105" xlink:type="simple"/></inline-formula>), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e106" xlink:type="simple"/></inline-formula>, and better than Q-learning model (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e107" xlink:type="simple"/></inline-formula>), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e108" xlink:type="simple"/></inline-formula>). Note that the Bayesian models have no free parameters, with the exception of the initial value of the prior belief about coupling structure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e109" xlink:type="simple"/></inline-formula> for the structure learning model, which is quickly swamped by the evidence. However, we allowed for individual differences in all five parameters of the Q-learning model. For all models, we assumed uniform priors on probabilities of reward (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e110" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e111" xlink:type="simple"/></inline-formula>, at the beginning of tasks).</p>
<p>The remainder of the results are organized as follows. Because essentially all models predict well a large number of trials that occur later in blocks (where evidence is high and the better option is easy to identify), we focus on the set of trials for which there is at least one disagreement between the models so that we can better tell them apart. We call this set of trials <italic>diagnostic</italic>. We show the structure learning model can better account for several aspects of decision-making on diagnostic trials. In particular, we show how uncertainty in task structure tracks qualitative and quantitative changes in choice behavior. Then we show that the structure learning model gives a principled explanation for strategies that appear suboptimal. Finally, we analyze decisions that are specifically diagnostic for the structure learning model (structure learning predicts differently than fixed models) and show that the structure learning model predicts human choice behavior better than models with fixed structure.</p>
<sec id="s2b1">
<title>Participants' decisions better captured by a structure learning model</title>
<p>We show 1) participants quickly adapt their choices to the environment that they are in, independent or coupled, and 2) normative belief about coupling predicts participants exploratory moves while learning which type of environment they are in.</p>
<p>Because optimal policies depend on the observed rewards for both options, we analyzed participants' choices as a function of two measures of the observed successes and failures: evidence and confidence. In essence, we categorized a trial based on the observation history that preceded it. The evidence measure is the log odds ratio of the observed reward rate of the better option (higher reward probability) to the worse (lower reward probability).<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e112" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e113" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e114" xlink:type="simple"/></inline-formula> denotes the observed number of successes and failures respectively and the subscripts <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e115" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e116" xlink:type="simple"/></inline-formula> denote the better and worse options, respectively. The confidence measure is the log of the ratio of the number of observations at each option<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e117" xlink:type="simple"/><label>(5)</label></disp-formula>Together the two measures capture the important aspects of the observed successes and failures for decision-making, and are commonly used to analyze proportional data <xref ref-type="bibr" rid="pcbi.1001003-Gelman1">[23]</xref>. Evidence measures which option appears better (in relative terms) based on the observed frequencies. Confidence measures the relative reliability of the evidence.</p>
<p>We compile all choices in the diagnostic trials with the same evidence and confidence and computed the fraction of these choices to the better option. We separated our analysis for the independent environment (<xref ref-type="fig" rid="pcbi-1001003-g005">Fig. 5A</xref>, left panel) and coupled environment (<xref ref-type="fig" rid="pcbi-1001003-g005">Fig. 5B</xref>, left panel). Multiple pair-wise comparisons between the models reveal that the structure learning model is significantly better at predicting participants' decisions than the rest of the models, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e118" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1001003-g005">Fig. 5A and B</xref>, right panels)</p>
<fig id="pcbi-1001003-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001003.g005</object-id><label>Figure 5</label><caption>
<title>Full behavior on diagnostic trials as a function of evidence and confidence.</title>
<p>Diagnostic trials are those in which there is at least one disagreement between the models. For each of these trials, we compute the evidence and confidence of each option. A cell in the graph indicates the empirical probability that the model (or participants) pick the better option as a function of evidence and confidence. The right panels show prediction rate of different models in diagnostic trials. All pair-wise differences are significant (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e119" xlink:type="simple"/></inline-formula>) <bold>A</bold>) Trials in Independent Environment <bold>B</bold>) Trials in Coupled Environment.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.g005" xlink:type="simple"/></fig></sec><sec id="s2b2">
<title>Participants' choices are tracked by structure uncertainty of structure learning model</title>
<p>To better test whether participants' decisions reflect structure learning, we analyze how coupling belief affected decisions within diagnostic trials. For each trial, we computed the learning model's coupling belief for the sequence of observed rewards (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e120" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e121" xlink:type="simple"/></inline-formula> is the reward history). We then computed the fraction of choices to the better option as a function of coupling belief, for both participants and for each of the models. The results are shown in <xref ref-type="fig" rid="pcbi-1001003-g006">Fig. 6A,B</xref>. Qualitatively, human choices mirror the structure learning model. Quantitatively, the structure learning model correlates strongly with participants in the coupled environment (<xref ref-type="fig" rid="pcbi-1001003-g006">Fig. 6D</xref>), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e122" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e123" xlink:type="simple"/></inline-formula>, and less on the independent environment (<xref ref-type="fig" rid="pcbi-1001003-g006">Fig. 6B</xref>), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e124" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e125" xlink:type="simple"/></inline-formula>. However, the correlation to fixed models is weaker in both environments (independent environment: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e126" xlink:type="simple"/></inline-formula> independent model, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e127" xlink:type="simple"/></inline-formula> coupled model; coupled environment: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e128" xlink:type="simple"/></inline-formula> independent model, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e129" xlink:type="simple"/></inline-formula> coupled model.). Taken together, these results suggest that people are behaving remarkably like an optimal structure learning model in a couple environment, with some unaccounted behavior in an independent environment.</p>
<fig id="pcbi-1001003-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001003.g006</object-id><label>Figure 6</label><caption>
<title>Better arm selection ratio.</title>
<p>In the diagnostic trials, <bold>A</bold>) and <bold>C</bold>) Belief in coupling tracks changes in participant choices similarly to the learning model <bold>B</bold>) and <bold>D</bold>) behavior vs. structure belief is well correlated with the learning model, but not with independent and coupled.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.g006" xlink:type="simple"/></fig></sec><sec id="s2b3">
<title>Behavior deemed suboptimal by fixed structure models are optimal for structure learning</title>
<p>In the following sections, we focus on explaining trials that are deemed suboptimal if the process of reward generation of the environment is assumed known by the participant. In particular, we show that uncertainty about task structure provides incentive for making these apparently sub-optimal choices.</p>
<p>Some studies have suggested that behavior in two independent option tasks is suboptimal when compared to an optimal model <xref ref-type="bibr" rid="pcbi.1001003-Meyer1">[24]</xref>–<xref ref-type="bibr" rid="pcbi.1001003-Gans1">[27]</xref> —that people explore too little to find the better option quicker, or explore too much, continuing to choose an option that should have been discarded. We tested whether these types of trials are better predicted by the learning model.</p>
<p>By under-exploration, we mean that subjects choose differently than an independent model for trials where the independent model selects the option with lower reward proportion (because the counts are low), and thus the independent model has a higher value for the lower reward probability option. By over-exploration, we mean that subjects choose differently than an independent model for trials where the independent model selects the option with higher reward proportion and high counts—i.e., the option chosen is clearly less rewarding and there should be nothing left to learn from it. A percentage of trials are under-explorative (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e130" xlink:type="simple"/></inline-formula>) or over-explorative (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e131" xlink:type="simple"/></inline-formula>) out of the number of trials in the independent environment (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e132" xlink:type="simple"/></inline-formula>). The learning model was able to predict most of the under-exploratory trials (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e133" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e134" xlink:type="simple"/></inline-formula>), and significantly more trials than other models, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e135" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1001003-g007">Fig. 7A</xref>). The learning model also predicted over-exploratory trials (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e136" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e137" xlink:type="simple"/></inline-formula>) better than the other models, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e138" xlink:type="simple"/></inline-formula>, but the predictive performance is relatively poor (<xref ref-type="fig" rid="pcbi-1001003-g007">Fig. 7A</xref>).</p>
<fig id="pcbi-1001003-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001003.g007</object-id><label>Figure 7</label><caption>
<title>Model comparison in different aspects of decision-making.</title>
<p><bold>A</bold> and <bold>B</bold>) Performance of learning model and coupled model for decisions not predicted by the independent model in the independent environment (separated into <italic>under-exploratory</italic> and <italic>over-exploratory</italic> trials) <bold>C</bold>) Prediction performance for trials where independent and coupled model prefer one option whereas the learning model prefers the other. These trials are called <italic>task learning</italic> trials.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.g007" xlink:type="simple"/></fig>
<p>The subset of trials classified as over-exploratory by the independent model were not well predicted by any of the models, which essentially corresponds to an anti-diagonal trend in participants decisions in the evidence versus confidence space (see <xref ref-type="fig" rid="pcbi-1001003-g006">Fig. 6</xref>, left panel). For negative evidence and positive confidence and for positive evidence but negative confidence, participants choose opposite to normative predictions. Both of these cases correspond to participants persisting in choices despite evidence to the contrary. We believe that this pattern may be a consequence of temporal dependence in participants choices, a possibility we return to in the <xref ref-type="sec" rid="s3">Discussion</xref> section.</p>
<p>Behavior in coupled environments has also been suggested to be sub-optimal <xref ref-type="bibr" rid="pcbi.1001003-Meyer1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Banks1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Edwards1">[28]</xref>–<xref ref-type="bibr" rid="pcbi.1001003-Horowitz1">[31]</xref>. Given that there is no need for exploration and the optimal behavior is inherently exploitative, we tested whether behavior that diverged from the coupled model's predictions would be better predicted by the learning model. A small percentage of trials (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e139" xlink:type="simple"/></inline-formula>) disagreed with the coupled model in the coupled environment (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e140" xlink:type="simple"/></inline-formula>). The learning model predicts <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e141" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e142" xlink:type="simple"/></inline-formula> of these trials, and has higher prediction rate than the independent model, although not significant, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e143" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2b4">
<title>Trials not predicted by the coupled or independent models are task-learning trials predicted by structure learning model</title>
<p>For structure learning tasks, there are decisions purely intended to diminish the uncertainty about the structure. A simple way to isolate these decisions is by selecting trials in which fixed models (coupled and independent) pick one option while the structure learning model picks the other. A Welch-Satterthwaite two-sample <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e144" xlink:type="simple"/></inline-formula>-test confirms the intuition that these trials happen earlier than other trials within an environment, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e145" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e146" xlink:type="simple"/></inline-formula>. For these trials, the learning model was able to predict almost all of participants' decisions (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e147" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e148" xlink:type="simple"/></inline-formula>), and thus the fixed models predicted almost none (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e149" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e150" xlink:type="simple"/></inline-formula>), exact binomial test <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e151" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1001003-g007">Fig. 7C</xref>). Q-learning predictions were also worse than chance on these trials <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e152" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e153" xlink:type="simple"/></inline-formula>), and worse than structure learning model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e154" xlink:type="simple"/></inline-formula>.</p>
</sec></sec></sec><sec id="s3">
<title>Discussion</title>
<p>We have provided evidence that structure learning may be an important missing piece in evaluating human sequential decision making. The idea of modeling sequential decision making under uncertainty as a structure learning problem is a natural extension of previous work on structure learning in models of cognition <xref ref-type="bibr" rid="pcbi.1001003-Tenenbaum1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Tenenbaum2">[16]</xref> (also see <xref ref-type="bibr" rid="pcbi.1001003-Gershman1">[32]</xref>), animal learning <xref ref-type="bibr" rid="pcbi.1001003-Courville1">[33]</xref> and motor control (e.g., see <xref ref-type="bibr" rid="pcbi.1001003-Braun1">[34]</xref>). It also extends previous work on Bayesian approaches to modeling sequential decision making in the Multi-armed bandits <xref ref-type="bibr" rid="pcbi.1001003-Acuna1">[35]</xref> by adding structure learning. It is important to note that we have intentionally focused on reward structure, ignoring issues involving dependencies across trials. Clearly reward structure learning must be integrated with learning about temporal dependencies <xref ref-type="bibr" rid="pcbi.1001003-Lee1">[36]</xref> (e.g. assumptions of a non-stationary environment <xref ref-type="bibr" rid="pcbi.1001003-Yi1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Behrens1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Steyvers1">[38]</xref>).</p>
<p>Interestingly, there were a set of participants' decisions that none of the models were able to capture and that constitute 9.4% of the data. These trials are predominately localized on positive evidence (Eq. 4), but negative confidence (Eq. 5) levels (see <xref ref-type="fig" rid="pcbi-1001003-g005">Fig. 5A and B</xref>, left panel, people column.). These choices corresponded to persisting in choosing the worst option despite statistical evidence supporting the better option. None of the models considered would choose the worse option under these conditions. Participants may have limited memory or may be considering a larger space of possible models; for example nonconstant reward rates (allowing for nonstationary reward probabilities).</p>
<p>Although we focused on learning coupling between options, there are other kinds of reward structure learning that may account for a broad variety of human decision making performance. In particular, allowing dependence between the probability of reward at a site and previous actions can produce large changes in decision making behavior. For example, in a “foraging” model where reward is collected from a site and probabilistically replenished, optimal strategies will produce choice sequences that alternate between reward sites <xref ref-type="bibr" rid="pcbi.1001003-Anderson2">[39]</xref>. Thus uncertainty about the independence of reward on previous actions can produce a continuum of behavior, from maximization to probability matching. Note that structure learning explanations for probability matching are significantly different than explanations based on reinforcing previously successful actions (the “law of effect”) <xref ref-type="bibr" rid="pcbi.1001003-Erev1">[40]</xref>. Instead of explaining behavior in terms of the idiosyncrasies of a learning rule, structure learning constitutes a fully rational response to uncertainty about the causal structure of rewards in the environment. By expanding the range of normative hypotheses for human decision-making, we believe we can begin to develop more principled accounts of human sequential decision-making.</p>
<p>The general alternative to the rational approach is to assume that choice behavior reflects some fundamental limitations in sensing, neural computation or storage. It is possible that the decisions we could not predict in any dependent environment result from human processing limitations. For example, one of the key decision patterns that does not fit in the normative approach is choice stickiness, a persistence in choosing the same option despite evidence suggesting it would be better to switch. This could reflect a transition to model-free learning in the independent environment. Participants may have learned a policy for choosing that option based on early reward evidence. However, we find no evidence for this possibility in our data. Another possibility is that participants have memory limitations that prevent them from compiling all of the evidence <xref ref-type="bibr" rid="pcbi.1001003-Acuna1">[35]</xref>—the observed persistence may be sensitivity to local reward. While limitations to human decision-making surely exist, and people are bounded rational, our results provide evidence that decisions are also driven by sophisticated structure learning. We believe that many aspects of human decision-making that appears mysterious may be the result of the brain's attempts to acquire compact and useful representations of the structure of its environment.</p>
<p>We foresee an adoption of more sophisticated models of sequential decision-making to account for the compact representation that humans might be using to act in diverse reward structures. While we believe that the theory to analyze these representations is available, it has only been cautiously adopted in Psychology and Neuroscience <xref ref-type="bibr" rid="pcbi.1001003-Acuna1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Smith1">[41]</xref>–<xref ref-type="bibr" rid="pcbi.1001003-Steyvers2">[43]</xref>. We have already seen this pattern of adoption occur in Artificial Intelligence where the development of efficient computational methods to solve Bellman's equation (i.e. model-free RL methods like Q-learning) led to the rapid development and application of RL methods starting in the 1980s, despite the fact that the theoretical foundations had been laid by control theorist more than two decades prior <xref ref-type="bibr" rid="pcbi.1001003-Bellman1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Howard1">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Feldbaum1">[45]</xref>. While Robotics, for example, today hardly uses model-free reinforcement learning to think about tasks of any level of complexity, much work remains for model-based reinforcement learning to make its way into mainstream human and animal sequential decision-making analysis.</p>
</sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<p>Informed consent was obtained and all investigations were conducted according to the principles expressed in the Declaration of Helsinki, under the Assurance of Compliance number FWA00000312.</p>
<sec id="s4a">
<title>Experimental methods</title>
<p>Sixteen volunteers solve 32 bandit tasks, 16 for each environment. The probabilities of rewards were randomly sampled from a uniform distribution, and the stopping times for each bandit task were sampled from a Geometric distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e155" xlink:type="simple"/></inline-formula>. The average stopping time was 48. The order of the tasks within an environment was randomized, and the order of presentation of the environments was randomized as well. All subjects were exposed to the same probabilities of rewards and stopping times.</p>
<p>Each option is shown in the screen as a slot machine. Subjects pull a machine by pressing a key in the keyboard. When pulled, an animation of the lever is shown, 200 msec later the reward appears in the machine's screen, and a sound mimicking dropping coins lasts proportionally to the amount gathered. We provide several cues, some redundant, to help subjects keep track of previous rewards. We display the number of pulls, total reward, and the current average reward per pull. Reward magnitudes were 0 or 100 points. The machine's screen changes the color according to the average reward, from red (zero points), through yellow (fifty points), and green (one hundred points). The machine's total reward is shown as a pile of coins underneath it. The total score, total pulls, and rankings within a game were presented.</p>
<p>All participants finished all tasks. Each participant performed 1194 trials on independent environment and 925 on the coupled environment, for a total of 33904 trials. In general, participants understood the task well. No apparent outliers were found nor missed trials.</p>
</sec><sec id="s4b">
<title>Models of sequential decision-making</title>
<p>The language of graphical models provides a useful framework for describing the possible structure of rewards in the environment. Consider an environment with several distinct reward sites that can be sampled, but the way models generate these rewards is unknown. In particular, rewards at each site may be independent, or there may be a latent cause which accounts for the presence of rewards at both sites. Uncertainty about which reward model is correct naturally produces a mixture as the appropriate learning model. This structure learning model is a special case of Bayesian Reinforcement Learning (BRL), where the states of the environment are the reward sites and the transitions between states are determined by the action of sampling a reward site. Uncertainty about reward dynamics and contingencies can be modeled by including within the belief state not only reward probabilities, but also the possibility of independent or coupled rewards. Then, the optimal balance of exploration and exploitation in BRL results in action selection that seeks to maximize (1) expected rewards (2) information about rewards dynamics, and (3) information about task structure.</p>
<p>The belief over dynamics is effectively a probability distribution over possible Markov Decision Processes that would explain observables. As such, the optimal policy can be described as a mapping from belief states to actions. In principle, the optimal solution can be found by solving Bellman optimality equations but generally there are countably or uncountably infinitely many states and solutions need approximations. If we were certain which of the two models were right, the action selection problem has known solution for both cases, presented below.</p>
<sec id="s4b1">
<title>Model with fixed independent structure</title>
<p>Learning and acting in an environment like the one described in <xref ref-type="fig" rid="pcbi-1001003-g002">Fig. 2A</xref> is known as the Multi-Armed Bandit (MAB) problem. The MAB problem is a special case of BRL because we can partition the belief <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e156" xlink:type="simple"/></inline-formula> into a disjoint set of beliefs about each option <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e157" xlink:type="simple"/></inline-formula>. Because beliefs about non-sampled options remain <italic>frozen</italic> until sampled again, independent learning and action selection for each option is possible. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e158" xlink:type="simple"/></inline-formula> be the reward of a deterministic option in<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e159" xlink:type="simple"/></disp-formula>such that both terms inside the maximization are equal. Gittins <xref ref-type="bibr" rid="pcbi.1001003-Gittins2">[18]</xref> proved that it is optimal to choose the option <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e160" xlink:type="simple"/></inline-formula> with the highest such reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e161" xlink:type="simple"/></inline-formula> (called the Gittins Index). This allows speedup of computation by transforming a <italic>many</italic>-arm bandit problem to <italic>many</italic> 2-arm bandit problems.</p>
<p>In our task, the belief about a binary reward may be represented by a Beta Distribution with sufficient statistics parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e162" xlink:type="simple"/></inline-formula> (both <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e163" xlink:type="simple"/></inline-formula>) such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e164" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e165" xlink:type="simple"/></inline-formula>. Thus, the belief about option <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e166" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e167" xlink:type="simple"/></inline-formula> expected reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e168" xlink:type="simple"/></inline-formula> and predicted probability of reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e169" xlink:type="simple"/></inline-formula> are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e170" xlink:type="simple"/></inline-formula>. The belief state transition is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e171" xlink:type="simple"/></inline-formula>. Therefore, the Gittins index may be found by solving the Bellman equations using dynamic programming<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e172" xlink:type="simple"/><label>(6)</label></disp-formula>to a sufficiently large horizon. In experiments, we use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e173" xlink:type="simple"/></inline-formula>, for which a horizon of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e174" xlink:type="simple"/></inline-formula> suffices.</p>
</sec><sec id="s4b2">
<title>Model with fixed coupled structure</title>
<p>Learning and acting in coupled environments (<xref ref-type="fig" rid="pcbi-1001003-g002">Fig. 2B</xref>) is trivial because there is no need to maximize information in acting. The belief state is represented by a Beta distribution with sufficient statistics <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e175" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e176" xlink:type="simple"/></inline-formula>). The expected reward of option <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e177" xlink:type="simple"/></inline-formula> is then defined as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e178" xlink:type="simple"/><label>(7)</label></disp-formula></p>
<p>The optimal value of action is myopic as follows<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e179" xlink:type="simple"/><label>(8)</label></disp-formula></p>
<p>The belief state transitions are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e180" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e181" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4b3">
<title>Learning and acting with structure learning model</title>
<p>We restrict ourselves to the following scenario. The agent is presented with a sequence of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e182" xlink:type="simple"/></inline-formula> bandit tasks, from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e183" xlink:type="simple"/></inline-formula>, each with initially unknown Bernoulli reward probabilities and coupling. Each task involves <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e184" xlink:type="simple"/></inline-formula> discrete choices, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e185" xlink:type="simple"/></inline-formula> is sampled from a Geometric distribution with parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e186" xlink:type="simple"/></inline-formula>.</p>
<p><xref ref-type="fig" rid="pcbi-1001003-g002">Fig. 2C</xref> shows the mixture of two possible reward models shown in <xref ref-type="fig" rid="pcbi-1001003-g002">Fig. 2A and B</xref>. Node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e187" xlink:type="simple"/></inline-formula> switches the mixture between the two possible reward models and encodes part of the belief state of the process. Notice that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e188" xlink:type="simple"/></inline-formula> is acting as a <italic>XOR</italic> gate between the two generative models. Given that it is unknown, the probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e189" xlink:type="simple"/></inline-formula> is the mixed proportion for independent reward structure and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e190" xlink:type="simple"/></inline-formula> is the mixed proportion for coupled reward structure. Specifically:</p>
<list list-type="order"><list-item>
<p>For the block: Coupling parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e191" xlink:type="simple"/></inline-formula> may be either 0 or 1, and is unknown for the agent. For learning, put Bernoulli prior with parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e192" xlink:type="simple"/></inline-formula>. Sample <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e193" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p>For the bandit task <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e194" xlink:type="simple"/></inline-formula>: Sample <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e195" xlink:type="simple"/></inline-formula> for parameters, all unknown for the agent. For learning, put Beta priors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e196" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e197" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p>For choice <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e198" xlink:type="simple"/></inline-formula>, with stochastic stopping time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e199" xlink:type="simple"/></inline-formula>:</p>
</list-item></list>
<list list-type="bullet"><list-item>
<p>Choose option 1: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e200" xlink:type="simple"/></inline-formula></p>
</list-item><list-item>
<p>Choose option 2: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e201" xlink:type="simple"/></inline-formula></p>
</list-item></list>
<p>Learning can be performed analytically. Let <bold>x</bold> be a sequence of rewards observed. For the likelihood term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e202" xlink:type="simple"/></inline-formula> in the posterior, the observations <bold>x</bold> are independent given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e203" xlink:type="simple"/></inline-formula>'s and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e204" xlink:type="simple"/></inline-formula>. Hence, we just need to keep track of the number of successes (1's) and failures (0's) of each option, rather than <italic>when</italic> rewards were observed. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e205" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e206" xlink:type="simple"/></inline-formula> be the number of successes and failures for option <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e207" xlink:type="simple"/></inline-formula> in <bold>x</bold>. It is clear that the posterior distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e208" xlink:type="simple"/></inline-formula> is not closed with respect to the prior, but still by keeping track of the counts we can compute the necessary quantities for the Bellman's equation in a straightforward manner.</p>
<p>After simple algebraic manipulation, we can obtain the posterior distribution on coupling. At the beginning of each bandit task, we assume the agent “resets” its belief about options (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e209" xlink:type="simple"/></inline-formula>, but the posterior over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e210" xlink:type="simple"/></inline-formula> is carried over and used as the prior on the next bandit task. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e211" xlink:type="simple"/></inline-formula> be the Beta function, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e212" xlink:type="simple"/></inline-formula> is the Gamma function. For simplicity, we define <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e213" xlink:type="simple"/></inline-formula>. The marginal posterior on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e214" xlink:type="simple"/></inline-formula> is as follows<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e215" xlink:type="simple"/><label>(9)</label></disp-formula></p>
<p>The beliefs about environment dynamics, however, may still be completely represented by the counts and prior parameters within a task with a probability distribution about environment dynamics as Eq. 9.</p>
<p>The predicted rewards are:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e216" xlink:type="simple"/><label>(10)</label></disp-formula>and similarly<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e217" xlink:type="simple"/><label>(11)</label></disp-formula></p>
<p>From now on, we define <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e218" xlink:type="simple"/></inline-formula> for simplicity. The action selection involves solving the following Bellman equations<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e219" xlink:type="simple"/><label>(12)</label></disp-formula></p>
<p>To obtain (12) using dynamic programming for a horizon <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e220" xlink:type="simple"/></inline-formula>, there will be a total of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e221" xlink:type="simple"/></inline-formula> computations which represent different occurrences of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e222" xlink:type="simple"/></inline-formula> out of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e223" xlink:type="simple"/></inline-formula> possible histories of rewards. This dramatic reduction allows us to be relatively accurate in our approximation to the optimal value of an action.</p>
<p>We use a horizon <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e224" xlink:type="simple"/></inline-formula> for computing values with Eq. 12. Notice that we can recover the action selection of fixed models by computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e225" xlink:type="simple"/></inline-formula> for the independent model and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e226" xlink:type="simple"/></inline-formula> for the coupled model. However, we use Eq. 6 for the independent model and Eq. 7 for the coupled environment because is much more efficient. We checked that actions of the learning model when the task certainty is very high (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e227" xlink:type="simple"/></inline-formula>) do not differ from Eq. 6 or Eq. 7, respectively.</p>
</sec><sec id="s4b4">
<title>Q-learning with soft-max</title>
<p>It is possible to optimally act without a model of the environment by using what is known as model-free reinforcement learning. One of the most popular model-free reinforcement learning algorithms is known as Q-learning, which can compute the optimal value of an action after infinitely many observations for each action and states <xref ref-type="bibr" rid="pcbi.1001003-Watkins1">[19]</xref>. However, Q-learning does not have a principle for performing exploratory actions and it is usually coupled with occasional random actions (e.g., see <xref ref-type="bibr" rid="pcbi.1001003-Dearden1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1001003-Poupart1">[12]</xref> for a contrast with Bayesian reinforcement learning). For example, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e228" xlink:type="simple"/></inline-formula>-greedy action selection chooses a random action an <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e229" xlink:type="simple"/></inline-formula> fraction of the time and the soft-max action selection uses the current estimates of values to construct a distribution on the probability where, roughly speaking, actions with higher value estimates have higher probability of selection. In practice, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e230" xlink:type="simple"/></inline-formula>-greedy and soft-max Q-learning are extremely fast methods for making decisions, but they do not keep track of the accuracy and need a great deal of data to correctly estimate values.</p>
<p>We use Q-learning with soft-max action selection as model for base comparison. Suppose that the value of each option at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e231" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e232" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e233" xlink:type="simple"/></inline-formula>, then the action selection is random and driven by the following soft-max rule:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e234" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e235" xlink:type="simple"/></inline-formula> has the following interpretation: a large value (e.g., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e236" xlink:type="simple"/></inline-formula>) indicates that the agent will always choose the option with highest <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e237" xlink:type="simple"/></inline-formula>, a value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e238" xlink:type="simple"/></inline-formula> indicates that the agent will pick an option uniformly at random, and a negative value (e.g., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e239" xlink:type="simple"/></inline-formula>) indicates that agent tends to choose in opposition to what is prescribed by the Q values.</p>
<p>After taking an action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e240" xlink:type="simple"/></inline-formula>, interacting with the environment and receiving a reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e241" xlink:type="simple"/></inline-formula>, the agent updates its estimation of the values by the temporal difference rule:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e242" xlink:type="simple"/><label>(14)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e243" xlink:type="simple"/></inline-formula> is known as the <italic>learning rate</italic> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e244" xlink:type="simple"/></inline-formula> is the discount factor. A learning rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e245" xlink:type="simple"/></inline-formula> indices that the agent won't consider new rewards in the estimation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e246" xlink:type="simple"/></inline-formula>, while a learning rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e247" xlink:type="simple"/></inline-formula> indicates that the agent will consider only the last reward in the estimation and not past rewards.</p>
<p>Q-learning needs an initial estimation of the value of each option (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e248" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e249" xlink:type="simple"/></inline-formula>), the learning rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e250" xlink:type="simple"/></inline-formula> and the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001003.e251" xlink:type="simple"/></inline-formula> for the soft-max rule. For our data analysis, we fit these parameters per participant so as to maximize the prediction rate of the Q-learning model.</p>
</sec></sec></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1001003-Bellman1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bellman</surname><given-names>RE</given-names></name>
</person-group>             <year>1956</year>             <article-title>A problem in the sequential design of experiments.</article-title>             <source>Sankhyā</source>             <volume>16</volume>             <fpage>221</fpage>             <lpage>229</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Gittins1"><label>2</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gittins</surname><given-names>JC</given-names></name>
</person-group>             <year>1989</year>             <source>Multi-armed bandit allocation indices</source>             <publisher-loc>Chichester [West Sussex]; New York</publisher-loc>             <publisher-name>Wiley</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001003-Whittle1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Whittle</surname><given-names>P</given-names></name>
</person-group>             <year>1988</year>             <article-title>Restless bandits: activity allocation in a changing world.</article-title>             <source>J Appl Probab</source>             <volume>25</volume>             <fpage>287</fpage>             <lpage>298</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Daw1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>
<name name-style="western"><surname>O'Doherty</surname><given-names>JP</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Seymour</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name>
</person-group>             <year>2006</year>             <article-title>Cortical substrates for exploratory decisions in humans.</article-title>             <source>Nature</source>             <volume>441</volume>             <fpage>876</fpage>             <lpage>879</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Yi1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Yi</surname><given-names>MS</given-names></name>
<name name-style="western"><surname>Steyvers</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Lee</surname><given-names>M</given-names></name>
</person-group>             <year>2009</year>             <article-title>Modeling human performance in restless bandits with particle filters.</article-title>             <source>The Journal of Problem Solving</source>             <volume>2</volume>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://docs.lib.purdue.edu/jps/vol2/iss2/5/" xlink:type="simple">http://docs.lib.purdue.edu/jps/vol2/iss2/5/</ext-link></comment>          </element-citation></ref>
<ref id="pcbi.1001003-Yu1"><label>6</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Yu</surname><given-names>AJ</given-names></name>
<name name-style="western"><surname>Cohen</surname><given-names>JD</given-names></name>
</person-group>             <year>2009</year>             <article-title>Sequential effects: Superstition or rational behavior?</article-title>             <source>Advances in Neural Information Processing Systems, 21</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>1873</fpage>             <lpage>1880</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Sakai1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sakai</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Fukai</surname><given-names>T</given-names></name>
</person-group>             <year>2008</year>             <article-title>When does reward maximization lead to matching law?</article-title>             <source>PLoS One</source>             <volume>3</volume>             <fpage>e3795</fpage>          </element-citation></ref>
<ref id="pcbi.1001003-Sakai2"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sakai</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Fukai</surname><given-names>T</given-names></name>
</person-group>             <year>2008</year>             <article-title>The actor-critic learning is behind the matching law: Matching vs. optimal behaviors.</article-title>             <source>Neural Comput</source>             <volume>20</volume>             <fpage>227</fpage>             <lpage>251</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Kaelbling1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kaelbling</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Littman</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Cassandra</surname><given-names>A</given-names></name>
</person-group>             <year>1998</year>             <article-title>Planning and acting in partially observable stochastic domains.</article-title>             <source>Artif Intell</source>             <volume>101</volume>             <fpage>99</fpage>             <lpage>134</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Dearden1"><label>10</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dearden</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Friedman</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Russell</surname><given-names>S</given-names></name>
</person-group>             <year>1998</year>             <article-title>Bayesian Q-learning.</article-title>             <fpage>761</fpage>             <lpage>768</lpage>             <comment>In: Fifteenth National Conf. on Artificial Intelligence (AAAI)</comment>          </element-citation></ref>
<ref id="pcbi.1001003-Strens1"><label>11</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Strens</surname><given-names>MJA</given-names></name>
</person-group>             <year>2000</year>             <article-title>A bayesian framework for reinforcement learning.</article-title>             <source>Proceedings of the Seventeenth International Conference on Machine Learning</source>             <publisher-name>Morgan Kaufmann Publishers Inc</publisher-name>             <fpage>943</fpage>             <lpage>950</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Poupart1"><label>12</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Poupart</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Vlassis</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Hoey</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Regan</surname><given-names>K</given-names></name>
</person-group>             <year>2006</year>             <article-title>An analytic solution to discrete bayesian reinforcement learning.</article-title>             <source>23rd International Conference on Machine Learning</source>             <publisher-loc>Pittsburgh, Penn</publisher-loc>             <fpage>697</fpage>             <lpage>704</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Tenenbaum1"><label>13</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name>
<name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name>
</person-group>             <year>2001</year>             <article-title>Structure learning in human causal induction.</article-title>             <source>Advances in Neural Information Processing Systems 13</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>59</fpage>             <lpage>65</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Heckerman1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Heckerman</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Geiger</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Chickering</surname><given-names>DM</given-names></name>
</person-group>             <year>1995</year>             <article-title>Learning bayesian networks: The combination of knowledge and statistical data.</article-title>             <source>Mach Learn</source>             <volume>20</volume>             <fpage>197</fpage>             <lpage>243</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Neapolitan1"><label>15</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Neapolitan</surname><given-names>RE</given-names></name>
</person-group>             <year>2004</year>             <source>Learning Bayesian networks</source>             <publisher-loc>Upper Saddle River, NJ</publisher-loc>             <publisher-name>Pearson Prentice Hall</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001003-Tenenbaum2"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name>
<name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name>
<name name-style="western"><surname>Kemp</surname><given-names>C</given-names></name>
</person-group>             <year>2006</year>             <article-title>Theory-based bayesian models of inductive learning and reasoning.</article-title>             <source>Trends Cogn Sci</source>             <volume>10</volume>             <fpage>309</fpage>             <lpage>318</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Bellman2"><label>17</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bellman</surname><given-names>RE</given-names></name>
</person-group>             <year>1957</year>             <source>Dynamic programming</source>             <publisher-loc>Princeton</publisher-loc>             <publisher-name>Princeton University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001003-Gittins2"><label>18</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gittins</surname><given-names>JC</given-names></name>
<name name-style="western"><surname>Jones</surname><given-names>DM</given-names></name>
</person-group>             <year>1974</year>             <article-title>A dynamic allocation index for the sequential design of experiments.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Gani</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Sarkadi</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Vincze</surname><given-names>I</given-names></name>
</person-group>             <source>Progress in statistics</source>             <publisher-loc>Amsterdam</publisher-loc>             <publisher-name>North-Holland Pub. Co</publisher-name>             <fpage>241</fpage>             <lpage>266</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Watkins1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Watkins</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
</person-group>             <year>1992</year>             <article-title>Technical note: Q-learning.</article-title>             <source>Mach Learn</source>             <volume>8</volume>             <fpage>279</fpage>             <lpage>292</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Sutton1"><label>20</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name>
<name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name>
</person-group>             <year>1998</year>             <source>Reinforcement learning: An introduction</source>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001003-Schultz1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Montague</surname><given-names>P</given-names></name>
</person-group>             <year>1997</year>             <article-title>A neural substrate of prediction and reward.</article-title>             <source>Science</source>             <volume>275</volume>             <fpage>1593</fpage>             <lpage>1599</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Schultz2"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>1998</year>             <article-title>Predictive reward signal of dopamine neurons.</article-title>             <source>J Neurophysiol</source>             <volume>80</volume>             <fpage>1</fpage>             <lpage>27</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Gelman1"><label>23</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gelman</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Carlin</surname><given-names>JB</given-names></name>
<name name-style="western"><surname>Stern</surname><given-names>HS</given-names></name>
<name name-style="western"><surname>Rubin</surname><given-names>DB</given-names></name>
</person-group>             <year>2003</year>             <source>Bayesian Data Analysis</source>             <publisher-name>Chapman &amp; Hall/CRC</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001003-Meyer1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Meyer</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>Shi</surname><given-names>Y</given-names></name>
</person-group>             <year>1995</year>             <article-title>Sequential choice under ambiguity: Intuitive solutions to the armed-bandit problem.</article-title>             <source>Manage Sci</source>             <volume>41</volume>             <fpage>817</fpage>             <lpage>834</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Banks1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Banks</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Olson</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Porter</surname><given-names>D</given-names></name>
</person-group>             <year>1997</year>             <article-title>An experimental analysis of the bandit problem.</article-title>             <source>Econ Theory</source>             <volume>10</volume>             <fpage>55</fpage>             <lpage>77</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Anderson1"><label>26</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Anderson</surname><given-names>C</given-names></name>
</person-group>             <year>2001</year>             <article-title>Behavioral Models of Strategies in Multi-Armed Bandit Problems.</article-title>             <comment>Ph.D. thesis, California Institute of Technology, Pasadena, CA</comment>          </element-citation></ref>
<ref id="pcbi.1001003-Gans1"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gans</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Knox</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Croson</surname><given-names>R</given-names></name>
</person-group>             <year>2007</year>             <article-title>Simple models of discrete choice and their performance in bandit experiments.</article-title>             <source>Manuf Serv Oper Manag</source>             <volume>9</volume>             <fpage>383</fpage>             <lpage>408</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Edwards1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Edwards</surname><given-names>W</given-names></name>
</person-group>             <year>1956</year>             <article-title>Reward probability, amount, and information as determiners of sequential two-alternative decisions.</article-title>             <source>J Exp Psychol</source>             <volume>52</volume>             <fpage>177</fpage>             <lpage>88</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Edwards2"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Edwards</surname><given-names>W</given-names></name>
</person-group>             <year>1961</year>             <article-title>Probability learning in 1000 trials.</article-title>             <source>J Exp Psychol</source>             <volume>62</volume>             <fpage>385</fpage>             <lpage>394</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Brackbill1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Brackbill</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Bravos</surname><given-names>A</given-names></name>
</person-group>             <year>1962</year>             <article-title>Supplementary report: The utility of correctly predicting infrequent events.</article-title>             <source>J Exp Psychol</source>             <volume>64</volume>             <fpage>648</fpage>             <lpage>649</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Horowitz1"><label>31</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Horowitz</surname><given-names>AD</given-names></name>
</person-group>             <year>1973</year>             <source>Experimental Study of the Two-Armed Bandit Problem. Ph.D. Dissertation</source>             <publisher-loc>Chapel Hill, NC</publisher-loc>             <publisher-name>University of North Carolina, Chapel Hill</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001003-Gershman1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gershman</surname><given-names>SJ</given-names></name>
<name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name>
</person-group>             <year>2010</year>             <article-title>Learning latent structure: carving nature at its joints.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>20</volume>             <fpage>251</fpage>             <lpage>256</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Courville1"><label>33</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Courville</surname><given-names>AC</given-names></name>
<name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>
<name name-style="western"><surname>Gordon</surname><given-names>GJ</given-names></name>
<name name-style="western"><surname>Touretzky</surname><given-names>DS</given-names></name>
</person-group>             <year>2004</year>             <article-title>Model uncertainty in classical conditioning.</article-title>             <source>Advances in Neural Information Processing Systems 16</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>977</fpage>             <lpage>986</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Braun1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Braun</surname><given-names>DA</given-names></name>
<name name-style="western"><surname>Mehring</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name>
</person-group>             <year>2009</year>             <article-title>Structure learning in action.</article-title>             <source>Behav Brain Res</source>             <volume>206</volume>             <fpage>157</fpage>             <lpage>165</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Acuna1"><label>35</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Acuna</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Schrater</surname><given-names>P</given-names></name>
</person-group>             <year>2008</year>             <article-title>Bayesian modeling of human sequential decision-making on the multi-armed bandit problem.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Sloutsky</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Love</surname><given-names>B</given-names></name>
<name name-style="western"><surname>McRae</surname><given-names>K</given-names></name>
</person-group>             <source>30th Annual Conference of the Cognitive Science Society</source>             <publisher-loc>Austin, TX</publisher-loc>             <publisher-name>Cognitive Science Society</publisher-name>             <fpage>2065</fpage>             <lpage>2070</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Lee1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lee</surname><given-names>MD</given-names></name>
</person-group>             <year>2006</year>             <article-title>A hierarchical Bayesian model of human decision-making on an optimal stopping problem.</article-title>             <source>Cogn Sci</source>             <volume>30</volume>             <fpage>1</fpage>             <lpage>26</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Behrens1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Behrens</surname><given-names>TEJ</given-names></name>
<name name-style="western"><surname>Woolrich</surname><given-names>MW</given-names></name>
<name name-style="western"><surname>Walton</surname><given-names>ME</given-names></name>
<name name-style="western"><surname>Rushworth</surname><given-names>MFS</given-names></name>
</person-group>             <year>2007</year>             <article-title>Learning the value of information in an uncertain world.</article-title>             <source>Nat Neurosci</source>             <volume>10</volume>             <fpage>1214</fpage>             <lpage>1221</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Steyvers1"><label>38</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Steyvers</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Brown</surname><given-names>S</given-names></name>
</person-group>             <year>2006</year>             <article-title>Prediction and change detection.</article-title>             <fpage>1281</fpage>             <lpage>1288</lpage>             <comment>In: NIPS 2006</comment>          </element-citation></ref>
<ref id="pcbi.1001003-Anderson2"><label>39</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Anderson</surname><given-names>J</given-names></name>
</person-group>             <year>2000</year>             <source>Learning and memory.</source>             <publisher-name>Wiley New York</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001003-Erev1"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Erev</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Roth</surname><given-names>AE</given-names></name>
</person-group>             <year>1998</year>             <article-title>Predicting how people play games: Reinforcement learning in experimental games with unique, mixed strategy equilibria.</article-title>             <source>Am Econ Rev</source>             <volume>88</volume>             <fpage>848</fpage>             <lpage>881</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Smith1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Smith</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Li</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Becker</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Kapur</surname><given-names>S</given-names></name>
</person-group>             <year>2006</year>             <article-title>Dopamine, prediction error and associative learning: A model-based account.</article-title>             <source>Network</source>             <volume>17</volume>             <fpage>61</fpage>             <lpage>84</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Johnson1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Johnson</surname><given-names>A</given-names></name>
<name name-style="western"><surname>van der Meer</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Redish</surname><given-names>A</given-names></name>
</person-group>             <year>2007</year>             <article-title>Integrating hippocampus and striatum in decision-making.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>17</volume>             <fpage>692</fpage>             <lpage>697</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Steyvers2"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Steyvers</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Lee</surname><given-names>MD</given-names></name>
<name name-style="western"><surname>Wagenmakers</surname><given-names>E</given-names></name>
</person-group>             <year>2009</year>             <article-title>A bayesian analysis of human decision-making on bandit problems.</article-title>             <source>J Math Psychol</source>             <volume>53</volume>             <fpage>168</fpage>             <lpage>179</lpage>          </element-citation></ref>
<ref id="pcbi.1001003-Howard1"><label>44</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Howard</surname><given-names>R</given-names></name>
</person-group>             <year>1960</year>             <source>Dynamic Programming</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001003-Feldbaum1"><label>45</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fel'dbaum</surname><given-names>A</given-names></name>
</person-group>             <year>1965</year>             <source>Optimal Control Systems</source>             <publisher-name>Academic Press</publisher-name>          </element-citation></ref>
</ref-list>

</back>
</article>