<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-02184</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1007331</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Experimental psychology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Experimental psychology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Markov models</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Fear</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Fear</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Behavioral conditioning</subject><subj-group><subject>Classical conditioning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Behavioral conditioning</subject><subj-group><subject>Classical conditioning</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A flexible and generalizable model of online latent-state learning</article-title>
<alt-title alt-title-type="running-head">A flexible and generalizable model of online latent-state learning</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6024-796X</contrib-id>
<name name-style="western">
<surname>Cochran</surname> <given-names>Amy L.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Cisler</surname> <given-names>Josh M.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Biostatistics and Medical Informatics, University of Wisconsin, Madison, Wisconsin, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Psychiatry, University of Wisconsin, Madison, Wisconsin, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Richards</surname> <given-names>Blake A.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Toronto at Scarborough, CANADA</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">cochran4@wisc.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>9</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>16</day>
<month>9</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>9</issue>
<elocation-id>e1007331</elocation-id>
<history>
<date date-type="received">
<day>29</day>
<month>12</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>13</day>
<month>8</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Cochran, Cisler</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1007331"/>
<abstract>
<p>Many models of classical conditioning fail to describe important phenomena, notably the rapid return of fear after extinction. To address this shortfall, evidence converged on the idea that learning agents rely on latent-state inferences, i.e. an ability to index disparate associations from cues to rewards (or penalties) and infer which index (i.e. latent state) is presently active. Our goal was to develop a model of latent-state inferences that uses latent states to predict rewards from cues efficiently and that can describe behavior in a diverse set of experiments. The resulting model combines a Rescorla-Wagner rule, for which updates to associations are proportional to prediction error, with an approximate Bayesian rule, for which beliefs in latent states are proportional to prior beliefs and an approximate likelihood based on current associations. In simulation, we demonstrate the model’s ability to reproduce learning effects both famously explained and not explained by the Rescorla-Wagner model, including rapid return of fear after extinction, the Hall-Pearce effect, partial reinforcement extinction effect, backwards blocking, and memory modification. Lastly, we derive our model as an online algorithm to maximum likelihood estimation, demonstrating it is an efficient approach to outcome prediction. Establishing such a framework is a key step towards quantifying normative and pathological ranges of latent-state inferences in various contexts.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Computational researchers are increasingly interested in a structured form of learning known as latent-state inferences. Latent-state inferences is a type of learning that involves categorizing, generalizing, and recalling disparate associations between observations in one’s environment and is used in situations when the correct association is latent or unknown. This type of learning has been used to explain overgeneralization of a fear memory and the cognitive role of certain brain regions important to cognitive neuroscience and psychiatry. Accordingly, latent-state inferences are an important area of inquiry. Through simulation and theory, we establish a new model of latent-state inferences. Moving forward, we aim to use this framework to measure latent-state inferences in healthy and psychiatric populations.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000025</institution-id>
<institution>National Institute of Mental Health</institution>
</institution-wrap>
</funding-source>
<award-id>112876</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6024-796X</contrib-id>
<name name-style="western">
<surname>Cochran</surname> <given-names>Amy L.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000025</institution-id>
<institution>National Institute of Mental Health</institution>
</institution-wrap>
</funding-source>
<award-id>108753</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Cisler</surname> <given-names>Josh M.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000025</institution-id>
<institution>National Institute of Mental Health</institution>
</institution-wrap>
</funding-source>
<award-id>106860</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Cisler</surname> <given-names>Josh M.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000874</institution-id>
<institution>Brain and Behavior Research Foundation</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name name-style="western">
<surname>Cisler</surname> <given-names>Josh M.</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>ALC is funded by the National Institute of Mental Health (US; MH112876; nimh.nih.gov). JMC is funded by the National Institute of Mental Health (US; MH108753; MH106860; nimh.nih.gov) and the Brain and Behavior Research Foundation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="9"/>
<table-count count="3"/>
<page-count count="31"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-09-26</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Accompanying code and data are publicly-available at <ext-link ext-link-type="uri" xlink:href="https://github.com/cochran4/OnlineLatentStateLearning" xlink:type="simple">https://github.com/cochran4/OnlineLatentStateLearning</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Learning and decision-making are fundamental aspects of day-to-day human life. Indeed, many mental health disorders can be conceptualized from the perspective of biases or errors in learning and decision-making [<xref ref-type="bibr" rid="pcbi.1007331.ref001">1</xref>]. Accordingly, the study of how humans learn and make decisions is an important topic of inquiry and the past two decades has witnessed a significant surge in the application of computational modeling approaches to the problem of human learning and decision-making [<xref ref-type="bibr" rid="pcbi.1007331.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1007331.ref005">5</xref>]. One significant insight this literature has made is the differentiation of model-free and model-based learning [<xref ref-type="bibr" rid="pcbi.1007331.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref007">7</xref>]. Model-free learning refers to relatively simple updating of cached values based on trial-and-error experience. An early and influential example of model free learning is the Rescorla-Wagner (RW) model [<xref ref-type="bibr" rid="pcbi.1007331.ref008">8</xref>], which proposed that associative strength (i.e. the degree to which a cue predicted an outcome) updates in response to new experiences in proportion to the magnitude of a prediction error (i.e., how wrong the current prediction was). The RW model, and similar model-free formulations that it inspired [<xref ref-type="bibr" rid="pcbi.1007331.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref010">10</xref>], powerfully explained many aspects of learning.</p>
<p>Nonetheless, a notable problem for model-free accounts of learning was the phenomenon of rapid recovery of responding following extinction learning [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>]. That is, model-free accounts of learning based on trial-by-trial updating predict that the return of responding following extinction should essentially occur at the same rate as initial learning. This prediction is not supported by a wealth of data demonstrating that extinguished responding can rapidly return via reinstatement (i.e., an un-signalled presentation of the outcome), context renewal (i.e., returning to the initial acquisition learning context), or spontaneous recovery (i.e., return of responding following the passage of time) [<xref ref-type="bibr" rid="pcbi.1007331.ref012">12</xref>]. This rapid return of extinguished responding is an important phenomenon with implications for treatment of clinical disorders such as anxiety disorders and substance use [<xref ref-type="bibr" rid="pcbi.1007331.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref014">14</xref>].</p>
<p>One of the first applications of a model-based account of learning that could address rapid renewal of responding following extinction was by Redish and colleagues in 2007 [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>]. They argued that an agent performs two concurrent tasks during a learning experiment: 1) updating associative strength between a cue and an outcome, and 2) recognizing the current state of the experiment and storing separate associative strengths for the acquisition and extinction contexts. Rapid recovery of responding can be explained by the agent inferring that the current state of the experiment has changed to the acquisition phase and therefore responding to the cue with the associative strength stored for the acquisition phase. Though this initial formulation had limitations [<xref ref-type="bibr" rid="pcbi.1007331.ref015">15</xref>], it stimulated subsequent research developing latent-state models of reinforcement learning [<xref ref-type="bibr" rid="pcbi.1007331.ref016">16</xref>–<xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>]. For example, if an agent is assumed to infer latent states to explain observed relationships between stimuli, actions, and outcomes, and if inferring separate latent states for acquisition and extinction phases of an experiment explains rapid recovery of responding, then it follows that blurring the experimental distinction between acquisition and extinction phases would result in less recovery of responding following extinction. That is, if the contingency between cue and outcome (e.g., 50% contingency) during the acquisition phase slowly transitions to extinction, rather than an abrupt change, then an agent is less likely to infer a separate extinction context and more likely to modify the initial acquisition memory. Because the acquisition associative strength is lower, less subsequent recovery of responding would be expected in a recall test. A carefully designed experiment using an animal model demonstrated exactly this observation, providing strong evidence for a latent-state model of learning [<xref ref-type="bibr" rid="pcbi.1007331.ref020">20</xref>].</p>
<p>The application and utility of latent-state learning models is not confined to explaining recovery of responding following extinction. Tolman’s early theory of cognitive maps [<xref ref-type="bibr" rid="pcbi.1007331.ref021">21</xref>] posits that an agent forms abstract representations of a task’s state space. There has been a recent resurgence of interest in this concept of cognitive maps in the cognitive neuroscience field informed by latent-state computational models [<xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref023">23</xref>]. This work, conducted both using animal and human experimental models, suggests that essential functions of the orbitofrontal cortex (OFC) and hippocampus are the encoding and representation of latent task space (i.e., forming cognitive maps of a task’s state space) [<xref ref-type="bibr" rid="pcbi.1007331.ref023">23</xref>]. This theory potentially powerfully describes a superordinate process that explains why the OFC and hippocampus are implicated in such a diverse array of cognitive and emotional functions. This theory also has implications that tie back into extinction learning and clinical disorders and suggest a novel understanding of the role of the OFC in mediating poor extinction learning. However, it is important to recognize that the role of the OFC remains highly debated.</p>
<p>As can be seen, latent-state theories have significant implications for our understanding of normative and disordered mechanisms of learning. The purpose of the current work is to introduce a new model of latent-state learning and to verify the suitability and utility of this model for explaining group-level effects of classical conditioning. The introduced model makes six key assumptions about how an agent performs latent-state inferences, notably that a learning agent uses latent states to index disparate <italic>associations</italic> between cues in an effort to predict rewards and that they assume latent states are relatively stable over time. Altogether, these assumptions differentiate our model from past efforts to describe latent state learning [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>]. These differences are highlighted in a series of simulation experiments. In addition to formalizing a new theory of latent-state learning, we were also motivated to provide the research community with a practical model that can be fit to data from a diverse set of experiments probing classical conditioning and latent-state learning. Even though every model has limitations and cannot be applied to every experiment, there is practical significance in being able to compare fitted parameters from a model across experiments.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Model</title>
<p>We present a model of how an agent uses latent states to learn psychological experiments consisting of a sequence of trials, wherein each trial a learning agent is exposed to a collection of cues followed by rewards (and/or penalties). We use the following notation. Cue <italic>n</italic> on trial <italic>t</italic> is denoted by <italic>c</italic><sub><italic>n</italic></sub>(<italic>t</italic>) and takes a value of 1 when the cue is present and 0 otherwise. Cues are collected in a vector <inline-formula id="pcbi.1007331.e001"><alternatives><graphic id="pcbi.1007331.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Rewards on trial <italic>t</italic> are denoted by <italic>R</italic>(<italic>t</italic>). After trial <italic>t</italic>, the strength of the association between cue <italic>n</italic> and rewards is denoted by <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>). An apostrophe denotes transposition, and the notation <italic>X</italic>(1:<italic>t</italic>) is shorthand for (<italic>X</italic>(1), …, <italic>X</italic>(<italic>t</italic>)).</p>
<p>Our model assumes a learning agent has a certain world view about how rewards are generated and wants to predict rewards efficiently based on this world view (<xref ref-type="fig" rid="pcbi.1007331.g001">Fig 1A</xref>). Their world view assumes rewards are a function of which cues are presented, a latent state, and a latent error. A latent state is an index to disparate associations between cues and rewards. In order to predict rewards, they must invert their view of the world to infer which latent state is active, how cues are associated with rewards for each latent state, and the expected uncertainty in rewards due to the latent error. We proceed to describe our model for how an agent performs these tasks. We leave details to the Methods section about how our model of latent-state learning can be derived formally from this world view.</p>
<fig id="pcbi.1007331.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007331.g001</object-id>
<label>Fig 1</label>
<caption>
<title/>
<p>A) A learning agent’s world view whereby rewards are generated according to cues, a latent state, and a latent error. In order to predict rewards, they must infer which latent state is active, the relationship between cues and rewards for each latent state, and the expected uncertainty in rewards due to the latent error. B) The proposed model for how a learning agent inverts their world view. They first observe cues to generate expectations or predictions for rewards based on <italic>L</italic> estimates of associative strengths corresponding to <italic>L</italic> latent states. Upon observing rewards, they use errors in their predictions to update associative strengths, measures of uncertainty, and beliefs in which state is active. The degree to which associative strengths can be updated depends on both the agent’s belief in the corresponding latent state and the corresponding effort matrix, which keeps track of how cues covary.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.g001" xlink:type="simple"/>
</fig>
<sec id="sec004">
<title>Building upon the Rescorla-Wagner (RW) model</title>
<p>If an agent believed in only one latent state, then a RW model could be used to determine the relationship between cues and rewards. The RW model proposes a linear relationship between cues and expected rewards. This relationship is captured by an associative strength for each cue updated according to:
<disp-formula id="pcbi.1007331.e002"><alternatives><graphic id="pcbi.1007331.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>V</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>V</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>:</mml:mo> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>c</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:munder> <mml:msub><mml:mi>V</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mstyle> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="2pt"/><mml:msub><mml:mi>c</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>V</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="2pt"/><mml:msub><mml:mi>c</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
with initial associative strength <italic>V</italic><sub><italic>n</italic></sub>(1) = 0. Term <italic>α</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is referred to as <italic>associability</italic> and is constant in the RW model. Collecting associative strengths in a vector <inline-formula id="pcbi.1007331.e003"><alternatives><graphic id="pcbi.1007331.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, we can express this update concisely as
<disp-formula id="pcbi.1007331.e004"><alternatives><graphic id="pcbi.1007331.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>The term <italic>A</italic>(<italic>t</italic>) is now a matrix capturing associability. The RW model assumes <italic>A</italic>(<italic>t</italic>) is diagonal with terms <italic>α</italic><sub><italic>n</italic></sub>(<italic>t</italic>) along its diagonal, whereas we will consider matrices that are not diagonal. Changes in associative strength depends on the learning agent’s current expectation or best guess <inline-formula id="pcbi.1007331.e005"><alternatives><graphic id="pcbi.1007331.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> for rewards given observed cues. If their expectation exceeds the actual reward, then associative strength declines for each cue present. If their expectation is below the actual reward, then associative strength increases for each cue present. By updating associative strength in vector form, we see that rewards are captured by a linear regression model of cues. Hence, cues can be continuous or represent interactions, i.e. the presence or absence of a compound of cues. That is, the same update formula applies for continuous cues and interaction terms. Also note that the RW model learns cue-reward associations rather than action-value associations as in temporal difference reinforcement learning.</p>
</sec>
<sec id="sec005">
<title>Allowing for latent-state learning</title>
<p>We build upon the RW model by allowing a learning agent to propose <italic>L</italic> competing (i.e. mutually-exclusive) <italic>associations</italic> between cues and rewards. The index to each association is referred to as a <italic>latent state</italic> and is associated with its own RW model for predicting rewards from cues. The agent learns about associations for each latent state while learning about their belief in which latent state best explains current observations. We capture belief in latent state <italic>l</italic> on trial <italic>t</italic> as a positive variable <italic>p</italic><sub><italic>l</italic></sub>(<italic>t</italic>) such that beliefs sum to one: <italic>p</italic><sub>1</sub>(<italic>t</italic>) + … + <italic>p</italic><sub><italic>L</italic></sub>(<italic>t</italic>) = 1.</p>
<p>We let <inline-formula id="pcbi.1007331.e006"><alternatives><graphic id="pcbi.1007331.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> represent the vector of associative strengths and <italic>A</italic><sub><italic>l</italic></sub>(<italic>t</italic>) represent the associability matrix for latent state <italic>l</italic>. The learning agent updates associative strength as before except for the subscript <italic>l</italic>:
<disp-formula id="pcbi.1007331.e007"><alternatives><graphic id="pcbi.1007331.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>E</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where we used the shortened notation <italic>E</italic><sub><italic>l</italic></sub>(<italic>t</italic>) to represent the prediction error for latent state <italic>l</italic> on trial <italic>t</italic>:
<disp-formula id="pcbi.1007331.e008"><alternatives><graphic id="pcbi.1007331.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>E</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula></p>
</sec>
<sec id="sec006">
<title>An associability process</title>
<p>Associability is constant in the RW model. By contrast, experiments such as those by Hall and Pearce [<xref ref-type="bibr" rid="pcbi.1007331.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref024">24</xref>] suggest associative strength is path-dependent: associability, i.e. how quickly associative strength changes, depends on the history of observations and changes over time [<xref ref-type="bibr" rid="pcbi.1007331.ref010">10</xref>]. We describe associability as depending on current beliefs and a matrix <italic>B</italic><sub><italic>l</italic></sub>(<italic>t</italic>) which we refer to as an <italic>effort</italic> matrix:
<disp-formula id="pcbi.1007331.e009"><alternatives><graphic id="pcbi.1007331.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>A</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>B</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
for a patent-specific parameter <italic>α</italic><sub>0</sub> ∈ [0, 1] controlling how much an individual weights older observations versus newer observations. The effort matrix <italic>B</italic><sub><italic>l</italic></sub>(<italic>t</italic>) is updated according to
<disp-formula id="pcbi.1007331.e010"><alternatives><graphic id="pcbi.1007331.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>B</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>B</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:mi>B</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
with <italic>B</italic><sub><italic>l</italic></sub>(1) = <italic>I</italic>. The effort matrix estimates the matrix of cue second moments associated with each latent state. Second moment information of cues has been integrated into other models of learning, but usually in the form of variances and covariances rather than second moments directly [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>]. The use of the effort matrix is motivated in our derivation of our model in the Methods section and is similar to matrices found in an online algorithm for solving the machine learning problem of contextual bandits with linear rewards [<xref ref-type="bibr" rid="pcbi.1007331.ref025">25</xref>].</p>
</sec>
<sec id="sec007">
<title>Updating latent-state beliefs</title>
<p>Beliefs link latent states in our model. The assumed world view of the learning agent proposes that rewards are generated on trial <italic>t</italic> as follows: a latent random variable <italic>X</italic>(<italic>t</italic>)∈{1, …, <italic>L</italic>} is drawn from some distribution and rewards are drawn depending on the current latent state <italic>X</italic>(<italic>t</italic>) and cues <inline-formula id="pcbi.1007331.e011"><alternatives><graphic id="pcbi.1007331.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> (<xref ref-type="fig" rid="pcbi.1007331.g001">Fig 1A</xref>). Further, rewards are assumed to be mutually independent between trials conditional on latent states, and latent states are assumed to be Markovian, i.e. <italic>X</italic>(<italic>t</italic>) depends on <italic>X</italic>(1:<italic>t</italic> − 1) only through <italic>X</italic>(<italic>t</italic> − 1). Under these assumptions, Bayes law yields a posterior distribution over latent states from observed rewards given by:
<disp-formula id="pcbi.1007331.e012"><alternatives><graphic id="pcbi.1007331.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mo>[</mml:mo> <mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>k</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>∝</mml:mo> <mml:mi mathvariant="double-struck">P</mml:mi> <mml:mo>[</mml:mo> <mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>k</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:munderover> <mml:mi mathvariant="double-struck">P</mml:mi> <mml:mo>[</mml:mo> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>l</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where we suppress the dependence on cue vectors to shorten notation. The posterior probability <italic>ρ</italic><sub><italic>l</italic></sub>(<italic>t</italic>) that latent state <italic>X</italic>(<italic>t</italic>) is <italic>l</italic> based on observations up to trial <italic>t</italic> is then:
<disp-formula id="pcbi.1007331.e013"><alternatives><graphic id="pcbi.1007331.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mo>[</mml:mo> <mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>l</mml:mi> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>L</mml:mi></mml:munderover> <mml:mi mathvariant="double-struck">P</mml:mi> <mml:mo>[</mml:mo> <mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>k</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>l</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>From this expression, we see that optimal Bayesian inference requires enumerating over all possible sequences of latent states, the number of which grows exponentially with <italic>t</italic>. Such computation is generally considered optimistic for human computation [<xref ref-type="bibr" rid="pcbi.1007331.ref026">26</xref>]. Efforts are made to reduce computation such as with particle methods [<xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>], assuming one transition between states [<xref ref-type="bibr" rid="pcbi.1007331.ref027">27</xref>], or using a functional form of memory that does not maintain the entire history of observations.</p>
<p>We capture a more general dynamic learning environment similar to [<xref ref-type="bibr" rid="pcbi.1007331.ref026">26</xref>], by assuming latent states are a Markov chain in which a latent variable transitions to itself from one trial to the next with probability 1 − <italic>γ</italic>(<italic>L</italic> − 1)/<italic>L</italic> and transitions to a new state (all new states being equally-likely) with probability <italic>γ</italic>(<italic>L</italic> − 1)/<italic>L</italic>. This assumption reflects that learning often involve blocks of consecutive trials, or stages, in which rewards are generated in an identical manner. An optimal Bayesian filtering equation could then be used to update beliefs
<disp-formula id="pcbi.1007331.e014"><alternatives><graphic id="pcbi.1007331.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∝</mml:mo> <mml:mo>(</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo> <mml:msub><mml:mi>ρ</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi>γ</mml:mi> <mml:mi>L</mml:mi></mml:mfrac> <mml:mo>)</mml:mo> <mml:mi mathvariant="double-struck">P</mml:mi> <mml:mo>[</mml:mo> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>|</mml:mo> <mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>l</mml:mi> <mml:mo>]</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>Unfortunately, this equation requires the learning agent knows the probability distribution of rewards for each latent state. We thus do not think the agent reasons in an optimal Bayesian way. Instead, we initialize beliefs <italic>p</italic><sub><italic>l</italic></sub>(0) = 1/<italic>L</italic> and propose that the agent uses an <italic>approximate Bayesian filtering equation</italic> (cf. [<xref ref-type="bibr" rid="pcbi.1007331.ref028">28</xref>])
<disp-formula id="pcbi.1007331.e015"><alternatives><graphic id="pcbi.1007331.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>l</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfrac> <mml:mo>(</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi>γ</mml:mi> <mml:mi>L</mml:mi></mml:mfrac> <mml:mo>)</mml:mo> <mml:mi>ϕ</mml:mi> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>E</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>σ</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
where <italic>ϕ</italic> is the probability density function of a standard normal random variable; <italic>σ</italic>(<italic>t</italic>) is the agent’s estimate of the standard deviation of rewards at the start of trial <italic>t</italic>; and
<disp-formula id="pcbi.1007331.e016"><alternatives><graphic id="pcbi.1007331.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>l</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>L</mml:mi></mml:munderover> <mml:mo>(</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi>γ</mml:mi> <mml:mi>L</mml:mi></mml:mfrac> <mml:mo>)</mml:mo> <mml:mi>ϕ</mml:mi> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>E</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>σ</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
is a normalizing constant to ensure beliefs sum to 1. In other words, we replace the distribution/density function of rewards in the optimal Bayesian filtering equation with a normal density function with mean given by the agent’s current estimate <inline-formula id="pcbi.1007331.e017"><alternatives><graphic id="pcbi.1007331.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and standard deviation given by the agent’s current estimate <italic>σ</italic>(<italic>t</italic>). Even though we use a normal density function, rewards do not need to be normally-distributed or even continuous.</p>
</sec>
<sec id="sec008">
<title>Measuring expected uncertainty</title>
<p>The learning agent may also track the standard deviation <italic>σ</italic>(<italic>t</italic>) of prediction errors. With only one standard deviation, we assume the learning agent estimates the standard deviation <italic>pooled</italic> over each latent state. We use pooled estimates to reduce the number of variables, but it may be more realistic to use a separate estimate for each latent state. We use the following update:
<disp-formula id="pcbi.1007331.e018"><alternatives><graphic id="pcbi.1007331.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mrow><mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>(</mml:mo> <mml:mover><mml:msup><mml:mi>E</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula>
where <inline-formula id="pcbi.1007331.e019"><alternatives><graphic id="pcbi.1007331.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is some initial estimate of the variance; <italic>β</italic><sub>0</sub> is a constant associability parameter for variance <italic>σ</italic><sup>2</sup>(<italic>t</italic>); and <inline-formula id="pcbi.1007331.e020"><alternatives><graphic id="pcbi.1007331.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:mover><mml:msup><mml:mi>E</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the squared prediction error averaged over latent states:
<disp-formula id="pcbi.1007331.e021"><alternatives><graphic id="pcbi.1007331.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover><mml:msup><mml:mi>E</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>l</mml:mi></mml:munder> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>E</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula></p>
<p>Variance is also used in other models of latent-state learning, such as the model in Redish et al [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>] and the model in Gershman et al [<xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>]. Further, Yu and Dayan [<xref ref-type="bibr" rid="pcbi.1007331.ref026">26</xref>] proposed a neural mechanism, via acetylcholine signalling, by which a learning agent keeps track of expected uncertainty in changing environments. As a staple to statistical inference, variance is a natural candidate for quantifying expected uncertainty.</p>
</sec>
<sec id="sec009">
<title>Measuring unexpected uncertainty</title>
<p>Many latent-state learning models assume the number of latent state can grow [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>]. Similarly, we use an <italic>online</italic> mechanism to trigger an addition of new latent states when needed to explain rewards. This mechanism reflects the need to capture <italic>unexpected uncertainty</italic>, as discussed in Yu an Dayan [<xref ref-type="bibr" rid="pcbi.1007331.ref026">26</xref>]. It is based on Page’s algorithm [<xref ref-type="bibr" rid="pcbi.1007331.ref029">29</xref>] for solving a problem known as change point detection (cf. [<xref ref-type="bibr" rid="pcbi.1007331.ref030">30</xref>]). The idea is to test whether we can reject our current model in favor of an alternative model with an extra latent state. Specifically, we let <italic>L</italic> be the number of latent states actively considered on trial <italic>t</italic>. We keep track of model performance using a statistic <italic>q</italic>(<italic>t</italic>):
<disp-formula id="pcbi.1007331.e022"><alternatives><graphic id="pcbi.1007331.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mo>(</mml:mo> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi>ϕ</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>l</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>δ</mml:mi> <mml:mo>,</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
with a scalar parameter <italic>δ</italic> and initialized <italic>q</italic>(1) = 0. Importantly, <italic>ϕ</italic>(0)/<italic>l</italic><sub>0</sub> is the likelihood ratio between a one-state model with expected rewards given by actual rewards <italic>R</italic>(<italic>t</italic>) and the current model, where <italic>l</italic><sub>0</sub> was the normalizing constant for beliefs at <xref ref-type="disp-formula" rid="pcbi.1007331.e016">Eq (6)</xref>. If <italic>q</italic>(<italic>t</italic>) exceeds a threshold <italic>η</italic>, we let
<disp-formula id="pcbi.1007331.e023"><alternatives><graphic id="pcbi.1007331.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mspace width="2.em"/><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>;</mml:mo> <mml:mspace width="2.em"/><mml:msub><mml:mover accent="true"><mml:mi>B</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>I</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula></p>
<p>Note that <inline-formula id="pcbi.1007331.e024"><alternatives><graphic id="pcbi.1007331.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> so that the current reward is the expected reward for the new state. We then replace <italic>L</italic> with <italic>L</italic> + 1.</p>
</sec>
<sec id="sec010">
<title>Changes in context</title>
<p>Learning often involves changes in context whether it be new surroundings, backdrop, or point in time. Context changes are believed to influence learning in a distinctly different way than a cue influences learning [<xref ref-type="bibr" rid="pcbi.1007331.ref012">12</xref>]. Our model supposes that these changes in context corrodes current beliefs, whereby beliefs in a latent state on a previous trial are thought to be less informative on the present trial. For a temporal shift in context, our model supposes this corrosion increases with time. While the Gershman (2017) model uses a temporal kernel, we replace beliefs <italic>p</italic><sub><italic>l</italic></sub>(<italic>t</italic>) at the end of trial <italic>t</italic> with
<disp-formula id="pcbi.1007331.e025"><alternatives><graphic id="pcbi.1007331.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>I</mml:mi> <mml:mi>T</mml:mi> <mml:mi>I</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>I</mml:mi> <mml:mi>T</mml:mi> <mml:mi>I</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>)</mml:mo> <mml:mo>/</mml:mo> <mml:mi>L</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
where <italic>ITI</italic> is the time between trials within a task phase. This update amounts to repeatedly updating latent state beliefs for <italic>ITI</italic> − 1 iterations according to the Markov chain transition probabilities. Beliefs would become evenly distributed between latent states in the limit as <italic>ITI</italic> goes to infinity. For a visual or spatial shift in context, we set <italic>ITI</italic> = ∞ in equation above, leading to uniform beliefs 1/<italic>L</italic>, i.e. all latent states are believed to be equally likely. As a result, an agent does not carry their beliefs forward to the next trial when there is a spatial or visual change of context.</p>
<p>A temporal shift in context might also alter how associative strengths and beliefs are updated. Gershman et al [<xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>] describes the influence of retrieval on a memory by way of “rumination”. This mechanism is captured in the model of Gershman (2017) [<xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>] and in our model by repeatedly replacing associative strengths <inline-formula id="pcbi.1007331.e026"><alternatives><graphic id="pcbi.1007331.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> with the new estimate <inline-formula id="pcbi.1007331.e027"><alternatives><graphic id="pcbi.1007331.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> performing the updates at the end of trial <italic>t</italic> again. This process is repeated for a number of trials given by min{<italic>χ</italic>, <italic>ITI</italic> − 1} where <italic>χ</italic> is a patient-specific parameter.</p>
</sec>
<sec id="sec011">
<title>Putting it all together</title>
<p>Our latent-state model of learning is summarized in <xref ref-type="fig" rid="pcbi.1007331.g001">Fig 1B</xref> and in Algorithm 1. Briefly, a learning agent undergoes the following each trial: they observe cues, generate expectations for rewards, observe a reward, measure error, update their belief in latent states, and update associative strength and measures of uncertainty. In between trials, a learning agent may adjust their beliefs and associative strengths to account for contextual shifts whether they be visual, spatial, or temporal. Further, parameters can be tuned to describe differences in learning between agents: <italic>α</italic><sub>0</sub> influences the rate of learning associative strengths, <italic>β</italic><sub>0</sub> influences the rate of learning variance, <italic>γ</italic> influences transitions between latent state, <italic>σ</italic><sub>0</sub> influences initial expected uncertainty, <italic>ν</italic> and <italic>δ</italic> influence unexpected uncertainty, and <italic>χ</italic> determines rumination.</p>
<p>Our model is grounded by six major predictions or assumptions about how an agent learns:</p>
<list list-type="order">
<list-item>
<p><bold>An agent learns</bold> <italic><bold>online</bold></italic>. Model variables are updated on each trial using only current observations and current values of model variables. Consequently, an agent can learn without remembering observations of cues and rewards from prior trials. Their memory and computational requirements grow only in the number of latent states rather than the number of trials, which may be a more realistic reflection of how humans learn. The Rescorla-Wagner model and other models also use online updates for certain model variables [<xref ref-type="bibr" rid="pcbi.1007331.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>].</p>
</list-item>
<list-item>
<p><bold>An agent uses latent states to</bold> <italic><bold>predict rewards</bold></italic>. In deriving our model (see <xref ref-type="sec" rid="sec023">Methods</xref> section), we start with the assumption that the agent’s goal is to predict rewards. The role of a latent state is to account for different predictions that can be made upon observing cues. By contrast, the role of a latent state could be to discriminate between different sets of cues-reward observations [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>], which in turn could be used to predict rewards.</p>
</list-item>
<list-item>
<p><bold>Associability, i.e. how quickly associations are updated, depends on beliefs in latent states and cue novelty</bold>. Associative strength updates quickly for latent states believed to be active, but slowly for latent states believed to be inactive. This feature is similar to the Mackintosh model [<xref ref-type="bibr" rid="pcbi.1007331.ref031">31</xref>] in that associability increases when rewards are accurately predicted. However, associative strength also updates quickly when an agent switches their belief to a new latent state to account for unexpected or surprising outcomes, i.e. outcomes in which prediction error is larger relative to the expected uncertainty. This feature is similar to Pearce-Hall model [<xref ref-type="bibr" rid="pcbi.1007331.ref009">9</xref>], which supposes associability increases in the presence of uncertainty. Meanwhile, the effort matrix, which tracks how often a cue appears (diagonal entries) and a pair of cues appear (off-diagonal entries), also determines associability. As a result, associative strength is updated more quickly when novel sets of cues are presented.</p>
</list-item>
<list-item>
<p><bold>Latent states are relatively stable between trials</bold>. Latent states are expected to be the same from one trial to the next with a certain probability. This assumption causes beliefs in latent states to be relatively consistent between trials. An alternative is to assume latent states are exchangeable between trials, allowing beliefs in latent states to shift more sharply between trials [<xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>]. Stability between trials, however, might degrade with time. Our model assumes that with time, the last trial becomes less and less informative about the latent state, to the point that the agent eventually believes each latent state is equally likely regardless of prior beliefs.</p>
</list-item>
<list-item>
<p><bold>Beliefs are maintained over multiple latent states</bold>. Our model allows an agent to maintain beliefs over multiple latent states on each trial as opposed to believing in only latent state. For example, an agent is able to believe that two latent states are equally likely. This allows the agent to explicitly state their beliefs in competing associations, as is required by some learning tasks [<xref ref-type="bibr" rid="pcbi.1007331.ref032">32</xref>]. Maintaining beliefs over multiple states is a common feature for Bayesian models [<xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref032">32</xref>]. An alternative is to classify each trial to one state [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref033">33</xref>].</p>
</list-item>
</list>
<p><bold>Algorithm 1</bold>: Proposed model of latent-state learning which updates variables <italic>online</italic>, i.e. does not keep track of past cues and rewards.</p>
<p specific-use="line">/* <monospace>Loop through trials</monospace>                */</p>
<p specific-use="line"><bold>for</bold> <italic>each trial</italic> <italic>t</italic> = 1, …, <italic>T</italic> <bold>do</bold></p>
<p specific-use="line"> /* <monospace>State inference</monospace>                  */</p>
<p specific-use="line"> Update latent-state beliefs <italic>p</italic><sub>1</sub>(<italic>t</italic>), …, <italic>p</italic><sub><italic>L</italic></sub>(<italic>t</italic>) with <xref ref-type="disp-formula" rid="pcbi.1007331.e015">Eq (5)</xref>;</p>
<p specific-use="line"> /* <monospace>Unexpected uncertainty learning</monospace>        */</p>
<p specific-use="line"> Update change point statistic <italic>q</italic>(<italic>t</italic>) with <xref ref-type="disp-formula" rid="pcbi.1007331.e022">Eq (9)</xref>;</p>
<p specific-use="line"> <bold>if</bold> <italic>q</italic>(<italic>t</italic> + 1) &gt; <italic>ν</italic> <bold>then</bold></p>
<p specific-use="line">  /* <monospace>Add new state</monospace>                  */</p>
<p specific-use="line">  Set <inline-formula id="pcbi.1007331.e028"><alternatives><graphic id="pcbi.1007331.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>B</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> with <xref ref-type="disp-formula" rid="pcbi.1007331.e023">Eq (10)</xref>;</p>
<p specific-use="line">  Update number of latent states <italic>L</italic> ← <italic>L</italic> + 1;</p>
<p specific-use="line"> <bold>end</bold></p>
<p specific-use="line"> /* <monospace>Value learning</monospace>                  */</p>
<p specific-use="line"> Update associative strengths <inline-formula id="pcbi.1007331.e029"><alternatives><graphic id="pcbi.1007331.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>L</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> with <xref ref-type="disp-formula" rid="pcbi.1007331.e007">Eq (1)</xref>;</p>
<p specific-use="line"> Update effort matrices <inline-formula id="pcbi.1007331.e030"><alternatives><graphic id="pcbi.1007331.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>B</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>B</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>L</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> with <xref ref-type="disp-formula" rid="pcbi.1007331.e010">Eq (4)</xref>;</p>
<p specific-use="line"> /* <monospace>Expected uncertainty learning</monospace>         */</p>
<p specific-use="line"> Update variance <italic>σ</italic><sup>2</sup>(<italic>t</italic>) with <xref ref-type="disp-formula" rid="pcbi.1007331.e018">Eq (7)</xref>;</p>
<p specific-use="line"> /* <monospace>Account for context</monospace>               */</p>
<p specific-use="line"> <bold>if</bold> <italic>Context shifts after trial</italic> <italic>t</italic> <bold>then</bold></p>
<p specific-use="line">  Repeatedly update associative strength <inline-formula id="pcbi.1007331.e031"><alternatives><graphic id="pcbi.1007331.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> with <xref ref-type="disp-formula" rid="pcbi.1007331.e007">Eq (1)</xref> if temporal shift;</p>
<p specific-use="line">  Update latent-state beliefs with <xref ref-type="disp-formula" rid="pcbi.1007331.e025">Eq (11)</xref>;</p>
<p specific-use="line"> <bold>end</bold></p>
<p specific-use="line"><bold>end</bold></p>
</sec>
</sec>
<sec id="sec012">
<title>Overview of simulation</title>
<p>We used simulation to verify that our latent-state model can reproduce a set of group-level effects observed from classical experiments on learning (<xref ref-type="table" rid="pcbi.1007331.t001">Table 1</xref>). Our model was compared to the Rescorla-Wagner (RW) model, fRL+decay model of Niv et al [<xref ref-type="bibr" rid="pcbi.1007331.ref034">34</xref>] and three latent-state learning models: the Redish (2007) model [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>], the infinite-mixture model of Gershman and Niv in [<xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>], and the Gershman (2017) model in [<xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>]. Other model comparisons are considered in <xref ref-type="supplementary-material" rid="pcbi.1007331.s001">S1 Text</xref> and <xref ref-type="supplementary-material" rid="pcbi.1007331.s002">S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1007331.s003">S2</xref> Figs. For each model, we computed associative strength per cue and belief in each latent state. For our model, the infinite-mixture model, and the Gershman (2017) model, we defined associative strength as the expected reward conditional on the cue being presented alone and belief in a latent state as the estimated probability of the latent state given observations. In order to compare the Redish (2007) model to other models, we defined associative strength for each cue as average estimated value between reinforcing and not reinforcing the cue presented alone and defined belief in a latent state to be 1 if the state was identified as the current agent state and 0 otherwise. Our model includes pairwise interactions between cues as additional cues and also centers rewards (i.e. <italic>R</italic>(<italic>t</italic>) = −1/2 and <italic>R</italic>(<italic>t</italic>) = 1/2 rather than <italic>R</italic>(<italic>t</italic>) = 0 and <italic>R</italic>(<italic>t</italic>) = 1). Additional details about the simulation are found in the Methods section.</p>
<table-wrap id="pcbi.1007331.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007331.t001</object-id>
<label>Table 1</label>
<caption>
<title>Simulated experiments and group-level effect observed in each experiment.</title>
<p>Models were evaluated based on its ability to reproduce the observed effect for each simulated experiment.</p>
</caption>
<alternatives>
<graphic id="pcbi.1007331.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Experiment</th>
<th align="left">Observed effect</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Blocking [<xref ref-type="bibr" rid="pcbi.1007331.ref035">35</xref>]</td>
<td align="left">Associative strength of a neutral cue is blocked when reinforced with a cue already associated with the reward</td>
</tr>
<tr>
<td align="left">Overexpectation [<xref ref-type="bibr" rid="pcbi.1007331.ref036">36</xref>]</td>
<td align="left">Associative strength of two cues weakens when these cues are reinforced together after reinforcing each cue separately</td>
</tr>
<tr>
<td align="left">Conditioned inhibition [<xref ref-type="bibr" rid="pcbi.1007331.ref037">37</xref>]</td>
<td align="left">A cue can inhibit responding to a reward after alternating between reinforcing a separate cue and presenting both cues with a weaker reward or no reward.</td>
</tr>
<tr>
<td align="left">Wilson <italic>et al</italic> (1992) [<xref ref-type="bibr" rid="pcbi.1007331.ref038">38</xref>]</td>
<td align="left">Associative strength of a cue increases more when the cue is a less accurate predictor of a reward (also known as the Hall-Pearce effect [<xref ref-type="bibr" rid="pcbi.1007331.ref024">24</xref>])</td>
</tr>
<tr>
<td align="left">Rescorla (2000) [<xref ref-type="bibr" rid="pcbi.1007331.ref039">39</xref>]</td>
<td align="left">Changes in associative strength can be greater for one cue over another when presented at the same time</td>
</tr>
<tr>
<td align="left">Partial reinforcement [<xref ref-type="bibr" rid="pcbi.1007331.ref040">40</xref>–<xref ref-type="bibr" rid="pcbi.1007331.ref042">42</xref>]</td>
<td align="left">Associative strength of a cue is slower to extinguish after partial reinforcement compared to continuous reinforcement (also known as the partial reinforcement extinction effect)</td>
</tr>
<tr>
<td align="left">Backwards blocking [<xref ref-type="bibr" rid="pcbi.1007331.ref043">43</xref>]</td>
<td align="left">Associative strength of a cue weakens when a separate cue is reinforced after reinforcing both cues together</td>
</tr>
<tr>
<td align="left">Rapid return of fear &amp; renewal [<xref ref-type="bibr" rid="pcbi.1007331.ref012">12</xref>]</td>
<td align="left">Associative strength of cue returns quickly after extinction, particularly when extinction is associated with a different context</td>
</tr>
<tr>
<td align="left">Spontaneous recovery [<xref ref-type="bibr" rid="pcbi.1007331.ref012">12</xref>]</td>
<td align="left">Associative strength of a cue returns more greatly after extinction with a longer interval between extinction and renewal</td>
</tr>
<tr>
<td align="left">Memory modification [<xref ref-type="bibr" rid="pcbi.1007331.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref045">45</xref>]</td>
<td align="left">Associative strength of a cue to a fearful outcome can be weakened if extinction occurs within a certain time window following a single retrieval trial</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec013">
<title>Blocking and other effects</title>
<p>We first tested whether our model could reproduce learning effects that established the RW model as a powerful model of associative learning (<xref ref-type="fig" rid="pcbi.1007331.g002">Fig 2</xref>). In a blocking experiment, for instance, a learning agent is conditioned to associate a cue (Cue A) with a reward, leading to a high associative strength for Cue A. Afterwards, Cue A is repeatedly presented with another cue (Cue B). The compound is then reinforced with a reward. Even though Cue B is reinforced, Cue B does not acquire the high associative strength as Cue A did. That is, conditioning of Cue A <italic>blocked</italic> conditioning of cue B. The RW model, our latent-state model, and the Gershman (2017) model predict blocking because of the way associative strength is updated in each model: updates depend on prediction error and predictions depend on the sum of associative strengths of present cues. Consequently, associative strength of Cue B changes slightly, because the compound of an excitatory Cue A and neutral Cue B yields small prediction errors.</p>
<fig id="pcbi.1007331.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007331.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Simulated associative strengths of cues during blocking, overexpectation, and conditioned inhibition experiments, which the Rescorla-Wagner (RW) model famously could explain.</title>
<p>Built upon the RW model, our latent-state model and the Gershman (2017) model can also explain these experiments. Gray dashed lines demarcate experimental stages.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.g002" xlink:type="simple"/>
</fig>
<p>Our latent-state model reproduces overexpectation and conditioned inhibition, two other group-level effects explained by the RW model. An overexpectation experiment consists of reinforcing two cues separately with a reward and then later as a compound. When presented as a compound, associative strengths decrease. A conditioned inhibition experiment consists of intermixing the reinforcement of one cue (Cue A) with the omission of a reward when the cue is presented with another cue (Cue B). Cue A increases in associative strength while Cue B decreases in associative strength. As with blocking, the RW model, our latent-state model, and the Gershman (2017) model capture overexpectation and conditioned inhibition because of the way associative strength is updated. For overexpectation, expectations are twice actual rewards when the cues are presented together, yielding negative prediction errors and a decrease in associative strength for each cue. For conditioned inhibition, the negative associative strength of Cue B negates the positive associative strength of Cue A so that the sum of their associative strengths predicts the weaker reward. The Redish (2007) and infinite-mixture models do not update associative strength in the same way as the RW model, whereas the fRL+decay model decays associative strength of cues that are not presented, leading to different predictions for blocking, overexpectation, or conditioned inhibition experiments.</p>
</sec>
<sec id="sec014">
<title>Learning effects not explained by the RW model</title>
<p>We also tested whether our latent-state learning model could reproduce historically-important learning effects not predicted by the RW model due to its assumption of constant associability (<xref ref-type="fig" rid="pcbi.1007331.g002">Fig 2</xref>).</p>
<sec id="sec015">
<title>Associability depends on beliefs in latent-states</title>
<p>Latent-state learning allows an agent to attribute sudden shifts in associations to new latent states, even when the shift is unsignalled. Associability starts relatively high when learning about a new state, thereby explaining the well-known Pearce-Hall effect [<xref ref-type="bibr" rid="pcbi.1007331.ref009">9</xref>] in which associability remains high in unpredictable circumstances.</p>
<p>To illustrate, we find that a shift in latent state beliefs can explain Experiment 1 from Wilson et al (1992) [<xref ref-type="bibr" rid="pcbi.1007331.ref038">38</xref>]. This experiment involved two groups, Group C and Group E. For Group C, the experiment consisted of alternating between reinforcing and not reinforcing two cues (a tone denoted by Cue B and a light denoted by Cue A) with a reward (food), followed by reinforcing just the light. Group E had similar experiment conditions, except the tone was omitted in the middle of the experiment on non-reinforced trials. The omission was expected to decrease the associative strength of the light for Group E relative to Group C, which would be observed when the light was paired with food. Surprisingly, Group E responded more favorably to the light than Group C. The leading explanation was that associability of the light was higher for Group E than Group C [<xref ref-type="bibr" rid="pcbi.1007331.ref010">10</xref>].</p>
<p>Our latent-state model offers an explanation: that while indeed the associative strength of the light does decrease for Group E relative to Group C, the pairing of the light (Cue A) solely with food is so surprising to Group E that they correctly infer the change in experimental conditions thereby shifting to a new latent state to build new expectations for the light (<xref ref-type="fig" rid="pcbi.1007331.g003">Fig 3</xref>). This shift would be observed as higher associability of the light in Group E than Group C, agreeing with experimental results. The Redish (2007) model also captures greater responding to the light in Group E than Group C. By contrast, the RW model, infinite-mixture, and Gershman (2017) models do not show greater responding to the light (cue A) in Group E. Meanwhile, the fRL+decay model captures this experiment but provides an alternative explanation: associative strength of the tone decays less in Group E because it is presented more often, which is compensated by an increase in the associative strength of the light.</p>
<fig id="pcbi.1007331.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007331.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Associability depends on latent-state beliefs.</title>
<p><bold>A)</bold> Experimental results from Stage 3 of the Wilson et al (1992) experiment [<xref ref-type="bibr" rid="pcbi.1007331.ref027">27</xref>]. Group E had higher magazine activity, i.e. greater responding, than Group C during the light (Cue A) in Stage 3. Thus, it was believed the light had greater associative strength in Group E than Group C during Stage 3. Reprinted from Paul N. Wilson, Patrick Boumphrey, &amp; John M. Pearce, Quarterly Journal of Experimental Psychology 44:1 pp. 17-36. Reprinted by Permission of SAGE Publications, Ltd. <bold>B)</bold> Simulation of associative strength of the light (Cue A) and latent state beliefs from the Wilson et al (1992) experiment. Our model predicts that only Group E detects the change in experimental conditions and shifts their beliefs. Because of this shift, associability is higher in Group E than C during Stage 3, leading to higher associative strength of the light. Beliefs in the first latent state (dark and light blue solid lines) and the second latent state (dark and light blue dashed lines) are shown for models with latent states. Gray dashed lines demarcate experimental stages.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.g003" xlink:type="simple"/>
</fig>
<p>Other learning effects can also be similarly explained by our model as a shift in latent-state beliefs (<xref ref-type="fig" rid="pcbi.1007331.g004">Fig 4</xref>). Experiments 1A-B in [<xref ref-type="bibr" rid="pcbi.1007331.ref039">39</xref>], for example, examined whether a cue (Cue A) would undergo the same change in associative strength as another cue (Cue B) when presented together, as would be predicted by the RW model. Rescorla, however, found that Cue A decreased more in associative strength than Cue B. Our latent-state model explains that differences in associability arises because of a shift in latent states. In Experiment 1B, for example, the omission of the reward is so surprising to the agent—Cue A was always reinforced—that learning agents correctly infer a change in conditions and switch their beliefs to a new latent state in which they do not expect Cues A and B to be reinforced. Since rewards were originally expected to follow Cue A but not cue B, the associative strength of Cue A experiences the greater change. By contrast, the only other models to capture greater associability of Cue B during stage 2 was the infinite-mixture model in experiment 1A and the Redish (2007) model in experiment 1B. Otherwise, the other models for the given set of parameters do not detect the change in experimental conditions, causing cues A and B to change a similar amount during Stage 2.</p>
<fig id="pcbi.1007331.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007331.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Another demonstration of associability depending on beliefs in latent states.</title>
<p>Experimental results from <bold>A)</bold> Experiment 1A and <bold>B)</bold> Experiment 1B by Rescorla (2000) [<xref ref-type="bibr" rid="pcbi.1007331.ref039">39</xref>]. In both experiments, Rescorla concluded that associative strength increased more for Cue B relative to Cue A when presented together based on compound tests that showed greater responding in the compound with the Cue B than the compound with Cue A. This result suggested that associability can differ between cues even when presented together. Reprinted from “Associative Changes in Excitors and Inhibitors Differ When They Are Conditioned in Compound” by R.A. Rescorla, 2000, <italic>Journal of Experimental Psychology</italic>, <italic>26</italic>, p. 430-431. Reprinted with permission from the American Psychological Association. <bold>C)</bold> Total change in associative strength was simulated during Stage 2 of Experiments 1A-B from Rescorla (2000) [<xref ref-type="bibr" rid="pcbi.1007331.ref039">39</xref>]. The RW model does not capture these effects since associability is constant whereas our model captures these effects because latent-state beliefs alters associability. Beliefs in the first latent state (black solid lines) and the second latent state (black dashed lines) are shown for models with latent states. Gray dashed lines demarcate experimental stages.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.g004" xlink:type="simple"/>
</fig>
<p>The partial extinction learning effect (PREE) is also similarly explained by our model as a shift in latent-state beliefs (<xref ref-type="fig" rid="pcbi.1007331.g005">Fig 5</xref>). Partial reinforcement involves alternating between reinforcing and not reinforcing a cue. It was found that a response was harder to extinguish after partial reinforcement compared to continuous reinforcement [<xref ref-type="bibr" rid="pcbi.1007331.ref040">40</xref>]. Our model explains that an agent is better able to discriminate between reinforcement and extinction with a continuous reinforcement schedule. This allows the agent to switch their beliefs to a new latent state and thus experience faster extinction. PREE was also observed even if continuous reinforcement was administered in between partial reinforcement and extinction [<xref ref-type="bibr" rid="pcbi.1007331.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref042">42</xref>]. In this case, an agent is still better able to discriminate between reinforcement and extinction with a continuous reinforcement schedule, but differences between reinforcement schedules are smaller as an agent shifts their beliefs to a new latent state in both cases. Only the Redish (2007) model was also able to capture greater extinction for continuous reinforcement compared to partial reinforcement in either of the two PREE simulation experiments.</p>
<fig id="pcbi.1007331.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007331.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Partial reinforcement extinction effect.</title>
<p><bold>A)</bold> Experimental results from Jenkins [<xref ref-type="bibr" rid="pcbi.1007331.ref041">41</xref>] demonstrating that the associative strength of a cue is harder to extinguish after partial reinforcement (Group 20P) compared to continuous reinforcement (Group 20R). Reprinted from “Resistance to extinction when partial reinforcement is followed by regular reinforcement” by H.M. Jenkins, 1962, <italic>Journal of Experimental Psychology</italic>, <italic>64</italic>, p. 443. Reprinted with permission from the American Psychological Association. <bold>B</bold>) Simulation of partial reinforcement effect (Experiment 1). This effect is observed even when partial reinforcement is followed by continuous reinforcement prior to extinction (Experiment 2). Our model captures these effects, because an agent is better able to discriminate between reinforcement and extinction with a continuous reinforcement schedule. The agent can thus shift their beliefs to a new latent state in order to build new associations for extinction. Beliefs in the first latent state are shown for models with latent states. Gray dashed lines demarcate experimental stages.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec016">
<title>Associability depends on history of cue presentation</title>
<p>Our model encodes the history of cue presentation in effort matrices <italic>B</italic><sub><italic>l</italic></sub>(<italic>t</italic>), endowing associability in our model with unique properties. One example is backwards blocking (<xref ref-type="fig" rid="pcbi.1007331.g006">Fig 6</xref>). A compound of two cues (Cues A and X) is reinforced with a reward followed by only one of the cues (Cue X) being reinforced. In the second part, Cue A gains associative strength while Cue X loses associative strength <italic>even though it is not presented</italic>. The RW model cannot explain backwards blocking, because a cue must be present to change its associative strength. The infinite-mixture and the Gershman (2017) model also did not capture backwards blocking. By contrast, the associative strength of Cue X correctly decreased in the second part for our latent-state model. Our latent-state model first learns the compound predicts a reward. Without additional information, it splits associative strength equally between the two cues. Later, our latent-state model learns Cue A predicts rewards. Reconciling both parts, our latent-state model increases the associative strength of Cue A while decreasing the associative strength of Cue X. In other words, our latent-state model learns about the <italic>difference</italic> in associative strengths between Cues A and X. Mathematically, applying the inverse of <italic>B</italic><sub><italic>l</italic></sub>(<italic>t</italic>) to the cue vector <italic>c</italic>(<italic>t</italic>) rotates the cue vector from being in the direction of Cue A to be in the direction of Cue A minus Cue X. Only the fRL+decay model was also able to reproduce backwards blocking, but provided an alternative explanation: associative strength decays for any cue that is absent, such as Cue X in Stage 2 of this experiment.</p>
<fig id="pcbi.1007331.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007331.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Associability depends on history of cue presentation.</title>
<p><bold>A)</bold> Experimental results of a backwards blocking experiment from Miller and Matute [<xref ref-type="bibr" rid="pcbi.1007331.ref043">43</xref>]. After two stages of the experiment, a backwards blocking group (BB) had significantly slower response time to Cue X than a control group (CON) even though Cue X was presented in an identical manner between groups. Their result suggested the associative strength of Cue X can change on trials it is not presented. Reprinted from “Biological Significance in Forward and Backward Blocking Discrepancy Between Animal Conditioning and Human Causal Judgement” by R.R. Miller and H. Matute, 1996, <italic>Journal of Experimental Psychology</italic>, <italic>125</italic>, p. 374. Reprinted with permission from the American Psychological Association. <bold>B)</bold> Simulation of a backwards blocking experiment. In our model, associability depends on the history of cue presentation through effort matrices. After the combined associative strength of Cue A and Cue X is learned, these effort matrices rotate the direction of learning into the direction of the difference of Cue A and Cue X. As a result, associative strength of Cue X decreases even though Cue A is presented alone, thereby allowing our model to capture backwards blocking. Gray dashed lines demarcate experimental stages.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.g006" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec017">
<title>Learning with latent-states</title>
<p>To complete our simulation study, we tested if our latent-state model could describe more recent experiments that examine latent-state learning.</p>
<sec id="sec018">
<title>Renewal</title>
<p>Latent-state learning was offered as an explanation of renewal of expectations after extinction and the role that context plays in this renewal [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref016">16</xref>]. We simulated renewal wherein a cue is reinforced, extinguished, and then reinforced (<xref ref-type="fig" rid="pcbi.1007331.g007">Fig 7</xref>). Two experimental conditions are considered: one in which the same visual/spatial context is provided through each phase and another in which a different visual/spatial context is provided during the extinction phase. Following prior models [<xref ref-type="bibr" rid="pcbi.1007331.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>], context was encoded as a separate cue in all models except our latent-state model. Our latent-state model encodes context as a shift in beliefs. Thus, while context may directly modulate both associative strengths and latent state inferences in other models, it only modulates latent state inferences in our model. This latter view is consistent with Bouton [<xref ref-type="bibr" rid="pcbi.1007331.ref012">12</xref>] who says “contexts modulate or ‘set the occasion’ for the current CS–US or CS–no US association. Put another way, they activate or retrieve the current relation of CS with the US”. Note for this example, expected rewards are given on each trial to account for the contribution of both the cue and the context rather than associative strength which only accounts for the contribution of the cue.</p>
<fig id="pcbi.1007331.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007331.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Changes in context influences beliefs.</title>
<p><bold>A)</bold> Experimental results from Ricker and Bouton [<xref ref-type="bibr" rid="pcbi.1007331.ref046">46</xref>] demonstrating a faster response during reacquistion (phase 3) than acquisition (phase 1) after extinction (phase 2). Reprinted by permission from Springer Nature Customer Service Centre GmbH: Springer Nature, Animal learning and behavior, Reacquisition following extinction in appetitive conditioning, Sean T. Ricker &amp; Mark E. Bouton, (1996). <bold>B)</bold> Experimental results from Bouton and King [<xref ref-type="bibr" rid="pcbi.1007331.ref047">47</xref>] demonstrating a more robust return of a fear when extinction occurs in different context (EXT-B) as opposed to the same context (EXT-A) as acquisition. Reprinted from “Contextual Control of the Extinction of Conditioned Fear: Tests for the Associative Value of the Context” by M.E. Bouton and D.A. King, 1983, <italic>Journal of Experimental Psychology: Animal Behavior Processes</italic>, <italic>9</italic>, p. 252. Reprinted with permission from the American Psychological Association. <bold>C)</bold> Simulation results of expected rewards when a response is reinstated after extinction with and without a change in a visual/spatial context. The second experiment examines associative strength of cue when a response is reinstated after extinction with and without a change in a temporal context (i.e. a time delay between trials). Our model shows a rapid reinstatement of expectations as the agent switch their beliefs back to the first latent state. Our model also shows that rapid reinstatement is more robust with changes in context, particular in the first few trials of reinstatement. Expected rewards are depicted rather than associative strengths to account for the influence of context on expectations in addition to the cue, since models other than our model treat context as an additional cue. Beliefs are shown for the first latent state. Gray dashed lines demarcate experimental stages.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.g007" xlink:type="simple"/>
</fig>
<p>In both experimental conditions, our latent-state learner slowly detects the shift when arm-reward contingencies first shift and switches their beliefs to a new latent state, effectively consolidating the memory of the first task phase. By detecting this shift, the agent can both construct new associative strengths for each arm and preserve the old associative strengths that were accurate for the first block of trials. These associative strengths are later <italic>recalled</italic> when arm-reward contingencies revert back and the agent switches their beliefs back to the original latent state. Introducing a different context during extinction allowed the learning agent to better detect the underlying structure of the task, recalling the first latent state earlier during reinstatement, resulting in higher associative strength in the final phase. This improvement in detection predicted by the model, however, was very slight and most noticeable in the first few renewal trials. The infinite-mixture model predicts that the learning agent switches beliefs in a new latent state only when a different context is introduced otherwise using one latent state to predict all phases of the task. For the given parameters, the Gershman (2017) model uses only one latent state for all three phases of the task for both the no renewal and renewal conditions. Consequently, other than our model, only the Redish (2007) model correctly predicts a rapid return of expectations.</p>
</sec>
<sec id="sec019">
<title>Spontaneous recovery</title>
<p>Latent-state learning also provides an explanation for how changes in temporal context influences reinstatement (<xref ref-type="fig" rid="pcbi.1007331.g008">Fig 8</xref>). For example, spontaneous recovery is a learning effect wherein the associative strength of a cue is reinstated more strongly after a time delay between extinction and renewal [<xref ref-type="bibr" rid="pcbi.1007331.ref012">12</xref>]. We simulated spontaneous recovery using the same schedule as renewal, but adding a time delay between extinction and renewal. Our model accounts for time delays by shifting beliefs towards uniform beliefs over latent states. Uniform beliefs reflects that beliefs from a prior trial is less informative on a trial when there is a time delay between trials. As a result, a time delay causes the associative strength of the cue predicted by our model to be about half the associative strength at the end of its initial acquisition (first latent state) and at the end of extinction (second latent state). Without the time delay, the associative strength predicted by our model is simply the associative strength at the end of extinction. Only the Gershman (2017) model adjusts its prediction for context changes due to temporal shifts, but does not capture a more robust return of associative strength due to a temporal shift. The other models also do not capture this more robust return.</p>
<fig id="pcbi.1007331.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007331.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Changes in temporal context also influences beliefs.</title>
<p><bold>A)</bold> Experimental results from Brooks and King [<xref ref-type="bibr" rid="pcbi.1007331.ref048">48</xref>] demonstrating a more robust return of a fear when extinction occurs after a 6 day delay as opposed to immediately after acquisition. Reprinted from “A Retrieval Cue for Extinction Attenuates Spontaneous Recovery” by D.C. Brooks and M.E. Bouton, 1993, <italic>Journal of Experimental Psychology: Animal Behavior Processes</italic>, <italic>19</italic>, p. 80. Reprinted with permission from the American Psychological Association. <bold>B)</bold> Simulation results of associative strength and beliefs when a response is reinstated after extinction with and without a change in a temporal context (i.e. a time delay between trials). Our model shows rapid reinstatement is more robust with changes in temporal context, particular in the first few trials of reinstatement. Beliefs are shown for the first latent state. Gray dashed lines demarcate experimental stages.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.g008" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec020">
<title>Memory modification</title>
<p>Our last simulation experiment explores the potential role of latent-state learning in memory modification. Monfils et al [<xref ref-type="bibr" rid="pcbi.1007331.ref044">44</xref>] in a rodent model and later Schiller et al [<xref ref-type="bibr" rid="pcbi.1007331.ref045">45</xref>] in humans showed that a single <italic>retrieval</italic> trial between acquisition and extinction can reduce the fear response to the cue in latter tests. The time between retrieval and extinction was further shown to modify this effect. Only certain windows of times reduced the fear response. They proposed that the retrieval trial helped to reconsolidate the acquisition memory to be less fearful within this <italic>reconsolidation</italic> window. The Gershman (2017) model [<xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>] includes a computational explanation of this phenomenon. With a similar Bayesian framework to the Gershman (2017) model, our model included similar memory modifications.</p>
<p>We simulated the Monfils-Schiller experiment in [<xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>], varying the time between retrieval and extinction (<xref ref-type="fig" rid="pcbi.1007331.g009">Fig 9</xref>). Our model reproduces a qualitatively similar result as the Gershman (2017) model. There is a reconsolidation window which can help reduce the fear memory. For example in both models, associative strength of the cue during testing is smaller for a time of 5 from retrieval to extinction relative to time of 1 or 100. Both models predict that beliefs in the first latent state (corresponding primarily with acquisition) remains relative high in the reconsolidation window, allowing the agent to update the acquisition memory to be less fearful. Outside the reconsolidation window, the agent updates the second latent state (corresponding primarily with extinction), leaving the acquisition memory relatively intact. While qualitatively similar between models, the effect of retrieval is more pronounced in our model in terms of differences in magnitude of cue associative strength during testing.</p>
<fig id="pcbi.1007331.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007331.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Memory modification.</title>
<p><bold>A)</bold> Experimental results from Schiller et al [<xref ref-type="bibr" rid="pcbi.1007331.ref045">45</xref>] demonstrating that different delays (10 min, 6 hr, no reminder) of a retrieval trial can significantly modify fear response after extinction. Reprinted by permission from Springer Nature Customer Service Centre GmbH: Springer Nature, Nature Preventing the return of fear in humans using reconsolidation update mechanisms, Schiller et al., (2010). <bold>B-C)</bold> Simulation results of associative strength and beliefs for three different time delays (1, 5, and 100) between a single retrieval and extinction. Both models correctly predict that certain time delays can weaken the associative strength upon testing. The bottom row depicts a ‘reconsolidation window’ of time delays after retrieval wherein the associative strength on testing is decreased.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.g009" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec021">
<title>Overview of simulation results</title>
<p>Our model reproduced group-level effects from a series of classical experiments. <xref ref-type="table" rid="pcbi.1007331.t002">Table 2</xref> summarizes which effects were captured by which model. Different aspects of the model are important for reproducing different effects. Updating associative strength similar to the RW model, for example, allows our model to capture effects famously reproduced by the RW model (e.g., blocking). Updating associability based on latent-state beliefs captures sharp increases in associability when a learner shifts their beliefs to a latent state, reproducing experiments from Wilson et al (1992) [<xref ref-type="bibr" rid="pcbi.1007331.ref038">38</xref>] and Rescorla (2000) [<xref ref-type="bibr" rid="pcbi.1007331.ref039">39</xref>], and experiments involving partial reinforcement [<xref ref-type="bibr" rid="pcbi.1007331.ref040">40</xref>–<xref ref-type="bibr" rid="pcbi.1007331.ref042">42</xref>]. Additionally, updating associability based on history of cue presentation rotates updates in different directions, allowing our model to reproduce backwards blocking. Further, using latent-states to index disparate cue-reward associations allows for a rapid return of prior expectations as a learner recalls a prior latent-state. Changing latent-state beliefs to reflect contextual changes allows our model to capture effects modulated by visual or temporal context such as renewal or spontaneous recovery, and together with rumination steps, allows our model to reproduce memory consolidation experiments [<xref ref-type="bibr" rid="pcbi.1007331.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref045">45</xref>]. Finally, our model uses interaction terms and centers rewards, which we found were important for explaining certain group-level effects by determining whether a learning agent would shift to a new latent state.</p>
<table-wrap id="pcbi.1007331.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007331.t002</object-id>
<label>Table 2</label>
<caption>
<title>Models were assessed based on whether or not they could reproduce the observed effect for each simulated experiment (yes = ✔ or no = ✘), with observed effects defined in <xref ref-type="table" rid="pcbi.1007331.t001">Table 1</xref>.</title>
<p>It is important to note, however, that this assessment does not account for the magnitude of the effect or whether a different set of parameters could reproduce the effect.</p>
</caption>
<alternatives>
<graphic id="pcbi.1007331.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Experiment</th>
<th align="center">RW</th>
<th align="center">fRL+decay</th>
<th align="center">Redish</th>
<th align="center">Infinite-mixture</th>
<th align="center">Gershman</th>
<th align="center">Our model</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Blocking</td>
<td align="center">✔</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
<td align="center">✔</td>
</tr>
<tr>
<td align="left">Overexpectation</td>
<td align="center">✔</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
<td align="center">✔</td>
</tr>
<tr>
<td align="left">Conditioned inhibition</td>
<td align="center">✔</td>
<td align="center">✔</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
<td align="center">✔</td>
</tr>
<tr>
<td align="left">Wilson <italic>et al</italic> (1992)</td>
<td align="center">✘</td>
<td align="center">✔</td>
<td align="center">✔</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
</tr>
<tr>
<td align="left">Rescorla (2000) Exp 1A</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
<td align="center">✘</td>
<td align="center">✔</td>
</tr>
<tr>
<td align="left">Rescorla (2000) Exp 1B</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
</tr>
<tr>
<td align="left">Partial reinforcement</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
</tr>
<tr>
<td align="left">Backwards blocking</td>
<td align="center">✘</td>
<td align="center">✔</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
</tr>
<tr>
<td align="left">Rapid return of fear &amp; renewal</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
</tr>
<tr>
<td align="left">Spontaneous recovery</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
</tr>
<tr>
<td align="left">Memory modification</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✘</td>
<td align="center">✔</td>
<td align="center">✔</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
</sec>
<sec id="sec022" sec-type="conclusions">
<title>Discussion</title>
<p>We presented a computational model of latent-state learning, using the Rescorla-Wagner (RW) model as its foundation, and positing that latent states represent disparate associations between cues and rewards. In a series of simulation experiments, we tested the model’s ability to reproduce learning experiments both famously explained and not explained by the RW model and to capture behavior in recent experiments testing latent-state inferences. Lastly, we formally derived our latent-state model under the premise of computational rationality, i.e. a learning agent wants to predict outcomes efficiently.</p>
<p>Our goal was to identify a model of latent-state learning that reproduces group-level effects from classical conditioning experiments. The resulting model makes five critical assumptions/predictions about how an agent learns cue-reward associations: an agent learns online, an agent uses latent states to predict rewards, associability depends on beliefs in latent states and cue novelty, latent states are relatively stable between trials, and beliefs are maintained over multiple latent states. Including these features helps to ensure that the proposed model could examine learning in numerous and diverse experiments, from experiments of classical conditioning to more recent experiments on latent-state inferences. We show that most features fall-out naturally when trying to develop an online approach to reward prediction which uses latent-state inferences. Other models of latent-state learning share some, but not all these features [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>].</p>
<p>For example, our model assumes a learning agent uses latent states to index disparate <italic>associations</italic>, or mappings, from cues to rewards in an effort to predict cues. By contrast, the Redish (2007) and infinite-mixture models use latent states to index disparate <italic>combined observations</italic> of cues and rewards, which can in turn be used to infer rewards from a set of cues. Also, our model assumes latent states evolve according to a Markov chain, whereas other models assume latent states are exchangeable or independent between trials. As a consequence, beliefs in latent states are relatively stable between trials. This stability was observed in certain simulation experiments, in which the predominant latent state would switch only one or two times for our model but over a dozen times for the infinite-mixture model and Redish (2007) model.</p>
<p>Interestingly, our model still updates associative strength in a similar manner to a RW model: change in associative strength is prediction error scaled by associability. In fact, the RW model is a specific case of our latent-state model when there is only one latent state and associability is fixed. For this reason, our latent-state model can reproduce similar learning effects as the RW model such as blocking, overexpectation, and conditioned inhibition. Some latent-state models cannot capture all of these classic learning features [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>]. However, our latent-state model goes beyond the RW model by allowing associability to vary across trials. This feature is thought to be important for describing learning effects not explained by the RW model, such as the Pearce-Hall effect, and is incorporated into newer models, such as the Hybrid RW model [<xref ref-type="bibr" rid="pcbi.1007331.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref049">49</xref>]. Associability in our latent-state model involves two components: effort matrices and beliefs in latent-states. Because associability depends on effort matrices, changes in associative strength are rotated into a direction which has not been previously learned, helping to explain backwards blocking. These matrices approximate Fisher information matrices, which are often incorporated into iterative approaches to maximum likelihood estimation [<xref ref-type="bibr" rid="pcbi.1007331.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref050">50</xref>]. Meanwhile, because associability also depends on beliefs, associative strength can be updated quickly when a learning agent infers a sudden shift in experimental conditions. This feature allowed our latent-state model to capture experimental results from Experiment 1 in Wilson et al (1992) [<xref ref-type="bibr" rid="pcbi.1007331.ref027">27</xref>] and Experiments 1A–B in Rescorla (2000) [<xref ref-type="bibr" rid="pcbi.1007331.ref039">39</xref>].</p>
<p>Similar to other latent-state models, our model also assumes an agent who learns online. Online learning requires only current observations rather than the entire history of observations to update parameters and quantities such as associative strength. Consequently, online approaches have computational and memory requirements that grow in the number of latent states rather than in trials, which could be a more realistic reflection of how humans learn. The infinite-mixture model also uses online updates for all its variables [<xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>], whereas the Gershman (2017) model and Redish model (2007) uses online updates for most variables [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>]. For comparison, an optimal Bayesian approach to latent-state learning would require enumerating over all possible sequences of latent-states, a number that grows exponentially with each trial, imposing a computational and memory burden that is believed to be overly optimistic for human computation [<xref ref-type="bibr" rid="pcbi.1007331.ref026">26</xref>].</p>
<p>Critically, our latent-state model relies on an approximate Bayesian filter to maintain and update beliefs over latent-states online. Other models of latent-state learning use other approaches to reduce computational burden such serial representations of committed beliefs rather than distributing belief across multiple hypotheses [<xref ref-type="bibr" rid="pcbi.1007331.ref011">11</xref>], particle methods [<xref ref-type="bibr" rid="pcbi.1007331.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>], or maximum a posteriori estimates [<xref ref-type="bibr" rid="pcbi.1007331.ref019">19</xref>]. Additional assumptions might also be used such as exchangeable trials and binary outcomes [<xref ref-type="bibr" rid="pcbi.1007331.ref017">17</xref>], one latent state [<xref ref-type="bibr" rid="pcbi.1007331.ref034">34</xref>], one latent-state transition [<xref ref-type="bibr" rid="pcbi.1007331.ref027">27</xref>], or known cue-reward associations [<xref ref-type="bibr" rid="pcbi.1007331.ref032">32</xref>]. Assumptions, however, limit the ability of any model to describe learning in varied experiments. For example, we can capture renewal, spontaneous recovery, and other learning phenomena that require a more general number of latent states and transitions because of our use of an approximate Bayesian filter [<xref ref-type="bibr" rid="pcbi.1007331.ref028">28</xref>]. We can also capture experiments such as [<xref ref-type="bibr" rid="pcbi.1007331.ref032">32</xref>] that require beliefs to be maintained over over multiple latent-states. Notably, the experiment [<xref ref-type="bibr" rid="pcbi.1007331.ref032">32</xref>] suggested a link between log-beliefs and activity in the orbitofrontal cortex.</p>
<p>Many of the assumptions/predictions of the presented model are testable. For example, associability for a latent-state is predicted to be directly dependent on the beliefs in that latent-state. Stronger beliefs lead to faster learning, whereas weaker beliefs lead to slower learning. An experiment could be designed to test, for example, if associability decreases by 50% when beliefs are split between two latent-states relative to when beliefs were concentrated on one latent-state. Further, associability is predicted to depend on whether cues had previously been presented together. New combination of cues can lead to quick learning even when individual cues making up the combination have already been presented. Last, latent-state beliefs are predicted to be updated in a non-optimal manner, suggesting one try to induce sub-optimal behavior in an experiment.</p>
<p>Several limitations of the proposed model should be considered. First, we anticipate there are group-level learning effects that our model fails to describe or there are other models that could explain participant behavior. Second, we did not examine to what extent beliefs in latent-states and other variables from our model are encoded in various brain regions. Third, we assume beliefs are maintained over multiple latent states, but some experiments suggest that an agent is committed to a state and alternates between committed states [<xref ref-type="bibr" rid="pcbi.1007331.ref033">33</xref>]. One possibility is that beliefs in latent states influence which state is committed and over time, the frequency of committed states mirrors these beliefs. Modalities such as fMRI might then suggest beliefs are maintained over multiple states, as in [<xref ref-type="bibr" rid="pcbi.1007331.ref032">32</xref>], because they lack the temporal resolution needed to detect the alternating committed states. Allowing beliefs to determine committed states might even lead to better explanations of participant data, as it allows for more flexibility in how states are updated for a given participant. Fourth, there may be better ways to integrate computational mechanisms of how memories are formed into our model [<xref ref-type="bibr" rid="pcbi.1007331.ref051">51</xref>]. For example, we use online updates in order to avoid memory requirements that grow in the number of trials, but it is possible that humans are either capable of such requirements or use other ways than online updates to integrate past observations. Fifth, some effects (e.g., memory modification effect [<xref ref-type="bibr" rid="pcbi.1007331.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1007331.ref045">45</xref>]) reproduced by our model might be too subtle to ever be detected experimentally. Sixth, we assume latent states are relatively stable between trials. This assumption may be accurate for describing learning in experiments, since experiments often involve blocks of trials in which rewards/cues are generated by the same rules, but may be less accurate for real-world learning environments. Seventh, we included interactions between cues to influence when a learning agent shifts to a new latent state, which we found was necessary to recover one of the Rescorla (2000) experiments [<xref ref-type="bibr" rid="pcbi.1007331.ref039">39</xref>]. This inclusion, however, should be investigated further to determine if it reflects how agents actually learn. Finally, we did not study model performance in terms of predicting rewards as a consequence of using an approximate Bayesian filter. It may useful to know how far from optimal our approximate method is for maximum likelihood estimation in various settings.</p>
<p>In sum, this work establishes the validity of an online model for latent-state inferences that generalizes across experiments. Establishing such a framework is a necessary step towards quantifying normative and pathological ranges of latent-state inferences (and mediating neurocircuitry) across varied contexts. Overall, this work moves us closer to a precise and mechanistic understanding of how humans infer latent-states.</p>
</sec>
<sec id="sec023" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec024">
<title>Simulation details</title>
<p>Parameters were fixed throughout the simulation (<xref ref-type="table" rid="pcbi.1007331.t003">Table 3</xref>). Parameters for alternative models were chosen from their respective papers, except for the maximum number of latent states which we fixed at 15 to reduce memory requirements. Our model includes cues throughout simulation to represent pairwise interaction terms (i.e. a binary indicator if a pair of cues are present or absent). Our model also centers rewards with <italic>R</italic>(<italic>t</italic>) = 1/2 when a reward is presented and <italic>R</italic>(<italic>t</italic>) = −1/2 when a reward is not presented. That way, initial associative strengths give rise to neutral (i.e. 50-50) expectations for reward. Thus, associative strengths are shifted by 1/2 for our model when compared to other models. Accompanying code is publicly-available at <ext-link ext-link-type="uri" xlink:href="https://github.com/cochran4/OnlineLatentStateLearning" xlink:type="simple">https://github.com/cochran4/OnlineLatentStateLearning</ext-link>. <xref ref-type="supplementary-material" rid="pcbi.1007331.s001">S1 Text</xref> provides further details on the schedules for how cues and rewards were delivered during each simulation task and examines sensitivity of our model’s predictions to changes in parameters.</p>
<table-wrap id="pcbi.1007331.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007331.t003</object-id>
<label>Table 3</label>
<caption>
<title>Fixed set of parameters used for all simulated experiments.</title>
</caption>
<alternatives>
<graphic id="pcbi.1007331.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Parameter</th>
<th align="left">Description</th>
<th align="center">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Our model</td>
<td align="center"><italic>α</italic><sub>0</sub></td>
<td align="left">Associative strength learning rate</td>
<td align="center">0.05</td>
</tr>
<tr>
<td align="center">Our model</td>
<td align="center"><italic>β</italic><sub>0</sub></td>
<td align="left">Variance learning rate</td>
<td align="center">0.05</td>
</tr>
<tr>
<td align="center">Our model</td>
<td align="center"><italic>γ</italic></td>
<td align="left">Latent-state transitions</td>
<td align="center">0.05</td>
</tr>
<tr>
<td align="center">Our model</td>
<td align="center"><italic>σ</italic><sub>0</sub></td>
<td align="left">Initial expected uncertainty</td>
<td align="center">0.5</td>
</tr>
<tr>
<td align="center">Our model</td>
<td align="center"><italic>ν</italic></td>
<td align="left">Threshold for new state</td>
<td align="center">0.2</td>
</tr>
<tr>
<td align="center">Our model</td>
<td align="center"><italic>δ</italic></td>
<td align="left">Unexpected uncertainty update</td>
<td align="center">0.6</td>
</tr>
<tr>
<td align="center">Our model</td>
<td align="center"><italic>χ</italic></td>
<td align="left">Rumination steps</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">Rescorla-Wagner</td>
<td align="center"><italic>α</italic></td>
<td align="left">Learning rate</td>
<td align="center">0.15</td>
</tr>
<tr>
<td align="center">Infinite-mixture</td>
<td align="center"><italic>α</italic></td>
<td align="left">Concentration parameter</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">Infinite-mixture</td>
<td align="center"><italic>M</italic></td>
<td align="left">Number of particles</td>
<td align="center">100</td>
</tr>
<tr>
<td align="center">Gershman (2017)</td>
<td align="center"><italic>α</italic></td>
<td align="left">Concentration parameter</td>
<td align="center">0.1</td>
</tr>
<tr>
<td align="center">Gershman (2017)</td>
<td align="center"><italic>η</italic></td>
<td align="left">Learning rate</td>
<td align="center">0.3</td>
</tr>
<tr>
<td align="center">Gershman (2017)</td>
<td align="center">
<inline-formula id="pcbi.1007331.e032">
<alternatives>
<graphic id="pcbi.1007331.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e032" xlink:type="simple"/>
<mml:math display="inline" id="M32">
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mi>r</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="left">Reward variance</td>
<td align="center">0.4</td>
</tr>
<tr>
<td align="center">Gershman (2017)</td>
<td align="center">
<inline-formula id="pcbi.1007331.e033">
<alternatives>
<graphic id="pcbi.1007331.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e033" xlink:type="simple"/>
<mml:math display="inline" id="M33">
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mi>x</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="left">Cue variance</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">Gershman (2017)</td>
<td align="center">n/a</td>
<td align="left">Rumination steps</td>
<td align="center">3</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec025">
<title>Mathematical justification</title>
<p>We justify our choice of model under the premise of <italic>computational rationality</italic> [<xref ref-type="bibr" rid="pcbi.1007331.ref052">52</xref>], i.e. a learning agent wants to predict rewards efficiently. An optimistic strategy for the learning agent would be to use maximize likelihood estimation (MLE) to aid prediction. This would entail starting with a probabilistic model of rewards defined up to an unknown parameter <italic>θ</italic> and finding <italic>θ</italic> to maximize the log-likelihood of rewards (scaled by 1/<italic>t</italic>):
<disp-formula id="pcbi.1007331.e034"><alternatives><graphic id="pcbi.1007331.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ℓ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:mi>f</mml:mi> <mml:mo>[</mml:mo> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>θ</mml:mi> <mml:mo>]</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula></p>
<p>Treating rewards as a continuous variable, we use <italic>f</italic> to denote a probability density function. The estimate of <italic>θ</italic> together with the reward model predict future rewards. With an eye towards efficiency, we focus on <italic>online</italic> approaches, such as the Rescorla-Wagner model, and refer the reader to the work in [<xref ref-type="bibr" rid="pcbi.1007331.ref050">50</xref>] and [<xref ref-type="bibr" rid="pcbi.1007331.ref028">28</xref>] on updating latent-variable models online.</p>
<sec id="sec026">
<title>Rescorla-Wagner model as an online approach to maximum likelihood estimation</title>
<p>Before we motivate our latent-state model, we explore how one could perform MLE online to predict rewards without latent-states. In the process, we show that the RW model is an online approach to MLE and illuminate a pathway for performing MLE online with latent-states. The most natural modeling choice for rewards is arguably a standard Gaussian regression model, ubiquitous in statistical inference:
<disp-formula id="pcbi.1007331.e035"><alternatives><graphic id="pcbi.1007331.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <inline-formula id="pcbi.1007331.e036"><alternatives><graphic id="pcbi.1007331.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the cue vector on trial <italic>t</italic> (independent variables); <inline-formula id="pcbi.1007331.e037"><alternatives><graphic id="pcbi.1007331.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula> are unknown parameters (unknown regression coefficients); and <italic>ϵ</italic>(<italic>t</italic>) is an independent Gaussian random variable (error) with mean zero and unknown variance <italic>ν</italic>. Unknown parameters are collected in <inline-formula id="pcbi.1007331.e038"><alternatives><graphic id="pcbi.1007331.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup></mml:mtd> <mml:mtd><mml:mi>ν</mml:mi></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>Although we could maximize <italic>ℓ</italic> directly, we want an online approach to maximization and instead consider applying Newton’s method for optimization (i.e., gradient descent), which involves repeatedly updating an estimate <inline-formula id="pcbi.1007331.e039"><alternatives><graphic id="pcbi.1007331.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> of <italic>θ</italic> with a new estimate of the form:
<disp-formula id="pcbi.1007331.e040"><alternatives><graphic id="pcbi.1007331.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e040" xlink:type="simple"/><mml:math display="block" id="M40"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>-</mml:mo> <mml:mo>λ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>H</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>∇</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>ℓ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo>|</mml:mo> <mml:mrow><mml:mi>θ</mml:mi> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
for some learning rate λ(<italic>t</italic>) and an appropriate matrix <italic>H</italic>(<italic>t</italic>). Matrix <italic>H</italic>(<italic>t</italic>) can take many forms and is important for determining how quickly estimates converge to a maximum/minimum. An ideal choice is the Hessian resulting in the Newton-Raphson method, but the Hessian is often noninvertible in MLE. A common alternative is the negative of the Fisher information matrix <inline-formula id="pcbi.1007331.e041"><alternatives><graphic id="pcbi.1007331.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mrow><mml:mi mathvariant="script">I</mml:mi> <mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> scaled by 1/<italic>t</italic> and evaluated at the current estimate [<xref ref-type="bibr" rid="pcbi.1007331.ref050">50</xref>]. Even with Newton’s method, rewards still need to be stored up to trial <italic>t</italic>, but Newton’s method can be turned into an online algorithm using the well-known Robbins-Monro method [<xref ref-type="bibr" rid="pcbi.1007331.ref053">53</xref>]. We simply replace the gradient with an (stochastic) approximation that depends only on the current trial:
<disp-formula id="pcbi.1007331.e042"><alternatives><graphic id="pcbi.1007331.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>∇</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>ℓ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:msub><mml:mo>∇</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mi>f</mml:mi> <mml:mo>[</mml:mo> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>θ</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>Note that each side of the equation is equal in expectation when rewards are independent and identically-distributed.</p>
<p>Upon replacing the gradient with a stochastic approximation, we arrive at the RW model if we use a constant learning rate λ(<italic>t</italic>) = λ<sub>0</sub> and the matrix
<disp-formula id="pcbi.1007331.e043"><alternatives><graphic id="pcbi.1007331.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e043" xlink:type="simple"/><mml:math display="block" id="M43"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>H</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mover accent="true"><mml:mi>ν</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mfrac> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mi>I</mml:mi></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mover accent="true"><mml:mi>ν</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>For the Gaussian regression model, matrix <italic>H</italic>(<italic>t</italic>) resembles
<disp-formula id="pcbi.1007331.e044"><alternatives><graphic id="pcbi.1007331.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e044" xlink:type="simple"/><mml:math display="block" id="M44"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:mfrac> <mml:mi mathvariant="script">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:mfrac> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:msubsup><mml:mo>∇</mml:mo> <mml:mi>θ</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo form="prefix">log</mml:mo> <mml:mi>f</mml:mi> <mml:mo>[</mml:mo> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>θ</mml:mi> <mml:mo>]</mml:mo> <mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>θ</mml:mi> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>ν</mml:mi></mml:mfrac> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:munderover> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>ν</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
evaluated at <inline-formula id="pcbi.1007331.e045"><alternatives><graphic id="pcbi.1007331.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mover accent="true"><mml:msup><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>^</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo>→</mml:mo></mml:mover></mml:mtd> <mml:mtd><mml:mover accent="true"><mml:mi>ν</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> We simply replaced <inline-formula id="pcbi.1007331.e046"><alternatives><graphic id="pcbi.1007331.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:mfrac> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>t</mml:mi></mml:msub> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> with the identity matrix <italic>I</italic>, which ensures <italic>H</italic>(<italic>t</italic>) is invertible. Putting the online approach together with the fact that
<disp-formula id="pcbi.1007331.e047"><alternatives><graphic id="pcbi.1007331.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e047" xlink:type="simple"/><mml:math display="block" id="M47"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>∇</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mi>f</mml:mi> <mml:mo>[</mml:mo> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>θ</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>ν</mml:mi></mml:mfrac> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>ν</mml:mi></mml:mrow></mml:mfrac> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
for the Gaussian regression model, we arrive at the following online update for the estimate <inline-formula id="pcbi.1007331.e048"><alternatives><graphic id="pcbi.1007331.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>:
<disp-formula id="pcbi.1007331.e049"><alternatives><graphic id="pcbi.1007331.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e049" xlink:type="simple"/><mml:math display="block" id="M49"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>+</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mn>0</mml:mn></mml:msub> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mover accent="true"><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mover accent="true"><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>ν</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>Critically, estimates <inline-formula id="pcbi.1007331.e050"><alternatives><graphic id="pcbi.1007331.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:mover accent="true"><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula> are updated exactly as in an RW model:
<disp-formula id="pcbi.1007331.e051"><alternatives><graphic id="pcbi.1007331.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e051" xlink:type="simple"/><mml:math display="block" id="M51"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>→</mml:mo></mml:mover> <mml:mo>+</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mn>0</mml:mn></mml:msub> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>In other words, the RW model is an online approach to maximum likelihood estimation.</p>
</sec>
<sec id="sec027">
<title>Latent-state learning as an online approach to expectation-maximization</title>
<p>Motivated by the previous argument, we now explore how one could perform MLE online to predict rewards with latent-states. A natural choice for modeling rewards is to use a Markov chain of Gaussian regression models. Latent random variable <italic>X</italic>(<italic>t</italic>)∈{1, …, <italic>L</italic>} represents the active hypothesis on trial <italic>t</italic>. Given the active hypothesis <italic>X</italic>(<italic>t</italic>) = <italic>l</italic>, rewards are described by a Gaussian linear regression model:
<disp-formula id="pcbi.1007331.e052"><alternatives><graphic id="pcbi.1007331.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e052" xlink:type="simple"/><mml:math display="block" id="M52"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <inline-formula id="pcbi.1007331.e053"><alternatives><graphic id="pcbi.1007331.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> are unknown parameters and <italic>ϵ</italic>(<italic>t</italic>) is an independent Gaussian random variable with mean zero and unknown variance <italic>ν</italic>. The extension to latent-state learning thus leads to <italic>L</italic> different Gaussian regression models for rewards rather than one. As discussed in the Model subsection, we assume <italic>X</italic>(<italic>t</italic>) is a Markov chain that transitions to itself from one trial to the next with probability 1 − <italic>γ</italic>(<italic>L</italic> − 1)/<italic>L</italic> and transitions to a new state (all new states being equally-likely) with probability <italic>γ</italic>(<italic>L</italic> − 1)/<italic>L</italic>. Parameter <italic>γ</italic> and initial probabilities for <italic>X</italic>(0) are assumed to be known/given, leaving unknown parameters collected in <inline-formula id="pcbi.1007331.e054"><alternatives><graphic id="pcbi.1007331.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mn>1</mml:mn> <mml:mo>′</mml:mo></mml:msubsup></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd> <mml:mtd><mml:msubsup><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>L</mml:mi> <mml:mo>′</mml:mo></mml:msubsup></mml:mtd> <mml:mtd><mml:mi>ν</mml:mi></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>In a latent-variable setting, a popular algorithm to perform MLE is expectation-maximization (EM) [<xref ref-type="bibr" rid="pcbi.1007331.ref054">54</xref>]. An iterative algorithm, EM improves upon a current estimate <inline-formula id="pcbi.1007331.e055"><alternatives><graphic id="pcbi.1007331.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> at each iteration in two steps. Fixing the current trial number <italic>t</italic>, an <italic>expectation</italic> step uses a current estimate <inline-formula id="pcbi.1007331.e056"><alternatives><graphic id="pcbi.1007331.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> to calculate the expectation of the complete log-likelihood of rewards and latent states given rewards and scaled by 1/<italic>t</italic>:
<disp-formula id="pcbi.1007331.e057"><alternatives><graphic id="pcbi.1007331.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e057" xlink:type="simple"/><mml:math display="block" id="M57"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>Q</mml:mi> <mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mrow><mml:mo>|</mml:mo> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:mfrac> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mi>f</mml:mi> <mml:mo>[</mml:mo> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>θ</mml:mi> <mml:mo>]</mml:mo> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:munderover> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mi>f</mml:mi> <mml:mo>[</mml:mo> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>]</mml:mo> <mml:mo>+</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mi mathvariant="double-struck">P</mml:mi> <mml:mo>[</mml:mo> <mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>A <italic>maximization</italic> step searches for a maximum of <inline-formula id="pcbi.1007331.e058"><alternatives><graphic id="pcbi.1007331.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:mrow><mml:mi>Q</mml:mi> <mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mrow><mml:mo>|</mml:mo> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> over <italic>θ</italic>. The maximum is then used as the estimate in the next iteration. The crux of the EM method is that <inline-formula id="pcbi.1007331.e059"><alternatives><graphic id="pcbi.1007331.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:mrow><mml:mi>Q</mml:mi> <mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mrow><mml:mo>|</mml:mo> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is easier to maximize than the log-likelihood <italic>ℓ</italic>(<italic>θ</italic>) of the observed data and that improvement in <inline-formula id="pcbi.1007331.e060"><alternatives><graphic id="pcbi.1007331.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:mrow><mml:mi>Q</mml:mi> <mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mrow><mml:mo>|</mml:mo> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> guarantees improvement in <italic>ℓ</italic>(<italic>θ</italic>).</p>
<p>Mirroring our argument for the Rescorla-Wagner model, we can replace the maximization step with a Newton update:
<disp-formula id="pcbi.1007331.e061"><alternatives><graphic id="pcbi.1007331.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e061" xlink:type="simple"/><mml:math display="block" id="M61"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>-</mml:mo> <mml:mo>λ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>H</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>∇</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo>|</mml:mo> <mml:mrow><mml:mi>θ</mml:mi> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
for appropriate learning rate λ(<italic>t</italic>) and matrix <italic>H</italic>(<italic>t</italic>). Even in a latent-variable setting, one choice for <italic>H</italic>(<italic>t</italic>) is still the negative of the Fisher’s information matrix scaled be 1/<italic>t</italic> and evaluated at <inline-formula id="pcbi.1007331.e062"><alternatives><graphic id="pcbi.1007331.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, but Newton’s method still requires all rewards up to trial <italic>t</italic>. To recover an online approach, we use a stochastic approximation to <inline-formula id="pcbi.1007331.e063"><alternatives><graphic id="pcbi.1007331.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e063" xlink:type="simple"/><mml:math display="inline" id="M63"><mml:mrow><mml:msub><mml:mo>∇</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> in the maximization step:
<disp-formula id="pcbi.1007331.e064"><alternatives><graphic id="pcbi.1007331.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e064" xlink:type="simple"/><mml:math display="block" id="M64"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>∇</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>≈</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:msub><mml:mo>∇</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mi>f</mml:mi> <mml:mo>[</mml:mo> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>]</mml:mo> <mml:mo>+</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mi mathvariant="double-struck">P</mml:mi> <mml:mo>[</mml:mo> <mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>L</mml:mi></mml:munderover> <mml:mi mathvariant="double-struck">P</mml:mi> <mml:mo>[</mml:mo> <mml:mrow><mml:mi>X</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>l</mml:mi> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>]</mml:mo> <mml:msub><mml:mo>∇</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mi>f</mml:mi> <mml:mo>[</mml:mo> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>l</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>We then replace <inline-formula id="pcbi.1007331.e065"><alternatives><graphic id="pcbi.1007331.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e065" xlink:type="simple"/><mml:math display="inline" id="M65"><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mo>[</mml:mo> <mml:mrow><mml:mi>X</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>l</mml:mi> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, which is computed using all the rewards, with latent-state beliefs <italic>p</italic><sub><italic>l</italic></sub>(<italic>t</italic>) defined at <xref ref-type="disp-formula" rid="pcbi.1007331.e015">Eq 5</xref> which is computed online. This replacement yields the stochastic approximation
<disp-formula id="pcbi.1007331.e066"><alternatives><graphic id="pcbi.1007331.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e066" xlink:type="simple"/><mml:math display="block" id="M66"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>∇</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>≈</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>L</mml:mi></mml:munderover> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo>∇</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mi>f</mml:mi> <mml:mo>[</mml:mo> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>l</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>ν</mml:mi></mml:mfrac> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>L</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>L</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>L</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>ν</mml:mi></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>L</mml:mi></mml:munderover> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>The last equality follows from our assumption that rewards are a Markov chain of Gaussian regression models.</p>
<p>Upon replacing the gradient in Newton’s method with this stochastic approximation, we can arrive at our latent-state learning model if we use λ(<italic>t</italic>) = <italic>α</italic><sub>0</sub> and matrix
<disp-formula id="pcbi.1007331.e067"><alternatives><graphic id="pcbi.1007331.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e067" xlink:type="simple"/><mml:math display="block" id="M67"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>H</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mover accent="true"><mml:mi>ν</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mfrac> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>B</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:msub><mml:mi>B</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd/><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd/><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd> <mml:mtd/><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd/><mml:mtd><mml:mrow><mml:msub><mml:mi>B</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mover accent="true"><mml:mi>ν</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where we used the definition of <italic>B</italic><sub><italic>l</italic></sub>(<italic>t</italic>) at <xref ref-type="disp-formula" rid="pcbi.1007331.e010">Eq 4</xref>. For a Markov chain of Gaussian regression models, matrix <italic>H</italic>(<italic>t</italic>) resembles the negative of the Fisher information matrix scaled by 1/<italic>t</italic>:
<disp-formula id="pcbi.1007331.e068"><alternatives><graphic id="pcbi.1007331.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e068" xlink:type="simple"/><mml:math display="block" id="M68"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:mfrac> <mml:mi mathvariant="script">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:mfrac> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:msubsup><mml:mo>∇</mml:mo> <mml:mrow><mml:mi>θ</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo form="prefix">log</mml:mo> <mml:mi>f</mml:mi> <mml:mo>[</mml:mo> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext>:</mml:mtext> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>θ</mml:mi> <mml:mo>]</mml:mo> <mml:mo>|</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>ν</mml:mi></mml:mfrac> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:munderover> <mml:msub><mml:mi>ρ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mtd> <mml:mtd/><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd> <mml:mtd/><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd/><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:munderover> <mml:mspace width="4pt"/><mml:msub><mml:mi>ρ</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>L</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>L</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>ν</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <inline-formula id="pcbi.1007331.e069"><alternatives><graphic id="pcbi.1007331.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="double-struck">P</mml:mi> <mml:mo>[</mml:mo> <mml:mi>X</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>l</mml:mi> <mml:mo>|</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. We simply replaced diagonal blocks with <italic>B</italic><sub><italic>l</italic></sub>(<italic>t</italic>) which ensures <italic>H</italic>(<italic>t</italic>) is invertible and replaced <italic>ρ</italic><sub><italic>l</italic></sub>(<italic>s</italic>, <italic>θ</italic>) with latent state beliefs <italic>p</italic><sub><italic>l</italic></sub>(<italic>s</italic>). With these choices for λ(<italic>t</italic>) and <italic>H</italic>(<italic>t</italic>), we update <inline-formula id="pcbi.1007331.e070"><alternatives><graphic id="pcbi.1007331.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e070" xlink:type="simple"/><mml:math display="inline" id="M70"><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> online according to:
<disp-formula id="pcbi.1007331.e071"><alternatives><graphic id="pcbi.1007331.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007331.e071" xlink:type="simple"/><mml:math display="block" id="M71"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>B</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msub><mml:mover accent="true"><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>→</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mspace width="6em"/><mml:mrow><mml:mo>⋮</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>L</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msub><mml:mi>p</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>B</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>L</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>L</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msub><mml:mover accent="true"><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>→</mml:mo></mml:mover> <mml:mi>L</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:mi>ν</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>L</mml:mi></mml:munderover> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msub><mml:mover accent="true"><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>→</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>ν</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>We thus arrive at our latent-state model for updating associative strengths at <xref ref-type="disp-formula" rid="pcbi.1007331.e007">Eq (1)</xref>. We can also recover our latent-state model for updating variance at <xref ref-type="disp-formula" rid="pcbi.1007331.e018">Eq (7)</xref>, if we replace the learning rate <italic>α</italic><sub>0</sub> with a different learning rate <italic>β</italic><sub>0</sub>. This completes our motivation for our latent-state model.</p>
</sec>
</sec>
</sec>
<sec id="sec028">
<title>Supporting information</title>
<supplementary-material id="pcbi.1007331.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Additional support and details.</title>
<p>This supplement is divided into four sections: Extensions, Comparison to other learning models, Simulation details, and Sensitivity of predictions to parameters. The first section discusses how to extend the latent-state model to allow for multidimensional rewards and decision-making. The second section examines simulated behavior from other learning models. The third section provides additional details on simulated experiments. The last section examines whether the ability of our model to reproduce target effects in simulation is sensitive to changes in parameters.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007331.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.s002" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Hybrid model.</title>
<p>Simulated behavior of the Hybrid model for the same learning experiments in the main text. Gray dashes demarcate experimental stages.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007331.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.s003" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Gershman (2017) model with higher concentration parameter.</title>
<p>Simulated behavior of Gershman (2017) model with concentration parameter <italic>α</italic> = 1 for the same learning experiments in the main text. Gray dashes demarcate experimental stages.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007331.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.s004" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Parameter sensitivity tests.</title>
<p>Quantities measured in each simulation experiment in order to test sensitivity of model predictions to changes in parameters. The value of each test quantity determines whether or not a target learning effect is reproduced by the model.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007331.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.s005" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Parameter sensitivity results.</title>
<p>Target quantity as a function of target effect (#1–13) and which parameter was varied, keeping all other parameters fixed. By checking whether each target quantity was positive, we could determine whether the model reproduced one of the target effects listed in the main text. In this table, all parameter sets reproduced target effects.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007331.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007331.s006" xlink:type="simple">
<label>S3 Table</label>
<caption>
<title>Parameter sensitivity results <italic>continued</italic>.</title>
<p>Target quantity as a function of target effect (#1–13) and which parameter was varied, keeping all other parameters fixed. By checking whether each target quantity was positive, we could determine whether the model reproduced one of the target effects listed in the main text. In this table, all parameter sets reproduced target effects, except in one case: decreasing parameter <italic>δ</italic> (which controlled unexpected uncertainty) did not reproduce memory modification effect (#13).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1007331.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huys</surname> <given-names>QJ</given-names></name>, <name name-style="western"><surname>Guitart-Masip</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Decision-theoretic psychiatry</article-title>. <source>Clinical Psychological Science</source>. <year>2015</year>;<volume>3</volume>(<issue>3</issue>):<fpage>400</fpage>–<lpage>421</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/2167702614562040" xlink:type="simple">10.1177/2167702614562040</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref002">
<label>2</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>, <etal>et al</etal>. <source>Reinforcement learning: An introduction</source>. <publisher-name>MIT press</publisher-name>; <year>1998</year>.</mixed-citation>
</ref>
<ref id="pcbi.1007331.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>O’doherty</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Cortical substrates for exploratory decisions in humans</article-title>. <source>Nature</source>. <year>2006</year>;<volume>441</volume>(<issue>7095</issue>):<fpage>876</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature04766" xlink:type="simple">10.1038/nature04766</ext-link></comment> <object-id pub-id-type="pmid">16778890</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Doll</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Simon</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>The ubiquity of model-based reinforcement learning</article-title>. <source>Current opinion in neurobiology</source>. <year>2012</year>;<volume>22</volume>(<issue>6</issue>):<fpage>1075</fpage>–<lpage>1081</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2012.08.003" xlink:type="simple">10.1016/j.conb.2012.08.003</ext-link></comment> <object-id pub-id-type="pmid">22959354</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref005">
<label>5</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Wilson</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Collins</surname> <given-names>A</given-names></name>. <source>Ten simple rules for the computational modeling of behavioral data</source>. <year>2019</year>;.</mixed-citation>
</ref>
<ref id="pcbi.1007331.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Berridge</surname> <given-names>KC</given-names></name>. <article-title>Model-based and model-free Pavlovian reward learning: revaluation, revision, and revelation</article-title>. <source>Cognitive, Affective, &amp; Behavioral Neuroscience</source>. <year>2014</year>;<volume>14</volume>(<issue>2</issue>):<fpage>473</fpage>–<lpage>492</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13415-014-0277-8" xlink:type="simple">10.3758/s13415-014-0277-8</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gläscher</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>. <article-title>States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning</article-title>. <source>Neuron</source>. <year>2010</year>;<volume>66</volume>(<issue>4</issue>):<fpage>585</fpage>–<lpage>595</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2010.04.016" xlink:type="simple">10.1016/j.neuron.2010.04.016</ext-link></comment> <object-id pub-id-type="pmid">20510862</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Wagner</surname> <given-names>AR</given-names></name>, <etal>et al</etal>. <article-title>A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</article-title>. <source>Classical conditioning II: Current research and theory</source>. <year>1972</year>;<volume>2</volume>:<fpage>64</fpage>–<lpage>99</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007331.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pearce</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Hall</surname> <given-names>G</given-names></name>. <article-title>A model for Pavlovian learning: variations in the effectiveness of conditioned but not of unconditioned stimuli</article-title>. <source>Psychological review</source>. <year>1980</year>;<volume>87</volume>(<issue>6</issue>):<fpage>532</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-295X.87.6.532" xlink:type="simple">10.1037/0033-295X.87.6.532</ext-link></comment> <object-id pub-id-type="pmid">7443916</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Le Pelley</surname> <given-names>ME</given-names></name>. <article-title>The role of associative history in models of associative learning: A selective review and a hybrid model</article-title>. <source>The Quarterly Journal of Experimental Psychology Section B</source>. <year>2004</year>;<volume>57</volume>(<issue>3b</issue>):<fpage>193</fpage>–<lpage>243</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02724990344000141" xlink:type="simple">10.1080/02724990344000141</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Jensen</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kurth-Nelson</surname> <given-names>Z</given-names></name>. <article-title>Reconciling reinforcement learning models with behavioral extinction and renewal: implications for addiction, relapse, and problem gambling</article-title>. <source>Psychological review</source>. <year>2007</year>;<volume>114</volume>(<issue>3</issue>):<fpage>784</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-295X.114.3.784" xlink:type="simple">10.1037/0033-295X.114.3.784</ext-link></comment> <object-id pub-id-type="pmid">17638506</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bouton</surname> <given-names>ME</given-names></name>. <article-title>Context and behavioral processes in extinction</article-title>. <source>Learning &amp; memory</source>. <year>2004</year>;<volume>11</volume>(<issue>5</issue>):<fpage>485</fpage>–<lpage>494</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/lm.78804" xlink:type="simple">10.1101/lm.78804</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vervliet</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Craske</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Hermans</surname> <given-names>D</given-names></name>. <article-title>Fear extinction and relapse: state of the art</article-title>. <source>Annual review of clinical psychology</source>. <year>2013</year>;<volume>9</volume>:<fpage>215</fpage>–<lpage>248</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-clinpsy-050212-185542" xlink:type="simple">10.1146/annurev-clinpsy-050212-185542</ext-link></comment> <object-id pub-id-type="pmid">23537484</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>Addiction as a computational process gone awry</article-title>. <source>Science</source>. <year>2004</year>;<volume>306</volume>(<issue>5703</issue>):<fpage>1944</fpage>–<lpage>1947</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1102384" xlink:type="simple">10.1126/science.1102384</ext-link></comment> <object-id pub-id-type="pmid">15591205</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Blei</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Context, learning, and extinction</article-title>. <source>Psychological review</source>. <year>2010</year>;<volume>117</volume>(<issue>1</issue>):<fpage>197</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0017808" xlink:type="simple">10.1037/a0017808</ext-link></comment> <object-id pub-id-type="pmid">20063968</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Hartley</surname> <given-names>CA</given-names></name>. <article-title>Individual differences in learning predict the return of fear</article-title>. <source>Learning &amp; behavior</source>. <year>2015</year>;<volume>43</volume>(<issue>3</issue>):<fpage>243</fpage>–<lpage>250</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13420-015-0176-z" xlink:type="simple">10.3758/s13420-015-0176-z</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Exploring a latent cause theory of classical conditioning</article-title>. <source>Learning &amp; behavior</source>. <year>2012</year>;<volume>40</volume>(<issue>3</issue>):<fpage>255</fpage>–<lpage>268</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13420-012-0080-8" xlink:type="simple">10.3758/s13420-012-0080-8</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Norman</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Discovering latent causes in reinforcement learning</article-title>. <source>Current Opinion in Behavioral Sciences</source>. <year>2015</year>;<volume>5</volume>:<fpage>43</fpage>–<lpage>50</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cobeha.2015.07.007" xlink:type="simple">10.1016/j.cobeha.2015.07.007</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Monfils</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Norman</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>The computational nature of memory modification</article-title>. <source>Elife</source>. <year>2017</year>;<volume>6</volume>:<fpage>e23763</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.23763" xlink:type="simple">10.7554/eLife.23763</ext-link></comment> <object-id pub-id-type="pmid">28294944</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Norman</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Monfils</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Gradual extinction prevents the return of fear: implications for the discovery of state</article-title>. <source>Frontiers in behavioral neuroscience</source>. <year>2013</year>;<volume>7</volume>:<fpage>164</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnbeh.2013.00164" xlink:type="simple">10.3389/fnbeh.2013.00164</ext-link></comment> <object-id pub-id-type="pmid">24302899</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tolman</surname> <given-names>EC</given-names></name>. <article-title>Cognitive maps in rats and men</article-title>. <source>Psychological review</source>. <year>1948</year>;<volume>55</volume>(<issue>4</issue>):<fpage>189</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0061626" xlink:type="simple">10.1037/h0061626</ext-link></comment> <object-id pub-id-type="pmid">18870876</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stalnaker</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Cooch</surname> <given-names>NK</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>. <article-title>What the orbitofrontal cortex does not do</article-title>. <source>Nature neuroscience</source>. <year>2015</year>;<volume>18</volume>(<issue>5</issue>):<fpage>620</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3982" xlink:type="simple">10.1038/nn.3982</ext-link></comment> <object-id pub-id-type="pmid">25919962</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wikenheiser</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>. <article-title>Over the river, through the woods: cognitive maps in the hippocampus and orbitofrontal cortex</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2016</year>;<volume>17</volume>(<issue>8</issue>):<fpage>513</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn.2016.56" xlink:type="simple">10.1038/nrn.2016.56</ext-link></comment> <object-id pub-id-type="pmid">27256552</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hall</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Pearce</surname> <given-names>JM</given-names></name>. <article-title>Latent inhibition of a CS during CS–US pairings</article-title>. <source>Journal of Experimental Psychology: Animal Behavior Processes</source>. <year>1979</year>;<volume>5</volume>(<issue>1</issue>):<fpage>31</fpage>. <object-id pub-id-type="pmid">528877</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref025">
<label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Agrawal S, Goyal N. Thompson sampling for contextual bandits with linear payoffs. In: International Conference on Machine Learning; 2013. p. 127–135.</mixed-citation>
</ref>
<ref id="pcbi.1007331.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Angela</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Uncertainty, neuromodulation, and attention</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>46</volume>(<issue>4</issue>):<fpage>681</fpage>–<lpage>692</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2005.04.026" xlink:type="simple">10.1016/j.neuron.2005.04.026</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Inferring relevance in a changing world</article-title>. <source>Frontiers in human neuroscience</source>. <year>2012</year>;<volume>5</volume>:<fpage>189</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2011.00189" xlink:type="simple">10.3389/fnhum.2011.00189</ext-link></comment> <object-id pub-id-type="pmid">22291631</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cappé</surname> <given-names>O</given-names></name>. <article-title>Online EM algorithm for hidden Markov models</article-title>. <source>Journal of Computational and Graphical Statistics</source>. <year>2011</year>;<volume>20</volume>(<issue>3</issue>):<fpage>728</fpage>–<lpage>749</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1198/jcgs.2011.09109" xlink:type="simple">10.1198/jcgs.2011.09109</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Page</surname> <given-names>ES</given-names></name>. <article-title>Continuous inspection schemes</article-title>. <source>Biometrika</source>. <year>1954</year>;<volume>41</volume>(<issue>1/2</issue>):<fpage>100</fpage>–<lpage>115</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/2333009" xlink:type="simple">10.2307/2333009</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref030">
<label>30</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Granjon</surname> <given-names>P</given-names></name>. <source>The CuSum algorithm-a small review</source>. <year>2013</year>;.</mixed-citation>
</ref>
<ref id="pcbi.1007331.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mackintosh</surname> <given-names>NJ</given-names></name>. <article-title>A theory of attention: variations in the associability of stimuli with reinforcement</article-title>. <source>Psychological review</source>. <year>1975</year>;<volume>82</volume>(<issue>4</issue>):<fpage>276</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0076778" xlink:type="simple">10.1037/h0076778</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chan</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Norman</surname> <given-names>KA</given-names></name>. <article-title>A probability distribution over latent causes, in the orbitofrontal cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2016</year>;<volume>36</volume>(<issue>30</issue>):<fpage>7817</fpage>–<lpage>7828</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0659-16.2016" xlink:type="simple">10.1523/JNEUROSCI.0659-16.2016</ext-link></comment> <object-id pub-id-type="pmid">27466328</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rich</surname> <given-names>EL</given-names></name>, <name name-style="western"><surname>Wallis</surname> <given-names>JD</given-names></name>. <article-title>Decoding subjective decisions from orbitofrontal cortex</article-title>. <source>Nature neuroscience</source>. <year>2016</year>;<volume>19</volume>(<issue>7</issue>):<fpage>973</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4320" xlink:type="simple">10.1038/nn.4320</ext-link></comment> <object-id pub-id-type="pmid">27273768</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Daniel</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Geana</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Leong</surname> <given-names>YC</given-names></name>, <name name-style="western"><surname>Radulescu</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Reinforcement learning in multidimensional environments relies on attention mechanisms</article-title>. <source>Journal of Neuroscience</source>. <year>2015</year>;<volume>35</volume>(<issue>21</issue>):<fpage>8145</fpage>–<lpage>8157</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2978-14.2015" xlink:type="simple">10.1523/JNEUROSCI.2978-14.2015</ext-link></comment> <object-id pub-id-type="pmid">26019331</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref035">
<label>35</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Kamin</surname> <given-names>LJ</given-names></name>. <source>Predictability, surprise, attention, and conditioning</source>. <year>1967</year>;.</mixed-citation>
</ref>
<ref id="pcbi.1007331.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lattal</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Nakajima</surname> <given-names>S</given-names></name>. <article-title>Overexpectation in appetitive Pavlovian and instrumental conditioning</article-title>. <source>Animal Learning &amp; Behavior</source>. <year>1998</year>;<volume>26</volume>(<issue>3</issue>):<fpage>351</fpage>–<lpage>360</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03199227" xlink:type="simple">10.3758/BF03199227</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>. <article-title>Pavlovian conditioned inhibition</article-title>. <source>Psychological Bulletin</source>. <year>1969</year>;<volume>72</volume>(<issue>2</issue>):<fpage>77</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0027760" xlink:type="simple">10.1037/h0027760</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wilson</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Boumphrey</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pearce</surname> <given-names>JM</given-names></name>. <article-title>Restoration of the orienting response to a light by a change in its predictive accuracy</article-title>. <source>The Quarterly Journal of Experimental Psychology Section B</source>. <year>1992</year>;<volume>44</volume>(<issue>1b</issue>):<fpage>17</fpage>–<lpage>36</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007331.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>. <article-title>Associative changes in excitors and inhibitors differ when they are conditioned in compound</article-title>. <source>Journal of Experimental Psychology: Animal Behavior Processes</source>. <year>2000</year>;<volume>26</volume>(<issue>4</issue>):<fpage>428</fpage>. <object-id pub-id-type="pmid">11056883</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Capaldi</surname> <given-names>E</given-names></name>. <article-title>The effect of different amounts of alternating partial reinforcement on resistance to extinction</article-title>. <source>The American Journal of Psychology</source>. <year>1957</year>;. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/1419584" xlink:type="simple">10.2307/1419584</ext-link></comment> <object-id pub-id-type="pmid">13458520</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jenkins</surname> <given-names>HM</given-names></name>. <article-title>Resistance to extinction when partial reinforcement is followed by regular reinforcement</article-title>. <source>Journal of Experimental Psychology</source>. <year>1962</year>;<volume>64</volume>(<issue>5</issue>):<fpage>441</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0048700" xlink:type="simple">10.1037/h0048700</ext-link></comment> <object-id pub-id-type="pmid">13964626</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Theios</surname> <given-names>J</given-names></name>. <article-title>The partial reinforcement effect sustained through blocks of continuous reinforcement</article-title>. <source>Journal of Experimental Psychology</source>. <year>1962</year>;<volume>64</volume>(<issue>1</issue>):<fpage>1</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0046302" xlink:type="simple">10.1037/h0046302</ext-link></comment> <object-id pub-id-type="pmid">13920533</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>RR</given-names></name>, <name name-style="western"><surname>Matute</surname> <given-names>H</given-names></name>. <article-title>Biological significance in forward and backward blocking: Resolution of a discrepancy between animal conditioning and human causal judgment</article-title>. <source>Journal of Experimental Psychology: General</source>. <year>1996</year>;<volume>125</volume>(<issue>4</issue>):<fpage>370</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0096-3445.125.4.370" xlink:type="simple">10.1037/0096-3445.125.4.370</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Monfils</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Cowansage</surname> <given-names>KK</given-names></name>, <name name-style="western"><surname>Klann</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>LeDoux</surname> <given-names>JE</given-names></name>. <article-title>Extinction-reconsolidation boundaries: key to persistent attenuation of fear memories</article-title>. <source>science</source>. <year>2009</year>;<volume>324</volume>(<issue>5929</issue>):<fpage>951</fpage>–<lpage>955</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1167975" xlink:type="simple">10.1126/science.1167975</ext-link></comment> <object-id pub-id-type="pmid">19342552</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schiller</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Monfils</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Raio</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>LeDoux</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Phelps</surname> <given-names>EA</given-names></name>. <article-title>Preventing the return of fear in humans using reconsolidation update mechanisms</article-title>. <source>Nature</source>. <year>2010</year>;<volume>463</volume>(<issue>7277</issue>):<fpage>49</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature08637" xlink:type="simple">10.1038/nature08637</ext-link></comment> <object-id pub-id-type="pmid">20010606</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ricker</surname> <given-names>ST</given-names></name>, <name name-style="western"><surname>Bouton</surname> <given-names>ME</given-names></name>. <article-title>Reacquisition following extinction in appetitive conditioning</article-title>. <source>Animal Learning &amp; Behavior</source>. <year>1996</year>;<volume>24</volume>(<issue>4</issue>):<fpage>423</fpage>–<lpage>436</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03199014" xlink:type="simple">10.3758/BF03199014</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bouton</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>DA</given-names></name>. <article-title>Contextual control of the extinction of conditioned fear: tests for the associative value of the context</article-title>. <source>Journal of Experimental Psychology: Animal Behavior Processes</source>. <year>1983</year>;<volume>9</volume>(<issue>3</issue>):<fpage>248</fpage>. <object-id pub-id-type="pmid">6886630</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brooks</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Bouton</surname> <given-names>ME</given-names></name>. <article-title>A retrieval cue for extinction attenuates spontaneous recovery</article-title>. <source>Journal of Experimental Psychology: Animal Behavior Processes</source>. <year>1993</year>;<volume>19</volume>(<issue>1</issue>):<fpage>77</fpage>. <object-id pub-id-type="pmid">8418218</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Phelps</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Differential roles of human striatum and amygdala in associative learning</article-title>. <source>Nature neuroscience</source>. <year>2011</year>;<volume>14</volume>(<issue>10</issue>):<fpage>1250</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2904" xlink:type="simple">10.1038/nn.2904</ext-link></comment> <object-id pub-id-type="pmid">21909088</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cappé</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Moulines</surname> <given-names>E</given-names></name>. <article-title>On-line expectation–maximization algorithm for latent data models</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>2009</year>;<volume>71</volume>(<issue>3</issue>):<fpage>593</fpage>–<lpage>613</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-9868.2009.00698.x" xlink:type="simple">10.1111/j.1467-9868.2009.00698.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Burgess</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Becker</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>O’Keefe</surname> <given-names>J</given-names></name>. <article-title>Memory for events and their spatial context: models and experiments</article-title>. <source>Philosophical Transactions of the Royal Society of London Series B: Biological Sciences</source>. <year>2001</year>;<volume>356</volume>(<issue>1413</issue>):<fpage>1493</fpage>–<lpage>1503</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2001.0948" xlink:type="simple">10.1098/rstb.2001.0948</ext-link></comment> <object-id pub-id-type="pmid">11571039</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Horvitz</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>. <article-title>Computational rationality: A converging paradigm for intelligence in brains, minds, and machines</article-title>. <source>Science</source>. <year>2015</year>;<volume>349</volume>(<issue>6245</issue>):<fpage>273</fpage>–<lpage>278</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.aac6076" xlink:type="simple">10.1126/science.aac6076</ext-link></comment> <object-id pub-id-type="pmid">26185246</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007331.ref053">
<label>53</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Robbins</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Monro</surname> <given-names>S</given-names></name>. <chapter-title>A stochastic approximation method</chapter-title>. In: <source>Herbert Robbins Selected Papers</source>. <publisher-name>Springer</publisher-name>; <year>1985</year>. p. <fpage>102</fpage>–<lpage>109</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007331.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dempster</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Laird</surname> <given-names>NM</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>DB</given-names></name>. <article-title>Maximum likelihood from incomplete data via the EM algorithm</article-title>. <source>Journal of the royal statistical society Series B (methodological)</source>. <year>1977</year>; p. <fpage>1</fpage>–<lpage>38</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.2517-6161.1977.tb01600.x" xlink:type="simple">10.1111/j.2517-6161.1977.tb01600.x</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>