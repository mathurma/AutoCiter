<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN"><front><journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">07-PLCB-RA-0688R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000093</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology</subject><subject>Computer Science/Applications</subject><subject>Evolutionary Biology/Bioinformatics</subject><subject>Genetics and Genomics/Population Genetics</subject></subj-group></article-categories><title-group><article-title>Machine-Learning Approaches for Classifying Haplogroup from Y Chromosome STR Data</article-title><alt-title alt-title-type="running-head">Machine-Learning Approaches for Classifying Y-STR</alt-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Schlecht</surname><given-names>Joseph</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Kaplan</surname><given-names>Matthew E.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Barnard</surname><given-names>Kobus</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Karafet</surname><given-names>Tatiana</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Hammer</surname><given-names>Michael F.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Merchant</surname><given-names>Nirav C.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>Computer Science Department, University of Arizona, Tucson, Arizona, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Arizona Research Laboratories, University of Arizona, Tucson, Arizona, United States of America</addr-line>       </aff><contrib-group><contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Ouzounis</surname><given-names>Christos A.</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">King's College London, United Kingdom</aff><author-notes><corresp id="cor1">* E-mail: <email xlink:type="simple">nirav@email.arizona.edu</email></corresp><fn fn-type="con"><p>Conceived and designed the experiments: JS MK KB NM. Performed the experiments: JS MK NM. Analyzed the data: JS MK TK MH NM. Contributed reagents/materials/analysis tools: JS MK NM. Wrote the paper: JS MK MH NM. Algorithm design and implementation: JS. Contributed to algorithm design: KB.</p></fn><fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>6</month><year>2008</year></pub-date><pub-date pub-type="epub"><day>13</day><month>6</month><year>2008</year></pub-date><volume>4</volume><issue>6</issue><elocation-id>e1000093</elocation-id><history><date date-type="received"><day>6</day><month>11</month><year>2007</year></date><date date-type="accepted"><day>1</day><month>5</month><year>2008</year></date></history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2008</copyright-year><copyright-holder>Schlecht et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract><p>Genetic variation on the non-recombining portion of the Y chromosome contains information about the ancestry of male lineages. Because of their low rate of mutation, single nucleotide polymorphisms (SNPs) are the markers of choice for unambiguously classifying Y chromosomes into related sets of lineages known as haplogroups, which tend to show geographic structure in many parts of the world. However, performing the large number of SNP genotyping tests needed to properly infer haplogroup status is expensive and time consuming. A novel alternative for assigning a sampled Y chromosome to a haplogroup is presented here. We show that by applying modern machine-learning algorithms we can infer with high accuracy the proper Y chromosome haplogroup of a sample by scoring a relatively small number of Y-linked short tandem repeats (STRs). Learning is based on a diverse ground-truth data set comprising pairs of SNP test results (haplogroup) and corresponding STR scores. We apply several independent machine-learning methods in tandem to learn formal classification functions. The result is an integrated high-throughput analysis system that automatically classifies large numbers of samples into haplogroups in a cost-effective and accurate manner.</p></abstract><abstract abstract-type="summary"><title>Author Summary</title><p>The Y chromosome is passed on from father to son as a nearly identical copy. Occasionally, small random changes occur in the Y DNA sequences that are passed forward to the next generation. There are two kinds of changes that may occur, and they both provide vital information for the study of human ancestry. Of the two kinds, one is a single letter change, and the other is a change in the number of short tandemly repeating sequences. The single-letter changes can be laborious to test, but they provide information on deep ancestry. Measuring the number of sequence repeats at multiple places in the genome simultaneously is efficient, and provides information about recent history at a modest cost. We present the novel approach of training a collection of modern machine-learning algorithms with these sequence repeats to infer the single-letter changes, thus assigning the samples to deep ancestry lineages.</p></abstract><funding-group><funding-statement>This study was supported by Arizona Research Laboratory (ARL) development fund.</funding-statement></funding-group><counts><page-count count="12"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Genetic variation on the non-recombining portion of the Y chromosome (NRY) has become the target of many recent studies with applications in a variety of disciplines, including DNA forensics <xref ref-type="bibr" rid="pcbi.1000093-Jobling1">[1]</xref>,<xref ref-type="bibr" rid="pcbi.1000093-Hammer1">[2]</xref>, medical genetics <xref ref-type="bibr" rid="pcbi.1000093-Jobling2">[3]</xref>, genealogical reconstruction <xref ref-type="bibr" rid="pcbi.1000093-Jobling3">[4]</xref>, molecular archeology <xref ref-type="bibr" rid="pcbi.1000093-Stone1">[5]</xref>, non-human primate genetics <xref ref-type="bibr" rid="pcbi.1000093-Stone2">[6]</xref>, and human evolutionary studies <xref ref-type="bibr" rid="pcbi.1000093-Hammer2">[7]</xref>–<xref ref-type="bibr" rid="pcbi.1000093-Hammer3">[9]</xref>. Two extremely useful classes of marker on the NRY include microsatellites or short tandem repeats (STRs) and single nucleotide polymorphism (SNPs) <xref ref-type="bibr" rid="pcbi.1000093-Hammer2">[7]</xref>. STRs consist of variable numbers of tandem repeat units ranging from 1 to 6-bp in length and mutate via a stepwise mutation mechanism, which favors very small (usually one repeat unit) changes in array length. Because high mutation rates (estimated to be 0.23%/STR/generation) in human pedigrees <xref ref-type="bibr" rid="pcbi.1000093-Heyer1">[10]</xref>,<xref ref-type="bibr" rid="pcbi.1000093-Kayser1">[11]</xref> often lead to situations where two alleles with the same repeat number are not identical by descent, STRs are not the marker of choice for constructing trees or for inferring relationships among divergent human populations. Rather, the high heterozygosity of STRs makes them useful for forensic and paternity analysis, and for inferring affinities among closely related populations.</p><p>Reconstructing relationships among globally dispersed populations or divergent male lineages requires polymorphisms with lower probabilities of back and parallel mutation (i.e., lower levels of homoplasy) and systems for which the ancestral state can be determined. SNPs and small indels, with mutation rates on the order of 2-4×10<sup>−8</sup>/site/generation, are best suited for these purposes. Because SNPs and indels are likely to have only two allelic classes segregating in human populations, they are sometimes referred to as binary markers (we refer to both classes of marker as SNPs). The combination of allelic states at many SNPs on a given Y chromosome is known as a haplogroup. A binary tree of NRY haplogroups with a standard nomenclature system (<xref ref-type="supplementary-material" rid="pcbi.1000093.s001">Figure S1</xref>) has been published and widely accepted among workers in the Y chromosome field <xref ref-type="bibr" rid="pcbi.1000093-YCC1">[12]</xref>,<xref ref-type="bibr" rid="pcbi.1000093-Jobling4">[13]</xref>. This Y chromosome tree is characterized by a hierarchically arranged set of 18 arbitrarily defined clusters of lineages (clades A–R), each with several sub-clades. By typing informative sets of SNPs, it is possible to assign samples to particular clades or subclades <xref ref-type="bibr" rid="pcbi.1000093-Hammer3">[9]</xref>,<xref ref-type="bibr" rid="pcbi.1000093-Underhill2">[14]</xref>.</p><p>One of the challenges for geneticists is the cost and time typically needed to genotype an appropriate number of SNPs to assign a given Y chromosome to a haplogroup. Multiplex strategies to type SNPs are also difficult and require a substantial initial investment to implement <xref ref-type="bibr" rid="pcbi.1000093-Sharan1">[15]</xref>. STRs on the NRY (Y-STRs) offer an alternative method for inferring the haplogroup of a sample. It has been recognized for some time that STR variability is partitioned to a greater extent by differences among haplogroups than by differences among populations <xref ref-type="bibr" rid="pcbi.1000093-Bosch1">[16]</xref>,<xref ref-type="bibr" rid="pcbi.1000093-Behar1">[17]</xref>. This suggests that Y-STRs contain information about the haplogroup status of a given Y chromosome. Because many Y-STRs can be genotyped in multiplex assays, typing appropriate sets of Y-STRs could represent a cost effective strategy for classifying Y chromosomes into haplogroups. In this paper, we assess this possibility from a computational perspective and show how a suite of modern machine learning algorithms can automatically classify and predict haplogroups based on allelic data from a suite of Y-STRs. We adapt three types of classifiers based on both generative and discriminative models to this problem. When all the methods agree in tandem, we combine the classifications from each into a haplogroup assignment. This enables an automatic, high throughput analysis pipeline for determining the haplogroup of a large number of samples in a cost effective and accurate manner.</p></sec><sec id="s2"><title>Results and Discussion</title><p>We obtained a data set collected by the Hammer laboratory that contains 8,414 globally diverse Y chromosome samples genotyped at 15 Y-STRs. The same samples were also typed with a battery of SNPs to identify the haplogroup of each sample. The SNPs typed and the resulting haplogroup tree are given in <xref ref-type="supplementary-material" rid="pcbi.1000093.s001">Figure S1</xref>, and the frequency of haplogroups in our data set is shown in <xref ref-type="fig" rid="pcbi-1000093-g001">Figure 1</xref>.</p><fig id="pcbi-1000093-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.g001</object-id><label>Figure 1</label><caption><title>Frequency of 30 haplogroups determined by SNP-typing a geographically diverse sample of 8,414 chromosomes.</title><p>This set of chromosomes, typed at 15 Y-linked STRs, was used as a ground-truth training set (see text for explanation). Haplogroups are named according to the mutation-based nomenclature <xref ref-type="bibr" rid="pcbi.1000093-YCC1">[12]</xref>, which retains the major haplogroup information (i.e., 18 capital letters) followed by the name of the terminal mutation that the sample is positive for (see <xref ref-type="supplementary-material" rid="pcbi.1000093.s001">Figure S1</xref>).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.g001" xlink:type="simple"/></fig><p>Since each sample of STR scores in our data is labeled with a haplogroup, we formalized the problem of classifying a new sample with a haplogroup as a supervised learning task and used our data set of 8,414 samples as a ground-truth set for training. In order to provide high classification accuracy, we combined the results of three disparate types of classifiers: decision trees, generative Bayesian models, and support vector machines. As we describe in <xref ref-type="sec" rid="s3">Materials and Methods</xref>, each algorithm has a unique method of learning haplogroup classification from training data; none commit exactly the same type of error and combining their output yields a more robust decision <xref ref-type="bibr" rid="pcbi.1000093-Dietterich1">[18]</xref>. Indeed, our results show that combining the classification output from these methods yields very accurate haplogroup assignment.</p><p>We compared our classification results to an informal nearest neighbor heuristic that labels STR samples with a haplogroup based on the stepwise mutation model <xref ref-type="bibr" rid="pcbi.1000093-Ohta1">[19]</xref>. We show that its results are not as effective as our tandem of machine learning techniques.</p><sec id="s2a"><title>Classifier Evaluation</title><p>We evaluated the performance of each classifier individually and in tandem using cross-validation on our 8,414 sample ground-truth training set, and compared the results with the nearest neighbor heuristic previously mentioned. We also performed cross-validation on publicly available data from other published research with Y-STR and haplogroup data. Finally, we tested the classification performance on the public data using our data for training. In brief, the results show the classifiers perform very well with a diverse training set and that the number of loci available in the data set is an important determining factor in their performance.</p><p>The cross-validation was accomplished by stochastically partitioning the data sets into <italic>k</italic> equally sized subsets, iteratively holding out each one while training on the remaining data, and then testing on the held out subset. More formally, let the ground-truth data set with <italic>N</italic> samples be <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e001" xlink:type="simple"/></inline-formula>. We create equally sized subsets <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e002" xlink:type="simple"/></inline-formula> for 1≤<italic>i</italic>≤<italic>k</italic> that form a partition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e003" xlink:type="simple"/></inline-formula>, i.e.,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e004" xlink:type="simple"/><label>(1)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e005" xlink:type="simple"/><label>(2)</label></disp-formula>We held out each subset <italic>A<sub>i</sub></italic> of the partition and trained the classifiers on the set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e006" xlink:type="simple"/></inline-formula>. A classification test was then performed on the held out set. In practice, the subset sizes may differ by one if <italic>N</italic>/<italic>k</italic> is not integral. For our experiments we chose to use <italic>k</italic> = 5 folds. The cross-validation was repeated 10 iterations, each time generating a random, equal partition of the data. The performance results were finally compiled with the mean and standard error statistics.</p><p>We combined the classification output for a sample from the decision trees (J48 and PART), Bayesian models, and support vector machines into a tandem decision. The output haplogroups from each of the classifiers were compared together, and if they were in agreement, accepted, or assigned, the classification; otherwise the sample was left <italic>unassigned</italic> if they disagree and held-out for further analysis. Since the classifications may not always be at the same depth in the haplogroup hierarchy, <xref ref-type="supplementary-material" rid="pcbi.1000093.s001">Figure S1</xref>, we compared the results up to the common level in the tree and accepted the classification if it was in agreement.</p><p>In practice, an unassigned sample for the tandem approach is selected for manual, expert analysis. Experienced personnel examine the haplogroup assignment from the individual classifiers for familiar patterns. The confidence values from the classifiers may also be analyzed to resolve frequently seen disagreements. If the ambiguity cannot be resolved at this stage, SNP testing is done to ensure a correct haplogroup label. The result of the SNP test is then added to the training set to continually improve the classifiers.</p><p>For the nearest neighbor heuristic we used the <italic>L</italic><sup>1</sup>-norm distance metric combined with the following rules. If the sum of allele value differences between a novel sample and one in the training set was zero, it was an exact match and the novel sample was labeled with the matching sample's haplogroup. If the allele values differed by only one or two, and the samples by which it differed were all in the same haplogroup, it was considered a match resulting from a stepwise mutation and again labeled with the matching samples' haplogroup. Otherwise, the sample was left unassigned.</p><p><xref ref-type="table" rid="pcbi-1000093-t001">Table 1</xref> shows the average overall performance of the classifiers, including tandem agreement and the nearest neighbor heuristic, for ten iterations of the 5 fold cross-validation on our ground-truth training set. The support vector machine was the best performing individual classifier with 95% accuracy. The performance of the Bayesian classifier and the decision trees was very comparable. The results for the tandem strategy show that of all the samples we attempted to classify, 86% were in agreement, and that almost 99% of those predictions were correct. Furthermore, the 14% unassignment rate of the tandem approach was much lower than the 26% of the nearest neighbor heuristic.</p><table-wrap id="pcbi-1000093-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.t001</object-id><label>Table 1</label><caption><title>Average classifier performance for cross-validation on our 8,414 sample ground-truth training set (see text for experiment details).</title></caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000093-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="2" rowspan="1">Percent Correct of Assigned</td><td align="left" colspan="2" rowspan="1">Correct</td><td align="left" colspan="2" rowspan="1">Incorrect</td><td align="left" colspan="2" rowspan="1">Unassigned</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Tandem</td><td align="left" colspan="1" rowspan="1">98.8</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">1426.1</td><td align="left" colspan="1" rowspan="1">1.8</td><td align="left" colspan="1" rowspan="1">17.9</td><td align="left" colspan="1" rowspan="1">0.7</td><td align="left" colspan="1" rowspan="1">238.8</td><td align="left" colspan="1" rowspan="1">1.9</td></tr><tr><td align="left" colspan="1" rowspan="1">SVM</td><td align="left" colspan="1" rowspan="1">95.0</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">1598.5</td><td align="left" colspan="1" rowspan="1">1.2</td><td align="left" colspan="1" rowspan="1">84.3</td><td align="left" colspan="1" rowspan="1">1.2</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">J48</td><td align="left" colspan="1" rowspan="1">92.7</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">1559.5</td><td align="left" colspan="1" rowspan="1">1.5</td><td align="left" colspan="1" rowspan="1">123.3</td><td align="left" colspan="1" rowspan="1">1.5</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">PART</td><td align="left" colspan="1" rowspan="1">92.5</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">1556.8</td><td align="left" colspan="1" rowspan="1">1.6</td><td align="left" colspan="1" rowspan="1">126.0</td><td align="left" colspan="1" rowspan="1">1.6</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Bayes</td><td align="left" colspan="1" rowspan="1">91.5</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">1540.2</td><td align="left" colspan="1" rowspan="1">1.5</td><td align="left" colspan="1" rowspan="1">142.6</td><td align="left" colspan="1" rowspan="1">1.5</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Nearest</td><td align="left" colspan="1" rowspan="1">98.3</td><td align="left" colspan="1" rowspan="1">0.2</td><td align="left" colspan="1" rowspan="1">1227.3</td><td align="left" colspan="1" rowspan="1">2.5</td><td align="left" colspan="1" rowspan="1">21.6</td><td align="left" colspan="1" rowspan="1">0.5</td><td align="left" colspan="1" rowspan="1">433.9</td><td align="left" colspan="1" rowspan="1">2.6</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt101"><p>Support vector machines has the highest individual accuracy. J48 and PART are decision tree classifiers. Combining the classifiers together into the tandem strategy boosts the performance to a very high accuracy while maintaining a much lower unassignment rate than the nearest neighbor heuristic.</p></fn></table-wrap-foot></table-wrap><p>The average accuracy for each of the classifiers per haplogroup is shown in the top panel of <xref ref-type="fig" rid="pcbi-1000093-g002">Figure 2</xref>, and the haplogroup frequency of the training data is below it in the bottom panel. It is clear from the figure that the accuracy of classification for a particular haplogroup is dependent on its frequency in the data. We also observe that the support vector machines perform the best, particularly in cases where training data for a haplogroup is most sparse. We believe that more training data from sparse groups, such as A, B, C, D, H, and N would improve the results to similar levels of more well represented haplogroups such as I, J, and R.</p><fig id="pcbi-1000093-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.g002</object-id><label>Figure 2</label><caption><title>Average accuracy of each classifier per haplogroup for cross-validation on the ground-truth training set.</title><p>Standard error bars are shown for each point. The lower panel shows the frequency of the 8,414 samples among haplogroups. Support vector machines has the best overall performance, especially in the case of haplogroups with a smaller number of samples in the training data.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.g002" xlink:type="simple"/></fig><p>The classification accuracy under the tandem approach was very high. <xref ref-type="fig" rid="pcbi-1000093-g003">Figure 3</xref> shows the performance for each haplogroup when all the classifiers agreed. While not all of the classifiers agreed in their output in all cases, we observe from the results in <xref ref-type="fig" rid="pcbi-1000093-g003">Figure 3</xref> and <xref ref-type="table" rid="pcbi-1000093-t002">Table 2</xref> that the rate of agreement was very low mostly in haplogroups with low representation in the data. Again, as we continue to increase the size and diversity of the training set, we expect that the level of agreement in the tandem approach will continue to improve.</p><fig id="pcbi-1000093-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.g003</object-id><label>Figure 3</label><caption><title>Average accuracy of the tandem approach for cross-validation on the ground-truth training set.</title><p>The average proportion of samples with agreement for all four classification methods is also shown. The haplogroups with the highest rate of tandem disagreement have a low representation in the training data.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.g003" xlink:type="simple"/></fig><table-wrap id="pcbi-1000093-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.t002</object-id><label>Table 2</label><caption><title>Average haplogroup assignment accuracy after cross-validation of the ground-truth training data for the tandem approach when all classifiers agree.</title></caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000093-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.t002" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" colspan="1" rowspan="1">Haplogroup</td><td align="left" colspan="2" rowspan="1">Correct</td><td align="left" colspan="2" rowspan="1">Incorrect</td><td align="left" colspan="2" rowspan="1">Unassigned</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">A- P97</td><td align="left" colspan="1" rowspan="1">3.1</td><td align="left" colspan="1" rowspan="1">0.2</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">0.0</td><td align="left" colspan="1" rowspan="1">2.4</td><td align="left" colspan="1" rowspan="1">0.2</td></tr><tr><td align="left" colspan="1" rowspan="1">B- M60</td><td align="left" colspan="1" rowspan="1">1.2</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">0.0</td><td align="left" colspan="1" rowspan="1">0.0</td><td align="left" colspan="1" rowspan="1">2.3</td><td align="left" colspan="1" rowspan="1">0.3</td></tr><tr><td align="left" colspan="1" rowspan="1">C- M130,M216</td><td align="left" colspan="1" rowspan="1">1.6</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">0.0</td><td align="left" colspan="1" rowspan="1">0.0</td><td align="left" colspan="1" rowspan="1">6.7</td><td align="left" colspan="1" rowspan="1">0.4</td></tr><tr><td align="left" colspan="1" rowspan="1">D- M174</td><td align="left" colspan="1" rowspan="1">0.7</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">0.2</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">2.5</td><td align="left" colspan="1" rowspan="1">0.2</td></tr><tr><td align="left" colspan="1" rowspan="1">E- M96,SRY4064</td><td align="left" colspan="1" rowspan="1">111.4</td><td align="left" colspan="1" rowspan="1">1.3</td><td align="left" colspan="1" rowspan="1">0.9</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">31.9</td><td align="left" colspan="1" rowspan="1">0.8</td></tr><tr><td align="left" colspan="1" rowspan="1">G- M201</td><td align="left" colspan="1" rowspan="1">114.9</td><td align="left" colspan="1" rowspan="1">1.2</td><td align="left" colspan="1" rowspan="1">2.4</td><td align="left" colspan="1" rowspan="1">0.2</td><td align="left" colspan="1" rowspan="1">39.3</td><td align="left" colspan="1" rowspan="1">0.9</td></tr><tr><td align="left" colspan="1" rowspan="1">H- M69</td><td align="left" colspan="1" rowspan="1">4.0</td><td align="left" colspan="1" rowspan="1">0.3</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">0.0</td><td align="left" colspan="1" rowspan="1">5.7</td><td align="left" colspan="1" rowspan="1">0.3</td></tr><tr><td align="left" colspan="1" rowspan="1">I- M170,P19</td><td align="left" colspan="1" rowspan="1">335.7</td><td align="left" colspan="1" rowspan="1">2.2</td><td align="left" colspan="1" rowspan="1">4.7</td><td align="left" colspan="1" rowspan="1">0.4</td><td align="left" colspan="1" rowspan="1">59.4</td><td align="left" colspan="1" rowspan="1">1.1</td></tr><tr><td align="left" colspan="1" rowspan="1">J- M267</td><td align="left" colspan="1" rowspan="1">125.4</td><td align="left" colspan="1" rowspan="1">1.5</td><td align="left" colspan="1" rowspan="1">2.0</td><td align="left" colspan="1" rowspan="1">0.2</td><td align="left" colspan="1" rowspan="1">9.3</td><td align="left" colspan="1" rowspan="1">0.4</td></tr><tr><td align="left" colspan="1" rowspan="1">J- M172</td><td align="left" colspan="1" rowspan="1">128.4</td><td align="left" colspan="1" rowspan="1">1.5</td><td align="left" colspan="1" rowspan="1">2.3</td><td align="left" colspan="1" rowspan="1">0.2</td><td align="left" colspan="1" rowspan="1">18.0</td><td align="left" colspan="1" rowspan="1">0.7</td></tr><tr><td align="left" colspan="1" rowspan="1">K- M70</td><td align="left" colspan="1" rowspan="1">62.9</td><td align="left" colspan="1" rowspan="1">1.0</td><td align="left" colspan="1" rowspan="1">0.9</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">3.8</td><td align="left" colspan="1" rowspan="1">0.3</td></tr><tr><td align="left" colspan="1" rowspan="1">L- M20</td><td align="left" colspan="1" rowspan="1">6.2</td><td align="left" colspan="1" rowspan="1">0.3</td><td align="left" colspan="1" rowspan="1">1.3</td><td align="left" colspan="1" rowspan="1">0.2</td><td align="left" colspan="1" rowspan="1">2.9</td><td align="left" colspan="1" rowspan="1">0.3</td></tr><tr><td align="left" colspan="1" rowspan="1">N- M231</td><td align="left" colspan="1" rowspan="1">0.6</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">0.9</td><td align="left" colspan="1" rowspan="1">0.2</td><td align="left" colspan="1" rowspan="1">3.0</td><td align="left" colspan="1" rowspan="1">0.2</td></tr><tr><td align="left" colspan="1" rowspan="1">O- M175</td><td align="left" colspan="1" rowspan="1">24.0</td><td align="left" colspan="1" rowspan="1">0.6</td><td align="left" colspan="1" rowspan="1">0.4</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">22.7</td><td align="left" colspan="1" rowspan="1">0.6</td></tr><tr><td align="left" colspan="1" rowspan="1">Q- P36,M242</td><td align="left" colspan="1" rowspan="1">40.5</td><td align="left" colspan="1" rowspan="1">0.8</td><td align="left" colspan="1" rowspan="1">0.4</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">7.3</td><td align="left" colspan="1" rowspan="1">0.4</td></tr><tr><td align="left" colspan="1" rowspan="1">R- SRY10831.2</td><td align="left" colspan="1" rowspan="1">100.0</td><td align="left" colspan="1" rowspan="1">1.3</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">0.0</td><td align="left" colspan="1" rowspan="1">5.7</td><td align="left" colspan="1" rowspan="1">0.3</td></tr><tr><td align="left" colspan="1" rowspan="1">R- M343</td><td align="left" colspan="1" rowspan="1">346.7</td><td align="left" colspan="1" rowspan="1">1.7</td><td align="left" colspan="1" rowspan="1">0.6</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">15.5</td><td align="left" colspan="1" rowspan="1">0.6</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt102"><p>The far right column gives the number of samples with an unassigned tandem classification—all four methods did not agree.</p></fn></table-wrap-foot></table-wrap><p>In addition to testing the performance of the classifiers on our 15-locus data set, we also tested them on published STR data collected from West, South and East Asian populations <xref ref-type="bibr" rid="pcbi.1000093-Sengupta1">[20]</xref>,<xref ref-type="bibr" rid="pcbi.1000093-Cinnioglu1">[21]</xref>. The combined public data sets have 1,527 samples of 9 loci at DYS394, DYS388, DYS389-I, DYS389-II, DYS390, DYS391, DYS392, DYS393, and DYS439. <xref ref-type="fig" rid="pcbi-1000093-g004">Figure 4</xref> shows the frequencies of Y chromosome haplogroups in this sample. We performed two types of experiments with this data. We first looked at performance using the public data both for training and testing with a 5 fold cross-validation. We then used our ground-truth data set restricted to the 9 applicable loci as training data, and tested the performance on the entire public data set.</p><fig id="pcbi-1000093-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.g004</object-id><label>Figure 4</label><caption><title>Frequency of 30 Y chromosome haplogroups inferred from a previously published sample of 1,527 Asian Y chromosomes.</title><p>The samples were typed with 9 Y-STRs and a battery of Y-linked SNPs. Haplogroup frequencies are statistically significantly different from those in our ground-truth training set (<xref ref-type="fig" rid="pcbi-1000093-g001">Figure 1</xref>). Haplogroups are named according to the mutation-based nomenclature <xref ref-type="bibr" rid="pcbi.1000093-YCC1">[12]</xref>, which retains the major haplogroup information (i.e., 18 capital letters) followed by the name of the terminal mutation that the sample is positive for (see <xref ref-type="supplementary-material" rid="pcbi.1000093.s001">Figure S1</xref>).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.g004" xlink:type="simple"/></fig><p>As before when testing the classifiers on our ground-truth data, we ran 10 iterations of five fold cross-validation on the public data. <xref ref-type="table" rid="pcbi-1000093-t003">Table 3</xref> gives the averaged results. In order to provide a meaningful comparison across the two data sets, the table also shows cross-validation results on the 9-locus subset of our data; six of the loci are not shared by both sets and may affect discriminative abilities of the classifiers. We observe that the classification accuracy between the two data sets is comparable. Indeed, the cross-validation on the public data has slightly better results. <xref ref-type="fig" rid="pcbi-1000093-g005">Figure 5</xref> and <xref ref-type="fig" rid="pcbi-1000093-g006">Figure 6</xref> show the average per haplogroup classification accuracy for cross-validation on the public data.</p><fig id="pcbi-1000093-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.g005</object-id><label>Figure 5</label><caption><title>Average accuracy of each classifier per haplogroup for cross-validation on the 9-locus public STR data.</title><p>Standard error bars are shown for each point. The lower panel shows the frequency of haplogroups in the 1,527 sample public data set.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.g005" xlink:type="simple"/></fig><fig id="pcbi-1000093-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.g006</object-id><label>Figure 6</label><caption><title>Average accuracy and agreement of the tandem approach for cross-validation on the 9-locus public STR data.</title></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.g006" xlink:type="simple"/></fig><table-wrap id="pcbi-1000093-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.t003</object-id><label>Table 3</label><caption><title>Comparison of classifier performance across two data sets.</title></caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000093-t003-3" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.t003" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" colspan="9" rowspan="1">9-Locus Public Data</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="2" rowspan="1">Percent Correct of Assigned</td><td align="left" colspan="2" rowspan="1">Correct</td><td align="left" colspan="2" rowspan="1">Incorrect</td><td align="left" colspan="2" rowspan="1">Unassigned</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Tandem</td><td align="left" colspan="1" rowspan="1">95.6</td><td align="left" colspan="1" rowspan="1">0.5</td><td align="left" colspan="1" rowspan="1">208.6</td><td align="left" colspan="1" rowspan="1">1.1</td><td align="left" colspan="1" rowspan="1">9.5</td><td align="left" colspan="1" rowspan="1">0.3</td><td align="left" colspan="1" rowspan="1">87.3</td><td align="left" colspan="1" rowspan="1">1.1</td></tr><tr><td align="left" colspan="1" rowspan="1">SVM</td><td align="left" colspan="1" rowspan="1">87.1</td><td align="left" colspan="1" rowspan="1">0.2</td><td align="left" colspan="1" rowspan="1">265.9</td><td align="left" colspan="1" rowspan="1">0.7</td><td align="left" colspan="1" rowspan="1">39.5</td><td align="left" colspan="1" rowspan="1">0.7</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">PART</td><td align="left" colspan="1" rowspan="1">84.3</td><td align="left" colspan="1" rowspan="1">0.3</td><td align="left" colspan="1" rowspan="1">257.6</td><td align="left" colspan="1" rowspan="1">0.8</td><td align="left" colspan="1" rowspan="1">47.8</td><td align="left" colspan="1" rowspan="1">0.8</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">J48</td><td align="left" colspan="1" rowspan="1">84.3</td><td align="left" colspan="1" rowspan="1">0.3</td><td align="left" colspan="1" rowspan="1">257.5</td><td align="left" colspan="1" rowspan="1">0.8</td><td align="left" colspan="1" rowspan="1">47.9</td><td align="left" colspan="1" rowspan="1">0.8</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Bayes</td><td align="left" colspan="1" rowspan="1">79.0</td><td align="left" colspan="1" rowspan="1">0.3</td><td align="left" colspan="1" rowspan="1">241.3</td><td align="left" colspan="1" rowspan="1">0.9</td><td align="left" colspan="1" rowspan="1">64.1</td><td align="left" colspan="1" rowspan="1">0.9</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Nearest</td><td align="left" colspan="1" rowspan="1">92.6</td><td align="left" colspan="1" rowspan="1">0.4</td><td align="left" colspan="1" rowspan="1">212.8</td><td align="left" colspan="1" rowspan="1">1.0</td><td align="left" colspan="1" rowspan="1">17.0</td><td align="left" colspan="1" rowspan="1">0.6</td><td align="left" colspan="1" rowspan="1">75.6</td><td align="left" colspan="1" rowspan="1">0.8</td></tr></tbody></table><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" colspan="9" rowspan="1">9-Locus Ground-Truth Data</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="2" rowspan="1">Percent Correct of Assigned</td><td align="left" colspan="2" rowspan="1">Correct</td><td align="left" colspan="2" rowspan="1">Incorrect</td><td align="left" colspan="2" rowspan="1">Unassigned</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">SE</td></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Tandem</td><td align="left" colspan="1" rowspan="1">92.4</td><td align="left" colspan="1" rowspan="1">0.2</td><td align="left" colspan="1" rowspan="1">1,100.2</td><td align="left" colspan="1" rowspan="1">2.4</td><td align="left" colspan="1" rowspan="1">90.0</td><td align="left" colspan="1" rowspan="1">1.1</td><td align="left" colspan="1" rowspan="1">616.7</td><td align="left" colspan="1" rowspan="1">2.4</td></tr><tr><td align="left" colspan="1" rowspan="1">SVM</td><td align="left" colspan="1" rowspan="1">82.4</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">1,386.1</td><td align="left" colspan="1" rowspan="1">1.9</td><td align="left" colspan="1" rowspan="1">296.7</td><td align="left" colspan="1" rowspan="1">1.9</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">PART</td><td align="left" colspan="1" rowspan="1">81.1</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">1,364.2</td><td align="left" colspan="1" rowspan="1">1.9</td><td align="left" colspan="1" rowspan="1">318.6</td><td align="left" colspan="1" rowspan="1">1.9</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">J48</td><td align="left" colspan="1" rowspan="1">81.2</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">1,366.5</td><td align="left" colspan="1" rowspan="1">1.9</td><td align="left" colspan="1" rowspan="1">316.3</td><td align="left" colspan="1" rowspan="1">1.9</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Bayes</td><td align="left" colspan="1" rowspan="1">76.9</td><td align="left" colspan="1" rowspan="1">0.1</td><td align="left" colspan="1" rowspan="1">1,293.4</td><td align="left" colspan="1" rowspan="1">2.1</td><td align="left" colspan="1" rowspan="1">389.4</td><td align="left" colspan="1" rowspan="1">2.1</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Nearest</td><td align="left" colspan="1" rowspan="1">91.8</td><td align="left" colspan="1" rowspan="1">0.4</td><td align="left" colspan="1" rowspan="1">618.6</td><td align="left" colspan="1" rowspan="1">2.9</td><td align="left" colspan="1" rowspan="1">55.6</td><td align="left" colspan="1" rowspan="1">1.1</td><td align="left" colspan="1" rowspan="1">1,008.7</td><td align="left" colspan="1" rowspan="1">3.3</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt103"><p>The top table shows the average classifier performance for cross-validation on the 9-locus public STR data. The bottom table is the performance for the same test, but on a 9-locus subset of our ground-truth training data. While overall performance is lower than the 15-locus cross-validation test on our ground-truth data (<xref ref-type="table" rid="pcbi-1000093-t001">Table 1</xref>), the two data sets perform similarly here, indicating that increasing the number of markers in the data set can significantly improve performance.</p></fn></table-wrap-foot></table-wrap><p>Compared with the earlier cross-validation results on our ground-truth data (<xref ref-type="table" rid="pcbi-1000093-t001">Table 1</xref>), the 9 Y-STR subset has a much lower performance than the original 15 Y-STR set. This implies that the 6 excluded markers contribute to a non-negligible increase in performance. Thus, if the public data set had these additional markers, we expect that its accuracy under cross-validation would also improve.</p><p>We tested haplogroup classification for the public STR data using classifiers trained with the 9-locus subset of our data set. The classification accuracy results are reported in <xref ref-type="table" rid="pcbi-1000093-t004">Table 4</xref>. <xref ref-type="supplementary-material" rid="pcbi.1000093.s002">Figures S2</xref> and <xref ref-type="supplementary-material" rid="pcbi.1000093.s003">S3</xref> show the average per haplogroup performance. Although the tandem approach still out-performed the nearest neighbor method, the overall performance shows a decrease in accuracy. We believe the performance is lower for two reasons: as we have already shown, training with 9 versus 15 Y-STRs substantially reduces the accuracy of classification (an almost 7 point reduction for the tandem approach when contrasting <xref ref-type="table" rid="pcbi-1000093-t001">Table 1</xref> with the lower panel of <xref ref-type="table" rid="pcbi-1000093-t003">Table 3</xref>); and the origins of the samples in the public data sets are from populations that are not as well represented in our data set.</p><table-wrap id="pcbi-1000093-t004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.t004</object-id><label>Table 4</label><caption><title>Classification results for the 9-locus public Y-STR data.</title></caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000093-t004-4" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.t004" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Percent CoA</td><td align="left" colspan="1" rowspan="1">Correct</td><td align="left" colspan="1" rowspan="1">Incorrect</td><td align="left" colspan="1" rowspan="1">Unassigned</td></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Tandem</td><td align="left" colspan="1" rowspan="1">83.9</td><td align="left" colspan="1" rowspan="1">732</td><td align="left" colspan="1" rowspan="1">140</td><td align="left" colspan="1" rowspan="1">655</td></tr><tr><td align="left" colspan="1" rowspan="1">SVM</td><td align="left" colspan="1" rowspan="1">67.2</td><td align="left" colspan="1" rowspan="1">1,026</td><td align="left" colspan="1" rowspan="1">501</td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">PART</td><td align="left" colspan="1" rowspan="1">70.5</td><td align="left" colspan="1" rowspan="1">1,077</td><td align="left" colspan="1" rowspan="1">450</td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">J48</td><td align="left" colspan="1" rowspan="1">70.5</td><td align="left" colspan="1" rowspan="1">1,076</td><td align="left" colspan="1" rowspan="1">451</td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Bayes</td><td align="left" colspan="1" rowspan="1">62.8</td><td align="left" colspan="1" rowspan="1">959</td><td align="left" colspan="1" rowspan="1">568</td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Nearest</td><td align="left" colspan="1" rowspan="1">81.9</td><td align="left" colspan="1" rowspan="1">398</td><td align="left" colspan="1" rowspan="1">88</td><td align="left" colspan="1" rowspan="1">1,041</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt104"><p>A 9-locus subset of our ground-truth data was used to train the classifiers (Percent CoA = % Correct of Assigned).</p></fn></table-wrap-foot></table-wrap></sec><sec id="s2b"><title>Conclusion</title><p>In this paper we have shown that by using machine learning algorithms and data derived only from a set of Y-linked STRs, it is possible to assign Y chromosome haplogroups to individual samples with a high degree of accuracy. We note that the number of Y-STRs used has a significant impact on the accuracy of haplogroup classification.</p><p>Our classification software provides a single turnkey interface to a tandem of machine learning algorithms. It is extensible in that other high-performing classification algorithms can be added to it when they are developed. We have made the software freely available to use for non-commercial purposes and posted it online at <ext-link ext-link-type="uri" xlink:href="http://bcf.arl.arizona.edu/haplo" xlink:type="simple">http://bcf.arl.arizona.edu/haplo</ext-link>.</p><p>Future work could focus on identifying an optimal set of Y-STRs to obtain the highest accuracy of haplogroup classification. Our preliminary results (data not shown) suggest that different Y-STRs are informative for different haplogroups. Additional work should help to better understand the properties that make different Y-STRs more or less informative for proper haplogroup assignment.</p><p>We have assumed in our Bayesian model that the Y-STR loci are statistically independent given the haplogroup. While we have observed good performance for this model, it most likely does not reflect the true relationship among loci. As more information about loci linkage becomes available and our ground-truth data set continues to expand, we could relax this assumption and begin to include such dependencies.</p><p>Our Bayesian model assumes that Y-STRs are statistically independent conditioned on the haplogroup. While we observe good performance using this model, this assumption is not realistic given the lack of crossing over among the Y-linked STRs used in our analysis. On the other hand, Y-STRs mutate independently in a stepwise fashion, which may cause particular Y-STRs to be effectively unlinked on some haplogroup backgrounds. As more information about linkage becomes available and our ground-truth data set continues to expand, we may be able to include such information to improve our model.</p><p>The software system can be effectively used to construct high throughput SNP test panels, particularly in the case of platforms that restrict the number of SNPs accommodated per panel. Given a corpus of STR data, the classifiers can identify a collection of candidate SNP sites to be placed on the panel to provide maximum coverage over potential haplogroups in a population. In this way the software provides a cost-effective first step in a multi-level process for deep haplogroup identification by facilitating targeted SNP testing.</p></sec></sec><sec id="s3"><title>Materials and Methods</title><p>The 15 Y chromosome STR loci used in this study are: DYS393, DYS390, DYS394 (both copies when duplicated), DYS391, DYS385a, DYS385b, DYS426, DYS388, DYS439, DYS389 I, DYS389 II, DYS392, DYS438, DYS457. These loci are commonly used in the fields of population genetics, forensic science, and commercial genealogical testing <xref ref-type="bibr" rid="pcbi.1000093-Butler1">[22]</xref>. The STR loci were amplified in two multiplex PCR reactions. The products of these reactions were mixed and analyzed on an Applied Biosystems 3730 capillary electrophoresis instrument.</p><p>The SNP and STR data for samples utilized to construct the training data set and models for this study were acquired and analyzed over an extended period of time by the Hammer laboratory. The SNPs were identified using a variety of techniques including: DNA sequencing, allele specific PCR scored by agarose electrophoresis, PCR and restriction digest, and TaqMan assays. A test panel comprising the SNPs in <xref ref-type="fig" rid="pcbi-1000093-g001">Figure 1</xref> was developed and validated for use on a Beckman Coulter SNPStream instrument <xref ref-type="bibr" rid="pcbi.1000093-Bell1">[23]</xref>. This instrument permits simultaneous testing for all SNPs represented on the panel for a given sample. Novel samples utilized for testing and validation of the models were STR tested and processed on the SNPStream instrument to verify the predicted SNP assignments.</p><p>What follows is a brief description of the classifiers we used and how each was adapted and extended to the haplogroup assignment problem. We first introduce some notation shared among all classifier descriptions. Let <italic>L</italic> be the number of analyzed Y-STRs and <italic>G</italic> be number of haplogroups under consideration. Denote the ground-truth data set of <italic>N</italic> samples by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e007" xlink:type="simple"/></inline-formula>. Each sample in the set comprises a tuple of haplogroup index and STR alleles <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e008" xlink:type="simple"/></inline-formula> where 1≤<italic>g</italic>≤<italic>G</italic> and <bold>x</bold> = (<italic>x</italic><sub>1</sub>,…,<italic>x<sub>L</sub></italic>). Where applicable, let <bold>X</bold> = (<italic>X</italic><sub>1</sub>,…,<italic>X<sub>L</sub></italic>) be random variables taking alleles from the <italic>L</italic> loci on the Y chromosome.</p><sec id="s3a"><title>Decision Trees</title><p>In a decision tree classifier, we learn a set of rules for separating samples into hierarchical classification groups according to locus and allele values. The internal nodes of the tree are comprised of locus tests for specific allele values and the terminal nodes represent haplogroup classification. The set of tests from the root node in the tree to a terminal node is the classification rule for a haplogroup. The tree is constructed from a set of training data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e009" xlink:type="simple"/></inline-formula> using the C4.5 algorithm <xref ref-type="bibr" rid="pcbi.1000093-Quinlan1">[24]</xref>, which hierarchically selects loci that best differentiate the training data into haplogroups.</p><p>The locus tests are constructed using a measure of information gain, which is based on information entropy <xref ref-type="bibr" rid="pcbi.1000093-Shannon1">[25]</xref>. The entropy of a random variable quantifies its randomness or uncertainty. In the case of haplogroups, entropy indicates how much diversity there is in the sample set.</p><p>Let <italic>n<sub>g</sub></italic> be the number of samples in haplogroup <italic>g</italic>. The entropy for <italic>G</italic> haplogroups over the data set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e010" xlink:type="simple"/></inline-formula> is defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e011" xlink:type="simple"/><label>(3)</label></disp-formula>where <italic>p</italic>(<italic>g</italic>) = <italic>n<sub>g</sub></italic>/<italic>N</italic> is the probability of the <italic>g</italic><sup>th</sup> haplogroup in the data set. Thus, higher entropy suggests higher diversity and a more uniform frequency of haplogroup representation in the sample set.</p><p>Knowing the allele value of a locus may affect the entropy of the data; additional information either does not change or decreases the entropy. When a particular allele at the <italic>i</italic><sup>th</sup> locus is known, the conditional entropy is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e012" xlink:type="simple"/><label>(4)</label></disp-formula>where <italic>p<sub>i</sub></italic>(<italic>g</italic>|<italic>x</italic>) is the probability the <italic>g</italic><sup>th</sup> haplogroup has allele <italic>x</italic> at locus <italic>i</italic>. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e013" xlink:type="simple"/></inline-formula> be the number of samples with the latter characteristic and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e014" xlink:type="simple"/></inline-formula> be the total number of samples in the data with allele <italic>x</italic> at locus <italic>i</italic>. Then <italic>p<sub>i</sub></italic> is defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e015" xlink:type="simple"/><label>(5)</label></disp-formula></p><p>We obtain a general conditional entropy for each locus by marginalizing out the allele values. This is equivalent to computing a weighted average of Equation 4, where the weights are given by the probability of each allele.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e016" xlink:type="simple"/><label>(6)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e017" xlink:type="simple"/><label>(7)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e018" xlink:type="simple"/><label>(8)</label></disp-formula>where <italic>p<sub>i</sub></italic>(<italic>x</italic>) is the probability of allele <italic>x</italic> at the <italic>i</italic><sup>th</sup> locus over all samples in the data. It is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e019" xlink:type="simple"/><label>(9)</label></disp-formula>The general conditional entropy in Equation 8 tells us how much Y-STR allelic variation is associated with a given haplogroup. A lower value indicates the allele values at the locus explain or predict the haplogroup well. This leads to the concept of information gain.</p><p>The difference in variation among haplogroups when a Y-STR allele is both known and unknown is the information gain. It is a measure of how well a locus explains haplogroup membership. Formally, it is defined for the <italic>i</italic><sup>th</sup> locus in the data as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e020" xlink:type="simple"/><label>(10)</label></disp-formula>The information gain will always be non-negative, since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e021" xlink:type="simple"/></inline-formula> for all loci.</p><p>Given the data set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e022" xlink:type="simple"/></inline-formula>, we trained a decision tree by hierarchically computing the information gain for each Y-STR. A branch in the tree is created from the locus yielding the maximum gain. The branch is a test created using the selected locus to divide the data set into subsets grouped by haplogroup and (possibly shared) allele values. Tests at lower levels of the tree are constructed from these subsets in a similar fashion. Once all the samples in a subset are in the same haplogroup, a terminal leaf on the tree is created, which represents a classification. <xref ref-type="fig" rid="pcbi-1000093-g007">Figure 7</xref> illustrates this process. To classify a new sample, we begin at the root and evaluate the locus tests down the tree with its allele values until a terminal node, representing the classified haplogroup, is reached.</p><p>The general decision tree approach has some limitations, including overfitting by creating too many branches and locus bias. The former can be handled by introducing thresholds or other heuristics for the amount of information gain required to create a branch. The latter is a more fundamental problem of the approach; by definition, the information gain favors Y-STRs taking many different allele values. We used the PART and J48 implementations <xref ref-type="bibr" rid="pcbi.1000093-Witten1">[26]</xref> of the decision tree algorithm in order to mitigate the effects of some of these limitations.</p><fig id="pcbi-1000093-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.g007</object-id><label>Figure 7</label><caption><title>Test creation process for decision trees.</title><p>Samples from four haplogroups in data set <italic>A</italic> are passed through locus-specific allele test conditions at each branch of the decision tree. The test for locus <italic>X<sub>i</sub></italic> is chosen so that <italic>i</italic> = arg max<italic><sub>l</sub></italic>{IG(<italic>A</italic>, <italic>X<sub>l</sub></italic>)} and <italic>X<sub>j</sub></italic> so that <italic>j</italic> = arg max<italic>l</italic>{ IG(<italic>B</italic><sub>1</sub>, <italic>X<sub>l</sub></italic>)}.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.g007" xlink:type="simple"/></fig></sec><sec id="s3b"><title>Bayesian Model</title><p>In the non-parametric Bayesian model, we define a posterior distribution over the haplogroups conditioned on observed allele values. The posterior is expressed as the normalized product of the data likelihood and model prior. For a given sample of allele values, the posterior gives a probability for each haplogroup it could belong to. It is defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e023" xlink:type="simple"/><label>(11)</label></disp-formula>where <italic>c</italic> is a normalization constant, and <italic>p</italic>(⋅)is the prior probability over the haplogroups. The likelihood function, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e024" xlink:type="simple"/></inline-formula>, is a measure of how likely it is that haplogroup <italic>g</italic> generated sample <bold>x</bold>.</p><p>The fundamental assumption of our naive Bayes model is the independence of the Y-STRs <bold>X</bold> = (<italic>X</italic><sub>1</sub>,…,<italic>X<sub>L</sub></italic>), given the haplogroup <italic>g</italic>. A number of possible sources of dependency exist that could weaken the validity of this assumption. For example, Y-STRs are located on the same chromosome and physically linked, which introduces co-inheritance and the possibility of statistical linkage over short time scales. However, such statistical relationships are not sufficiently understood to be easily incorporated. Furthermore, attempting to exploit them through direct use of our ground-truth training data is not feasible because the relatively large number of dimensions <xref ref-type="bibr" rid="pcbi.1000093-Sharan1">[15]</xref> would require far more data. In short, the simplifying conditional independence assumption makes using our data tractable. Interestingly, the accuracy of naive Bayes classifiers is not tightly linked to the validity of this assumption <xref ref-type="bibr" rid="pcbi.1000093-Rish1">[27]</xref>,<xref ref-type="bibr" rid="pcbi.1000093-Zhange1">[28]</xref>, which directly affects the accuracy of the posterior computation, but only indirectly affects the ability of the model to distinguish between groups on real data. In practice, the naive Bayes classier often performs well, and thus we chose to empirically study it for haplogroup identification.</p><p>Mathematically, the independence assumption leads to defining the likelihood as a product over each Y-STR density function,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e025" xlink:type="simple"/><label>(12)</label></disp-formula>We estimated the density functions <italic>f<sub>i</sub></italic>(⋅)using histograms constructed from the data. For each Y-STR and haplogroup, we created a normalized histogram from the training data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e026" xlink:type="simple"/></inline-formula> with bins corresponding to the different allele values the Y-STRs can take. For the <italic>i</italic><sup>th</sup> locus under haplogroup <italic>g</italic>, the bins for allele value <italic>x</italic> are given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e027" xlink:type="simple"/><label>(13)</label></disp-formula>As an example, a set of <italic>L</italic> densities for a haplogroup and how they are evaluated for a given sample are shown in <xref ref-type="fig" rid="pcbi-1000093-g008">Figure 8</xref>.</p><fig id="pcbi-1000093-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.g008</object-id><label>Figure 8</label><caption><title>Bayesian likelihood construction and evaluation.</title><p>For each haplogroup, the density functions <italic>f</italic><sub>1</sub>,…<italic>f<sub>L</sub></italic> are constructed as normalized histograms from the training data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e028" xlink:type="simple"/></inline-formula>. Given a sample x = (<italic>x</italic><sub>1</sub>,…<italic>x<sub>L</sub></italic>), its likelihood under a haplogroup is the product of its evaluated locus bin frequencies.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.g008" xlink:type="simple"/></fig><p>The distribution Equation 11 is defined over all haplogroups, but is not by itself a classifier. To make a decision, we choose the maximum under the posterior (MAP)<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e029" xlink:type="simple"/><label>(14)</label></disp-formula>This minimizes our risk of an incorrect classification. A benefit of the generative classifier is the ability to provide alternative classifications and a real probability associated with each decision.</p></sec><sec id="s3c"><title>Support Vector Machines</title><p>Support vector machines learn a hyperplane with maximal margin of separation between two classes of data samples in a feature space <xref ref-type="bibr" rid="pcbi.1000093-Vapnik1">[29]</xref>,<xref ref-type="bibr" rid="pcbi.1000093-Bishop1">[30]</xref>,<xref ref-type="bibr" rid="pcbi.1000093-Hastie1">[31]</xref>. We trained SVMs for binary haplogroup classification by treating locus alleles for a sample as <italic>L</italic>-dimensional vectors in Euclidean space and learning a hyperplane to separate them. A new sample is classified according to which side of the hyperplane its allele values fall on. We first address the case of deciding between two haplogroups to describe the standard support vector machine approach. We then introduce a method to combine binary classifiers into a multi-way classifier for all haplogroups using evolutionary evidence for haplogroup relationships.</p><p>For a sample <bold>x</bold><italic><sub>n</sub></italic> of locus alleles, consider the task of deciding between two haplogroups with labels {−1,1}. If we assume the locus allele values between the two haplogroups are linearly separable in some feature space, we can use the classification model<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e030" xlink:type="simple"/><label>(15)</label></disp-formula>where <italic>y</italic>(<bold>x</bold>) = 0 is a <italic>L</italic>-dimensional hyperplane separating the two haplogroups; <italic>φ</italic>(⋅) is any constant transformation of the allele values into a feature space. Thus, for the <italic>n</italic><sup>th</sup> sample, the haplogroup is <italic>g<sub>n</sub></italic> = 1 when <italic>y</italic>(<bold>x</bold><italic><sub>n</sub></italic>)&gt;0 and <italic>g<sub>n</sub></italic> = −1 when <italic>y</italic>(<bold>x</bold><italic><sub>n</sub></italic>)&lt;0.</p><p>The goal of training a support vector machine is to find the hyperplane, defined by <bold>w</bold>, <italic>b</italic> in Equation 15, giving the maximum margin of separation between the data points in the two haplogroups, <xref ref-type="fig" rid="pcbi-1000093-g009">Figure 9</xref>. The margin is defined as the smallest perpendicular distance between the separating plane and any of the data points in the sample.</p><fig id="pcbi-1000093-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.g009</object-id><label>Figure 9</label><caption><title>Maximum margin hyperplane used in support vector machines.</title><p>Example showing the hyperplane with maximal margin of separation between samples from two different haplogroups. The shaded points lying on the margin define the support vectors.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.g009" xlink:type="simple"/></fig><p>By noting that the distance of a sample <bold>x</bold><italic><sub>n</sub></italic> from the hyperplane is |<italic>y</italic>(<bold>x</bold><italic><sub>n</sub></italic>)|/||<bold>w</bold>||, and that <italic>g<sub>n</sub>y</italic>(<bold>x</bold><italic><sub>n</sub></italic>)&gt;0 for all samples in the training data, then the maximum margin solution is described by the optimization<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e031" xlink:type="simple"/><label>(16)</label></disp-formula>However, solving this optimization problem directly is difficult, so we re-formulate it as follows.</p><p>Without loss of generality, we can rescale <bold>w</bold>, <italic>b</italic> so that the sample(s) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e032" xlink:type="simple"/></inline-formula> with allele values closest to the hyperplane satisfy<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e033" xlink:type="simple"/><label>(17)</label></disp-formula>as in <xref ref-type="fig" rid="pcbi-1000093-g009">Figure 9</xref>. Then the optimization problem Equation 16 reduces to maximizing ||<bold>w</bold>||<sup>−1</sup>, which is equivalently re-formulated for convenience as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e034" xlink:type="simple"/><label>(18)</label></disp-formula>with the constraint that <italic>g<sub>n</sub></italic>(<bold>w</bold><sup>T</sup><italic>φ</italic>(<bold>x</bold><italic><sub>n</sub></italic>)+<italic>b</italic>)≥1, for all 1≤<italic>n</italic>≤<italic>N</italic>. This can be solved as a quadratic programming problem by introducing Lagrange multipliers <italic>a<sub>n</sub></italic>≥0 for each constraint, giving the function<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e035" xlink:type="simple"/><label>(19)</label></disp-formula>By differentiating <italic>L</italic>(⋅)<italic>L</italic>(⋅) with respect to <bold>w</bold> and setting it equal to zero, we see that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e036" xlink:type="simple"/><label>(20)</label></disp-formula>Substituting the above into the classification Equation 15, we obtain<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e037" xlink:type="simple"/><label>(21)</label></disp-formula>where the kernel function is defined as <italic>k</italic>(<bold>x</bold><italic><sub>n</sub></italic>, <bold>x</bold>) = <italic>φ</italic>(<bold>x</bold><italic><sub>n</sub></italic>)<sup>T</sup><italic>φ</italic>(<bold>x</bold>). Therefore, training the model amounts to solving the quadratic programming problem to determine the Lagrange multipliers <bold>a</bold> and the parameter <italic>b</italic>. This is typically done by solving for the dual representation of the problem.</p><p>Transforming the problem into its dual shows that the optimization exhibits the Karush-Kuhn-Tucker conditions that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e038" xlink:type="simple"/><label>(22)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e039" xlink:type="simple"/><label>(23)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e040" xlink:type="simple"/><label>(24)</label></disp-formula>Therefore, every sample in the training set will either have its Lagrange multiplier <italic>a<sub>n</sub></italic> = 0, or <italic>g<sub>n</sub>y</italic>(<bold>x</bold><italic><sub>n</sub></italic>) = 1. The samples whose multiplier is zero have no contribution to the sum in Equation 21, so they do not impact the classification. The samples that have non-zero multipliers are the support vectors and lie on the maximum margin hyperplanes, as in <xref ref-type="fig" rid="pcbi-1000093-g009">Figure 9</xref>; they define the sparse subset of data used to classify new samples.</p><p>A common and effective kernel to use for SVMs is the Gaussian, which has the form<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e041" xlink:type="simple"/><label>(25)</label></disp-formula>We chose to use this kernel and assume the haplogroups are linearly separable in this transformed space over locus allele values.</p><p>In order to make the SVM approach work on data that may not be perfectly separable, we allow for some small amount of the training data to be misclassified. Thus, rather than having infinite error when incorrect (zero error when correct), we allow some of the data points to be classified on the wrong side of the separating hyperplane. To accomplish this, we follow the standard treatment of introducing slack variables that act as a penalty with linearly increasing value for the distance from the wrong side <xref ref-type="bibr" rid="pcbi.1000093-Bishop1">[30]</xref>,<xref ref-type="bibr" rid="pcbi.1000093-Hastie1">[31]</xref>.</p><p>A slack variable <italic>ξ<sub>n</sub></italic>≥0 is defined for each training sample with <italic>ξ<sub>n</sub></italic> = 0 if the sample is on or inside the correct margin boundary and <italic>ξ<sub>n</sub></italic> = |<italic>g<sub>n</sub></italic>−<italic>y</italic>(<bold>x</bold><italic><sub>n</sub></italic>) if it is incorrect. So we now minimize<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.e042" xlink:type="simple"/><label>(26)</label></disp-formula>where <italic>ξ<sub>n</sub></italic> are the slack variables, one for each data point, and <italic>C</italic>&gt;0 weights the significance of the slack variables to the margin in the optimization.</p><p>The optimization process is similar to before, but the Lagrange multipliers are now subject to the constraint 0≤<italic>a<sub>n</sub></italic>≤<italic>C</italic>. As before, the samples whose multipliers are non-zero are the support vectors. However, if <italic>a<sub>n</sub></italic> = <italic>C</italic>, then the sample may lie inside the margin and be either correctly or incorrectly classified, depending on the value of <italic>ξ<sub>n</sub></italic>.</p><p>Since SVMs train a binary classifier and we have multiple haplogroups to distinguish between, we trained an SVM for each haplogroup in a one-vs-many fashion. In general, an SVM trained as one-vs-many for a particular haplogroup uses samples in that haplogroup as positive exemplars and samples in other haplogroups we wish to compare against as negative exemplars.</p><p>We organized the set of binary classifiers into a hierarchy based on the currently known binary haplogroup lineage <xref ref-type="bibr" rid="pcbi.1000093-YCC1">[12]</xref>,<xref ref-type="bibr" rid="pcbi.1000093-Jobling4">[13]</xref>. At each level of the hierarchy, <xref ref-type="fig" rid="pcbi-1000093-g010">Figure 10</xref>, the one-vs-many classifiers are trained using only samples with haplogroups at that level, descendant levels, or ancestors; the samples at other branches are not used. Classification down the tree is accomplished by choosing the SVM result that has a positive classification. When there is more than one positive classification (or all negative), we choose the result with the closest distance to the support vectors. If the haplogroup the sample is best associated with is not a leaf node, it is further evaluated down the tree until a leaf is reached.</p><fig id="pcbi-1000093-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000093.g010</object-id><label>Figure 10</label><caption><title>Y chromosome haplogroup hierarchy.</title><p>Only the top-level haplogroups are shown.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.g010" xlink:type="simple"/></fig></sec><sec id="s3d"><title>Implementation</title><p>In order to create a high throughput classification system, all samples of STR data needing haplogroup prediction are batch selected from a database at regular intervals and classified. Once selected, our tandem classification software predicts the haplogroup for each sample and updates its record in our laboratory information management system (LIMS). Laboratory technicians and data reviewers can then view the results in a web interface (<xref ref-type="supplementary-material" rid="pcbi.1000093.s004">Figure S4</xref>) for the classified batch of samples. The LIMS displays which samples need to be SNP tested for haplogroup verification (based on lack of tandem agreement). Once verified, the tested samples are added to the ground-truth set to improve future classifications.</p><p>The tandem classification software brings together a collection of algorithms implementing naive Bayes, support vector machines, and decision tree classifiers. Where available, we used standard implementations of these algorithms that are open and available to the public.</p><p>For support vector machines, we used the freely available software package libSVM <xref ref-type="bibr" rid="pcbi.1000093-Chang1">[32]</xref>, which is written in C++. We added a customized extension to the library to support multi-class haplogroup prediction as previously described, where the set of trained one-vs-many binary SVM classifiers are organized into a hierarchy that follows <xref ref-type="fig" rid="pcbi-1000093-g010">Figure 10</xref>. In addition to providing training and binary classification algorithms, the SVM library provides tools to efficiently iterate over possible constants and kernel parameters using cross-validation in order to find the best set to use.</p><p>The decision tree classifiers J48 and PART were used as components of the Weka machine learning software suite <xref ref-type="bibr" rid="pcbi.1000093-Witten1">[26]</xref>. The software is written in Java and called from our tandem classification software as an external program.</p></sec></sec><sec id="s4"><title>Supporting Information</title><supplementary-material id="pcbi.1000093.s001" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.s001" xlink:type="simple"><label>Figure S1</label><caption><p>NRY haplogroup SNP tree used to type each sample in our ground-truth training set.</p><p>(0.02 MB EPS)</p></caption></supplementary-material><supplementary-material id="pcbi.1000093.s002" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.s002" xlink:type="simple"><label>Figure S2</label><caption><p>Accuracy of each classifier per haplogroup of predicting the public STR data set using the 9-locus subset of our ground-truth data as training. The lower panel shows the frequency of samples among haplogroups in the public data set.</p><p>(1.06 MB EPS)</p></caption></supplementary-material><supplementary-material id="pcbi.1000093.s003" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.s003" xlink:type="simple"><label>Figure S3</label><caption><p>Tandem classification accuracy and agreement per haplogroup on the public data set using the 9-locus subset of our ground-truth data as training.</p><p>(0.03 MB EPS)</p></caption></supplementary-material><supplementary-material id="pcbi.1000093.s004" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000093.s004" xlink:type="simple"><label>Figure S4</label><caption><p>Screen capture of the STR score review console from the laboratory information management system. The red dashed box contains STR loci names as headings for the columns of collected loci score data. The pop-up window (in black) show the quality assessment score for a selected sample. The yellow dashed ellipse highlights our software's haplogroup classification and confidence value for each algorithm in the tandem approach.</p><p>(0.44 MB EPS)</p></caption></supplementary-material></sec></body><back><ack><p>We would like to thank Fernando Mendez for helping us find large amounts of publicly available STR data. We would also like to thank Saharon Rosset for his insightful comments and suggestions.</p></ack><ref-list><title>References</title><ref id="pcbi.1000093-Jobling1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jobling</surname><given-names>MA</given-names></name><name name-style="western"><surname>Pandya</surname><given-names>A</given-names></name><name name-style="western"><surname>Tyler-Smith</surname><given-names>C</given-names></name></person-group>             <year>1997</year>             <article-title>The Y chromosome in forensic analysis and paternity testing.</article-title>             <source>Int J Legal Med</source>             <volume>110</volume>             <fpage>118</fpage>             <lpage>124</lpage>          </element-citation></ref><ref id="pcbi.1000093-Hammer1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hammer</surname><given-names>MF</given-names></name><name name-style="western"><surname>Chamberlain</surname><given-names>VF</given-names></name><name name-style="western"><surname>Kearney</surname><given-names>VF</given-names></name><name name-style="western"><surname>Stover</surname><given-names>D</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G</given-names></name><etal/></person-group>             <year>2006</year>             <article-title>Population structure of Y chromosome SNP haplogroups in the United States and forensic implications for constructing Y chromosome STR databases.</article-title>             <source>Forensic Sci Int</source>             <volume>164</volume>             <fpage>45</fpage>             <lpage>55</lpage>          </element-citation></ref><ref id="pcbi.1000093-Jobling2"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jobling</surname><given-names>MA</given-names></name><name name-style="western"><surname>Tyler-Smith</surname><given-names>C</given-names></name></person-group>             <year>2000</year>             <article-title>New uses for new haplotypes - the human Y chromosome, disease and selection.</article-title>             <source>Trends Genet</source>             <volume>16</volume>             <fpage>356</fpage>             <lpage>362</lpage>          </element-citation></ref><ref id="pcbi.1000093-Jobling3"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jobling</surname><given-names>MA</given-names></name></person-group>             <year>2001</year>             <article-title>In the name of the father: surnames and genetics.</article-title>             <source>Trends Genet</source>             <volume>17</volume>             <fpage>353</fpage>             <lpage>357</lpage>          </element-citation></ref><ref id="pcbi.1000093-Stone1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stone</surname><given-names>AC</given-names></name><name name-style="western"><surname>Milner</surname><given-names>GR</given-names></name><name name-style="western"><surname>Paabo</surname><given-names>S</given-names></name><name name-style="western"><surname>Stoneking</surname><given-names>M</given-names></name></person-group>             <year>1996</year>             <article-title>Sex determination of ancient human skeletons using DNA.</article-title>             <source>Am J Phys Anthropol</source>             <volume>99</volume>             <fpage>231</fpage>             <lpage>238</lpage>          </element-citation></ref><ref id="pcbi.1000093-Stone2"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stone</surname><given-names>AC</given-names></name><name name-style="western"><surname>Griffiths</surname><given-names>RC</given-names></name><name name-style="western"><surname>Zegura</surname><given-names>SL</given-names></name><name name-style="western"><surname>Hammer</surname><given-names>MF</given-names></name></person-group>             <year>2002</year>             <article-title>High levels of Y-chromosome nucleotide diversity in the genus pan.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <fpage>43</fpage>             <lpage>48</lpage>          </element-citation></ref><ref id="pcbi.1000093-Hammer2"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hammer</surname><given-names>MF</given-names></name><name name-style="western"><surname>Zegura</surname><given-names>SL</given-names></name></person-group>             <year>1996</year>             <article-title>The role of the Y chromosome in human evolutionary studies.</article-title>             <source>Evol Anthropol: Issues, News, and Reviews</source>             <volume>5</volume>             <fpage>116</fpage>             <lpage>134</lpage>          </element-citation></ref><ref id="pcbi.1000093-Underhill1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Underhill</surname><given-names>PA</given-names></name><name name-style="western"><surname>Shen</surname><given-names>P</given-names></name><name name-style="western"><surname>Lin</surname><given-names>AA</given-names></name><name name-style="western"><surname>Jin</surname><given-names>L</given-names></name><etal/></person-group>             <year>2000</year>             <article-title>Y chromosome sequence variation and the history of human populations.</article-title>             <source>Nat Genet</source>             <volume>25</volume>             <fpage>358</fpage>             <lpage>361</lpage>          </element-citation></ref><ref id="pcbi.1000093-Hammer3"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hammer</surname><given-names>MF</given-names></name><name name-style="western"><surname>Karafet</surname><given-names>TM</given-names></name><name name-style="western"><surname>Redd</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Jarjanazi</surname><given-names>H</given-names></name><etal/></person-group>             <year>2001</year>             <article-title>Hierarchical patterns of global human Y-chromosome diversity.</article-title>             <source>Mol Biol Evol</source>             <volume>18</volume>             <fpage>1189</fpage>             <lpage>1203</lpage>          </element-citation></ref><ref id="pcbi.1000093-Heyer1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Heyer</surname><given-names>E</given-names></name><name name-style="western"><surname>Puymirat</surname><given-names>J</given-names></name><name name-style="western"><surname>Dieltjes</surname><given-names>P</given-names></name><name name-style="western"><surname>Bakker</surname><given-names>E</given-names></name><name name-style="western"><surname>de Knijff</surname><given-names>P</given-names></name></person-group>             <year>1997</year>             <article-title>Estimating Y chromosome specific microsatellite mutation frequencies using deep rooting pedigrees.</article-title>             <source>Hum Mol Genet</source>             <volume>6</volume>             <fpage>799</fpage>             <lpage>803</lpage>          </element-citation></ref><ref id="pcbi.1000093-Kayser1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kayser</surname><given-names>M</given-names></name><name name-style="western"><surname>Roewer</surname><given-names>L</given-names></name><name name-style="western"><surname>Hedman</surname><given-names>M</given-names></name><name name-style="western"><surname>Henke</surname><given-names>L</given-names></name><etal/></person-group>             <year>2000</year>             <article-title>Characteristics and frequency of germline mutations at microsatellite loci from the human Y chromosome, as revealed by direct observation in father/son pairs.</article-title>             <source>Am J Hum Genett</source>             <volume>66</volume>             <fpage>1580</fpage>             <lpage>1588</lpage>          </element-citation></ref><ref id="pcbi.1000093-YCC1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <collab xlink:type="simple">YCC</collab>             <year>2002</year>             <article-title>A nomenclature system for the tree of human y-chromosomal binary haplogroups.</article-title>             <source>Genome Res</source>             <volume>12</volume>             <fpage>339</fpage>             <lpage>348</lpage>          </element-citation></ref><ref id="pcbi.1000093-Jobling4"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jobling</surname><given-names>MA</given-names></name><name name-style="western"><surname>Tyler-Smith</surname><given-names>C</given-names></name></person-group>             <year>2003</year>             <article-title>The human Y chromosome: an evolutionary marker comes of age.</article-title>             <source>Nat Rev Genet</source>             <volume>4</volume>             <fpage>598</fpage>             <lpage>612</lpage>          </element-citation></ref><ref id="pcbi.1000093-Underhill2"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Underhill</surname><given-names>PA</given-names></name><name name-style="western"><surname>Passarino</surname><given-names>G</given-names></name><name name-style="western"><surname>Lin</surname><given-names>AA</given-names></name><name name-style="western"><surname>Shen</surname><given-names>P</given-names></name><etal/></person-group>             <year>2001</year>             <article-title>The phylogeography of Y chromosome binary haplotypes and the origins of modern human populations.</article-title>             <source>Ann Hum Genet</source>             <volume>65</volume>             <fpage>43</fpage>             <lpage>62</lpage>          </element-citation></ref><ref id="pcbi.1000093-Sharan1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sharan</surname><given-names>R</given-names></name><name name-style="western"><surname>Gramm</surname><given-names>J</given-names></name><name name-style="western"><surname>Yakhini</surname><given-names>Z</given-names></name><name name-style="western"><surname>Ben-Dor</surname><given-names>A</given-names></name></person-group>             <year>2005</year>             <article-title>Multiplexing schemes for generic SNP genotyping assays.</article-title>             <source>J Comput Biol</source>             <volume>12</volume>             <fpage>514</fpage>             <lpage>533</lpage>          </element-citation></ref><ref id="pcbi.1000093-Bosch1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bosch</surname><given-names>E</given-names></name><name name-style="western"><surname>Calafell</surname><given-names>F</given-names></name><name name-style="western"><surname>Santos</surname><given-names>F</given-names></name><name name-style="western"><surname>Perez-Lezaun</surname><given-names>A</given-names></name><etal/></person-group>             <year>1999</year>             <article-title>Variation in short tandem repeats is deeply structured by genetic background on the human Y chromosome.</article-title>             <source>Am J Hum Genet</source>             <volume>65</volume>             <fpage>1623</fpage>             <lpage>1638</lpage>          </element-citation></ref><ref id="pcbi.1000093-Behar1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Behar</surname><given-names>DM</given-names></name><name name-style="western"><surname>Garrigan</surname><given-names>D</given-names></name><name name-style="western"><surname>Kaplan</surname><given-names>ME</given-names></name><name name-style="western"><surname>Mobasher</surname><given-names>Z</given-names></name><etal/></person-group>             <year>2004</year>             <article-title>Contrasting patterns of Y chromosome variation in Ashkenazi Jewish and host non-Jewish European populations.</article-title>             <source>Hum Genet</source>             <volume>114</volume>             <fpage>354</fpage>             <lpage>365</lpage>          </element-citation></ref><ref id="pcbi.1000093-Dietterich1"><label>18</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dietterich</surname><given-names>TG</given-names></name></person-group>             <year>2000</year>             <article-title>Ensemble methods in machine learning.</article-title>             <source>Proc of the First International Workshop on Multiple Classifier Systems. volume 1857</source>             <fpage>1</fpage>             <lpage>15</lpage>          </element-citation></ref><ref id="pcbi.1000093-Ohta1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ohta</surname><given-names>T</given-names></name><name name-style="western"><surname>Kimura</surname><given-names>M</given-names></name></person-group>             <year>1973</year>             <article-title>The model of mutation appropriate to estimate the number of electrophoretically detectable alleles in a genetic population.</article-title>             <source>Genet Res</source>             <volume>22</volume>             <fpage>201</fpage>             <lpage>204</lpage>          </element-citation></ref><ref id="pcbi.1000093-Sengupta1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sengupta</surname><given-names>S</given-names></name><name name-style="western"><surname>Zhivotovsky</surname><given-names>LA</given-names></name><name name-style="western"><surname>King</surname><given-names>R</given-names></name><name name-style="western"><surname>Mehdi</surname><given-names>SQ</given-names></name><etal/></person-group>             <year>2006</year>             <article-title>Polarity and temporality of high-resolution y-chromosome distributions in India identify both indigenous and exogenous expansions and reveal minor genetic influence of central asian pastoralists.</article-title>             <source>Am J Hum Genet</source>             <volume>78</volume>             <fpage>202</fpage>             <lpage>221</lpage>          </element-citation></ref><ref id="pcbi.1000093-Cinnioglu1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cinnioglu</surname><given-names>C</given-names></name><name name-style="western"><surname>King</surname><given-names>R</given-names></name><name name-style="western"><surname>Kivisild</surname><given-names>T</given-names></name><name name-style="western"><surname>Kalfoglu</surname><given-names>E</given-names></name><etal/></person-group>             <year>2004</year>             <article-title>Excavating y-chromosome haplotype strata in Anatolia.</article-title>             <source>Hum Genet</source>             <volume>114</volume>             <fpage>127</fpage>             <lpage>148</lpage>          </element-citation></ref><ref id="pcbi.1000093-Butler1"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Butler</surname><given-names>JM</given-names></name><name name-style="western"><surname>Schoske</surname><given-names>R</given-names></name><name name-style="western"><surname>Vallone</surname><given-names>PM</given-names></name><name name-style="western"><surname>Kline</surname><given-names>MC</given-names></name><name name-style="western"><surname>Redd</surname><given-names>AJ</given-names></name><etal/></person-group>             <year>2002</year>             <article-title>A novel multiplex for simultaneous amplification of 20 Y chromosome str markers.</article-title>             <source>Forensic Sci Int</source>             <volume>129</volume>             <fpage>10</fpage>             <lpage>24</lpage>          </element-citation></ref><ref id="pcbi.1000093-Bell1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bell</surname><given-names>PA</given-names></name><name name-style="western"><surname>Chaturvedi</surname><given-names>S</given-names></name><name name-style="western"><surname>Gelfand</surname><given-names>CA</given-names></name><name name-style="western"><surname>Huang</surname><given-names>CY</given-names></name><name name-style="western"><surname>Kochersperger</surname><given-names>M</given-names></name><etal/></person-group>             <year>2002</year>             <article-title>SNPstream UHT: ultra-high throughput SNP genotyping for pharmacogenomics and drug discovery.</article-title>             <source>BioTechniques</source>             <volume>34</volume>             <fpage>496</fpage>          </element-citation></ref><ref id="pcbi.1000093-Quinlan1"><label>24</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Quinlan</surname><given-names>JR</given-names></name></person-group>             <year>1993</year>             <source>C4.5: Programs for Machine Learning</source>             <publisher-loc>San Francisco</publisher-loc>             <publisher-name>Morgan Kaufmann</publisher-name>          </element-citation></ref><ref id="pcbi.1000093-Shannon1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shannon</surname><given-names>CE</given-names></name></person-group>             <year>1948</year>             <article-title>A mathematical theory of communication.</article-title>             <source>Bell System Technical Journal</source>             <volume>27</volume>             <fpage>379</fpage>             <lpage>423</lpage>             <page-range>623–656</page-range>          </element-citation></ref><ref id="pcbi.1000093-Witten1"><label>26</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Witten</surname><given-names>IH</given-names></name><name name-style="western"><surname>Frank</surname><given-names>E</given-names></name></person-group>             <year>2005</year>             <source>Data mining: practical machine learning tools and techniques</source>             <publisher-loc>San Francisco</publisher-loc>             <publisher-name>Morgan Kaufmann, second edition</publisher-name>          </element-citation></ref><ref id="pcbi.1000093-Rish1"><label>27</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rish</surname><given-names>I</given-names></name></person-group>             <year>2001</year>             <article-title>An empirical study of the naive Bayes classifier.</article-title>             <source>IJCAI 2001 Workshop on Empirical Methods in Artificial Intelligence</source>          </element-citation></ref><ref id="pcbi.1000093-Zhange1"><label>28</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zhange</surname><given-names>H</given-names></name></person-group>             <year>2004</year>             <article-title>The optimality of naive Bayes.</article-title>             <publisher-name>Proc of the 17th International FLAIRS conference</publisher-name>          </element-citation></ref><ref id="pcbi.1000093-Vapnik1"><label>29</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vapnik</surname><given-names>VN</given-names></name></person-group>             <year>1998</year>             <source>Statistical Learning Theory</source>             <publisher-name>Wiley and Sons Inc</publisher-name>          </element-citation></ref><ref id="pcbi.1000093-Bishop1"><label>30</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bishop</surname><given-names>CM</given-names></name></person-group>             <year>2006</year>             <source>Pattern recognition and machine learning</source>             <publisher-name>Springer</publisher-name>          </element-citation></ref><ref id="pcbi.1000093-Hastie1"><label>31</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name><name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name><name name-style="western"><surname>Friedman</surname><given-names>J</given-names></name></person-group>             <year>2006</year>             <source>The elements of statistical learning</source>             <publisher-name>Springer</publisher-name>          </element-citation></ref><ref id="pcbi.1000093-Chang1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>CC</given-names></name><name name-style="western"><surname>Lin</surname><given-names>CJ</given-names></name></person-group>             <year>2001</year>             <article-title>LIBSVM: a library for support vector machines.</article-title>             <comment>Software available at <ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm" xlink:type="simple">http://www.csie.ntu.edu.tw/̃cjlin/libsvm</ext-link></comment>          </element-citation></ref></ref-list></back></article>