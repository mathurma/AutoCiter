<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004155</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-01023</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A Neural Mechanism for Background Information-Gated Learning Based on Axonal-Dendritic Overlaps</article-title>
<alt-title alt-title-type="running-head">Much ADO about BIG Learning</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Mainetti</surname>
<given-names>Matteo</given-names>
</name>
<xref rid="aff001" ref-type="aff"/>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Ascoli</surname>
<given-names>Giorgio A.</given-names>
</name>
<xref rid="cor001" ref-type="corresp">*</xref>
<xref rid="aff001" ref-type="aff"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Krasnow Institute for Advanced Study, George Mason University, Fairfax, Virginia, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Hilgetag</surname>
<given-names>Claus C.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Hamburg University, GERMANY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: MM GAA. Performed the experiments: MM. Analyzed the data: MM GAA. Contributed reagents/materials/analysis tools: GAA. Wrote the paper: MM GAA.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">ascoli@gmu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>13</day>
<month>3</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<month>3</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>3</issue>
<elocation-id>e1004155</elocation-id>
<history>
<date date-type="received">
<day>10</day>
<month>6</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>26</day>
<month>1</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Mainetti, Ascoli</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004155" xlink:type="simple"/>
<abstract>
<p>Experiencing certain events triggers the acquisition of new memories. Although necessary, however, actual experience is not sufficient for memory formation. One-trial learning is also gated by knowledge of appropriate background information to make sense of the experienced occurrence. Strong neurobiological evidence suggests that long-term memory storage involves formation of new synapses. On the short time scale, this form of structural plasticity requires that the axon of the pre-synaptic neuron be physically proximal to the dendrite of the post-synaptic neuron. We surmise that such “axonal-dendritic overlap” (ADO) constitutes the neural correlate of background information-gated (BIG) learning. The hypothesis is based on a fundamental neuroanatomical constraint: an axon must pass close to the dendrites that are near other neurons it contacts. The topographic organization of the mammalian cortex ensures that nearby neurons encode related information. Using neural network simulations, we demonstrate that ADO is a suitable mechanism for BIG learning. We model knowledge as associations between terms, concepts or indivisible units of thought via directed graphs. The simplest instantiation encodes each concept by single neurons. Results are then generalized to cell assemblies. The proposed mechanism results in learning real associations better than spurious co-occurrences, providing definitive cognitive advantages.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>We introduce and evaluate a new biologically-motivated learning rule for neural networks. The proposed mechanism explains why it is easier to acquire knowledge when it relates to known background information than when it is completely novel. We posit that this “background information-gated” (BIG) learning emerges from the necessity of neuronal axons and dendrites to be adjacent to each other in order to establish new synapses. Such basic geometric requirement, which was explicitly recognized in Donald Hebb’s original formulation of synaptic plasticity, is not usually accounted for in neural network learning rules. More generally, the level of abstraction of current computational models is insufficient to capture the details of axonal and dendritic shape. Here we show that “axonal-dendritic overlap” (ADO) can be parsimoniously related to connectivity by assuming optimal neuronal placement to minimize axonal wiring. Incorporating this new relationship into classic connectionist learning algorithms, we show that networks trained in a given domain more easily acquire further knowledge in the same domain than in others. Surprisingly, the morphologically-motivated constraint on structural plasticity also endows neural nets with the powerful computational ability to discriminate real associations of events, like the sight of a lightning and the sound of the thunder, from spurious co-occurrences, such as between the thunder and the beetle that flew by during the storm. Thus, the selectivity of synaptic formation implied by the ADO requirement is shown to provide a fundamental cognitive advantage over classic artificial neural networks.</p>
</abstract>
<funding-group>
<funding-statement>This work was supported in part by NIH (<ext-link ext-link-type="uri" xlink:href="http://www.nih.gov" xlink:type="simple">www.nih.gov</ext-link>) grant R01 NS39600, Office of Naval Research (<ext-link ext-link-type="uri" xlink:href="http://www.onr.navy.mil" xlink:type="simple">www.onr.navy.mil</ext-link>) grant MURI N00014-10-1-0198, and NSF (<ext-link ext-link-type="uri" xlink:href="http://www.nsf.org" xlink:type="simple">www.nsf.org</ext-link>) grant RI IIS-1302256. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="4"/>
<table-count count="0"/>
<page-count count="17"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>Code can be downloaded freely from <ext-link ext-link-type="uri" xlink:href="http://krasnow1.gmu.edu/cn3/BigAdoAllCode.zip" xlink:type="simple">http://krasnow1.gmu.edu/cn3/BigAdoAllCode.zip</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Reading about a newly discovered insect species, an entomologist can rapidly learn various details of their development, communication, and mating. Studying the same material, it is much harder for someone with different expertise to learn the same facts. While it is commonsense that new information is easier to memorize if it relates to prior knowledge, the cognitive and neural mechanisms underlying this familiar phenomenon are not established. More specifically, one-trial learning of “neutral” events, as opposed to emotionally charged or surprising experiences [<xref rid="pcbi.1004155.ref001" ref-type="bibr">1</xref>], is gated by knowledge of appropriate background information to make sense of the experienced occurrence [<xref rid="pcbi.1004155.ref002" ref-type="bibr">2</xref>, <xref rid="pcbi.1004155.ref003" ref-type="bibr">3</xref>]. Consider experiencing for the first time the co-occurrence of a buzzing sound with the sight of a beetle (<xref rid="pcbi.1004155.g001" ref-type="fig">Fig. 1A</xref>). Learning that “beetles can buzz” may depend on background information that renders the “buzzing beetle” association sensible. Prior knowledge might include that wasps, flies, and bees also buzz. Such facts are <italic>relevant</italic> because they involve <italic>related concepts</italic>: these insects share several common associations with beetles (e.g. small size, crawling, flying, erratic trajectories). The remainder of this paper refers to this cognitive phenomenon as “background information gating” or BIG learning.</p>
<fig id="pcbi.1004155.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004155.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Instantiation of background information-gated (BIG) learning by the neuroanatomical mechanism of axonal-dendrite overlap (ADO).</title>
<p><bold>A</bold>. Cognitive model: Previously acquired background information, reflected in the structure of the association network, provides a gating mechanism for the formation of novel associations. The ability to acquire the new piece of information (associating the buzz to the beetle) depends on prior knowledge of relevant facts: in this example, that other buzzing animals (e.g. wasps) fly erratically. The green fonts <italic>a</italic>, <italic>b</italic>, <italic>c</italic>, and <italic>d</italic> refer to the <italic>proximity</italic> formula (also in green), fully described in the Materials and Methods. <bold>B</bold>. Neural correlate: In this simplified (“grandmother” cells) model, each concept of panel A is represented by a single neuron, with axonal and dendritic trees drawn respectively in red and blue. The axon of the “Buzzing” neuron has a synaptic contact with the dendrite of the “Wasp” neuron. Thus, it must pass close to the dendrites of other nearby neurons. Neurons are likely to be near each other if they receive synapses from the same axons. Here, “Beetle” is near “Wasp” as they both receive synapses from the axon of the “Erratic Flight” neuron. Thus, prior knowledge of relevant background information, instantiated by the three existing synapses, provides proper conditions to learn the new association, i.e. forming the “Buzzing”-“Beetle” synapse.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004155.g001" position="float" xlink:type="simple"/>
</fig>
<p>Mounting neurobiological evidence implicates formation of new synapses in long-term memory storage [<xref rid="pcbi.1004155.ref004" ref-type="bibr">4</xref>, <xref rid="pcbi.1004155.ref005" ref-type="bibr">5</xref>, <xref rid="pcbi.1004155.ref006" ref-type="bibr">6</xref>]. Building on those ideas, we propose a possible neuroanatomical correlate of BIG learning. The hypothesized mechanism is initially best illustrated under the over-simplifying assumption that associations are stored by connecting “grandmother” neurons, each corresponding to individual concepts (<xref rid="pcbi.1004155.g001" ref-type="fig">Fig. 1B</xref>). The computational simulations presented in this work, however, demonstrate that this same concept also seamlessly works with distributed neuronal representations.</p>
<p>In order to establish a synapse, according to Hebbian theory, the axon and dendrites of the two co-activated neurons must be juxtaposed [<xref rid="pcbi.1004155.ref007" ref-type="bibr">7</xref>]. We henceforth refer to this “potential synapse” configuration [<xref rid="pcbi.1004155.ref008" ref-type="bibr">8</xref>] as <italic>axonal-dendritic overlap</italic> or ADO. Intuitively, the reason the axon passes near the dendrite is because it is connected to other dendrites in that vicinity. Why then is the potential post-synaptic dendrite close to other dendrites contacted by the potential pre-synaptic axon? Wiring cost considerations suggest that neurons should be placed nearby if they receive synapses from the same axons [<xref rid="pcbi.1004155.ref009" ref-type="bibr">9</xref>]. If knowledge representation is stored in pairwise neural connections [<xref rid="pcbi.1004155.ref010" ref-type="bibr">10</xref>], this particular topology should correspond to relevant background information. Here we formulate this notion quantitatively with a new neural network learning rule, demonstrating by construction that ADO is a suitable mechanism for BIG learning.</p>
<p>In our model, neural activation reflects associations sampled from various graphs taken as a simplified representation of everyday experience. Specifically, every instant of experience is represented as a subset of co-occurring elementary observables, each corresponding to a node of a “reality graph,” in which edges denote probability of co-occurrence (see <xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 1.1 for a more extended description). We study networks pre-trained with an initial connectivity by comparing their ability to learn new information that is related or unrelated to prior knowledge. Such pre-existing background information may derive from repetition learning [<xref rid="pcbi.1004155.ref011" ref-type="bibr">11</xref>] or from experience earlier in life: if the BIG ADO were enforced from the start in a fully disconnected network, no new synapses could ever form. The simplest instantiation encodes each concept by single neurons; results are then shown to generalize robustly to realistic cell assemblies. Noticeably, the proposed mechanism results in learning real associations better than spurious co-occurrences, providing definitive cognitive advantages.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and Methods</title>
<p>The original simulation software used in this work was written in R, and the source code is freely available at <ext-link ext-link-type="uri" xlink:href="http://krasnow1.gmu.edu/cn3/BigAdoAllCode.zip" xlink:type="simple">http://krasnow1.gmu.edu/cn3/BigAdoAllCode.zip</ext-link>. Here we explain the research design pertaining to the findings reported in the main text. The detailed methodologies are more thoroughly described in <xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 2.1–2.4.</p>
<sec id="sec003">
<title>Neural Network Model and the BIG ADO Learning Rule</title>
<p>This work assumes the classic model of neural networks as directed graphs in which nodes represent neurons and each directional edge represents a connection between the axon of the pre-synaptic neuron and the dendrite of the post-synaptic neuron. The network only contains excitatory neurons. In this model, formation of new binary connections (a form of <italic>structural plasticity</italic>) underlies associative learning, and knowledge is encoded by the connectivity of the network [<xref rid="pcbi.1004155.ref010" ref-type="bibr">10</xref>].</p>
<p>Activity-dependent plasticity is traditionally framed in terms of the Hebbian rule: “When an axon of cell <italic>a</italic> is near enough to excite cell <italic>b</italic> and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that <italic>a</italic>’s efficiency, as one of the cells firing <italic>b</italic>, is increased” [<xref rid="pcbi.1004155.ref007" ref-type="bibr">7</xref>]. Many variants of Hebbian synaptic modification exist [<xref rid="pcbi.1004155.ref012" ref-type="bibr">12</xref>], often summarized as ‘<italic>neurons that fire together wire together</italic>’. This popular quip, however, misses the essential requirement, clearly stressed in Hebb’s original formulation, that the axon of the pre-synaptic neuron must be sufficiently close to its post-synaptic target for plasticity to take place.</p>
<p>The learning rule introduced in this work implements a form of structural plasticity in neural networks that incorporates the constraint of proximity between pre- and post-synaptic partners or axonal-dendritic overlap (ADO): if two neurons <italic>a</italic> and <italic>b</italic> fire together, a connection from <italic>a</italic> to <italic>b</italic> is only formed if the axon of <italic>a</italic> comes within a threshold distance from a dendrite of <italic>b</italic>. In mathematical terms, this condition can be defined as a non-symmetric real-valued function between neurons corresponding to the distance from the axon of the candidate pre-synaptic neuron to the dendrite of the post-synaptic neuron.</p>
<p>Now we introduce an approximation to express the axonal-dendritic overlap between neurons in terms of the connectivity of the rest of the network on the basis of two assumptions. The first assumption is that the axon of <italic>a</italic> passes near the dendrite of neuron <italic>b</italic> because it connects to another neuron <italic>c</italic> that is near neuron <italic>b</italic>. This assumption corresponds to a principle of parsimony in the use of axonal wiring: since the goal of axons is to carry signals to other neurons, the locations of axonal branches are part of trajectories towards synaptic contacts. The second assumption is that if neurons <italic>b</italic> and <italic>c</italic> are near each other, it is because they are both contacted by the same set of axons, which we generically call <italic>d</italic> (<xref rid="pcbi.1004155.g001" ref-type="fig">Fig. 1</xref>). This assumption presumes optimal neuronal placement once again to minimize axonal wiring, consistent with the existence of topographic maps e.g. in the mammalian cortex [<xref rid="pcbi.1004155.ref013" ref-type="bibr">13</xref>], but also in invertebrate nervous systems [<xref rid="pcbi.1004155.ref014" ref-type="bibr">14</xref>].</p>
<p>These two assumptions can be combined into the assertion that the tendency of the axon of neuron <italic>a</italic> to overlap with a dendrite of neuron <italic>b</italic> increases with the number of neurons <italic>c</italic> and <italic>d</italic> such that <italic>a</italic> is connected to <italic>c</italic> and <italic>d</italic> is connected to both <italic>b</italic> and <italic>c</italic>. This idea is quantified by the following <italic>proximity</italic> (<italic>π</italic>) function:
<disp-formula id="pcbi.1004155.e001">
<alternatives>
<graphic id="pcbi.1004155.e001g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004155.e001" xlink:type="simple"/>
<mml:math display="block" id="M1" overflow="scroll">
<mml:mi>π</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mi>×</mml:mi><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mi>×</mml:mi><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
where <italic>ω</italic><sub><italic>a</italic>,<italic>c</italic></sub> equals 1 if and only if the axon of <italic>a</italic> connects to the dendrite of <italic>c</italic>, and 0 otherwise (likewise for <italic>ω</italic><sub><italic>d</italic>,<italic>c</italic></sub> and <italic>ω</italic><sub><italic>d</italic>,<italic>b</italic></sub>), and the indices <italic>c</italic> and <italic>d</italic> run over all neurons in the network (see also <xref rid="pcbi.1004155.g001" ref-type="fig">Fig. 1A</xref>). The above formula can be elegantly expressed as the product of three matrices:
<disp-formula id="pcbi.1004155.e002">
<alternatives>
<graphic id="pcbi.1004155.e002g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004155.e002" xlink:type="simple"/>
<mml:math display="block" id="M2" overflow="scroll"><mml:mrow><mml:mo>∏</mml:mo><mml:mo>=</mml:mo><mml:mi>Ω</mml:mi><mml:mi>Ω</mml:mi><mml:mo>×</mml:mo><mml:mi>Ω</mml:mi><mml:msup><mml:mi>Ω</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>×</mml:mo><mml:mi>Ω</mml:mi><mml:mi>Ω</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic>Ω</italic> = {<italic>ω</italic><sub><italic>m</italic>,<italic>n</italic></sub>} is the (binary) network connectivity (also called adjacency matrix), with the number of rows and columns equal to the number of neurons in the network, and each row and column representing a neuron’s pre- and post-synaptic contacts, respectively, with all other neurons; <italic>Ω</italic><sup><italic>t</italic></sup> is the transpose matrix in which every row is substituted with the corresponding column and vice versa (this operation is equivalent to switching axons and dendrites for each neuron); and <italic>Π</italic> = {<italic>π (m</italic>,<italic>n</italic>)} is the proximity matrix, which (like <italic>Ω</italic>) is square and non-symmetric.</p>
<p>The results presented in the main text are obtained by choosing a value for the proximity threshold <italic>θ</italic> in order to discriminate between proximal and distant pairs of neurons: <italic>a</italic> is deemed proximal to <italic>b</italic>, that is there is a potential synapse between <italic>a</italic> and <italic>b</italic>, whenever <italic>π</italic> (a,b) &gt; <italic>θ</italic>. The proximity threshold is one of several parameters that have to be fixed when running simulations of an actual system; robustness of the mechanism is discussed in <xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 3.2. As an alternative to such a discontinuous threshold, we also implemented a probabilistic criterion for relating potential connectivity to proximity. In this case, the probability of <italic>a</italic> and <italic>b</italic> being proximal was not a binary function of proximity but it instead followed a sigmoid curve. This probabilistic variant, while introducing an additional source of noise in the simulations, yielded results (also described in <xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 3.2) that confirmed the main results of this work. However, this more general approach also increases the complexity of the model, by requiring the specification of an additional parameter to define the slope of the sigmoid.</p>
<p>Note, in a similar vein, that the above proximity formula seamlessly extends to non-binary connectivity matrices. For instance, network connectivity could be expressed as a matrix <italic>Ω</italic> recording not just the existence of a connection between two neurons, but the number of their physical contacts or other relevant measures, such as the stability of the synapses [<xref rid="pcbi.1004155.ref015" ref-type="bibr">15</xref>]. In the simple formulation used in this work, which presumes optimal neuronal placement to minimize axonal wiring, high proximity values make axonal-dendritic overlap likely, but not absolutely warranted.</p>
<p>The learning rule described above relates closely to earlier works proposing similar learning mechanisms to explain generalization and grammatical rule extraction. Most strikingly, a learning procedure with a very similar structure was described [<xref rid="pcbi.1004155.ref016" ref-type="bibr">16</xref>] to explain a generalization of a novel sequence (b-d) based on experienced sequences (a-c), (a-d), and (b-c). Despite this similarity (which we discovered during peer-review), the formulation introduced in the current work was derived independently, starting from the interpretation in terms of axonal-dendritic overlaps and structural plasticity. More generally, circuit connectivity, synaptic plasticity, and neuronal placement are interrelated in a broad class of other common neural network approaches, including Kohonen-type self-organizing maps [<xref rid="pcbi.1004155.ref017" ref-type="bibr">17</xref>]. In our model, the ADO constraint on structural plasticity is reduced to simple topological proximity rather than physical distance between neurons. Moreover, the application to background information-gated learning, the neural network implementation, and the analyses presented here are all novel.</p>
<p>To explain why axonal-dendritic overlap (and the approximation captured by the above proximity formula) constitutes the neural correlate of background information gating (BIG), we revert to the (admittedly simplistic) “grandmother cell” interpretation in which each individual neuron represents a corresponding observable (<xref rid="pcbi.1004155.g001" ref-type="fig">Fig. 1B</xref>). With such a one-to-one mapping in place, existing synapses reflect learned associations between previously co-occurred observables (solid arrows in <xref rid="pcbi.1004155.g001" ref-type="fig">Fig. 1A</xref>), altogether constituting already acquired knowledge. When witnessing a new co-occurrence between the two observables <italic>a</italic> and <italic>b</italic>, the association of their internal representations will only be allowed if consistent with prior relevant knowledge, ultimately corresponding to background information.</p>
</sec>
<sec id="sec004">
<title>Pre-Training and Testing Design</title>
<p>This work investigates the computational characteristics of the BIG ADO learning rule starting from well-defined reality-generating graphs (described in the next sub-section of these <xref rid="sec002" ref-type="sec">Materials and Methods</xref>). In the general simulation design, the network of the agent’s internal representation is created by copying the set of nodes from the reality-generating graph, but connecting them by sampling only a subset of edges. This process produces a network effectively encoding a certain amount of knowledge of reality consistent with prior experience. The same result would be obtained by “pre-training” a(n initially) fully disconnected network with the common “firing together, wiring together” rule (without BIG ADO filter) and sequentially activating pairs of neurons corresponding to the sampled subset of the reality-generating graph.</p>
<p>This design models the agent’s representation of background information related to previously experienced aspects of reality. Such a set-up allows investigation of the effect of the BIG ADO filter on subsequent learning. In the testing phase, further experience is sampled from not-yet learned edges of the reality-generating graph. These can be chosen so as to represent co-occurrences of observables more or less closely related to the pre-trained knowledge (mimicking expert or novice agents, respectively). Specifically, when initially connecting the neural network, we select the pre-training subset of edges non-uniformly from the reality-generating graph, such that distinct groups of nodes are differentially represented. For example, if the neural network is pre-trained with 50% of the edges from the reality-generating graph, three quarters of these edges can be sampled from half of the nodes, and one quarter of the edges from the other half. The resulting neural network is an “expert” on half of the reality-generating graph (because it knows a majority of the corresponding structure), and a “novice” on the other half (where it only knows a minority of the structure). In the “learning test” phase, the network is presented with new edges selected either from within the domain of expertise (that is, from the one quarter of edges not used in pre-training) or from the outside (from the three quarters of unused edges in the other half of nodes). The network learns new edges only if the proximity of the corresponding nodes is above threshold.</p>
<p>Moreover, two (or more) edges of the reality-generating graph can be presented at once (e.g. x-y and w-z) to allow measurement of differential learning between the “real” and “spurious” associations. The former types reflect actual edges in the reality-generating graph (i.e. x-y and w-z), while the latter correspond to “random” co-occurrences (x-w, x-z, y-w, and y-z).</p>
<p>The requirement of axonal-dendritic overlap for the formation of new connections is implemented by ways of the proximity function, which itself depends on pre-acquired connectivity. Thus, if the BIG ADO filter were in place from the beginning, no synapses would ever form in the network. The above pre-training design, which circumvents this impasse, can be justified by a two-stage developmental model [<xref rid="pcbi.1004155.ref018" ref-type="bibr">18</xref>]. Early in development, neurons are still optimizing their placements, and axonal branches undergo frequent rearrangements; in the subsequent mature stage, experience-dependent synapse formation and pruning are still common, but neuronal wiring is much more stable. Nevertheless, the “pre-training” model adopted here is also consistent with non-developmental scenarios. Even in adulthood, growth processes can be triggered by continuous repetition or by neuromodulation reflecting emotionally salience (e.g. shock, pleasure, etc.). These conditions can explain the acquisition of prior knowledge (background information). The BIG ADO filter, in contrast, constitutes a neuroanatomically-inspired model of one-trial, emotionally neutral learning.</p>
</sec>
<sec id="sec005">
<title>Word Association Graph</title>
<p>The dataset of word associations used in the first test of the BIG ADO learning rule (<xref rid="pcbi.1004155.g002" ref-type="fig">Fig. 2A-B</xref>) was derived from a compilation of noun/adjective pairings in Wikipedia. In its original form, it consisted of 32 million adjective-modified nouns (<ext-link ext-link-type="uri" xlink:href="http://wiki.ims.uni-stuttgart.de/extern/WordGraph" xlink:type="simple">http://wiki.ims.uni-stuttgart.de/extern/WordGraph</ext-link>). After identifying nouns corresponding to animals and household objects, we skimmed infrequent adjectives and removed ambiguous terms (see <xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 2.1 for exact protocol). The resulting bipartite graph consisted of 50 animal nouns, 50 household object nouns, 285 adjectives and 2,682 edges (1,324 for animals and 1,358 for objects). Next, two networks were pre-trained by connecting half of the noun-adjective pairs from the graph. One of the networks associated more edges pertaining to animal nodes (becoming an animal expert and object novice), while the other associated more edges pertaining to object nodes (object expert, animal novice). Moreover, the amount of specialization was also varied to mimic different levels of specialization. This was achieved by varying the ratio between animals and objects learned in pre-training. Learning was then tested on the other half of the noun-adjective pairs using the BIG ADO rule with a proximity threshold (<italic>θ</italic> in <xref rid="pcbi.1004155.e001" ref-type="disp-formula">equation 1</xref>) of 6. In the random equivalent graphs, edges between 100 “noun” nodes and 285 “adjective” nodes were generated stochastically by preserving both the overall noun and adjective degree distributions of the word graph. In this “control” condition, networks were pre-trained with expertise on one arbitrary subset of nodes.</p>
<fig id="pcbi.1004155.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004155.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Word association with grandmother neurons.</title>
<p><bold>A</bold>. Adjective-noun associations in different domains of expertise: Portion of the bipartite association graph extracted from Wikipedia based on adjective pairing frequency for animals (red) and objects (blue) nouns. Arrows represent associations that have been learned during pre-training (solid lines) as well as those present in the bipartite graph but not used for pre-training (dotted lines). This example illustrates greater pre-training with animal associations (“animal expert”). Consequently, this network will be more likely to acquire newly presented associations that belong to the <italic>animal</italic> class (yellow highlight) as opposed to the <italic>object</italic> class (orange highlight). <bold>B</bold>. Background information-gated learning in the word graph: Proportion of newly acquired associations in the bipartite association graph. Networks were pre-trained with half of the edges, varying the amount of expertise from <italic>highly specialized</italic> (top row: 40% animal edges and 10% object edges or vice versa) to <italic>mildly specialized</italic> (middle: 30%-20% animal-object edges or vice versa) to <italic>not specialized</italic> (bottom: 25%-25%). A third network was pre-trained with the same proportions of two arbitrary subsets of edges in a random equivalent bipartite graph. The expert groups (left to right pairs in each row: animal, object, random) always outperformed the “novice” group (object, animal, random). The improved learning for animals relative to object (and random) cases is due to intrinsic background information (see text).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004155.g002" position="float" xlink:type="simple"/>
</fig>
<p>The “intrinsic background information” of a noun class can be quantified from the bipartite graph with the <italic>Proximity</italic> function and Pearson’s product-moment correlation coefficients (<xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 3.1). Specifically, consider the proximities of a noun with the set of all adjectives: the correlation of these values can be then computed between any two nouns. The intrinsic background information of a noun class will be reflected by a statistically larger mean correlation coefficient over all pairs of nouns within that class than over all pairs of nouns from two different classes. The mean correlation was significantly greater for animal-animal than the animal-object pairs (0.69 <italic>vs</italic>. 0.47, p&lt;10<sup>-4</sup>), while there was no statistical difference (p&gt;0.1) between the mean correlations of the object-object (0.48) and object-animal (0.46) pairs (see <xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 3.1 for details).</p>
</sec>
<sec id="sec006">
<title>BIG Learning in Watts-Strogatz Networks</title>
<p>To test the BIG ADO learning rule in more broadly applicable cases than noun-adjective associations, we generated small-world graphs adapting the algorithm of Watts and Strogatz [<xref rid="pcbi.1004155.ref019" ref-type="bibr">19</xref>]. Specifically, unless otherwise noted, Watts-Strogatz graphs were initially produced with degree 20 and 10% rewiring probability. Next, a random direction was selected for 90% of the edges, while the remaining 10% was made bidirectional. A random 20% of the nodes, along with all their incoming edges, were then labeled as belonging to the agent’s area of expertise. In the pre-training phase, networks were wired with a random set of edges of the graph, with the constraint that half of them must belong to the area of expertise, unless otherwise specified. The resulting connectivity consisted of a sub-graph of the initial graph, whose nodes in the area of expertise had higher average degree than those outside the agent’s expertise. In the “grandmother cell” implementation (<xref rid="pcbi.1004155.g003" ref-type="fig">Fig. 3</xref>), the BIG ADO threshold was set at 1. When the size of the graph (N) was varied to assess the robustness of the BIG ADO findings with respect to the parameter space, the degree (d) and the number of associations (edges) used to pre-train the network (T) also varied as d = N/50 and T = N×d/4, in order to keep the fraction of associations learned during pre-training constant.</p>
<fig id="pcbi.1004155.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004155.g003</object-id>
<label>Fig 3</label>
<caption>
<title>The cognitive value of BIG computations.</title>
<p><bold>A</bold>. BIG ADO in generic co-occurrence graphs: Simplified representation of the Watts-Strogatz graph-based model. During pre-training, half of the associations the network learns (solid lines) correspond to edges terminating in 20% of the nodes (black: “domain of expertise”). The other half is sampled from the remaining 80% of the graph (gray: novice domain). After pre-training, the ability to learn new (dashed) associations is tested both within and outside the domain of expertise. If two or more pairs of nodes are co-activated at once, spurious associations (dotted) could be learned across the pairs. <bold>B</bold>. BIG learning in small-world graphs: Differential ability of the pre-trained network to acquire new associations within (72.1±2.3%) and outside (3.9±0.4%) domain of expertise. <bold>C</bold>. Differentiating real from spurious associations: To discern the ability to learn real versus spurious associations in Watts-Strogatz graphs, pairs of new co-occurrences were presented, such as “buzzing beetle” and “buzzing grapefruit” (as if seeing/hearing a buzzing beetle while eating a grapefruit). The former is real (it belongs to the Watts-Strogatz graph), while the latter is spurious. Almost 13% of real associations were learned, including both those within and outside domain of expertise (black and gray lines in Fig. 3A), as opposed to less than 2% of spurious associations (dotted line in Fig. 3A).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004155.g003" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Extension of the ADO Rule to Cell Assemblies</title>
<p>Neural network simulations with realistic cell assemblies (<xref rid="pcbi.1004155.g004" ref-type="fig">Fig. 4</xref>) implemented the Zip Net model [<xref rid="pcbi.1004155.ref020" ref-type="bibr">20</xref>], a computational enhancement of classic Associative Nets [<xref rid="pcbi.1004155.ref021" ref-type="bibr">21</xref>] that ensures optimal Bayesian learning [<xref rid="pcbi.1004155.ref022" ref-type="bibr">22</xref>]. Briefly, learning the association between two concepts A and B represented respectively by neurons a<sub>1</sub>, a<sub>2</sub>, …, a<sub>s</sub> and b<sub>1</sub>, b<sub>2</sub>, …, b<sub>s</sub>, entails strengthening (or forming) synapses between co-active neurons and weakening or eliminating those between active and inactive neurons. Specifically, in the “incidence” matrix M with rows and columns respectively representing pre- and post-synaptic neurons, the entries in columns b<sub>j</sub>’s of all a<sub>i</sub>’s rows are increased while the remaining entries are decreasing by an appropriate amount to keep the total synaptic input constant (<xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 2.3).</p>
<fig id="pcbi.1004155.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004155.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Generalization of ADO to biologically realistic mechanisms.</title>
<p><bold>A</bold>. BIG learning with cell-assemblies in small-world graphs of different connectivity: Ratios between the percentages of associations learned in the novice vs. expert domain (bottom surface) and for spurious vs. real co-occurrences (top surface) with varying graph degrees and rewiring probabilities when using cell assembly representation of Watts-Strogatz graphs. Lower rewiring probabilities and, to some extent, higher degrees improve the ability to discriminate real from spurious co-occurrences. These conditions correspond to highly clustered (as opposed to fully random) graphs. The ability to learn new associations within the domain of expertise remains more than double compared to a novice domain. <bold>B</bold>. Robustness of the BIG ADO mechanism: Ratios between the percentages of associations learned in the novice vs. expert domain with cell assembly representation of Watts-Strogatz graphs when varying (typically one at a time) several model parameters. The full ordinate scale is used to allow comparison with panel C, but the same data are also expanded in the <bold>inset</bold> to emphasize the invariance of the results (error bars: standard deviation). All parameter values are reported in the <bold>table legend</bold> below the plot (with default values in bold). The parameters and their abbreviations are: the number of nodes in the Watts-Strogatz graph (N), which also implies a change in the graph degree, d (kept at 2% of N) as well as the number of pre-training associations (corresponding to N×d/4, that is one half of the pool of available associations); the number of neurons in the network (N<sub>n</sub>) and the cell assembly size (S), whereas N was also varied together with S (<sup>S</sup>N<sub>n</sub>) so as to keep their ratio constant at 200; the activation threshold (AT), i.e. the fraction of neurons in the cell assembly that need to be synchronously active in order to “identify” the node of the graph represented by that assembly; the firing threshold (FT), i.e. the proportion of presynaptic neuron required to fire in order to activate a postsynaptic neuron; the matrix load (ML), i.e. the constant fraction of presynaptic neurons connected to each postsynaptic neuron in the cell assembly learning model; and the proximity load (PL), i.e. the (top) fraction of axonal-dendritic overlaps throughout the network that are considered to be potential synapses (see also <xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 2.4). <bold>C</bold>. Optimal conditions for one-trial learning of real but not spurious associations: Ratios between the percentages of associations learned for spurious vs. real co-occurrences with cell assembly representation of Watts-Strogatz graphs when varying the same model parameters as in Fig. 4B. The most tunable parameters are the firing threshold (neuronal excitability) and the proximity load (strength of BIG ADO filter: see <xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 2.4).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004155.g004" position="float" xlink:type="simple"/>
</fig>
<p>In the pre-training phase, the connectivity matrix is generated from the incidence matrix simply by keeping a fixed number of synapses per neuron (those with highest weight), and setting the rest to zero. During BIG ADO testing, two neurons a and b can only form a new synapse upon co-activation if they have an axonal-dendritic overlap, which is expressed as the triple matrix product ΩΩ<sup>t</sup>Ω computed from the positive values of the incidence matrix (<xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 2.4). Lastly, retrieval works as a classic dendritic sum: given a stimulus A’ represented by neurons a’<sub>1</sub>, a’<sub>2</sub>, …, a’<sub>s</sub>, all the entries in the rows corresponding to the a<sub>i</sub>’s are added up for each column, and those sums exceeding a given firing threshold correspond to activated (post-synaptic) neurons. If enough neurons belonging to the same cell assembly B’ fire, concept B’ gets activated.</p>
</sec>
</sec>
<sec id="sec008" sec-type="results">
<title>Results</title>
<sec id="sec009">
<title>Prior Knowledge Gates Learning of Word Associations by Grandmother Neurons</title>
<p>We tested the BIG ADO paradigm on a bipartite association graph derived from a compilation of 32 million noun/adjective co-occurrences in Wikipedia. We identified two classes of nouns (animals and household objects) and pre-trained two networks to learn a subset of the noun/adjective associations, each with “expertise” mostly in one of the two noun classes (<xref rid="pcbi.1004155.g002" ref-type="fig">Fig. 2A</xref>). Specifically, one network was pre-trained with a greater proportion of animal/adjective associations than of object/adjective associations (and vice versa for the other network). BIG learning facilitated networks to acquire new information that was related to the information already stored. Moreover, the magnitude of this phenomenon increased with the level of specialization between animals and objects (<xref rid="pcbi.1004155.g002" ref-type="fig">Fig. 2B</xref>). Note that, even in their “novice” domain of knowledge, networks cannot be completely “naïve.” Even if the pre-trained proportion of “novice” edges is lower than in the domain of expertise, it must still be non-zero or else no subsequent associations could be learned.</p>
<p>Interestingly, the effect was greater for animal expertise than for object expertise. Furthermore, more animal associations were learned when the network was pre-trained with the same number of animal and object edges. Both of these differences can be explained by two independent forms of background information: one intrinsic in the source data, and another dependent on the sample used to pre-train the network. The former was eliminated by repeating the simulations on random equivalent graphs (<xref rid="pcbi.1004155.g002" ref-type="fig">Fig. 2B</xref>: right bar pairs). Direct analysis of Pearson’s coefficients of the bipartite graph Proximity function (see <xref rid="sec002" ref-type="sec">Materials and Methods</xref>) confirmed that the noun/adjective association is more specific for animals than for objects (0.69 <italic>vs</italic>. 0.48, p&lt;10<sup>-4</sup>).</p>
</sec>
<sec id="sec010">
<title>BIG Learning in Small-World Graphs: Ability to Differentiate Real from Spurious Associations</title>
<p>To validate the above results against broadly applicable cases besides word associations, we tested the BIG ADO learning rule in a general class of random small-world graphs [<xref rid="pcbi.1004155.ref019" ref-type="bibr">19</xref>] resembling real-world architectures, organizations, and interactions (<xref rid="pcbi.1004155.g003" ref-type="fig">Fig. 3A</xref>). Networks were pre-trained with samples of associations biased towards an arbitrary subset of nodes. As in the bipartite graph, the ADO filter gated subsequent learning of new associations by favoring those pertaining to this background information (<xref rid="pcbi.1004155.g003" ref-type="fig">Fig. 3B</xref>). Next we investigated the ability of BIG to differentiate between “real” and “spurious” associations. Most co-occurrences experienced in everyday life do not reflect real associations, but rather events that happened together by chance. For example, suppose you were eating a grapefruit while experiencing the buzzing beetle described in the <italic>Introduction</italic>. Why should buzzing be associated with beetle and not with grapefruit?</p>
<p>Hebbian models form both associations, relying on later experience to reinforce those that reoccur and eliminating the others [<xref rid="pcbi.1004155.ref012" ref-type="bibr">12</xref>], e.g. upon repeatedly dissociated experiences of eating a grapefruit without buzz and vice versa. Strikingly, the BIG ADO filter distinguished real from spurious associations (<xref rid="pcbi.1004155.g003" ref-type="fig">Fig. 3C</xref>), facilitating the ability to learn relevant co-occurrences over “occasional” ones the first time around. In a simple protocol, each experience consisted of the co-activation of two independent pairs of connected nodes in the Watts-Strogatz graph. The resulting six co-occurrences correspond to two real associations (between the two connected nodes in each of the pair) and four spurious associations (between neurons across the pairs).</p>
<p>Inspection of the simulation outcomes confirmed that spurious “buzzing grapefruit” co-occurrences were not remembered <italic>because</italic> they lacked relevant background information. In the pre-trained network, the axon of buzzing overlaps with the dendrite of beetle (high ADO) thanks to the already acquired buzzing-wasp, flying erratically-wasp, and flying erratically-beetle associations. Thus, the potential association buzzing-beetle ‘passes’ the BIG ADO filter. In contrast, buzzing and grapefruit have little if any axonal-dendritic overlap; thus, the corresponding association is not formed according to the BIG ADO mechanism. The learning differentials of both expert-over-novice networks and real-over-spurious associations increased with the bias towards a subset of nodes in the Watts-Strogatz graph, and were observed over a broad range of model parameters (see <xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 3.2 for additional results).</p>
</sec>
<sec id="sec011">
<title>Generalization to Realistic Cell Assemblies</title>
<p>The notion of representing mental states or elementary concepts in single (“grandmother”) neurons is appealing [<xref rid="pcbi.1004155.ref023" ref-type="bibr">23</xref>] but unrealistic [<xref rid="pcbi.1004155.ref024" ref-type="bibr">24</xref>]. Theories and experiments estimate that at least 50–200 cells take part in encoding each unit of thought [<xref rid="pcbi.1004155.ref025" ref-type="bibr">25</xref>, <xref rid="pcbi.1004155.ref026" ref-type="bibr">26</xref>, <xref rid="pcbi.1004155.ref027" ref-type="bibr">27</xref>]. Cell assemblies provide for redundancy, error-correction, and larger storage capacity. We thus extended the BIG ADO paradigm to cell assemblies. In cell assembly models, acquiring a new association between two co-occurring events entails formation of new synapses between the neurons representing one event and the neurons representing the other event. With the BIG ADO filter, forming synapse between a pair of co-active neurons requires appropriate pre-existing connections similarly to <xref rid="pcbi.1004155.g001" ref-type="fig">Fig. 1B</xref>, with the notable difference that the same neuron typically belongs to several cell assemblies.</p>
<p>Among the first (and simplest) neural network models employing cell assemblies are Willshaw’s Associative Nets [<xref rid="pcbi.1004155.ref021" ref-type="bibr">21</xref>]. Simulations with the Willshaw model confirmed the BIG ADO results with the word association graph (see <xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 2.3 for implementation detail and <xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 3.2 for analysis). However, the original Associative Nets achieve maximal storage capacity when cell assembly size is log-proportional to the number of neurons [<xref rid="pcbi.1004155.ref020" ref-type="bibr">20</xref>]. Such limitation on cell assembly size makes this approach unsuitable for learning realistic Watts-Strogatz graphs. A more sophisticated variant of this model, which achieves optimal Bayesian learning [<xref rid="pcbi.1004155.ref022" ref-type="bibr">22</xref>], attains excellent performance for cell assembly sizes compatible with those estimated for real brains. This latter model (Zip Nets) enabled cell assembly implementation of the BIG ADO mechanism with generic Watts-Strogatz graphs. In a typical configuration, the network learned 50% of novel associations within its domain of expertise, but only 9% unrelated to prior knowledge. When two node pairs (sampled randomly within and outside domain of expertise) were co-activated at once, 30% of real associations were learned vs. 7% of the spurious ones. Sampling only within or outside the domain of expertise, the learning proportions for real and spurious pairs were 50% and 12% or 9% and 3%, respectively.</p>
<p>Similar outcomes were consistently observed across a broad range of connectivity parameters in the small-world graphs. In particular, a substantially higher proportion of associations were learned within the domain of expertise than outside for any graph degree <italic>d</italic> (the average number of edges per node) from 8 to 24 and rewiring probability up to 80% (<xref rid="pcbi.1004155.g004" ref-type="fig">Fig. 4A</xref>). The rewiring probability R defines by construction Watts-Strogatz graphs as hybrids between regular (R = 0%) and random graphs (R = 100%). The fraction of spurious associations learned was substantially lower than that of real associations for degrees above 5 and rewiring probability below 50% (<xref rid="pcbi.1004155.g004" ref-type="fig">Fig. 4A</xref>). This suggests that prior connectivity (ADO) provides a biologically realistic neural correlate of background information and its ability to gate learning in any highly clustered networks. In clustered networks, two nodes are more likely to be interconnected if they are both connected to a third node. This is a common property of many types of graphs that extends beyond Watts-Strogatz networks [<xref rid="pcbi.1004155.ref028" ref-type="bibr">28</xref>].</p>
</sec>
<sec id="sec012">
<title>Robustness Analysis and Optimal Conditions</title>
<p>Although the adopted connectionist framework is an over-simplified model of nervous systems, this simplicity also reflects the foundational applicability of the BIG ADO learning rule. Specifically, the described mechanism does not depend on specific choices of parameters such as graph dimension, number of associations presented, learning threshold, and others. In particular, the main effect of axonal-dendritic overlap to selectively gate learning by background information was consistently reproduced in every combination of parameters conducive to adequate memory storage (<xref rid="pcbi.1004155.g004" ref-type="fig">Fig. 4B</xref>). Moreover, the discrimination between real and spurious associations with cell assemblies in small-world graphs was also largely unaffected by the choice of numerical values. Importantly, however, this latter effect varied quantitatively as a function of selected model parameters (<xref rid="pcbi.1004155.g004" ref-type="fig">Fig. 4C</xref>), such as the <italic>proximity load</italic>, which determines how topologically close an axon and a dendrite must be to constitute a potential synapse (see section 2.4 of <xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref>). This is the key parameter distinguishing BIG ADO from traditional Hebbian learning: a new synapse is formed between two neurons when they fire together <italic>if and only if</italic> a potential synapse is already present. Thus, certain circuits might be better designed than others to support efficient one-trial learning depending on their specific plasticity and excitability (see <xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref> 3.2 for additional results).</p>
</sec>
</sec>
<sec id="sec013" sec-type="conclusions">
<title>Discussion</title>
<p>This report introduced a new biologically-motivated learning rule for neural networks that explains why it is easier to acquire knowledge when it relates to known background information than when it is completely novel [<xref rid="pcbi.1004155.ref011" ref-type="bibr">11</xref>]. The key idea is that this “background information-gated” (BIG) learning emerges from the necessity of neuronal axons and dendrites to be adjacent to each other in order to establish new synapses. Such basic geometric requirement was explicitly recognized in Hebb’s original formulation of synaptic plasticity, yet is not usually accounted for in neural network learning rules. The claim that existing structure matters for learning is not new [<xref rid="pcbi.1004155.ref029" ref-type="bibr">29</xref>]. However, the level of abstraction of current computational models of brain function fails to capture the details of axonal and dendritic shape.</p>
<p>The critical breakthrough of this work consisted of parsimoniously relating “axonal-dendritic overlap” (ADO) to circuit connectivity by assuming optimal neuronal placement to minimize axonal wiring. This corresponds to a fundamental neuroanatomical constraint: an axon must pass close to the dendrites that are <italic>near other neurons it contacts</italic>. The topographic organization of the mammalian cortex ensures that nearby neurons on average encode related information [<xref rid="pcbi.1004155.ref030" ref-type="bibr">30</xref>]. Incorporating this new relationship into classic connectionist learning algorithms, we found that networks trained in a given domain more easily acquire further knowledge in the same domain than in others. If the proximity threshold is set to zero, the model reverts to a traditional neural network unconditionally learning all associations. From this perspective, the BIG ADO rule could be considered as a biological constraint on learning.</p>
<p>However, to our initial surprise, the morphologically-motivated constraint on structural plasticity also endows neural nets with the powerful computational ability to discriminate real associations of events, like the sight of a lightning and the sound of the thunder, from spurious co-occurrences, such as between the thunder and the beetle that flew by during the storm. Thus, we surmise that the selectivity of synaptic formation implied by the ADO requirement provides a fundamental cognitive advantage over the unconstrained “fire together, wire together” plasticity rule of classic artificial neural networks. Of course the ability to associate completely unrelated facts or events may also be useful in many circumstances. Several different models have proposed that the hippocampus might be specialized for precisely that function, possibly leveraging its superior plasticity rate [<xref rid="pcbi.1004155.ref031" ref-type="bibr">31</xref>] or adult neurogenesis [<xref rid="pcbi.1004155.ref032" ref-type="bibr">32</xref>]. Our model suggests that this ability might also derive from the lack of topographic mapping in this structure (e.g. hippocampal area CA3). Moreover, the profuse axonal arbors of cortical neurons may enable access to a surprisingly large pool of intertwining dendrites through neurite outgrowth [<xref rid="pcbi.1004155.ref033" ref-type="bibr">33</xref>], perhaps providing a counter-mechanism to balance the BIG ADO rule.</p>
<p>The computational advantage of the BIG ADO algorithm over alternative learning rules can be quantified in terms of discrimination between real associations and spurious co-occurrences. If <italic>k</italic> pairs of real associations (A<sub>1</sub>-B<sub>1</sub>, A<sub>2</sub>-B<sub>2</sub>, …, A<sub>k</sub>-B<sub>k</sub>) are presented at the same time, BIG ADO selectively learns the correctly paired events over spuriously co-occurring ones (e.g. A<sub>1</sub>-B<sub>2</sub>, A<sub>2</sub>-B<sub>1</sub>, etc.). A “fire-together, wire-together” rule without ADO constraint can achieve similar selectivity by repetition. In this case, each association must be presented multiple times in order to attain the same discrimination power displayed by BIG ADO in one-trial learning. The number of required repetitions grows with the number <italic>k</italic> of real associations presented together and also depends on the structure of the association graph. For example, in the conditions of <xref rid="pcbi.1004155.g003" ref-type="fig">Fig. 3</xref>, BIG ADO learns real associations at a rate of 6:1 relative to spurious co-occurrences upon the first presentation. To obtain the same ratio in the absence of ADO if just five pairs are presented together, <italic>each</italic> association has to be repeated on average four times.</p>
<p>Mammalian brains display greatest plasticity during development, but certain cortical regions remain plastic throughout adulthood [<xref rid="pcbi.1004155.ref034" ref-type="bibr">34</xref>, <xref rid="pcbi.1004155.ref035" ref-type="bibr">35</xref>]. Our research design is consistent with an initial phase of maximal plasticity, followed by a ‘mature’ state of conditional plasticity. Specifically, during pre-training, all witnessed associations are learned. Clearly, the anatomical constraint of axonal-dendritic overlap holds in all phases of development. However, the more prominent neuronal and axonal movements in earlier developmental stages would largely circumvent or alleviate the ADO filter. In practice, we pre-load the network directly with synaptic connectivity equivalent to that resulting from such an initial developmental phase (representing ‘background knowledge’). Afterword, the model preferentially learns associations related to previously acquired information. The resulting mature network not only avoids associating the (most numerous) spurious co-occurrences, but is also optimally structured to learn the associations most relevant to the environment in which it developed. Besides providing clear evolutionary advantages, these key features could also be applied in artificial intelligence and search engines.</p>
<p>Background information gating explains the familiar ability to form stable memories based on single experiences (as opposed to repetition). This process is complementary to (and as fundamental as) other factors known to control learning, such as valence and novelty. The proposed mechanism of axonal-dendritic overlap, based on the elementary anatomical organization of neuronal circuits, is also independent of neuromodulatory pathways likely to underlie alternative or parallel regulation of one-trial learning. This framework can also be useful to describe how semantic knowledge can be incorporated into existing knowledge. Moreover, the model offers a possible neural network correlate for the rapid memory consolidation occurring when new information is assimilated into a pre-existing associative “schema” or mental representation [<xref rid="pcbi.1004155.ref036" ref-type="bibr">36</xref>]. Other recent models have been proposed to explain the dependence of learning on prior knowledge [<xref rid="pcbi.1004155.ref037" ref-type="bibr">37</xref>].</p>
<p>The proposed BIG ADO learning rule is only conceptually related to axonal-dendritic overlap, as the anatomical data necessary to generate a complete model of all axons and dendrites in a network is still unavailable (see e.g. [<xref rid="pcbi.1004155.ref038" ref-type="bibr">38</xref>]). Realistically, potential synapses might work in synergy with additional mechanisms conducive to the same learning rule. For example, presentation of individual elemental associations (buzzing wasp, flying wasp, and flying beetle) may lead to the formation of cell assemblies representing associations between higher-order concepts and their properties (“flying insect”), as previously hypothesized [<xref rid="pcbi.1004155.ref039" ref-type="bibr">39</xref>], possibly supported by ongoing structural plasticity [<xref rid="pcbi.1004155.ref040" ref-type="bibr">40</xref>]. Moreover, axonal-dendritic overlap may provide powerful constraints for the recruitment of individual neurons into cell assemblies. While cell assembly selection has been proposed as the core of knowledge representation in neural systems [<xref rid="pcbi.1004155.ref041" ref-type="bibr">41</xref>], the underlying anatomical mechanisms have so far remained elusive [<xref rid="pcbi.1004155.ref026" ref-type="bibr">26</xref>]. Thus, the proposed link between neuronal structure and function may constitute an essential foundation for brain-based theories of cognition.</p>
</sec>
<sec id="sec014">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004155.s001" xlink:href="info:doi/10.1371/journal.pcbi.1004155.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Much ADO About BIG Learning: Supplementary Information.</title>
<p>The single Supporting Information file (<xref rid="pcbi.1004155.s001" ref-type="supplementary-material">S1 Text</xref>) describing the model’s underlying assumptions, detailed methodologies, and supplementary results includes additional text, illustration, and references.</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Dr. James L. Olds for feedback on an earlier version of the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004155.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wright</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Gaskell</surname> <given-names>GD</given-names></name> (<year>1995</year>) <article-title>Flashbulb memories: conceptual and methodological issues</article-title>. <source>Memory</source> <volume>3</volume>: <fpage>67</fpage>–<lpage>80</lpage>. <object-id pub-id-type="pmid">8556535</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larkin</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>McDermott</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Simon</surname> <given-names>DP</given-names></name>, <name name-style="western"><surname>Simon</surname> <given-names>HA</given-names></name> (<year>1980</year>) <article-title>Expert and novice performance in solving physics problems</article-title>. <source>Science</source> <volume>208</volume>(<issue>4450</issue>): <fpage>1335</fpage>–<lpage>1342</lpage>. <object-id pub-id-type="pmid">17775709</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kintsch</surname> <given-names>W</given-names></name> (<year>1988</year>) <article-title>The role of knowledge in discourse comprehension: a construction-integration model</article-title>. <source>Psychol Rev</source>. <volume>95</volume>(<issue>2</issue>): <fpage>163</fpage>–<lpage>182</lpage>. <object-id pub-id-type="pmid">3375398</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Butz</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wörgötter</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>van Ooyen</surname> <given-names>A</given-names></name> (<year>2009</year>) <article-title>Activity-dependent structural plasticity</article-title>. <source>Brain Res Rev</source>. <volume>60</volume>: <fpage>287</fpage>–<lpage>305</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.brainresrev.2008.12.023" xlink:type="simple">10.1016/j.brainresrev.2008.12.023</ext-link></comment> <object-id pub-id-type="pmid">19162072</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Holtmaat</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Svoboda</surname> <given-names>K</given-names></name> (<year>2009</year>) <article-title>Experience-dependent structural synaptic plasticity in the mammalian brain</article-title>. <source>Nature Rev Neurosci</source>. <volume>10</volume>: <fpage>647</fpage>–<lpage>658</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2699" xlink:type="simple">10.1038/nrn2699</ext-link></comment> <object-id pub-id-type="pmid">19693029</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Caroni</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Donato</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Muller</surname> <given-names>D</given-names></name> (<year>2012</year>) <article-title>Structural plasticity upon learning: regulation and functions</article-title>. <source>Nat Rev Neurosci</source>. <volume>13</volume>: <fpage>478</fpage>–<lpage>490</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn3258" xlink:type="simple">10.1038/nrn3258</ext-link></comment> <object-id pub-id-type="pmid">22714019</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref007"><label>7</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hebb</surname> <given-names>DO</given-names></name> (<year>1949</year>) <source>The Organization of Behavior: A Neuropsychological Theory</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley and Sons</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1004155.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stepanyants</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name> (<year>2005</year>) <article-title>Neurogeometry and potential synaptic connectivity</article-title>. <source>Trends Neurosci</source>. <volume>28</volume>: <fpage>387</fpage>–<lpage>394</lpage>. <object-id pub-id-type="pmid">15935485</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rivera-Alba</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Peng</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>de Polavieja</surname> <given-names>GG</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name> (<year>2014</year>) <article-title>Wiring economy can account for cell body placement across species and brain areas</article-title>. <source>Curr Biol</source>. <volume>24</volume>: <fpage>R109</fpage>–<lpage>R110</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2013.12.012" xlink:type="simple">10.1016/j.cub.2013.12.012</ext-link></comment> <object-id pub-id-type="pmid">24502781</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name> (<year>2009</year>) <article-title>Reading the book of memory: sparse sampling versus dense mapping of connectomes</article-title>. <source>Neuron</source>. <volume>62</volume>: <fpage>17</fpage>–<lpage>29</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.03.020" xlink:type="simple">10.1016/j.neuron.2009.03.020</ext-link></comment> <object-id pub-id-type="pmid">19376064</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sadtler</surname> <given-names>PT</given-names></name>, <name name-style="western"><surname>Quick</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Golub</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Chase</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>SI</given-names></name>, <etal>et. al</etal>. (<year>2014</year>) <article-title>Neural constraints on learning</article-title>. <source>Nature</source> <volume>512</volume>: <fpage>423</fpage>–<lpage>426</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature13665" xlink:type="simple">10.1038/nature13665</ext-link></comment> <object-id pub-id-type="pmid">25164754</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname> <given-names>TH</given-names></name>, <name name-style="western"><surname>Kairiss</surname> <given-names>EW</given-names></name>, <name name-style="western"><surname>Keenan</surname> <given-names>CL</given-names></name> (<year>1990</year>) <article-title>Hebbian synapses: biophysical mechanisms and algorithms</article-title>. <source>Annu Rev Neurosci</source>. <volume>13</volume>: <fpage>475</fpage>–<lpage>511</lpage>. <object-id pub-id-type="pmid">2183685</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koulakov</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name> (<year>2001</year>) <article-title>Orientation preference patterns in mammalian visual cortex: a wire length minimization approach</article-title>. <source>Neuron</source> <volume>29</volume>: <fpage>519</fpage>–<lpage>527</lpage>. <object-id pub-id-type="pmid">11239440</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rivera-Alba</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Vitaladevuni</surname> <given-names>SN</given-names></name>, <name name-style="western"><surname>Mishchenko</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Takemura</surname> <given-names>SY</given-names></name>, <etal>et. al</etal>. (<year>2011</year>) <article-title>Wiring economy and volume exclusion determine neuronal placement in the Drosophila brain</article-title>. <source>Curr. Biol</source>. <volume>21</volume>: <fpage>2000</fpage>–<lpage>2005</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2011.10.022" xlink:type="simple">10.1016/j.cub.2011.10.022</ext-link></comment> <object-id pub-id-type="pmid">22119527</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Drew</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name> (<year>2005</year>) <article-title>Cascade models of synaptically stored memories</article-title>. <source>Neuron</source> <volume>45</volume>: <fpage>599</fpage>–<lpage>611</lpage>. <object-id pub-id-type="pmid">15721245</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pulvermüller</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Knoblauch</surname> <given-names>A</given-names></name> (<year>2009</year>) <article-title>Discrete combinatorial circuits emerging in neural networks: A mechanism for rules of grammar in the human brain?</article-title> <source>Neural Networks</source> <volume>22</volume>:<fpage>161</fpage>–<lpage>172</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2009.01.009" xlink:type="simple">10.1016/j.neunet.2009.01.009</ext-link></comment> <object-id pub-id-type="pmid">19237262</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kohonen</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Honkela</surname> <given-names>T</given-names></name> (<year>2007</year>) <article-title>Kohonen network</article-title>. <source>Scholarpedia</source>, <volume>2</volume>(<issue>1</issue>):<fpage>1568</fpage>. scholarpedia.org/article/Kohonen_network.</mixed-citation></ref>
<ref id="pcbi.1004155.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bavelier</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Levi</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Dan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hensch</surname> <given-names>TK</given-names></name> (<year>2010</year>) <article-title>Removing brakes on adult brain plasticity: from molecular to behavioral interventions</article-title>. <source>J. Neurosci</source>. <volume>30</volume>: <fpage>14964</fpage>–<lpage>14971</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4812-10.2010" xlink:type="simple">10.1523/JNEUROSCI.4812-10.2010</ext-link></comment> <object-id pub-id-type="pmid">21068299</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watts</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Strogatz</surname> <given-names>SH</given-names></name> (<year>1998</year>) <article-title>Collective dynamics of 'small-world' networks</article-title>. <source>Nature</source> <volume>393</volume>: <fpage>440</fpage>–<lpage>442</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004155.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knoblauch</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Palm</surname> <given-names>G</given-names></name>, and <name name-style="western"><surname>Sommer</surname> <given-names>FT</given-names></name> (<year>2010</year>) <article-title>Memory capacities for synaptic and structural plasticity</article-title>. <source>Neural Comput</source>. <volume>22</volume>: <fpage>289</fpage>–<lpage>341</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2009.08-07-588" xlink:type="simple">10.1162/neco.2009.08-07-588</ext-link></comment> <object-id pub-id-type="pmid">19925281</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willshaw</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Buneman</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Longuet-Higgins</surname> <given-names>H</given-names></name> (<year>1969</year>) <article-title>Non-holographic associative memory</article-title>. <source>Nature</source> <volume>222</volume>: <fpage>960</fpage>–<lpage>962</lpage>. <object-id pub-id-type="pmid">5789326</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knoblauch</surname> <given-names>A</given-names></name> (<year>2011</year>) <article-title>Neural associative memory with optimal Bayesian learning</article-title>. <source>Neural Comput</source>. <volume>23</volume>: <fpage>1393</fpage>–<lpage>1451</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00127" xlink:type="simple">10.1162/NECO_a_00127</ext-link></comment> <object-id pub-id-type="pmid">21395440</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bowers</surname> <given-names>JS</given-names></name> (<year>2009</year>) <article-title>On the biological plausibility of grandmother cells: implications for neural network theories in psychology and neuroscience</article-title>. <source>Psychol Rev</source>. <volume>116</volume>: <fpage>220</fpage>–<lpage>251</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0014462" xlink:type="simple">10.1037/a0014462</ext-link></comment> <object-id pub-id-type="pmid">19159155</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Quiroga</surname> <given-names>RQ</given-names></name>, <name name-style="western"><surname>Kreiman</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fried</surname> <given-names>I</given-names></name> (<year>2008</year>) <article-title>Sparse but not 'grandmother-cell' coding in the medial temporal lobe</article-title>. <source>Trends Cogn Sci</source>. <volume>12</volume>: <fpage>87</fpage>–<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2007.12.003" xlink:type="simple">10.1016/j.tics.2007.12.003</ext-link></comment> <object-id pub-id-type="pmid">18262826</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Levy</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hasson</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Malach</surname> <given-names>R</given-names></name> (<year>2004</year>) <article-title>One picture is worth at least a million neurons</article-title>. <source>Curr. Biol</source>. <volume>14</volume>: <fpage>996</fpage>–<lpage>1001</lpage>. <object-id pub-id-type="pmid">15182673</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wallace</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Kerr</surname> <given-names>JN</given-names></name> (<year>2010</year>) <article-title>Chasing the cell assembly</article-title>. <source>Curr Opin Neurobiol</source>. <volume>20</volume>: <fpage>296</fpage>–<lpage>305</lpage>. <object-id pub-id-type="pmid">20545018</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Quiroga</surname> <given-names>RQ</given-names></name> (<year>2012</year>) <article-title>Concept cells: the building blocks of declarative memory functions</article-title>. <source>Nat. Rev. Neurosci</source> <volume>13</volume>: <fpage>587</fpage>–<lpage>597</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn3251" xlink:type="simple">10.1038/nrn3251</ext-link></comment> <object-id pub-id-type="pmid">22760181</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bullmore</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Sporns</surname> <given-names>O</given-names></name> (<year>2009</year>) <article-title>Complex brain networks: graph theoretical analysis of structural and functional systems</article-title>. <source>Nat Rev Neurosci</source>. <volume>10</volume>: <fpage>186</fpage>–<lpage>198</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2575" xlink:type="simple">10.1038/nrn2575</ext-link></comment> <object-id pub-id-type="pmid">19190637</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Altmann</surname> <given-names>GT</given-names></name> (<year>2002</year>) <article-title>Learning and development in neural networks—the importance of prior experience</article-title>. <source>Cognition. Sep</source>;<volume>85</volume>(<issue>2</issue>): <fpage>B43</fpage>–<lpage>50</lpage>. <object-id pub-id-type="pmid">12127703</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Koulakov</surname> <given-names>AA</given-names></name> (<year>2004</year>) <article-title>Maps in the brain: what can we learn from them?</article-title> <source>Annu Rev Neurosci</source>. <volume>27</volume>: <fpage>369</fpage>–<lpage>392</lpage>. <object-id pub-id-type="pmid">15217337</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O'Reilly</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Norman</surname> <given-names>KA</given-names></name> (<year>2002</year>) <article-title>Hippocampal and neocortical contributions to memory: advances in the complementary learning systems framework</article-title>. <source>Trends Cogn Sci</source>. <volume>6</volume>(<issue>12</issue>):<fpage>505</fpage>–<lpage>510</lpage>. <object-id pub-id-type="pmid">12475710</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aimone</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Wiles</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Gage</surname> <given-names>FH</given-names></name> (<year>2009</year>) <article-title>Computational influence of adult neurogenesis on memory encoding</article-title>. <source>Neuron</source>. <volume>61</volume>(<issue>2</issue>):<fpage>187</fpage>–<lpage>202</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2008.11.026" xlink:type="simple">10.1016/j.neuron.2008.11.026</ext-link></comment> <object-id pub-id-type="pmid">19186162</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Svoboda</surname> <given-names>K</given-names></name> (<year>2004</year>) <article-title>Cortical rewiring and storage capacity</article-title>. <source>Nature</source>. <volume>31</volume>:<fpage>782</fpage>–<lpage>788</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004155.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fu</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Zuo</surname> <given-names>Y</given-names></name> (<year>2011</year>) <article-title>Experience-dependent structural plasticity in the cortex</article-title>. <source>Trends Neurosci</source>. <volume>34</volume>: <fpage>177</fpage>–<lpage>187</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2011.02.001" xlink:type="simple">10.1016/j.tins.2011.02.001</ext-link></comment> <object-id pub-id-type="pmid">21397343</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leuner</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Gould</surname> <given-names>E</given-names></name> (<year>2010</year>) <article-title>Structural plasticity and hippocampal function</article-title>. <source>Annu. Rev. Psychol</source>. <volume>61</volume>: <fpage>111</fpage>–<lpage>140</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.psych.093008.100359" xlink:type="simple">10.1146/annurev.psych.093008.100359</ext-link></comment> <object-id pub-id-type="pmid">19575621</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tse</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Langston</surname> <given-names>RF</given-names></name>, <name name-style="western"><surname>Kakeyama</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bethus</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Spooner</surname> <given-names>PA</given-names></name>, <etal>et. al</etal>. (<year>2007</year>) <article-title>Schemas and memory consolidation</article-title>. <source>Science</source>. <volume>316</volume>(<issue>5821</issue>):<fpage>76</fpage>–<lpage>82</lpage>. <object-id pub-id-type="pmid">17412951</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McClelland</surname> <given-names>JL</given-names></name> (<year>2013</year>) <article-title>Incorporating rapid neocortical learning of new schema-consistent information into complementary learning systems theory</article-title>. <source>J Exp Psychol Gen</source>. <volume>142</volume>(<issue>4</issue>):<fpage>1190</fpage>–<lpage>1210</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0033812" xlink:type="simple">10.1037/a0033812</ext-link></comment> <object-id pub-id-type="pmid">23978185</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ropireddy</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ascoli</surname> <given-names>GA</given-names></name> (<year>2011</year>) <article-title>Potential Synaptic Connectivity of Different Neurons onto Pyramidal Cells in a 3D Reconstruction of the Rat Hippocampus</article-title>. <source>Front Neuroinform</source>. <volume>5</volume>:<fpage>5</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fninf.2011.00005" xlink:type="simple">10.3389/fninf.2011.00005</ext-link></comment> <object-id pub-id-type="pmid">21779242</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pulvermüller</surname> <given-names>F</given-names></name> (<year>2002</year>) <article-title>A brain perspective on language mechanisms: From discrete neuronal ensembles to serial order</article-title>. <source>Progr. Neurobiol</source>. <volume>67</volume>: <fpage>85</fpage>–<lpage>111</lpage>. <object-id pub-id-type="pmid">12126657</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knoblauch</surname> <given-names>A</given-names></name>., <name name-style="western"><surname>Koerner</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Koerner</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>FT</given-names></name> (<year>2014</year>) <article-title>Structural plasticity has high memory capacity and can explain graded amnesia, catastrophic forgetting, and the spacing effect</article-title>. <source>PLoS ONE</source> <volume>9</volume>(<issue>5</issue>):<fpage>e96485</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0096485" xlink:type="simple">10.1371/journal.pone.0096485</ext-link></comment> <object-id pub-id-type="pmid">24858841</object-id></mixed-citation></ref>
<ref id="pcbi.1004155.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Edelman</surname> <given-names>GM</given-names></name> (<year>1993</year>) <article-title>Neural Darwinism: selection and reentrant signaling in higher brain function</article-title>. <source>Neuron</source> <volume>10</volume>: <fpage>115</fpage>–<lpage>125</lpage>. <object-id pub-id-type="pmid">8094962</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>