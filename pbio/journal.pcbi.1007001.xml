<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1007001</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-01739</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Magnetoencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Magnetoencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Magnetoencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Human performance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Human performance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Information technology</subject><subj-group><subject>Information processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject><subj-group><subject>Support vector machines</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject><subj-group><subject>Recurrent neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject><subj-group><subject>Recurrent neural networks</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Beyond core object recognition: Recurrent processes account for object recognition under occlusion</article-title>
<alt-title alt-title-type="running-head">Temporal dynamics of occluded object recognition</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Rajaei</surname>
<given-names>Karim</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8525-957X</contrib-id>
<name name-style="western">
<surname>Mohsenzadeh</surname>
<given-names>Yalda</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7013-8078</contrib-id>
<name name-style="western">
<surname>Ebrahimpour</surname>
<given-names>Reza</given-names>
</name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Khaligh-Razavi</surname>
<given-names>Seyed-Mahdi</given-names>
</name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>School of Cognitive Sciences (SCS), Institute for Research in Fundamental Sciences (IPM), Niavaran, Tehran, Iran</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Computer Science and AI Lab (CSAIL), MIT, Cambridge, Massachusetts, United States of America</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Department of Computer Engineering, Shahid Rajaee Teacher Training University, Tehran, Iran</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Department of Brain and Cognitive Sciences, Cell Science Research Center, Royan Institute for Stem Cell Biology and Technology, ACECR, Tehran, Iran</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Isik</surname>
<given-names>Leyla</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>MIT, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">rebrahimpour@srttu.edu</email> (RE); <email xlink:type="simple">skhaligh@mit.edu</email> (S-MK-R)</corresp>
</author-notes>
<pub-date pub-type="epub">
<day>15</day>
<month>5</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>5</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>5</issue>
<elocation-id>e1007001</elocation-id>
<history>
<date date-type="received">
<day>17</day>
<month>10</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>2</day>
<month>4</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Rajaei et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1007001"/>
<abstract>
<p>Core object recognition, the ability to rapidly recognize objects despite variations in their appearance, is largely solved through the feedforward processing of visual information. Deep neural networks are shown to achieve human-level performance in these tasks, and explain the primate brain representation. On the other hand, object recognition under more challenging conditions (i.e. beyond the core recognition problem) is less characterized. One such example is object recognition under occlusion. It is unclear to what extent feedforward and recurrent processes contribute in object recognition under occlusion. Furthermore, we do not know whether the <bold>conventional</bold> deep neural networks, such as AlexNet, which were shown to be successful in solving core object recognition, can perform similarly well in problems that go beyond the core recognition. Here, we characterize neural dynamics of object recognition under occlusion, using magnetoencephalography (MEG), while participants were presented with images of objects with various levels of occlusion. We provide evidence from multivariate analysis of MEG data, behavioral data, and computational modelling, demonstrating an essential role for recurrent processes in object recognition under occlusion. Furthermore, the computational model with local recurrent connections, used here, suggests a mechanistic explanation of how the human brain might be solving this problem.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>In recent years, deep-learning-based computer vision algorithms have been able to achieve human-level performance in several object recognition tasks. This has also contributed in our understanding of how our brain may be solving these recognition tasks. However, object recognition under more challenging conditions, such as occlusion, is less characterized. Temporal dynamics of object recognition under occlusion is largely unknown in the human brain. Furthermore, we do not know if the previously successful deep-learning algorithms can similarly achieve human-level performance in these more challenging object recognition tasks. By linking brain data with behavior, and computational modeling, we characterized temporal dynamics of object recognition under occlusion, and proposed a computational mechanism that explains both behavioral and the neural data in humans. This provides a plausible mechanistic explanation for how our brain might be solving object recognition under more challenging conditions.</p>
</abstract>
<funding-group>
<funding-statement>MIT Libraries partially funded the costs for OA publication fees. SMKR received a return-home fellowship grant from the Iranian National Elite Foundation. The funding was not specific to this study, and the funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="0"/>
<page-count count="30"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-05-28</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Dataset related to the manuscript are available at Megocclusion-vr3. RepOD (<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.18150/repod.2004402" xlink:type="simple">http://dx.doi.org/10.18150/repod.2004402</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>There is abundance of feedforward, and recurrent connections in the primate visual cortex [<xref ref-type="bibr" rid="pcbi.1007001.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref002">2</xref>]. The feedforward connections form a hierarchy of cortical areas along the visual pathway, playing a significant role in various aspects of visual object processing [<xref ref-type="bibr" rid="pcbi.1007001.ref003">3</xref>]. However, the role of recurrent connections in visual processing have remained poorly understood [<xref ref-type="bibr" rid="pcbi.1007001.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref007">7</xref>].</p>
<p>Several complementary behavioral, neuronal, and computational modeling studies have confirmed that a large class of object recognition tasks called “core recognition” are largely solved through a single sweep of feedforward visual information processing [<xref ref-type="bibr" rid="pcbi.1007001.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref013">13</xref>]. Object recognition is defined as the ability to differentiate an object’s identity or category from many other objects having a range of identity-preserving changes [<xref ref-type="bibr" rid="pcbi.1007001.ref008">8</xref>]. Core recognition refers to the ability of visual system to rapidly recognize objects despite variations in their appearance, e.g. position, scale, and rotation [<xref ref-type="bibr" rid="pcbi.1007001.ref008">8</xref>].</p>
<p>Object recognition under challenging conditions, such as high variations [<xref ref-type="bibr" rid="pcbi.1007001.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref015">15</xref>], object deletion and occlusion [<xref ref-type="bibr" rid="pcbi.1007001.ref016">16</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref024">24</xref>], and crowding [<xref ref-type="bibr" rid="pcbi.1007001.ref025">25</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref027">27</xref>] goes beyond the core recognition problem, which is thought to require more than the feedforward processes. Object recognition under occlusion is one of the key challenging conditions that occurs in many of the natural scenes we interact with every day. How our brain solves object recognition under such challenging condition is still an open question. Object deletion and object occlusion are shown to have different temporal dynamics [<xref ref-type="bibr" rid="pcbi.1007001.ref028">28</xref>]. While object deletion has been studied before in humans [<xref ref-type="bibr" rid="pcbi.1007001.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref029">29</xref>], we do not know much about the dynamics of object processing under the challenging condition of occlusion; in particular there has not been any previous MEG study of occluded object processing, with multivariate pattern analyses approach, linking models with both brain and behavior. Furthermore, as opposed to the core object recognition problem, where the <bold>conventional</bold> feedforward CNNs are shown to explain brain representations [<xref ref-type="bibr" rid="pcbi.1007001.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref013">13</xref>], we do not yet have computational models that successfully explain human brain representation and behavior under this challenging condition [Regarding feedforward CNNs, also see [<xref ref-type="bibr" rid="pcbi.1007001.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref031">31</xref>] where CNNs cannot fully explain patterns of human behavioral decisions].</p>
<p>Few fMRI studies have investigated how and where occluded objects are represented in the human brain [<xref ref-type="bibr" rid="pcbi.1007001.ref032">32</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref036">36</xref>]. Hulme and Zeki [<xref ref-type="bibr" rid="pcbi.1007001.ref033">33</xref>] found that faces and houses in fusiform face area (FFA) and lateral occipital cortex (LOC) are represented similary with and without occclusion. Ban et al. [<xref ref-type="bibr" rid="pcbi.1007001.ref035">35</xref>] used topographic mapping with simple geometric shapes (e.g. triangles), finding that the occluded portion of the shape is represented topographically in human V1 and V2, suggesting the involvement of early visual areas in object completion. A more recent study showed that the early visual areas may only code spatial information about occluded objects, but not their identity, and higher-order visual areas instead represent object-specific information, such as category or identity of occluded objects [<xref ref-type="bibr" rid="pcbi.1007001.ref036">36</xref>]. While these studies provide insights about object processing under occlusion, they do not provide any information about the temporal dynamics of these processes, and whether object recognition under occlusion requires recurrent processing.</p>
<p>Our focus in this study is understanding the temporal dynamics of object recognition under occlusion; and whether recurrent connections are critical in processing occluded objects? If yes, in what form are they engaged (e.g. long range feedback or local recurrent?), and how much is their contribution compared to the contribution of the feedforward visual information? We constructed a controlled image set of occluded objects, and used the combination of multivariate pattern analyses (MVPA) of MEG signals, computational modeling, backward masking, and behavioral experiments to characterize representational dynamics of object processing under occlusion, and to determine unique contributions of feedforward and recurrent processes.</p>
<p>Here, we provide five complementary evidence for the contribution of recurrent processes in recognizing occluded objects. <italic>First</italic>, MEG decoding time courses show that onset and peak for occluded objects—without backward masking—are significantly delayed compared to when the whole object is presented without occlusion. The timing of visual information plays an important role in discriminating the processing stages (i.e. feedforward vs. recurrent) with early brain responses reaching higher visual areas being dominantly feedforward and the delayed responses being mainly driven by recurrent processes [<xref ref-type="bibr" rid="pcbi.1007001.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref037">37</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref042">42</xref>]. <italic>Second</italic>, time-time decoding analysis (i.e. temporal generalization) suggests that occluded object processing goes through a relatively long sequence of stages that involve recurrent interaction—likely local recurrent. <italic>Third</italic>, the results of backward masking demonstrate that while the masking significantly impairs both human categorization performances and MEG decoding performances under occlusion, it has no significant effect on object recognition when objects are not occluded. <italic>Fourth</italic>, results from two computational models showed that a <bold>conventional</bold> feedforward CNN (AlexNet) that could achieve human-level performance in the no-occlusion condition, performed significantly worse than humans when objects were occluded. Additionally, the feedforward CNN could only explain the human MEG data when objects were presented without occlusion; but failed to explain the MEG data under the occlusion condition. In contrast, a hierarchical CNN with local recurrent connections (recurrent ResNet) achieved human-level performance and the representational geometry of the model was significantly correlated with that of the MEG neural data when objects were occluded. Finally, we quantified contributions of feedforward and recurrent processes in explaining the neural data, showing a significant unique contribution only for the recurrent processing under occlusion. These findings demonstrate significant involvement of recurrent processes in occluded object recognition, and improve our understand of object recognition beyond the core problem. To our knowledge this is the first MVPA study of MEG data linking feedforward and recurrent deep neural network architectures with both brain and behavior to investigate object recognition under occlusion in humans.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>We used multivariate pattern analysis (MVPA) of MEG data to characterize representational dynamics of object recognition under occlusion [<xref ref-type="bibr" rid="pcbi.1007001.ref043">43</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref046">46</xref>]. MEG along with MVPA allows for a fine-grained investigation of the underlying object recognition processes across time [<xref ref-type="bibr" rid="pcbi.1007001.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref047">47</xref>]. Subjects (N = 15) were presented with images of objects with varying levels of occlusion (i.e., 0% = no-occlusion, 60% and 80% occlusion; see <xref ref-type="sec" rid="sec017">Methods</xref>). We also took advantage of the visual backward masking [<xref ref-type="bibr" rid="pcbi.1007001.ref048">48</xref>] as a tool to further control the feedforward and feedback flow of visual information processing. In the MEG experiment, each stimulus was presented for 34 ms [2 × screen frame rate (17ms) = 34ms], followed by a blank-screen ISI, and then in half of the trials followed by a dynamic mask (<xref ref-type="supplementary-material" rid="pcbi.1007001.s001">S1 Fig</xref>). We extracted and pre-processed MEG signals from -200 ms to 700 ms with regard to the stimulus onset. To calculate pairwise discriminability between objects, a support vector machine (SVM) classifier was trained and tested at each time point (<xref ref-type="fig" rid="pcbi.1007001.g001">Fig 1a</xref>). MEG decoding time-courses show the pairwise discriminability of object images averaged across individuals. We first present the MEG results of the no-mask trials. After that in section “The neural basis of masking effect” we discuss the effect of backward masking.</p>
<fig id="pcbi.1007001.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007001.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Temporal dynamics object recognition under various levels of occlusion.</title>
<p><bold>(a)</bold> Multivariate pattern classification of MEG data. We extracted MEG signals from -200 ms to 700 ms relative to the stimulus onset. At each time point (ms resolution), we computed average pairwise classification accuracy between all exemplars. <bold>(b)</bold> Time courses of pairwise decoding accuracies for the three different occlusion levels (without backward masking) averaged across 15 subjects. Thicker lines indicate a decoding accuracy significantly above chance (right-sided signrank test, FDR corrected across time, p &lt; 0.05), showing that MEG signals can discriminate between object exemplars. Shaded error bars represent standard error of the mean (SEM). The two vertical shaded areas show the time from onset to peak, for 60% occluded and 0% occluded objects, which are largely non-overlapping. The onset latency is 79±3 ms (mean ± SD) in the no-occlusion condition; and 123±15 ms in the 60% occlusion; the difference between onset latencies is significant (p&lt;10<sup>−4</sup>, two-sided signrank test). Arrows above the curves indicate peak latencies. The peak latencies are 139±1ms and 199±3ms for the 0% occluded and partially occluded (60%) objects respectively. The difference between the peak latencies is also statistically significant (p &lt; 10<sup>−4</sup>). Images shown to participants are available from here: <ext-link ext-link-type="uri" xlink:href="https://github.com/krajaei/Megocclusion/blob/master/Sample_occlusion_dataset.png" xlink:type="simple">https://github.com/krajaei/Megocclusion/blob/master/Sample_occlusion_dataset.png</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.g001" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>Object recognition is significantly delayed under occlusion</title>
<p>We used pairwise decoding analysis of MEG signals to measure how object information evolves over time (<xref ref-type="fig" rid="pcbi.1007001.g001">Fig 1a</xref>). Significantly above-chance decoding accuracy means that objects can be discriminated using the information available from the brain data at that time-point. The decoding onset latency indicates the earliest time that the object-specific information becomes available and the peak decoding latency is the time-point wherein we have the highest object-discrimination performance.</p>
<p>We found that object information emerges significantly later under occlusion compared to the no-occlusion condition. Object decoding under no-occlusion had an early onset latency at 79ms [±3 ms standard deviation (SD)] and was followed by a sharp increase reaching its maximum accuracy (i.e. peak latency) at 139±1 ms (<xref ref-type="fig" rid="pcbi.1007001.g001">Fig 1b</xref>). This early and rapidly evolving dynamic is well consistent with the known time-course of the feedforward visual object processing [see <xref ref-type="supplementary-material" rid="pcbi.1007001.s002">S2 Fig</xref> and [<xref ref-type="bibr" rid="pcbi.1007001.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref044">44</xref>]].</p>
<p>However, when objects were partially occluded (i.e. 60% occlusion), decoding time-courses were significantly slower than the 0% occlusion condition: the onset for decoding accuracy was at 123±15 ms followed by a gradual increase in decoding accuracy until it reached its peak decoding accuracy at 199±3 ms (<xref ref-type="fig" rid="pcbi.1007001.g001">Fig 1b</xref>). The difference between onset latencies and peak latencies were both statistically significant with p&lt;10<sup>−4</sup> (two sided sign-rank test). Analysis of the behavioral response times was also consistent with the MEG object decoding curves, showing a significant delay in participants’ response times when increasing the occlusion level (<xref ref-type="supplementary-material" rid="pcbi.1007001.s003">S3 Fig</xref>). The slow temporal dynamics of object recognition under occlusion and the observed significant temporal delay in processing occluded objects compared to un-occluded (0% occlusion) objects do not match with a fully feedforward account of visual information processing. Previous studies have shown that first responses to visual stimuli that contain category-related information can reach higher visual areas in as little as 100 ms. [<xref ref-type="bibr" rid="pcbi.1007001.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref050">50</xref>]. Therefore, the observed late onset and the significant delay in peak and onset of the decoding accuracy for occluded objects, may be best explained by the engagement of recurrent processes.</p>
<p>Under 80% occlusion, the MEG decoding results did not reach significance [right-sided signrank test, FDR corrected across time, p &lt; 0.05] (<xref ref-type="fig" rid="pcbi.1007001.g001">Fig 1b</xref>). However, behaviorally, human subjects still performed above-chance in object categorization even under 80% occlusion. This discrepancy might be due to MEG acquisition noise, whereas the behavioral categorization task is by definition free from that type of noise.</p>
<p>While the MEG and behavioral data have different levels of noise, we showed that within the MEG data itself, presented images with different levels of occlusion (0%, 60%, 80%) did not differ in terms of their level of MEG noise (<xref ref-type="supplementary-material" rid="pcbi.1007001.s004">S4 Fig</xref>). Thus, the difference in decoding performance between different levels of occlusion cannot be simply explained by the difference in noise. Furthermore, patterns of cross-decoding analyses (see the next section) demonstrate that the observed delay in peak latency under occlusion cannot be simply explained by a difference in signal strength.</p>
</sec>
<sec id="sec004">
<title>Time-time decoding analysis for occluded objects suggests a neural architecture with recurrent interactions</title>
<p>We performed time-time decoding analysis measuring how information about object discrimination generalizes across time (<xref ref-type="fig" rid="pcbi.1007001.g002">Fig 2a</xref>). Time-time decoding matrices are constructed by training a SVM classifier at a given time point and testing its generalization performance at all other time-points (see <xref ref-type="sec" rid="sec017">Methods</xref>). The pattern of temporal generalization provides useful information about the underlying processing architecture [<xref ref-type="bibr" rid="pcbi.1007001.ref051">51</xref>].</p>
<fig id="pcbi.1007001.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007001.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Temporal generalization patterns of object recognition with and without occlusion.</title>
<p><bold>(a)</bold> Time-time decoding analysis. The procedure is similar to the calculations of pairwise decoding accuracies explained in <xref ref-type="fig" rid="pcbi.1007001.g001">Fig 1</xref>, except that here the classifier is trained at a given time-point, and then tested at all time-points. In other words, for each pair of time points (t<sub>x</sub>, t<sub>y</sub>), a SVM classifier is trained by N-1 MEG pattern vectors at time t<sub>x</sub> and tested by the remaining one pattern vector at time t<sub>y</sub>, resulting to an 801x801 time-time decoding matrix. <bold>(b-c)</bold> Time-time decoding accuracy and plausible processing architecture for no-occlusion and 60% occlusion. The results are for MEG trials without backward masking. Horizontal axis indicates testing times and vertical axis indicates training times. Color bars represent percent of decoding accuracies (chancel level = 50%); please note that in the time-time decoding matrices, the color bar ranges for 0% occlusion and 60% occlusion are different. Within the time-time decoding matrices, significantly above chance decoding accuracies, are surrounded by the white dashed contour lines (right-sided signrank test, FDR corrected across the whole 801x801 decoding matrix, p &lt; 0.05). For each time-time decoding matrix, we also show the plausible processing architecture corresponding to it. These are derived from the observed patterns of temporal generalization from onset-to-peak decoding (shown by the gray dashed rectangles) [see Fig 5 of [<xref ref-type="bibr" rid="pcbi.1007001.ref052">52</xref>]]. Generalization patterns for the no-occlusion condition are consistent with a hierarchical feedforward architecture; whereas, for the occluded objects (60%) the temporal generalization patterns are consistent with a hierarchical architecture with recurrent connections. <bold>(d)</bold> Difference in time-time decoding accuracies between no-occlusion and occlusion conditions. Significantly above zero differences are surrounded by the white dashed contour lines (right-sided signrank test, FDR corrected across [80–240]ms matrix at p &lt; 0.05).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.g002" xlink:type="simple"/>
</fig>
<p>We were interested to see if there are differences between temporal generalization patterns of occluded and un-occluded objects. Different processing dynamics may lead to distinct patterns of generalization in the time-time decoding matrix [see [<xref ref-type="bibr" rid="pcbi.1007001.ref051">51</xref>] for a review]. For example, a narrow diagonal pattern suggests a hierarchical sequence of processing stages wherein information is sequentially transferred between neural stages. This hierarchical architecture is well consistent with the feedforward account of neural information processing across the ventral visual pathway. On the other hand, a time-time decoding pattern with off-diagonal generalization suggests a neural architecture with recurrent interactions between processing stages [see Fig 5 in [<xref ref-type="bibr" rid="pcbi.1007001.ref052">52</xref>]].</p>
<p>The temporal generalization pattern under no-occlusion (<xref ref-type="fig" rid="pcbi.1007001.g002">Fig 2b</xref>) indicated a sequential architecture, without an off-diagonal generalization until its early peak latency at 140 ms. This is consistent with a dominantly feedforward account of visual information processing. There was some off-diagonal generalization after 140 ms, however that was not of interest here, because the ongoing recurrent activity after the peak latency (as shown in <xref ref-type="fig" rid="pcbi.1007001.g001">Fig 1b</xref>) did not carry any information that further improves pairwise decoding performance of 0% occluded objects. On the other hand, when objects were occluded, the temporal generalization matrix (<xref ref-type="fig" rid="pcbi.1007001.g002">Fig 2c</xref>) indicated a significantly delayed peak latency at 199ms with extensive off-diagonal generalization before reaching its peak. In other words, for occluded objects, we see a discernible pattern of temporal generalization, which is characterized by 1) a relatively weak early diagonal pattern of the decoding accuracy during [100 150]ms with limited temporal generalization, which is in contrast with the high accuracy decoding of 0% occluded objects in the same time period. 2) A relatively late peak decoding accuracy with a wide generalization pattern around 200ms. This pattern of temporal generalization can be simulated by a hierarchical neural architecture with local recurrent interactions within the network [Fig 5 of [<xref ref-type="bibr" rid="pcbi.1007001.ref052">52</xref>]]</p>
<p>We also performed sensorwise decoding analysis to explore spatio-temporal dynamics of object information. To calculate sensorwise decoding, pairwise decoding analysis was conducted on 102 neighboring triplets of MEG sensors (2 gradiometers and 1 magnetometer in each location) yielding a decoding map of brain activity at each time-point. The sensorwise decoding patterns indicated the approximate locus of neural activity: in particular, we see that for both 0% occlusion (<xref ref-type="supplementary-material" rid="pcbi.1007001.s015">S2 Movie</xref>) and 60% occlusion (<xref ref-type="supplementary-material" rid="pcbi.1007001.s014">S1 Movie</xref>) conditions, during the onset of decoding as well as the peak decoding time, the main source of object decoding is in the left posterior-temporal sensors. From [110ms to 200ms], the peak of decoding accuracy remains locally around the same sensors, suggesting a sustained local recurrent activity.</p>
<sec id="sec005">
<title>Generalization across time and occlusion levels</title>
<p>Time-time decoding analyses can be further expanded by training the classifiers in one condition (e.g. occluded) and testing their ability to generalize to the other condition (e.g. un-occluded). The resulting pattern of generalization across time and occlusion level provides diagnostic information about the organization of brain processes (<xref ref-type="fig" rid="pcbi.1007001.g003">Fig 3</xref>). In particular, this can provide us with further evidence as to whether the observed decoding results under occlusion is due to changes in activation intensity (e.g. weakening of the signal), or a genuine difference in latency. As shown in ([<xref ref-type="bibr" rid="pcbi.1007001.ref051">51</xref>], <xref ref-type="fig" rid="pcbi.1007001.g004">Fig 4</xref>) each of these two come with distinct cross-condition generalization matrices. A change of signal intensity leads to asymmetric generalizations across conditions because it can be more efficient to train a decoder with relatively high signal-to-noise ratio (SNR) data (e.g. 0% occlusion) and generalize it to low SNR data (e.g. 60% occlusion) rather than vice versa. However, in <xref ref-type="fig" rid="pcbi.1007001.g003">Fig 3</xref>, we do not see such asymmetry in decoding strengths, instead we observe different generalization patterns that are more consistent with changes in the speed of information processing (i.e. latency). More specifically, when the classifier is trained with 0% occlusion and tested on 60% occlusion (upper-left matrix in <xref ref-type="fig" rid="pcbi.1007001.g003">Fig 3a</xref>) the generalization pattern is shifted to the above diagonal and vice versa when trained with 60% occlusion and tested on 0% occlusion (the lower-right matrix).</p>
<fig id="pcbi.1007001.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007001.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Generalization across time and occlusion levels.</title>
<p><bold>(a)</bold> The classifier is trained on an occlusion level (e.g. 0% occlusion) and tested on the other occlusion level (e.g. 60% occlusion). Time-points with significant decoding accuracy are shown inside the dashed contours (right-sided signrank test, FDR-corrected across time, p&lt;0.05). The contour of significant time-points has a shift towards the upper side of the diagonal when the classifier is trained with 0% occlusion and tested on 60% occlusion (i.e. 63% of significant time points are above the diagonal) whereas in the lower right matrix we see the opposite pattern (66% of significant time points are located below the diagonal). <bold>(b)</bold> The two color maps below the decoding matrices show the difference between the two decoding matrices located above them.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.g003" xlink:type="simple"/>
</fig>
<fig id="pcbi.1007001.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007001.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Backward masking significantly impairs object decoding under occlusion, but has no significant effect on object decoding under no occlusion.</title>
<p><bold>(a)</bold> Time-courses of the average pairwise decoding accuracies under no-occlusion. Thicker lines indicate significant time-points (right-sided signrank test, FDR corrected across time, p &lt; 0.05). Shaded error bars indicate SEM (standard error of the mean). Downward pointing arrows indicate peak decoding accuracies. There is no significant difference between decoding time-courses for mask and no-mask trials, under no-occlusion <bold>(b)</bold> Time-courses of the average pairwise decoding under 60% occlusion (for 80% occlusion see <xref ref-type="supplementary-material" rid="pcbi.1007001.s005">S5 Fig</xref>). Under occlusion, the decoding onset latency for the no-mask trials is 123±15ms, with its peak decoding accuracy at 199±3ms; whereas the time-course for the masked trials does not reach statistical significance, demonstrating that backward masking significantly impairs object recognition under occlusion. Black horizontal lines below the curves show the time-points at which the two decoding curves are significantly different. This is particularly evident around the peak latency of the no-mask trials [from 185ms to 237ms]. <bold>(c, d)</bold> Time-time decoding matrices of 60% occluded and (0%) un-occluded objects with and without backward masking. Horizontal axes indicate testing times and the vertical axes indicate training times. Color bars show percent of decoding accuracies. Please note that in the time-time decoding matrices, the color bar ranges for 0% occlusion and 60% occlusion are different. Significantly above chance decoding accuracies, are surrounded by the white dashed contour lines (right-sided signrank test, FDR corrected across the whole 801x801 decoding matrix, p &lt; 0.05). <bold>(f)</bold> Difference between time-time decoding matrices with and without backward masking. Statistically significant differences are surrounded by the black dotted contours (right-sided signrank test, FDR corrected across time at p &lt; 0.05). There are significant differences between mask and no-mask only under occlusion.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.g004" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec006">
<title>Backward masking significantly impaired object recognition only under occlusion</title>
<p>Visual backward masking has been used as a tool to disrupt the flow of recurrent information processing, while feedforward processes are left relatively intact [<xref ref-type="bibr" rid="pcbi.1007001.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref053">53</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref056">56</xref>]. Our time-time decoding results (<xref ref-type="fig" rid="pcbi.1007001.g004">Fig 4d</xref> 0% occluded) additionally supports the recurrent explanation of backward masking: off-diagonal generalization in time-time decoding matrices are representative of recurrent interactions; these off-diagonal components disappear when backward masking is present.</p>
<p>Considering the recurrent explanation of the masking effect, we further examined how the recurrent processes contribute in object processing under occlusion. We found that backward masking significantly reduced both MEG decoding accuracy time-course (<xref ref-type="fig" rid="pcbi.1007001.g004">Fig 4b</xref>) and subjects’ behavioral performances (<xref ref-type="fig" rid="pcbi.1007001.g005">Fig 5d</xref>), only when objects were occluded. When occluded objects are masked, the MEG decoding time-course from 185ms to 237ms is significantly lower than the decoding time-course when in no-mask condition (<xref ref-type="fig" rid="pcbi.1007001.g004">Fig 4b</xref>, black horizontal lines; two-sided signrank test, FDR-corrected across time p &lt; 0.05). On the other hand, for un-occluded objects, we did not find any significant difference between decoding time-courses of the mask and no-mask conditions (<xref ref-type="fig" rid="pcbi.1007001.g004">Fig 4a</xref>).</p>
<fig id="pcbi.1007001.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007001.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Comparing human MEG and behavioral data with feedforward and recurrent computational models of visual hierarchy.</title>
<p><bold>(a)</bold> Time-varying representational similarity analysis between human MEG data and the computational models. We, first, obtained representational dissimilarity matrices (RDM) for each computational model—using feature values of the layer before the softmax operation—, and for the MEG data at each time-point. For each subject, their MEG RDMs were correlated (Spearman’ R) with the computational model RDMs (i.e. AlexNet &amp; HRRN) across time; the results were then averaged across subjects. <bold>(b, c)</bold> Time-courses of RDM correlations between the models and the human MEG data. HRRN readout stage 0 represents the purely feedforward version of HRRN. Thicker lines show significant time points (right-sided signrank test, FDR-corrected across time, p &lt; = 0.05). We indicate peak correlation latencies by numbers (mean ± SD) above the downward pointing arrows. Under no-occlusion, AlexNet and HRRN demonstrate almost similar time-courses except that the peak latency for HRRN (249±10ms) is significantly later than the peak latency for AlexNet (219±12ms). However, under occlusion, only HRRN showed significant correlation with MEG data, with a peak latency of 182±19ms. <bold>(d)</bold> Object recognition performance of humans (mask and no-mask trials) and models [AlexNet and HRRN-ReadoutStage-0 (feedforward) and HRRN(recurrent)] across different levels of occlusion. We evaluated model accuracies on a multiclass recognition task similar to the multiclass behavioral experiment done in humans (<xref ref-type="supplementary-material" rid="pcbi.1007001.s006">S6 Fig</xref>). The models’ performances were calculated by holding out an occlusion level for testing, and training a SVM classifier on the remaining levels of occlusion. Error bars are SEM.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.g005" xlink:type="simple"/>
</fig>
<p>Consistent with the MEG decoding results, while the masking significantly reduced behavioral categorization performances when objects were occluded, it had no significant effect on the categorization performance for the un-occluded objects (<xref ref-type="fig" rid="pcbi.1007001.g005">Fig 5d</xref>) [two-sided signrank test]. Particularly, the backward masking removed the late MEG decoding peak (around 200ms) under occlusion (<xref ref-type="fig" rid="pcbi.1007001.g004">Fig 4f</xref>) likely due to disruption of later recurrent interactions.</p>
<p>Taken together, we demonstrated that visual backward masking, which is suggested to disrupt recurrent processes [<xref ref-type="bibr" rid="pcbi.1007001.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref055">55</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref057">57</xref>], significantly impairs object recognition only under occlusion. On the other hand, masking did not affect object processing under no occlusion, when information from the feedforward sweep is shown to be sufficient for object recognition. Thus, providing further evidence for the essential role of recurrent processes in object recognition under occlusion.</p>
</sec>
<sec id="sec007">
<title>How well does a computational model with local recurrent interactions explain neural and behavioral data under occlusion?</title>
<p>Recent studies have shown that convolutional neural networks (CNNs) achieve human-level performance and explain neural data under non-challenging conditions—also referred to as the core object recognition [<xref ref-type="bibr" rid="pcbi.1007001.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref011">11</xref>]. Here, we first examined whether such feedforward CNNs (i.e. AlexNet) can explain the observed human neuronal and behavioral data in a challenging object recognition task when objects are occluded. The model accuracy was evaluated by the same object recognition task used to measure human behavioral performance (<xref ref-type="supplementary-material" rid="pcbi.1007001.s006">S6 Fig</xref>). A multiclass linear SVM classifier was trained on images from two occlusion levels and tested on the left-out occlusion level, using features from the penultimate layer of the model (e.g. ‘fc7’ in AlexNet). The classification task was to categorize images into car, motor, deer, or camel. This procedure was repeated for 15 times, and the mean categorization performance is reported here.</p>
<p>We, additionally, used representational similarity analysis (RSA) to assess model’s performance in explaining the human MEG data. RSA correlates time-resolved human MEG representations with that of the model, on the same set of stimuli. First, dissimilarity matrices (RDMs) were separately calculated for the MEG signals and the model. The model RDM were then correlated with MEG RDMs across time (<xref ref-type="fig" rid="pcbi.1007001.g005">Fig 5a</xref>; also see <xref ref-type="sec" rid="sec017">Methods</xref>).</p>
<p>We found that in the no-occlusion condition, the feedforward CNN achieved human-level performance and CNN representations were significantly correlated with the MEG data. Significant correlation between the model and MEG representational dissimilarity matrices (RDMs) started at ~90ms after the stimulus onset and remained significant for several hundred milliseconds with two peaks at 150ms and 220ms (<xref ref-type="fig" rid="pcbi.1007001.g005">Fig 5b</xref>). However, the feedforward CNNs (i.e. AlexNet and the purely feedforward version of HRRN (HRRN with readout stage 0)) failed to explain human MEG data when objects were occluded. And the model performance was significantly lower than that of human in the occluded object recognition task.</p>
<p>We were wondering if a model with local recurrent connections could account for object recognition under occlusion. Inspired by recent advancements in deep convolutional neural networks [<xref ref-type="bibr" rid="pcbi.1007001.ref058">58</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref061">61</xref>], we built a hierarchical recurrent ResNet (HRRN) that follows the hierarchy of the ventral visual pathway (<xref ref-type="fig" rid="pcbi.1007001.g006">Fig 6</xref>, also see <xref ref-type="sec" rid="sec017">Methods</xref> for more details about the model). The recurrent model (HRRN) could rival the human performance in the occluded object recognition task (<xref ref-type="fig" rid="pcbi.1007001.g005">Fig 5d</xref>), performing strikingly better than AlexNet in 60% and 80% occlusion. We also compared confusion matrices (patterns of errors) between the models and human (<xref ref-type="supplementary-material" rid="pcbi.1007001.s007">S7 Fig</xref>). Under the no-mask condition, HRNN had a significantly higher correlation with humans under 0% and 80% occlusion (the difference was not significant in 60% occlusion, <xref ref-type="supplementary-material" rid="pcbi.1007001.s007">S7b Fig</xref>).</p>
<fig id="pcbi.1007001.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007001.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Hierarchical recurrent ResNet (HRRN) in unfolded form is equivalent to an ultra-deep ResNet.</title>
<p><bold>(a)</bold> A hierarchy of convolutional layers with local recurrent connections. This hierarchical structure models the feedforward and local recurrent connections along the hierarchy of ventral visual pathway (e.g. V1, V2, V4, IT). <bold>(b)</bold> Each recurrent unit is equivalent to a deep ResNet with arbitrary number of layers depending on the unfolding depth. <italic>h</italic><sub><italic>t</italic></sub> is the layer activity at a specific time (t) and K<sub>t</sub> represents a sequence of nonlinear operations (e.g. convolution, batch normalization, and ReLU). [see [<xref ref-type="bibr" rid="pcbi.1007001.ref063">63</xref>] for more info].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.g006" xlink:type="simple"/>
</fig>
<p>Additionally, the HRRN model representation was significantly correlated with that of the human MEG data under occlusion [<xref ref-type="fig" rid="pcbi.1007001.g005">Fig 5c</xref>] (onset = 138±2ms; peak = 182±19ms). It is worth noting that the recurrent model here may be considered functionally equivalent to an arbitrarily deep feedforward CNN. We think the key difference between AlexNet and HRRN, is indeed in the number of non-linear transformations applied to the input image (please refer to section “Does a feedforward system with arbitrarily long depth work the same as a recurrent system with limited depth? “for a discussion about this).</p>
<p>The models here, the purely feedforward models (i.e. HRRN readout stage 0, and AlexNet) and the model with local recurrent connections, were both trained on the same object image dataset [ImageNet [<xref ref-type="bibr" rid="pcbi.1007001.ref062">62</xref>]] and had equal number of feedforward convolutional layers. Both models performed similarly in object recognition under no-occlusion, and achieved human-level performance. However, under occlusion, only the HRRN (i.e. the model with recurrent connections) could partially explain the human MEG data and achieved human-level performance, whereas the purely feedforward models failed to achieve human-level performance under occlusion—in both MEG and behavior.</p>
</sec>
<sec id="sec008">
<title>Contribution of feedforward and recurrent processes in solving object recognition under occlusion</title>
<p>To quantify the contribution of feedforward and recurrent processes in solving object recognition under occlusion, we first correlated the feedforward and recurrent model RDMs with the average MEG RDMs extracted from two time spans: 80 to 150 ms, which is dominantly feedforward [<xref ref-type="bibr" rid="pcbi.1007001.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref050">50</xref>], and 151 to 300 ms (significant involvement of recurrent processes). The results are shown in <xref ref-type="fig" rid="pcbi.1007001.g007">Fig 7a</xref>. HRRN and AlexNet both have a similar correlation with the MEG data at [80–150 ms]. However, the HRRN shows a significantly higher correlation with the MEG data at [151–300 ms] compared to the AlexNet.</p>
<fig id="pcbi.1007001.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007001.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Contribution of the feedforward and recurrent models in explaining MEG data under 60% occlusion.</title>
<p><bold>(a)</bold> Correlation between the models RDMs and the average MEG RDM over two different time bins. <bold>(b)</bold> Unique contribution of each model (semipartial correlation) in explaining the MEG data. Error bars represent SEM (Standard Error of the Mean). Significantly above zero correlations/semipartial-correlations and significant differences between the two models are indicated by stars. * = p&lt;0.05; ** = p&lt;0.01; *** = p&lt;0.001.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.g007" xlink:type="simple"/>
</fig>
<p>We were further interested to determine the unique contribution of the models in explaining the neural data under occlusion. To this end, we calculated semipartial correlations between the model RDMs and the MEG RDMs (<xref ref-type="fig" rid="pcbi.1007001.g007">Fig 7b</xref>). We find that the HRRN and AlexNet perform similarly in explaining the mainly feedforward component (i.e. 80-150ms) of the MEG data and they do not have a significant unique contribution. On the other hand, for the later component (150–300 ms) of the MEG data, only the HRRN model has a unique contribution.</p>
</sec>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Discussion</title>
<p>We investigated how the human brain processes visual objects under occlusion. Using multivariate MEG analysis, behavioral data, backward masking and computational modeling, we demonstrated that recurrent processing plays a major role in object recognition under occlusion.</p>
<p>One of the unique advantages of the current work in comparison to a number of previous studies that have investigated object recognition under partial visibility [<xref ref-type="bibr" rid="pcbi.1007001.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref029">29</xref>] was bringing together neural data, behavioral data and computational models in a single study. This enabled us to study the link between brain and behavior and propose a neural architecture that is consistent with both. Whereas previous studies either related models with behavior (e.g. [<xref ref-type="bibr" rid="pcbi.1007001.ref020">20</xref>]), missing the link with brain data; or otherwise highlighting the role of recurrent processes using neural data [<xref ref-type="bibr" rid="pcbi.1007001.ref029">29</xref>] without suggesting a computational mechanism that could explain the data and the potential underlying mechanisms (however, please also see the recently published [<xref ref-type="bibr" rid="pcbi.1007001.ref024">24</xref>]).</p>
<p>Another unique advantage of the current study was the comparison of different deep neural network architectures and comparing their ability in explaining neural and behavioral data under occlusion. To our knowledge this was the first work that specifically compared deep convolutional networks with neural data in occluded object processing.</p>
<sec id="sec010">
<title>Beyond core object recognition</title>
<p>Several recent findings have indicated that a large class of object recognition tasks referred to as ‘core object recognition’ are mainly solved in the human brain within the first ~100 ms after stimulus onset [<xref ref-type="bibr" rid="pcbi.1007001.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref064">64</xref>], largely associated with the feedforward path of visual information processing [<xref ref-type="bibr" rid="pcbi.1007001.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref012">12</xref>]. More challenging tasks, such as object recognition under occlusion, go beyond the core recognition problem. So far it has been unclear whether the visual information from the feedforward sweep can fully account for this or otherwise recurrent information are essential to solve object recognition under occlusion.</p>
<sec id="sec011">
<title>Temporal dynamics</title>
<p>We found that under the no-occlusion condition, the MEG object-decoding time-course peaked at 140ms with an early onset at 79ms, consistent with findings from previous studies [<xref ref-type="bibr" rid="pcbi.1007001.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref043">43</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref045">45</xref>]. Intracranial recordings in human visual cortex have shown that early responses to visual stimuli can reach higher visual areas in as little as 100 ms, with a peak decoding performance after 150 ms [<xref ref-type="bibr" rid="pcbi.1007001.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref050">50</xref>]. These results suggest that category-related information around ~100 ms (±20 ms) after stimulus onset are mainly driven by feedforward processes (see <xref ref-type="supplementary-material" rid="pcbi.1007001.s002">S2 Fig</xref> for time course of visual processing in human). M/EEG studies in humans have additionally shown that approximately after 150 ms, a very complex and dynamic phase of visual processing may strengthen the category-specific semantic representations [<xref ref-type="bibr" rid="pcbi.1007001.ref065">65</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref066">66</xref>], which are likely to be driven by recurrent [<xref ref-type="bibr" rid="pcbi.1007001.ref049">49</xref>]. In our study, when objects were occluded, object decoding accuracy peaked at 200ms, with a late onset at 123ms—significantly later than the peak and onset under the no-occlusion condition (i.e. 140ms and 79ms)—suggesting the involvement of recurrent processes. Given the results from the temporal generalization analysis (<xref ref-type="fig" rid="pcbi.1007001.g002">Fig 2c</xref>), and the computational modelling, we argue for the engagement of mostly <italic>local</italic> recurrent connections as opposed to long-range top-down feedback in solving object recognition under occlusion for this image set. Previous studies also suggest that long-range top-down recurrent (e.g. PFC to IT) is prominently engaged after 200ms from stimulus onset [<xref ref-type="bibr" rid="pcbi.1007001.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref067">67</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref069">69</xref>]. However, we do not rule out the possibility of involvement of long-range feedback in processing occluded objects, specifically when stimuli become more complicated (e.g. when objects are presented on natural backgrounds).</p>
<p>The additional time needed for processing occluded objects may facilitate object recognition by providing integrated semantic information from visible parts of the target objects, for example, via local recurrent in higher visual areas in the form of attractor networks [<xref ref-type="bibr" rid="pcbi.1007001.ref070">70</xref>]. In other words, partial semantic information (e.g. having wheels, having legs, etc.) may activate prior information associated with the category of the target object [<xref ref-type="bibr" rid="pcbi.1007001.ref066">66</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref071">71</xref>]. Overall these suggest the observed temporal delay under 60% occlusion can be best explained by the engagement of recurrent processes—mostly local recurrent connections.</p>
</sec>
<sec id="sec012">
<title>Computational modeling</title>
<p>Feedforward CNNs have been shown to be able to account for the core object recognition [<xref ref-type="bibr" rid="pcbi.1007001.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref072">72</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref076">76</xref>]. The natural question to ask next is whether these models perform similarly well under more challenging conditions, beyond the core object recognition. To address this, we compared a conventional feedforward CNN with a recurrent convolutional network in terms of their object recognition performance, and their representational similarity with that of the human MEG data, under the challenging condition of occlusion. The feedforward CNN only achieved human-level performance when objects were not occluded; and performed significantly lower than the humans and the recurrent network when objects were occluded. The conventional feedforward CNN also failed to explain human neural data when objects were occluded. On the other hand, the convolutional network with local recurrent connections could achieve human-level performance in occluded object recognition and explained a significant variance of the human neural data. Thus, demonstrating that the conventional feedforward CNNs (such as AlexNet) do not account for object recognition under such challenging conditions, where recurrent computations have a prominent contribution.</p>
</sec>
</sec>
<sec id="sec013">
<title>Unique contribution of recurrent processes in solving object recognition under occlusion</title>
<p>AlexNet (i.e. feedforward model) and HRRN (i.e. recurrent model) both equally explained the early component of the MEG data (&lt;150 ms) with and without occlusion. Consistent with this, the semipartial correlation analyses further revealed no unique variance for these models in explaining the early component of the MEG data. These results suggest that the early component of the MEG data under both conditions (with and without occlusion) are mainly feedforward and both AlexNet and HRRN share a common feedforward component that is significantly correlated with dominantly feedforward MEG representations before 150 ms (<xref ref-type="supplementary-material" rid="pcbi.1007001.s008">S8 Fig</xref> shows a plausible Venn diagram describing the relationship between the two models and the MEG data.).</p>
<p>On the other hand, the later component of the MEG data (&gt;150 ms) under occlusion was only correlated with the recurrent model, which had a significant unique contribution in explaining the MEG data under this condition. Under no occlusion, while the later component of the MEG data is significantly correlated with both AlexNet and HRRN, only the HRRN model showed a significant unique contribution in explaining the data (<xref ref-type="supplementary-material" rid="pcbi.1007001.s009">S9 Fig</xref>). This shows that under no-occlusion the later component of the MEG data can still be partially explained by the common feedforward component of the two models, perhaps because object recognition under no-occlusion is primarily a feedforward process, however the recurrent model has some unique advantages in explaining later MEG components—even under no-occlusion.</p>
</sec>
<sec id="sec014">
<title>Object occlusion vs. object deletion</title>
<p>Object recognition when part of an object is removed without an occluder is one of the challenging conditions that has been previously studied [<xref ref-type="bibr" rid="pcbi.1007001.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref029">29</xref>] and may partly look similar to occlusion. However, as shown by Johnson and Olshausen [<xref ref-type="bibr" rid="pcbi.1007001.ref028">28</xref>] deleting part of an object is different from occluding it with another object; even under short stimulus presentation times, there are meaningful differences between occlusion and deletion at the level of behavior (reaction time and accuracy of responses). Furthermore, Johnson and Olshausen report significant diffrences in ERP responses between occlusion and deletion, observed as early as ~130ms after simulus onset. See <xref ref-type="supplementary-material" rid="pcbi.1007001.s010">S10 Fig</xref> for sample images of occlusion and deletion. Occlusion occurs when an object or shape appears in front of another one [<xref ref-type="bibr" rid="pcbi.1007001.ref028">28</xref>], in which case the occluding object might serve as an additional depth-base image cue for object completion. On the other hand, deletion occurs when part of an object is removed without additional cues about the shape or the place of the missing part. Occlusion is closely related with amodal completion which is an important visual process for perceptual filling-in of missing parts of an occluded object [<xref ref-type="bibr" rid="pcbi.1007001.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref077">77</xref>]. Given the difference between these two phenomena at the level of stimulus set, we expect the dynamics of object processing (and the underlying computational mechanisms) to be also different when part of an object is occluded compared to when it is deleted. Consistent with this, Johnson and Olshausen [<xref ref-type="bibr" rid="pcbi.1007001.ref028">28</xref>] demonstrated that ERP responses in occipital and parietal electrodes are signifcantly different between object occlusion and deletion. Furthermore, there were significant behavioural differences between object occlusion and deletion, including differneces in recogntion memory and response confidence.</p>
<p>Object deletion has been previously studied in humans using a variety of techniques: Wyatte et. al. [<xref ref-type="bibr" rid="pcbi.1007001.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref020">20</xref>] used human behaviour (in a backward masking paradigm), and computational modelling to show that object recogntion, when part of the object is deleted, requires recurrent processing. Tang et. al. [<xref ref-type="bibr" rid="pcbi.1007001.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref029">29</xref>] used intracranial field potential recording on epileptic patients to study temporal dynamics of object deletion; and proposed an attractor-based recurrent model that could explain the neural data. They found ~100 ms delay in processing objects when part of the object was deleted, compared to when the whole object was present. In comparison, in our study we found ~60 ms delay in object processing when objects were occluded. This suggests that while object recognition under both occlusion and deletion requires recurrent processes, temporal dynamics of object deletion is slower, potentially due to the absence of the occluder, which can make the recognition task more difficult.</p>
<p>To summarize, while object deletion has been previously studied in humans, to our knowledge, temporal dynamics of object occlusion had not been studied before. In particular this was the first MVPA study in humans that charachaterized representational dynamics of object recognition under occlusion, and further provided a computational account of the underlying processes that explained both behavioral and neural data.</p>
</sec>
<sec id="sec015">
<title>Does a feedforward system with arbitrarily long depth work the same as a recurrent system with limited depth?</title>
<p>While conventional CNNs could not account for object recognition beyond the core recognition problem, we do not rule out the possibility that much deeper CNNs could perform better under such challenging conditions.</p>
<p>Computational studies have shown that very deep CNNs outperform shallow ones on a variety of object recognition tasks [<xref ref-type="bibr" rid="pcbi.1007001.ref078">78</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref080">80</xref>]. Specifically, residual learning allows for a much deeper neural network with hundreds [<xref ref-type="bibr" rid="pcbi.1007001.ref058">58</xref>] and even thousands [<xref ref-type="bibr" rid="pcbi.1007001.ref059">59</xref>] of the layers providing better performance. This is due to the fact that the complex functions that can be represented by deeper architectures cannot be represented by shallow architectures [<xref ref-type="bibr" rid="pcbi.1007001.ref081">81</xref>]. Recent computational modeling studies have tried to clarify why increasing the depth of a network can improve its performance [<xref ref-type="bibr" rid="pcbi.1007001.ref060">60</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref063">63</xref>]. These efforts have demonstrated that unfolding a recurrent architecture across time leads to a feedforward network with arbitrary depth, in which the weights are shared among the layers. Although such a recurrent network has far fewer parameters, Liao and Poggio [<xref ref-type="bibr" rid="pcbi.1007001.ref060">60</xref>] have empirically shown that it performs as well as a very deep feedforward network <italic>without</italic> shared weights. We also showed that a very deep ResNet (e.g. with 150 layers) can be reformulated into the form of a recurrent CNN with much fewer layers (e.g. five layers) (<xref ref-type="fig" rid="pcbi.1007001.g006">Fig 6</xref>). Thus, a compact architecture that resembles these very deep networks in terms of performance is a recurrent hierarchical network with much fewer layers. This compact architecture is probably what the human visual system has selected to be like [<xref ref-type="bibr" rid="pcbi.1007001.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref002">2</xref>], given the biological constraints of having a very deep neural network inside the brain [<xref ref-type="bibr" rid="pcbi.1007001.ref082">82</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref086">86</xref>].</p>
<p>From a computational viewpoint, recognition of complex images might require more processing efforts; in other words, they might need to go through more layers of processing to be prepared for the final readout. Similarly, in a recurrent architecture, more processing means more iterations. Our modeling results supports this assumption, showing that under more challening recogntion tasks, more iterations are required to reach human-level perfomrance. For example, under 60% and 80% occlusion, the HRRN model reached human level performance, respectively after going through 13 recurrent stages, and 43 recurrent stages (<xref ref-type="supplementary-material" rid="pcbi.1007001.s011">S11 Fig</xref>). With more iterations, the HRRN model tends to achieve a performance slightly better than the average categorization performance in humans.</p>
<p>Our choice of a recurrent architecture, as opposed to an arbitrarily deep neural network, is mainly driven by the plausibly of such architecture with the hierarchy of vision, where there is only a limited number of processing layers. However, in terms of performance in real-world object recognition tasks (e.g. object recognition under occlusion), the key in achieving a good performance is the number of non-linear operations, which can come either in the form of deeper networks in a feedforward architecture or otherwise more iterations in a recurrent architecture.</p>
</sec>
<sec id="sec016">
<title>The neural basis of masking effect</title>
<p>Backward masking is a useful tool for studying temporal dynamics of visual object processing [<xref ref-type="bibr" rid="pcbi.1007001.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref053">53</xref>]. It can impair recognition of the target object and reduce or eliminate perceptual visibility through the presentation of a second stimulus (mask) immediately or with an interval after the target stimulus, e.g. 50 ms after the target’s onset. While the origin of masking effect was not the focus of the current study, our MEG results could provide some insights about the underlying mechanisms of backward masking.</p>
<p>There are several accounts of backward masking in the literature: Breitmeyer and Ganz [<xref ref-type="bibr" rid="pcbi.1007001.ref087">87</xref>] provided a purely feedforward explanation (two-channel model), arguing that the mask travels rapidly through the fast channel disrupting recognition of the target object traveling through the slow channel. A number of other studies, however, suggest that the masking mainly interferes with the top-down feedback processes [<xref ref-type="bibr" rid="pcbi.1007001.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref055">55</xref>]. And finally, Macknik and Martinez-Conde [<xref ref-type="bibr" rid="pcbi.1007001.ref057">57</xref>] explain the masking effect by the lateral inhibition mechanism of neural circuits within different levels of the visual hierarchy; arguing that the mask interferes with the recognition of the target object through lateral inhibition (i.e. inhibitory interactions between target and mask).</p>
<p>The last two accounts of masking, while being different, both argue for the disruption of recurrent processes by the mask: either the top-down recurrent processes, or the local recurrent processes (e.g. lateral interactions). With a short interval between the target and mask, the mask may interfere with the fast recurrent processes (i.e. local recurrent) while with a relatively long interval it may interfere with the slow recurrent processes (i.e. top-down feedback).</p>
<p>Our results of MEG decoding time-courses, time-time decoding and behavioral performances under the no-occlusion condition does not support the purely feedforward account of visual backward masking. We showed that the backward masking did not have a significant effect on disrupting the fast feedforward processes of object recognition under no occlusion (MEG: <xref ref-type="fig" rid="pcbi.1007001.g004">Fig 4a</xref>; behaviorally: <xref ref-type="fig" rid="pcbi.1007001.g005">Fig 5d</xref>). On the other hand, when objects were occluded the backward masking significantly impaired object recognition both behaviorally (<xref ref-type="fig" rid="pcbi.1007001.g005">Fig 5d</xref>) and neurally (<xref ref-type="fig" rid="pcbi.1007001.g004">Fig 4b</xref>). Additionally, the time-time decoding results (<xref ref-type="fig" rid="pcbi.1007001.g004">Fig 4c, 4d and 4f</xref>) showed that backward masking, under no occlusion, had no significant effect on disrupting the diagonal component of the temporal generalization matrix that is mainly associated with the feedforward path of visual processing. On the other hand, the masking removed the off-diagonal components and the late peak (&gt;200ms) observed in the temporal generalization matrix of the occluded objects.</p>
<p>Taken together, our MEG and behavioral results are in favor of a recurrent account for backward masking. Particularly in our experiment with a short stimulus onset asynchrony (SOA = time from stimulus onset to the mask onset), the mask seems to have affected mostly the local recurrent connections.</p>
</sec>
</sec>
<sec id="sec017" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec018">
<title>Ethics statement</title>
<p>The study was conducted according to the Declaration of Helsinki. The study involved human participants. The experiment protocol was approved by the local ethics committee at Massachusetts institute of technology. Volunteers completed a consent form before participating in the experiment and were financially compensated after finishing the experiment.</p>
</sec>
<sec id="sec019">
<title>Occluded objects image set</title>
<p>Images of four different object categories (i.e. camel, deer, car, and motorcycle) were used as the stimulus set (see <ext-link ext-link-type="uri" xlink:href="https://github.com/krajaei/Megocclusion/blob/master/Sample_occlusion_dataset.png" xlink:type="simple">https://github.com/krajaei/Megocclusion/blob/master/Sample_occlusion_dataset.png</ext-link> for sample images of occlusion). Object images were transformed to be similar in terms of size and contrast level. To generate an occluded image, in an iterative process we added several black circles (as artificial occluders) of different sizes in random positions on the image. The configuration of black circles (i.e. number, size, and their positions on the images) were randomly selected as such that a V1-like model could not discriminate between images with 0%, 60% and 80% occlusion. To simulate the type of occlusion that occurs in natural scenes, the black circles are positioned in both front and back of the target object. The percent of object occlusion is defined as the percent of the target object covered by the black occluders. We defined three levels of occlusion: 0% (no occlusion), 60% occlusion and 80% occlusion. Black circles also existed in the 0% occlusion, but not covering the target object; this was to make sure that the difference observed between occluded and un-occluded objects cannot be solely explained by the presence of these circles. The generated image set is comprised of 12 conditions: four objects × three occlusion levels. For each condition, we generated M = 64 sample images varying by the occlusion pattern and the target object position. To remove the potential effect of low-level visual features in object discrimination—objects positions were slightly changed around the center of the images (by Δx ≤ 15, Δy ≤ 15 pixels). Overall, we controlled for low-level image statistics, as such that images of different levels of occlusion could not be discriminated simply by using low-level visual features (i.e. Gist and V1 model).</p>
</sec>
<sec id="sec020">
<title>Participants and MEG experimental design</title>
<p>Fifteen young volunteers (22–38 year-old, all right-handed; 7 female) participated in the experiment. During the experiment, participants completed eight runs; each run consisted of 192 trials and lasted for approximately eight minutes (total experiment time for each participant = ~70min). Each trial started with 1sec fixation followed by 34ms (2 × screen frame rate (17ms) = 34ms) presentation of an object image (6° visual angle). In half the trials, we employed backward masking in which a dynamic mask was presented for 102ms shortly after the stimulus offset—inter-stimulus-interval (ISI) of 17ms—(<xref ref-type="supplementary-material" rid="pcbi.1007001.s001">S1 Fig</xref>). In each run, each object image (i.e. camel, deer, car, motor) was repeated 8 times under different levels of occlusions without backward masking; and another 8 repetitions with backward masking. In other words, each condition (i.e. combination of object-image, occlusion-level, mask or no-mask) was repeated 64 times over the duration of the whole experiment.</p>
<p>Every 1–3 trials, a question mark appeared on the screen (lasted for 1.5 sec) prompting participants to choose animate if the last stimulus was deer/camel and inanimate if the last stimulus was car/motor (<xref ref-type="supplementary-material" rid="pcbi.1007001.s001">S1 Fig</xref>; see <xref ref-type="supplementary-material" rid="pcbi.1007001.s012">S12 Fig</xref> for behavioral performance of animate/inanimate task). Participants were instructed to only respond and blink during the question trials to prevent contamination of MEG signals with motor activity and the eye-blink artifact. The question trials were excluded from further MEG analyses.</p>
<p>The dynamic mask was a sequence of random images (n = 6 images; each presented for 17ms) selected from a pool of the synthesized mask images. They were generated by using a texture synthesis toolbox that is available at: <ext-link ext-link-type="uri" xlink:href="http://www.cns.nyu.edu/~lcv/texture/" xlink:type="simple">http://www.cns.nyu.edu/~lcv/texture/</ext-link> [<xref ref-type="bibr" rid="pcbi.1007001.ref088">88</xref>]. The synthesized images have low-level feature statistics similar to the original stimuli.</p>
</sec>
<sec id="sec021">
<title>MEG acquisition</title>
<p>To acquire brain signals with millisecond temporal resolution, we used 306-sensors MEG system (Elekta Neuromag, Stockholm). The sampling rate was 1000Hz and band-pass filtered online between 0.03 and 330 Hz. To reduce noise and correct for head movements, raw data were cleaned by spatiotemporal filters [Maxfilter software, Elekta, Stockholm; [<xref ref-type="bibr" rid="pcbi.1007001.ref089">89</xref>]]. Further pre-processing was conducted by Brainstorm toolbox [<xref ref-type="bibr" rid="pcbi.1007001.ref090">90</xref>]. Trials were extracted -200ms to 700ms relative to the stimulus onset. The signals were then normalized by their baseline (-200ms to 0ms), and were temporally smoothed by low-pass filtering at 20Hz.</p>
</sec>
<sec id="sec022">
<title>Behavioral task of multiclass object recognition</title>
<p>We ran a psychophysical experiment, outside MEG, to evaluate human performance on a multi-class occluded object recognition task. Sixteen subjects participated in a session lasting about 40 minutes. The experiment was a combination of mask and no-mask trials that were randomly distributed across the experiment. Each trial, started by a fixation point presented for 0.5s followed by a stimulus presentation of 34ms. In the masked trials, a dynamic mask of 102ms was presented after a short ISI of 17ms (<xref ref-type="supplementary-material" rid="pcbi.1007001.s005">S5 Fig</xref>). Subjects were instructed to respond accurately and as soon as possible after detecting the target stimulus. They were asked to categorize the object images by pressing one of the pre-assigned four keys on a keyboard corresponding to the four object categories: camel, deer, car, and motorcycle.</p>
<p>Overall, 16 human subjects (25 to 40 years-old) participated in this experiment. Before the experiment, participants performed a short training phase on a totally different image-set to learn the task and reach a predefined performance level in the multi-class object recognition task. The main experiment consisted of 768 trials that were randomly distributed into four blocks of 192 trials (32 repetitions of object images with small variations in position and the pattern of occlusion × three occlusion levels × two masking conditions × four object categories = 768). Images of 256x256 pixels size were presented at a distance of 70 cm at the center of a CRT monitor with the frame rate of 60 Hz and a resolution of 1024×768. We used the MATLAB based psychophysics toolbox of [<xref ref-type="bibr" rid="pcbi.1007001.ref091">91</xref>].</p>
</sec>
<sec id="sec023">
<title>Multivariate pattern analyses (MVPA)</title>
<sec id="sec024">
<title>Pairwise decoding analysis</title>
<p>To measure temporal dynamics of object information processing, we used pairwise decoding analysis on the MEG data [<xref ref-type="bibr" rid="pcbi.1007001.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref092">92</xref>]. For each subject, at each time-point, we created a data matrix of 64-trials × 306-sensors per condition. We used a support vector machine (SVM) to pairwise decode any two conditions, with a leave-one-out cross-validation approach. At each time-point, for each condition, <italic>N-1</italic> pattern vectors were used to train the linear classifier [SVM; LIBSVM, [<xref ref-type="bibr" rid="pcbi.1007001.ref093">93</xref>], software available at <ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" xlink:type="simple">http://www.csie.ntu.edu.tw/~cjlin/libsvm/</ext-link>], and the remaining <italic>N</italic><sup><italic>th</italic></sup> vector was used for evaluation. The above procedure was repeated 100 times with random reassignment of the data to training and testing sets. This was then averaged over the six pairwise decoding accuracies. The SVM decoding analysis is done independently for each subject and then we report the average decoding performance over these individuals (<xref ref-type="fig" rid="pcbi.1007001.g001">Fig 1a</xref>).</p>
</sec>
<sec id="sec025">
<title>Time-time decoding analysis</title>
<p>We also reported time-time decoding accuracies, obtained by cross-decoding across time. For each pair of objects, a SVM classifier is trained at a given time and tested at all other time-points, thus showing the generalization of the classifier across time. The results are then averaged across all the pairwise classifications. This yields an 801x801 (t = -100 to 700 ms) matrix of average pairwise decoding accuracies for each subject. <xref ref-type="fig" rid="pcbi.1007001.g002">Fig 2</xref> shows the time-time decoding matrices averaged across 15 participants. To test for statistical significance, we did one-sided signrank test against the chance-level and then corrected for multiple comparison using FDR.</p>
</sec>
<sec id="sec026">
<title>Sensorwise decoding analysis</title>
<p>We also examined a sensorwise visualization of pairwise object decoding across time (<xref ref-type="supplementary-material" rid="pcbi.1007001.s014">S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1007001.s015">S2</xref> Movies). To this end, we trained and tested the SVM classifier at the level of sensors (i.e. combination of three neighboring sensors) across the whole 306 sensors. First, 306 MEG sensors were grouped into 102 triplets (Elekta Triux system; 2 gradiometers and 1 magnetometer in each location). At each time-point, we applied the same pairwise decoding procedure as previously explained in 4.5.1, this time at the level of groups of 3 adjacent sensors (instead of taking all the 306 MEG sensors together). Average pairwise decoding accuracies across subjects, at each time point, are color-coded across the head surface. We used black dots to indicate channels with significantly above chance accuracy (FDR-corrected across both time and sensors), and gray dots to show accuracies with p&lt;0.05, before correcting for multiple comparison. At each time-point, we also specify the channel with peak decoding accuracy by a red dot.</p>
</sec>
<sec id="sec027">
<title>Representational similarity analysis (RSA) over time</title>
<p>We used representational similarity analysis (RSA) [<xref ref-type="bibr" rid="pcbi.1007001.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref094">94</xref>–<xref ref-type="bibr" rid="pcbi.1007001.ref096">96</xref>], to compare representations of computational models with time-resolved representations derived from MEG data.</p>
<p>For the MEG data, representational dissimilarity matrices (RDM) were calculated at each time-point by computing the dissimilarity (1—Spearman’s R) between all pairs of the MEG patterns elicited by object images. Time-resolved MEG RDMs were then correlated (Spearman’s R) with the computational model RDMs, yielding a correlation vector over time (<xref ref-type="fig" rid="pcbi.1007001.g005">Fig 5a</xref>).</p>
<p><bold>Semipartial correlation.</bold> We additionally calculated semipartial correlations between the MEG RDMs and the computational model RDMs (<xref ref-type="fig" rid="pcbi.1007001.g007">Fig 7b</xref>). Semipartial correlation indicates the unique relationship between a model representation (e.g. AlexNet RDM) and the MEG data, by taking out the shared contribution of other models (e.g. HRRN RDM) in explaining the MEG data. The semipartial correlation is computed as follows:
<disp-formula id="pcbi.1007001.e001">
<alternatives>
<graphic id="pcbi.1007001.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007001.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>(</mml:mo><mml:mrow><mml:mn>2.3</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>13</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
</disp-formula>
′<italic>r</italic><sub>1(2.3)</sub>′ is the correlation between <italic>model X</italic><sub><italic>1</italic></sub> and <italic>model X</italic><sub><italic>2</italic></sub> controlling for the effect of <italic>model X</italic><sub><italic>3</italic></sub> (i.e. removing X<sub>3</sub> only from X<sub>2</sub>) [<xref ref-type="bibr" rid="pcbi.1007001.ref097">97</xref>].</p>
<p>To construct CNN model RDMs, we used the extracted features from the penultimate layer of the networks (i.e. the layer before softmax operation). Significant correlations were determined by one-sided signrank test (p &lt; 0.5, FDR-corrected across time).</p>
</sec>
</sec>
<sec id="sec028">
<title>Significance testing</title>
<p>We used the non-parametric Wilcoxon signrank test [<xref ref-type="bibr" rid="pcbi.1007001.ref098">98</xref>] for random effect analysis. To determine time-points with significantly above chance decoding accuracy (or significant RDM correlations), we used a right-sided signrank test across n = 15 participants. To adjust p-values for multiple comparisons (e.g. across time), we further applied the false discovery rate (FDR) correction [<xref ref-type="bibr" rid="pcbi.1007001.ref099">99</xref>] [RSA-Toolbox: is available from <ext-link ext-link-type="uri" xlink:href="https://github.com/rsagroup/rsatoolbox" xlink:type="simple">https://github.com/rsagroup/rsatoolbox</ext-link> [<xref ref-type="bibr" rid="pcbi.1007001.ref100">100</xref>]].</p>
<p>To determine whether two time-courses (e.g. correlation or decoding) are significantly different at any time interval, we used a two-sided signrank test, FDR corrected across time.</p>
<p><bold>Onset latency.</bold> We defined onset latency as the earliest time where performance became significantly above chance for at least ten consecutive milliseconds. Mean and standard deviation (SD) for onset latencies were calculated by leave-one-subject-out repeated for N = 15 times.</p>
<p><bold>Peak latency.</bold> The time for peak decoding accuracy was defined as the time where the decoding accuracy was the maximum value. The mean and SD for peak latencies were calculated similar to the onset latencies.</p>
</sec>
<sec id="sec029">
<title>Computational modeling</title>
<sec id="sec030">
<title>Feedforward computational model (AlexNet)</title>
<p>We used a well-known CNN (AlexNet) [<xref ref-type="bibr" rid="pcbi.1007001.ref101">101</xref>] that is shown to account for the core object recognition [<xref ref-type="bibr" rid="pcbi.1007001.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref074">74</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref075">75</xref>]. CNNs are cascades of hierarchically organized feature extraction layers. Each layer has several hundred convolutional filters and each convolutional filter scans various places on the input generating a feature map at its output. A convolutional layer may be followed by a local or global pooling layer merging outputs of a group of units. The pooling layers make the feature maps invariant to small variations [<xref ref-type="bibr" rid="pcbi.1007001.ref081">81</xref>]. AlexNet has eight cascading layers: five convolutional layers, some of which followed by pooling layers, and three fully-connected layers [<xref ref-type="bibr" rid="pcbi.1007001.ref101">101</xref>]. The last fully-connected layer is a 1000-way softmax that corresponds to the 1000 class labels. The network has 60 million free parameters. A pre-trained version of the model, which is trained on 1.2 million images from ImageNet dataset [<xref ref-type="bibr" rid="pcbi.1007001.ref102">102</xref>] is used for the experiments here. We used the features extracted by the fc7 layer (before softmax operation) as the model output.</p>
</sec>
<sec id="sec031">
<title>Hierarchical recurrent ResNet (HRRN)</title>
<p>In convolutional neural networks, performance in visual recognition tasks can be substantially improved by adding to the depth of the network [<xref ref-type="bibr" rid="pcbi.1007001.ref078">78</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref079">79</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref103">103</xref>]. However, this comes at a cost: deeper networks of simply stacking layers (plain nets) have higher training errors due to the vanishing gradients (degradation) [<xref ref-type="bibr" rid="pcbi.1007001.ref104">104</xref>] problem that prevents convergence in the training phase. To address this problem, He et al. [<xref ref-type="bibr" rid="pcbi.1007001.ref058">58</xref>] introduced a deep residual learning framework. Residual networks can overcome the vanishing gradient problem during learning by employing <italic>identity shortcut connections</italic> that allow bypassing residual layers. This framework enables training ultra-deep networks, e.g. with 1202 layers, leading to much better performances compared to the shallower networks [<xref ref-type="bibr" rid="pcbi.1007001.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref059">59</xref>].</p>
<p>Residual connections give ResNet an interesting characteristic of having several possible pathways with different lengths from the network’s input to the output instead of a single deep pathway [<xref ref-type="bibr" rid="pcbi.1007001.ref061">61</xref>]. For example, the ultra-deep 152-layers ResNet in its simplest form—by skipping all the residual layers—is a hierarchy of five convolutional layers. By including additional residual layers, more complex networks with various depths are constructed [see table 1 in [<xref ref-type="bibr" rid="pcbi.1007001.ref058">58</xref>]]. Each series of the residual modules can be reformulated into the form of a convolutional layer with recurrent connections [<xref ref-type="bibr" rid="pcbi.1007001.ref063">63</xref>]. Additionally, Liao and Poggio [<xref ref-type="bibr" rid="pcbi.1007001.ref060">60</xref>] show that a ResNet with shared weights can retain most of the performance of the corresponding network with non-shared weights.</p>
<p>In this study, we proposed a generalization of this convolutional neural network by redefining residual layers as local recurrent connections. As shown in <xref ref-type="fig" rid="pcbi.1007001.g006">Fig 6</xref>, we reformulated the 152-layers ResNet of [<xref ref-type="bibr" rid="pcbi.1007001.ref058">58</xref>] into the form of a five-layer convolutional network with folded residual layers as its local recurrent connections. The unfolded HRRN (= 152 layers ResNet) is deeper than the AlexNet and has different normalization (i.e. batch normalization) and filter sizes, however, they still have a similar number of free parameters (60M). Comparing AlexNet with a purely feedforward version of HRRN (readout stage 0, with five layers), AlexNet performs slightly better than HRRN in readout stage 0, and gradually after 4 iterations HRRN reaches a categorization performance similar to that of AlexNet (<xref ref-type="supplementary-material" rid="pcbi.1007001.s013">S13 Fig</xref>). The model is pre-trained on ImageNet 2012 dataset with a training set similar to that of Alexnet (1.2 million training images). It is shown experimentally that an unfolded recurrent CNN (with shared weights) is similar to a very deep feedforward network with non-shared weights [<xref ref-type="bibr" rid="pcbi.1007001.ref060">60</xref>]. In our analyses, we used the extracted features of the penultimate layer (i.e. layer pool5, which is before the softmax layer) as the model output.</p>
</sec>
</sec>
</sec>
<sec id="sec032">
<title>Supporting information</title>
<supplementary-material id="pcbi.1007001.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>MEG experimental paradigm.</title>
<p>The experiment was divided into two types of trials: mask and no mask trials, shown in random order. Each trial started by a fixation of 1 sec followed by a target stimulus presented for 34ms. In the mask trials, after a short inter-stimulus-interval (ISI) of 17ms, a dynamic mask of 102ms duration was presented. Every 1–3 trials (average = 2) a question mark appeared on the screen. Subjects were asked to select whether the last image was animate or inanimate. They were also instructed to restrict their blinking (and swallowing) to the question-mark trials.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Time course of visual processing and masking in humans.</title>
<p>Earliest responses reaching each of the visual areas from V1 to IT are indicated by the oblique lines, when a stimulus is on for 34ms, followed by an ISI of 17 ms, followed by a mask. The grey shaded area indicates the effect of mask when it disrupts the information that is being fed back from higher visual areas to lower visual areas. The approximate timings are set according to human [<xref ref-type="bibr" rid="pcbi.1007001.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref050">50</xref>, <xref ref-type="bibr" rid="pcbi.1007001.ref105">105</xref>] and non-human (i.e. macaque) studies [<xref ref-type="bibr" rid="pcbi.1007001.ref004">4</xref>] controlling for the fact that the macaque cortex is smaller, with a shorter neural distance and therefore faster transmission of visual information [<xref ref-type="bibr" rid="pcbi.1007001.ref037">37</xref>].</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Average response times of behavioral experiment across three occlusion levels.</title>
<p>The results are averaged over n = 15 human participants. Error bars represent SEM. Significant difference between occlusion levels are indicated by stars (signrank test). *** = p&lt;0.001.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Split-half replicability for different conditions.</title>
<p>The MEG trials for each condition (i.e. 0% occlusion, 60% occlusion, and 80% occlusion) were divided into two halves, the replicability is measured as the correlation between these two halves. Thicker lines indicate significantly above chance correlations (right sided sign-rank test, FDR corrected across time, p&lt;0.05). No significant difference was observed between the replicability of different conditions (two-sided signrank test, FDR-corrected across time), thus indicating that different conditions do not differ in their level of noise. In more details, for each condition, we randomly split M = 64 trial repetitions into two groups of 32 trials. Distance matrices were then calculated for average raw pattern vectors of each group by computing pairwise dissimilarity (1-correlation) between the patterns (12x12 matrices; 12 experimental stimuli). Spearman’s R was used as the replicability measure between these two split-half matrices across time.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Time-course of average pairwise decoding accuracy for mask and no mask trials under 80 percent occlusion.</title>
<p>Shaded error bars represent standard error of the mean (SEM). Decoding accuracy was not significantly above chance at any time-point for both mask and no mask (right-sided signrank test, FDR-corrected across time, p &lt; 0.5).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Experimental design of the multiclass behavioral task.</title>
<p>The behavioral experiment had two types of trials: mask and no mask trials (in random order). Each trial started by 0.5sec fixation, followed by a short presentation of stimulus for 34ms. In the masked trials, 17ms after the stimulus offset (short ISI) a dynamic mask of 100ms was presented. The dynamic mask was a sequence of synthesized images. The subjects were instructed to respond as soon and accurate as possible. Subject’s response was to categorize the presented image by pressing one of the four pre-specified keys on a keyboard corresponding to the four object categories (camel, deer, car, and motorcycle).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s007" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Confusion matrices of the human (mask/no-mask) and models across three occlusion levels.</title>
<p>To compare patterns of errors in the models and humans, we computed confusion matrices. To obtain a confusion matrix, we first trained a SVM classifier on a multiclass object recognition task similar to the behavioral experiment. Then, we calculated the percentage of predicted labels assigned to a category. We display these percentages using color-codes in the matrix. Elements in the main diagonal of the confusion matrix show classification performances and off-diagonal elements show errors made in the classification. <bold>(a)</bold> Confusion matrices for the three levels of occlusion. The color bar, at the bottom-right corner, indicates the percentage of labels assigned to a category. <bold>(b)</bold> Bars indicate correlations between confusion matrices of the models with that of humans (mask and no-mask). Stars show significant differences between HRRN and AlexNet (signrank test, across subjects). * = p&lt;0.05; ** = p&lt;0.01.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s008" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>Venn diagram of MEG and the models.</title>
<p>Red area indicates the unique contribution of HRRN in explaining MEG data. AlexNet has no unique contribution likely due to a component shared between the two models (i.e. feedforward component).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s009" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s009" xlink:type="simple">
<label>S9 Fig</label>
<caption>
<title>Contribution of the feedforward and recurrent models in explaining MEG data under 0% occlusion.</title>
<p>(a) Correlation between the models RDMs and the average MEG RDM over two different time bins. (b) Unique contribution of each model (semipartial correlation) in explaining the MEG data. Error bars represent SEM (Standard Error of the Mean). Significantly above zero correlations/semipartial-correlations and significant differences between the two models are indicated by stars. * = p&lt;0.05; ** = p&lt;0.01; *** = p&lt;0.001.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s010" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s010" xlink:type="simple">
<label>S10 Fig</label>
<caption>
<title>Sample images of object occlusion versus deletion.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s011" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s011" xlink:type="simple">
<label>S11 Fig</label>
<caption>
<title>HRRN accuracy across readout stages for different levels of occlusion.</title>
<p>Shaded error bars indicate SD. Black circles are average accuracies across n = 16 human participants. Readout stage: readout stage refers to the number of local recurrent iterations involved in processing the input image throughout the hierarchy of the network. Readout stage 0 is when the model is fully feedforward (no local recurrent is active). And readout stage 1 is when only one recurrent iteration is engaged and readout stage <italic>n</italic> is when the network has gone through <italic>n</italic> recurrent iterations.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s012" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s012" xlink:type="simple">
<label>S12 Fig</label>
<caption>
<title>Behavioral performance of animate/inanimate categorization task of the MEG experiment.</title>
<p>Stars indicate significant differences between mask and no-mask trials. The results are averaged over N = 15 human participants.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s013" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s013" xlink:type="simple">
<label>S13 Fig</label>
<caption>
<title>Categorization accuracies of HRRN across different readout stages compared with Alexnet for different occlusion levels.</title>
<p>Shaded error bars indicate SD. Readout stage: readout stage refers to the number of local recurrent iterations involved in processing the input image throughout the hierarchy of the network. Readout stage 0 is when the model is fully feedforward (no local recurrent is active). And readout stage 1 is when only one recurrent iteration is engaged and readout stage n is when the network has gone through n recurrent iterations. Black circles are average accuracies for Alexnet, which are shown around the approximate corresponding HRRN readout stages.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s014" mimetype="video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s014" xlink:type="simple">
<label>S1 Movie</label>
<caption>
<title>Sensorwise visualization of pairwise object decoding across time for no-occlusion condition.</title>
<p>Color map represents percent of decoding accuracies across head surface (chance level = 50%). Circles indicate neighboring triplets (102 triplets) of MEG sensors (2 gradiometers and 1 magnetometer in each location). Significantly above chance decoding accuracies after correction for multiple comparison are shown by black dots (FDR-corrected across 102 triplets and 801 time-points). Gray dots indicate decoding accuracies with p&lt;0.05 (right sided signed rank test) that did not remain signifiant after FDR-correction. At each time point, the peak decoding accuracy is indicated by a red dot.</p>
<p>(MP4)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007001.s015" mimetype="video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007001.s015" xlink:type="simple">
<label>S2 Movie</label>
<caption>
<title>Sensorwise visualization of pairwise object decoding across time for occlusion condition.</title>
<p>Color map represents percent of decoding accuracies across head surface (chance level = 50%). Circles indicate neighboring triplets (102 triplets) of MEG sensors (2 gradiometers and 1 magnetometer in each location). Significantly above chance decoding accuracies after correction for multiple comparison are shown by black dots (FDR-corrected across 102 triplets and 801 time-points). Gray dots indicate decoding accuracies with p&lt;0.05 (right sided signed rank test) that did not remain signifiant after FDR-correction. At each time point, the peak decoding accuracy is indicated by a red dot.</p>
<p>(MP4)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The study was conducted at the Athinoula A. Martinos Imaging Center at the McGovern Institute for Brain Research, Massachusetts Institute of Technology. We would like to thank Aude Oliva and Dimitrios Pantazis for their help and support in conducting this study. We would also like to thank Radoslaw Martin Cichy for helpful comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1007001.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lamme</surname> <given-names>VA</given-names></name>, <name name-style="western"><surname>Super</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Spekreijse</surname> <given-names>H</given-names></name>. <article-title>Feedforward, horizontal, and feedback processing in the visual cortex</article-title>. <source>Current opinion in neurobiology</source>. <year>1998</year>;<volume>8</volume>(<issue>4</issue>):<fpage>529</fpage>–<lpage>35</lpage>. <object-id pub-id-type="pmid">9751656</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sporns</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Zwi</surname> <given-names>JD</given-names></name>. <article-title>The small world of the cerebral cortex</article-title>. <source>Neuroinformatics</source>. <year>2004</year>;<volume>2</volume>(<issue>2</issue>):<fpage>145</fpage>–<lpage>62</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1385/NI:2:2:145" xlink:type="simple">10.1385/NI:2:2:145</ext-link></comment> <object-id pub-id-type="pmid">15319512</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Felleman</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Van Essen</surname> <given-names>DC</given-names></name>. <article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title>. <source>Cerebral cortex</source>. <year>1991</year>;<volume>1</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>47</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/1.1.1" xlink:type="simple">10.1093/cercor/1.1.1</ext-link></comment> <object-id pub-id-type="pmid">1822724</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lamme</surname> <given-names>VA</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>. <article-title>The distinct modes of vision offered by feedforward and recurrent processing</article-title>. <source>Trends in neurosciences</source>. <year>2000</year>;<volume>23</volume>(<issue>11</issue>):<fpage>571</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">11074267</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gilbert</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>W</given-names></name>. <article-title>Top-down influences on visual processing</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2013</year>;<volume>14</volume>(<issue>5</issue>):<fpage>350</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3476" xlink:type="simple">10.1038/nrn3476</ext-link></comment> <object-id pub-id-type="pmid">23595013</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kafaligonul</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Breitmeyer</surname> <given-names>BG</given-names></name>, <name name-style="western"><surname>Öğmen</surname> <given-names>H</given-names></name>. <article-title>Feedforward and feedback processes in vision</article-title>. <source>Frontiers in psychology</source>. <year>2015</year>;<volume>6</volume>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref007"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Klink PC, Dagnino B, Gariel-Mathis M-A, Roelfsema PR. Distinct Feedforward and Feedback Effects of Microstimulation in Visual Cortex Reveal Neural Mechanisms of Texture Segregation. Neuron. 2017.</mixed-citation></ref>
<ref id="pcbi.1007001.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Cox</surname> <given-names>DD</given-names></name>. <article-title>Untangling invariant object recognition</article-title>. <source>Trends in cognitive sciences</source>. <year>2007</year>;<volume>11</volume>(<issue>8</issue>):<fpage>333</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2007.06.010" xlink:type="simple">10.1016/j.tics.2007.06.010</ext-link></comment> <object-id pub-id-type="pmid">17631409</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Zoccolan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>. <article-title>How does the brain solve visual object recognition?</article-title> <source>Neuron</source>. <year>2012</year>;<volume>73</volume>(<issue>3</issue>):<fpage>415</fpage>–<lpage>34</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2012.01.010" xlink:type="simple">10.1016/j.neuron.2012.01.010</ext-link></comment> <object-id pub-id-type="pmid">22325196</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>S-M</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>11</issue>):<fpage>e1003915</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003915" xlink:type="simple">10.1371/journal.pcbi.1003915</ext-link></comment> <object-id pub-id-type="pmid">25375136</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Seibert</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year>;<volume>111</volume>(<issue>23</issue>):<fpage>8619</fpage>–<lpage>24</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yamins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Pinto</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ardila</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <etal>et al</etal>. <article-title>Deep neural networks rival the representation of primate IT cortex for core visual object recognition</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>12</issue>):<fpage>e1003963</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003963" xlink:type="simple">10.1371/journal.pcbi.1003963</ext-link></comment> <object-id pub-id-type="pmid">25521294</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wen</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Shi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>Z</given-names></name>. <article-title>Deep Residual Network Predicts Cortical Representation and Organization of Visual Features for Rapid Categorization</article-title>. <source>Scientific Reports</source>. <year>2018</year>;<volume>8</volume>(<issue>1</issue>):<fpage>3752</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-018-22160-9" xlink:type="simple">10.1038/s41598-018-22160-9</ext-link></comment> <object-id pub-id-type="pmid">29491405</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghodrati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Farzmahdi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rajaei</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ebrahimpour</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>S-M</given-names></name>. <article-title>Feedforward object-vision models only tolerate small image variations compared to human</article-title>. <source>Frontiers in computational neuroscience</source>. <year>2014</year>;<volume>8</volume>:<fpage>74</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2014.00074" xlink:type="simple">10.3389/fncom.2014.00074</ext-link></comment> <object-id pub-id-type="pmid">25100986</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Karimi-Rouzbahani</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Bagheri</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ebrahimpour</surname> <given-names>R</given-names></name>. <article-title>Hard-wired feed-forward visual mechanisms of the brain compensate for affine variations in object recognition</article-title>. <source>Neuroscience</source>. <year>2017</year>;<volume>349</volume>:<fpage>48</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroscience.2017.02.050" xlink:type="simple">10.1016/j.neuroscience.2017.02.050</ext-link></comment> <object-id pub-id-type="pmid">28245990</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rensink</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Enns</surname> <given-names>JT</given-names></name>. <article-title>Early completion of occluded objects</article-title>. <source>Vision research</source>. <year>1998</year>;<volume>38</volume>(<issue>15</issue>):<fpage>2489</fpage>–<lpage>505</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nielsen</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>, <name name-style="western"><surname>Rainer</surname> <given-names>G</given-names></name>. <article-title>Dissociation between local field potentials and spiking activity in macaque inferior temporal cortex reveals diagnosticity-based encoding of complex objects</article-title>. <source>Journal of Neuroscience</source>. <year>2006</year>;<volume>26</volume>(<issue>38</issue>):<fpage>9639</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2273-06.2006" xlink:type="simple">10.1523/JNEUROSCI.2273-06.2006</ext-link></comment> <object-id pub-id-type="pmid">16988034</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wyatte</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Curran</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>O’Reilly</surname> <given-names>R</given-names></name>. <article-title>The limits of feedforward vision: Recurrent processing promotes robust object recognition when objects are degraded</article-title>. <source>Journal of Cognitive Neuroscience</source>. <year>2012</year>;<volume>24</volume>(<issue>11</issue>):<fpage>2248</fpage>–<lpage>61</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_00282" xlink:type="simple">10.1162/jocn_a_00282</ext-link></comment> <object-id pub-id-type="pmid">22905822</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Wyatte</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Herd</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mingus</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Jilk</surname> <given-names>DJ</given-names></name>. <article-title>Recurrent processing during object recognition</article-title>. <source>Frontiers in psychology</source>. <year>2013</year>;<volume>4</volume>:<fpage>124</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2013.00124" xlink:type="simple">10.3389/fpsyg.2013.00124</ext-link></comment> <object-id pub-id-type="pmid">23554596</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref020"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Wyatte D, Jilk DJ, O’Reilly RC. Early recurrent feedback facilitates visual object recognition under challenging conditions. 2014.</mixed-citation></ref>
<ref id="pcbi.1007001.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kosai</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>El-Shamayleh</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Fyall</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Pasupathy</surname> <given-names>A</given-names></name>. <article-title>The role of visual area V4 in the discrimination of partially occluded shapes</article-title>. <source>Journal of Neuroscience</source>. <year>2014</year>;<volume>34</volume>(<issue>25</issue>):<fpage>8570</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1375-14.2014" xlink:type="simple">10.1523/JNEUROSCI.1375-14.2014</ext-link></comment> <object-id pub-id-type="pmid">24948811</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref022"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Choi H, Pasupathy A, Shea-Brown E. Predictive coding in area V4: dynamic shape discrimination under partial occlusion. arXiv preprint arXiv:161205321. 2016.</mixed-citation></ref>
<ref id="pcbi.1007001.ref023"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Spoerer C, McClure P, Kriegeskorte N. Recurrent Convolutional Neural Networks: A Better Model Of Biological Object Recognition Under Occlusion. bioRxiv. 2017:133330.</mixed-citation></ref>
<ref id="pcbi.1007001.ref024"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Tang H, Schrimpf M, Lotter W, Moerman C, Paredes A, Caro JO, et al. Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences. 2018:201719397.</mixed-citation></ref>
<ref id="pcbi.1007001.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Livne</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Sagi</surname> <given-names>D</given-names></name>. <article-title>Multiple levels of orientation anisotropy in crowding with Gabor flankers</article-title>. <source>Journal of vision</source>. <year>2011</year>;<volume>11</volume>(<issue>13</issue>):<fpage>18</fpage>-. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/11.13.18" xlink:type="simple">10.1167/11.13.18</ext-link></comment> <object-id pub-id-type="pmid">22101017</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref026"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Manassi M, Herzog M, editors. Crowding and grouping: how much time is needed to process good Gestalt? Perception; 2013.</mixed-citation></ref>
<ref id="pcbi.1007001.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clarke</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Herzog</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Francis</surname> <given-names>G</given-names></name>. <article-title>Visual crowding illustrates the inadequacy of local vs. global and feedforward vs. feedback distinctions in modeling visual perception</article-title>. <source>Frontiers in psychology</source>. <year>2014</year>;<volume>5</volume>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>. <article-title>The recognition of partially visible natural objects in the presence and absence of their occluders</article-title>. <source>Vision research</source>. <year>2005</year>;<volume>45</volume>(<issue>25</issue>):<fpage>3262</fpage>–<lpage>76</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Buia</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Madhavan</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Crone</surname> <given-names>NE</given-names></name>, <name name-style="western"><surname>Madsen</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>WS</given-names></name>, <etal>et al</etal>. <article-title>Spatiotemporal dynamics underlying object completion in human ventral visual cortex</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>83</volume>(<issue>3</issue>):<fpage>736</fpage>–<lpage>48</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.06.017" xlink:type="simple">10.1016/j.neuron.2014.06.017</ext-link></comment> <object-id pub-id-type="pmid">25043420</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref030"><label>30</label><mixed-citation publication-type="other" xlink:type="simple">Eberhardt S, Cader JG, Serre T, editors. How deep is the feature analysis underlying rapid visual categorization? Advances in neural information processing systems; 2016.</mixed-citation></ref>
<ref id="pcbi.1007001.ref031"><label>31</label><mixed-citation publication-type="other" xlink:type="simple">Rajalingham R, Issa EB, Bashivan P, Kar K, Schmidt K, DiCarlo JJ. Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. bioRxiv. 2018:240614.</mixed-citation></ref>
<ref id="pcbi.1007001.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rauschenberger</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Slotnick</surname> <given-names>SD</given-names></name>, <name name-style="western"><surname>Yantis</surname> <given-names>S</given-names></name>. <article-title>Temporally unfolding neural representation of pictorial occlusion</article-title>. <source>Psychological Science</source>. <year>2006</year>;<volume>17</volume>(<issue>4</issue>):<fpage>358</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-9280.2006.01711.x" xlink:type="simple">10.1111/j.1467-9280.2006.01711.x</ext-link></comment> <object-id pub-id-type="pmid">16623695</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hulme</surname> <given-names>OJ</given-names></name>, <name name-style="western"><surname>Zeki</surname> <given-names>S</given-names></name>. <article-title>The sightless view: neural correlates of occluded objects</article-title>. <source>Cerebral Cortex</source>. <year>2007</year>;<volume>17</volume>(<issue>5</issue>):<fpage>1197</fpage>–<lpage>205</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhl031" xlink:type="simple">10.1093/cercor/bhl031</ext-link></comment> <object-id pub-id-type="pmid">16844722</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hegdé</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Fang</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>SO</given-names></name>, <name name-style="western"><surname>Kersten</surname> <given-names>D</given-names></name>. <article-title>Preferential responses to occluded objects in the human visual cortex</article-title>. <source>Journal of vision</source>. <year>2008</year>;<volume>8</volume>(<issue>4</issue>):<fpage>16</fpage>-. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/8.4.16" xlink:type="simple">10.1167/8.4.16</ext-link></comment> <object-id pub-id-type="pmid">18484855</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ban</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yamamoto</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hanakawa</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Urayama</surname> <given-names>S-i</given-names></name>, <name name-style="western"><surname>Aso</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Fukuyama</surname> <given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Topographic representation of an occluded object and the effects of spatiotemporal context in human early visual areas</article-title>. <source>Journal of Neuroscience</source>. <year>2013</year>;<volume>33</volume>(<issue>43</issue>):<fpage>16992</fpage>–<lpage>7007</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1455-12.2013" xlink:type="simple">10.1523/JNEUROSCI.1455-12.2013</ext-link></comment> <object-id pub-id-type="pmid">24155304</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Erlikhman</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Caplovitz</surname> <given-names>GP</given-names></name>. <article-title>Decoding information about dynamically occluded objects in visual cortex</article-title>. <source>NeuroImage</source>. <year>2017</year>;<volume>146</volume>:<fpage>778</fpage>–<lpage>88</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2016.09.024" xlink:type="simple">10.1016/j.neuroimage.2016.09.024</ext-link></comment> <object-id pub-id-type="pmid">27663987</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorpe</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Fabre-Thorpe</surname> <given-names>M</given-names></name>. <article-title>Seeking categories in the brain</article-title>. <source>Science</source>. <year>2001</year>;<volume>291</volume>(<issue>5502</issue>):<fpage>260</fpage>–<lpage>3</lpage>. <object-id pub-id-type="pmid">11253215</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Agam</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Madsen</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Kreiman</surname> <given-names>G</given-names></name>. <article-title>Timing, timing, timing: fast decoding of object information from intracranial field potentials in human visual cortex</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>62</volume>(<issue>2</issue>):<fpage>281</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.02.025" xlink:type="simple">10.1016/j.neuron.2009.02.025</ext-link></comment> <object-id pub-id-type="pmid">19409272</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>S-M</given-names></name>, <name name-style="western"><surname>Carlin</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>The effects of recurrent dynamics on ventral-stream representational geometry</article-title>. <source>Journal of vision</source>. <year>2015</year>;<volume>15</volume>(<issue>12</issue>):<fpage>1089</fpage>-.</mixed-citation></ref>
<ref id="pcbi.1007001.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grootswagers</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Carlson</surname> <given-names>T</given-names></name>. <article-title>Decoding the emerging representation of degraded visual objects in the human brain</article-title>. <source>Journal of vision</source>. <year>2015</year>;<volume>15</volume>(<issue>12</issue>):<fpage>1087</fpage>-.</mixed-citation></ref>
<ref id="pcbi.1007001.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kaneshiro</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Guimaraes</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>H-S</given-names></name>, <name name-style="western"><surname>Norcia</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Suppes</surname> <given-names>P</given-names></name>. <article-title>A Representational Similarity Analysis of the Dynamics of Object Processing Using Single-Trial EEG Classification</article-title>. <source>PloS one</source>. <year>2015</year>;<volume>10</volume>(<issue>8</issue>):<fpage>e0135697</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0135697" xlink:type="simple">10.1371/journal.pone.0135697</ext-link></comment> <object-id pub-id-type="pmid">26295970</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mohsenzadeh</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Qin</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Pantazis</surname> <given-names>D</given-names></name>. <article-title>Ultra-Rapid serial visual presentation reveals dynamics of feedforward and feedback processes in the ventral visual pathway</article-title>. <source>Elife</source>. <year>2018</year>;<volume>7</volume>:<fpage>e36329</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.36329" xlink:type="simple">10.7554/eLife.36329</ext-link></comment> <object-id pub-id-type="pmid">29927384</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carlson</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tovar</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Alink</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Representational dynamics of object vision: the first 1000 ms</article-title>. <source>Journal of vision</source>. <year>2013</year>;<volume>13</volume>(<issue>10</issue>):<fpage>1</fpage>-. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/13.10.1" xlink:type="simple">10.1167/13.10.1</ext-link></comment> <object-id pub-id-type="pmid">23908380</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Pantazis</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>Resolving human object recognition in space and time</article-title>. <source>Nature neuroscience</source>. <year>2014</year>;<volume>17</volume>(<issue>3</issue>):<fpage>455</fpage>–<lpage>62</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3635" xlink:type="simple">10.1038/nn.3635</ext-link></comment> <object-id pub-id-type="pmid">24464044</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Isik</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Meyers</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Leibo</surname> <given-names>JZ</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>The dynamics of invariant object recognition in the human visual system</article-title>. <source>Journal of neurophysiology</source>. <year>2014</year>;<volume>111</volume>(<issue>1</issue>):<fpage>91</fpage>–<lpage>102</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00394.2013" xlink:type="simple">10.1152/jn.00394.2013</ext-link></comment> <object-id pub-id-type="pmid">24089402</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref046"><label>46</label><mixed-citation publication-type="other" xlink:type="simple">Grootswagers T, Wardle SG, Carlson TA. Decoding dynamic brain patterns from evoked responses: A tutorial on multivariate pattern analysis applied to time series neuroimaging data. Journal of cognitive neuroscience. 2017.</mixed-citation></ref>
<ref id="pcbi.1007001.ref047"><label>47</label><mixed-citation publication-type="other" xlink:type="simple">Contini EW, Wardle SG, Carlson TA. Decoding the time-course of object recognition in the human brain: From visual features to categorical decisions. Neuropsychologia. 2017.</mixed-citation></ref>
<ref id="pcbi.1007001.ref048"><label>48</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Breitmeyer</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Öğmen</surname> <given-names>H</given-names></name>. <source>Visual masking: Time slices through conscious and unconscious vision</source>: <publisher-name>Oxford University Press</publisher-name>; <year>2006</year>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorpe</surname> <given-names>SJ</given-names></name>. <article-title>The speed of categorization in the human visual system</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>62</volume>(<issue>2</issue>):<fpage>168</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.04.012" xlink:type="simple">10.1016/j.neuron.2009.04.012</ext-link></comment> <object-id pub-id-type="pmid">19409262</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Pantazis</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>Similarity-based fusion of MEG and fMRI reveals spatio-temporal dynamics in human cortex during visual object recognition</article-title>. <source>Cerebral Cortex</source>. <year>2016</year>;<volume>26</volume>(<issue>8</issue>):<fpage>3563</fpage>–<lpage>79</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhw135" xlink:type="simple">10.1093/cercor/bhw135</ext-link></comment> <object-id pub-id-type="pmid">27235099</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>King</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>. <article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title>. <source>Trends in cognitive sciences</source>. <year>2014</year>;<volume>18</volume>(<issue>4</issue>):<fpage>203</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2014.01.002" xlink:type="simple">10.1016/j.tics.2014.01.002</ext-link></comment> <object-id pub-id-type="pmid">24593982</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>King</surname> <given-names>J-R</given-names></name>, <name name-style="western"><surname>Pescetelli</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>. <article-title>Brain mechanisms underlying the brief maintenance of seen and unseen sensory information</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>92</volume>(<issue>5</issue>):<fpage>1122</fpage>–<lpage>34</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.10.051" xlink:type="simple">10.1016/j.neuron.2016.10.051</ext-link></comment> <object-id pub-id-type="pmid">27930903</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lamme</surname> <given-names>VA</given-names></name>, <name name-style="western"><surname>Zipser</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Spekreijse</surname> <given-names>H</given-names></name>. <article-title>Masking interrupts figure-ground signals in V1</article-title>. <source>Journal of cognitive neuroscience</source>. <year>2002</year>;<volume>14</volume>(<issue>7</issue>):<fpage>1044</fpage>–<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089892902320474490" xlink:type="simple">10.1162/089892902320474490</ext-link></comment> <object-id pub-id-type="pmid">12419127</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bacon-Macé</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Macé</surname> <given-names>MJ-M</given-names></name>, <name name-style="western"><surname>Fabre-Thorpe</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Thorpe</surname> <given-names>SJ</given-names></name>. <article-title>The time course of visual processing: Backward masking and natural scene categorisation</article-title>. <source>Vision research</source>. <year>2005</year>;<volume>45</volume>(<issue>11</issue>):<fpage>1459</fpage>–<lpage>69</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2005.01.004" xlink:type="simple">10.1016/j.visres.2005.01.004</ext-link></comment> <object-id pub-id-type="pmid">15743615</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fahrenfort</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VA</given-names></name>. <article-title>Masking disrupts reentrant processing in human visual cortex</article-title>. <source>Journal of cognitive neuroscience</source>. <year>2007</year>;<volume>19</volume>(<issue>9</issue>):<fpage>1488</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2007.19.9.1488" xlink:type="simple">10.1162/jocn.2007.19.9.1488</ext-link></comment> <object-id pub-id-type="pmid">17714010</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serre</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>A feedforward architecture accounts for rapid categorization</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2007</year>;<volume>104</volume>(<issue>15</issue>):<fpage>6424</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Macknik</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Martinez-Conde</surname> <given-names>S</given-names></name>. <article-title>The role of feedback in visual masking and visual processing</article-title>. <source>Advances in cognitive psychology</source>. <year>2007</year>;<volume>3</volume>(<issue>1–2</issue>):<fpage>125</fpage>–<lpage>52</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref058"><label>58</label><mixed-citation publication-type="other" xlink:type="simple">He K, Zhang X, Ren S, Sun J, editors. Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition; 2016.</mixed-citation></ref>
<ref id="pcbi.1007001.ref059"><label>59</label><mixed-citation publication-type="other" xlink:type="simple">He K, Zhang X, Ren S, Sun J, editors. Identity mappings in deep residual networks. European Conference on Computer Vision; 2016: Springer.</mixed-citation></ref>
<ref id="pcbi.1007001.ref060"><label>60</label><mixed-citation publication-type="other" xlink:type="simple">Liao Q, Poggio T. Bridging the gaps between residual learning, recurrent neural networks and visual cortex. arXiv preprint arXiv:160403640. 2016.</mixed-citation></ref>
<ref id="pcbi.1007001.ref061"><label>61</label><mixed-citation publication-type="other" xlink:type="simple">Veit A, Wilber MJ, Belongie S, editors. Residual networks behave like ensembles of relatively shallow networks. Advances in Neural Information Processing Systems; 2016.</mixed-citation></ref>
<ref id="pcbi.1007001.ref062"><label>62</label><mixed-citation publication-type="other" xlink:type="simple">Deng J, Dong W, Socher R, Li L-J, Li K, Fei-Fei L, editors. Imagenet: A large-scale hierarchical image database. Computer Vision and Pattern Recognition, 2009 CVPR 2009 IEEE Conference on; 2009: IEEE.</mixed-citation></ref>
<ref id="pcbi.1007001.ref063"><label>63</label><mixed-citation publication-type="other" xlink:type="simple">Liang M, Hu X, editors. Recurrent convolutional neural network for object recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; 2015.</mixed-citation></ref>
<ref id="pcbi.1007001.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Khosla</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pantazis</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title>. <source>Scientific reports</source>. <year>2016</year>;<volume>6</volume>:<fpage>27755</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep27755" xlink:type="simple">10.1038/srep27755</ext-link></comment> <object-id pub-id-type="pmid">27282108</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clarke</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Taylor</surname> <given-names>KI</given-names></name>, <name name-style="western"><surname>Tyler</surname> <given-names>LK</given-names></name>. <article-title>The evolution of meaning: spatio-temporal dynamics of visual object recognition</article-title>. <source>Journal of cognitive neuroscience</source>. <year>2011</year>;<volume>23</volume>(<issue>8</issue>):<fpage>1887</fpage>–<lpage>99</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2010.21544" xlink:type="simple">10.1162/jocn.2010.21544</ext-link></comment> <object-id pub-id-type="pmid">20617883</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clarke</surname> <given-names>A</given-names></name>. <article-title>Dynamic information processing states revealed through neurocognitive models of object semantics. Language</article-title>, <source>cognition and neuroscience</source>. <year>2015</year>;<volume>30</volume>(<issue>4</issue>):<fpage>409</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tomita</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Ohbayashi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nakahara</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Hasegawa</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Miyashita</surname> <given-names>Y</given-names></name>. <article-title>Top-down signal from prefrontal cortex in executive control of memory retrieval</article-title>. <source>Nature</source>. <year>1999</year>;<volume>401</volume>(<issue>6754</issue>):<fpage>699</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/44372" xlink:type="simple">10.1038/44372</ext-link></comment> <object-id pub-id-type="pmid">10537108</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Garrido</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Kilner</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Kiebel</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Evoked brain responses are generated by feedback loops</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2007</year>;<volume>104</volume>(<issue>52</issue>):<fpage>20961</fpage>–<lpage>6</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goddard</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Carlson</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Dermody</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Woolgar</surname> <given-names>A</given-names></name>. <article-title>Representational dynamics of object recognition: Feedforward and feedback information flows</article-title>. <source>NeuroImage</source>. <year>2016</year>;<volume>128</volume>:<fpage>385</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2016.01.006" xlink:type="simple">10.1016/j.neuroimage.2016.01.006</ext-link></comment> <object-id pub-id-type="pmid">26806290</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref070"><label>70</label><mixed-citation publication-type="other" xlink:type="simple">Devereux BJ, Clarke AD, Tyler LK. Integrated deep visual and semantic attractor neural networks predict fMRI pattern-information along the ventral object processing pathway. Scientific Reports. 2018.</mixed-citation></ref>
<ref id="pcbi.1007001.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clarke</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Tyler</surname> <given-names>LK</given-names></name>. <article-title>Object-specific semantic coding in human perirhinal cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2014</year>;<volume>34</volume>(<issue>14</issue>):<fpage>4766</fpage>–<lpage>75</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2828-13.2014" xlink:type="simple">10.1523/JNEUROSCI.2828-13.2014</ext-link></comment> <object-id pub-id-type="pmid">24695697</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Güçlü</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>van Gerven</surname> <given-names>MA</given-names></name>. <article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title>. <source>Journal of Neuroscience</source>. <year>2015</year>;<volume>35</volume>(<issue>27</issue>):<fpage>10005</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5023-14.2015" xlink:type="simple">10.1523/JNEUROSCI.5023-14.2015</ext-link></comment> <object-id pub-id-type="pmid">26157000</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kubilius</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bracci</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>de Beeck</surname> <given-names>HPO</given-names></name>. <article-title>Deep neural networks as a computational model for human shape sensitivity</article-title>. <source>PLoS computational biology</source>. <year>2016</year>;<volume>12</volume>(<issue>4</issue>):<fpage>e1004896</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004896" xlink:type="simple">10.1371/journal.pcbi.1004896</ext-link></comment> <object-id pub-id-type="pmid">27124699</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kheradpisheh</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Ghodrati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ganjtabesh</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Masquelier</surname> <given-names>T</given-names></name>. <article-title>Deep networks can resemble human feed-forward vision in invariant object recognition</article-title>. <source>Scientific reports</source>. <year>2016</year>;<volume>6</volume>:<fpage>32672</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep32672" xlink:type="simple">10.1038/srep32672</ext-link></comment> <object-id pub-id-type="pmid">27601096</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kheradpisheh</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Ghodrati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ganjtabesh</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Masquelier</surname> <given-names>T</given-names></name>. <article-title>Humans and deep networks largely agree on which kinds of variation make object recognition harder</article-title>. <source>Frontiers in computational neuroscience</source>. <year>2016</year>;<fpage>10</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>S-M</given-names></name>, <name name-style="western"><surname>Henriksson</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Kay</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Fixed versus mixed RSA: Explaining visual representations by fixed and mixed feature sets from shallow and deep computational models</article-title>. <source>Journal of Mathematical Psychology</source>. <year>2017</year>;<volume>76</volume>:<fpage>184</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jmp.2016.10.007" xlink:type="simple">10.1016/j.jmp.2016.10.007</ext-link></comment> <object-id pub-id-type="pmid">28298702</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Fang</surname> <given-names>F</given-names></name>. <article-title>Time course of amodal completion in face perception</article-title>. <source>Vision research</source>. <year>2009</year>;<volume>49</volume>(<issue>7</issue>):<fpage>752</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2009.02.005" xlink:type="simple">10.1016/j.visres.2009.02.005</ext-link></comment> <object-id pub-id-type="pmid">19233227</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref078"><label>78</label><mixed-citation publication-type="other" xlink:type="simple">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:14091556. 2014.</mixed-citation></ref>
<ref id="pcbi.1007001.ref079"><label>79</label><mixed-citation publication-type="other" xlink:type="simple">Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, et al., editors. Going deeper with convolutions. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; 2015.</mixed-citation></ref>
<ref id="pcbi.1007001.ref080"><label>80</label><mixed-citation publication-type="other" xlink:type="simple">Taigman Y, Yang M, Ranzato MA, Wolf L, editors. Deepface: Closing the gap to human-level performance in face verification. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; 2014.</mixed-citation></ref>
<ref id="pcbi.1007001.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>. <article-title>Scaling learning algorithms towards AI</article-title>. <source>Large-scale kernel machines</source>. <year>2007</year>;<volume>34</volume>(<issue>5</issue>).</mixed-citation></ref>
<ref id="pcbi.1007001.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dunbar</surname> <given-names>RI</given-names></name>. <article-title>Neocortex size as a constraint on group size in primates</article-title>. <source>Journal of human evolution</source>. <year>1992</year>;<volume>22</volume>(<issue>6</issue>):<fpage>469</fpage>–<lpage>93</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kaas</surname> <given-names>JH</given-names></name>. <article-title>Why is brain size so important: Design problems and solutions as neocortex gets biggeror smaller</article-title>. <source>Brain and Mind</source>. <year>2000</year>;<volume>1</volume>(<issue>1</issue>):<fpage>7</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Weaver</surname> <given-names>AH</given-names></name>. <article-title>Reciprocal evolution of the cerebellum and neocortex in fossil humans</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2005</year>;<volume>102</volume>(<issue>10</issue>):<fpage>3576</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0500692102" xlink:type="simple">10.1073/pnas.0500692102</ext-link></comment> <object-id pub-id-type="pmid">15731345</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Isler</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>van Schaik</surname> <given-names>CP</given-names></name>. <article-title>The expensive brain: a framework for explaining evolutionary changes in brain size</article-title>. <source>Journal of Human Evolution</source>. <year>2009</year>;<volume>57</volume>(<issue>4</issue>):<fpage>392</fpage>–<lpage>400</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jhevol.2009.04.009" xlink:type="simple">10.1016/j.jhevol.2009.04.009</ext-link></comment> <object-id pub-id-type="pmid">19732937</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bosman</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Aboitiz</surname> <given-names>F</given-names></name>. <article-title>Functional constraints in the evolution of brain circuits</article-title>. <source>Frontiers in neuroscience</source>. <year>2015</year>;<volume>9</volume>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Breitmeyer</surname> <given-names>BG</given-names></name>, <name name-style="western"><surname>Ganz</surname> <given-names>L</given-names></name>. <article-title>Implications of sustained and transient channels for theories of visual pattern masking, saccadic suppression, and information processing</article-title>. <source>Psychological review</source>. <year>1976</year>;<volume>83</volume>(<issue>1</issue>):<fpage>1</fpage>. <object-id pub-id-type="pmid">766038</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Portilla</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>A parametric texture model based on joint statistics of complex wavelet coefficients</article-title>. <source>International journal of computer vision</source>. <year>2000</year>;<volume>40</volume>(<issue>1</issue>):<fpage>49</fpage>–<lpage>70</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Taulu</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Simola</surname> <given-names>J</given-names></name>. <article-title>Spatiotemporal signal space separation method for rejecting nearby interference in MEG measurements</article-title>. <source>Physics in Medicine &amp; Biology</source>. <year>2006</year>;<volume>51</volume>(<issue>7</issue>):<fpage>1759</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tadel</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Baillet</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mosher</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Pantazis</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Leahy</surname> <given-names>RM</given-names></name>. <article-title>Brainstorm: a user-friendly application for MEG/EEG analysis</article-title>. <source>Computational intelligence and neuroscience</source>. <year>2011</year>;<volume>2011</volume>:<fpage>8</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pelli</surname> <given-names>DG</given-names></name>. <article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title>. <source>Spatial vision</source>. <year>1997</year>;<volume>10</volume>(<issue>4</issue>):<fpage>437</fpage>–<lpage>42</lpage>. <object-id pub-id-type="pmid">9176953</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kietzmann</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Gert</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Tong</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>König</surname> <given-names>P</given-names></name>. <article-title>Representational dynamics of facial viewpoint encoding</article-title>. <source>Journal of cognitive neuroscience</source>. <year>2017</year>;<volume>29</volume>(<issue>4</issue>):<fpage>637</fpage>–<lpage>51</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_01070" xlink:type="simple">10.1162/jocn_a_01070</ext-link></comment> <object-id pub-id-type="pmid">27791433</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname> <given-names>C-C</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>C-J</given-names></name>. <article-title>LIBSVM: a library for support vector machines</article-title>. <source>ACM transactions on intelligent systems and technology (TIST)</source>. <year>2011</year>;<volume>2</volume>(<issue>3</issue>):<fpage>27</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Relating population-code representations between man, monkey, and computational models</article-title>. <source>Frontiers in Neuroscience</source>. <year>2009</year>;<volume>3</volume>:<fpage>35</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref095"><label>95</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kievit</surname> <given-names>RA</given-names></name>. <article-title>Representational geometry: integrating cognition, computation, and the brain</article-title>. <source>Trends in cognitive sciences</source>. <year>2013</year>;<volume>17</volume>(<issue>8</issue>):<fpage>401</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2013.06.007" xlink:type="simple">10.1016/j.tics.2013.06.007</ext-link></comment> <object-id pub-id-type="pmid">23876494</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref096"><label>96</label><mixed-citation publication-type="other" xlink:type="simple">Khaligh-Razavi S-M, Bainbridge WA, Pantazis D, Oliva A. From what we perceive to what we remember: Characterizing representational dynamics of visual memorability. bioRxiv. 2016:049700.</mixed-citation></ref>
<ref id="pcbi.1007001.ref097"><label>97</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Pedzahur</surname> <given-names>E</given-names></name>. <source>Multiple regression in behavioral research: Explanation and prediction</source>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>Wadsworth, Thompson Learning</publisher-name>. <year>1997</year>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref098"><label>98</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Gibbons</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Chakraborti</surname> <given-names>S</given-names></name>. <chapter-title>Nonparametric statistical inference</chapter-title>. <source>International encyclopedia of statistical science</source>: <publisher-name>Springer</publisher-name>; <year>2011</year>. p. <fpage>977</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref099"><label>99</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benjamini</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hochberg</surname> <given-names>Y</given-names></name>. <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source>Journal of the royal statistical society Series B (Methodological)</source>. <year>1995</year>:<fpage>289</fpage>–<lpage>300</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref100"><label>100</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nili</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wingfield</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Walther</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Marslen-Wilson</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>A toolbox for representational similarity analysis</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>4</issue>):<fpage>e1003553</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003553" xlink:type="simple">10.1371/journal.pcbi.1003553</ext-link></comment> <object-id pub-id-type="pmid">24743308</object-id></mixed-citation></ref>
<ref id="pcbi.1007001.ref101"><label>101</label><mixed-citation publication-type="other" xlink:type="simple">Krizhevsky A, Sutskever I, Hinton GE, editors. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems; 2012.</mixed-citation></ref>
<ref id="pcbi.1007001.ref102"><label>102</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Russakovsky</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Deng</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Krause</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Satheesh</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Imagenet large scale visual recognition challenge</article-title>. <source>International Journal of Computer Vision</source>. <year>2015</year>;<volume>115</volume>(<issue>3</issue>):<fpage>211</fpage>–<lpage>52</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007001.ref103"><label>103</label><mixed-citation publication-type="other" xlink:type="simple">He K, Zhang X, Ren S, Sun J, editors. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. Proceedings of the IEEE international conference on computer vision; 2015.</mixed-citation></ref>
<ref id="pcbi.1007001.ref104"><label>104</label><mixed-citation publication-type="other" xlink:type="simple">Glorot X, Bengio Y, editors. Understanding the difficulty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics; 2010.</mixed-citation></ref>
<ref id="pcbi.1007001.ref105"><label>105</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mormann</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Kornblith</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Quiroga</surname> <given-names>RQ</given-names></name>, <name name-style="western"><surname>Kraskov</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Cerf</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fried</surname> <given-names>I</given-names></name>, <etal>et al</etal>. <article-title>Latency and selectivity of single neurons indicate hierarchical processing in the human medial temporal lobe</article-title>. <source>Journal of Neuroscience</source>. <year>2008</year>;<volume>28</volume>(<issue>36</issue>):<fpage>8865</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1640-08.2008" xlink:type="simple">10.1523/JNEUROSCI.1640-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18768680</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>