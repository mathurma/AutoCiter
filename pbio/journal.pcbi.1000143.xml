<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">07-PLCB-RA-0754R2</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000143</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology/Computational Neuroscience</subject><subject>Computational Biology/Evolutionary Modeling</subject></subj-group></article-categories><title-group><article-title>Falling towards Forgetfulness: Synaptic Decay Prevents Spontaneous Recovery of Memory</article-title><alt-title alt-title-type="running-head">Falling towards Forgetfulness</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Stone</surname><given-names>James V.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Jupp</surname><given-names>Peter E.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Psychology Department, Sheffield University, Sheffield, United Kingdom</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>School of Mathematics and Statistics, University of St Andrews, St Andrews, United Kingdom</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">j.v.stone@sheffield.ac.uk</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: JS. Performed the experiments: JS. Analyzed the data: JS. Contributed reagents/materials/analysis tools: JS. Wrote the paper: JS PEJ. Mathematical proofs: PEJ.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>8</month><year>2008</year></pub-date><pub-date pub-type="epub"><day>22</day><month>8</month><year>2008</year></pub-date><volume>4</volume><issue>8</issue><elocation-id>e1000143</elocation-id><history>
<date date-type="received"><day>7</day><month>12</month><year>2007</year></date>
<date date-type="accepted"><day>25</day><month>6</month><year>2008</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2008</copyright-year><copyright-holder>Stone, Jupp</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>Long after a new language has been learned and forgotten, relearning a few words seems to trigger the recall of other words. This “free-lunch learning” (FLL) effect has been demonstrated both in humans and in neural network models. Specifically, previous work proved that linear networks that learn a set of associations, then partially forget them all, and finally relearn some of the associations, show improved performance on the remaining (i.e., nonrelearned) associations. Here, we prove that relearning forgotten associations <italic>decreases</italic> performance on nonrelearned associations; an effect we call negative free-lunch learning. The difference between free-lunch learning and the negative free-lunch learning presented here is due to the particular method used to induce forgetting. Specifically, if forgetting is induced by isotropic <italic>drifting</italic> of weight vectors (i.e., by adding isotropic noise), then free-lunch learning is observed. However, as proved here, if forgetting is induced by weight values that simply decay or <italic>fall</italic> towards zero, then negative free-lunch learning is observed. From a biological perspective, and assuming that nervous systems are analogous to the networks used here, this suggests that evolution may have selected physiological mechanisms that involve forgetting using a form of synaptic drift rather than synaptic decay, because synaptic drift, but not synaptic decay, yields free-lunch learning.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>If you learn a skill, then partially forget it, does relearning part of that skill induce recovery of other parts of the skill? More generally, if you learn a set of associations, then partially forget them, does relearning a subset induce recovery of the remaining associations? In previous work, in which participants learned the layout of a scrambled computer keyboard, the answer to this question appeared to be “yes.” More recently, we modeled this “free-lunch learning” effect using artificial neural networks, in which the synaptic strength between each pair of model neurons is a connection weight. We proved that if forgetting is induced by allowing each weight value to drift randomly, then free-lunch learning is almost inevitable. However, if, after learning a set of associations, forgetting is induced by allowing each connection weight to <italic>decay</italic> or <italic>fall</italic> toward zero, then relearning a subset of associations decreases performance on the remaining associations. This suggests that evolution may have selected physiological mechanisms that involve forgetting using a form of synaptic drift rather than synaptic decay, because synaptic drift yields free-lunch learning, whereas decay does not.</p>
</abstract><funding-group><funding-statement>No funding was received for this work.</funding-statement></funding-group><counts><page-count count="8"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>The idea that structural changes underpin the formation of new memories can be traced to the 19th century <xref ref-type="bibr" rid="pcbi.1000143-Tanzi1">[1]</xref>. More recently, Hebb proposed that “When an axon of cell A is near enough to excite B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased” <xref ref-type="bibr" rid="pcbi.1000143-Hebb1">[2]</xref>. It is now widely accepted that learning involves some form of Hebbian adaptation, and a growing body of evidence suggests that Hebbian adaptation is associated with the long-term potentiation (LTP) observed in neuronal systems <xref ref-type="bibr" rid="pcbi.1000143-Abraham1">[3]</xref>. LTP is an increase in synaptic efficacy which occurs in the presence of pre-synaptic and post-synaptic activity, and can be specific to a single synapse. One consequence of Hebbian adaptation is that information regarding a specific association is distributed amongst many synaptic connections, and therefore gives rise to a distributed representation of each association.</p>
<p>In <xref ref-type="bibr" rid="pcbi.1000143-Stone1">[4]</xref>, participants learned the layout of letters on a “scrambled” keyboard. After a period of forgetting, participants relearned a subset of letter positions. Crucially, this improved performance on the remaining (i.e., nonrelearned) letter positions. However, whereas relearning some associations shows evidence of FLL in some studies <xref ref-type="bibr" rid="pcbi.1000143-Stone1">[4]</xref>–<xref ref-type="bibr" rid="pcbi.1000143-Weekes1">[6]</xref>, this is not found in not all studies <xref ref-type="bibr" rid="pcbi.1000143-Atkins1">[7]</xref>. This discrepancy may be because the many studies performed to investigate this general phenomenon use a wide variety of different materials and procedures, with some measuring recall and others measuring recognition performance, for example. However, within the realms of psychology, one relevant effect is known as part-set cueing inhibition.</p>
<p>Part-set cueing inhibition <xref ref-type="bibr" rid="pcbi.1000143-Roediger1">[8]</xref> occurs when a subject is exposed to part of a set of previously learned items, which is found to reduce recall of nonrelearned items. However, <xref ref-type="bibr" rid="pcbi.1000143-Serra1">[9]</xref> showed that a learned row of words was better recalled if the cues consisted of a subset of words placed in their learned positions than if cue words were placed in other positions. In this case, part-set cueing seems to improve performance, but only if each “part” appears in the spatial position in which it was originally learned. This position-specificity is consistent with the FLL effect reported using the “scrambled keyboard” procedure in <xref ref-type="bibr" rid="pcbi.1000143-Stone1">[4]</xref> but has no obvious concomitant in network models (e.g., <xref ref-type="bibr" rid="pcbi.1000143-Stone1">[4]</xref>,<xref ref-type="bibr" rid="pcbi.1000143-Hinton1">[10]</xref>,<xref ref-type="bibr" rid="pcbi.1000143-Atkins2">[11]</xref>).</p>
<p>If the brain stores information as distributed representations, then each neuron contributes to the storage of many associations. Therefore, relearning some old and partially forgotten associations should affect the integrity of other associations learned at about the same time. As noted above, previous work has shown that relearning some forgotten associations does not disrupt other associations, but partially restores them. This FLL effect has also been demonstrated in neural network models (<xref ref-type="bibr" rid="pcbi.1000143-Hinton1">[10]</xref>,<xref ref-type="bibr" rid="pcbi.1000143-Stone2">[12]</xref>), where it can accelerate evolution of adaptive behaviors <xref ref-type="bibr" rid="pcbi.1000143-Stone3">[13]</xref>. Crucially, in <xref ref-type="bibr" rid="pcbi.1000143-Stone2">[12]</xref>, the proof that relearning some associations partially restores other associations assumes that forgetting is caused by the addition of isotropic noise to connection weights, which could result from the cumulative effect of small random changes in connection weights. In contrast, here we prove that if forgetting is induced by shrinking weights towards zero, so that weights “fall” towards the origin, then relearning some associations disrupts other associations.</p>
<p>The protocol used to examine FLL here is the same as that used in <xref ref-type="bibr" rid="pcbi.1000143-Stone1">[4]</xref> and <xref ref-type="bibr" rid="pcbi.1000143-Stone2">[12]</xref> and is as follows (see <xref ref-type="fig" rid="pcbi-1000143-g001">Figure 1</xref>). First, learn a set of <italic>n</italic><sub>1</sub>+<italic>n</italic><sub>2</sub> associations <italic>A</italic> = <italic>A</italic><sub>1</sub>∪<italic>A</italic><sub>2</sub> consisting of two subsets <italic>A</italic><sub>1</sub> and <italic>A</italic><sub>2</sub> of <italic>n</italic><sub>1</sub> and <italic>n</italic><sub>2</sub> associations, respectively. After all learned associations <italic>A</italic> have been partially forgotten, measure performance error on subset <italic>A</italic><sub>1</sub>. Finally, relearn <italic>only</italic> subset <italic>A</italic><sub>2</sub> and then remeasure performance on subset <italic>A</italic><sub>1</sub>. FLL occurs if relearning subset <italic>A</italic><sub>2</sub> improves performance on <italic>A</italic><sub>1</sub>.</p>
<fig id="pcbi-1000143-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000143.g001</object-id><label>Figure 1</label><caption>
<title>Free-lunch learning protocol.</title>
<p>Two subsets of associations <italic>A</italic><sub>1</sub> and <italic>A</italic><sub>2</sub> are learned. After partial forgetting (see text), performance error <italic>E</italic><sub>pre</sub> on subset <italic>A</italic><sub>1</sub> is measured. Subset <italic>A</italic><sub>2</sub> is then relearned to pre-forgetting levels of performance, and performance error <italic>E</italic><sub>post</sub> on subset <italic>A</italic><sub>1</sub> is re-measured. If <italic>E</italic><sub>post</sub>&lt;<italic>E</italic><sub>pre</sub> then FLL has occurred, and the amount of FLL is <italic>δ</italic> = <italic>E</italic><sub>pre</sub>−<italic>E</italic><sub>post</sub>. Redrawn from <xref ref-type="bibr" rid="pcbi.1000143-Stone2">[12]</xref>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.g001" xlink:type="simple"/></fig>
<p>In order to preclude a common misunderstanding, we emphasize that, for a network with <italic>n</italic> connection weights, it is assumed that <italic>n</italic>≥<italic>n</italic><sub>1</sub>+<italic>n</italic><sub>2</sub> ; that is, the number of connection weights on each output unit is not less than the number <italic>n</italic><sub>1</sub>+<italic>n</italic><sub>2</sub> of learned associations. Using the class of linear network models described below, up to <italic>n</italic> associations can be learned perfectly (see <xref ref-type="bibr" rid="pcbi.1000143-Stone2">[12]</xref>).</p>
<p>The proofs below refer to a network with one output unit. However, these proofs apply to networks with multiple output units, because the <italic>n</italic> connections to each output unit can be considered as a distinct network, in which case our results can be applied to the network associated with each output unit.</p>
<sec id="s1a">
<title>Definition of Performance Error</title>
<p>Each association consists of an input vector <bold>x</bold> and a corresponding target value <italic>d</italic>. For a network with weight vector <bold>w</bold>, the response to an input vector <bold>x</bold> is <italic>y</italic> = <bold>w·x</bold>. We define the <italic>performance error</italic> for input vectors <bold>x</bold><sub>1</sub>,…,<bold>x</bold><italic><sub>k</sub></italic> and desired outputs <italic>d</italic><sub>1</sub>,…,<italic>d<sub>k</sub></italic> to be<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e001" xlink:type="simple"/><label>(1)</label></disp-formula>where <italic>y<sub>i</sub></italic> = <bold>w</bold>·<bold>x</bold><italic><sub>i</sub></italic> is the output response to the input vector <bold>x</bold><italic><sub>i</sub></italic>. By putting <bold>X</bold> = (<bold>x</bold><sub>1</sub>,…,<bold>x</bold><italic><sub>k</sub></italic>)<italic><sup>T</sup></italic>, <bold>d</bold> = (<italic>d</italic><sub>1</sub>,…,<italic>d<sub>k</sub></italic>)<italic><sup>T</sup></italic> and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e002" xlink:type="simple"/></disp-formula>we can write Equation 1 succinctly as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e003" xlink:type="simple"/><label>(2)</label></disp-formula></p>
<p>The two subsets <italic>A</italic><sub>1</sub> and <italic>A</italic><sub>2</sub> consist of <italic>n</italic><sub>1</sub> and <italic>n</italic><sub>2</sub> associations, respectively. Let <bold>w</bold><sub>0</sub> be the network weight vector after <italic>A</italic><sub>1</sub> and <italic>A</italic><sub>2</sub> are learned. When <italic>A</italic><sub>1</sub> and <italic>A</italic><sub>2</sub> are forgotten, the network weight vector changes to <bold>w</bold><sub>1</sub>, say, and the performance error on <italic>A</italic><sub>1</sub> becomes <italic>E</italic><sub>pre</sub> = <italic>E</italic>(<bold>X</bold>;<bold>w</bold><sub>1</sub>,<bold>d</bold>). Finally, relearning <italic>A</italic><sub>2</sub> yields a new weight vector, <bold>w</bold><sub>2</sub>, say, and the performance error on <italic>A</italic><sub>1</sub> is <italic>E</italic><sub>post</sub> = <italic>E</italic>(<bold>X</bold>;<bold>w</bold><sub>2</sub>,<bold>d</bold>). Free-lunch learning has occurred if performance error on <italic>A</italic><sub>1</sub> is less after relearning <italic>A</italic><sub>2</sub> than it was before relearning <italic>A</italic><sub>2</sub> (i.e., if <italic>E</italic><sub>post</sub>&lt;<italic>E</italic><sub>pre</sub>).</p>
<p>Given weight vectors <bold>w</bold><sub>1</sub> and <bold>w</bold><sub>2</sub>, a matrix <bold>X</bold> of input vectors, and a vector <bold>d</bold> of desired outputs, define<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e004" xlink:type="simple"/><label>(3)</label></disp-formula>which we shall also refer to simply as <italic>δ</italic>.</p>
<p>In previous work <xref ref-type="bibr" rid="pcbi.1000143-Stone2">[12]</xref>, we assumed that the “forgetting vector” <bold>v</bold> (defined as <bold>v</bold> = <bold>w</bold><sub>1</sub>−<bold>w</bold><sub>0</sub>) has an isotropic distribution. Here we shall assume instead that the post-forgetting weight vector <bold>w</bold><sub>1</sub> is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e005" xlink:type="simple"/><label>(4)</label></disp-formula>for some (possibly random) scalar <italic>r</italic>, so that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e006" xlink:type="simple"/><label>(5)</label></disp-formula>and therefore<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e007" xlink:type="simple"/><label>(6)</label></disp-formula>The interpretation of Equation 6 is that forgetting consists of making the optimal weight vector <bold>w</bold><sub>0</sub> “fall” towards the origin by a <italic>falling factor</italic> 1−<italic>r</italic>.</p>
</sec></sec><sec id="s2">
<title>Results</title>
<p>We provide theoretical results, and compare these with results obtained using computer simulations. In essence, our theoretical and simulation results indicate that falling weights induce negative FLL, which decreases with the square of the falling factor 1−<italic>r</italic>.</p>
<sec id="s2a">
<title>Theoretical Results</title>
<p>Our two main theorems are summarised here, and proofs are provided in the <xref ref-type="sec" rid="s4">Methods</xref> section. These theorems apply to a network with <italic>n</italic> weights which learns <italic>n</italic><sub>1</sub>+<italic>n</italic><sub>2</sub> associations <italic>A</italic> = <italic>A</italic><sub>1</sub>∪<italic>A</italic><sub>2</sub>, and then after partial forgetting, relearns the <italic>n</italic><sub>2</sub> associations in <italic>A</italic><sub>2</sub>.</p>
<p>We prove that if <italic>n</italic><sub>1</sub>+<italic>n</italic><sub>2</sub>≤<italic>n</italic> (so that, in general, the associations <italic>A</italic><sub>1</sub> and <italic>A</italic><sub>2</sub> are consistent) and the joint distribution of (<bold>X</bold><sub>1</sub>,<bold>d</bold><sub>1</sub>) is isotropic (where <bold>X</bold><sub>1</sub> and <bold>d</bold><sub>1</sub> are the matrix of inputs and the vector of desired outputs for subset <italic>A</italic><sub>1</sub> of associations) then the expected value of <italic>δ</italic> is negative (recall that <italic>δ</italic> is defined in Equation 3). We then prove that the probability <italic>P</italic>(<italic>δ</italic>&lt;0) that <italic>δ</italic> is negative approaches unity as <italic>n</italic><sub>1</sub> approaches ∞.</p>
</sec><sec id="s2b">
<title>Theorem 1</title>
<p>For every non-zero value of <italic>r</italic>, the expected value of <italic>δ</italic> given <italic>r</italic> is negative. More precisely,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e008" xlink:type="simple"/><label>(7)</label></disp-formula>with equality only in trivial cases, and where the constant of proportionality is guaranteed to be positive. Thus, the expected amount of FLL is negative (or zero).</p>
<p>From a physiological perspective, the case <italic>r</italic>&lt;1 is obviously of interest because it represents synaptic weight decay. However, from a mathematical perspective, Theorem 1 applies to every value of <italic>r</italic>, and so it also holds for <italic>r</italic>&gt;1. In other words, any movement of the weight vector <bold>w</bold> along the the line connecting <bold>w</bold><sub>0</sub> to the origin yields an expectation of negative FLL, in accordance with Theorem 1.</p>
</sec><sec id="s2c">
<title>Theorem 2</title>
<p>Under mild conditions on the distributions of the input/output pairs (<bold>X</bold><sub>1</sub>,<bold>d</bold><sub>1</sub>) and (<bold>X</bold><sub>2</sub>,<bold>d</bold><sub>2</sub>),<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e009" xlink:type="simple"/><label>(8)</label></disp-formula>where <bold>x</bold> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e010" xlink:type="simple"/></inline-formula> are any columns of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e011" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e012" xlink:type="simple"/></inline-formula>, respectively, and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e013" xlink:type="simple"/></disp-formula></p>
<p>Theorem 2 implies that, if (i) the number (<italic>n</italic><sub>1</sub>) of associations in <italic>A</italic><sub>1</sub> is a fixed non-zero proportion ( <italic>n</italic><sub>1</sub>/<italic>n</italic> ) of the number <italic>n</italic> of connection weights, (ii) <bold>E</bold>[∥<bold>d</bold><sub>1</sub>∥<sup>2</sup>]<bold>E</bold>[∥<bold>d</bold><sub>2</sub>∥<sup>−2</sup>] is bounded as <italic>n</italic> → ∞, and (iii) <italic>γ</italic>(<italic>n</italic>) → 0 as <italic>n</italic> → ∞ then <italic>P</italic>(<italic>δ</italic>&gt;0) → 0 as <italic>n</italic> → ∞, i.e., the amount of FLL is negative, with a probability which tends to 1 as <italic>n</italic> → ∞.</p>
<p>For example, if we assume that (i) each input vector <bold>x</bold> = (<italic>x</italic><sub>1</sub>,…,<italic>x<sub>n</sub></italic>) is chosen from an isotropic Gaussian distribution and (ii) the variance of <italic>x<sub>i</sub></italic> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e014" xlink:type="simple"/></inline-formula> then <italic>γ</italic>(<italic>n</italic>) = 2/<italic>n</italic>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e015" xlink:type="simple"/></inline-formula>, and <bold>E</bold>[∥<bold>d</bold><sub>1</sub>∥<sup>2</sup>]<bold>E</bold>[∥<bold>d</bold><sup>2</sup>∥<sup>−2</sup>] = <italic>n</italic><sub>1</sub>/(<italic>n</italic><sub>2</sub>−1). This ensures that <italic>P</italic>(<italic>δ</italic>&gt;0) → 0 as <italic>n</italic> → ∞.</p>
</sec><sec id="s2d">
<title>Simulation Results</title>
<p>Simulation was carried out on a network with <italic>n</italic> input units and one output unit. The set <italic>A</italic> of associations consisted of <italic>k</italic> input vectors (<bold>x</bold><sub>1</sub>,…,<bold>x</bold><italic><sub>k</sub></italic>) and <italic>k</italic> corresponding desired scalar output values (<italic>d</italic><sub>1</sub>,…,<italic>d<sub>k</sub></italic>). Each input vector comprised <italic>n</italic> elements <bold>x</bold> = (<italic>x</italic><sub>1</sub>,…,<italic>x<sub>n</sub></italic>). The values of <italic>x<sub>i</sub></italic> and <italic>d<sub>i</sub></italic> were chosen from a Gaussian distribution with unit variance (i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e016" xlink:type="simple"/></inline-formula>). A network's output <italic>y<sub>i</sub></italic> is a weighted sum of input values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e017" xlink:type="simple"/></inline-formula>, where <italic>x<sub>ij</sub></italic> is the <italic>j</italic>th component of the <italic>i</italic>th input vector <bold>x</bold><italic><sub>i</sub></italic>, and each weight <italic>w<sub>j</sub></italic> is the connection between the <italic>j</italic>th input unit and the output unit.</p>
<p>Given that the network error for a given set of <italic>k</italic> associations is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e018" xlink:type="simple"/></inline-formula>, the derivative <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e019" xlink:type="simple"/></inline-formula> of <italic>E</italic> with respect to <bold>w</bold> yields the delta learning rule <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e020" xlink:type="simple"/></inline-formula>, where <italic>η</italic> is the learning rate, which is adjusted according to the number of weights.</p>
<p>However, in order to save time, we used an equivalent learning method. Learning of the <italic>k</italic> = <italic>n</italic> associations in <italic>A</italic> = <italic>A</italic><sub>1</sub>∪<italic>A</italic><sub>2</sub> was performed by solving a set of <italic>n</italic> simultaneous equations using a standard method, after which the weight vector <bold>w</bold><sub>0</sub> was obtained; this provided perfect performance on all <italic>n</italic> associations. Partial forgetting was induced by making weights “fall” towards the origin <bold>w</bold><sub>1</sub> = <italic>r</italic><bold>w</bold><sub>0</sub>, after which performance error was <italic>E</italic><sub>pre</sub>. Relearning the <italic>n</italic><sub>2</sub> = <italic>n</italic>/2 associations in <italic>A</italic><sub>2</sub> was implemented with <italic>k</italic> = <italic>n</italic><sub>2</sub> as above, after which performance error was <italic>E</italic><sub>post</sub>.</p>
<p>In each simulation, each value in each input vector <bold>x</bold><italic><sub>i</sub></italic>, and each target value <italic>d<sub>i</sub></italic> was chosen from the same isotropic gaussian distribution with unit variance. There were 100 input units, and one output unit. The subsets <italic>A</italic><sub>1</sub> and <italic>A</italic><sub>2</sub> each consisted of 50 associations. The value of <italic>δ</italic> = <italic>E</italic><sub>pre</sub>−<italic>E</italic><sub>post</sub> was obtained in each of 100 simulations, using a different random seed for each simulation. In <xref ref-type="fig" rid="pcbi-1000143-g002">Figure 2</xref>, the mean of 100 values of <italic>δ</italic> is shown for various values of the falling factor 1−<italic>r</italic>.</p>
<fig id="pcbi-1000143-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000143.g002</object-id><label>Figure 2</label><caption>
<title>Free-lunch learning decreases as the network's weight vector falls toward the origin.</title>
<p>A network with 100 input units and one output unit learns two subsets <italic>A</italic><sub>1</sub> and <italic>B</italic><sub>2</sub>, each of which consists of 50 associations. After learning <italic>A</italic><sub>1</sub> and <italic>A</italic><sub>2</sub>, the network has a weight vector w = w<sub>0</sub>, but after partial forgetting, the weight vector is w = w<sub>1</sub>. If forgetting consists of subtracting a proportion 1−<italic>r</italic> of w<sub>0</sub> such that w<sub>1</sub> = w<sub>0</sub>−(1−<italic>r</italic>)w<sub>0</sub> then the weight vector “falls” towards the origin; the factor 1−<italic>r</italic> is called the <italic>falling factor</italic>. After forgetting, performance error on <italic>A</italic><sub>1</sub> is <italic>E</italic><sub>pre</sub>, an error which changes to <italic>E</italic><sub>post</sub> after relearning <italic>A</italic><sub>2</sub>, where this change is <italic>δ</italic> = <italic>E</italic><sub>pre</sub>−<italic>E</italic><sub>post</sub>. Given that there are <italic>A</italic><sub>1</sub> associations in <italic>A</italic><sub>1</sub>, the expected free-lunch learning per association in <italic>A</italic><sub>1</sub> is therefore E[<italic>δ</italic>/<italic>n</italic><sub>1</sub>|<italic>r</italic>]. <italic>Solid curve</italic>: the expected FLL, <italic>E</italic>[<italic>δ</italic>/<italic>n</italic><sub>1</sub>|<italic>r</italic>], where this expectation is taken over 100 computer simulations. <italic>Dashed curve</italic>: theoretical prediction of <italic>E</italic>[<italic>δ</italic>/<italic>n</italic><sub>1</sub>|<italic>r</italic>] (see Equation 7), using a constant of proportionality equal to unity, so that the predicted free-lunch learning is <italic>E</italic><sub>predict</sub>[<italic>δ</italic>/<italic>n</italic><sub>1</sub>|<italic>r</italic>] = −(1−<italic>r</italic>)<sup>2</sup>. As predicted, free-lunch learning <italic>E</italic>[<italic>δ</italic>/<italic>n</italic><sub>1</sub>|<italic>r</italic>] becomes more negative as the falling factor 1−<italic>r</italic> increases.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.g002" xlink:type="simple"/></fig></sec><sec id="s2e">
<title>The Geometry of Forgetting</title>
<p>We present a brief account of the geometry which underpins the results reported here, for a network with two input units and one output unit, as shown in <xref ref-type="fig" rid="pcbi-1000143-g003">Figure 3A</xref>. This network learns two associations <italic>A</italic><sub>1</sub> = (<italic>X</italic><sub>1</sub>,<italic>d</italic><sub>1</sub>) and <italic>A</italic><sub>2</sub> = (<italic>X</italic><sub>2</sub>,<italic>d</italic><sub>2</sub>).</p>
<fig id="pcbi-1000143-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000143.g003</object-id><label>Figure 3</label><caption>
<title>Geometric example of how relearning <italic>A</italic><sub>2</sub> increases the error on <italic>A</italic><sub>1</sub>.</title>
<p>(A) A network with two input units and one output unit, with connection weights <italic>ω<sub>a</sub></italic> and <italic>ω<sub>b</sub></italic> defines a weight vector w = (<italic>ω<sub>a</sub></italic>,<italic>ω<sub>b</sub></italic>). The network learns two associations <italic>A</italic><sub>1</sub> and <italic>A</italic><sub>2</sub>. For example, <italic>A</italic><sub>1</sub> is the mapping from input vector x<sub>1</sub> = (<italic>x</italic><sub>11</sub>,<italic>x</italic><sub>12</sub>) to desired output value <italic>d</italic><sub>1</sub>, and learning <italic>A</italic><sub>1</sub> consists of adjusting w until the network output <italic>y</italic><sub>1</sub> = w·x<sub>1</sub> equals <italic>d</italic><sub>1</sub>. (B) For a given association <italic>A</italic><sub>2</sub> = (<italic>X</italic><sub>2</sub>,<italic>d</italic><sub>2</sub>), the corresponding constraint line in the space defined by (<italic>ω<sub>a</sub></italic>,<italic>ω<sub>b</sub></italic>) is <italic>L</italic><sub>2</sub>. Irrespective of the precise value of the target output value <italic>d</italic><sub>1</sub> in association <italic>A</italic><sub>1</sub>, if <italic>d</italic><sub>1</sub> is distributed isotropically then +<italic>d</italic><sub>1</sub> is as probable as −<italic>d</italic><sub>1</sub>. When averaged over +<italic>d</italic><sub>1</sub> and −<italic>d</italic><sub>1</sub>, the change <italic>δ</italic> in error on <italic>A</italic><sub>1</sub> induced by relearning <italic>A</italic><sub>2</sub> can be shown to be −(1−<italic>r</italic>)<sup>2</sup><italic>e</italic><sup>2</sup>, where w<sub>1</sub><sup>±</sup> = <italic>r</italic>w<sub>0</sub><sup>±</sup>. Since this is less than zero, the expected change <italic>E</italic>[<italic>δ</italic>|<italic>r</italic>]&lt;0. (<xref ref-type="fig" rid="pcbi-1000143-g003">Figure 3A</xref> redrawn from <xref ref-type="bibr" rid="pcbi.1000143-Stone2">[12]</xref>).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.g003" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1000143-g003">Figure 3B</xref> provides a geometric example of how relearning <italic>A</italic><sub>2</sub> increases the error on <italic>A</italic><sub>1</sub>. After learning <italic>A</italic><sub>1</sub> and <italic>A</italic><sub>2</sub>, <bold>w</bold> = <bold>w</bold><sub>0</sub>. The effects of forgetting and relearning can be seen by ignoring the ± superscripts and subscripts for now. After partial forgetting, <bold>w</bold> = <bold>w</bold><sub>1</sub>, and performance error <italic>E</italic><sub>pre</sub> = <italic>p</italic><sup>2</sup>. Relearning <italic>A</italic><sub>2</sub> yields <bold>w</bold><sub>2</sub>, the orthogonal projection of <bold>w</bold><sub>1</sub> on to <italic>L</italic><sub>2</sub>, and performance error is <italic>E</italic><sub>post</sub> = <italic>q</italic><sup>2</sup>. FLL occurs if <italic>δ</italic> = <italic>E</italic><sub>pre</sub>−<italic>E</italic><sub>post</sub>&gt;0, or equivalently if <italic>p</italic><sup>2</sup>−<italic>q</italic><sup>2</sup>&gt;0 (see <xref ref-type="bibr" rid="pcbi.1000143-Stone2">[12]</xref>, Appendices A–C for proofs). Forgetting here consists of reducing <bold>w</bold><sub>0</sub> by a factor <italic>r</italic>&lt;1, so that <bold>w</bold><sub>1</sub> = <italic>r</italic><bold>w</bold><sub>0</sub>.</p>
<p>The plus and minus signs in <xref ref-type="fig" rid="pcbi-1000143-g003">Figure 3B</xref> refer to two versions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e021" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e022" xlink:type="simple"/></inline-formula> of association <italic>A</italic><sub>1</sub>, in which <italic>X</italic><sub>1</sub> is the same and the target <italic>d</italic><sub>1</sub> has the same magnitude, but opposite signs: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e023" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e024" xlink:type="simple"/></inline-formula>.</p>
<p>We now find the expected change in error induced by relearning a given association <italic>A</italic><sub>2</sub>. After learning <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e025" xlink:type="simple"/></inline-formula> followed by forgetting, the change in error on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e026" xlink:type="simple"/></inline-formula> after relearning <italic>A</italic><sub>2</sub> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e027" xlink:type="simple"/></inline-formula>. After learning <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e028" xlink:type="simple"/></inline-formula> followed by forgetting, the change in error on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e029" xlink:type="simple"/></inline-formula> after relearning <italic>A</italic><sub>2</sub> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e030" xlink:type="simple"/></inline-formula>. Using similar triangles in <xref ref-type="fig" rid="pcbi-1000143-g003">Figure 3B</xref>,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e031" xlink:type="simple"/><label>(9)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e032" xlink:type="simple"/><label>(10)</label></disp-formula>Therefore, the total change in error on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e033" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e034" xlink:type="simple"/></inline-formula> induced by relearning <italic>A</italic><sub>2</sub> (on different occasions) is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e035" xlink:type="simple"/><label>(11)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e036" xlink:type="simple"/><label>(12)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e037" xlink:type="simple"/><label>(13)</label></disp-formula>Irrespective of the precise value of the target output value <italic>d</italic><sub>1</sub> in <italic>A</italic><sub>1</sub>, if the distribution of <italic>d</italic><sub>1</sub> is isotropic then +<italic>d</italic><sub>1</sub> is as probable as −<italic>d</italic><sub>1</sub>. If the total change in error for two instances (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e038" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e039" xlink:type="simple"/></inline-formula>) of <italic>A</italic><sub>1</sub> is −2(1−<italic>r</italic>)<sup>2</sup><italic>e</italic><sup>2</sup> then the expected change (conditional on <italic>e</italic> ) is <italic>E</italic>[<italic>δ</italic>|<italic>e</italic>] = −(1−<italic>r</italic>)<sup>2</sup><italic>e</italic><sup>2</sup>. Therefore, if forgetting is induced by falling weight values, then the expected change in error <bold>E</bold>[<italic>δ</italic>]&lt;0.</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<p>We have proved and demonstrated that, in one of the simplest forms of neural network model, relearning part of a previously learned set of associations reduces performance on the remaining non-relearned associations. This result is in stark contrast to our previous results, which proved that relearning induced partial recovery of non-relearned items <xref ref-type="bibr" rid="pcbi.1000143-Stone2">[12]</xref>. The only difference between these two studies is the way in which forgetting was induced.</p>
<p>An obvious physiological concomitant of Hebbian learning is long-term potentiation (LTP), which seems to underpin learned behaviors <xref ref-type="bibr" rid="pcbi.1000143-Whitlock1">[14]</xref>. LTP can last for hours, days or even months, and usually follows an exponential decay <xref ref-type="bibr" rid="pcbi.1000143-Abraham1">[3]</xref>. However, some forms of LTP do not seem to decay <xref ref-type="bibr" rid="pcbi.1000143-Staubli1">[15]</xref>, and have been shown to be stable for up to one year <xref ref-type="bibr" rid="pcbi.1000143-Abraham2">[16]</xref>. Such stability is remarkable, but from a statistical point of view, would almost certainly be accompanied by random fluctuations which would have a cumulative effect over time; and indeed, fluctuations are apparent in the stable LTP reported in <xref ref-type="bibr" rid="pcbi.1000143-Abraham2">[16]</xref>. Crucially, it is not known if the forgetting of learned behaviors is caused by decaying efficacy at many synapses, or by the cumulative effect of random fluctuations in stable LTP-induced synaptic efficacies. Here, decaying efficacy is analogous to weight values that fall toward zero in network models, whereas the cumulative effect of random fluctuations is analogous to the addition of random noise, or drifting, of weight values in network models.</p>
<p>Given a choice between forgetting via synaptic weights that fall towards zero and weights that drift isotropically, has evolution chosen drifting or falling? If all other things were equal then forgetting via synaptic drift would seem to be the obvious choice. This is because drifting ensures that relearning a subset of associations improves performance on other associations, whereas falling decreases performance. However, other things are rarely equal. The expected magnitude of weights increases with drifting but decreases with falling. (Consider a hypersphere centered on the origin, with radius ∥<bold>w</bold><sub>0</sub>∥ . Simple geometry shows that more than half of all directions emanating from <bold>w</bold><sub>0</sub> yield a new weight vector <bold>w</bold><sub>1</sub> which lies outside the hypersphere, and therefore <bold>E</bold>[∥<bold>w</bold><sub>1</sub>∥]&gt;<bold>E</bold>[∥<bold>w</bold><sub>0</sub>∥] (assuming, for example, that all vectors <bold>w</bold><sub>1</sub>−<bold>w</bold><sub>0</sub> have the same length).) This decrease in weight magnitudes effectively reduces neuronal firing rates, which reduces metabolic costs relative to costs incurred by synaptic drift. Synaptic drift therefore confers mnemonic benefits, but these benefits come at a metabolic price. Thus the increased fitness gained from the mnemonic benefits of synaptic drift must be offset against their metabolic costs. In essence, even free-lunch learning comes at a price.</p>
</sec><sec id="s4">
<title>Methods</title>
<p>We proceed by deriving expressions for <italic>E</italic><sub>pre</sub>, <italic>E</italic><sub>post</sub>, and for <italic>δ</italic> = <italic>E</italic>pre−<italic>E</italic><sub>post</sub>. We prove that if <italic>n</italic><sub>1</sub>+<italic>n</italic><sub>2</sub>≤<italic>n</italic> then the expected value of <italic>δ</italic> is negative. We then prove that the probability <italic>P</italic>(<italic>δ</italic>&lt;0) that <italic>δ</italic> is negative approaches unity as <italic>n</italic><sub>1</sub> approaches ∞.</p>
<sec id="s4a">
<title>Performance Errors</title>
<p>Given a <italic>c</italic>×<italic>n</italic> matrix <bold>X</bold> and a <italic>c</italic> -dimensional vector <bold>d</bold>, let <italic>L</italic><bold><sub>X</sub></bold><sub>,<bold>d</bold></sub> be the affine subspace<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e040" xlink:type="simple"/></disp-formula>of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e041" xlink:type="simple"/></inline-formula>. If <bold>X</bold> and <bold>d</bold> are consistent (i.e., there is a <bold>w</bold> such that <bold>Xw</bold> = <bold>d</bold>) then<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e042" xlink:type="simple"/></disp-formula>Given weight vectors <bold>w</bold><sub>1</sub> and <bold>w</bold><sub>2</sub>, a matrix <bold>X</bold> of input vectors, and a vector <bold>d</bold> of desired outputs, define<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e043" xlink:type="simple"/></disp-formula>where <italic>E</italic><sub>pre</sub> = <italic>E</italic>(<bold>X</bold>;<bold>w</bold><sub>1</sub>,<bold>d</bold>) and <italic>E</italic><sub>post</sub> = <italic>E</italic>(<bold>X</bold>;<bold>w</bold><sub>2</sub>,<bold>d</bold>). Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e044" xlink:type="simple"/></inline-formula> be any element of <italic>L</italic><bold><sub>X</sub></bold><sub>,<bold>d</bold></sub>. Then <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e045" xlink:type="simple"/><label>(14)</label></disp-formula></p>
<p>If <bold>X</bold><italic><sub>i</sub></italic> has rank <italic>n<sub>i</sub></italic> then transposing the QR decomposition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e046" xlink:type="simple"/></inline-formula> (or, equivalently, using Gram–Schmidt orthonormalisation of the rows of <bold>X</bold><italic><sub>i</sub></italic>) gives<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e047" xlink:type="simple"/></disp-formula>for unique <italic>n<sub>i</sub></italic>×<italic>n<sub>i</sub></italic> and <italic>n<sub>i</sub></italic>×<italic>n</italic> matrices <bold>T</bold><italic><sub>i</sub></italic> and <bold>Z</bold><italic><sub>i</sub></italic> with <bold>T</bold><italic><sub>i</sub></italic> lower triangular with positive diagonal elements, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e048" xlink:type="simple"/></inline-formula>. Simple calculation shows that, for any weight vector <bold>w</bold>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e049" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e050" xlink:type="simple"/></inline-formula> are orthogonal. Since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e051" xlink:type="simple"/></inline-formula>, it follows that the matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e052" xlink:type="simple"/></inline-formula> represents the operator that projects orthogonally onto the image of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e053" xlink:type="simple"/></inline-formula>. Because<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e054" xlink:type="simple"/><label>(15)</label></disp-formula>the image of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e055" xlink:type="simple"/></inline-formula> is contained in that of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e056" xlink:type="simple"/></inline-formula>. As both these images have dimension <italic>n<sub>i</sub></italic>, they must be equal, and so <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e057" xlink:type="simple"/></inline-formula> represents the operator which projects orthogonally onto the image of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e058" xlink:type="simple"/></inline-formula>.</p>
<p>Now suppose that <bold>X</bold> and <bold>d</bold> are consistent, where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e059" xlink:type="simple"/></disp-formula></p>
<p>Then, after the network has learned <italic>A</italic><sub>1</sub> and <italic>A</italic><sub>2</sub>, the weight vector <bold>w</bold><sub>0</sub> satisfies<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e060" xlink:type="simple"/><label>(16)</label></disp-formula>(If, as below, <italic>n</italic><sub>1</sub>+<italic>n</italic><sub>2</sub>≤<italic>n</italic>, <bold>X</bold><sub>2</sub> and <bold>d</bold><sub>2</sub> are consistent, and (<bold>X</bold><sub>1</sub>,<bold>d</bold><sub>1</sub>) has a continuous distribution then Equation 16 holds with probability 1.)</p>
</sec><sec id="s4b">
<title>Falling</title>
<p>We now assume that forgetting is induced by weight values “falling” towards the origin at zero, i.e., forgetting consists of shrinking the weight vector <bold>w</bold><sub>0</sub> by a (possibly random) factor <italic>r</italic> towards the “dead state” <bold>0</bold>. Thus the post-forgetting weight vector <bold>w</bold><sub>1</sub> is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e061" xlink:type="simple"/><label>(17)</label></disp-formula>and so the “forgetting vector” <bold>v</bold> = <bold>w</bold><sub>1</sub>−<bold>w</bold><sub>0</sub> is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e062" xlink:type="simple"/><label>(18)</label></disp-formula></p>
<p>The form of forgetting given by Equation 17 is very different from that investigated in <xref ref-type="bibr" rid="pcbi.1000143-Stone2">[12]</xref>, where <bold>v</bold> has an isotropic distribution and is independent of (<bold>X</bold><sub>1</sub>,<bold>d</bold><sub>1</sub>) and (<bold>X</bold><sub>2</sub>,<bold>d</bold><sub>2</sub>).</p>
<p>Let <bold>w</bold><sub>2</sub> be the orthogonal projection of <bold>w</bold><sub>1</sub> onto <italic>L</italic><sub>2</sub>. Then<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e063" xlink:type="simple"/></disp-formula></p>
<p>Manipulation gives<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e064" xlink:type="simple"/><label>(19)</label></disp-formula>and so<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e065" xlink:type="simple"/><label>(20)</label></disp-formula></p>
<p>Then Equations 14, 16, and 18–20 yield<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e066" xlink:type="simple"/><label>(21)</label></disp-formula></p>
</sec><sec id="s4c">
<title>The Case of Isotropic Random (<bold>X</bold><sub>1</sub>,<bold>d</bold><sub>1</sub>)</title>
<p>In this section we assume that the distribution of (<bold>X</bold><sub>1</sub>,<bold>d</bold><sub>1</sub>) is isotropic, i.e., that (<bold>UX</bold><sub>1</sub><bold>V</bold>,<bold>Ud</bold><sub>1</sub>) has the same distribution as (<bold>X</bold><sub>1</sub>,<bold>d</bold><sub>1</sub>) for all orthogonal <italic>n</italic><sub>1</sub>×<italic>n</italic><sub>1</sub> matrices <bold>U</bold> and all orthogonal <italic>n</italic>×<italic>n</italic> matrices <bold>V</bold>. Then taking the conditional expectation of Equation 21 for given <bold>X</bold><sub>2</sub>, <bold>d</bold><sub>2</sub>, and <italic>r</italic> gives the following theorem.</p>
</sec><sec id="s4d">
<title>Theorem 1</title>
<p>If</p>
<list list-type="order"><list-item>
<p><italic>n</italic><sub>1</sub>+<italic>n</italic><sub>2</sub>≤<italic>n</italic>,</p>
</list-item><list-item>
<p><bold>X</bold><sub>2</sub> and <bold>d</bold><sub>2</sub> are consistent,</p>
</list-item><list-item>
<p>the distribution of (<bold>X</bold><sub>1</sub>,<bold>d</bold><sub>1</sub>) is continuous and isotropic,</p>
</list-item><list-item>
<p><bold>X</bold><sub>1</sub>, <bold>d</bold><sub>1</sub>, and (<bold>X</bold><sub>2</sub>,<bold>d</bold><sub>2</sub>,<italic>r</italic>) are independent.</p>
</list-item></list>
<p>then<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e067" xlink:type="simple"/><label>(22)</label></disp-formula>where <bold>x</bold> is any column of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e068" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4e">
<title>Corollary 1</title>
<p>If 1.-3. of Theorem 1 hold then<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e069" xlink:type="simple"/><label>(23)</label></disp-formula>with equality if and only if either <italic>r</italic> = 1 or <bold>d</bold><sub>2</sub> = 0.</p>
<p>Corollary 1 says that (apart from trivial exceptions) the expected amount of FLL is negative.</p>
<p>To obtain Theorem 2, it is useful to have some moments of isotropic distributions. Let <bold>x</bold> be isotropically distributed on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e070" xlink:type="simple"/></inline-formula>. Then Equations 9.6.1 and 9.6.2 of Mardia and Jupp (2000), together with some algebraic manipulation, yield<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e071" xlink:type="simple"/><label>(24)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e072" xlink:type="simple"/><label>(25)</label></disp-formula>as in Equations A.14 and A.15 of <xref ref-type="bibr" rid="pcbi.1000143-Stone2">[12]</xref>.</p>
<p>The other tool used in proving Theorem 2 is the formula<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e073" xlink:type="simple"/><label>(26)</label></disp-formula>for any random variables <italic>X</italic>,<italic>Y</italic>,<italic>Z</italic> for which these quantities exist. Equation 26 is an application to the conditional distribution of <italic>Y</italic>|<italic>Z</italic> of the standard conditional variance formula that is given in Equation 2b.3.6 on page 97 of <xref ref-type="bibr" rid="pcbi.1000143-Rao1">[17]</xref>.</p>
<p>Taking the expectation and variance of Equation 21 as only <bold>d</bold><sub>1</sub> varies and using Equation 24 gives<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e074" xlink:type="simple"/><label>(27)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e075" xlink:type="simple"/><label>(28)</label></disp-formula></p>
<p>Taking the expectation of Equation 28 as only <bold>X</bold><sub>1</sub> varies and using Equation 24 gives<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e076" xlink:type="simple"/><label>(29)</label></disp-formula></p>
<p>We now suppose that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e077" xlink:type="simple"/><label>(30)</label></disp-formula></p>
<p>Then taking the variance of Equation 27 as only <bold>X</bold><sub>1</sub> varies and using Equation 25 gives<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e078" xlink:type="simple"/><label>(31)</label></disp-formula></p>
<p>Adding Equations 29 and 30 and using Equation 26 yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e079" xlink:type="simple"/><label>(32)</label></disp-formula></p>
<p>To obtain an upper bound on the conditional probability of FLL (i.e., on <italic>P</italic>(<italic>δ</italic>≥0|<bold>X</bold><sub>2</sub>,<bold>d</bold><sub>2</sub>,<italic>r</italic>)), we use Chebyshev's inequality, which states that, for any random variable <italic>Y</italic> and any positive value of <italic>t</italic><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e080" xlink:type="simple"/></disp-formula></p>
<p>Applying Chebyshev's inequality to the conditional distribution of δ(<bold>w</bold><sub>1</sub>,<bold>w</bold><sub>2</sub>,<bold>X</bold><sub>1</sub>,<bold>d</bold><sub>1</sub>) given (<bold>X</bold><sub>2</sub>,<bold>d</bold><sub>2</sub>,<italic>r</italic>), taking <italic>t</italic> = <bold>E</bold>[<italic>δ</italic>(<bold>w</bold><sub>1</sub>,<bold>w</bold><sub>2</sub>;<bold>X</bold><sub>1</sub>,<bold>d</bold><sub>1</sub>)|<bold>X</bold><sub>2</sub>,<bold>d</bold><sub>2</sub>,<italic>r</italic>], and noting that (by Equation 23) <italic>t</italic>≤0, we obtain<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e081" xlink:type="simple"/><label>(33)</label></disp-formula></p>
<p>Substituting Equations 22 and 32 into Equation 33 gives<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e082" xlink:type="simple"/><label>(34)</label></disp-formula>where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e083" xlink:type="simple"/></disp-formula></p>
<p>For any positive-definite symmetric matrix <bold>A</bold> and vector <bold>x</bold>, diagonalization of <bold>A</bold>, together with the fact that <italic>x</italic>+1/<italic>x</italic>≥2 for positive <italic>x</italic>, yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e084" xlink:type="simple"/><label>(35)</label></disp-formula></p>
<p>Combining Equations 34 and 35 with the fact that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e085" xlink:type="simple"/></inline-formula> gives<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e086" xlink:type="simple"/><label>(36)</label></disp-formula></p>
<p>Taking the expectation of Equation 36 over <bold>X</bold><sub>2</sub> yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e087" xlink:type="simple"/><label>(37)</label></disp-formula>where <bold>x</bold> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e088" xlink:type="simple"/></inline-formula> are any columns of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e089" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e090" xlink:type="simple"/></inline-formula>, respectively.</p>
<p>Taking the expectation of Equation 37 over <bold>d</bold><sub>2</sub> and <italic>r</italic> yields the following theorem.</p>
</sec><sec id="s4f">
<title>Theorem 2</title>
<p>If (a) conditions 1.-4. of Theorem 1 hold, (b) the columns <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e091" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e092" xlink:type="simple"/></inline-formula> are distributed independently, (c) <bold>X</bold><sub>2</sub>, <bold>d</bold><sub>2</sub>, and <italic>r</italic> are independent, (d) the distribution of (<bold>X</bold><sub>2</sub>,<bold>d</bold><sub>2</sub>) is isotropic, and (e) <bold>E</bold>[∥<bold>d</bold><sub>2</sub>∥<sup>−2</sup>] is finite then<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e093" xlink:type="simple"/><label>(38)</label></disp-formula>where <bold>x</bold> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e094" xlink:type="simple"/></inline-formula> are any columns of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e095" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e096" xlink:type="simple"/></inline-formula>, respectively, and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e097" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s4g">
<title>Corollary 2</title>
<p>If the conditions of Theorem 2 hold and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e098" xlink:type="simple"/></disp-formula>where <bold>x</bold> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e099" xlink:type="simple"/></inline-formula> are any columns of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e100" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e101" xlink:type="simple"/></inline-formula>, respectively, then<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e102" xlink:type="simple"/></disp-formula></p>
<p>Thus<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000143.e103" xlink:type="simple"/></disp-formula>provided that <italic>n</italic><sub>1</sub>/<italic>n</italic> and <italic>n</italic><sub>2</sub>/<italic>n</italic> are bounded away from zero.</p>
</sec></sec></body>
<back>
<ack>
<p>Thanks to David Sterratt for asking, “What would happen to free-lunch learning if weights decayed?” and to three anonymous reviewers for their detailed comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1000143-Tanzi1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tanzi</surname><given-names>E</given-names></name>
</person-group>             <year>1893</year>             <article-title>I fatti e le induzioni nellodierna isologia del sistema nervosa.</article-title>             <source>Riv Sper Freniatr Med Leg</source>             <volume>19</volume>             <fpage>419</fpage>             <lpage>472</lpage>          </element-citation></ref>
<ref id="pcbi.1000143-Hebb1"><label>2</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hebb</surname><given-names>D</given-names></name>
</person-group>             <year>1949</year>             <source>The Organization of Behavior: A Neuropsychological Theory</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Wiley</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000143-Abraham1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Abraham</surname><given-names>W</given-names></name>
</person-group>             <year>2003</year>             <article-title>How long will long-term potentiation last?</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>358</volume>             <fpage>735</fpage>             <lpage>744</lpage>          </element-citation></ref>
<ref id="pcbi.1000143-Stone1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Stone</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Hunkin</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Hornby</surname><given-names>A</given-names></name>
</person-group>             <year>2001</year>             <article-title>Predicting spontaneous recovery of memory.</article-title>             <source>Nature</source>             <volume>414</volume>             <fpage>167</fpage>             <lpage>168</lpage>          </element-citation></ref>
<ref id="pcbi.1000143-Coltheart1"><label>5</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Coltheart</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Byng</surname><given-names>S</given-names></name>
</person-group>             <year>1989</year>             <article-title>A treatment for surface dyslexia.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Seron</surname><given-names>X</given-names></name>
</person-group>             <source>Cognitive Approaches in Neuropsychological Rehabilitation</source>             <publisher-loc>London</publisher-loc>             <publisher-name>Lawrence Erlbaum Associates</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000143-Weekes1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Weekes</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Coltheart</surname><given-names>M</given-names></name>
</person-group>             <year>1996</year>             <article-title>Surface dyslexia and surface dysgraphia: treatment studies and their theoretical implications.</article-title>             <source>Cogn Neuropsychol</source>             <volume>13</volume>             <fpage>277</fpage>             <lpage>315</lpage>          </element-citation></ref>
<ref id="pcbi.1000143-Atkins1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Atkins</surname><given-names>P</given-names></name>
</person-group>             <year>2001</year>             <article-title>What happens when we relearn part of what we previously knew? Predictions and constraints for models of long-term memory.</article-title>             <source>Psychol Res</source>             <volume>65</volume>             <fpage>202</fpage>             <lpage>215</lpage>          </element-citation></ref>
<ref id="pcbi.1000143-Roediger1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Roediger</surname><given-names>H</given-names><suffix>III</suffix></name>
</person-group>             <year>1973</year>             <article-title>Inhibition in recall from cueing with recall targets.</article-title>             <source>J Verbal Learn Verbal Behav</source>             <volume>12</volume>             <fpage>644</fpage>             <lpage>657</lpage>          </element-citation></ref>
<ref id="pcbi.1000143-Serra1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Serra</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Nairne</surname><given-names>J</given-names></name>
</person-group>             <year>2000</year>             <article-title>Part-set cuing of order information: implications for associative theories.</article-title>             <source>Mem Cognit</source>             <volume>28</volume>             <fpage>847</fpage>             <lpage>855</lpage>          </element-citation></ref>
<ref id="pcbi.1000143-Hinton1"><label>10</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hinton</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Plaut</surname><given-names>D</given-names></name>
</person-group>             <year>1987</year>             <article-title>Using fast weights to deblur old memories.</article-title>             <source>Proceedings Ninth Annual Conference of the Cognitive Science Society</source>             <fpage>177</fpage>             <lpage>186</lpage>          </element-citation></ref>
<ref id="pcbi.1000143-Atkins2"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Atkins</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Murre</surname><given-names>J</given-names></name>
</person-group>             <year>1998</year>             <article-title>Recovery of unrehearsed items in connectionist models.</article-title>             <source>Connect Sci</source>             <volume>10</volume>             <fpage>99</fpage>             <lpage>119</lpage>          </element-citation></ref>
<ref id="pcbi.1000143-Stone2"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Stone</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Jupp</surname><given-names>P</given-names></name>
</person-group>             <year>2007</year>             <article-title>Free-lunch learning: modelling spontaneous recovery of memory.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>194</fpage>             <lpage>217</lpage>          </element-citation></ref>
<ref id="pcbi.1000143-Stone3"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Stone</surname><given-names>J</given-names></name>
</person-group>             <year>2007</year>             <article-title>Distributed representations accelerate evolution of adaptive behaviours.</article-title>             <source>PLoS Comput Biol</source>             <volume>3</volume>             <fpage>e147</fpage>             <comment>doi:10.1371/journal.pcbi.0030147</comment>          </element-citation></ref>
<ref id="pcbi.1000143-Whitlock1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Whitlock</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Heynen</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Shuler</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Bear</surname><given-names>M</given-names></name>
</person-group>             <year>2006</year>             <article-title>Learning induces long-term potentiation in the hippocampus.</article-title>             <source>Science</source>             <volume>313</volume>             <fpage>1093</fpage>             <lpage>1097</lpage>          </element-citation></ref>
<ref id="pcbi.1000143-Staubli1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Staubli</surname><given-names>U</given-names></name>
<name name-style="western"><surname>Lynch</surname><given-names>G</given-names></name>
</person-group>             <year>1987</year>             <article-title>Stable hippocampal long-term potentiation elicited by theta pattern stimulation.</article-title>             <source>Brain Res</source>             <volume>435</volume>             <fpage>227</fpage>             <lpage>234</lpage>          </element-citation></ref>
<ref id="pcbi.1000143-Abraham2"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Abraham</surname><given-names>WC</given-names></name>
<name name-style="western"><surname>Logan</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Greenwood</surname><given-names>JM</given-names></name>
<name name-style="western"><surname>Dragunow</surname><given-names>M</given-names></name>
</person-group>             <year>2002</year>             <article-title>Induction and experience-dependent consolidation of stable long-term potentiation lasting months in the hippocampus.</article-title>             <source>J Neurosci</source>             <volume>22</volume>             <fpage>9626</fpage>             <lpage>9634</lpage>          </element-citation></ref>
<ref id="pcbi.1000143-Rao1"><label>17</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rao</surname><given-names>C</given-names></name>
</person-group>             <year>1973</year>             <source>Linear Statistical Inference and its Applications. 2nd edition</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Wiley</publisher-name>          </element-citation></ref>
</ref-list>

</back>
</article>