<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">10-PLCB-RA-2391R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1001052</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Neuroscience/Theoretical Neuroscience</subject></subj-group></article-categories><title-group><article-title>Practical Measures of Integrated Information for Time-Series Data</article-title><alt-title alt-title-type="running-head">Practical Measures of Integrated Information</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Barrett</surname><given-names>Adam B.</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Seth</surname><given-names>Anil K.</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
</contrib-group><aff id="aff1">          <addr-line>Sackler Centre for Consciousness Science and School of Informatics, University of Sussex, Brighton, United Kingdom</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">adam.barrett@sussex.ac.uk</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: ABB AKS. Performed the experiments: ABB. Analyzed the data: ABB AKS. Contributed reagents/materials/analysis tools: ABB. Wrote the paper: ABB AKS.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>1</month><year>2011</year></pub-date><pub-date pub-type="epub"><day>20</day><month>1</month><year>2011</year></pub-date><volume>7</volume><issue>1</issue><elocation-id>e1001052</elocation-id><history>
<date date-type="received"><day>20</day><month>6</month><year>2010</year></date>
<date date-type="accepted"><day>6</day><month>12</month><year>2010</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Barrett, Seth</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>A recent measure of ‘integrated information’, Φ<sub>DM</sub>, quantifies the extent to which a system generates more information than the sum of its parts as it transitions between states, possibly reflecting levels of consciousness generated by neural systems. However, Φ<sub>DM</sub> is defined only for discrete Markov systems, which are unusual in biology; as a result, Φ<sub>DM</sub> can rarely be measured in practice. Here, we describe two new measures, Φ<sub>E</sub> and Φ<sub>AR</sub>, that overcome these limitations and are easy to apply to time-series data. We use simulations to demonstrate the in-practice applicability of our measures, and to explore their properties. Our results provide new opportunities for examining information integration in real and model systems and carry implications for relations between integrated information, consciousness, and other neurocognitive processes. However, our findings pose challenges for theories that ascribe physical meaning to the measured quantities.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>A key feature of the human brain is its ability to represent a vast amount of information, and to integrate this information in order to produce specific and selective behaviour, as well as a stream of unified conscious scenes. Attempts have been made to quantify so-called ‘integrated information’ by formalizing in mathematics the extent to which a system as a whole generates more information than the sum of its parts. However, so far, the resulting measures have turned out to be inapplicable to real neural systems. In this paper we introduce two new measures that can be applied to both realistic neural models and to time-series data garnered from a broad range of neuroimaging and electrophysiological methods. Our work provides new opportunities for examining the role of integrated information in cognition and consciousness, and indeed in the function of any complex biological system. However, our results also pose challenges for theories that ascribe a direct physical meaning to any version of integrated information so far described.</p>
</abstract><funding-group><funding-statement>AKS is supported by EPSRC Leadership Fellowship EP/G007543/1, which also supports the work of ABB. Support is also gratefully acknowledged from the Dr. Mortimer and Theresa Sackler Foundation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="18"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>How can the complex dynamics exhibited by networks of interconnected elements best be measured? Answering this question promises to shed substantial new light on many complex systems, biological and non-biological. Neural systems in particular are characterized by richly interconnected elements exhibiting complex dynamics at multiple spatiotemporal scales <xref ref-type="bibr" rid="pcbi.1001052-Honey1">[1]</xref>, which have been associated with a variety of behavioral, cognitive, and phenomenal properties <xref ref-type="bibr" rid="pcbi.1001052-Sporns1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Bressler1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Edelman1">[4]</xref>. Characterizing dynamical complexity for such systems therefore presents a key challenge for developing new theoretical accounts <xref ref-type="bibr" rid="pcbi.1001052-Seth1">[5]</xref> and for designing and evaluating new experiments. A common and attractive intuition is that dynamical complexity consists in the coexistence of <italic>differentiation</italic> (subsets of a system are dynamically distinct) and <italic>integration</italic> (the system as a whole exhibits coherence) in a system's dynamics. Applied to neural systems, this intuition may underpin notions of cognitive and behavioral flexibility. A system that is able to respond specifically and selectively to a broad range of stimuli, in an integrated way, may require conjoined functional integration and differentiation <xref ref-type="bibr" rid="pcbi.1001052-Seth2">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Lungarella1">[7]</xref>. More ambitiously, the intuition may also characterize basic aspects of conscious experience <xref ref-type="bibr" rid="pcbi.1001052-Tononi1">[8]</xref>. At the phenomenal level, each conscious scene is composed of many different parts and is different from every other conscious scene ever experienced (differentiation), yet each conscious scene is experienced as a coherent whole (integration). Therefore, dynamical complexity in neural systems may actually <italic>account for</italic> (and not merely correlate with) fundamental aspects of consciousness <xref ref-type="bibr" rid="pcbi.1001052-Seth3">[9]</xref>.</p>
<p>Several measures now exist which operationalize the above intuition under different assumptions and with varying practical applicability <xref ref-type="bibr" rid="pcbi.1001052-Seth1">[5]</xref>. In this paper, we critically evaluate ‘integrated information’ (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e001" xlink:type="simple"/></inline-formula>) <xref ref-type="bibr" rid="pcbi.1001052-Tononi2">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref>, a candidate measure that has received significant recent attention, especially in the domain of consciousness science <xref ref-type="bibr" rid="pcbi.1001052-Tononi3">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi2">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Tononi4">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Tononi5">[15]</xref>. We present new versions of this measure that are both theoretically well-grounded and, in contrast to previous versions, practically applicable given time-series data. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e002" xlink:type="simple"/></inline-formula> has been proposed as a measure of the amount of information that is integrated by a system, where ‘information’ reflects the differentiated states of a system and ‘integration’ their global cohesion. According to the ‘integrated information theory of consciousness’ (IITC), this quantity is identical to the quantity of consciousness generated by the system; in other words, on the IITC, consciousness <italic>is</italic> integrated information <xref ref-type="bibr" rid="pcbi.1001052-Tononi3">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Tononi4">[14]</xref>. This dramatic claim invites a close examination of the in-principle and in-practice properties of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e003" xlink:type="simple"/></inline-formula>.</p>
<p>A first version of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e004" xlink:type="simple"/></inline-formula> (which we call <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e005" xlink:type="simple"/></inline-formula>, ‘<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e006" xlink:type="simple"/></inline-formula>-capacity’,) was conceived as a measure of the <italic>capacity</italic> of a system to integrate information, and did not take into account time or changing dynamics <xref ref-type="bibr" rid="pcbi.1001052-Tononi2">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Tononi3">[12]</xref>. Also, measuring <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e007" xlink:type="simple"/></inline-formula> requires flexible, repeated, and reversible perturbation of arbitrary system subsets, which is infeasible for non-trivial systems (except in simulation). We do not discuss this measure any further. Recently, a new version of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e008" xlink:type="simple"/></inline-formula> has been introduced in the context of the IITC, which we call <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e009" xlink:type="simple"/></inline-formula>, ‘<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e010" xlink:type="simple"/></inline-formula>-discrete/Markov’ <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref>. In contrast to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e011" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e012" xlink:type="simple"/></inline-formula> is defined for systems of discrete elements that evolve through time with Markovian transitions. Specifically, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e013" xlink:type="simple"/></inline-formula> measures the information generated when a system transitions to one particular state out of a repertoire of possible states, but only to the extent that this information is generated by the whole system, over and above the information generated independently by the parts <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref>. Importantly, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e014" xlink:type="simple"/></inline-formula> measures information as reduction in entropy from a prior <italic>maximum entropy</italic> distribution, which is taken to represent the repertoire of possible states.</p>
<p>It has been shown, using simulations, that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e015" xlink:type="simple"/></inline-formula> behaves consistently with several intuitions about dynamical complexity <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref>. In particular, high values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e016" xlink:type="simple"/></inline-formula> are generated by networks that exhibit both differentiation and integration in their dynamics. However, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e017" xlink:type="simple"/></inline-formula> is defined only for idealized discrete Markovian systems (a Markovian system is one for which the future depends only on the present, and not on the past). This in-principle restriction severely limits its in-practice applicability because complex biological systems are typically continuous (or are measured as continuous) and are non-Markovian). This limitation in turn imposes a serious obstacle for developing and evaluating theories, such as the IITC, which depend on quantifying integrated information.</p>
<p>In this paper we introduce an alternative measure of integrated information, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e018" xlink:type="simple"/></inline-formula> (‘<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e019" xlink:type="simple"/></inline-formula>-empirical’), which is applicable to time-series data, and to continuous or discrete stochastic systems, Markovian or otherwise (and without perturbation of the studied system). These key features arise because <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e020" xlink:type="simple"/></inline-formula> is based on the reduction in Shannon entropy from the empirical, as opposed to the maximum entropy, distribution. Our basic formulation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e021" xlink:type="simple"/></inline-formula> therefore addresses the in-principle restrictions of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e022" xlink:type="simple"/></inline-formula> mentioned above. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e023" xlink:type="simple"/></inline-formula> is best suited for application to stationary systems, for which it provides a single value for a given stationary epoch. However, its in-practice applicability still faces the difficulty of accurately estimating entropies from limited data. This is a problem that scales poorly as the number of elements (variables) increases, especially for continuous systems <xref ref-type="bibr" rid="pcbi.1001052-Cover1">[16]</xref>. Confronting this problem, we show that when states are Gaussian distributed, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e024" xlink:type="simple"/></inline-formula> can be computed directly from empirical covariance matrices, rendering it extremely easy to apply in practice for these systems. Meanwhile, for non-Gaussian systems, we introduce a second measure, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e025" xlink:type="simple"/></inline-formula> (‘auto-regressive <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e026" xlink:type="simple"/></inline-formula>’), which is based on auto-regressive prediction error. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e027" xlink:type="simple"/></inline-formula> can be understood as measuring how well the present state of a system predicts some previous state, but only to the extent that predictions based on the whole outstrip predictions based on the parts considered independently. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e028" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e029" xlink:type="simple"/></inline-formula> are constructed analogously, and indeed for Gaussian systems we are able to show, using a connection between linear regression and information theory <xref ref-type="bibr" rid="pcbi.1001052-Barnett1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Barrett1">[18]</xref>, that they are precisely equivalent. Recognizing this equivalence allows us to interpret <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e030" xlink:type="simple"/></inline-formula> in the same way as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e031" xlink:type="simple"/></inline-formula>, i.e., in terms of predictive ability. Importantly, although for non-Gaussian systems <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e032" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e033" xlink:type="simple"/></inline-formula> may differ, the former remains easy to measure in practice from empirical covariance matrices.</p>
<p>The difference between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e034" xlink:type="simple"/></inline-formula>/<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e035" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e036" xlink:type="simple"/></inline-formula> is not only a matter of practical applicability. Using the empirical distribution as opposed to the maximum entropy distribution substantially changes possible interpretations of the measure. According to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e037" xlink:type="simple"/></inline-formula>, integrated information is a measure of a <italic>process</italic>, since the empirical distribution is a characterization of the actual behavior of the system. According to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e038" xlink:type="simple"/></inline-formula> integrated information is to some extent a measure of <italic>capacity</italic> <xref ref-type="bibr" rid="pcbi.1001052-Tononi4">[14]</xref>, since the maximum entropy distribution is maximally agnostic about the behavior of the system, representing instead its potential or capacity.</p>
<p>The above distinction carries implications for theories, such as the IITC, that ascribe physical meaning to measures of integrated information. Under the IITC, consciousness is explicitly characterized in terms of the capacity of a system <xref ref-type="bibr" rid="pcbi.1001052-Tononi4">[14]</xref>, and not, following William James <xref ref-type="bibr" rid="pcbi.1001052-James1">[19]</xref>, as a process. Our new measures imply a Jamesian modification of the IITC by considering consciousness as a process; they also challenge the identity relation between consciousness and integrated information assumed in the IITC. More generally, many other brain-based phenomena are best considered in terms of process rather than capacity, and may admit useful interpretations in terms of integrated information. For example, multi-modal binding and perceptual categorization <xref ref-type="bibr" rid="pcbi.1001052-Seth4">[20]</xref> could involve integrated information in the perceptual domain, and action selection (decision making) <xref ref-type="bibr" rid="pcbi.1001052-Cisek1">[21]</xref> may require the integration of sensory, cognitive and motor processes, while retaining differentiation among competing alternatives. In these and other cases, having a measure of integrated information framed in terms of process, that is practically applicable to time-series data, will permit the formulation of testable hypotheses and synthetic models relating information integration to cognitive and neural operations.</p>
</sec><sec id="s2">
<title>Results</title>
<p>The ‘<xref ref-type="sec" rid="s2">Results</xref>’ section is organized as follows. In the ‘Notation, conventions and preliminaries’ section we lay out our notation and introduce some necessary mathematical concepts. In the section ‘The previous measure, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e039" xlink:type="simple"/></inline-formula>’ we review <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e040" xlink:type="simple"/></inline-formula> using our current notation, noting its limitations especially with respect to discrete Markovian systems. The section ‘The new measure, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e041" xlink:type="simple"/></inline-formula>’ describes the new measure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e042" xlink:type="simple"/></inline-formula> and provides practical recipes for its computation either numerically from time-series or analytically, given a generative model of the system, both under Gaussian assumptions. We note that for non-Gaussian systems <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e043" xlink:type="simple"/></inline-formula> remains well-defined even if it is more challenging to calculate. The section ‘<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e044" xlink:type="simple"/></inline-formula> for Markovian Gaussian systems’ presents the results of various simulations, designed to illustrate the in-practice applicability of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e045" xlink:type="simple"/></inline-formula> and to explore its properties. We compute <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e046" xlink:type="simple"/></inline-formula> for some canonical networks, optimize connectivity under simple dynamics, and examine the numerical stability of the measure. We also compare <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e047" xlink:type="simple"/></inline-formula> with a version of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e048" xlink:type="simple"/></inline-formula> modified to apply to continuous systems, showing quantitative congruence in most cases. The section ‘Extension to multiple lags and to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e049" xlink:type="simple"/></inline-formula> processes’ describes some additional simulation results, showing how <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e050" xlink:type="simple"/></inline-formula> can measure integrated information over arbitrary time-steps (lags). In the section ‘Auto-regressive <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e051" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e052" xlink:type="simple"/></inline-formula>)’ we describe <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e053" xlink:type="simple"/></inline-formula> and explain its derivation in terms of relations among conditional entropy, covariance, and linear regression prediction error. We demonstrate the utility of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e054" xlink:type="simple"/></inline-formula> by calculating integrated information for representative systems animated by exponentially distributed (i.e., non-Gaussian) dynamics.</p>
<sec id="s2a">
<title>Notation, conventions and preliminaries</title>
<p>We use bold upper-case letters to denote multivariate random variables, and corresponding bold lower-case letters to denote actualizations of random variables. Matrices are denoted by upper-case letters. The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e055" xlink:type="simple"/></inline-formula>-dimensional identity matrix is denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e056" xlink:type="simple"/></inline-formula> and the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e057" xlink:type="simple"/></inline-formula>-dimensional square matrix of zeros by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e058" xlink:type="simple"/></inline-formula>. The transpose operator is denoted by ‘<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e059" xlink:type="simple"/></inline-formula>’, and the determinant by ‘det’. Our convention for logarithms is to take them to the natural base <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e060" xlink:type="simple"/></inline-formula>, and to denote them by ‘log’.</p>
<p>Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e061" xlink:type="simple"/></inline-formula> be a random variable that takes values in the space <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e062" xlink:type="simple"/></inline-formula>. Then we denote the probability density function by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e063" xlink:type="simple"/></inline-formula>, the mean by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e064" xlink:type="simple"/></inline-formula> and the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e065" xlink:type="simple"/></inline-formula> matrix of covariances, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e066" xlink:type="simple"/></inline-formula>, by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e067" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e068" xlink:type="simple"/></inline-formula> be a second random variable. Then we denote the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e069" xlink:type="simple"/></inline-formula> matrix of cross-covariances, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e070" xlink:type="simple"/></inline-formula>, by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e071" xlink:type="simple"/></inline-formula>. The following quantity will be useful:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e072" xlink:type="simple"/><label>(0.1)</label></disp-formula>We call this the partial covariance of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e073" xlink:type="simple"/></inline-formula> given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e074" xlink:type="simple"/></inline-formula>, and it is well-defined when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e075" xlink:type="simple"/></inline-formula> is invertible. If <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e076" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e077" xlink:type="simple"/></inline-formula> are both multivariate Gaussian variables then the partial covariance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e078" xlink:type="simple"/></inline-formula> is precisely the covariance matrix of the conditional variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e079" xlink:type="simple"/></inline-formula>, for any <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e080" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e081" xlink:type="simple"/><label>(0.2)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e082" xlink:type="simple"/></inline-formula>.</p>
<p>Entropy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e083" xlink:type="simple"/></inline-formula> characterizes uncertainty, and is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e084" xlink:type="simple"/><label>(0.3)</label></disp-formula>if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e085" xlink:type="simple"/></inline-formula> is a discrete random variable, or<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e086" xlink:type="simple"/><label>(0.4)</label></disp-formula>if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e087" xlink:type="simple"/></inline-formula> is a continuous random variable. (Note, strictly, Eq. (0.4) is the differential entropy, since entropy itself is infinite for continuous variables. However, considering continuous variables as continuous limits of discrete variable approximations, entropy differences and hence information remain well-defined in the continuous limit and may be consistently measured using Eq. (0.4) <xref ref-type="bibr" rid="pcbi.1001052-Cover1">[16]</xref>. Moreover, this equation assumes that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e088" xlink:type="simple"/></inline-formula> has a density with respect to the Lebesgue measure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e089" xlink:type="simple"/></inline-formula>; this assumption is upheld whenever we discuss continuous random variables.)</p>
<p>We write <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e090" xlink:type="simple"/></inline-formula> for the conditional entropy of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e091" xlink:type="simple"/></inline-formula> given that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e092" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e093" xlink:type="simple"/></inline-formula> for the expected conditional entropy of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e094" xlink:type="simple"/></inline-formula> given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e095" xlink:type="simple"/></inline-formula>, i.e.,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e096" xlink:type="simple"/><label>(0.5)</label></disp-formula>if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e097" xlink:type="simple"/></inline-formula> is discrete; for continuous <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e098" xlink:type="simple"/></inline-formula> replace the summation by integration. The mutual information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e099" xlink:type="simple"/></inline-formula> between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e100" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e101" xlink:type="simple"/></inline-formula> is the average information, or reduction in uncertainty (entropy), about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e102" xlink:type="simple"/></inline-formula>, knowing the outcome of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e103" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e104" xlink:type="simple"/><label>(0.6)</label></disp-formula>Mutual information can also be written in the useful form<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e105" xlink:type="simple"/><label>(0.7)</label></disp-formula>from which it follows that mutual information is symmetric in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e106" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e107" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1001052-Cover1">[16]</xref>. If <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e108" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e109" xlink:type="simple"/></inline-formula> are both Gaussian,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e110" xlink:type="simple"/><label>(0.8)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e111" xlink:type="simple"/><label>(0.9)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e112" xlink:type="simple"/><label>(0.10)</label></disp-formula>All these quantities are straightforward to compute empirically from the empirical covariance matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e113" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e114" xlink:type="simple"/></inline-formula>, and the expression (0.1).</p>
<p>The Kullback-Leibler (KL) divergence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e115" xlink:type="simple"/></inline-formula> is a (non-symmetric) measure of the difference between two probability distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e116" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e117" xlink:type="simple"/></inline-formula> (well-defined when the variables take values in the same space, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e118" xlink:type="simple"/></inline-formula>). It is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e119" xlink:type="simple"/><label>(0.11)</label></disp-formula>if the variables are discrete, or<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e120" xlink:type="simple"/><label>(0.12)</label></disp-formula>if the variables are continuous.</p>
<p>We examine integrated information generated by systems of interconnected dynamical elements. We use the letter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e121" xlink:type="simple"/></inline-formula> to denote such a system, and the number of elements in the system is denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e122" xlink:type="simple"/></inline-formula>. A partition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e123" xlink:type="simple"/></inline-formula> divides the elements of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e124" xlink:type="simple"/></inline-formula> into non-overlapping, non-trivial sub-systems, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e125" xlink:type="simple"/></inline-formula>. The state of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e126" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e127" xlink:type="simple"/></inline-formula> is a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e128" xlink:type="simple"/></inline-formula>-dimensional random vector denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e129" xlink:type="simple"/></inline-formula>, with entries corresponding to states of individual elements of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e130" xlink:type="simple"/></inline-formula>. Time is discretized, so <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e131" xlink:type="simple"/></inline-formula> takes integer values. We denote the set of possible states of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e132" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e133" xlink:type="simple"/></inline-formula>, and the size of this set by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e134" xlink:type="simple"/></inline-formula>. Analogous notation is used for the states of sub-systems of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e135" xlink:type="simple"/></inline-formula>.</p>
<p>A stationary system is one for which the probability density function for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e136" xlink:type="simple"/></inline-formula> does not change with time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e137" xlink:type="simple"/></inline-formula>. For such systems <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e138" xlink:type="simple"/></inline-formula> denotes the stationary covariance matrix, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e139" xlink:type="simple"/></inline-formula> the auto-covariance matrix with time-lag <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e140" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e141" xlink:type="simple"/><label>(0.13)</label></disp-formula></p>
</sec><sec id="s2b">
<title>The previous measure, Φ<sub>DM</sub></title>
<p>In this section we review, following Ref. <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref>, the most recent version of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e142" xlink:type="simple"/></inline-formula> within integrated information theory, using our current notation. This measure, which we call <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e143" xlink:type="simple"/></inline-formula> (‘<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e144" xlink:type="simple"/></inline-formula>-discrete/Markovian’), was defined for discrete, Markovian systems, i.e. systems with (i) a discrete set of possible states, and (ii) dynamics for which the current state depends only on the state at the previous time-step. After laying out the formal description of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e145" xlink:type="simple"/></inline-formula>, we briefly discuss these limitations, which motivate our new measures <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e146" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e147" xlink:type="simple"/></inline-formula>.</p>
<p>Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e148" xlink:type="simple"/></inline-formula> be a discrete, Markovian system. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e149" xlink:type="simple"/></inline-formula> compares the information generated by the whole system to information generated by its parts, when the system transitions to a particular state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e150" xlink:type="simple"/></inline-formula> from a preceding state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e151" xlink:type="simple"/></inline-formula> characterized by the maximum entropy distribution for the system. This is performed by use of KL divergence to compare (i) the conditional probability distribution for the preceding state of the whole given the current state; (ii) the joint distribution for the preceding states of parts given their respective current states.</p>
<p>The <italic>effective information</italic>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e152" xlink:type="simple"/></inline-formula>, generated by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e153" xlink:type="simple"/></inline-formula> being in state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e154" xlink:type="simple"/></inline-formula>, with respect to the partition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e155" xlink:type="simple"/></inline-formula>, is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e156" xlink:type="simple"/><label>(0.14)</label></disp-formula>Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e157" xlink:type="simple"/></inline-formula> is the state of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e158" xlink:type="simple"/></inline-formula> sub-system of the partition when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e159" xlink:type="simple"/></inline-formula> has state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e160" xlink:type="simple"/></inline-formula>.</p>
<p>To specify the probability distributions in (0.14), one must use Bayes' rule. For the distribution of the whole system the formula is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e161" xlink:type="simple"/><label>(0.15)</label></disp-formula>Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e162" xlink:type="simple"/></inline-formula> is the maximum entropy distribution, so<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e163" xlink:type="simple"/><label>(0.16)</label></disp-formula>for all possible initial states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e164" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e165" xlink:type="simple"/></inline-formula> is the conditional probability density for the state at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e166" xlink:type="simple"/></inline-formula> given that the state at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e167" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e168" xlink:type="simple"/></inline-formula>. Given a generative model of the system, this distribution can be derived analytically by examining the transitions allowed by the model. In the absence of a generative model the distribution can be obtained by empirical measurement of the equivalent distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e169" xlink:type="simple"/></inline-formula>. Note that in neither case is perturbation of the system required, although in the latter case the system must visit all possible states multiple times to allow reasonable estimation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e170" xlink:type="simple"/></inline-formula>. Finally, the denominator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e171" xlink:type="simple"/></inline-formula> is computed from<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e172" xlink:type="simple"/><label>(0.17)</label></disp-formula>For a part <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e173" xlink:type="simple"/></inline-formula> the analogous Bayes' rule formula is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e174" xlink:type="simple"/><label>(0.18)</label></disp-formula>Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e175" xlink:type="simple"/></inline-formula> is the maximum entropy distribution on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e176" xlink:type="simple"/></inline-formula>. To compute the conditional probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e177" xlink:type="simple"/></inline-formula> for the state at time 1 given the state at time 0 it is necessary to average over states external <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e178" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e179" xlink:type="simple"/></inline-formula> denote the complement of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e180" xlink:type="simple"/></inline-formula> within <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e181" xlink:type="simple"/></inline-formula>, so <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e182" xlink:type="simple"/></inline-formula>. Then we have<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e183" xlink:type="simple"/><label>(0.19)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e184" xlink:type="simple"/><label>(0.20)</label></disp-formula>(Note that in Ref. <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e185" xlink:type="simple"/></inline-formula> is instead computed using a perturbed version of the sub-system <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e186" xlink:type="simple"/></inline-formula>, for which the joint distribution of the noise in all the afferent connections (‘wires’) to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e187" xlink:type="simple"/></inline-formula> is taken to be maximum entropy. Here we instead assign the maximum entropy distribution to <italic>states</italic> external to the sub-system. By doing so, we eliminate the step of perturbing sub-systems, and need only perturb the whole system once, namely to impose the maximum entropy distribution as the initial state of the whole system. This choice enables simpler notation and description and does not affect the qualitative behavior of the measure <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref>.) Finally, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e188" xlink:type="simple"/></inline-formula> is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e189" xlink:type="simple"/><label>(0.21)</label></disp-formula>Given the probability distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e190" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e191" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e192" xlink:type="simple"/></inline-formula>, the effective information is computed using the formula (0.11) for the KL divergence.</p>
<p>The <italic>integrated information</italic> is defined as the effective information with respect to the minimum information partition (MIP). The MIP, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e193" xlink:type="simple"/></inline-formula>, is defined as the partition that minimizes the effective information when it is normalized by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e194" xlink:type="simple"/><label>(0.22)</label></disp-formula>Normalization is necessary because sub-systems that are almost as large as the whole system typically generate almost as much information as the whole system. Therefore, without normalization, most systems would have a highly imbalanced MIP, (e.g., one element versus the remainder of the system) and a trivially small value for integrated information. The normalization <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e195" xlink:type="simple"/></inline-formula> ensures that integrated information is specified using a partition defined using a weighted minimization of the effective information, with a bias towards partitions into sub-systems of roughly equal size. We will discuss the importance of normalization further in the section ‘<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e196" xlink:type="simple"/></inline-formula> for Markovian Gaussian systems’. Thus, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e197" xlink:type="simple"/></inline-formula> is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e198" xlink:type="simple"/><label>(0.23)</label></disp-formula>Given the MIP, the integrated information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e199" xlink:type="simple"/></inline-formula> generated by the system <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e200" xlink:type="simple"/></inline-formula> entering state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e201" xlink:type="simple"/></inline-formula> is simply the <italic>non-normalized</italic> effective information with respect to the MIP,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e202" xlink:type="simple"/><label>(0.24)</label></disp-formula>Importantly, the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e203" xlink:type="simple"/></inline-formula> is furnished by the non-normalized effective information because it is supposed to represent a physically meaningful property of the system in the corresponding ‘integrated information theory’ <xref ref-type="bibr" rid="pcbi.1001052-Tononi4">[14]</xref>.</p>
<p>For a state-independent alternative to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e204" xlink:type="simple"/></inline-formula>, one can replace the effective information with its expectation with respect to the current state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e205" xlink:type="simple"/></inline-formula>, and define the <italic>expected integrated information</italic>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e206" xlink:type="simple"/></inline-formula>, as the expected effective information across the partition that minimizes the normalized expected effective information <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref>. The expected effective information, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e207" xlink:type="simple"/></inline-formula>, is given by <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e208" xlink:type="simple"/><label>(0.25)</label></disp-formula>or equivalently<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e209" xlink:type="simple"/><label>(0.26)</label></disp-formula>Note that the second expression (0.26), but not the first (0.25), requires that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e210" xlink:type="simple"/></inline-formula> have the maximum entropy distribution <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref>. To derive (0.26) from (0.25), one uses that the maximum entropy distribution is uniform, so that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e211" xlink:type="simple"/><label>(0.27)</label></disp-formula>This ensures that one can add <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e212" xlink:type="simple"/></inline-formula> to the second term on the RHS of (0.25) and subtract <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e213" xlink:type="simple"/></inline-formula> from the first term, and then use Eq. (0.6) to obtain the expression (0.26).</p>
<p>We emphasize that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e214" xlink:type="simple"/></inline-formula> was defined only for systems that are both discrete and Markovian. The measure can not be applied to continuous systems (except those with a compact i.e. closed and bounded set of states) because there is no uniquely defined maximum entropy distribution for a continuous random variable defined on the real number line <xref ref-type="bibr" rid="pcbi.1001052-Cover1">[16]</xref>. (In fact, the measure is also not applicable to discrete systems with an infinite set of states.) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e215" xlink:type="simple"/></inline-formula> can only be applied to Markovian systems because for a non-Markovian system it is not clear how to impose the maximum entropy distribution as an initial condition, implying that the conditional probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e216" xlink:type="simple"/></inline-formula> cannot be uniquely specified by any generative model. For instance three alternatives are (i) to make all past states independent and maximum entropy; (ii) to set all past states to zero except the most recent; (iii) to just set one past state to maximum entropy and obtain the distribution for other past states from the generative model. There is no immediately apparent way to choose among these alternatives. Taken together, these limitations are important because complex (e.g. neural) systems are typically non-Markovian, and neural signals are often recorded as continuous variables. In ‘<xref ref-type="sec" rid="s4">Methods</xref>’ we describe an extension to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e217" xlink:type="simple"/></inline-formula> that renders it well-defined for stationary continuous, but still Markovian, systems by choosing a maximum entropy distribution based on the stationary variances of the states of individual elements. This enables us to compare <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e218" xlink:type="simple"/></inline-formula> with our new measure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e219" xlink:type="simple"/></inline-formula> for some example cases.</p>
</sec><sec id="s2c">
<title>The new measure, Φ<sub>E</sub></title>
<sec id="s2c1">
<title>The general case</title>
<p>In this section we introduce a new measure of integrated information, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e220" xlink:type="simple"/></inline-formula>, constructed analogously to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e221" xlink:type="simple"/></inline-formula>, but with modifications to broaden its applicability, both in theory and in practice. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e222" xlink:type="simple"/></inline-formula> is designed for stochastic stationary systems, for which it provides a single time- and state-independent value (given a timescale of measurement, discussed below). The measure is particularly easy to apply to stationary Gaussian systems, either from time-series data or from a generative model.</p>
<p>The key modification is that rather than measuring information generated by transitions from a hypothetical maximum entropy past state, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e223" xlink:type="simple"/></inline-formula> instead utilizes the actual distribution of the past state; hence the name <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e224" xlink:type="simple"/></inline-formula>, ‘<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e225" xlink:type="simple"/></inline-formula>-empirical’. This ensures that the measure does not suffer from the in-principle restrictions that pertain to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e226" xlink:type="simple"/></inline-formula>, and can be applied to both discrete and continuous systems with either Markovian or non-Markovian dynamics. (More specifically, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e227" xlink:type="simple"/></inline-formula> will be well-defined as long as the states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e228" xlink:type="simple"/></inline-formula> of the system are either discrete or have continuous probability densities with respect to a Lebesgue measure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e229" xlink:type="simple"/></inline-formula>.) A second difference is that, in order to be state-independent, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e230" xlink:type="simple"/></inline-formula> is based on the <italic>average</italic> information generated by the current state about the past state, as opposed to information generated by a particular current state. Finally, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e231" xlink:type="simple"/></inline-formula> is defined so as to enable a choice of timescale (indicated by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e232" xlink:type="simple"/></inline-formula>) over which integrated information is measured. Thus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e233" xlink:type="simple"/></inline-formula> is the integrated information generated by the current state of the system about the state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e234" xlink:type="simple"/></inline-formula> time-steps in the past.</p>
<p>We now define <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e235" xlink:type="simple"/></inline-formula> for a stochastic system with stationary dynamics. As for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e236" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e237" xlink:type="simple"/></inline-formula> is defined via ‘effective information’. For the new measure we define the effective information generated by the current state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e238" xlink:type="simple"/></inline-formula> about the state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e239" xlink:type="simple"/></inline-formula> time-steps ago, with respect to bipartition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e240" xlink:type="simple"/></inline-formula>, to be the mutual information generated by the whole system minus the sum of the mutual information generated by the parts within the bipartition. Thus<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e241" xlink:type="simple"/><label>(0.28)</label></disp-formula></p>
<p>The integrated information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e242" xlink:type="simple"/></inline-formula> is then the non-normalized effective information with respect to the minimum information bipartition (MIB),<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e243" xlink:type="simple"/><label>(0.29)</label></disp-formula>where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e244" xlink:type="simple"/><label>(0.30)</label></disp-formula>and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e245" xlink:type="simple"/><label>(0.31)</label></disp-formula></p>
<p><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e246" xlink:type="simple"/></inline-formula> can either be computed analytically from a generative model, or estimated numerically from time-series data. In either case, one must first obtain estimates of the probability distributions for the states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e247" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e248" xlink:type="simple"/></inline-formula>, and their joint distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e249" xlink:type="simple"/></inline-formula>, as well as the corresponding distributions for all sub-systems. Then, given these distributions, the corresponding entropies can be computed using Eq. (0.3), for a system with discrete states, or Eq. (0.4) for a system with continuous states. Having obtained these entropies, Eq. (0.7) can be used to obtain the mutual information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e250" xlink:type="simple"/></inline-formula> between the past and current state of the system, and likewise for all sub-systems. Given these quantities, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e251" xlink:type="simple"/></inline-formula> can then be obtained directly from Eqs. (0.28)–(0.31).</p>
<p>For numerical computation, the required probability distributions can in principle be obtained directly from data, although in practice it may be difficult to obtain sufficient data to enable accurate estimation of all the relevant entropies. As we explain in the section ‘Computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e252" xlink:type="simple"/></inline-formula> empirically under Gaussian assumptions’, this difficulty can be readily overcome if states are Gaussian distributed.</p>
<p>For analytic computation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e253" xlink:type="simple"/></inline-formula> given a generative model, we note that the probability distributions for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e254" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e255" xlink:type="simple"/></inline-formula> <italic>individually</italic> are both simply equal to the stationary distribution for the state of the system. Obtaining the joint distribution for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e256" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e257" xlink:type="simple"/></inline-formula> <italic>together</italic> will depend on the details of the generative model. Once again the situation is much easier in practice for Gaussian systems, in which case only the covariance matrix of each probability distribution is needed (see equation (0.8)). As we show in the section ‘Computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e258" xlink:type="simple"/></inline-formula> analytically for a Gaussian system’, these matrices can be derived easily from a generative model expressed as a generalized connectivity matrix, assuming Gaussian dynamics.</p>
<p>A few further remarks about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e259" xlink:type="simple"/></inline-formula> are worth making. First, that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e260" xlink:type="simple"/></inline-formula> remains well-defined as a time-dependent quantity for non-stationary stochastic systems; we focus on the stationary case for simplicity, and because of our interest in empirical measurement of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e261" xlink:type="simple"/></inline-formula> via sampling from time-series data. Second, unlike <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e262" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e263" xlink:type="simple"/></inline-formula> is not defined for deterministic systems. This is because it does not incorporate a perturbation through which to introduce probabilities into a deterministic system. Third, we restrict attention to bipartitions for computational efficiency. This is standard practice for computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e264" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Tononi4">[14]</xref>. Extension to general partitions is trivial, albeit computationally expensive. Finally, since mutual information is symmetric in its two arguments (0.7), effective information as given by (0.28) can alternatively be read in terms of information generated by the past state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e265" xlink:type="simple"/></inline-formula> about the current state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e266" xlink:type="simple"/></inline-formula>.</p>
<p>Our definition (0.28) for the effective information, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e267" xlink:type="simple"/></inline-formula>, is based on the expression (0.26) for the <italic>expected</italic> effective information, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e268" xlink:type="simple"/></inline-formula> in the construction of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e269" xlink:type="simple"/></inline-formula>. A viable alternative would be to instead use<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e270" xlink:type="simple"/><label>(0.32)</label></disp-formula>the analogue of (0.25). This quantity has previously been defined in Ref. <xref ref-type="bibr" rid="pcbi.1001052-Ay1">[22]</xref> as ‘stochastic interaction. It is the average KL divergence between (i) the past of the whole given the present of the whole, and (ii) the product of this for parts <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref>. Replacing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e271" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e272" xlink:type="simple"/></inline-formula> in the definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e273" xlink:type="simple"/></inline-formula> leads to a second measure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e274" xlink:type="simple"/></inline-formula>. In general, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e275" xlink:type="simple"/></inline-formula> will not be exactly equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e276" xlink:type="simple"/></inline-formula>. (Equality of their analogues for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e277" xlink:type="simple"/></inline-formula> relies on the past state being maximum entropy, see section ‘The previous measure, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e278" xlink:type="simple"/></inline-formula>’.) However, we show in <xref ref-type="table" rid="pcbi-1001052-t001">Table 1</xref> that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e279" xlink:type="simple"/></inline-formula> behaves very similarly to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e280" xlink:type="simple"/></inline-formula> for the examples we consider in this paper. We choose to focus on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e281" xlink:type="simple"/></inline-formula> because it explicitly operationalizes the concept of ‘information generated by the whole minus the sum of information generated by the parts’ (0.28).</p>
<table-wrap id="pcbi-1001052-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001052.t001</object-id><label>Table 1</label><caption>
<title>Integrated information computed in various ways for the networks shown in <xref ref-type="fig" rid="pcbi-1001052-g001">Figs. 1</xref> and <xref ref-type="fig" rid="pcbi-1001052-g002">2</xref>.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1001052-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Network</td>
<td align="left" colspan="1" rowspan="1">(i) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e282" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e283" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e284" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">(iv) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e285" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">(v) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e286" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e287" xlink:type="simple"/></inline-formula></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">1(a)</td>
<td align="left" colspan="1" rowspan="1">0.0323</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e288" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e289" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.0323</td>
<td align="left" colspan="1" rowspan="1">0.0323</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e290" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">1(b)</td>
<td align="left" colspan="1" rowspan="1">0.0645</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e291" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e292" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.0645</td>
<td align="left" colspan="1" rowspan="1">0.0645</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e293" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">1(c)</td>
<td align="left" colspan="1" rowspan="1">0.1283</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e294" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e295" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.1387</td>
<td align="left" colspan="1" rowspan="1">0.1313</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e296" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">1(d)</td>
<td align="left" colspan="1" rowspan="1">0.0795</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e297" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e298" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.0894</td>
<td align="left" colspan="1" rowspan="1">0.0755</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e299" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">1(e)</td>
<td align="left" colspan="1" rowspan="1">0.1285</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e300" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e301" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.1376</td>
<td align="left" colspan="1" rowspan="1">0.1303</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e302" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">1(f)</td>
<td align="left" colspan="1" rowspan="1">0.1294</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e303" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e304" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.1383</td>
<td align="left" colspan="1" rowspan="1">0.1307</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e305" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">1(g)</td>
<td align="left" colspan="1" rowspan="1">0.1266</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e306" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e307" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.1362</td>
<td align="left" colspan="1" rowspan="1">0.1288</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e308" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">2(a)</td>
<td align="left" colspan="1" rowspan="1">0.2502</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e309" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e310" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.2652</td>
<td align="left" colspan="1" rowspan="1">0.1254</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e311" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">2(b)</td>
<td align="left" colspan="1" rowspan="1">0.2965</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e312" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e313" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.3012</td>
<td align="left" colspan="1" rowspan="1">0.2647</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e314" xlink:type="simple"/></inline-formula></td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt101"><label/><p>Methods of computation are (i) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e315" xlink:type="simple"/></inline-formula> computed analytically; (ii) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e316" xlink:type="simple"/></inline-formula> computed numerically from 10 trials of 3000 data points each; (iii) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e317" xlink:type="simple"/></inline-formula> computed numerically from 10 trials of 10,000 data points each; (iv) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e318" xlink:type="simple"/></inline-formula> computed analytically; (v) (extended) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e319" xlink:type="simple"/></inline-formula> computed analytically, and (vi) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e320" xlink:type="simple"/></inline-formula> computed numerically from 10 trials of 3000 data points each, with the noise exponentially distributed. For numerical computation, means and standard deviations are given; the number of trials resulting in each value is given in parentheses. In all cases <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e321" xlink:type="simple"/></inline-formula>.</p></fn></table-wrap-foot></table-wrap>
<p>In summary, we have defined a new measure of integrated information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e322" xlink:type="simple"/></inline-formula> that is broadly well-defined, and which is easy to measure under Gaussian dynamics, either from time-series data or given a generative model (see below). In contrast, the previous measure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e323" xlink:type="simple"/></inline-formula> is only defined for discrete, Markovian systems. As a consequence, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e324" xlink:type="simple"/></inline-formula> but not <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e325" xlink:type="simple"/></inline-formula> is applicable to realistic continuous non-Markovian stochastic models of neural systems.</p>
</sec><sec id="s2c2">
<title>Computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e326" xlink:type="simple"/></inline-formula> empirically under Gaussian assumptions</title>
<p>Under Gaussian assumptions, equation (0.10) furnishes an expression for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e327" xlink:type="simple"/></inline-formula> simply in terms of covariance matrices, enabling straightforward empirical computation. The effective information is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e328" xlink:type="simple"/><label>(0.33)</label></disp-formula>and the normalization factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e329" xlink:type="simple"/></inline-formula> by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e330" xlink:type="simple"/><label>(0.34)</label></disp-formula>In practice, the procedure for computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e331" xlink:type="simple"/></inline-formula> is as follows. First one obtains empirically the covariance matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e332" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e333" xlink:type="simple"/></inline-formula> and analogues for all sub-systems. Then one uses Eq. (0.1) to obtain the partial covariance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e334" xlink:type="simple"/></inline-formula> and its sub-system analogues. Given these quantities, equations (0.33) and (0.34) furnish estimates for the effective information and normalized effective information with respect to any given bipartition. These estimates allow identification of the MIB and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e335" xlink:type="simple"/></inline-formula>, via equations (0.29) and (0.30).</p>
</sec><sec id="s2c3">
<title>Computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e336" xlink:type="simple"/></inline-formula> analytically for a Gaussian system</title>
<p>In this section we describe analytical computation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e337" xlink:type="simple"/></inline-formula> for Gaussian systems, assuming that the generative model is known. We first recognize that a generative model for a Gaussian stationary system is always equivalent to an <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e338" xlink:type="simple"/></inline-formula> (multivariate auto-regressive) process <xref ref-type="bibr" rid="pcbi.1001052-Barrett1">[18]</xref><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e339" xlink:type="simple"/><label>(0.35)</label></disp-formula>where the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e340" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e341" xlink:type="simple"/></inline-formula>, can be understood as generalized connectivity matrices acting at different time-lags, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e342" xlink:type="simple"/></inline-formula> is a stationary multivariate Gaussian ‘white noise’ source with zero mean and vanishing auto-covariance function, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e343" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e344" xlink:type="simple"/></inline-formula>. (Technically, there also exists the case <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e345" xlink:type="simple"/></inline-formula>, but we do not consider this here, because in practical application there will always be an optimal range of finite <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e346" xlink:type="simple"/></inline-formula> for model fitting.) Below, we show how to calculate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e347" xlink:type="simple"/></inline-formula> for an MVAR(1) system at timescale <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e348" xlink:type="simple"/></inline-formula>. Extension to the general <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e349" xlink:type="simple"/></inline-formula>, general <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e350" xlink:type="simple"/></inline-formula> case is given in the ‘<xref ref-type="sec" rid="s4">Methods</xref>’ section. Consider the generative model<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e351" xlink:type="simple"/><label>(0.36)</label></disp-formula>Taking the covariance of both sides of (0.36) gives<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e352" xlink:type="simple"/><label>(0.37)</label></disp-formula>Noticing that this equation is the discrete-time Lyapunov equation, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e353" xlink:type="simple"/></inline-formula> can be computed numerically, given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e354" xlink:type="simple"/></inline-formula>, for example, in Matlab via use of the ‘dlyap’ command. To compute the partial covariance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e355" xlink:type="simple"/></inline-formula> we need the single time-step auto-covariance matrix<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e356" xlink:type="simple"/><label>(0.38)</label></disp-formula>We can then use equation (0.1) to obtain the partial covariance as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e357" xlink:type="simple"/><label>(0.39)</label></disp-formula>Having values for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e358" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e359" xlink:type="simple"/></inline-formula> allows calculation of the first term in the RHS of (0.33). Calculation of the second term, and of the normalization factor, requires consideration of sub-systems. For a sub-system <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e360" xlink:type="simple"/></inline-formula>, we consider the bipartition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e361" xlink:type="simple"/></inline-formula>, and the block decomposition of vectors and matrices according to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e362" xlink:type="simple"/></inline-formula>. The matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e363" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e364" xlink:type="simple"/></inline-formula> can then be written in the form<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e365" xlink:type="simple"/><label>(0.40)</label></disp-formula>and we can use that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e366" xlink:type="simple"/><label>(0.41)</label></disp-formula>Then, again from (0.1), the partial covariance is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e367" xlink:type="simple"/><label>(0.42)</label></disp-formula>Equations (0.37)–(0.42) together furnish the covariance matrices needed to compute the effective information and normalized effective information from the formulae (0.33) and (0.34) valid for Gaussian systems. Finally, the MIB and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e368" xlink:type="simple"/></inline-formula> are obtained from Eqs. (0.29) and (0.30).</p>
</sec></sec><sec id="s2d">
<title>Φ<sub>E</sub> for Markovian Gaussian systems</title>
<sec id="s2d1">
<title>Canonical examples</title>
<p>We present results from computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e369" xlink:type="simple"/></inline-formula>, for timescale <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e370" xlink:type="simple"/></inline-formula>, for some example Markovian Gaussian systems. Results are given for analytical computation given the generative model, and for numerical computation given simulated time-series data. The example systems are characterized by the MVAR(1) dynamics<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e371" xlink:type="simple"/><label>(0.43)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e372" xlink:type="simple"/></inline-formula> contains 8 variables, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e373" xlink:type="simple"/></inline-formula> is the connectivity matrix, and each component of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e374" xlink:type="simple"/></inline-formula> is an independent Gaussian random variable of mean 0 and variance 1. We considered seven systems, with connectivity as shown in <xref ref-type="fig" rid="pcbi-1001052-g001">Fig. 1(a)–(g)</xref>; we refer to these systems ‘1(a)’, ‘1(b)’, and so on. The corresponding values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e375" xlink:type="simple"/></inline-formula> are given in <xref ref-type="fig" rid="pcbi-1001052-g001">Fig. 1(h)</xref> and <xref ref-type="table" rid="pcbi-1001052-t001">Table 1</xref>. For analytic computation, we performed the procedure described in the section ‘Computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e376" xlink:type="simple"/></inline-formula> analytically for a Gaussian system’. For simulated measurements, we first obtained time-series data from equation (0.43), and then computed <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e377" xlink:type="simple"/></inline-formula> using the recipe described in the section ‘Computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e378" xlink:type="simple"/></inline-formula> empirically under Gaussian assumptions’. To examine numerical stability of simulation measurements, we performed 10 trials for each network with 3000 post-equilibrium data points and a separate set of 10 trials with 10,000 post-equilibrium data points.</p>
<fig id="pcbi-1001052-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001052.g001</object-id><label>Figure 1</label><caption>
<title>Integrated information in Markovian Gaussian systems.</title>
<p>(a)–(g) Connectivity diagrams for seven systems as specified by the corresponding connectivity matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e379" xlink:type="simple"/></inline-formula>. Arrow widths reflect connection strengths: for (a)–(c) and (e)–(g), all connection strengths are 0.25; for system (d) each connection strength is 1/14, thus the total afferent connection to each element is 0.5. (h) Integrated information, as measured by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e380" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e381" xlink:type="simple"/></inline-formula>) for each of the systems (a)–(g), via simulated data (bars) and analytically via the generative model (asterisks). For simulated data, 10 trials were performed, with each trial generating 3000 data points. Bars show mean values; error bars show plus/minus one standard deviation. For system 1(g), sizes of sub-systems in the MIB varied across trials, falling into two distinct groups which are shown separately (the top bar reflects a group of 6 trials; the bottom bar, 4 trials).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.g001" xlink:type="simple"/></fig>
<p>For all systems, except 1(g) (which we discuss below), the analytically derived (true) value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e382" xlink:type="simple"/></inline-formula> lay within <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e383" xlink:type="simple"/></inline-formula> standard deviation of the mean value obtained via the simulations, both for 3000 and 10,000 data points (see <xref ref-type="fig" rid="pcbi-1001052-g001">Fig. 1(h)</xref> and <xref ref-type="table" rid="pcbi-1001052-t001">Table 1</xref>). This correspondence confirms the consistency of the numerical and analytical approaches described above.</p>
<p>The values of integrated information mostly correspond with expectations. For example, a ring of reciprocal connections (1(c)) integrates approximately twice as much information as a ring of unidirectional connections (1(b)), which itself integrates approximately twice as much information as a (non-closed) chain of unidirectional connections (1(a)). Also as expected, the homogenous system 1(d) has a low <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e384" xlink:type="simple"/></inline-formula> value. Perhaps in contrast to expectations, adding sparse long-range ‘short-cut’ connections to a reciprocal ring (1(e)–1(g)), in the style of a so-called ‘small world’ network <xref ref-type="bibr" rid="pcbi.1001052-Watts1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Watts2">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Shanahan1">[25]</xref>, does not increase <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e385" xlink:type="simple"/></inline-formula> (compare with network 1(c)).</p>
<p>For values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e386" xlink:type="simple"/></inline-formula> to be meaningful it is essential that they are stable with respect to numerical computation. To assess numerical stability, we calculated the coefficient of variation (the standard deviation divided by mean) across each set of 10 trials. For all networks other than 1(g), and for trial sets of both 3,000 and 10,000 data points, the coefficient of variation was less than 0.11, confirming that empirical calculation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e387" xlink:type="simple"/></inline-formula> from time-series data is stable for these networks.</p>
<p>Network 1(g) exhibited instability when measuring <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e388" xlink:type="simple"/></inline-formula> from simulation. As shown in <xref ref-type="fig" rid="pcbi-1001052-g001">Fig. 1(h)</xref>, the corresponding values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e389" xlink:type="simple"/></inline-formula> fell close to one of two values, one of which was the true (analytically derived) value. For simulations of 3,000 data points 6/10 trials produced <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e390" xlink:type="simple"/></inline-formula> estimates close to the true value; for 10,000 data points 4/10 trials provided such estimates. This instability arises from the use of <italic>normalized</italic> effective information (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e391" xlink:type="simple"/></inline-formula>) in identifying the MIB, but <italic>non-normalized</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e392" xlink:type="simple"/></inline-formula> in specifying the corresponding value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e393" xlink:type="simple"/></inline-formula>. Given finite data, estimates of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e394" xlink:type="simple"/></inline-formula> cannot be guaranteed to be accurate. As a result, inter-trial variation in measuring <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e395" xlink:type="simple"/></inline-formula> from data can arise when (i) there are two (or more) partitions with similar values of normalized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e396" xlink:type="simple"/></inline-formula> close to the true minimum (used to identify the MIB), and (ii) these partitions have substantially different values for non-normalized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e397" xlink:type="simple"/></inline-formula>. The latter condition will typically hold when partitions with similar normalized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e398" xlink:type="simple"/></inline-formula> have significantly different sub-system sizes (see the section ‘The previous measure, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e399" xlink:type="simple"/></inline-formula>’). Network 1(g) illustrates this difficulty. For this network, the true MIB is the bipartition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e400" xlink:type="simple"/></inline-formula>, for which the normalized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e401" xlink:type="simple"/></inline-formula> is 0.0213. However, there is an uneven bipartition, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e402" xlink:type="simple"/></inline-formula> for which the normalized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e403" xlink:type="simple"/></inline-formula> is 0.0218, i.e., very similar to the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e404" xlink:type="simple"/></inline-formula> for the true MIB. However, the non-normalized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e405" xlink:type="simple"/></inline-formula> for the MIB (i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e406" xlink:type="simple"/></inline-formula>) is 0.1266, whereas the value for the uneven bipartition is 0.0966. <xref ref-type="fig" rid="pcbi-1001052-g001">Fig. 1(h)</xref> and <xref ref-type="table" rid="pcbi-1001052-t001">Table 1</xref> show that empirical measurements of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e407" xlink:type="simple"/></inline-formula> cluster around these two values.</p>
<p>One may consider that this problem of instability could be avoided by using non-normalized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e408" xlink:type="simple"/></inline-formula> to identify the MIB. However, as discussed in the section ‘The previous measure, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e409" xlink:type="simple"/></inline-formula>’, in this case <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e410" xlink:type="simple"/></inline-formula> would always be trivially small because, for any non-trivial system <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e411" xlink:type="simple"/></inline-formula>, a bipartition of the form <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e412" xlink:type="simple"/></inline-formula> would generate almost as much information as the whole system. A second solution would be to specify <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e413" xlink:type="simple"/></inline-formula> in terms of normalized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e414" xlink:type="simple"/></inline-formula>. However, in this case the meaning of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e415" xlink:type="simple"/></inline-formula> would be substantially altered inasmuch as it could no longer be considered a measure of the quantity of information generated (or integrated) by a system.</p>
</sec><sec id="s2d2">
<title>Optimization of networks for generating high <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e416" xlink:type="simple"/></inline-formula></title>
<p>To examine whether network structures other than reciprocally connected rings could generate high levels of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e417" xlink:type="simple"/></inline-formula>, we performed numerical optimizations using a genetic algorithm (GA). Specifically, we used <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e418" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e419" xlink:type="simple"/></inline-formula>) as an objective function for evolving populations of networks with dynamics governed by MVAR(1) processes (see Eq. (0.43)). We performed two sets of optimizations under different constraints on the connectivity matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e420" xlink:type="simple"/></inline-formula>. In the first set, all connection strengths were fixed (‘fixed’ condition; two afferents per element each with strength 0.25). In the second set, connection strengths were allowed to vary (‘vary’ condition; total afferent to each element equal to 0.5, all afferents to a given element equal and positive). Each condition consisted of 20 separate GAs, each with 30 randomly initialized networks in the population; (in the ‘vary’ condition networks were initialized with elements having on average 2 afferent connections). Each GA ran for 200 generations, allowing fitness to asymptote. Within each generation, the fitness of each network was determined by analytical computation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e421" xlink:type="simple"/></inline-formula>; networks were then ranked by fitness and a new population was formed by rank-based selection and mutation. In the ‘fixed’ condition, each network was mutated by rearranging 2 connections; in the ‘vary’ condition each network was mutated by (with equal probability) adding, removing, or swapping 2 connections, followed by renormalization of total afference to each element to 0.5.</p>
<p>The results of the optimizations are shown in <xref ref-type="fig" rid="pcbi-1001052-g002">Fig. 2</xref> and <xref ref-type="table" rid="pcbi-1001052-t001">Table 1</xref>. Network 2(a) is the fittest (highest <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e422" xlink:type="simple"/></inline-formula>) across all 20 GAs in the ‘fixed’ condition; this network topology was discovered by 6 out of the 20 GAs in this condition. The network has <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e423" xlink:type="simple"/></inline-formula>, approximately twice the value of the reciprocal ring networks shown in <xref ref-type="fig" rid="pcbi-1001052-g001">Fig. 1</xref>. Network 2(b) is the fittest across all 20 GAs in the ‘vary’ condition, exhibiting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e424" xlink:type="simple"/></inline-formula>, i.e., substantially higher again. This particular topology was discovered by only 2/20 GAs, perhaps due to the larger search-space in this condition. It is noteworthy that both of these ‘fittest’ networks show highly heterogeneous connectivity patterns, consistent with the intuition that integrated information is characterized by the coexistence of differentiated and integrated dynamics.</p>
<fig id="pcbi-1001052-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001052.g002</object-id><label>Figure 2</label><caption>
<title>Networks optimized for high integrated information.</title>
<p>(a) Optimal network for 2 afferents of 0.25 to each node. This has <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e425" xlink:type="simple"/></inline-formula>. (b) Optimal network for total afferent of 0.5 to each node, and all connections to a given node equal. This has <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e426" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.g002" xlink:type="simple"/></fig>
<p>The observation that the fittest network found in each condition was only reached by a minority of GAs suggests that the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e427" xlink:type="simple"/></inline-formula> landscape across MVAR(1) systems has local maxima and may exhibit ruggedness and discontinuities. To characterize this landscape, we first plotted the distribution of fitness values across all networks in the final populations from GAs that yielded the (fittest) networks 2(a) and 2(b). <xref ref-type="fig" rid="pcbi-1001052-g003">Figs. 3(a,b)</xref> show that in both cases the modal value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e428" xlink:type="simple"/></inline-formula> was substantially less than the maximum value, indicating a lack of convergence suggestive of local maxima and/or ruggedness <xref ref-type="bibr" rid="pcbi.1001052-Mitchell1">[26]</xref>. We next examined the sensitivity of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e429" xlink:type="simple"/></inline-formula> to single mutations. <xref ref-type="fig" rid="pcbi-1001052-g003">Figs. 3(c) and 3(d)</xref> show the percentage decrease in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e430" xlink:type="simple"/></inline-formula> following 200 separate mutations of networks 2(a) and 2(b) respectively (the corresponding mutation type was used in each case, i.e., ‘fixed’ for 2(a) and ‘vary’ for 2(b)). For network 2(a), post-mutation fitness decreases cluster in the range 10–20%, with a few instances of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e431" xlink:type="simple"/></inline-formula>. For network 2(b), more than 20% of mutations resulted in a fitness decrease of 50% or more. Together, these observations show that the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e432" xlink:type="simple"/></inline-formula> generated by a network is highly sensitive to small changes in topology and connection strength, further pointing to the ruggedness of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e433" xlink:type="simple"/></inline-formula> landscape.</p>
<fig id="pcbi-1001052-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001052.g003</object-id><label>Figure 3</label><caption>
<title>Examination of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e434" xlink:type="simple"/></inline-formula> landscape with respect to network connectivity.</title>
<p>(a) Histogram of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e435" xlink:type="simple"/></inline-formula> for the 30 networks in the final population of a GA that yielded optimal network 2(a). (b) Histogram of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e436" xlink:type="simple"/></inline-formula> for the 30 networks in the final population of a GA that yielded optimal network 2(b). (c) Histogram of percentage decrease in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e437" xlink:type="simple"/></inline-formula> following single mutations of network 2(a) (200 evaluations). (d) Histogram of percentage decrease in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e438" xlink:type="simple"/></inline-formula> following single mutations of network 2(b) (200 evaluations). (e) Discontinuity in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e439" xlink:type="simple"/></inline-formula> as connection strength from element 6 to element 1 continuously changes (network 2(a); (all other connections fixed at 0.25)).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.g003" xlink:type="simple"/></fig>
<p>The instability arising from using normalized effective information to find the MIB, (see ‘Canonical examples’), suggests that there may be discontinuities, as well as ruggedness, in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e440" xlink:type="simple"/></inline-formula> landscape. We were able to confirm the existence of such discontinuities by incrementally perturbing a specific connection in the example network 2(a). The MIB for this network is the bipartition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e441" xlink:type="simple"/></inline-formula>, for which the normalized effective information is 0.0421. However, there is an uneven bipartition, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e442" xlink:type="simple"/></inline-formula> with the very similar normalized effective information of 0.0424. We incrementally weakened the connection between the two sub-systems in this uneven bipartition, finding that there is a discontinuous change in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e443" xlink:type="simple"/></inline-formula> at the point at which the uneven bipartition becomes the MIB (see <xref ref-type="fig" rid="pcbi-1001052-g003">Fig. 3(e)</xref>).</p>
</sec><sec id="s2d3">
<title>Comparison with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e444" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e445" xlink:type="simple"/></inline-formula>, and full table of MVAR(1) results</title>
<p>It is instructive to compare results obtained using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e446" xlink:type="simple"/></inline-formula> with those obtained from the version of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e447" xlink:type="simple"/></inline-formula> extended to apply to stationary continuous (but still Markovian) systems (see sections ‘The previous measure, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e448" xlink:type="simple"/></inline-formula>’ and ‘<xref ref-type="sec" rid="s4">Methods</xref>’). <xref ref-type="table" rid="pcbi-1001052-t001">Table 1</xref> shows (extended) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e449" xlink:type="simple"/></inline-formula> values for the various networks discussed above, as well as the corresponding <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e450" xlink:type="simple"/></inline-formula> values. For networks 1(a) and 1(b) the two measures are exactly equivalent, which is explained by the stationary and maximum entropy distributions coinciding. For the remaining networks, (except network 2(a), discussed below), the two measures remain very similar, confirming <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e451" xlink:type="simple"/></inline-formula> as a valid and useful measure of integrated information.</p>
<p>The network 2(a) has a value for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e452" xlink:type="simple"/></inline-formula> that is approximately double that of the corresponding <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e453" xlink:type="simple"/></inline-formula>. This discrepancy can also be attributed to the instability arising from normalization. Specifically, the difference between the stationary and maximum entropy distributions in this case is sufficient to lead to two different MIBs, with constituent sub-systems of different sizes. In fact, use of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e454" xlink:type="simple"/></inline-formula> leads to the MIB <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e455" xlink:type="simple"/></inline-formula> of the <italic>perturbed</italic> version of this network discussed in ‘Optimization of networks for generating high <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e456" xlink:type="simple"/></inline-formula>’.</p>
<p>We also compared results obtained using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e457" xlink:type="simple"/></inline-formula> with those obtained using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e458" xlink:type="simple"/></inline-formula>, the measure constructed using the alternative expression (0.32) for the effective information (<xref ref-type="table" rid="pcbi-1001052-t001">Table 1</xref>). We found that the two measures behave in qualitatively the same way across all examples.</p>
</sec></sec><sec id="s2e">
<title>Extension to multiple lags and to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e459" xlink:type="simple"/></inline-formula> processes</title>
<p>The analyses in the previous section were concerned with integrated information measured across a single time-step for MVAR(1) processes. However, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e460" xlink:type="simple"/></inline-formula> is well-defined for general <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e461" xlink:type="simple"/></inline-formula> processes and can measure integrated information over any number of time-steps (lags). Here we illustrate this property using three simple examples in which <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e462" xlink:type="simple"/></inline-formula> was computed analytically, via the method outlined in ‘Computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e463" xlink:type="simple"/></inline-formula> analytically for a Gaussian system’ and ‘<xref ref-type="sec" rid="s4">Methods</xref>’. <xref ref-type="fig" rid="pcbi-1001052-g004">Fig. 4(a)</xref> shows <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e464" xlink:type="simple"/></inline-formula> measured for various values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e465" xlink:type="simple"/></inline-formula>, (where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e466" xlink:type="simple"/></inline-formula> specifies the lag), for the network 1(c). <xref ref-type="fig" rid="pcbi-1001052-g004">Fig. 4(b)</xref> shows the same analysis conducted for network 2(b). Note that both of these networks are animated by MVAR(1) processes, which explains why <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e467" xlink:type="simple"/></inline-formula> peaks at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e468" xlink:type="simple"/></inline-formula> in both cases, (in other words, for these networks, most of the integrated information generated about past states by the current state is generated about the most recent past state (i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e469" xlink:type="simple"/></inline-formula>)).</p>
<fig id="pcbi-1001052-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001052.g004</object-id><label>Figure 4</label><caption>
<title>Integrated information, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e470" xlink:type="simple"/></inline-formula> measured for states multiple time-steps in the past, i.e. for varying <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e471" xlink:type="simple"/></inline-formula>.</title>
<p>(a) Network 1(c). (b) Network 2(b). (c) Example MVAR(3) process, see Eq. (0.44).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.g004" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1001052-g004">Fig. 4(c)</xref> shows <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e472" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e473" xlink:type="simple"/></inline-formula> for the MVAR(3) process<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e474" xlink:type="simple"/><label>(0.44)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e475" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e476" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e477" xlink:type="simple"/></inline-formula> are respectively the connectivity matrices of networks 1(c), 2(b) and 2(a), each divided by 2. Note that this generalized connectivity matrix was chosen purely to provide an example of an MVAR(3) process. For this system, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e478" xlink:type="simple"/></inline-formula> peaks at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e479" xlink:type="simple"/></inline-formula>, indicating that most information is integrated about the state two time-steps previous to the current state. These examples verify that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e480" xlink:type="simple"/></inline-formula> can be applied at arbitrary lags to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e481" xlink:type="simple"/></inline-formula> processes, and that it does detect integrated information at time-scales corresponding to a system's underlying generative mechanism.</p>
</sec><sec id="s2f">
<title>Auto-regressive Φ (Φ<sub>AR</sub>)</title>
<p>We have presented a measure of integrated information, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e482" xlink:type="simple"/></inline-formula>, that is practical to measure from time-series data under Gaussian assumptions. However, in the case of stationary, non-Gaussian distributed time-series, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e483" xlink:type="simple"/></inline-formula> can no longer be obtained directly from empirical covariance matrices, and the required entropies must be obtained via estimation of the corresponding probability distributions. For non-trivial systems accurate entropy estimation may typically require the collection of more data than is practical.</p>
<p>We now describe how, even for the non-Gaussian case, the recipe used to calculate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e484" xlink:type="simple"/></inline-formula> under Gaussian assumptions can nonetheless lead to a meaningful quantity reflecting integrated information. We call this quantity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e485" xlink:type="simple"/></inline-formula> (‘auto-regressive <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e486" xlink:type="simple"/></inline-formula>’). By construction, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e487" xlink:type="simple"/></inline-formula> is equivalent to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e488" xlink:type="simple"/></inline-formula> for Gaussian systems, however, for non-Gaussian systems it may differ. In all cases, because it is based on empirical covariance matrices, it remains easy to measure in practice. The motivation for considering <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e489" xlink:type="simple"/></inline-formula> as a useful measure of integrated information rests on relations between conditional entropy, partial covariance and linear regression prediction error, explained below <xref ref-type="bibr" rid="pcbi.1001052-Barnett1">[17]</xref>.</p>
<p>First we rehearse the concept of linear regression. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e490" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e491" xlink:type="simple"/></inline-formula> be two multivariate random variables. Then the linear regression of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e492" xlink:type="simple"/></inline-formula> on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e493" xlink:type="simple"/></inline-formula> is the expression<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e494" xlink:type="simple"/><label>(0.45)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e495" xlink:type="simple"/></inline-formula> is termed the regression matrix, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e496" xlink:type="simple"/></inline-formula> is a vector of constants, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e497" xlink:type="simple"/></inline-formula> is the prediction error (or ‘residual’) <xref ref-type="bibr" rid="pcbi.1001052-Wilks1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Davidson1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Ding1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Barnett1">[17]</xref>. The residual is a random vector uncorrelated with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e498" xlink:type="simple"/></inline-formula>. This representation is unique given the distributions of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e499" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e500" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e501" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e502" xlink:type="simple"/></inline-formula> given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e503" xlink:type="simple"/><label>(0.46)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e504" xlink:type="simple"/><label>(0.47)</label></disp-formula>The residual has zero mean and, importantly, its covariance matrix is precisely the partial covariance of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e505" xlink:type="simple"/></inline-formula> given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e506" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1001052-Barnett1">[17]</xref>, thus<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e507" xlink:type="simple"/><label>(0.48)</label></disp-formula>Note that this identity holds for any <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e508" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e509" xlink:type="simple"/></inline-formula>, Gaussian or otherwise. For the case that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e510" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e511" xlink:type="simple"/></inline-formula> are Gaussian, we can use Eq. (0.9) to obtain, for all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e512" xlink:type="simple"/></inline-formula>,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e513" xlink:type="simple"/><label>(0.49)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e514" xlink:type="simple"/></inline-formula> is the dimension of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e515" xlink:type="simple"/></inline-formula>. This relation between conditional entropy and linear regression prediction error implies that, for Gaussian systems, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e516" xlink:type="simple"/></inline-formula> can be re-expressed in terms of linear regression prediction errors. Thus, the formula (0.33) for effective information can be re-written as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e517" xlink:type="simple"/><label>(0.50)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e518" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e519" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e520" xlink:type="simple"/></inline-formula> are the residuals in the regressions<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e521" xlink:type="simple"/><label>(0.51)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e522" xlink:type="simple"/><label>(0.52)</label></disp-formula>For a non-Gaussian system, although Eq. (0.50) does not hold, its RHS nonetheless constitutes a quantity that is easy to measure empirically. This quantity forms the basis of the alternative measure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e523" xlink:type="simple"/></inline-formula>, which we now define. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e524" xlink:type="simple"/></inline-formula> be a stationary, not necessarily Gaussian, system, and let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e525" xlink:type="simple"/></inline-formula> be the RHS of Eq. (0.50), i.e.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e526" xlink:type="simple"/><label>(0.53)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e527" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e528" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e529" xlink:type="simple"/></inline-formula> are the residuals in the regressions (0.51) and (0.52). Then <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e530" xlink:type="simple"/></inline-formula> is simply <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e531" xlink:type="simple"/></inline-formula> for the bipartition that minimizes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e532" xlink:type="simple"/></inline-formula> divided by the normalization factor<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e533" xlink:type="simple"/><label>(0.54)</label></disp-formula>Thus,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e534" xlink:type="simple"/><label>(0.55)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e535" xlink:type="simple"/><label>(0.56)</label></disp-formula>For Gaussian systems, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e536" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e537" xlink:type="simple"/></inline-formula> are exactly equal. For non-Gaussian systems the two measures differ, because the relation (0.49) between conditional entropy and linear regression prediction error no longer holds. However the equivalence (0.48) between partial covariance and prediction error does still hold. Hence, for <italic>any</italic> stationary system, the recipe for computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e538" xlink:type="simple"/></inline-formula> under Gaussian assumptions (as laid out in ‘Computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e539" xlink:type="simple"/></inline-formula> empirically under Gaussian assumptions’) yields precisely <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e540" xlink:type="simple"/></inline-formula>. Notably, this recipe implies that it is not necessary to explicitly carry out the linear regressions; rather, the equivalence (0.48) shows that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e541" xlink:type="simple"/></inline-formula> can be calculated using empirical covariance matrices.</p>
<p><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e542" xlink:type="simple"/></inline-formula> is meaningful as a measure of integrated information because of its formulation in terms of linear regression prediction error. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e543" xlink:type="simple"/></inline-formula> compares the whole system to the sum of its parts in terms of the log-ratio of the variance of the past state to the variance of the residual of a linear regression of the past on the present. In other words, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e544" xlink:type="simple"/></inline-formula> can be understood as a measure of the extent to which the <italic>present</italic> global state of the system predicts the <italic>past</italic> global state of the system, as compared to predictions based on the most informative decomposition of the system into its component parts. When Gaussian conditions are satisfied, the interpretation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e545" xlink:type="simple"/></inline-formula> in terms of (backwards) prediction becomes exactly equivalent to the interpretation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e546" xlink:type="simple"/></inline-formula> in terms of Shannon information. Note that in fact, by the symmetry of mutual information (0.7), (0.28), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e547" xlink:type="simple"/></inline-formula> could also be expressed in terms of entirely analogous linear regressions in which the <italic>present</italic> is used to predict the <italic>future</italic>. Understood this way, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e548" xlink:type="simple"/></inline-formula> provides an interesting complement to complexity measures based on Granger causality, such as <italic>causal density</italic> <xref ref-type="bibr" rid="pcbi.1001052-Seth1">[5]</xref>, which are also based on linear regression models <xref ref-type="bibr" rid="pcbi.1001052-Granger1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Seth1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Barrett1">[18]</xref> (see ‘Comparison with causal density and neural complexity’).</p>
<p>To demonstrate the use of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e549" xlink:type="simple"/></inline-formula> as distinct from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e550" xlink:type="simple"/></inline-formula>, we re-animated the networks 1(a)–1(g), 2(a) and 2(b) with non-Gaussian dynamics. Specifically, we replaced the Gaussian noise sources <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e551" xlink:type="simple"/></inline-formula> in Eq. (0.43) with independent random variables drawn from exponential distributions with mean (and variance) 1. This selection was motivated by the observation that aggregate assemblies of Poissonian spiking neurons typically follow an exponential distribution <xref ref-type="bibr" rid="pcbi.1001052-Dayan1">[31]</xref>. <xref ref-type="fig" rid="pcbi-1001052-g005">Fig. 5</xref> shows representative examples of single-element empirical stationary distributions resulting from this modified dynamics; all show a large deviation from the Gaussian. For each network we computed <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e552" xlink:type="simple"/></inline-formula> empirically from 10 trials of 3000 data points each. The results, shown in <xref ref-type="table" rid="pcbi-1001052-t001">Table 1</xref>, suggest that in each case <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e553" xlink:type="simple"/></inline-formula> for the non-Gaussian dynamics is approximately equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e554" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e555" xlink:type="simple"/></inline-formula>) for the Gaussian dynamics. This finding provides support for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e556" xlink:type="simple"/></inline-formula> as a useful alternative to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e557" xlink:type="simple"/></inline-formula>, applicable to non-Gaussian dynamics.</p>
<fig id="pcbi-1001052-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001052.g005</object-id><label>Figure 5</label><caption>
<title>Stationary distributions for elements in networks animated with exponentially distributed noise.</title>
<p>Each panel shows an empirical probability distribution as a histogram taken from 3000 data points from element 1 in (a) network 1(b), (b) network 1(d), and (c) network 2(b).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.g005" xlink:type="simple"/></fig></sec></sec><sec id="s3">
<title>Discussion</title>
<p>In this paper we have presented two new measures of integrated information, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e558" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e559" xlink:type="simple"/></inline-formula>. As with a previous measure, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e560" xlink:type="simple"/></inline-formula>, our measures quantify the information generated by a system over and above that which can be accounted for by its parts acting independently <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref>. However, whereas <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e561" xlink:type="simple"/></inline-formula> is defined only for discrete Markovian systems, and is therefore difficult to measure in practice, our quantities are well defined much more generally, and are easily applicable to stationary time-series data. Our key innovations are (i) to treat information in terms of reduction in uncertainty from the empirical as opposed to the maximum entropy distribution (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e562" xlink:type="simple"/></inline-formula>), and (ii) to interpret integrated information in terms of predictive ability of the present of a system with respect to its past (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e563" xlink:type="simple"/></inline-formula>). Simulations showed that our measures conform to intuitions regarding conjoined dynamical integration and segregation; where comparisons could be made, in most cases our measures quantitatively aligned with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e564" xlink:type="simple"/></inline-formula>. By showing how to measure integrated information from time-series data and for non-trivial non-Markovian systems, our results provide new opportunities for examining the role of integrated information in complex biological systems of all kinds, and carry implications for integrated information theories of consciousness. In the following discussion, we use the symbol <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e565" xlink:type="simple"/></inline-formula> to refer to integrated information independently of its method of measurement.</p>
<sec id="s3a">
<title>Empirical and maximum entropy distributions</title>
<p>As mentioned, many of the restrictions in applicability of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e566" xlink:type="simple"/></inline-formula> arise from the use of the maximum entropy distribution to measure information. The maximum entropy distribution is maximally agnostic with respect to the behavior of a system, and represents, in some sense, its potential, or ‘capacity’ (see ‘Integrated information as a measure of consciousness’ and ‘Comparison with causal density and neural complexity’). However, since the maximum entropy distribution typically does not arise spontaneously, it must be introduced as the distribution of a hypothetical initial state <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref>. To compute <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e567" xlink:type="simple"/></inline-formula> one therefore has to characterize evolution from all possible initial states of the system. However, for most practical purposes, especially in biology, it is only possible to experimentally examine systems in the context of their ongoing evolution as a sequence of states. Unless the system is Markovian, evolution from a state with history is not the same as evolution from a hypothetical initial state, implying that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e568" xlink:type="simple"/></inline-formula> cannot be applied to non-Markovian systems (with the exception of idealized simulated systems for which a separate generative model can be written down for evolution from the initial state). Equally important, but easier to appreciate, is that it is not possible to apply <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e569" xlink:type="simple"/></inline-formula> to continuous systems (except those with a compact, i.e. closed and bounded, set of states) because there is no uniquely defined maximum entropy distribution for a continuous random variable defined on the real number line <xref ref-type="bibr" rid="pcbi.1001052-Cover1">[16]</xref>.</p>
<p>Our new measure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e570" xlink:type="simple"/></inline-formula> eliminates the need to consider the maximum entropy distribution by being based instead on the information generated by the current state of the system about the <italic>actual</italic> state of the system some number of time-steps in the past. This approach lifts the conditions that the system be discrete and Markovian. (Note however that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e571" xlink:type="simple"/></inline-formula> but not <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e572" xlink:type="simple"/></inline-formula> is applicable to deterministic systems, by virtue of introducing probabilities via the maximum entropy initial state.)</p>
<p>In principle, use of the empirical distribution de-emphasizes the notion of ‘capacity’ because the generation of information is measured with respect to what the system <italic>has done</italic> rather than what it <italic>could do</italic>. However, over large samples and for ergodic systems, this distinction becomes increasingly blurred. In practice, computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e573" xlink:type="simple"/></inline-formula> via sampling from time-series requires the data to be stationary. We recognize that not all complex biological systems generate stationary dynamics (see, e.g., Ref. <xref ref-type="bibr" rid="pcbi.1001052-Buzsaki1">[32]</xref>). However, stationarity is a common pre-requisite for statistical analysis of time-series data <xref ref-type="bibr" rid="pcbi.1001052-Hamilton1">[33]</xref>, and neural data can often be brought into this form, for example by detrending, taking first-differences and/or binning observations into short time windows <xref ref-type="bibr" rid="pcbi.1001052-Seth5">[34]</xref>. Furthermore, neural dynamics are often characterized as a series of ‘metastable’ states <xref ref-type="bibr" rid="pcbi.1001052-Werner1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Freeman1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Bressler2">[37]</xref>, each of which may be locally stationary. Stationarity can also depend on the spatiotemporal granularity of observation. Dynamics that appear non-stationary at one time scale may exhibit stationarity when sampled over different time scales, underlining the principle that data acquisition should be guided by the constraints of subsequent analysis methods.</p>
<p>Use of the empirical, rather than maximum entropy distribution also changes the means by which <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e574" xlink:type="simple"/></inline-formula> is computed. To compute <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e575" xlink:type="simple"/></inline-formula>, one requires the conditional probability distributions for the past state given the present state, but with an <italic>a priori</italic> maximum entropy distribution on the past state. Because of the maximum entropy condition (which represents ‘perturbation’ of the system), these distributions cannot be obtained empirically, but they can be obtained by applying Bayes' rule given a forward dynamical model estimated from the data (i.e. conditional probability distributions for the present state, given the past state). By contrast, computation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e576" xlink:type="simple"/></inline-formula> does not require Bayes' rule because, in the absence of (maximum entropy) perturbation, one can obtain the full joint distribution for the past and present directly from the data.</p>
</sec><sec id="s3b">
<title>Practical applicability and Gaussian dynamics</title>
<p><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e577" xlink:type="simple"/></inline-formula> is particularly easy to apply to data under Gaussian assumptions. This is because the relevant entropies can be estimated directly from empirical covariance matrices. It is also possible to compute <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e578" xlink:type="simple"/></inline-formula> analytically from a generative model for a Gaussian system, (i.e., to any desired level of accuracy, without explicitly simulating or observing its dynamics); in that case, one obtains the necessary covariance matrices analytically. This means that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e579" xlink:type="simple"/></inline-formula> can be evaluated in practice for a broad range of biological systems.</p>
<p>While Gaussian dynamics are common in biology (and the assumption of Gaussianity even more so), many systems depart from this assumption. For example, the spiking activity of populations of neurons typically exhibit exponentially distributed dynamics. For the non-Gaussian case, one can still in principle calculate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e580" xlink:type="simple"/></inline-formula> by obtaining the necessary entropies directly from data. However, in practice, accurately obtaining all of the underlying probability distributions may typically require the collection of more data than is practical. To overcome this, we introduced the second measure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e581" xlink:type="simple"/></inline-formula>. This is constructed analogously to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e582" xlink:type="simple"/></inline-formula>, but with information replaced by the reduction in the generalized covariance of the past state under prediction via linear regression on the current state. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e583" xlink:type="simple"/></inline-formula> is interpreted as measuring how well the present state of a system predicts some previous state, but only to the extent that predictions based on the whole outstrip predictions based on parts independently. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e584" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e585" xlink:type="simple"/></inline-formula> are equivalent for Gaussian systems, but otherwise differ; (recall however that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e586" xlink:type="simple"/></inline-formula> can be obtained for any system by using the recipe for computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e587" xlink:type="simple"/></inline-formula> for a Gaussian system). In our examples, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e588" xlink:type="simple"/></inline-formula> was in fact insensitive to a change from Gaussian noise to exponentially distributed noise, supporting its use as an alternative to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e589" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s3c">
<title>Normalization and instability</title>
<p>All versions of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e590" xlink:type="simple"/></inline-formula> require a normalization step. Specifically, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e591" xlink:type="simple"/></inline-formula> is determined by the <italic>non-normalized</italic> effective information (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e592" xlink:type="simple"/></inline-formula>) across a minimum information bipartition (MIB) which is specified as the bipartition which minimizes the <italic>normalized</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e593" xlink:type="simple"/></inline-formula> (the informational ‘weakest link’). Normalization enforces a bias towards bipartitions consisting of sub-systems of roughly equal size. Without normalization, MIBs would typically divide systems into single elements versus the remainder of the system, leading to trivially small values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e594" xlink:type="simple"/></inline-formula>. On the other hand, it remains important to determine the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e595" xlink:type="simple"/></inline-formula> using the non-normalized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e596" xlink:type="simple"/></inline-formula> in order to allow <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e597" xlink:type="simple"/></inline-formula> to be interpreted as a quantity of information.</p>
<p>The use of normalization, as just described, leads to instabilities. Our simulations have shown that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e598" xlink:type="simple"/></inline-formula> can be (i) discontinuous under a continuous perturbation of dynamics, and (ii) highly sensitive to the accuracy of entropy estimation from finite data. In our examples, these instabilities arose precisely when there were multiple partitions with similar values of normalized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e599" xlink:type="simple"/></inline-formula> close to the true minimum <italic>and</italic> these partitions had substantially different values of non-normalized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e600" xlink:type="simple"/></inline-formula>. This instability does not arise for all systems, and indeed for most of our examples <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e601" xlink:type="simple"/></inline-formula> is numerically stable. Nonetheless, the embedding of normalization within the definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e602" xlink:type="simple"/></inline-formula> challenges ascription of physical meaning to any measured value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e603" xlink:type="simple"/></inline-formula>. This is because the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e604" xlink:type="simple"/></inline-formula> is in all cases dependent to some arbitrary degree on the normalization process involved in determining the MIB.</p>
</sec><sec id="s3d">
<title>Integrated information as a measure of consciousness</title>
<p>Previous measures of integrated information (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e605" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e606" xlink:type="simple"/></inline-formula>) were formulated in the context of a theory of consciousness, the ‘integrated information theory of consciousness’ (IITC). According to the IITC, consciousness <italic>is</italic> integrated information, and has the status of a fundamental property of the universe, equivalent to mass, charge, and the like <xref ref-type="bibr" rid="pcbi.1001052-Tononi4">[14]</xref>. On this theory a low value of integrated information would correspond to a low conscious ‘level’ (e.g., coma, general anesthesia, deep dreamless sleep) and a high value to normal conscious wakefulness. If one subscribes to the theory using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e607" xlink:type="simple"/></inline-formula>, then one must interpret consciousness (integrated information) as a function of state transitions <xref ref-type="bibr" rid="pcbi.1001052-Balduzzi1">[11]</xref>; accordingly, one cannot ask about the conscious level of a system <italic>per se</italic>. By contrast, if one applies <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e608" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e609" xlink:type="simple"/></inline-formula> to a stationary system then they are state-independent and so, subscribing to the IITC with these measures involves viewing integrated information as a property of the system's dynamics. This in turn would imply that (i) conscious level is constant during each stationary epoch in brain activity, and (ii) conscious level changes when functional connectivity changes, modifying the stationary statistics. This view recalls William James' notion of consciousness as a process <xref ref-type="bibr" rid="pcbi.1001052-James1">[19]</xref> and is consistent with a large amount of empirical evidence showing correlations between conscious level and plausibly stationary epochs of brain activity. For example, normal conscious wakefulness is characterized by low-amplitude high-frequency oscillations in the cortical EEG <xref ref-type="bibr" rid="pcbi.1001052-Seth6">[38]</xref>, whereas epileptic absence seizures are characterized instead by increased synchrony in thalamocortical systems <xref ref-type="bibr" rid="pcbi.1001052-Arthuis1">[39]</xref>. As mentioned in the section ‘Empirical and maximum entropy distributions’, neural dynamics may be metastable <xref ref-type="bibr" rid="pcbi.1001052-Werner1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Freeman1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Bressler2">[37]</xref>, with locally stationary periods corresponding to a conscious state with a particular level and content. Our results now make it possible to measure the integrated information corresponding to these various states and to compare these values with other indices of consciousness, both subjective (e.g., verbal reports, confidence ratings, etc.) and objective (e.g., EEG synchrony, widespread brain activity, etc.) <xref ref-type="bibr" rid="pcbi.1001052-Seth7">[40]</xref>. Importantly, it is now possible to quantitatively compare integrated information with other measures of neural dynamics that operationalize in different ways the notion that consciousness conjoins dynamical integration and differentiation, such as ‘causal density’ <xref ref-type="bibr" rid="pcbi.1001052-Seth8">[41]</xref> and ‘neural complexity’ <xref ref-type="bibr" rid="pcbi.1001052-Tononi1">[8]</xref> (see ‘Comparison with causal density and neural complexity’).</p>
<p>An important feature of the IITC as previously expressed is that consciousness <italic>qua</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e610" xlink:type="simple"/></inline-formula> is best considered as a capacity (equivalently a potential, or disposition), and not as an ‘object’ or a process <xref ref-type="bibr" rid="pcbi.1001052-Tononi4">[14]</xref>. The original <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e611" xlink:type="simple"/></inline-formula> operationalized the notion of capacity by subjecting a system to all possible perturbations and examining its responses. The recent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e612" xlink:type="simple"/></inline-formula> measures information as a reduction in entropy from the maximum entropy distribution, which can be taken to correspond to the capacity of a system. However, because <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e613" xlink:type="simple"/></inline-formula> is specified by state transitions it is not a ‘pure’ measure of capacity; rather, it is a measure of capacity modulated by a system's dynamics. By measuring <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e614" xlink:type="simple"/></inline-formula> with reference to the stationary distribution, our measures depart from the notion of consciousness as a capacity. The stationary distribution characterizes the capacity of a system only to the extent that it is realized in the system's behaviour. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e615" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e616" xlink:type="simple"/></inline-formula> can therefore be construed as measures of a process modulated by capacity, aligning more closely with the Jamesian intuition.</p>
<p>The notion that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e617" xlink:type="simple"/></inline-formula> exists as a ‘fundamental property’ deserves comment. As described in the section ‘Normalization and instability’, our results challenge the ascription of physical meaning to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e618" xlink:type="simple"/></inline-formula>, in virtue of its exquisite sensitivity to the normalization process involved in specifying the MIB: this challenge pertains equally to the notion of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e619" xlink:type="simple"/></inline-formula> as a ‘fundamental quantity’. A further challenge to the ascription of physical meaning to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e620" xlink:type="simple"/></inline-formula> is the fact that it is not invariant under a change of coordinates, since this leads to a different set of sub-systems over which to minimize the effective information. An interesting question for future work is to examine whether, under certain conditions, the set of coordinates that maximizes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e621" xlink:type="simple"/></inline-formula> could be taken to define ‘natural’ coordinates, or macroscopic variables, for the system. In any case, it does not seem necessary to consider <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e622" xlink:type="simple"/></inline-formula> as a strict physical quantity in order to measure the integrated information corresponding to a system's state transitions or stationary dynamics, nor to relate these measurements to conscious level and content. In other words, one can depart from the IITC by interpreting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e623" xlink:type="simple"/></inline-formula> as accounting for particular aspects of consciousness without the further step of claiming identity <xref ref-type="bibr" rid="pcbi.1001052-Seth3">[9]</xref>.</p>
</sec><sec id="s3e">
<title>Integrated information in other neurocognitive processes</title>
<p>Although <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e624" xlink:type="simple"/></inline-formula> was originally developed in the context of a theory of consciousness, it is plausible that integrated information, and (more generally) conjoined functional integration and differentiation, play key roles in other cognitive and neural processes. Previous formulations (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e625" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e626" xlink:type="simple"/></inline-formula>) are poorly suited to investigating these roles, not only because of practical inapplicability, but also because they characterize integrated information in terms of capacity rather than process. Whereas consciousness under some theories may be considered as a capacity (see above), neurocognitive properties in general are best considered as processes. Having a measure of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e627" xlink:type="simple"/></inline-formula> that is framed in terms of process, and that is easy to apply in practice, therefore permits the framing of testable hypotheses, and the specification of synthetic models, aimed at examining the role of integrated information in neurocognitive processes broadly construed. For example, multi-modal binding and perceptual categorization <xref ref-type="bibr" rid="pcbi.1001052-Seth4">[20]</xref>, and action selection (decision making) <xref ref-type="bibr" rid="pcbi.1001052-Cisek1">[21]</xref> plausibly involve integrated information and could be profitably analyzed using our methods. Already, related measures of dynamical complexity (neural complexity and causal density, see below) have been correlated with the ability of simulated agents to deploy flexible behavior, suggesting a role for such dynamics in sensorimotor coordination in rich environments <xref ref-type="bibr" rid="pcbi.1001052-Seth2">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Seth8">[41]</xref>. Our results now allow integrated information to be applied in similar situations, facilitating comparative analyses.</p>
</sec><sec id="s3f">
<title>Comparison with causal density and neural complexity</title>
<p><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e628" xlink:type="simple"/></inline-formula> is one among a family of recent measures that aim to characterize, in different ways, the coexistence of integration and differentiation in a system's dynamics. Two alternative measures are ‘causal density’ <xref ref-type="bibr" rid="pcbi.1001052-Seth8">[41]</xref> and ‘neural complexity’ <xref ref-type="bibr" rid="pcbi.1001052-Tononi6">[42]</xref>. Here, we briefly summarize the similarities and differences among these measures, in order to set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e629" xlink:type="simple"/></inline-formula> into a broader context.</p>
<p>Causal density, like <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e630" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e631" xlink:type="simple"/></inline-formula> (but in contrast to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e632" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e633" xlink:type="simple"/></inline-formula>), is a measure of process rather than capacity. In virtue of being based on ‘Granger causality’, it also shares with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e634" xlink:type="simple"/></inline-formula> a sensitivity to causal interactions within a system. A key difference, however, is that causal density is based on <italic>all</italic> causal interactions, and not just those across a particular partition; thus causal density avoids the normalization problems described above (‘Normalization and instability’). Briefly, Granger causality is a statistical measure of causal influence which asserts that a variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e635" xlink:type="simple"/></inline-formula> ‘Granger causes’ another variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e636" xlink:type="simple"/></inline-formula> if information in the past of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e637" xlink:type="simple"/></inline-formula> helps predict the future of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e638" xlink:type="simple"/></inline-formula>, above and beyond information already in the past of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e639" xlink:type="simple"/></inline-formula> (and, optionally, in the past of a set of conditioning variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e640" xlink:type="simple"/></inline-formula>) <xref ref-type="bibr" rid="pcbi.1001052-Granger1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Geweke1">[43]</xref>. Causal density is then the (weighted) fraction of causal interactions among all elements that are statistically significant. High causal density indicates that elements within a system are both globally coordinated in their activity (to be useful for predicting each others' activity) and at the same time dynamically distinct (so that different elements contribute in different ways to these predictions). Granger causality (and causal density) is typically calculated using linear auto-regressive models, which brings about an interesting comparison with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e641" xlink:type="simple"/></inline-formula>. In a loose sense, integrated information, as measured by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e642" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e643" xlink:type="simple"/></inline-formula>, can be thought of as a variety of ‘causal density’, that quantifies the strength of the weakest bidirectional causal link between any two halves of the system. Forthcoming work will investigate further the links between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e644" xlink:type="simple"/></inline-formula> and causal density.</p>
<p>Neural complexity is calculated as the sum of the average mutual information across all bipartitions of a system <xref ref-type="bibr" rid="pcbi.1001052-Tononi6">[42]</xref>. Unlike <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e645" xlink:type="simple"/></inline-formula> and causal density, it does not reflect causal interactions within a system, however, like causal density, it is a measure of process rather than capacity. Neural complexity is maximal in a system that is globally integrated at the level of large subsystems, while exhibiting a high degree of segregation between smaller subsystems. (Note: The original papers describing neural complexity contained an error in calculating the covariance matrix from a generative model, which has been subsequently corrected in <xref ref-type="bibr" rid="pcbi.1001052-Barnett2">[44]</xref>. However, it appears that this error may still affect extant calculations of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e646" xlink:type="simple"/></inline-formula>.) A recent result <xref ref-type="bibr" rid="pcbi.1001052-Barnett1">[17]</xref> showing an equivalence between Granger causality and ‘transfer entropy’ (a time-directed version of mutual information) allows causal density to be related directly to neural complexity. Specifically, one can define a ‘bipartition causal density’ as a weighted average Granger causality (transfer entropy) across all bipartitions of a system (this definition also requires extension of Granger causality to multivariate variables) <xref ref-type="bibr" rid="pcbi.1001052-Barrett1">[18]</xref>. This measure furnishes a ‘time-directed’ version of neural complexity based on transfer entropy rather than mutual information.</p>
<p>These relations together suggest common foundations for measures of coexisting integration and differentiation. However, further work is needed to fully establish their theoretical interdependencies and their empirical convergences and divergences.</p>
</sec><sec id="s3g">
<title>Comparison with other measures</title>
<p>Characterizing complexity is a diverse field, and there are other measures that capture complex properties other than conjoined differentiation and integration. For example, ‘thermodynamic depth’ <xref ref-type="bibr" rid="pcbi.1001052-Pagels1">[45]</xref> can be interpreted as a measure of how hard it is to put a system together, and is based on the joint entropy of all past states, given the current state. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e647" xlink:type="simple"/></inline-formula> by contrast considers only one past state. An interesting further modification to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e648" xlink:type="simple"/></inline-formula> could involve information between the present and the whole past trajectory of the system. Another measure of statistical interdependence, ‘informational coherence’, considers the optimal predictive state for each time-series, and then measures mutual information between these <xref ref-type="bibr" rid="pcbi.1001052-Camperi1">[46]</xref>. In related work by Ay et al., the whole system is compared to the sum of individual elements <xref ref-type="bibr" rid="pcbi.1001052-Ay1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Wennekers1">[47]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Ay2">[48]</xref>, and the analysis goes beyond examination of conditional entropies to a more thorough mathematical treatment in terms of information geometry. While it is beyond the present scope to examine the formal correspondences among these measures, other related measures, and the measures described above, the growing interest in quantitative measures of complexity further emphasizes the need to formulate theoretically principled measures that are also simple to apply in practice.</p>
</sec><sec id="s3h">
<title>Limitations and extensions</title>
<p>Although our measures represent substantial improvements in practical applicability of measures of integrated information, several limitations remain. Most prominently, the normalization procedure leads to instabilities in the measurement process and undercuts ascription of physical meaning to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e649" xlink:type="simple"/></inline-formula>. Addressing this problem stands as a key theoretical challenge. We have only considered application of our measures to stationary dynamics. Future work may extend consideration to non-stationary (but still continuous and non-Markovian) processes, potentially capturing important non-stationary aspects of neural dynamics. In addition, our measures are applicable only to stochastic systems. While extension to closed deterministic systems may be of some value, most complex biological systems have stochastic components, especially when considered in interaction with a (stochastic) environment <xref ref-type="bibr" rid="pcbi.1001052-Rolls1">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-McDonnell1">[50]</xref>. Finally, our measures share with previous measures the computational challenge posed by the combinatorial explosion in partitions of a system as the number of elements increases. Possibly, imposing priors on the search for the minimum information partition may mitigate this challenge.</p>
<p>We have only considered a first-order, linear approximation for computing entropies/information from data. While this is useful for drawing comparison with Granger causality and causal density, there now exist more advanced approximation techniques that could be used in future work, for example additive regression <xref ref-type="bibr" rid="pcbi.1001052-Yao1">[51]</xref> or kernel regression <xref ref-type="bibr" rid="pcbi.1001052-Bosq1">[52]</xref>. Regarding estimation of entropy and mutual information without employing a regression model, we have only considered this via the intermediate step of density estimation. Again, future work could investigate the applicability of more advanced techniques <xref ref-type="bibr" rid="pcbi.1001052-Kraskov1">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1001052-Paninski1">[54]</xref> that avoid this step.</p>
<p>As well as addressing the above challenges, future work will (i) empirically examine integrated information for time-series data acquired from neuroimaging and other biological datasets, in order to test intuitions regarding consciousness and other neurocognitive processes; (ii) investigate in models how integrated information is modulated by input and output relations of a system embedded in, and interacting with, a surrounding environment, and (iii) determine theoretically the relations between integrated information and alternative measures of dynamical complexity and metastability.</p>
</sec></sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<p><xref ref-type="supplementary-material" rid="pcbi.1001052.s001">Text S1</xref> in ‘Supporting Information’ contains software enabling calculation of ΦE and ΦAR, as well as functions which allow regeneration of some of the simulations we describe.</p>
<sec id="s4a">
<title>Extension and computation of Φ<sub>DM</sub> for an MVAR(1) process</title>
<p>To extend <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e650" xlink:type="simple"/></inline-formula> to stationary continuous Markovian systems, we have to address the problem that there is no well-defined maximum entropy distribution for such systems. We do this by replacing the ‘maximum entropy distribution’ with the distribution for which the state of each element is independent of the states of all other elements, is Gaussian distributed, and has mean and variance equal to those of its corresponding stationary distribution. Thus, we take <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e651" xlink:type="simple"/></inline-formula>, where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e652" xlink:type="simple"/><label>(0.57)</label></disp-formula></p>
<p>Having defined a distribution for the initial state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e653" xlink:type="simple"/></inline-formula>, we explain how to compute the expected integrated information, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e654" xlink:type="simple"/></inline-formula>, for MVAR(1) processes (0.36). The computation proceeds analytically, given the generative model, which is specified by the connectivity matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e655" xlink:type="simple"/></inline-formula> and the covariance matrix of the noise, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e656" xlink:type="simple"/></inline-formula>. Alternatively, an estimate of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e657" xlink:type="simple"/></inline-formula> from time-series data can be obtained by using estimates of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e658" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e659" xlink:type="simple"/></inline-formula>. The linear-regression formulae (0.46) and (0.48) yield the estimates<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e660" xlink:type="simple"/><label>(0.58)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e661" xlink:type="simple"/><label>(0.59)</label></disp-formula>where the symbol <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e662" xlink:type="simple"/></inline-formula> denotes empirical quantities.</p>
<p>Given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e663" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e664" xlink:type="simple"/></inline-formula>, (or their estimates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e665" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e666" xlink:type="simple"/></inline-formula>), the covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e667" xlink:type="simple"/></inline-formula> can be obtained via the discrete-time Lyapunov equation (0.37),<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e668" xlink:type="simple"/><label>(0.60)</label></disp-formula>and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e669" xlink:type="simple"/></inline-formula> from Eq. (0.57).</p>
<p>To compute the conditional probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e670" xlink:type="simple"/></inline-formula> we first use the MVAR(1) dynamics (0.36) to obtain the distribution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e671" xlink:type="simple"/></inline-formula> as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e672" xlink:type="simple"/><label>(0.61)</label></disp-formula>Then we use Bayes' rule (0.15) to obtain<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e673" xlink:type="simple"/><label>(0.62)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e674" xlink:type="simple"/><label>(0.63)</label></disp-formula>From the term quadratic in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e675" xlink:type="simple"/></inline-formula> we can obtain the inverse of the covariance matrix of (the Gaussian distributed) conditional variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e676" xlink:type="simple"/></inline-formula> as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e677" xlink:type="simple"/><label>(0.64)</label></disp-formula>and hence express the conditional entropy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e678" xlink:type="simple"/></inline-formula> in terms of the connectivity and stationary covariance matrices:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e679" xlink:type="simple"/><label>(0.65)</label></disp-formula>For a given a sub-system <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e680" xlink:type="simple"/></inline-formula>, we have to consider the bipartition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e681" xlink:type="simple"/></inline-formula>, and the block decomposition of vectors and matrices according to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e682" xlink:type="simple"/></inline-formula> so that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e683" xlink:type="simple"/><label>(0.66)</label></disp-formula>and similarly for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e684" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e685" xlink:type="simple"/></inline-formula>. To obtain the distribution for the conditional random variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e686" xlink:type="simple"/></inline-formula>, we express <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e687" xlink:type="simple"/></inline-formula> in terms of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e688" xlink:type="simple"/></inline-formula> as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e689" xlink:type="simple"/><label>(0.67)</label></disp-formula>and note that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e690" xlink:type="simple"/></inline-formula>. Hence<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e691" xlink:type="simple"/><label>(0.68)</label></disp-formula>From Bayes' rule, we can then calculate the inverse of the covariance matrix of (the Gaussian distributed) conditional variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e692" xlink:type="simple"/></inline-formula> as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e693" xlink:type="simple"/><label>(0.69)</label></disp-formula>and hence<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e694" xlink:type="simple"/><label>(0.70)</label></disp-formula>The entropy formulae (0.65) and (0.70) furnish the sufficient quantities for computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e695" xlink:type="simple"/></inline-formula> as described in the section ‘The previous measure, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e696" xlink:type="simple"/></inline-formula>’, using the expression (0.25) for the expected effective information. For present purposes, as with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e697" xlink:type="simple"/></inline-formula>, we restrict attention to bipartitions only.</p>
</sec><sec id="s4b">
<title>Analytical computation of Φ<sub>E</sub> for a general Gaussian case</title>
<p>Here we show how to compute <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e698" xlink:type="simple"/></inline-formula> analytically, for a general stationary Gaussian system, for any timescale <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e699" xlink:type="simple"/></inline-formula>. Importantly, the generative model for such a system <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e700" xlink:type="simple"/></inline-formula> is always equivalent to an <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e701" xlink:type="simple"/></inline-formula> process <xref ref-type="bibr" rid="pcbi.1001052-Barrett1">[18]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e702" xlink:type="simple"/><label>(0.71)</label></disp-formula>where the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e703" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e704" xlink:type="simple"/></inline-formula>, can be thought of as generalized connectivity matrices acting at different time-lags, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e705" xlink:type="simple"/></inline-formula> is a stationary multivariate Gaussian ‘white noise’ source with zero mean and vanishing auto-covariance function, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e706" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e707" xlink:type="simple"/></inline-formula>. (We ignore the case <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e708" xlink:type="simple"/></inline-formula> corresponding to an MA(1), i.e. moving average, process.) This system is stationary if and only if the roots of the equation<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e709" xlink:type="simple"/><label>(0.72)</label></disp-formula>lie outside the unit circle <xref ref-type="bibr" rid="pcbi.1001052-Hamilton1">[33]</xref>.</p>
<p>The method outlined in ‘Computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e710" xlink:type="simple"/></inline-formula> analytically for a Gaussian system’ for computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e711" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e712" xlink:type="simple"/></inline-formula> for an MVAR(1) process is easy to extend to the more general <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e713" xlink:type="simple"/></inline-formula>, any <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e714" xlink:type="simple"/></inline-formula>, case given by equation (0.71). Suppose we wish to compute <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e715" xlink:type="simple"/></inline-formula> for any value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e716" xlink:type="simple"/></inline-formula> up to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e717" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e718" xlink:type="simple"/></inline-formula>. We first use the fact <xref ref-type="bibr" rid="pcbi.1001052-Hamilton1">[33]</xref> that the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e719" xlink:type="simple"/></inline-formula> process is equivalent to the MVAR(1) process<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e720" xlink:type="simple"/><label>(0.73)</label></disp-formula>involving the block quantities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e721" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e722" xlink:type="simple"/></inline-formula> and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e723" xlink:type="simple"/><label>(0.74)</label></disp-formula>The stationary covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e724" xlink:type="simple"/></inline-formula> for this process can be obtained from the Lyapunov equation, by analogy with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e725" xlink:type="simple"/></inline-formula> for the MVAR(1) case (0.37):<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e726" xlink:type="simple"/><label>(0.75)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e727" xlink:type="simple"/></inline-formula>. Then the stationary covariance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e728" xlink:type="simple"/></inline-formula> and auto-covariance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e729" xlink:type="simple"/></inline-formula> are obtained respectively as the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e730" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e731" xlink:type="simple"/></inline-formula> component blocks of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e732" xlink:type="simple"/></inline-formula>. We can then proceed as for the MVAR(1), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e733" xlink:type="simple"/></inline-formula> case:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e734" xlink:type="simple"/><label>(0.76)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e735" xlink:type="simple"/><label>(0.77)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e736" xlink:type="simple"/><label>(0.78)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e737" xlink:type="simple"/><label>(0.79)</label></disp-formula>The above expressions furnish the quantities needed to compute <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001052.e738" xlink:type="simple"/></inline-formula> from equations (0.29), (0.30), (0.33) and (0.34).</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1001052.s001" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001052.s001" xlink:type="simple"><label>Text S1</label><caption>
<p>Toolbox for computing integrated information as Φ<sub>E</sub> or Φ<sub>AR</sub>. ‘phiemvarp.m’ computes Φ<sub>E</sub> from an MVAR(<italic>p</italic>) generative model. ‘ARphidata.m’ computes Φ<sub>AR</sub> ( = Φ<sub>E</sub> if Gaussian), from stationary time-series data. ‘statdata.m’ creates time-series data from an MVAR(<italic>p</italic>) generative model. ‘A2b.mat’ contains the connectivity matrix for the optimal network, <xref ref-type="fig" rid="pcbi-1001052-g002">Fig. 2(b)</xref>. ‘timereverse.m’ is an m-file for time-reversing the data (required to run ARphidata.m).</p>
<p>(0.01 MB ZIP)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We are grateful to Lionel Barnett and Chang-Sub Kim for useful discussions. Nihat Ay, David Balduzzi and one anonymous reviewer provided extremely detailed and valuable comments on a first draft of this paper.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1001052-Honey1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Honey</surname><given-names>CJ</given-names></name>
<name name-style="western"><surname>Kötter</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Breakspear</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name>
</person-group>             <year>2007</year>             <article-title>Network structure of cerebral cortex shapes functional connectivity on multiple time scales.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>104</volume>             <fpage>10240</fpage>             <lpage>10245</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Sporns1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Chialvo</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Kaiser</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Hilgetag</surname><given-names>C</given-names></name>
</person-group>             <year>2004</year>             <article-title>Organization, development and function of complex brain networks.</article-title>             <source>Trends Cogn Sci</source>             <volume>8</volume>             <fpage>418</fpage>             <lpage>425</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Bressler1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bressler</surname><given-names>SL</given-names></name>
<name name-style="western"><surname>Tognoli</surname><given-names>E</given-names></name>
</person-group>             <year>2006</year>             <article-title>Operational principles of neurocognitive networks.</article-title>             <source>Int J Psychophysiol</source>             <volume>60</volume>             <fpage>139</fpage>             <lpage>148</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Edelman1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Edelman</surname><given-names>GM</given-names></name>
</person-group>             <year>2003</year>             <article-title>Naturalizing consciousness: A theoretical framework.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>100</volume>             <fpage>5520</fpage>             <lpage>5524</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Seth1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name>
<name name-style="western"><surname>Izhikevich</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Reeke</surname><given-names>GN</given-names></name>
<name name-style="western"><surname>Edelman</surname><given-names>GM</given-names></name>
</person-group>             <year>2006</year>             <article-title>Theories and measures of consciousness: An extended framework.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>103</volume>             <fpage>10799</fpage>             <lpage>10804</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Seth2"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name>
<name name-style="western"><surname>Edelman</surname><given-names>GM</given-names></name>
</person-group>             <year>2004</year>             <article-title>Environment and behavior influence the complexity of evolved neural networks.</article-title>             <source>Adapt Behav</source>             <volume>12</volume>             <fpage>5</fpage>             <lpage>21</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Lungarella1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lungarella</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name>
</person-group>             <year>2006</year>             <article-title>Mapping information flow in sensorimotor networks.</article-title>             <source>PLoS Comput Biol</source>             <volume>2</volume>             <fpage>e144</fpage>          </element-citation></ref>
<ref id="pcbi.1001052-Tononi1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Edelman</surname><given-names>GM</given-names></name>
</person-group>             <year>1998</year>             <article-title>Consciousness and complexity.</article-title>             <source>Science</source>             <volume>282</volume>             <fpage>1846</fpage>             <lpage>1851</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Seth3"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name>
</person-group>             <year>2009</year>             <article-title>Explanatory correlates of consciousness: Theoretical and computational challenges.</article-title>             <source>Cogn Comput</source>             <volume>1</volume>             <fpage>50</fpage>             <lpage>63</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Tononi2"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name>
</person-group>             <year>2003</year>             <article-title>Measuring information integration.</article-title>             <source>BMC Neurosci</source>             <volume>4</volume>             <fpage>31</fpage>          </element-citation></ref>
<ref id="pcbi.1001052-Balduzzi1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Balduzzi</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name>
</person-group>             <year>2008</year>             <article-title>Integrated information in discrete dynamical systems: Motivation and theoretical framework.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <issue>6</issue>             <fpage>e1000091</fpage>          </element-citation></ref>
<ref id="pcbi.1001052-Tononi3"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name>
</person-group>             <year>2004</year>             <article-title>An information integration theory of consciousness.</article-title>             <source>BMC Neurosci</source>             <volume>5</volume>             <fpage>42</fpage>          </element-citation></ref>
<ref id="pcbi.1001052-Balduzzi2"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Balduzzi</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name>
</person-group>             <year>2009</year>             <article-title>Qualia: The geometry of integrated information.</article-title>             <source>PLoS Comput Biol</source>             <volume>5</volume>             <issue>8</issue>             <fpage>e1000462</fpage>          </element-citation></ref>
<ref id="pcbi.1001052-Tononi4"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name>
</person-group>             <year>2008</year>             <article-title>Consciousness as integrated information: A provisional manifesto.</article-title>             <source>Biol Bull</source>             <volume>215</volume>             <issue>3</issue>             <fpage>216</fpage>             <lpage>242</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Tononi5"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name>
</person-group>             <year>2005</year>             <article-title>Consciousness, information integration, and the brain.</article-title>             <source>Prog Brain Res</source>             <volume>150</volume>             <fpage>109</fpage>             <lpage>126</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Cover1"><label>16</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cover</surname><given-names>TM</given-names></name>
<name name-style="western"><surname>Thomas</surname><given-names>JA</given-names></name>
</person-group>             <year>1991</year>             <article-title>Elements of information theory.</article-title>             <publisher-loc>New York</publisher-loc>             <publisher-name>Wiley-Interscience</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">776</size>           </element-citation></ref>
<ref id="pcbi.1001052-Barnett1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Barnett</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Barrett</surname><given-names>AB</given-names></name>
<name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name>
</person-group>             <year>2009</year>             <article-title>Granger causality and transfer entropy are equivalent for Gaussian variables.</article-title>             <source>Phys Rev Lett</source>             <volume>103</volume>             <fpage>238701</fpage>          </element-citation></ref>
<ref id="pcbi.1001052-Barrett1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Barrett</surname><given-names>AB</given-names></name>
<name name-style="western"><surname>Barnett</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name>
</person-group>             <year>2010</year>             <article-title>Multivariate Granger causality and generalized variance.</article-title>             <source>Phys Rev E</source>             <volume>81</volume>             <fpage>041907</fpage>          </element-citation></ref>
<ref id="pcbi.1001052-James1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>James</surname><given-names>W</given-names></name>
</person-group>             <year>1904</year>             <article-title>Does consciousness exist?</article-title>             <source>J Philos Psychol Sci Meth</source>             <volume>1</volume>             <fpage>477</fpage>             <lpage>491</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Seth4"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name>
<name name-style="western"><surname>McKinstry</surname><given-names>JL</given-names></name>
<name name-style="western"><surname>Edelman</surname><given-names>GM</given-names></name>
<name name-style="western"><surname>Krichmar</surname><given-names>JL</given-names></name>
</person-group>             <year>2004</year>             <article-title>Visual binding through reentrant connectivity and dynamic synchronization in a brain-based device.</article-title>             <source>Cereb Cortex</source>             <volume>14</volume>             <fpage>1185</fpage>             <lpage>1199</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Cisek1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cisek</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Kalaska</surname><given-names>JF</given-names></name>
</person-group>             <year>2010</year>             <article-title>Neural mechanisms for interacting with a world full of action choices.</article-title>             <source>Annu Rev Neurosci</source>             <volume>33</volume>             <fpage>269</fpage>             <lpage>298</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Ay1"><label>22</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ay</surname><given-names>N</given-names></name>
</person-group>             <year>2001</year>             <article-title>Information geometry on complexity and stochastic interaction.</article-title>             <comment>MPI MIS Preprint 95. Available: <ext-link ext-link-type="uri" xlink:href="http://www.mis.mpg.de/publications/preprints/2001/prepr2001-95.html" xlink:type="simple">http://www.mis.mpg.de/publications/preprints/2001/prepr2001-95.html</ext-link>.</comment>          </element-citation></ref>
<ref id="pcbi.1001052-Watts1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Watts</surname><given-names>DJ</given-names></name>
<name name-style="western"><surname>Strogatz</surname><given-names>SH</given-names></name>
</person-group>             <year>1998</year>             <article-title>Collective dynamics of ‘small world’ networks.</article-title>             <source>Nature</source>             <volume>393</volume>             <fpage>440</fpage>             <lpage>442</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Watts2"><label>24</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Watts</surname><given-names>DJ</given-names></name>
</person-group>             <year>2004</year>             <source>Small worlds: The dynamics of networks between order and randomness (Princeton studies in complexity)</source>             <publisher-loc>Princeton</publisher-loc>             <publisher-name>Princeton University Press</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">264</size>           </element-citation></ref>
<ref id="pcbi.1001052-Shanahan1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Shanahan</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Dynamical complexity in small-world networks of spiking neurons.</article-title>             <source>Phys Rev E</source>             <volume>78</volume>             <fpage>041924</fpage>          </element-citation></ref>
<ref id="pcbi.1001052-Mitchell1"><label>26</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mitchell</surname><given-names>M</given-names></name>
</person-group>             <year>1997</year>             <article-title>An introduction to genetic algorithms.</article-title>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">221</size>           </element-citation></ref>
<ref id="pcbi.1001052-Wilks1"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wilks</surname><given-names>SS</given-names></name>
</person-group>             <year>1932</year>             <article-title>Certain generalizations in the analysis of variance.</article-title>             <source>Biometrika</source>             <volume>24</volume>             <fpage>471</fpage>             <lpage>494</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Davidson1"><label>28</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Davidson</surname><given-names>J</given-names></name>
</person-group>             <year>2000</year>             <article-title>Econometric theory.</article-title>             <publisher-loc>Oxford</publisher-loc>             <publisher-name>Wiley-Blackwell</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">528</size>           </element-citation></ref>
<ref id="pcbi.1001052-Ding1"><label>29</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ding</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Chen</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Bressler</surname><given-names>S</given-names></name>
</person-group>             <year>2006</year>             <article-title>Granger causality: Basic theory and application to neuroscience.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Schelter</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Winterhalder</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Timmer</surname><given-names>J</given-names></name>
</person-group>             <source>Handbook of time series analysis</source>             <publisher-loc>Wienheim</publisher-loc>             <publisher-name>Wiley</publisher-name>             <fpage>438</fpage>             <lpage>460</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Granger1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Granger</surname><given-names>CWJ</given-names></name>
</person-group>             <year>1969</year>             <article-title>Investigating causal relations by econometric models and cross-spectral methods.</article-title>             <source>Econometrica</source>             <volume>37</volume>             <fpage>424</fpage>             <lpage>438</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Dayan1"><label>31</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name>
</person-group>             <year>2001</year>             <article-title>Theoretical neuroscience: Computational and mathematical modeling of neural systems.</article-title>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>The MIT Press</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">576</size>           </element-citation></ref>
<ref id="pcbi.1001052-Buzsaki1"><label>32</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Buzsaki</surname><given-names>G</given-names></name>
</person-group>             <year>2006</year>             <article-title>Rhythms of the brain.</article-title>             <publisher-loc>Oxford</publisher-loc>             <publisher-name>Oxford University Press</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">464</size>           </element-citation></ref>
<ref id="pcbi.1001052-Hamilton1"><label>33</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hamilton</surname><given-names>JD</given-names></name>
</person-group>             <year>1994</year>             <article-title>Time series analysis.</article-title>             <publisher-loc>Princeton</publisher-loc>             <publisher-name>Princeton University Press</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">820</size>           </element-citation></ref>
<ref id="pcbi.1001052-Seth5"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name>
</person-group>             <year>2010</year>             <article-title>A MATLAB toolbox for Granger causal connectivity analysis.</article-title>             <source>J Neurosci Meth</source>             <volume>186</volume>             <fpage>262</fpage>             <lpage>273</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Werner1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Werner</surname><given-names>G</given-names></name>
</person-group>             <year>2007</year>             <article-title>Metastability, criticality and phase transitions in brain and its models.</article-title>             <source>Biosystems</source>             <volume>90</volume>             <fpage>496</fpage>             <lpage>508</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Freeman1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Freeman</surname><given-names>WJ</given-names></name>
<name name-style="western"><surname>Skarda</surname><given-names>CA</given-names></name>
</person-group>             <year>1985</year>             <article-title>Spatial EEG patterns, non-linear dynamics and perception: The neo-Sherringtonian view.</article-title>             <source>Brain Res</source>             <volume>357</volume>             <fpage>147</fpage>             <lpage>175</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Bressler2"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bressler</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Kelso</surname><given-names>J</given-names></name>
</person-group>             <year>2001</year>             <article-title>Cortical coordination dynamics and cognition.</article-title>             <source>Trends Cogn Sci</source>             <volume>5</volume>             <fpage>26</fpage>             <lpage>36</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Seth6"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name>
<name name-style="western"><surname>Baars</surname><given-names>BJ</given-names></name>
<name name-style="western"><surname>Edelman</surname><given-names>DB</given-names></name>
</person-group>             <year>2005</year>             <article-title>Criteria for consciousness in humans and other mammals.</article-title>             <source>Conscious Cogn</source>             <volume>14</volume>             <fpage>119</fpage>             <lpage>139</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Arthuis1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Arthuis</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Valton</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Régis</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Chauvel</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Wendling</surname><given-names>F</given-names></name>
<etal/></person-group>             <year>2009</year>             <article-title>Impaired consciousness during temporal lobe seizures is related to increased long-distance cortical-subcortical synchronization.</article-title>             <source>Brain</source>             <volume>132</volume>             <fpage>2091</fpage>             <lpage>2101</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Seth7"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name>
<name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name>
<name name-style="western"><surname>Cleeremans</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Overgaard</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Pessoa</surname><given-names>L</given-names></name>
</person-group>             <year>2008</year>             <article-title>Measuring consciousness: Relating behavioural and neurophysiological approaches.</article-title>             <source>Trends Cogn Sci</source>             <volume>12</volume>             <fpage>314</fpage>             <lpage>321</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Seth8"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name>
</person-group>             <year>2005</year>             <article-title>Causal connectivity of evolved neural networks during behavior.</article-title>             <source>Network</source>             <volume>16</volume>             <fpage>35</fpage>             <lpage>54</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Tononi6"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Edelman</surname><given-names>GM</given-names></name>
</person-group>             <year>1994</year>             <article-title>A measure for brain complexity: Relating functional segregation and integration in the nervous system.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>91</volume>             <fpage>5033</fpage>             <lpage>5037</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Geweke1"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Geweke</surname><given-names>J</given-names></name>
</person-group>             <year>1982</year>             <article-title>Measurement of linear dependence and feedback between multiple time series.</article-title>             <source>J Am Stat Assoc</source>             <volume>77</volume>             <fpage>304</fpage>             <lpage>313</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Barnett2"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Barnett</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Buckley</surname><given-names>CL</given-names></name>
<name name-style="western"><surname>Bullock</surname><given-names>S</given-names></name>
</person-group>             <year>2009</year>             <article-title>Neural complexity and structural connectivity.</article-title>             <source>Phys Rev E</source>             <volume>79</volume>             <fpage>051914</fpage>          </element-citation></ref>
<ref id="pcbi.1001052-Pagels1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pagels</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Lloyd</surname><given-names>S</given-names></name>
</person-group>             <year>1988</year>             <article-title>Complexity as thermodynamic depth.</article-title>             <source>Ann Phys</source>             <volume>188</volume>             <fpage>186</fpage>             <lpage>213</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Camperi1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Camperi</surname><given-names>MF</given-names></name>
<name name-style="western"><surname>Klinkner</surname><given-names>KL</given-names></name>
<name name-style="western"><surname>Shalizi</surname><given-names>CR</given-names></name>
</person-group>             <year>2005</year>             <article-title>Measuring shared information and coordinated activity in neuronal networks.</article-title>             <source>Adv Neural In</source>             <volume>18</volume>             <fpage>667</fpage>             <lpage>674</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Wennekers1"><label>47</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wennekers</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Ay</surname><given-names>N</given-names></name>
</person-group>             <year>2001</year>             <article-title>Dynamical properties of strongly interacting markov chains.</article-title>             <source>Neural Netw</source>             <volume>16</volume>             <fpage>1483</fpage>             <lpage>1497</lpage>          </element-citation></ref>
<ref id="pcbi.1001052-Ay2"><label>48</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ay</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Olbrich</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Bertschinger</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Jost</surname><given-names>J</given-names></name>
</person-group>             <year>2001</year>             <article-title>A unifying framework for complexity measures of finite systems.</article-title>             <comment>Proceedings of ECCS'06, Santa Fe Institute Working Paper 06-08-028 Available: <ext-link ext-link-type="uri" xlink:href="http://www.santafe.edu/media/workingpapers/06-08-028.pdf" xlink:type="simple">http://www.santafe.edu/media/workingpapers/06-08-028.pdf</ext-link>.</comment>          </element-citation></ref>
<ref id="pcbi.1001052-Rolls1"><label>49</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name>
<name name-style="western"><surname>Deco</surname><given-names>G</given-names></name>
</person-group>             <year>2010</year>             <article-title>The noisy brain: Stochastic dynamics as a principle of brain function.</article-title>             <publisher-loc>Oxford</publisher-loc>             <publisher-name>Oxford University Press</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">304</size>           </element-citation></ref>
<ref id="pcbi.1001052-McDonnell1"><label>50</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>McDonnell</surname><given-names>MD</given-names></name>
<name name-style="western"><surname>Abbott</surname><given-names>D</given-names></name>
</person-group>             <year>2009</year>             <article-title>What is stochastic resonance? Definitions, misconceptions, debates, and its relevance to biology.</article-title>             <source>PLoS Comput Biol</source>             <volume>5</volume>             <fpage>e1000348</fpage>          </element-citation></ref>
<ref id="pcbi.1001052-Yao1"><label>51</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Yao</surname><given-names>Q</given-names></name>
<name name-style="western"><surname>Fan</surname><given-names>J</given-names></name>
</person-group>             <year>2003</year>             <article-title>Nonlinear time series: Nonparametric and parametric methods.</article-title>             <publisher-loc>Berlin</publisher-loc>             <publisher-name>Springer-Verlag</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">552</size>           </element-citation></ref>
<ref id="pcbi.1001052-Bosq1"><label>52</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bosq</surname><given-names>D</given-names></name>
</person-group>             <year>1998</year>             <article-title>Nonparametric statistics for stochastic processes: Estimation and prediction, 2nd edn.</article-title>             <publisher-loc>Berlin</publisher-loc>             <publisher-name>Springer-Verlag</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">232</size>           </element-citation></ref>
<ref id="pcbi.1001052-Kraskov1"><label>53</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kraskov</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Stoegbauer</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Grassberger</surname><given-names>P</given-names></name>
</person-group>             <year>2004</year>             <article-title>Estimating mutual information.</article-title>             <source>Phys Rev E</source>             <volume>69</volume>             <fpage>066138</fpage>          </element-citation></ref>
<ref id="pcbi.1001052-Paninski1"><label>54</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name>
</person-group>             <year>2003</year>             <article-title>Estimation of entropy and mutual information.</article-title>             <source>Neural Comput</source>             <volume>15</volume>             <fpage>1191</fpage>             <lpage>1254</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>