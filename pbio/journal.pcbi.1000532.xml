<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">09-PLCB-RA-0224R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000532</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Neuroscience/Theoretical Neuroscience</subject><subject>Neuroscience/Natural and Synthetic Vision</subject></subj-group></article-categories><title-group><article-title>Towards a Mathematical Theory of Cortical Micro-circuits</article-title><alt-title alt-title-type="running-head">A Mathematical Theory of Cortical Micro-circuits</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>George</surname><given-names>Dileep</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Hawkins</surname><given-names>Jeff</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
</contrib-group><aff id="aff1">          <addr-line>Numenta Inc., Redwood City, California, United States of America</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">dgeorge@numenta.com</email></corresp>
<fn fn-type="con"><p>Wrote the paper: DG JH. Formulated the mathematical model: DG. Established mapping to biology: DG JH. Conceived and performed the experiments: DG.</p></fn>
<fn fn-type="conflict"><p>Jeff Hawkins and Dileep George are cofounders of Numenta and own shares of Numenta. Numenta is a for-profit startup company in Redwood City, CA.</p></fn></author-notes><pub-date pub-type="collection"><month>10</month><year>2009</year></pub-date><pub-date pub-type="epub"><day>9</day><month>10</month><year>2009</year></pub-date><volume>5</volume><issue>10</issue><elocation-id>e1000532</elocation-id><history>
<date date-type="received"><day>3</day><month>3</month><year>2009</year></date>
<date date-type="accepted"><day>11</day><month>9</month><year>2009</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2009</copyright-year><copyright-holder>George, Hawkins</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>The theoretical setting of hierarchical Bayesian inference is gaining acceptance as a framework for understanding cortical computation. In this paper, we describe how Bayesian belief propagation in a spatio-temporal hierarchical model, called Hierarchical Temporal Memory (HTM), can lead to a mathematical model for cortical circuits. An HTM node is abstracted using a coincidence detector and a mixture of Markov chains. Bayesian belief propagation equations for such an HTM node define a set of functional constraints for a neuronal implementation. Anatomical data provide a contrasting set of organizational constraints. The combination of these two constraints suggests a theoretically derived interpretation for many anatomical and physiological features and predicts several others. We describe the pattern recognition capabilities of HTM networks and demonstrate the application of the derived circuits for modeling the subjective contour effect. We also discuss how the theory and the circuit can be extended to explain cortical features that are not explained by the current model and describe testable predictions that can be derived from the model.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>Understanding the computational and information processing roles of cortical circuitry is one of the outstanding problems in neuroscience. In this paper, we work from a theory of neocortex that models it as a spatio-temporal hierarchical system to derive a biological cortical circuit. This is achieved by combining the computational constraints provided by the inference equations for this spatio-temporal hierarchy with anatomical data. The result is a mathematically consistent biological circuit that can be mapped to the cortical laminae and matches many prominent features of the mammalian neocortex. The mathematical model can serve as a starting point for the construction of machines that work like the brain. The resultant biological circuit can be used for modeling physiological phenomena and for deriving testable predictions about the brain.</p>
</abstract><funding-group><funding-statement>This work was supported by Numenta Inc. Jeff Hawkins, one of the investors of Numenta, is a contributing author on this paper and was involved in the study, decision to publish and preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="26"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Understanding the computational and information processing roles of cortical circuitry is one of the outstanding problems in neuroscience. The circuits of the neocortex are bewildering in their complexity and anatomical detail. Although enormous progress has been made in the collection and assimilation of data about the physiological properties and connectivity of cortical neurons, the data are not sufficient to derive a computational theory in a purely bottom-up fashion.</p>
<p>The theoretical setting of hierarchical Bayesian inference is gaining acceptance as the framework for understanding cortical computation <xref ref-type="bibr" rid="pcbi.1000532-Lee1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1000532-Friston1">[5]</xref>. Tai Sing Lee and David Mumford <xref ref-type="bibr" rid="pcbi.1000532-Lee1">[1]</xref> suggested that algorithms for Bayesian belief propagation might model the interactive feed-forward and feedback cortical computations. Concurrently, Karl Friston <xref ref-type="bibr" rid="pcbi.1000532-Friston1">[5]</xref> reviewed the structure of the anatomical organization of the neocortex and suggested its strong correspondence to hierarchical Bayesian generative models. Friston recently expanded on this to suggest an inversion method for hierarchical Bayesian dynamic models and to point out that the brain, in principle, has the infrastructure needed to invert hierarchical dynamic models <xref ref-type="bibr" rid="pcbi.1000532-Friston2">[6]</xref>. However, there still remains a gap between our understanding of learning and inference in hierarchical Bayesian models and our understanding of how it is implemented in cortical circuits. In a recent review, Hegde and Felleman pointed out that the “Bayesian framework is not yet a neural model. [The Bayesian] framework currently helps explain the computations that underlie various brain functions, but not how the brain implements these computations” <xref ref-type="bibr" rid="pcbi.1000532-Hegde1">[2]</xref>. This paper is an attempt to fill this gap by deriving a computational model for cortical circuits based on the mathematics of Bayesian belief propagation in the context of a particular Bayesian framework called Hierarchical Temporal Memory (HTM).</p>
<p>Belief propagation techniques can be applied to many different types of networks. The networks can vary significantly in their topology, in how they learn (supervised, unsupervised, or non-learning), and in how they incorporate or do not incorporate time. Therefore, to map the mathematics of Bayesian belief propagation onto cortical architecture and microcircuits we must start with a particular Bayesian framework that specifies these variables. The starting point for the work presented in this paper is a model called the Memory-Prediction Framework, first described by one of this paper's authors, Hawkins, in a book titled “On Intelligence” <xref ref-type="bibr" rid="pcbi.1000532-Hawkins1">[7]</xref>. The Memory-Prediction Framework proposed that the neocortex uses memory of sequences in a hierarchy to model and infer causes in the world. The Memory-Prediction Framework proposed several novel learning mechanisms and included a detailed mapping onto large scale cortical-thalamic architecture as well as onto the microcircuits of cortical columns. However, the Memory-Prediction Framework was not described in Bayesian terms and was presented without the rigor of a mathematical formulation.</p>
<p>This paper's other author, George, recognized that the Memory-Prediction framework could be formulated in Bayesian terms and given a proper mathematical foundation <xref ref-type="bibr" rid="pcbi.1000532-George1">[8]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-George2">[9]</xref>. We call this formulation Hierarchical Temporal Memory (HTM) and it is currently being applied to problems of machine learning and inference. The final step in this theory is to map the mathematics of HTM directly to cortical-thalamic anatomy and the microcircuits of cortical columns. That is the goal of this paper. We will work back from the formal expression of HTM and derive cortical microcircuits by matching the computational specifications of the theory with known biological data. The resultant biological circuit supports all the Bayesian computations required for temporal, feed-forward, and feedback inference. The elements of the circuits are also consistent with each other in that they operate under the same set of assumptions and work together in a hierarchy.</p>
<p>Several researchers have proposed detailed models for cortical circuits <xref ref-type="bibr" rid="pcbi.1000532-Grossberg1">[10]</xref>–<xref ref-type="bibr" rid="pcbi.1000532-Martin1">[12]</xref>. Some of these models exhibit interesting pattern recognition properties and some have been used in the explanation of physiological phenomena. However, these models do not incorporate the concepts of Bayesian inference in a hierarchical temporal model. Other researchers <xref ref-type="bibr" rid="pcbi.1000532-Deneve1">[4]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-Rao2">[13]</xref> have proposed detailed mechanisms by which Bayesian belief propagation techniques can be implemented in neurons. Their work suggests that, at a neuron level, machinery exists for implementing the types of computations required for belief propagation. However, they did not attempt to map these implementations to detailed cortical anatomy. To our knowledge, the work in this paper is the first attempt to map the theory of Bayesian belief propagation and hierarchical and temporal inference onto cortical circuitry. (Partial details of this work have been published earlier <xref ref-type="bibr" rid="pcbi.1000532-George2">[9]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-George3">[14]</xref>.)</p>
<p>Deciphering the functional connectivity of the cortical circuits is a formidable task and is associated with the perils involved in the reverse engineering of a complex system. The circuits derived in this chapter can provide a hypothesis-driven framework for examining the neural connectivity. As with any theory, it is expected that the particular instantiation described here will need to be revised as more data is obtained and more aspects of cortical computations, like attention, timing, and motor action, are incorporated. The circuit derived here could act as a basis for such explorations. In addition to providing a template for understanding cortical circuits <xref ref-type="bibr" rid="pcbi.1000532-Douglas1">[15]</xref>, the theory presented here can be useful in the modeling of physiological phenomena. As an example, we simulate the subjective contour effect using feedback from a high-level belief using the derived circuits. Having a complete biological mapping of a computational theory can also help in the design of hypothesis-driven biological experiments.</p>
<p>The rest of this paper is organized in such a manner that the computational parts are clearly separated from the biological aspects. The <xref ref-type="sec" rid="s2">Model</xref> section deals exclusively with the computational aspects of HTMs. In this section, we briefly describe the HTM theory and take a detailed look at the inference mechanism in HTM nodes. The Bayesian belief propagation equations for the computations in an HTM node are described. We then describe an abstract circuit implementation of these equations using neuron-like elements. The <xref ref-type="sec" rid="s3">Results</xref> section of the paper, which deals primarily with the biological implementation, maps this abstract neural implementation to the laminar biological cortical circuitry by matching the computational specifications with anatomical data. This section also provides example applications of this circuit in the modeling of physiological phenomena. In the <xref ref-type="sec" rid="s4">Discussion</xref> section we discuss variations, omissions, and extensions of the proposed circuits.</p>
</sec><sec id="s2">
<title>Model</title>
<sec id="s2a">
<title>Hierarchical Temporal Memory</title>
<p>Hierarchical Temporal Memory is a theory of the neocortex that postulates that the neocortex builds a model of the world using a spatio-temporal hierarchy. According to this theory, the operation of the neocortex can be approximated by replicating a basic computational unit – called a node – in a tree structured hierarchy. Each node in the hierarchy uses the same learning and inference algorithm, which entails storing spatial patterns and then sequences of those spatial patterns. The feed-forward output of a node is represented in terms of the sequences that it has stored. The spatial patterns stored in a higher-level node record co-occurrences of sequences from its child nodes. The HTM hierarchy is organized in such a way that higher levels of the hierarchy represent larger amounts of space and longer durations of time. The states at the higher levels of the hierarchy vary at a slower rate compared to the lower levels. It is speculated that this kind of organization leads to efficient learning and generalization because it mirrors the spatio-temporal organization of causes in the world.</p>
<p>In our research, HTMs have been used successfully in invariant pattern recognition on gray-scale images, in the identification of speakers in the auditory domain and in learning a model for motion capture data in an unsupervised manner. Other researchers have reported success in using HTMs in content-based image retrieval <xref ref-type="bibr" rid="pcbi.1000532-Bobier1">[16]</xref>, object categorization <xref ref-type="bibr" rid="pcbi.1000532-Vutsinas1">[17]</xref>, and power system security analysis <xref ref-type="bibr" rid="pcbi.1000532-Sun1">[18]</xref>. Another set of researchers has explored hardware implementations and parallel architectures for HTM algorithms <xref ref-type="bibr" rid="pcbi.1000532-Csapo1">[19]</xref>.</p>
<p>HTMs can be specified mathematically using a generative model. A simplified two-level generative model is shown in <xref ref-type="fig" rid="pcbi-1000532-g001">Figure 1</xref>. Each node in the hierarchy contains a set of <italic>coincidence patterns</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e001" xlink:type="simple"/></inline-formula> and a set of Markov chains <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e002" xlink:type="simple"/></inline-formula> where each Markov chain is defined over a subset of the set coincidence patterns in that node. A coincidence pattern in a node represents a co-activation of the Markov chains of its child nodes. A coincidence pattern that is generated by sampling a Markov chain in a higher level node concurrently activates its constituent Markov chains in the lower level nodes. For a particular coincidence pattern and Markov chain that is ‘active’ at a higher-level node, sequences of coincidence patterns are generated concurrently by sampling from the activated Markov chains of the child nodes.</p>
<fig id="pcbi-1000532-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g001</object-id><label>Figure 1</label><caption>
<title>Generative model for HTM.</title>
<p>Hierarchical Temporal Memory (HTM) is a model of neocortical function. HTMs can be specified using a generative model. Shown is a simple two-level three-node HTM-type generative model. Each node in the hierarchy contains a set of coincidence patterns (labeled with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e003" xlink:type="simple"/></inline-formula>) and a set of Markov chains (labeled with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e004" xlink:type="simple"/></inline-formula>) defined over the set of coincidence patterns. A coincidence pattern in a node represents a co-activation of particular Markov chains of its child nodes. HTM generative model is a spatio-temporal hierarchy in which higher levels remain stable for longer durations of time and can generate faster changing activations in lower levels.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g001" xlink:type="simple"/></fig>
<p>The process of learning an HTM model for spatio-temporal data is the process of learning the coincidence patterns and Markov-chains in each node at every level of the hierarchy. Although algorithms of varying levels of sophistication can be used to learn the states of an HTM node, the basic process can be understood using two operations, (1) memorization of coincidence patterns, and (2) learning a mixture of Markov chains over the space of coincidence patterns. In the case of a simplified generative model, an HTM node remembers all the coincidence patterns that are generated by the generative model. In real world cases, where it is not possible to store all coincidences encountered during learning, we have found that storing a fixed number of a random selection of the coincidence patterns is sufficient as long as we allow multiple coincidence patterns to be active at the same time. Motivation for this method came from the field of compressed sensing <xref ref-type="bibr" rid="pcbi.1000532-Donoho1">[20]</xref>. The HMAX model of visual cortex <xref ref-type="bibr" rid="pcbi.1000532-Riesenhuber1">[21]</xref> and some versions of convolutional neural networks <xref ref-type="bibr" rid="pcbi.1000532-MarcAurelioRanzato1">[22]</xref> also use this strategy. We have found that reasonable results can be achieved with a wide range of the number of coincidences stored. We have not yet developed a good heuristic for determining an optimal value of this parameter. For simplicity, we will only illustrate the case where a single coincidence pattern is active in a node at a time, but in our real implementations we use sparse distributed activations of the coincidence patterns. Each Markov chain in a node represents a set of coincidence patterns that are likely to occur sequentially in time. This temporal proximity constraint is analogous to the temporal slowness principle used in the learning of of invariant features <xref ref-type="bibr" rid="pcbi.1000532-Mitchison1">[23]</xref>–<xref ref-type="bibr" rid="pcbi.1000532-Stringer1">[26]</xref>. The learning of the mixture of Markov chains is simplified considerably because of the slowness constraint. We have found that a simple way to learn the mixture of Markov chains for real world cases is to learn a large transition matrix that is then partitioned using a graph partitioning algorithm <xref ref-type="bibr" rid="pcbi.1000532-George4">[27]</xref>. Details of one method of learning higher order Markov chains is available in <xref ref-type="bibr" rid="pcbi.1000532-Hawkins2">[28]</xref>.</p>
<p>For the rest of this paper, we will focus on the inference mechanism in HTM nodes that have finished their learning process. A node that has finished its learning process has a set of coincidence patterns and a set of Markov chains in it. <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2(A)</xref> shows a node that has 5 coincidence patterns and 2 Markov chains.</p>
<fig id="pcbi-1000532-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g002</object-id><label>Figure 2</label><caption>
<title>Structure and flow of a reference HTM node.</title>
<p>(A) Structure of the reference node, with five coincidence patterns and two Markov chains. This is an HTM node that has finished its learning process. It is assumed that this is the first node at level 2 of a network and is therefore labeled as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e005" xlink:type="simple"/></inline-formula>. Each coincidence pattern represents a co-occurrence of the Markov chains of the children. This node has 2 children. Child 1 has 3 Markov chains and child 2 has 4 Markov chains – hence there are seven elements in each coincidence pattern. The portions of the coincidence pattern coming from the first and second child are shown in different shades of gray. (B) Information flow in the reference node for the computation of the belief propagation equations shown in <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref>. The rectangles inside the node are processing units for the equations in the rows corresponding to the number displayed in each rectangle. We will use ‘feed-forward’ or ‘bottom-up’ to qualify messages received from children and messages sent up to the parent of this node. We will use ‘feedback’ or ‘top-down’ to qualify messages received from the parent and messages sent to the child nodes of this node. The node shown in the figure has two bottom-up input messages coming from the two children and has two top-down outputs which are the messages sent to these children. The arrows show vectors of inputs, outputs, and intermediate computational results. The number of components of each vector is represented using an array of boxes placed on these arrows.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g002" xlink:type="simple"/></fig>
<p>The inference mechanism in an HTM network is based on the propagation of new evidence from anywhere in the network to all other parts of the network. The presentation of a new image to the first level of an HTM vision network is an example of new evidence. Propagation of this evidence to other parts of the network results in each node in the network adjusting its belief states given this evidence. For example, a new image can lead to a different belief in the top level of the network regarding the identity of the object in that image. In general, HTM networks infer on time-varying inputs. Inference on a static input is a special case of this computation. Information can also be propagated down in the hierarchy for attention, segmentation, and filling in missing inputs.</p>
<p>HTM networks use Bayesian belief propagation for inference. Bayesian belief propagation originally was derived for inference in Bayesian networks <xref ref-type="bibr" rid="pcbi.1000532-Pearl1">[29]</xref>. Since an HTM node abstracts space as well as time, new equations must be derived for belief propagation in HTM nodes. These equations are described in the next section.</p>
</sec><sec id="s2b">
<title>Belief propagation in HTM nodes</title>
<p>In general, the messages that come into an HTM node from its children represent the degree of certainty over the child Markov chains. The node converts these messages to its own degree of certainty over its coincidence patterns. Based on the history of messages received, it also computes a degree of certainty in each of its Markov chains. This is then passed up to the next higher-level node. What the node receives from its parent is the parent's degree of certainty over this HTM node's Markov chains. The Markov chains are then ‘unwound’ in a step-by-step manner to find the top-down probability distribution over coincidence patterns. From this, the node's degrees of certainty over its child nodes' Markov chains are calculated. These feedback messages are then sent to the child nodes.</p>
<p><xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref> summarizes the computation of belief propagation messages in an HTM node. We will now describe the notation and meaning of these equations using the reference HTM node shown in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2</xref>. Detailed derivations of these equations are given in supporting information <xref ref-type="supplementary-material" rid="pcbi.1000532.s001">Text S1</xref>. A summary of the notation in these equations is given in <xref ref-type="table" rid="pcbi-1000532-t002">Table 2</xref>. Each equation is considered in detail in the sections that follow.</p>
<table-wrap id="pcbi-1000532-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.t001</object-id><label>Table 1</label><caption>
<title>Belief propagation equations for an HTM node.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000532-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/></colgroup>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">1) Calculate likelihood over coincidence patterns.</td>
<td align="left" colspan="1" rowspan="1"><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e006" xlink:type="simple"/><label>(2)</label></disp-formula>where coincidence pattern <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e007" xlink:type="simple"/></inline-formula> is the co-occurrence of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e008" xlink:type="simple"/></inline-formula> Markov chain from child 1, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e009" xlink:type="simple"/></inline-formula> Markov chain from child 2, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e010" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e011" xlink:type="simple"/></inline-formula> Markov chain from child <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e012" xlink:type="simple"/></inline-formula>.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">2) Calculate the feed-forward likelihood of Markov chains using dynamic programming</td>
<td align="left" colspan="1" rowspan="1"><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e013" xlink:type="simple"/><label>(3)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e014" xlink:type="simple"/><label>(4)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e015" xlink:type="simple"/><label>(5)</label></disp-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">3) Calculate the belief distribution over coincidence patterns</td>
<td align="left" colspan="1" rowspan="1"><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e016" xlink:type="simple"/><label>(6)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e017" xlink:type="simple"/><label>(7)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e018" xlink:type="simple"/><label>(8)</label></disp-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">4) Calculate the messages to be sent to child nodes.</td>
<td align="left" colspan="1" rowspan="1"><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e019" xlink:type="simple"/><label>(9)</label></disp-formula>where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e020" xlink:type="simple"/><label>(10)</label></disp-formula></td>
</tr>
</tbody>
</table></alternatives></table-wrap><table-wrap id="pcbi-1000532-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.t002</object-id><label>Table 2</label><caption>
<title>Summary of notation used for belief propagation in HTM nodes.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000532-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.t002" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Symbol</td>
<td align="left" colspan="1" rowspan="1">Meaning</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e021" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e022" xlink:type="simple"/></inline-formula> coincidence in the node</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e023" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e024" xlink:type="simple"/></inline-formula> Markov chain in the node.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e025" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Bottom-up evidence. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e026" xlink:type="simple"/></inline-formula> indicates the evidence at particular instant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e027" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e028" xlink:type="simple"/></inline-formula> indicates the sequence of bottom-up evidence from time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e029" xlink:type="simple"/></inline-formula> to time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e030" xlink:type="simple"/></inline-formula>.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e031" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Top-down evidence. Time indexing is similar to that of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e032" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e033" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Feed-forward output message of the node. This is a vector of length equal to the number of Markov chains in the node.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e034" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Feed-forward input message to the node from the child node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e035" xlink:type="simple"/></inline-formula>. This is a vector of length equal to the number of Markov chains in the child node.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e036" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Feedback input message to the node. This is a vector of length equal to the number of Markov chains in the node.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e037" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Feedback output message of the node to child node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e038" xlink:type="simple"/></inline-formula>. This is a vector of length equal to the number of Markov chains in the child node.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e039" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">The bottom-up likelihood over coincidence patterns in a node. This is one of the inputs for the feed-forward sequence likelihood calculation.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e040" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Bottom-up state variable for the Markov chains in a node. This a vector of length equal to the total number of states of all Markov chains in the node.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e041" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">State that combines bottom-up and top-down evidence for the Markov chains in a node. This state variable has the same dimension as that of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e042" xlink:type="simple"/></inline-formula>.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e043" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Belief in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e044" xlink:type="simple"/></inline-formula> coincidence pattern in a node.</td>
</tr>
</tbody>
</table></alternatives></table-wrap>
<p>In these equations, the coincidence patterns are referred to using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e045" xlink:type="simple"/></inline-formula> and the Markov chains are referred to using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e046" xlink:type="simple"/></inline-formula>. The HTM node shown in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2(A)</xref> contains 5 coincidence patterns and 2 Markov chains. The transition probability matrix of the Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e047" xlink:type="simple"/></inline-formula> is denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e048" xlink:type="simple"/></inline-formula>. This term appears in Equations 4 and 7. Each coincidence pattern in the node represents a co-occurrence of the temporal groups from its children. Coincidence pattern specifications are used in the computations described in equations 2 and 9.</p>
<p>Each node receives feed-forward input messages from its children and sends feed-forward messages to its parent. The feed-forward input messages are denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e049" xlink:type="simple"/></inline-formula>. The feed-forward output message of the node is denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e050" xlink:type="simple"/></inline-formula>. Similarly, the node receives feedback messages from its parent and sends feedback messages to its child nodes. The feedback input message to the node is denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e051" xlink:type="simple"/></inline-formula>. The feedback output messages that the node sends to its child nodes are denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e052" xlink:type="simple"/></inline-formula>. The equations shown in <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref> describe how the output messages are derived from the input messages. From the viewpoint of the node, the feed-forward messages carry information about the <italic>evidence</italic> from below. Evidence from below at any time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e053" xlink:type="simple"/></inline-formula> is denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e054" xlink:type="simple"/></inline-formula>. Similarly evidence from the parent is denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e055" xlink:type="simple"/></inline-formula>.</p>
<p>Equation 2 describes how the node calculates its likelihood of coincidence patterns, using the messages it gets from the children. The bottom-up likelihood of coincidence pattern <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e056" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e057" xlink:type="simple"/></inline-formula> is represented by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e058" xlink:type="simple"/></inline-formula>. The likelihood of each coincidence pattern is calculated as the product of the message components corresponding to that coincidence pattern.</p>
<p>In Equation 3, the bottom-up likelihood of Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e059" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e060" xlink:type="simple"/></inline-formula> is denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e061" xlink:type="simple"/></inline-formula>, where the term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e062" xlink:type="simple"/></inline-formula> represents the sequence of bottom-up evidences from time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e063" xlink:type="simple"/></inline-formula> to time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e064" xlink:type="simple"/></inline-formula>. This reflects that the likelihood of the Markov chains depends on the sequence of inputs received by the node. The variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e065" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e066" xlink:type="simple"/></inline-formula> defined in Equations 4 and 7 are state variables that are updated in a recursive manner at every time instant. These are dynamic programming <xref ref-type="bibr" rid="pcbi.1000532-Bellman1">[30]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-Howard1">[31]</xref> variables, each defined over all pairwise combinations of coincidence patterns and Markov chains. For example, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e067" xlink:type="simple"/></inline-formula> is value of the feed-forward dynamic programming variable at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e068" xlink:type="simple"/></inline-formula> corresponding to coincidence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e069" xlink:type="simple"/></inline-formula> and Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e070" xlink:type="simple"/></inline-formula>. In Equations 4 and 7, the states are updated every time step by passing the state from the previous time step through the Markov transition matrices and by combining them with bottom-up/top-down evidence.</p>
<p>An illustrative example showing how the belief propagation equations map onto a toy visual pattern recognition problem is given in supporting information <xref ref-type="supplementary-material" rid="pcbi.1000532.s002">Text S2</xref>. Readers who are not familiar with belief propagation can use this example to develop intuition for the nature of the messages. We examine the equations in <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref> in more detail in the next section as we consider how to implement them using neuron-like elements.</p>
</sec><sec id="s2c">
<title>Neuronal implementation of HTM belief propagation</title>
<p>This section describes an implementation of the HTM belief propagation equations using neuron-like elements. The implementation will be described with respect to the reference HTM node in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2</xref>. The neuronal implementation of the equations in <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref> is described in the following subsections. The subsections follow the order of table row numbers.</p>
<p>The purpose of this section is to show how the equations of HTM belief propagation can map onto a hypothetical neuronal system. In the <xref ref-type="sec" rid="s3">Results</xref> section, we map this hypothetical model onto actual cortical anatomy.</p>
<sec id="s2c1">
<title>Calculating the likelihood of coincidence patterns</title>
<p>The bottom-up input to the HTM node is the feed-forward output messages from its children. These output messages carry information about the degree of certainty of the Markov chains in the child nodes. Each message is a vector of length equal to the number of Markov chains in the corresponding child. The likelihood of coincidences is derived from these input messages according to Equation 2. This operation is performed by the rectangle marked 1 in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2(B)</xref>. <xref ref-type="fig" rid="pcbi-1000532-g003">Figure 3</xref> shows an abstract neuronal implementation of this calculation for the reference HTM node.</p>
<fig id="pcbi-1000532-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g003</object-id><label>Figure 3</label><caption>
<title>Coincidence likelihood circuit.</title>
<p>Circuit for calculating the bottom-up probability over coincidence patterns. Coincidence pattern neurons are represented by diamond shapes. The inputs to the circuit are the messages from the children, which are denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e071" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e072" xlink:type="simple"/></inline-formula>. The output of the circuit is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e073" xlink:type="simple"/></inline-formula>, as calculated by Equation 2 in <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref>. The input connections to each neuron represent its coincidence pattern. For example, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e074" xlink:type="simple"/></inline-formula> is the co-occurrence of Markov chain 3 from the left child and Markov chain 1 from the right child. The probabilities are calculated by multiplying the inputs to each neuron.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g003" xlink:type="simple"/></fig>
<p>In <xref ref-type="fig" rid="pcbi-1000532-g003">Figure 3</xref>, each neuron corresponds to a stored coincidence pattern. The pattern corresponding to the co-occurrence is stored in the connections this neuron makes to the messages from the child input nodes. For example, the neuron corresponding to coincidence pattern <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e075" xlink:type="simple"/></inline-formula> has connections to the first position of the message from the first child and the third position of the message from the second child. These connections correspond to first row of the coincidence-pattern matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e076" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2(A)</xref>. Each neuron calculates its output by multiplying its inputs. For example, the output of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e077" xlink:type="simple"/></inline-formula> is proportional to the product of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e078" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e079" xlink:type="simple"/></inline-formula>. The output, denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e080" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2(B)</xref>, is a vector of 5 components, one component corresponding to each coincidence pattern. This vector represents the likelihood of coincidence patterns, based on the messages received from the child nodes.</p>
</sec><sec id="s2c2">
<title>Calculating the feed-forward likelihood of Markov chains</title>
<p>The next step in the computation of feed-forward messages, corresponding to the rectangle marked 2 in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2(B)</xref>, is the calculation of the degree of certainty of the HTM node in each of its Markov chains. The quantity that needs be to calculated is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e081" xlink:type="simple"/></inline-formula> for each Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e082" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e083" xlink:type="simple"/></inline-formula> represent the bottom-up evidence distributions received from time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e084" xlink:type="simple"/></inline-formula> to time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e085" xlink:type="simple"/></inline-formula>. The likelihood of Markov chains depends on the sequence of messages that the node has received from its children. A brute-force computation of this quantity is not feasible because this requires the enumeration of the likelihoods of an exponentially growing number of sample paths. To calculate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e086" xlink:type="simple"/></inline-formula> efficiently, all the past evidence needs to be collapsed into a state variable that can be updated recursively every time instant. This is done using a technique called dynamic programming <xref ref-type="bibr" rid="pcbi.1000532-Bellman1">[30]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-Howard1">[31]</xref> as represented in Equation 4. The derivation of this equation is described in supporting information <xref ref-type="supplementary-material" rid="pcbi.1000532.s001">Text S1</xref>.</p>
<p>Equation 4 can have a very efficient neuronal implementation as shown in <xref ref-type="fig" rid="pcbi-1000532-g004">Figure 4</xref>. The ‘circle’ neurons in this circuit implement the sequence memory of the Markov chains in the HTM node. The connections between the circle neurons implement the transition probabilities of the Markov chain. As the ‘axons’ between these neurons encode a one time-unit delay, the output of a circle neuron is available at the input of the circle neuron that it connects to after one time step. (This is a very limited method of representing time. We will discuss more sophisticated representations of time in a later section.)</p>
<fig id="pcbi-1000532-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g004</object-id><label>Figure 4</label><caption>
<title>Markov chain likelihood circuit.</title>
<p>The circuit for calculating the likelihoods of Markov chains based on a sequence of inputs. In this figure there are five possible bottom-up input patterns (c1–c5) and two Markov chains (g1, g2). The circle neurons represent a specific bottom-up coincidence within a learned Markov chain (two Markov chains are shown, one in blue and one in green). Each rectangular neuron represents the likelihood of an entire Markov chain to be passed to a parent node. This circuit implements the dynamic programming Equation 4 in <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g004" xlink:type="simple"/></fig>
<p>All the circle neurons co-located in a column have the same bottom-up input. They are all driven by the same coincidence-pattern likelihood neuron – represented by diamonds – from below. Each column, considering only bottom-up input, can be thought of as representing a particular coincidence pattern. In addition to the bottom-up input, these circle neurons also have ‘lateral’ inputs that come from other circle neurons in the same Markov chain. The lateral connections specify the meaning of a neuron in a sequence. A circle neuron that is labeled as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e087" xlink:type="simple"/></inline-formula> represents the coincidence pattern <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e088" xlink:type="simple"/></inline-formula> in the context of Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e089" xlink:type="simple"/></inline-formula>. The same coincidence pattern can belong to different Markov chains and can hence be active under different temporal contexts. For example, the circle neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e090" xlink:type="simple"/></inline-formula> will be activated only in the context of Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e091" xlink:type="simple"/></inline-formula>, whereas the circle neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e092" xlink:type="simple"/></inline-formula> will be activated only in the context of Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e093" xlink:type="simple"/></inline-formula>.</p>
<p>Each circle neuron in this circuit does the same computation. Each neuron calculates its output by multiplying the bottom-up input with the weighted sum of its lateral inputs. The output of a circle neuron is denoted using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e094" xlink:type="simple"/></inline-formula>. With this, the output of any circle neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e095" xlink:type="simple"/></inline-formula> is calculated as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e096" xlink:type="simple"/><label>(1)</label></disp-formula>That is, the output of the a circle neuron at any time is the weighted sum of the outputs of the neurons in the same Markov chain at the previous time step multiplied by the current bottom-up activation. (Again, the above equation assumes a simple step-wise notion of time which is insufficient for encoding duration and for non-discrete time problems. We believe that in real brains, time duration is captured using a separate mechanism. This will be discussed in the <xref ref-type="sec" rid="s3">Results</xref> section.) The above equation corresponds to Equation 4 if we replace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e097" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e098" xlink:type="simple"/></inline-formula>. Therefore, the circle-neuron circuits shown in <xref ref-type="fig" rid="pcbi-1000532-g004">Figure 4</xref> implement Equation 4 and the weights on the lateral time-delayed connections correspond to the transition matrix entries in each Markov chain.</p>
<p>Now consider the third kind of neurons – the ‘rectangle’ neurons – in <xref ref-type="fig" rid="pcbi-1000532-g004">Figure 4</xref>. The rectangle neuron marked <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e099" xlink:type="simple"/></inline-formula> receives its inputs from the outputs of all the circle neurons in the Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e100" xlink:type="simple"/></inline-formula>. The rectangle neurons pool the outputs of all the coincidence-pattern neurons in the context of a Markov chain. At any time point, the output of a rectangle neuron is calculated as the sum (or maximum) of the inputs to that neuron.</p>
<p>Note that the operation of the rectangle neurons corresponds to pooling over the activations of all the circle neurons of the same Markov chain. It is easy to verify that this is the operation involved in the calculation of the message this node sends to its parent according to Equation 3. The concatenated outputs of the rectangle neurons is the message <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e101" xlink:type="simple"/></inline-formula> that this node sends to its parent. As noted in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2(B)</xref>, this message is a vector of two components, corresponding to the two Markov chains in the reference node in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2(A)</xref>. This completes the description of the abstract neuronal implementation of equations in the second row of <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref> and of the operations performed by the rectangle marked (2) in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2(B)</xref>.</p>
</sec><sec id="s2c3">
<title>Calculating the belief distribution over coincidence patterns</title>
<p>An HTM node calculates its degree of belief in a coincidence pattern by combining bottom-up, top-down, and temporal evidences according to the equations on the third row of <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref>. This corresponds to the operations of the rectangle marked (3) in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2(B)</xref>. The top-down input to the node is a vector of length equal to the number of Markov chains of the node. The output of this computation is the belief-vector over the coincidence patterns, in this case, a vector of length 5.</p>
<p>The belief calculation, described in Equation 6, has almost the same form as the forward dynamic programming Equations 4. The state variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e102" xlink:type="simple"/></inline-formula> has the same form as the state variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e103" xlink:type="simple"/></inline-formula> and a very similar update equation. The only difference between these two is the multiplication by a top-down factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e104" xlink:type="simple"/></inline-formula> in the belief calculation equations. Therefore, the neuronal implementation of the dynamic programming part of the belief calculation equation is very similar to that of the forward dynamic programming variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e105" xlink:type="simple"/></inline-formula>. This implementation is shown in <xref ref-type="fig" rid="pcbi-1000532-g005">Figure 5</xref>. The filled-circle neurons correspond to the circle neurons in the forward calculation. Note that, in contrast to the circle neurons in <xref ref-type="fig" rid="pcbi-1000532-g004">Figure 4</xref>, the filled-circle neurons now also have a top-down multiplicative input that corresponds to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e106" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1000532-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g005</object-id><label>Figure 5</label><caption>
<title>Belief circuit.</title>
<p>Circuit for calculating the belief distribution over coincidence patterns by integrating the sequence of bottom-up inputs with the top-down inputs. The pentagon-shaped neurons are the belief neurons. These neurons pool over all the neurons representing the same coincidence in different Markov chains to calculate the belief value for each coincidence pattern. This circuit implements the Equation 6 in <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g005" xlink:type="simple"/></fig>
<p>The pentagon neurons in <xref ref-type="fig" rid="pcbi-1000532-g005">Figure 5</xref> are the belief neurons. These neurons pool over the activities of the same coincidence neurons in different Markov chains to calculate the belief value for each coincidence pattern. This operation corresponds to the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e107" xlink:type="simple"/></inline-formula> operation in Equation 6. Note that the operation of the pentagon neuron is different from that of the rectangle neuron in <xref ref-type="fig" rid="pcbi-1000532-g004">Figure 4</xref>. The rectangle neuron pools over different coincidence patterns in the same Markov chain. The pentagon neuron pools over the same coincidence pattern in different Markov chains.</p>
</sec><sec id="s2c4">
<title>Calculating the messages to be sent to child nodes</title>
<p>The step that remains to be explained is the conversion of the belief messages to the messages that a node sends to its children. This step is described by Equation 9 and corresponds to the operations performed by the rectangle marked (4) in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2(B)</xref>. The input for this operation is the belief vector. The outputs are the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e108" xlink:type="simple"/></inline-formula> messages that are sent to the child nodes. A message is sent to each child and the message describes the degree of certainty this node has about the child nodes' Markov chains.</p>
<p><xref ref-type="fig" rid="pcbi-1000532-g006">Figure 6</xref> shows how this equation can be implemented using neurons. The input belief is fed to ‘hexagonal neurons’ that compute the messages for child nodes. <xref ref-type="fig" rid="pcbi-1000532-g006">Figure 6</xref> shows two sets of hexagonal neurons corresponding to the two child nodes of this node. Each hexagonal neuron corresponds to a Markov chain of the child node. The left child node has 3 Markov chains and the right child node has 4 Markov chains. The outputs of these hexagonal neurons are the messages that are sent to the respective children.</p>
<fig id="pcbi-1000532-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g006</object-id><label>Figure 6</label><caption>
<title>Feedback message circuit.</title>
<p>The circuit for computing the messages to be sent to children according to Equation 9. The two sets of hexagonal neurons correspond to the Markov chains of the two children of the reference node.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g006" xlink:type="simple"/></fig>
<p>The connections between the input and the hexagonal neurons encode the constituents of coincidence patterns. For example, the first input is connected to the hexagonal neuron representing the first Markov chain of the left child and to the hexagonal neuron representing the third Markov chain of the right child. This is because the coincidence pattern <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e109" xlink:type="simple"/></inline-formula> is defined as the co-occurrence of the first Markov chain from the left child and the third Markov chain from the right child. The hexagonal neurons calculate their outputs as a sum of their inputs as described in Equations 9 and 10.</p>
<p>The operation of the hexagonal neurons shown in <xref ref-type="fig" rid="pcbi-1000532-g006">Figure 6</xref> can be thought of as the reverse of the operations performed by the diamond neurons that were described in <xref ref-type="fig" rid="pcbi-1000532-g003">Figure 3</xref>. The weights on the inputs to both these kinds of neurons define the coincidence patterns. In the case of the diamond neurons, they calculate the probability over coincidences from the probability distribution over Markov chains from each child. The hexagonal neurons do the reverse; they calculate the probability distributions over the Markov chains from each child from the probability distribution over coincidence patterns.</p>
</sec></sec><sec id="s2d">
<title>Further considerations of belief propagation equations</title>
<p>The equations in <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref> are self-consistent and sufficient for some learning and inference tasks. However, they do not address several issues required for many real world problems. Specifically, they do not address how feedback from a parent node to a child node can influence the child node's feed-forward output, and they do not address issues of specific timing. The following sections address these issues.</p>
<sec id="s2d1">
<title>Role of feedback in the current model</title>
<p>Even though feedback propagation in the current model does not affect feed-forward propagation, it plays an important role in understanding the evidence presented to a network. For example, for an image given as input to the network, feed-forward propagation results in a distribution at the top level about the objects that could be present in the image. Feedback propagation can then be used to identify the features in the input image that produced a particular hypothesis at the top, to identify whether a particular edge in the image belongs to an object or to the background, and to assign ownership of features if there are multiple objects in the scene. In the <xref ref-type="sec" rid="s3">Results</xref> section we give examples of feedback propagation in the current model.</p>
</sec><sec id="s2d2">
<title>Role of feedback in loopy graphs</title>
<p>In the general case, nodes in an HTM network will have overlapping receptive fields. This gives rise to HTM network structures where each node in the network can have multiple parents. Such network structures are ‘loopy’ because of the cycles in their underlying graphs. Belief propagation is theoretically guaranteed to give accurate results in non-loopy graphs. Even though theoretical guarantees do not exist for belief propagation in loopy graphs, it is found to work well in practice on many problems involving loops <xref ref-type="bibr" rid="pcbi.1000532-Frey1">[32]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-Murphy1">[33]</xref>.</p>
<p>HTM nodes with multiple parents can be treated like the causal poly-tree structures described by Pearl <xref ref-type="bibr" rid="pcbi.1000532-Pearl1">[29]</xref>. Poly-tree structures imply that multiple higher-level causes influence a lower level cause. Belief propagation computations in poly tree structures have the property that the message from a child to a parent is influenced by the messages from all other parents to the child. This modifies the flow of information in the HTM node in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2(b)</xref> by introducing an arrow between box 3 and box 2. Local poly-tree structures can produce loops if repeated in a hierarchy. These sources of loops are likely to be common in brains. Multiple top-down causes can be combined efficiently using the noisy OR-gate structures described in Pearl's book <xref ref-type="bibr" rid="pcbi.1000532-Pearl1">[29]</xref>.</p>
<p>For the sake of simplicity of exposition, we deal exclusively with singly connected (non-loopy) networks in this paper. It is straightforward to extend belief propagation in HTMs to include multiple parents for each node.</p>
</sec><sec id="s2d3">
<title>Role of feedback for attention</title>
<p>The propagation equations in <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref> compute approximate posterior distributions at every node in the hierarchy given the evidence. Calculating the posterior distributions at every node is one type of query that can be answered using such propagation techniques. Making inferences about real world situations often requires more complex queries involving a subset of nodes. Pearl discussed this need in his book as a motivation for controlling attention <xref ref-type="bibr" rid="pcbi.1000532-Pearl1">[29]</xref> (page 319), “In many cases, there is a small set of hypotheses that stand at the center of one's concerns, so the reasoning task can focus on a narrow subset of variables, and propagation through the entire network is unnecessary.”</p>
<p>For example, one interesting query to answer is what would be the evidence in support of top-level hypothesis 1, if all other top-level hypotheses are assumed to be inactive. This is a form of top-down attention. It can be achieved using local computations by propagating down the influence of hypothesis 1, and inhibiting, at all levels, the bottom-up support that conflicts with the high level hypothesis. In the case of vision, such a mechanism can be used to pay attention to a single object when multiple objects are present in the scene. The top-down propagation of a higher-level hypothesis will, in this case, identify the lower level nodes and coincidence patterns that support the hypothesized object. Turning off all other nodes can increase or decrease the certainty in that hypothesis.</p>
<p>Another form of query that can be answered is to ask what other hypotheses might be active if the top-level hypothesis is considered to be inactive. For example, while recognizing a complex scene, it could be advantageous to not pay attention to an object that is already recognized so as to focus on other objects in the scene. This requires a mechanism that propagates down the currently active hypothesis and turning off all the evidence that supports this hypothesis exclusively.</p>
<p>Both of the above cases correspond to gating the bottom-up evidence using top-down activation. The gating signal at each node, corresponding to an activated top-level hypothesis, can be derived from the computed beliefs at that node. However, maintaining this gating during further computations requires external control mechanisms that are not part of the standard belief propagation machinery. There are several places where this gating can be applied, at the inputs to coincidences, at the coincidences themselves, or at the output of the Markov chains.</p>
</sec><sec id="s2d4">
<title>Incorporating variable speed and duration into the belief calculation</title>
<p>As expressed in the equations, the Markov chains advance their state with every time tick and can model only sequences that happen at a particular speed. The prime enabler of sequential inference in those equations is the property that the outputs of the pre-synaptic neurons at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e110" xlink:type="simple"/></inline-formula> is available at the lateral input of the post-synaptic neuron at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e111" xlink:type="simple"/></inline-formula>, exactly at the time when the bottom-up activity of the post-synaptic neuron arrives. If this lateral activity is maintained at the lateral input of the post-synaptic neuron for a longer duration, the bottom-up input activity for the post synaptic cell do not need to arrive exactly at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e112" xlink:type="simple"/></inline-formula>. The lateral input that is maintained in the post-synaptic cell can be extinguished when either sufficient bottom-up activity arrives at the post-synaptic cell to produce a ‘firing’ event or when a temporal window is exhausted after the pre-synaptic event. Such a mechanism that strengthens the correct sequence as long as the next event arrives within a temporal window after the previous event would enable variable speed sequential inference that is robust to local temporal warps. Achieving this in the equations requires writing the dynamic programming equations using events rather than time steps, where events are defined using thresholds on the combined lateral and bottom-up activity. Variable speed inference can be achieved with the same neuronal connectivity we showed for fixed speed inference if we assume that mechanisms for maintaining lateral activity and for determining event thresholds are implemented within each neuron. Further explication of this mechanism is left for future work.</p>
<p>Another element missing from the equations in the previous section is an explicit duration model associated with the states of Markov chains. In certain cases of temporal inference, the next event is expected at a precise time after the previous event rather than in a temporal window as discussed in the above paragraph. Music is one example. Humans also have the ability to speed up and slow down this specific duration mechanism. Several techniques exist for incorporating explicit duration models into Markov chains <xref ref-type="bibr" rid="pcbi.1000532-Levinson1">[34]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-Rabiner1">[35]</xref>. Some of these techniques introduce self-loops into the Markov chain states. However, self-loops lead to an exponential duration probability density that is inappropriate for most physical signals <xref ref-type="bibr" rid="pcbi.1000532-Rabiner1">[35]</xref>. Instead, we assume that durations are signaled to a node by an external timing unit that determines the rate of change of the signals using some system-level measurements. This means that the state change computations will have two components. The first component, as described in the previous sections, determines the distribution of the next state without considering when exactly that distribution is going to be active. The second component, the external timing signal, determines when the belief distribution is going to be active.</p>
<p><xref ref-type="fig" rid="pcbi-1000532-g007">Figure 7</xref> is similar to <xref ref-type="fig" rid="pcbi-1000532-g005">Figure 5</xref> with the addition of a variable time-delay mechanism. Two types of belief neurons are shown. The pentagonal neurons, previously shown in <xref ref-type="fig" rid="pcbi-1000532-g005">Figure 5</xref>, calculate the node's belief, and rounded rectangle neurons represent the belief at a particular time delay. The outputs of the rounded rectangle neurons are passed through an external variable delay unit. The rounded rectangle neurons act as a gate that opens only when a timing signal and a belief value are both available at its inputs. The activation of these neurons triggers the next timing cycle. The timing signal is used to gate the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e113" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e114" xlink:type="simple"/></inline-formula> calculations. Only the gating of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e115" xlink:type="simple"/></inline-formula> calculation is shown in the figure. The external timing circuit achieves the effect of a specific duration model whose tempo can be changed.</p>
<fig id="pcbi-1000532-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g007</object-id><label>Figure 7</label><caption>
<title>Timing circuit.</title>
<p>The same circuit as shown in <xref ref-type="fig" rid="pcbi-1000532-g005">Figure 5</xref> with the addition of circuitry for incorporating variable time delays between elements of the Markov chains. The pentagon neurons represent the belief at each node. The rounded rectangle neurons represent the belief at each node at the appropriate time delay. An external variable time delay mechanism provides time duration information to all the neurons involved in encoding sequences.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g007" xlink:type="simple"/></fig></sec></sec></sec><sec id="s3">
<title>Results</title>
<p>In this section, we interpret anatomical data of the neocortex within the context of the computational specifications from the previous sections. Anatomical data gives us important constraints on input and output layers, intra- and inter-laminar connections and placement of cell bodies and dendrites. Assignment of a particular function to a particular layer imposes constraints on what functions can be performed by other layers. The challenge is to find an organization that is self-consistent in the sense that it implements the belief propagation equations while conforming to the constraints imposed by biology.</p>
<p>Our working hypothesis can be stated simply: The cortical circuits implement the HTM belief propagation equations described in <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref>. A hypothetical neuronal implementation of these equations was described in the previous section. Under the assumption that the cortical circuits are implementing these equations, what remains to be explained is how the abstract neuronal implementation of the previous section is physically organized in the layers and columns of actual cortical anatomy. This is accomplished by comparing the abstract neural implementations with anatomical data. We describe the results in two stages. First we describe the high-level mapping between the neocortical hierarchy and the HTM hierarchy. Then we describe how the circuits based on HTM belief propagation equations can be mapped to cortical columns and laminae.</p>
<sec id="s3a">
<title>Mapping between neocortex hierarchy and HTM network hierarchy</title>
<p>An area of cortex can be thought of as encoding a set of patterns and sequences in relation to the patterns and sequences in regions hierarchically above and below it. The patterns correspond to the coincidence patterns in an HTM node and the sequences correspond to the Markov chains.</p>
<p>An HTM Node, as described earlier in this paper, encodes a set of mutually exclusive patterns and Markov chains. A region of cortex that has several patterns simultaneously active will be implemented using several HTM nodes. <xref ref-type="fig" rid="pcbi-1000532-g008">Figure 8(D)</xref> shows the HTM implementation of the logical cortical hierarchy shown in 8(C). This arrangement corresponds to one of the basic organizing principles of the visual system where neurons in higher-level visual areas receive inputs from many neurons with smaller receptive fields in lower-level visual areas <xref ref-type="bibr" rid="pcbi.1000532-Hubel1">[36]</xref>. In addition, due to the temporal nature of HTM, this arrangement corresponds to a temporal hierarchy analogous to the kind reported by Hasson and colleagues <xref ref-type="bibr" rid="pcbi.1000532-Hasson1">[37]</xref>. In this highly simplified mapping, the area V1 is implemented using 4 HTM nodes while area V2 is implemented using 2 HTM nodes. Typically, the number of non-exclusive patterns that needs to be maintained decreases as you ascend in the hierarchy. Therefore, higher-level cortical regions can possibly be modeled using a fewer number of HTM nodes. Note that this is a representative diagram. A cortex-equivalent implementation of V1 and V2 could require several thousand HTM nodes for each cortical area and the receptive fields of the nodes would typically be overlapping.</p>
<fig id="pcbi-1000532-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g008</object-id><label>Figure 8</label><caption>
<title>Mapping between neocortex hierarchy and HTM hierarchy.</title>
<p>(A) Schematic of neocortex inside the skull. The neocortex is a thin sheet of several layers of neurons. Different areas of the neocortical sheet process different information. Three successive areas of the visual hierarchy – V1, V2 and V4 – are marked on this sheet. The connections between the areas are reciprocal. The feed-forward connections are represented using green arrows and the feedback connections are represented using red arrows. (B) A slice of the neocortical sheet, showing its six layers and columnar organization. The cortical layers are numbered 1 to 6: layer 1 is closest to the skull, and layer 6 is the inner layer, closest to the white matter. (C) Areas in the neocortex are connected in a hierarchical manner. This diagram shows the logical hierarchical arrangement of the areas which are physically organized as shown in (A). (D) An HTM network that corresponds to the logical cortical hierarchy shown in (C). The number of nodes shown at each level in the HTM hierarchy is greatly reduced for clarity. Also, in real HTM networks the receptive fields of the nodes overlap. Here they are shown non-overlapping for clarity.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g008" xlink:type="simple"/></fig>
<p>The coincidence patterns and Markov chains in an HTM node can be represented using random variables. A cortical column can be thought of as encoding a particular value of the random variable that represents the coincidence patterns in the HTM node. The feed-forward and feedback connections to a set of cortical columns carry the belief propagation messages. Observed information anywhere in the cortex is propagated to other regions through these messages and can alter the probability values associated with the hypotheses maintained by other cortical columns. In HTMs these messages are computed using the mathematics of Bayesian belief propagation as we described earlier.</p>
</sec><sec id="s3b">
<title>A detailed proposal for the computations performed by cortical layers</title>
<p>Our proposal for the function, connectivity and physical organization of cortical layers and columns is shown in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>. This figure corresponds to the laminar and columnar cortical circuit implementation of the belief propagation equations for the reference HTM node in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2</xref>. <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref> was created by arranging the neurons of the abstract neuronal implementation of HTM belief propagation into columns and laminae in such a way that the resultant circuit matched most of the prominent features found in mammalian neocortex. In the following sections we de-construct this picture and examine the anatomical and physiological evidences for the specific proposals. This will also illuminate the process that we went through to arrive at the circuit shown in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>.</p>
<fig id="pcbi-1000532-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g009</object-id><label>Figure 9</label><caption>
<title>A laminar biological instantiation of the Bayesian belief propagation equations used in the HTM nodes.</title>
<p>The circuit shown here corresponds exactly to the instantiation of the reference HTM node shown in <xref ref-type="fig" rid="pcbi-1000532-g002">Figure 2</xref>. The five vertical ‘columns’ in the circuit correspond to the 5 coincidence patterns stored in the reference node. Layers 1 to 6 are marked according to the standard practice in neuroscience. Emphasis is given to the functional connectivity between neurons and the placement of the cell bodies and dendrites. Detailed dendritic morphologies are not shown. Axons are shown using arrow-tipped lines. Feed-forward inputs and outputs are shown using green axons and feedback inputs and outputs are shown using red axons. Whether an axon is an input or output can be determined by looking at the direction of the arrows. The blue axons entering and exiting the region represent timing-duration signals. ‘T’ junctions represent the branching of axons. However, axonal crossings at ‘X’ junctions do not connect to each other. Inter-columnar connections exist mostly between neurons in layer 2/3, between layer 5 cells, and between layer 6 cells. The inter-columnar connections in layer 2/3 that represent sequence memories are represented using thicker lines.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g009" xlink:type="simple"/></fig>
<p>The circuits in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref> provide an exemplar instantiation of the Bayesian computations in laminar and columnar biological cortical circuits. Several plausible variations and exceptions of this circuit can be found because of the degrees of freedom in the implementation of the belief propagation equations and because of the incompleteness of anatomical data. We will tackle some of these exceptions and variations as they come up in the appropriate context and also in the <xref ref-type="sec" rid="s4">Discussion</xref> section.</p>
<sec id="s3b1">
<title>Columnar organization</title>
<p>The cortical circuit shown in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref> is organized as 5 columns corresponding to the 5 coincidence patterns in the reference HTM node that we started with. The neurons in each column represent some aspect of the coincidence pattern that the column represents. For example, the neurons in layer 2/3 represent the coincidence pattern in the context of different sequences, whereas the neurons in layer 6 represent the participation of the coincidence pattern in the calculation of feedback messages. The 5 columnar structures shown represent a set of 5 mutually exclusive hypotheses about the same input space. For example, these columns can correspond to a set of columns in the primary visual cortex V1 that receive input from a small area of the visual field. The 5 coincidence patterns might correspond to different orientations of a line. If the receptive field is small enough, the different orientations can be considered mutually exclusive - the activity of one reduces the activity of the other. This kind of columnar organization is typical in biology <xref ref-type="bibr" rid="pcbi.1000532-Tsunoda1">[38]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-Churchland1">[39]</xref>.</p>
<p>In the idealized cortical column model, each different aspect that needs to be represented for a coincidence pattern is represented using a single neuron. For example, there is exactly one neuron representing coincidence pattern 1 in the context of Markov chain 1. This means that there is no redundancy in this idealized cortical representation. Nothing about the computation or the representation changes if we replicate each neuron in this circuit a few times, while maintaining their connectivity. A coincidence that is represented by a single neuron in our cortical column can be represented by a cluster of laterally interconnected neurons.</p>
<p>One prediction of our model is that many of the connections within a vertical column of cells can be established without any learning. <xref ref-type="fig" rid="pcbi-1000532-g010">Figure 10(A)</xref> shows a single idealized column. The connections within this column that can be established a-priori are shown in black. These connections act as a backbone for carrying out belief propagation computations. This feature makes our idealized cortical column a good candidate to be a developmental feature. The idealized cortical column in <xref ref-type="fig" rid="pcbi-1000532-g010">Figure 10(A)</xref> can correspond to what is known as the <italic>mini-columns</italic> or <italic>ontogenetic columns</italic> of the cortex <xref ref-type="bibr" rid="pcbi.1000532-Rakic1">[40]</xref>. Mini-columns are developmental units that contain about 80 to 100 neurons. By the 26th gestational week, the human neocortex is composed of a large number of mini-columns in parallel vertical arrays <xref ref-type="bibr" rid="pcbi.1000532-Mountcastle1">[41]</xref>. In real brains we would not want to represent something with a single cell. Therefore, we assume that in real brains the basic computational column will consist of many redundant cells bound together using common input and short-range intra-laminar connections resulting in a column as shown in <xref ref-type="fig" rid="pcbi-1000532-g010">Figure 10(B)</xref> <xref ref-type="bibr" rid="pcbi.1000532-Mountcastle1">[41]</xref>.</p>
<fig id="pcbi-1000532-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g010</object-id><label>Figure 10</label><caption>
<title>Columnar organization of the microcircuit.</title>
<p>(A) A single idealized cortical column. This idealization could correspond to what is often referred to as a biological mini-column. It is analogous to one of the five columnar structures in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>. (B) A more dense arrangement of cells comprising several copies of the column (A). Although we typically show single cells performing computations, we assume there is always redundancy and that multiple cells within each layer are performing similar functions.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g010" xlink:type="simple"/></fig>
<p>For the rest of the discussion we will focus on the idealized cortical column and the idealized cortical circuit with no redundancy.</p>
</sec><sec id="s3b2">
<title>Layer 4 stellate neurons implement the feed-forward probability calculation over coincidence patterns</title>
<p>The excitatory neurons in layer 4 of the cortex consist primarily of star-shaped neurons called stellate neurons and pyramidal neurons <xref ref-type="bibr" rid="pcbi.1000532-Thomson1">[42]</xref>. Layer 4 is generally accepted as the primary feed-forward input layer to cortical regions <xref ref-type="bibr" rid="pcbi.1000532-Hegde1">[2]</xref>. In the cat primary visual cortex, the outputs from the retina pass through the lateral geniculate nucleus (LGN) of the thalamus and then terminate on layer 4 stellate cells. Most of these connections are known to be proximal to the cell body and can drive the cells. The major projection (output) of layer 4 stellate neurons is to layer 3 cells <xref ref-type="bibr" rid="pcbi.1000532-Douglas1">[15]</xref>.</p>
<p>We propose that the layer 4 stellate cells implement the probability calculation described in Equation 2 and shown in <xref ref-type="fig" rid="pcbi-1000532-g003">Figure 3</xref>. This means that layer 4 neurons are coincidence detectors and that the synapses of the layer 4 neurons represent co-occurrence patterns on its inputs.</p>
<p>We realize this is a dramatic simplification of layer 4 cell connectivity. It does not address the very large number of synapses formed on distal dendrites, nor does it address the fact that many feed-forward connections from Thalamus terminate in layer 3 cells and that in some regions of cortex layer 4 is greatly diminished. These facts can be supported by HTM theory. The horizontal connections between layer 4 cells can implement spatial pooling or temporal pooling without timing. Layer 3 cells can also act as coincidence detectors of inputs from thalamus that make proximal connections, and layer 3 cells can take the full burden of coincidence detection. However, we choose to present the simplest explanation of layer 4 cells for clarity and discuss some of the variations in the <xref ref-type="sec" rid="s4">Discussion</xref> section.</p>
<p>In <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>, the layer 4 neurons are shown in red. The inputs to these neurons are the outputs of lower levels of the cortical hierarchy, possibly routed through the thalamus. It is easy to verify that the connections of these neurons correspond to the ‘diamond’ neurons in our belief propagation implementation shown in <xref ref-type="fig" rid="pcbi-1000532-g003">Figures 3</xref> , <xref ref-type="fig" rid="pcbi-1000532-g004">4</xref> and <xref ref-type="fig" rid="pcbi-1000532-g005">5</xref>. Note that in the implementation of the belief propagation equations shown in <xref ref-type="fig" rid="pcbi-1000532-g004">Figures 4</xref> and <xref ref-type="fig" rid="pcbi-1000532-g005">5</xref>, the neurons that calculate the probability distribution on coincidence patterns (the diamond neurons) have only feed-forward inputs. This is in contrast to many other neurons that receive feed-forward, feedback and lateral inputs. In neuroscience, it is accepted that the feedback inputs to a cortical region generally avoid layer 4 <xref ref-type="bibr" rid="pcbi.1000532-Hegde1">[2]</xref>. This is consistent with our proposal for the function of layer 4 neurons.</p>
<p>Making layer 4 correspond to the feed-forward computation of the probability over coincidence patterns imposes some constraints on the computational roles for other layers. For example, the major projection of layer 4 is to layer 3. This means that any computation that requires major inputs from layer 4 will need to be done at layer 3 and should match the general characteristics of layer 3. The proposals for layer 3 computations, described in a subsequent section, match these constraints.</p>
</sec><sec id="s3b3">
<title>Layer 1: The broadcast layer for feedback information and timing information</title>
<p>Feedback connections from higher levels of the cortex rise to layer 1. The recipients of these connections are the cells with apical dendrites in layer 1. Layer 1 is comprised mostly of axons carrying feedback from higher levels of cortex, axons from non-specific thalamic cells, apical dendrites, and a minor concentration of cell bodies <xref ref-type="bibr" rid="pcbi.1000532-Thomson1">[42]</xref>.</p>
<p>To remain consistent with this biological data, the layer 1 in our mapping will be the ‘broadcast’ layer for feedback and timing information. The axons carrying feedback information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e116" xlink:type="simple"/></inline-formula> will be available at layer 1 and accessed by the apical dendrites of neurons that require this information. In addition, the availability of a timing signal at layer 1 is assumed. The purpose of this timing signal is discussed in a subsequent section where we discuss the layer 5 cells.</p>
</sec><sec id="s3b4">
<title>Layer 2/3 pyramidal cells: Sequence memory, pooling over sequences, incorporating feedback information</title>
<p>The primary inter-laminar excitatory input to layer 2/3 is from the stellate cells of layer 4. In addition, the layer 2/3 neurons receive excitatory inputs from other layer 2/3 neurons via extensive lateral connections <xref ref-type="bibr" rid="pcbi.1000532-Bannister1">[43]</xref>. Many layer 2/3 neurons project to higher levels of cortex and to layer 5 <xref ref-type="bibr" rid="pcbi.1000532-Thomson2">[44]</xref>.</p>
<p>We propose three different roles for the layer 2/3 pyramidal neurons in cortical circuits: (1) Calculation of feed-forward Markov chain (sequence) states, (2) Projection of Markov chain information to higher level cortical areas, and (3) Computation of sequence states that incorporate feedback information. We now consider each proposal in detail and then examine anatomical evidence in support of these circuits.</p>
<list list-type="order"><list-item>
<p><bold>Pyramidal cells for calculating feed-forward sequence states:</bold> The pyramidal neurons shown in green in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref> implement the Markov chain sequences and the dynamic programming computations for feed-forward sequential inference. These neurons correspond to the ‘circle neurons’ that we described in the <xref ref-type="sec" rid="s2">Model</xref> section and implement the dynamic programming Equation 4 in <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref>. These pyramidal neurons receive ‘vertical’ excitatory inputs from the outputs of layer 4 stellate neurons and ‘lateral’ inputs from other pyramidal cells within layer 2/3. Circuits in layer 2/3 of <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref> show our proposal for how Markov chain sequences are implemented in biology. The green pyramidal neurons with blue outlines and blue axons correspond to Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e117" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1000532-g004">Figure 4</xref> and the green pyramidal neurons with magenta outlines correspond to Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e118" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1000532-g004">Figure 4</xref>. The axons from these pyramidal cells cross column boundaries and connect to other pyramidal neurons that belong to the same sequence. Since these connections correspond to learned sequence memories, they will be very precise about which columns and which specific neurons within these columns they target.</p>
</list-item><list-item>
<p><bold>Pyramidal cells that project to higher order cortex:</bold> The next issue we want to address is how the Markov chain identities are sent to higher level cortical regions. We see several possibilities. One is to use a second set of pyramidal cells in layer 2/3. These pyramidal cells correspond to the Markov chain identities and get excitatory inputs from the layer 2/3 pyramidal cells that belong to the same Markov chain. This second set of pyramidal neurons in layer 2/3 corresponds to the rectangle neurons in <xref ref-type="fig" rid="pcbi-1000532-g004">Figure 4</xref>. These neurons send their outputs to higher-level cortical regions. In <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>, these pyramidal neurons are shown in blue color in layer 2/3 and send their axons down to the white matter to reach higher cortical areas. The second proposal does not require a second set of neurons but instead relies on long lasting metabotropic responses of neurons. The cells in layer 3 which represent the individual elements in Markov chains will become active in turn as sequences are learned and recalled. We need a way of generating a constant response that persists as the individual sequence elements are traversed. If the layer 3 cells that represent the sequence elements project to metabotropic receptors in higher cortical regions, those destination neurons could stay active for the duration of sequences. Strong evidence suggesting which of these two, or other, mechanisms is used is lacking. It is a strong theoretical prediction that a mechanism must exist in each region of cortex for forming constant representations for sequences of activity. It is an area for further study to determine what is the most likely mechanism for this.</p>
</list-item><list-item>
<p><bold>Pyramidal cells for computing sequences based on feedback:</bold> In the <xref ref-type="sec" rid="s2">Model</xref> section, we saw that a second set of dynamic programming states were required for the calculation of the belief of coincidence patterns and as an intermediate step in deriving the feedback messages to be sent to the children. These neurons do the sequence computations while integrating feedback information from the higher layers. We propose a third set of pyramidal neurons in layer 2/3 for this purpose. These neurons correspond to the filled-circle neurons in <xref ref-type="fig" rid="pcbi-1000532-g005">Figure 5</xref>. In <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>, these neurons are represented using yellow colored pyramidal neurons in layer 2/3. The lateral connections of these neurons are similar to the lateral connections of the layer 2/3 green pyramids that we just described. However, these yellow layer 2/3 neurons also integrate feedback information from layer 1 using their apical dendrites in layer 1 as shown in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>. A prediction arising from this mechanism is that two classes of neurons in layer 2/3 can be differentiated by the connections they make in layer 1. One class of layer 2/3 neuron will form layer 1 synapses with feedback axons from higher levels of cortex. The other class of layer 2/3 neuron will not form synapses with feedback axons, but will form synapses with axons from non-specific thalamic cells needed for timing (discussed more in a later section).</p>
</list-item></list>
<p>Now, let us examine the anatomical evidence that led us to these proposals. The major bottom-up input required for the above calculations is the feed-forward probability over coincidence patterns that was assigned to layer 4 neurons in a previous section. The major excitatory projection of layer 4 neurons is to layer 2/3 neurons <xref ref-type="bibr" rid="pcbi.1000532-Thomson3">[45]</xref>. For example, L4 spiny neurons in the barrel cortex of the mouse are characterized by mainly vertically-oriented, predominantly intra-columnar, axons that target layer 2/3 pyramidal cells <xref ref-type="bibr" rid="pcbi.1000532-Lubke1">[46]</xref>. Note that the green and yellow neurons in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref> receive inputs from layer 4 neurons that are in the same column.</p>
<p>Cells in layer 2/3 are known to be ‘complex’ cells that respond to sequence of motion or cells that respond invariantly to different translations of the same feature. Unlike cells in layer 4 that respond to more impoverished stimuli, cells in layer 2/3 of the visual and barrel cortices strongly prefer richer stimuli, such as motion in the preferred direction <xref ref-type="bibr" rid="pcbi.1000532-Hirsch1">[47]</xref>. This is consistent with our proposal that most layer 2/3 cells represent different coincidence patterns in the context of different Markov chain sequences. They become most active only in the context of the correct sequence. In biology, it is found that axons of the layer 2/3 pyramidal neurons travel several millimeters parallel to the layer 2/3 – layer 4 boundary and re-enter layer 2/3 to make excitatory connections to pyramidal cells there <xref ref-type="bibr" rid="pcbi.1000532-Bannister1">[43]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-Lund1">[48]</xref>. This is akin to the blue and magenta axons that we show in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref> and is consistent with the implementation of sequence memories and dynamic programming computations. The green neurons and the yellow neurons in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref> correspond to this description and are assumed to encode states within sequences.</p>
<p>We show green and yellow layer 2/3 neurons in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref> because we need to learn two sets of sequences. One set of sequences is used in feed-forward calculations and the other set of sequences is used in feedback calculations. In our figures the green neurons are feed-forward and the yellow neurons feedback. The yellow neurons need to have apical dendrites in layer 1 to receive feedback from higher cortical areas. The green neurons may also have apical dendrites in layer 1 to receive timing information. But the green feed-forward neurons should not make connections with the feedback signal. This is a theoretical prediction currently without experimental data for support or falsification.</p>
<p>The computation that the sequence state cells in layer 2/3 need to perform for inference involves a weighted sum of their lateral connections multiplied by a bottom-up input. We found several data points suggesting that neurons in layer 2/3 are capable of approximating a similar computation. Yoshimura et al <xref ref-type="bibr" rid="pcbi.1000532-Yoshimura1">[49]</xref> report that long distance horizontal connections to pyramidal cells in layer 2/3 exhibit different properties than those from vertical connections. They found that, under depolarized conditions, the EPSP evoked by the activation of an individual input pathway (either horizontal or vertical, but not both) was smaller than that evoked without the depolarization. They also found that when both the vertical and horizontal inputs were driven simultaneously, the evoked EPSP was larger than the mathematical summation of the individual EPSPs. They concluded that this indicated multiplicative supralinear summation of EPSPs caused by simultaneous activation of long range horizontal and vertical inputs under depolarized conditions, and suggested that the observed nonlinear summation is attributable to the intrinsic membrane properties of the pyramidal cells or the synaptic properties of the inputs, rather than the properties of the global neuronal circuitry. Another study <xref ref-type="bibr" rid="pcbi.1000532-Feldmeyer1">[50]</xref> suggested that the projections of layer 4 spiny neurons to layer 2/3 pyramidal neurons act as a gate for the lateral spread of excitation in layer 2/3.</p>
<p>Our model requires that sequences at higher levels of the hierarchy represent longer durations of time. The difference in temporal scales can be orders of magnitude depending on the depth of the hierarchy. In the <xref ref-type="sec" rid="s2">Model</xref> section, we outlined how variable durations can be encoded in the same sequence circuit by maintaining lateral inputs to the post-synaptic neurons for a temporal window. The biological mechanisms underlying such maintained activity is not well understood. One possibility is that these activities are mediated by pre-synaptic calcium <xref ref-type="bibr" rid="pcbi.1000532-Mongillo1">[51]</xref>. The layer 2/3 circuit that we described can be thought of as a minimal set of circuits that are needed for temporal inference on multiple scales. If intrinsic properties of neurons are not adequate to represent the longer time scales required by our model, it can be achieved via additional network mechanisms. A network mechanism to this effect is described in a subsequent section.</p>
<p>To calculate the belief in a coincidence pattern, the outputs of all the yellow neurons in the same column have to be summed up. This corresponds to pooling the evidence for that coincidence pattern from all the different Markov chains (sequences) in which the coincidence participates. Layer 5 is ideally suited for doing this. It is known that layer 2/3 pyramidal cell axons have two distinct projection fields: one horizontal (the long range axon collaterals), and one vertical <xref ref-type="bibr" rid="pcbi.1000532-Lubke1">[46]</xref>. The horizontal, trans-columnar connections target other layer 2/3 pyramidal cells <xref ref-type="bibr" rid="pcbi.1000532-Feldmeyer2">[52]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-Holmgren1">[53]</xref> and correspond to the sequence memory circuits that were described above. Both the green neurons and the yellow neurons in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref> take part in these circuits, with the yellow neurons receiving feedback information as well. It is known that the trans-laminar projections of layer 2/3 neurons are to a class of cells known as layer 5-B <xref ref-type="bibr" rid="pcbi.1000532-Thomson2">[44]</xref>. It is also known that layer 3 pyramidal cells that connect to layer 5 cells have their apical dendrites in layer 1. The projections from layer 3 to layer 5 are confined to the same column <xref ref-type="bibr" rid="pcbi.1000532-Lubke1">[46]</xref>. In the next section we will see that this is consistent with our proposal for the belief calculation cells in layer 5.</p>
</sec><sec id="s3b5">
<title>Layer 5: Implementation of belief calculation</title>
<p>We propose that a class of layer 5 pyramidal neurons in cortical circuits calculate the belief over coincidence patterns according to Equation 6. This corresponds to the computations performed by the pentagonal neurons in <xref ref-type="fig" rid="pcbi-1000532-g005">Figure 5</xref>. In the biological implementation shown in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>, these neurons are shown in light cyan color in layer 5. These neurons receive inputs from the yellow neurons in layer 2/3. Logically, the operation of these layer 5 belief neurons corresponds to the pooling of evidence for a particular coincidence from the different sequences that this coincidence participates in.</p>
</sec><sec id="s3b6">
<title>Layer 5 pyramidal cells for duration models</title>
<p>As mentioned in the <xref ref-type="sec" rid="s2">Model</xref> section, a method of encoding time duration is needed in memorizing and recalling sequences within the Markov chains. Exactly how this is done is not critical to the main ideas in this paper. However, the biological possibilities for encoding duration are somewhat limited and one possible implementation suggests itself. In this section we explore this mechanism starting with some assumptions that led to it.</p>
<p>Our model makes the assumption that cortical circuits store duration of individual elements within sequences and that the mechanism used to store duration must be present in many if not all cortical areas. Further, a human can store specific durations, such as duration of notes in music, that last up to about a second. This is too long for typical process delay times in neurons, suggesting the existence of a separate duration encoding mechanism. Humans also have the ability to speed up and slow down memorized sequences during recall, which suggests a partially centralized mechanism that can influence the rate of recall over multiple elements in a sequence. Duration information must also be available over broad areas of cortex so that duration information can be associated between any subsequent elements in a Markov chain. And finally, encoding duration between elements in a sequence requires a signal that marks when a new element has started. This suggests the need for cells with a brief burst of activity. When we looked for anatomical data that satisfied these constraints, the cortical projections to and from non-specific thalamic cells were the best fit.</p>
<p>In the proposed circuit, layer 5 pyramidal cells remember the precise time at which a belief is going to be active as measured as a duration from the previous element in the sequence. These neurons, shown as the dark cyan neurons in the layer 5 of <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>, correspond to the rounded-rectangle neurons in <xref ref-type="fig" rid="pcbi-1000532-g007">Figure 7</xref>. The timing signal, assumed to be broadly available in layer 1, is shown as blue colored axons. The dark cyan timing neurons have their apical dendrites in layer 1 to access this timing signal. It is assumed that the belief-timing neurons project to non-specific thalamic regions (the thalamic matrix) <xref ref-type="bibr" rid="pcbi.1000532-Jones1">[54]</xref> which implement a variable delay mechanism that projects back to layer 1 to complete a timing loop, as shown in <xref ref-type="fig" rid="pcbi-1000532-g007">Figure 7</xref>. LaBerge's <xref ref-type="bibr" rid="pcbi.1000532-LaBerge1">[55]</xref> research has identified the recurrent connection from layer-5 to the matrix thalamus to the apical dendrites of layer 2/3 and layer 5 neurons as the circuit responsible for sustaining activity for extended durations to support cue-target delay tasks. The connections through the matrix thalamus have also been proposed as a mechanism for thalamo-cortical synchrony <xref ref-type="bibr" rid="pcbi.1000532-Jones1">[54]</xref>.</p>
<p>Now let us examine the anatomical evidence for these neurons and connections. There are primarily two kinds of pyramidal neurons in layer 5 of the cortex. The first type are called ‘regular-spiking’ (RS) neurons and the second type are called ‘intrinsically bursting’ (IB) neurons. The IB cells are larger, they extend apical dendrites into layer 1, and as their name suggests they exhibit a burst of action potentials when they become active. The RS cells are smaller, their apical dendrites are mostly in superficial layer 4, and they exhibit a stream of action potentials when active. It is also known that the RS cells are mostly pre-synaptic to the IB cells <xref ref-type="bibr" rid="pcbi.1000532-Bannister1">[43]</xref>. That is, RS cells send their outputs to IB cells. In our mapping in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>, the RS cells are the light-cyan colored neurons in layer 5. The IB cells are the dark-cyan colored neurons in layer 5 with their apical dendrites in layer 1. The output of the RS cell goes to the IB cell. These mappings are consistent with anatomical data <xref ref-type="bibr" rid="pcbi.1000532-Thomson1">[42]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-Thomson3">[45]</xref>.</p>
<p>Most of the excitatory connections from the layer 2/3 pyramidal cells (the yellow neurons in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>) to layer 5 go to the IB cells <xref ref-type="bibr" rid="pcbi.1000532-Thomson1">[42]</xref>. This layer 2/3 input, plus the apical dendrite extending to layer 1, and the bursting behavior, suggest the IB cells are ideally situated for both the pooling of evidence and encoding the beginning (and hence timing) of a sequence element.</p>
<p>What role then might the RS cells play? In our survey, we could not find detailed information about the inputs to RS cells. The existence of RS cells can be justified if there is utility in representing a belief in a coincidence pattern that does not incorporate precise timing information. Introspection leads us to believe that there is indeed the need for such a neuron. Consider the case of listening to music. We anticipate which note is going to happen next, well before it happens. The RS cells in layer 5 can be thought of as belief cells that ‘anticipate’ the occurrence of the belief, whereas the IB cells represent the same belief at a precise time point.</p>
<p>The RS cells are known to project to sub-cortical areas like the striatum and the superior colliculus <xref ref-type="bibr" rid="pcbi.1000532-Thomson1">[42]</xref> where the anticipation signal could be used to anticipate actions. The IB neurons of layer 5 also project to sub-cortical areas and to motor areas. If a cortical area is to influence actions, it makes sense that the signals for that should be based on the belief of that cortical area, because the belief represents the best possible information about the coincidence patterns represented in that cortical area. Therefore, the fact that layer 5 RS neurons and IB neurons project to sub-cortical areas that influence motor actions is consistent with the proposal that they compute the belief.</p>
<p>The timing loop requires the projection of the IB neurons to an external timing circuit. Hawkins <xref ref-type="bibr" rid="pcbi.1000532-Hawkins1">[7]</xref> has proposed the projections of IB cells to the non-specific thalamus as the mechanism for generating a variable timing signal. Non-specific thalamic cells were recommended for this role because they have the required connectivity, receiving input from layer 5 cells and projecting broadly back to layer 1. Beyond this connectivity there is nothing else to support this conjecture. Our literature search and private conversations with several thalamic anatomists have yielded no evidence of anyone ever recording from non-specific thalamic cells. For this conjecture to be true we would expect to see something like a cascade of non-specific cells become active in sequence in response to a burst on a layer 5 IB cell. The cascade would last for several hundred milliseconds.</p>
<p>We can imagine alternate mechanisms for encoding duration. For example, the cerebellum is known to encode specific timing and its connectivity to the thalamus suggests it could play this role. However, the layer 1 synapses of layer 5 IB cells appear to be in a more efficient and logical location for storing duration in that these synapses are directly connected to the beliefs being calculated in layer 5. Again the result of this paper, the mapping of a hierarchical belief propagation model onto cortical anatomy, does not depend on how duration is encoded, but it does require it is encoded by some means that can handle time scales varying in orders of magnitude. LaBerge's research <xref ref-type="bibr" rid="pcbi.1000532-LaBerge1">[55]</xref> provides a summary of arguments for the hypothesis that a recurrent cortico-thalamo-cortical circuit as proposed here can provide stable levels of modulatory activity at the soma of cortical pyramidal neurons that can persist over extended periods of time. However, it is noted that the biophysical and network mechanisms underlying persistent temporal representations is still an area of active research.</p>
</sec><sec id="s3b7">
<title>Layer 6: Computing the feedback messages for children</title>
<p>We assign to layer 6 pyramidal neurons the role of computing the feedback messages that are to be sent to regions that are hierarchically below. This corresponds to the hexagonal neurons in <xref ref-type="fig" rid="pcbi-1000532-g006">Figure 6</xref> and Equation 9 in <xref ref-type="table" rid="pcbi-1000532-t001">Table 1</xref>. In <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>, these are shown as the purple colored neurons in layer 6. Feedback messages are derived from the results of the belief calculations from a set of columns. This means that the layer 6 neurons will receive inputs from the layer 5 neurons involved in the calculation of beliefs. This is shown in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>. A given set of columns will send feedback messages to all its ‘child regions’. The feedback message sent to one child is not the same as the feedback message sent to the other child. In <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>, some of the layer 6 neurons project to the left child while the rest project to the right child.</p>
<p>Layer 6 is known to be a primary source of cortical feedback connections <xref ref-type="bibr" rid="pcbi.1000532-Douglas1">[15]</xref>. There is a class of pyramidal neurons in layer 6 that have short dendritic tufts extending primarily to layer 5. The axons of these neurons project cortico-cortically <xref ref-type="bibr" rid="pcbi.1000532-Thomson1">[42]</xref> in a feedback direction. Hence they are appropriately situated for calculating the feedback messages and their connectivity is consistent with our proposals for other layers. Note that in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>, the axonal inputs to layer 6 neurons from layer 5 neurons cross several columns. The input connections to a layer 6 cell come from the columns corresponding to the coincidence patterns that have the child nodes Markov chain as a component.</p>
<p>In <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>, different layer 6 neurons project to different child nodes. An alternative implementation is for these neurons to be located in layer 2 of the respective child nodes. This implementation has the advantage that the higher-level node can send the same feedback signal to all the child nodes. In either case, the input connections to these neurons represent the participation of the child node's Markov chain in the higher-level node's coincidence patterns. These connections will need to be learned through the simultaneous activation of the bottom-up outputs from children with top-down outputs from the parent.</p>
<p>There are several other neuron types that are identified in layer 6. We do not attempt to explain the functions of those neurons. However, it is worth mentioning that some of the layer 6 cortical circuits already identified by other neuroscientists as possible candidates for the gating of feedforward activation (control of attention) <xref ref-type="bibr" rid="pcbi.1000532-Guillery1">[56]</xref>–<xref ref-type="bibr" rid="pcbi.1000532-Rees1">[59]</xref> are compatible with our model. According to these studies, cells in layer 6 of V1 provide extensive feedback connections to the LGN of the thalamus. These feedback connections target the distal dendrites of relay cells and also contact inhibitory interneurons. The same layer 6 cells also send collateral axons to layer 4 where LGN afferents contact the cortex.</p>
<p>The connections that a set of layer 6 neurons makes to layer 4 and the thalamus are ideally suited for the attention control mechanism that we outlined for the HTM belief propagation. We described how the belief responses generated at every level can be used for gating feed-forward evidence in accordance with a top-level hypothesis. These signals need to be passed through control mechanisms that will maintain the gating while further bottom-up and top-down propagation for the attended to stimuli alter the belief neuron responses. A layer 6 neuron that receives input from layer 5 belief cells is ideally situated for this purpose. Why would layer 6 neurons feed back to layer 4 and also to thalamus? One possible explanation is that each connection provides a different kind of attention modulation. For example, the layer 4 connection could be for attending to the coincidence pattern corresponding to the currently active belief and the thalamus connection could be for attending to every coincidence pattern that is not part of the current belief. These conjectures about layer 6 cells need further research and refinement.</p>
</sec><sec id="s3b8">
<title>Exceptions from model</title>
<p>The six-layered cortical architecture we have described so far is most typical of sensory regions of cortex. Many variations in cortical architecture are known to exist, such as variations in the density and type of cells in a layer, and variations in the number of layers. In addition, there are many known common features of cortical architecture that are not explicitly addressed in our model. Included in this category are the previously mentioned cell types in layer 6 and all the classes of inhibitory cells. These variations and omissions are not necessarily at odds with the model presented in this paper. Not all functions of the belief propagation equations have to be implemented exclusively in one layer. Some layers, such as layer 3 and layer 4 may both be implementing feed-forward coincidence detection and grouping but over differing spatial and temporal resolutions, which could explain why layer 4 typically gets less prominent as you ascend the cortical hierarchy. Different cell types may be needed for short term memory (not included in our model) and different types of attention. Inhibitory cells are needed to implement learning. These topics are beyond the scope of this paper.</p>
<p>Given the behavioral flexibility and resilience of the cortex, we should expect some flexibility in the mapping between a theoretical model and its anatomical instantiation. If our model required a precise and unwavering mapping onto many unique cell types and their connections it is unlikely such a system could evolve. However, we suggest that the mapping of our model to cortical anatomy is sufficiently constrained to suggest its validity and provide testable predictions, but not so constrained to forbid useful variations in its biological implementation. The basic model can remain intact even though variations in timing mechanisms, attention mechanisms, motor mechanisms, etc. are expressed in variations in cortical architecture.</p>
</sec><sec id="s3b9">
<title>Summary</title>
<p>A summary of the proposed computational roles is given in <xref ref-type="table" rid="pcbi-1000532-t003">Table 3</xref>.</p>
<table-wrap id="pcbi-1000532-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.t003</object-id><label>Table 3</label><caption>
<title>Summary of anatomical features and their proposed computational functions.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000532-t003-3" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.t003" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">#</td>
<td align="left" colspan="1" rowspan="1">Anatomical feature</td>
<td align="left" colspan="1" rowspan="1">Proposed computational role</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">Feed-forward thalamic projection to layer 4.</td>
<td align="left" colspan="1" rowspan="1">Storage and detection of coincidence patterns.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">2</td>
<td align="left" colspan="1" rowspan="1">Layer 4 cell dendrites are mostly within layer 4. These cells make vertical projections to layers 2 and 3.</td>
<td align="left" colspan="1" rowspan="1">Bottom-up inputs required for the sequence likelihood calculation in equation.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">3</td>
<td align="left" colspan="1" rowspan="1">Layer 3 cells with inter-columnar lateral projections to other layer 2/3 cells. Some of these cells send their outputs to higher order cortex.</td>
<td align="left" colspan="1" rowspan="1">Calculation of sequence likelihoods for feed-forward and feedback calculations.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">4</td>
<td align="left" colspan="1" rowspan="1">Layer 5 cells with apical dendrites in the superficial layer 4 and bottom of layer 3.</td>
<td align="left" colspan="1" rowspan="1">Belief calculation without specific timing.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">5</td>
<td align="left" colspan="1" rowspan="1">Layer 5 cells with apical dendrites in layer 1. These send outputs to subcortical regions and non-specific thalamus.</td>
<td align="left" colspan="1" rowspan="1">Belief calculation with specific timing.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">6</td>
<td align="left" colspan="1" rowspan="1">Layer 6 neurons with apical dendrites in layer 5.</td>
<td align="left" colspan="1" rowspan="1">Computation of feedback messages for child regions.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">7</td>
<td align="left" colspan="1" rowspan="1">Projections to layer 1 from higher level regions and from non-specific thalamic cells.</td>
<td align="left" colspan="1" rowspan="1">High level input is feedback information. Non-specific thalamic input is timing information for Markov chains.</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt101"><label/><p>See <xref ref-type="bibr" rid="pcbi.1000532-Thomson1">[42]</xref> for details on the anatomical features summarized in this table.</p></fn></table-wrap-foot></table-wrap></sec></sec><sec id="s3c">
<title>Object recognition experiments using HTMs</title>
<p>Although the main purpose of this paper is the exposition of HTM theory and its connection to biology, we believe it is useful to discuss our work applying HTMs to practical problems. In this section, we summarize the results of the work being done at Numenta in applying HTMs to the problem of visual object recognition. A detailed treatment of this topic is beyond the scope of this paper.</p>
<p>We started by applying HTMs to a line drawing recognition problem that we call the <italic>Pictures</italic> problem. The Pictures data set consists of line drawings of 48 categories of objects. These line drawings are shown in <xref ref-type="supplementary-material" rid="pcbi.1000532.s003">Figure S1</xref>. Each pattern is of size 32 pixels by 32 pixels. The goal was to train an HTM network to recognize test patterns with translations, severe distortions, scale and aspect ratio changes, clutter and noise. The Pictures data set has some properties that make it attractive for applying HTMs. Most objects occupy only a fraction of the 32×32 pixel input. This enables the creation of test images with large translations and scale variations while still maintaining the 32×32 pixel input dimensions. The objects are of different sizes. Some objects (for example, the ‘dog’) contain other objects (the ‘cat’). Most of the objects are constructed from the same set of local features. This means that techniques that use local features alone are not adequate to recognize these objects. The spatial configuration of the local features (i.e, the shape) is important. Recognizing test patterns despite translations, distortions and clutter is a challenging task even on this seemingly simple data set.</p>
<p>We found that HTM network hierarchies with four levels work best for the Pictures task. Adding more levels did not help in improving the recognition accuracy on our test set. The HTM networks are trained in a level-by-level manner, starting with the coincidence patterns and Markov chains at the first level and then moving up the hierarchy. During training, the network is shown programmatically constructed movies in which the objects undergo translations and scale variations in a smooth manner. The training strategy we outlined in the <xref ref-type="sec" rid="s2">Model</xref> section was used for learning the coincidence patterns and Markov chains. More details about the training methods and the learned coincidence patterns and Markov chains can be found in <xref ref-type="bibr" rid="pcbi.1000532-George1">[8]</xref>. A representative set of learned Markov chains is shown in <xref ref-type="supplementary-material" rid="pcbi.1000532.s004">Figure S2</xref>. A challenging test set was created by programmatically distorting the training images and by adding noise. Examples of test images for the ‘table lamp’ category are shown in <xref ref-type="supplementary-material" rid="pcbi.1000532.s005">Figure S3</xref>. The HTM networks reported in our previous work <xref ref-type="bibr" rid="pcbi.1000532-George2">[9]</xref> used Markov chains based temporal pooling only at level 1 of the hierarchy and gave 49% recognition accuracy on this test set. We found that incorporating Markov chains based temporal pooling at higher levels increased the recognition accuracy on test sets to 72%. In comparison, a nearest neighbor classifier using exactly the training paradigm used to train the top level of the HTM gives only 35% accuracy. A stand-alone demonstration of this project that lets users interactively draw images to test the network is included with the NuPIC software available for download from Numenta's website (<ext-link ext-link-type="uri" xlink:href="http://www.numenta.com" xlink:type="simple">http://www.numenta.com</ext-link>). The network performs impressively in qualitative testing. The Pictures demo, data set, and parameter files are supplied as part of the NuPIC software available from Numenta.</p>
<p>We modified the network structure while maintaining the same spatial and temporal learning/inference algorithms to create an HTM network that can recognize grayscale images. In this network, the first level of coincidences were replaced with Gabor filters of different orientations. At all levels, the coincidence patterns were restricted to have spatial receptive fields smaller than that of the Markov chains. With these modifications, we could successfully train several gray scale image recognition networks. On the standard Caltech-101 benchmark <xref ref-type="bibr" rid="pcbi.1000532-FeiFei1">[60]</xref>, our initial experiments with the network achieved 50% recognition accuracy with 15 training images and 62% recognition accuracy with 30 training images. We used a simple nearest neighbor classifier at the top of the hierarchy. Experiments on the Caltech-101 dataset were performed primarily to make sure that we are within the range of reported accuracies. We share many of the concerns expressed by Pinto et al <xref ref-type="bibr" rid="pcbi.1000532-Pinto1">[61]</xref> that the Caltech-101 data set and the associated train/test protocols are not sufficiently informative of the overall recognition capability of a system. For this reason, we did not spend time optimizing the performance of our networks for this data set.</p>
<p>Caltech-101 images have low intra-category variation. Most of the images are centered and approximately of the same size. To see whether our system can handle large intra-category variations in gray-scale images, including translations and scale variations, we trained a network with 4 categories of images. These categories had a large amount of intra-category variation. The top of the network was exposed to over 10000 different training images. <xref ref-type="supplementary-material" rid="pcbi.1000532.s006">Figure S4</xref> shows some examples of training images and <xref ref-type="supplementary-material" rid="pcbi.1000532.s007">Figure S5</xref> shows some examples of test images for this network. On a hold out set, this network gave 92% accuracy. We also found that the network performs impressively in qualitative testing. A stand-alone demonstration of this network that lets users test their own images under different transformations is available for download from Numenta's website (<ext-link ext-link-type="uri" xlink:href="http://www.numenta.com/about-numenta/demoapps.php" xlink:type="simple">http://www.numenta.com/about-numenta/demoapps.php</ext-link>). We are also happy to note that researchers outside Numenta have had success training recognition systems using HTMs. A case study on recognizing architecture drawings, including detailed parameter files for NuPIC software, is available at <ext-link ext-link-type="uri" xlink:href="http://www.numenta.com/links/vision_exp.php" xlink:type="simple">http://www.numenta.com/links/vision_exp.php</ext-link></p>
<p>We have done a small set of experiments exploring the use of temporal information during inference. These experiments were performed on the Pictures data set. During inference, the network was shown a sequence of images. The first level of the network used the sequential information to compute the likelihood of Markov chains according to the equations we described in the <xref ref-type="sec" rid="s2">Model</xref> section. We measured the recognition accuracy, on a frame-by-frame basis, while playing short (4 time frames) of translating inputs in a noisy background. The temporal boundaries where the input switched from one category to another were not marked or transmitted to the network. The recognition accuracy of the network that used temporal inference was up to 30% higher compared to the recognition accuracy obtained by a sliding window averaging (window length = 4) of frame-by-frame instantaneous recognition. More details on this experiment is available on Numenta's website (<ext-link ext-link-type="uri" xlink:href="http://www.numenta.com/links/tbi_overview.php" xlink:type="simple">http://www.numenta.com/links/tbi_overview.php</ext-link>). This experiment is also available as part of the NuPIC software from Numenta. We have not done any studies incorporating temporal inference for grayscale image recognition or incorporating it at multiple levels of the hierarchy. These topics are currently under investigation and development.</p>
<p>We have also done experiments using feedback propagation in HTMs. The goal of these experiments was to verify that top-down propagation in HTMs can be used to locate and segment out objects in cluttered scenes with multiple objects. <xref ref-type="fig" rid="pcbi-1000532-g011">Figure 11</xref> shows the results of inference and top-down propagation in a network that was trained on eight categories of images. During training, the objects were shown in isolation on a clean background. The test images contained multiple novel objects superposed on busy backgrounds. In most cases, one of the objects in the test image was the top result in the inference. Feedback propagation is initiated from the top of the network after the first flow of feed-forward propagation. After bottom-up propagation, the belief vector at the top of the network is modified such that the winning coincidence has strength one and all other coincidences have strength zero. This message is then propagated down in the network by combining with bottom-up information in the rest of the levels of the hierarchy. The resultant image obtained at the lowest level of the network isolates the contours of the recognized image from the background clutter and from other objects in the scene. These experiments show how top-down propagation in the current model can be used for segmentation, for the assignment of border-ownership, and for the ‘binding’ of features corresponding to a top-level hypothesis <xref ref-type="bibr" rid="pcbi.1000532-Bartels1">[62]</xref>. More examples of top-down propagation are available at <ext-link ext-link-type="uri" xlink:href="http://www.numenta.com/links/top_down.php" xlink:type="simple">http://www.numenta.com/links/top_down.php</ext-link></p>
<fig id="pcbi-1000532-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g011</object-id><label>Figure 11</label><caption>
<title>Top-down segmentation.</title>
<p>Figures A and B show the effect of top-down propagation in HTM networks. The top half of each figure shows the original image submitted to the HTM, along with blue bars illustrating the recognition scores on the top five of the eight categories on which the network was trained. The bottom-left panel in each figure shows the input image after Gabor filtering. The bottom-right panel in each figure shows the image obtained after the feedback propagation of the winning category at the top of the HTM network. In these Gabor-space images, the colors illustrate different orientations, but the details of the color map are not pertinent. A). The input image has a car superposed on background clutter. The network recognizes the car. Top-down propagation segments out the car's contours from that of the background. B). The input image contains multiple objects superposed on a cluttered background and with some foreground occlusions. The network recognition result identifies teddy bear as the top category. Feedback propagation of this winning category correctly isolates the contours corresponding to the teddy bear.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g011" xlink:type="simple"/></fig></sec><sec id="s3d">
<title>Example application: a model for the subjective contour effect</title>
<p>The cortical circuit described in this paper can be used for studying and modeling physiological phenomena. In this section, we report some preliminary positive results that we obtained modeling the <italic>subjective contour</italic> effect in visual inference <xref ref-type="bibr" rid="pcbi.1000532-Kanizsa1">[63]</xref> using these circuits. The primary goal of this section is to serve as a proof of concept for the possible applications of the circuit model. A detailed investigation of the subjective contours effect is beyond the scope of this paper.</p>
<p>The subjective contour effect is a well known cognitive and physiological phenomenon. <xref ref-type="fig" rid="pcbi-1000532-g012">Figure 12</xref> shows examples of Kanizsa diagrams that produce this effect. When viewing such diagrams, humans perceive edges even in regions where there is no direct visual evidence for edges. Lee and Nguyen <xref ref-type="bibr" rid="pcbi.1000532-Lee2">[64]</xref> found that neurons in area V1 responded to such illusory contours even though their feed-forward receptive fields do not have any evidence supporting the presence of a line. In addition to finding the neurons in V1 that respond to the illusory contours, Lee and Nguyen also studied the temporal dynamics of their responses. The summary of their findings is that the population averaged response to illusory contours emerged 100 milliseconds after stimulus onset in the superficial layers of V1 and at approximately 120 to 190 millisecond in the deep layers. The responses to illusory contours in area V2 occurred earlier, at 70 milliseconds in the superficial layers and at 95 milliseconds in the deep layers. These findings suggest that top-down feedback is used in the generation of illusory contours.</p>
<fig id="pcbi-1000532-g012" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g012</object-id><label>Figure 12</label><caption>
<title>Kanizsa diagrams.</title>
<p>A Kanizsa square (left) and a Kanizsa triangle (right) are shown.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g012" xlink:type="simple"/></fig>
<p>In <xref ref-type="bibr" rid="pcbi.1000532-Lee1">[1]</xref>, Lee and Mumford suggested this could be the result of Bayesian computations. Their argument was that the presented stimulus, according to the statistics of the visual world, is adequate to create a high-level hypothesis of the rectangle even though the edges are missing. The activation of this global hypothesis, at areas V2 and above, in turn constrains the activity of lower level neurons through the feedback messages. The HTM theory provides a mechanism for training a visual cortical hierarchy and the HTM circuit model gives a detailed anatomical circuit that can be used to test this hypothesis.</p>
<sec id="s3d1">
<title>Subjective contour effect in HTMs</title>
<p>We used Numenta's NuPIC software environment to train a visual pattern recognition HTM network on which we tested the subjective contour effect. We started with an HTM network that was trained to recognize four different categories of objects: binoculars, cars, cell phones, and rubber ducks. This network had a three level HTM hierarchy. <xref ref-type="fig" rid="pcbi-1000532-g013">Figure 13</xref> shows examples of training and testing images for these categories. When presented with a test image, the output from the top-level node is a distribution that indicates the network certainty in different categories. In addition to recognizing input patterns, the HTM network can also propagate information down in the hierarchy using the belief propagation techniques that we described in earlier sections. Feeding information back in the hierarchy is used to segment the object from clutter and to locate the object in the image. More details about the training process for HTMs is available in <xref ref-type="bibr" rid="pcbi.1000532-George4">[27]</xref> and in <xref ref-type="bibr" rid="pcbi.1000532-George1">[8]</xref>.</p>
<fig id="pcbi-1000532-g013" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g013</object-id><label>Figure 13</label><caption>
<title>Examples of training and testing images for an HTM network trained for visual object recognition.</title>
<p>The top two rows are examples of training images. The bottom three rows are examples of correctly recognized test images. The last row shows test images that incorporated distracter backgrounds.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g013" xlink:type="simple"/></fig>
<p>In this example the network recognizes a static visual image. This is a special case of the dynamic programming computations we described in that it uses only a single instant in time for inference. (An example that uses temporal inference to recognize time-varying patterns is available as part of the NuPIC software from Numenta. More details on this example are available at <ext-link ext-link-type="uri" xlink:href="http://www.numenta.com/for-developers/education/tbi-overview.php" xlink:type="simple">http://www.numenta.com/for-developers/education/tbi-overview.php</ext-link>.) HTMs need time-varying patterns to learn, and the general mode of operation is to perform inference on time-varying test patterns. However, in some problem domains such as image recognition, there is often sufficient information to perform inference without using time-changing patterns. In such cases, correct recognition can be obtained by a single feed forward pass through the network. This is consistent with observations about the speed of processing in the human visual system <xref ref-type="bibr" rid="pcbi.1000532-Thorpe1">[65]</xref>.</p>
<p>In order to perform the subjective contours experiment, we trained this network on an additional category: rectangles. This was done by presenting the network with a few images of rectangles during the training session. Only intact rectangle images were shown during training. The network then recognized novel rectangles of different aspect ratios.</p>
<p>We then tested the network on a Kanizsa square test pattern. <xref ref-type="fig" rid="pcbi-1000532-g014">Figure 14</xref> shows the response of the network to the Kanizsa square test pattern. The network classifies this pattern as a rectangle, even though this type of pattern was not seen during training. We examined the network for the presence of illusory contour responses. Illusory contour responses are characterized by top-down activations with no bottom-up activation. We used the capability of Numenta's software to inspect the node states of a network to probe for illusory contour responses. <xref ref-type="fig" rid="pcbi-1000532-g015">Figure 15</xref> shows the feed-forward and feedback inputs to nodes at 4 different locations. The subjective contour effect can be seen in <xref ref-type="fig" rid="pcbi-1000532-g015">Figure 15(C)</xref>. There are no actual contours in the receptive field of this node. Therefore, the feed-forward input of this node is zero. However, the feedback input is nonzero because the network expects the edges of a rectangle. This is the subjective contour effect.</p>
<fig id="pcbi-1000532-g014" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g014</object-id><label>Figure 14</label><caption>
<title>Recognition of the Kanizsa square by an HTM network.</title>
<p>The network was not shown Kanizsa squares during training. The bar graph displays the order of recognition certainty of the HTM.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g014" xlink:type="simple"/></fig><fig id="pcbi-1000532-g015" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g015</object-id><label>Figure 15</label><caption>
<title>Subjective contour effect in HTM</title>
<p>. Feed-forward and feedback inputs of 4 different nodes at level 1 of the HTM network for the Kanizsa rectangle stimulus. Four figures, (a) to (d), are shown corresponding to 4 different nodes from which the responses are recorded. In each figure, the left top panel is the input stimulus and the left bottom panel is the input stimulus as seen by the network after Gabor filtering. In these panels, the receptive field of the HTM node is indicated using a small blue square. In each figure, the top-right panel shows the feed-forward input to the node and the bottom-right panel shows the feedback input to the node. The feed-forward inputs correspond to the activity on thalamo-cortical projections. The feedback inputs correspond to the activations of the layer 6 cells that project backward from the higher level in the hierarchy. (a) The receptive field of this node does not contain any edges. There is no feed-forward input and no feedback input. (b) The receptive field of this node has a real contour in its input field. The node has both feed-forward and feedback inputs. (c) The subjective contour node. The receptive field of this node has no real contours. Therefore, the feed-forward input is zero. However, the feedback input is not zero because the network expects the edges of a rectangle. This is the subjective contour effect. (d) The opposite of the subjective contour effect. In this case, a real contour is present in the receptive field of this node but it does not contribute to the high-level perception of the rectangle. Hence the feedback input to this node is zero even though the feed-forward response is non-zero.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g015" xlink:type="simple"/></fig>
<p>We did an additional experiment where we presented a corrupted Kanizsa square identical to one of the control experiments used in <xref ref-type="bibr" rid="pcbi.1000532-Lee2">[64]</xref>. As shown in <xref ref-type="fig" rid="pcbi-1000532-g016">Figure 16</xref>, the corrupted rectangle produces a subjective contour response similar to, but substantially weaker than, the one produced by an intact Kanizsa figure. This is consistent with the results that Nguyen and Lee saw in monkeys. In our experiment the corrupted figure was recognized as a rectangle at the top of the network, albeit with a lower level of certainty. This lower level of certainty is reflected in the lower activation level of the subjective contour. Had we put a threshold on the strength of recognition at the top level to filter out input images that were not close to any category, we could have reduced the subjective contour response to close to zero.</p>
<fig id="pcbi-1000532-g016" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000532.g016</object-id><label>Figure 16</label><caption>
<title>Reduced subjective contour effect.</title>
<p>When presented with a corrupted version of a Kanizsa rectangle, the HTM still recognizes a rectangle but with reduced certainty. Shown are the feed-forward and feedback inputs to a node analogous to <xref ref-type="fig" rid="pcbi-1000532-g015">Figure 15(C)</xref>. The node is receiving feedback indicating the network expects an edge at this location, but the strength of this expectation is substantially reduced compared to a non-corrupted rectangle.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.g016" xlink:type="simple"/></fig>
<p>It is proposed that the delayed onset of the illusory contour response reported by Lee and Nguyen <xref ref-type="bibr" rid="pcbi.1000532-Lee2">[64]</xref> occurs because of the delays caused by propagating messages up and down in the hierarchy. Lee and Nguyen also showed that the illusory contour response occurs first in the superficial layers and then in the deep layers. This is also consistent with the cortical circuit model in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref> because the feedback information first reaches the Markov chain neurons in layer 2/3 (the yellow neurons in <xref ref-type="fig" rid="pcbi-1000532-g009">Figure 9</xref>) and then is integrated into the layer 5 neurons.</p>
<p>The subjective contour effect in our model was generated exclusively by feedback circuits. This is in contrast to models that rely only on lateral connections within a level for contour completion. For example, <xref ref-type="bibr" rid="pcbi.1000532-Williams1">[66]</xref> uses a stochastic contour completion algorithm. However, using only local information completes contours that might not be in agreement with the higher-level perception. The use of top-down information is more in agreement with visual experience and with studies that suggest that visual understanding and awareness might be needed for the perception of illusory figures <xref ref-type="bibr" rid="pcbi.1000532-Rubin1">[67]</xref>. Lee and Mumford <xref ref-type="bibr" rid="pcbi.1000532-Lee1">[1]</xref> suggested that the formation of illusory contours is primarily a top-down mechanism, as also suggested in our experiments, in combination with lateral mechanisms proposed in <xref ref-type="bibr" rid="pcbi.1000532-Williams1">[66]</xref>.</p>
<p>Using our cortical circuits theory, it is possible to study this phenomena in more detail. For example, it is possible to identify specific neurons in specific laminae and specific columns that will be active with top-down input and also to study their temporal characteristics. This is left as future work.</p>
</sec></sec></sec><sec id="s4">
<title>Discussion</title>
<p>The mathematical model and biological mapping for cortical circuits derived in this paper is a starting point to achieve the final goal of a complete understanding of cortical circuits at least in prototypical sensory areas. We see three ways of advancing the circuits derived here: (1) Incorporation of more elements of HTM theory including learning, attention, actions, and asynchronous messages between levels, (2) Incorporation of more biological data including more detailed modeling of dendritic properties, and specific inhibitory cells, and (3) Incorporation of other constraints such as wiring length optimization and ease of learning. In the following subsections we discuss how a combination of the above factors could explain many aspects of cortical circuits that are not modeled in this paper.</p>
<sec id="s4a">
<title/>
<sec id="s4a1">
<title>Learning mechanisms and inhibitory neurons</title>
<p>The circuits discussed in this paper have been mapped to the belief propagation equations in a learned HTM node. We have not discussed how the learning algorithms themselves can be analyzed for their biological plausibility. We saw in the <xref ref-type="sec" rid="s3">Results</xref> section that some of the intra-columnar vertical connections required to support the belief propagation equations can be pre-wired because these connections do not depend on external stimuli. Most other connections, the ones representing sequence memories and coincidence patterns, are learned. Learning these connections requires mechanisms that support competition, inhibition and online learning.</p>
<p>Throughout this paper we have adopted the common assumption that excitatory neurons provide the prominent information-processing pathway and that inhibitory neurons largely play a supporting role in implementing the learning algorithms. This assumption is partially based on the fact that most of the inter-laminar and long distance connections within a cortical area are provided by the spiny excitatory neurons, whereas the smooth inhibitory interneurons more prevalently connect locally within their layer of origin. It is the excitatory cells that connect long distance in both vertical and lateral dimensions and their activity is then molded by local inhibitory neurons <xref ref-type="bibr" rid="pcbi.1000532-Douglas1">[15]</xref>. It is expected that inhibitory neurons will play a prominent role when biologically realistic mechanisms are considered for the learning of the HTM node states. Inhibitory mechanisms are required for competition during learning. Inhibitory neurons could also be required for avoiding instabilities produced by positive feedback loops.</p>
</sec><sec id="s4a2">
<title>Overlapping nodes and sparse representations</title>
<p>The HTM nodes described in this paper are shown as discrete entities with abrupt boundaries, which does not correspond to biology where overlapping receptive fields and imprecise boundaries are commonly found. The idealized HTM node instantiation gives us the flexibility to create mathematical abstractions that can be analyzed; however, it needs to be modified to make a full biological correspondence. One way to accommodate this could be to use HTM nodes with heavily overlapped input fields to construct a region. With overlapped input fields, the resultant network, viewed as a Bayesian network, has cycles in it. Although theoretical guarantees do not exist for the convergence of belief propagation in such hierarchies, successful systems have been built based on belief propagation in loopy graphs <xref ref-type="bibr" rid="pcbi.1000532-Frey1">[32]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-Murphy1">[33]</xref> and our limited experience with implementing overlapping input fields have similarly shown no tendency for loop-induced failures.</p>
<p>The HTM model in this paper uses sparse-distributed representations <xref ref-type="bibr" rid="pcbi.1000532-Sallee1">[68]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-Wersing1">[69]</xref> when considering the representations within an entire hierarchical level. However, it does not use sparse-distributed representations within a node. In domains where a node is exposed to data that has rich characteristics, the model would require modifications to include sparse-distributed representations within an HTM node. This can be achieved by relaxing the assumption that a node represents a set of mutually exclusive hypotheses. We have made some recent progress with this formulation. However, this is beyond the scope of this paper.</p>
</sec><sec id="s4a3">
<title>Cortical maps</title>
<p>The circuits derived here attempt to explain only the information processing in a learned model. Any spatial arrangement of the columns of the circuit that preserves the connections between columns would still do the same information processing. Hence, the circuits here provide no explanation for observed cortical maps <xref ref-type="bibr" rid="pcbi.1000532-Wandell1">[70]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-Swindale1">[71]</xref>.</p>
<p>We can think of several plausible reasons for the existence of cortical maps that are consistent with the circuits in this paper. One reason is that organizing the columns in a particular manner in space could reduce wiring length or some other resource that needs to be conserved. Another reason could be that a topographical organization of “similar” patterns could reduce the search space for coincidence-detection and sequence-learning algorithms. Circuits for implementing self organizing map <xref ref-type="bibr" rid="pcbi.1000532-Kohonen1">[72]</xref> algorithms need to be incorporated into the theory. This work is left for the future.</p>
</sec><sec id="s4a4">
<title>Asynchronous message passing</title>
<p>The belief propagation messages in this paper were derived under the simplifying assumption that child-node states change synchronously. This assumption made the derivations and the circuits easier to understand. Our preliminary investigations indicate that relaxing this assumption may require additional communication between hierarchical levels which may explain the role of some of the layer 6 cells.</p>
</sec><sec id="s4a5">
<title>Attention mechanisms</title>
<p>As mentioned earlier, the circuits derived in this paper do not incorporate a detailed mechanism for attention control. Hypothesis-driven attention is an important aspect of perception and plays an important role in belief propagation as well <xref ref-type="bibr" rid="pcbi.1000532-Pearl1">[29]</xref>. It is known that thalamus plays an important role in cortico-cortical communication, acting as a dynamic control of information passed from one cortical area to another <xref ref-type="bibr" rid="pcbi.1000532-Sherman1">[57]</xref>,<xref ref-type="bibr" rid="pcbi.1000532-Olshausen1">[73]</xref>. There are multiple connections to the thalamus. There are feedback connections that control the gating of feed-forward information and the feed-forward connections through the thalamus are viewed as an alternate pathway to the direct cortico-cortical projections. There are computational reasons why all these pathways should exist. In belief propagation, the messages required for attention control are different from those of standard feedback messages. The attention control messages instantiate variables at intermediate levels and therefore affect the results of feed-forward propagation, whereas the standard feedback messages in belief propagation do not interact with feed-forward messages.</p>
<p>Some forms of attention can also be considered as an internal motor action because the attention control mechanism activates parts of the network and blocks the other parts. In that sense, the attention control mechanism can also be thought of as analogous to the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000532.e119" xlink:type="simple"/></inline-formula> operator proposed by Pearl <xref ref-type="bibr" rid="pcbi.1000532-Pearl2">[74]</xref> to model the effect of actions in causal Bayesian networks. It is a tantalizing clue that Guillery and Sherman <xref ref-type="bibr" rid="pcbi.1000532-Guillery2">[75]</xref> found that the layer 5 pyramidal cells that project to the pulvinar of the thalamus also project to motor structures. Incorporation of the attention pathway into the derived circuits is left for future research.</p>
</sec><sec id="s4a6">
<title>Neuron biophysics and dendritic properties</title>
<p>Much is known about the properties and biophysics of dendrites, dendritic action potentials, and the biochemical pathways related to synapses <xref ref-type="bibr" rid="pcbi.1000532-Koch1">[76]</xref>. The model presented here does not address most of this knowledge. We see the potential for extending the HTM model in these directions. Indeed, we believe the best way to understand the detailed properties of neurons is within the context of a larger scale theoretical framework.</p>
</sec><sec id="s4a7">
<title>Predictions of the theory</title>
<p>In this section we give a brief summary of potential predictions that can be generated from the theory.</p>
<list list-type="bullet"><list-item>
<p>Different cells in layer 2/3 of the same cortical column (same bottom-up feature) will become active as part of different sequential contexts.</p>
</list-item><list-item>
<p>Layer 2/3 contains two sets of pyramidal neurons. Neurons in one set deal exclusively with feed-forward processing and receive no feedback connections from higher levels and the neurons in the other set deal with the combination of feed-forward and feedback information. Unintended interactions between these two sets could produce cognitive defects.</p>
</list-item><list-item>
<p>Long-range lateral connections in layer 2/3 encode sequential information. These connections can be altered by training with temporal patterns that have different statistics.</p>
</list-item><list-item>
<p>Some vertical connections in a column pre-exist to provide a backbone for belief propagation computation. These can be wired using genetic information. We described several of these connections.</p>
</list-item><list-item>
<p>A set of layer 5 cells represent the belief that combines both top-down and bottom-up influences for a stimulus. This prediction can be tested by examining the information represented locally in the layer 5 cells after the presentation of a stimulus that is locally ambiguous but globally coherent.</p>
</list-item><list-item>
<p>Disabling feedback pathways will have the effect of confusing segmentation and border-ownership assignments. For example, subjective contour responses will get disrupted.</p>
</list-item><list-item>
<p>With an appropriately defined training paradigm, top-down signals can predict missing/occluded bottom-up information, even when it cannot be predicted by local contour continuation.</p>
</list-item><list-item>
<p>In the case of visual pattern recognition, smooth movement of the stimulus will increase recognition accuracy even in situations were background subtraction does not explain the effect. (For example, in a stimulus where background changes every instant in addition to the foreground object.)</p>
</list-item><list-item>
<p>In cases where sequential information is required to disambiguate a stimulus, responses of layer 2/3 cells will become sparser (reduced activity) as more temporal information is accumulated.</p>
</list-item></list>
</sec><sec id="s4a8">
<title>Degrees of freedom and variations</title>
<p>Even with the combination of computational constraints and available anatomical data, several degrees of freedom remain in the mapping to biology. Because of this, the mapping to biology does not produce a unique circuit.</p>
<p>One source of variation can be found at the boundaries between lamina and between hierarchical regions. Let us consider the boundary between layer 4 and layer 3. The typical picture of layer 4 neurons is that they receive inputs from the thalamus and project to layer 3. However, a layer 4 neuron that projects to layer 3 can do the same computation even if that neuron is moved to layer 3 and if it receives direct bottom-up input from the thalamus. A similar degree of freedom exists between different levels of the hierarchy. For example, a neuron in layer 6 that sends feedback information to layer 2 of a child region can actually be moved to layer 2 of the child region.</p>
<p>These variations do not violate the computational principles we described and can be thought of as variations of the same theme. We believe that computational constraints will need to be combined with resource optimization constraints and physical constraints to completely understand why biology chooses some implementations over others. For example, some of these variations could be more advantageous than others for wiring length optimization <xref ref-type="bibr" rid="pcbi.1000532-George3">[14]</xref>. It also is possible that these tradeoffs change with position in the hierarchy, the amount of convergence of bottom-up inputs, the need to send outputs and receive inputs from sub-cortical circuits, etc. The circuit derived in this paper provides a template to explore such variations.</p>
</sec></sec><sec id="s4b">
<title>Conclusion</title>
<p>In this paper we have mapped a model of how the neocortex performs inference onto neocortical anatomy. The model, called Hierarchical Temporal Memory (HTM), is a type of Bayesian network which assumes a hierarchy of nodes where each node learns spatial coincidences and then learns a mixture of Markov models over the set of coincidences. The hierarchy of the model corresponds to the hierarchy of cortical regions. The nodes in the model correspond to small regions of cortex. We performed the mapping to biology in two stages. Starting with a mathematical expression of how each node performs inference, we created an abstract neuronal implementation. Next we mapped this abstract implementation onto observed anatomical data of cell types, cell layers, and micro-circuits in the cortex. We also showed results of an experiment where an HTM-based vision system exhibited the effects of illusory contours.</p>
<p>There are many unknowns and variations in cortical anatomy, and similarly there are many functions of the neocortex that are not accounted for by the HTM model. However, we believe the theoretical and anatomical constraints are sufficiently strong that the merger of the two is non-trivial and instructive. The ultimate goal of our work is to have a theoretical model of neocortex sufficiently tied to biological data so that the biology can lead to refinements of the theory, and the theory can lead to testable predictions about the biology. The work we have done, including that in this paper, suggests HTM is a good starting point for such a biologically grounded neocortical model.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1000532.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.s001" xlink:type="simple"><label>Text S1</label><caption>
<p>Derivation of belief propagation in HTM networks</p>
<p>(0.29 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000532.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.s002" xlink:type="simple"><label>Text S2</label><caption>
<p>A toy example for belief propagation in HTM networks</p>
<p>(0.24 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000532.s003" mimetype="image/png" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.s003" xlink:type="simple"><label>Figure S1</label><caption>
<p>The Pictures data set. The Pictures data set consists of 48 categories of binary line drawings. An example of each category is shown in the figure. Images are of size 32 pixels by 32 pixels. Training sequenes for HTMs are generated by animating these binary images with smooth translations and scale variations.</p>
<p>(0.05 MB PNG)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000532.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.s004" xlink:type="simple"><label>Figure S2</label><caption>
<p>Learned Markov chain temporal groups. Figure shows a subset of the Markov chain temporal groups learned at the first level of the Pictures HTM network. The rows correspond to different Markov chains. The states of the Markov chains are shown as two-dimensional representations of their corresponding coincidence patterns. The connectivity between the elements of the Markov chains are not shown. The states within a Markov chain are perceptually similar even though their corresponding coincidence patterns are not similar in the pixel space.</p>
<p>(0.04 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000532.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.s005" xlink:type="simple"><label>Figure S3</label><caption>
<p>Test examples for the table lamp category. These test images were generated by programmatically modifying the training images through translations, aspect ratio changes, pixel deletions and insertion of noise pixels.</p>
<p>(0.09 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000532.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.s006" xlink:type="simple"><label>Figure S4</label><caption>
<p>Examples of grayscale training images. Figure shows examples of the training images used for training a 4 category HTM network. Most training images had an uncluttered background. The images presented to the network were of size 200 pixels by 200 pixels. The training images have a large amount of intra category variation in shape. In addition, the network was trained to recognize translations and scale variations of these categories.</p>
<p>(1.98 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000532.s007" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000532.s007" xlink:type="simple"><label>Figure S5</label><caption>
<p>Test images. Examples of test images used for the 4 category gray scale network. The test images were novel examples with significant variations in size and location in addition to the presence of background clutter.</p>
<p>(1.09 MB PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We would like to thank Bruno Olshausen for his helpful comments on the manuscript. We would like to thank all the Numenta employees involved in the creation of the NuPIC software used for implementing the experiments in this paper. Ron Marianetti implemented the feedback/attention pathway in NuPIC that was then adapted for the experiments in the paper. James Niemasik implemented the inspectors used for displaying the results. We would like to thank the anonymous reviewers for their critical insights and helpful comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1000532-Lee1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lee</surname><given-names>TS</given-names></name>
<name name-style="western"><surname>Mumford</surname><given-names>D</given-names></name>
</person-group>             <year>2003</year>             <article-title>Hierarchical Bayesian inference in the visual cortex.</article-title>             <source>Journal of the Optical Society of America</source>             <volume>2</volume>             <fpage>1434</fpage>             <lpage>1448</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Hegde1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hegde</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Felleman</surname><given-names>DJ</given-names></name>
</person-group>             <year>2007</year>             <article-title>Reappraising the functional implications of the primate visual anatomical hierarchy.</article-title>             <source>Neuroscientist</source>             <volume>13</volume>             <fpage>416</fpage>             <lpage>421</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Rao1"><label>3</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name>
</person-group>             <year>2005</year>             <article-title>Hierarchical Bayesian inference in networks of spiking neurons.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Saul</surname><given-names>LK</given-names></name>
<name name-style="western"><surname>Weiss</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Bottou</surname><given-names>L</given-names></name>
</person-group>             <source>Advances in Neural Information Processing Systems 17</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000532-Deneve1"><label>4</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Deneve</surname><given-names>S</given-names></name>
</person-group>             <year>2005</year>             <article-title>Bayesian inference in spiking neurons, MIT Press.</article-title>             <fpage>353</fpage>             <lpage>360</lpage>             <comment>Advances in Neural Information Processing Systems 17</comment>          </element-citation></ref>
<ref id="pcbi.1000532-Friston1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
</person-group>             <year>2003</year>             <article-title>Learning and inference in the brain.</article-title>             <source>Neural Networks</source>             <volume>16</volume>             <fpage>1325</fpage>             <lpage>1352</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Friston2"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
</person-group>             <year>2008 Nov</year>             <article-title>Hierarchical models in the brain.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <fpage>e1000211</fpage>          </element-citation></ref>
<ref id="pcbi.1000532-Hawkins1"><label>7</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hawkins</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Blakeslee</surname><given-names>S</given-names></name>
</person-group>             <year>2004</year>             <source>On Intelligence</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Henry Holt and Company</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000532-George1"><label>8</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>George</surname><given-names>D</given-names></name>
</person-group>             <year>2008</year>             <article-title>How the Brain Might Work: A Hierarchical and Temporal Model for Learning and Recognition.</article-title>             <comment>Ph.D. thesis, Stanford University</comment>          </element-citation></ref>
<ref id="pcbi.1000532-George2"><label>9</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>George</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Hawkins</surname><given-names>J</given-names></name>
</person-group>             <year>2005</year>             <article-title>A hierarchical Bayesian model of invariant pattern recognition in the visual cortex.</article-title>             <fpage>1812</fpage>             <lpage>1817</lpage>             <comment>In: Proceedings of the International Joint Conference on Neural Networks, IEEE, volume 3</comment>          </element-citation></ref>
<ref id="pcbi.1000532-Grossberg1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name>
</person-group>             <year>2007</year>             <article-title>Towards a unified theory of neocortex: laminar cortical circuits for vision and cognition.</article-title>             <source>Prog Brain Res</source>             <volume>165</volume>             <fpage>79</fpage>             <lpage>104</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Granger1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Granger</surname><given-names>R</given-names></name>
</person-group>             <year>2006</year>             <article-title>Engines of the brain: The computational instruction set of human cognition.</article-title>             <source>AI Magazine</source>             <volume>27</volume>             <fpage>15</fpage>             <lpage>31</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Martin1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Martin</surname><given-names>KAC</given-names></name>
</person-group>             <year>2002</year>             <article-title>Microcircuits in visual cortex.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>12</volume>             <fpage>418</fpage>             <lpage>425</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Rao2"><label>13</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name>
</person-group>             <year>2005</year>             <article-title>Hierarchical Bayesian Inference in Networks of Spiking Neurons, MIT Press.</article-title>             <fpage>1113</fpage>             <lpage>1120</lpage>             <comment>Advances in Neural Information Processing Systems 17</comment>          </element-citation></ref>
<ref id="pcbi.1000532-George3"><label>14</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>George</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Hawkins</surname><given-names>J</given-names></name>
</person-group>             <year>2005</year>             <article-title>Belief propagation and wiring length optimization as organization principles for cortical microcircuits.</article-title>             <comment>Technical report, Redwood Neuroscience Institute</comment>          </element-citation></ref>
<ref id="pcbi.1000532-Douglas1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>Martin</surname><given-names>KA</given-names></name>
</person-group>             <year>2004</year>             <article-title>Neuronal circuits of the neocortex.</article-title>             <source>Annual Review of Neuroscience</source>             <volume>27</volume>             <fpage>419</fpage>             <lpage>451</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Bobier1"><label>16</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bobier</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Wirth</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Content-based image retrieval using hierarchical temporal memory.</article-title>             <source>Proceeding of the 16th ACM international conference on Multimedia</source>             <publisher-loc>New York, NY, USA</publisher-loc>             <publisher-name>ACM</publisher-name>             <fpage>925</fpage>             <lpage>928</lpage>             <comment>URL <ext-link ext-link-type="uri" xlink:href="http://portal.acm.org/citation.cfm?id=1459359.1459523" xlink:type="simple">http://portal.acm.org/citation.cfm?id=1459359.1459523</ext-link></comment>          </element-citation></ref>
<ref id="pcbi.1000532-Vutsinas1"><label>17</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Vutsinas</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Taha</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Rice</surname><given-names>K</given-names></name>
</person-group>             <year>2008</year>             <article-title>A neocortex model implementation on reconfigurable logic with streaming memory.</article-title>             <fpage>1</fpage>             <lpage>8</lpage>             <comment>In: Parallel and Distributed Processing, 2008. IPDPS 2008. IEEE International Symposium on</comment>          </element-citation></ref>
<ref id="pcbi.1000532-Sun1"><label>18</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sun</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Lee</surname><given-names>S</given-names></name>
</person-group>             <year>2008</year>             <article-title>Power system security pattern recognition based on phase space visualization.</article-title>             <fpage>964</fpage>             <lpage>969</lpage>             <comment>In: Electric Utility Deregulation and Restructuring and Power Technologies, 2008. DRPT 2008. Third International Conference on</comment>          </element-citation></ref>
<ref id="pcbi.1000532-Csapo1"><label>19</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Csapo</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Baranyi</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Tikk</surname><given-names>D</given-names></name>
</person-group>             <year>2007</year>             <article-title>Object categorization using vfa-generated nodemaps and hierarchical temporal memories.</article-title>             <fpage>257</fpage>             <lpage>262</lpage>             <comment>In: Computational Cybernetics, 2007. ICCC 2007. IEEE International Conference on</comment>          </element-citation></ref>
<ref id="pcbi.1000532-Donoho1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Donoho</surname><given-names>D</given-names></name>
</person-group>             <year>2006</year>             <article-title>Compressed sensing.</article-title>             <source>Information Theory, IEEE Transactions on</source>             <volume>52</volume>             <fpage>1289</fpage>             <lpage>1306</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Riesenhuber1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name>
</person-group>             <year>1999</year>             <article-title>Hierarchical models of object recognition in cortex.</article-title>             <source>Nature Neuroscience</source>             <volume>2</volume>             <fpage>1019</fpage>             <lpage>1025</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-MarcAurelioRanzato1"><label>22</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Marc'Aurelio Ranzato</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Boureau</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name>
</person-group>             <year>2007</year>             <article-title>Unsupervised learning of invariant feature hierarchies with applications to object recognition.</article-title>             <source>In: Proceedings of the International Conference on Computer Vision and Pattern Recognition. IEEE Press</source>          </element-citation></ref>
<ref id="pcbi.1000532-Mitchison1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mitchison</surname><given-names>G</given-names></name>
</person-group>             <year>1991</year>             <article-title>Removing time variation with the anti-Hebbian differential synapse.</article-title>             <source>Neural Computation</source>             <volume>3</volume>             <fpage>312</fpage>             <lpage>320</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Fldik1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Földiák</surname><given-names>P</given-names></name>
</person-group>             <year>1991</year>             <article-title>Learning invariance from transformation sequences.</article-title>             <source>Neural Computation</source>             <volume>3</volume>             <fpage>194</fpage>             <lpage>200</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Wiskott1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name>
</person-group>             <year>2002</year>             <article-title>Slow feature analysis: Unsupervised learning of invariances.</article-title>             <source>Neural Computation</source>             <volume>14</volume>             <fpage>715</fpage>             <lpage>770</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Stringer1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Stringer</surname><given-names>SM</given-names></name>
<name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name>
</person-group>             <year>2002</year>             <article-title>Invariant object recognition in the visual system with novel views of 3 d objects.</article-title>             <source>Neural Comput</source>             <volume>14</volume>             <fpage>2585</fpage>             <lpage>2596</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-George4"><label>27</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>George</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Jaros</surname><given-names>B</given-names></name>
</person-group>             <year>2007</year>             <source>The HTM Learning Algorithms</source>             <publisher-loc>Menlo Park, CA</publisher-loc>             <publisher-name>Numenta, Inc</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000532-Hawkins2"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hawkins</surname><given-names>J</given-names></name>
<name name-style="western"><surname>George</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Niemasik</surname><given-names>J</given-names></name>
</person-group>             <year>2009</year>             <article-title>Sequence memory for prediction, inference and behaviour.</article-title>             <source>Philosophical Transactions B</source>             <volume>364</volume>             <fpage>1203</fpage>          </element-citation></ref>
<ref id="pcbi.1000532-Pearl1"><label>29</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pearl</surname><given-names>J</given-names></name>
</person-group>             <year>1988</year>             <source>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</source>             <publisher-loc>San Francisco, California</publisher-loc>             <publisher-name>Morgan Kaufmann</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000532-Bellman1"><label>30</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bellman</surname><given-names>R</given-names></name>
</person-group>             <year>1957</year>             <source>Dynamic Programming</source>             <publisher-name>Princeton University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000532-Howard1"><label>31</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Howard</surname><given-names>RA</given-names></name>
</person-group>             <year>1960</year>             <source>Dynamic Programming and Markov Processes</source>             <publisher-loc>Cambridge, Massachusetts</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000532-Frey1"><label>32</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Frey</surname><given-names>BJ</given-names></name>
<name name-style="western"><surname>MacKay</surname><given-names>DJC</given-names></name>
</person-group>             <year>1998</year>             <article-title>A revolution: Belief propagation in graphs with cycles.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name>
<name name-style="western"><surname>Kearns</surname><given-names>MJ</given-names></name>
<name name-style="western"><surname>Solla</surname><given-names>SA</given-names></name>
</person-group>             <source>Advances in Neural Information Processing Systems</source>             <publisher-name>The MIT Press, volume 10</publisher-name>             <comment>URL <ext-link ext-link-type="uri" xlink:href="http://citeseer.ist.psu.edu/frey97revolution.html" xlink:type="simple">citeseer.ist.psu.edu/frey97revolution.html</ext-link></comment>          </element-citation></ref>
<ref id="pcbi.1000532-Murphy1"><label>33</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Murphy</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Weiss</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Jordan</surname><given-names>M</given-names></name>
</person-group>             <year>2000</year>             <article-title>Loopy-belief propagation for approximate inference: An empirical study.</article-title>             <fpage>467</fpage>             <lpage>475</lpage>             <source>In: Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann</source>          </element-citation></ref>
<ref id="pcbi.1000532-Levinson1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Levinson</surname><given-names>S</given-names></name>
</person-group>             <year>Apr 1986</year>             <article-title>Continuously variable duration hidden markov models for speech analysis. Acoustics, Speech, and Signal Processing.</article-title>             <source>IEEE International Conference on ICASSP '86</source>             <volume>11</volume>             <fpage>1241</fpage>             <lpage>1244</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Rabiner1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rabiner</surname><given-names>LR</given-names></name>
</person-group>             <year>1989</year>             <article-title>A tutorial on hidden Markov models and selected applications in speech recognition.</article-title>             <source>Proceedings of the IEEE</source>             <volume>77</volume>             <fpage>257</fpage>             <lpage>286</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Hubel1"><label>36</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hubel</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Wensveen</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Wick</surname><given-names>B</given-names></name>
</person-group>             <year>1988</year>             <source>Eye, brain, and vision</source>             <publisher-name>Scientific American Library New York</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000532-Hasson1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hasson</surname><given-names>U</given-names></name>
<name name-style="western"><surname>Yang</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Vallines</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Heeger</surname><given-names>DJ</given-names></name>
<name name-style="western"><surname>Rubin</surname><given-names>N</given-names></name>
</person-group>             <year>2008</year>             <article-title>A hierarchy of temporal receptive windows in human cortex.</article-title>             <source>J Neurosci</source>             <volume>28</volume>             <fpage>2539</fpage>             <lpage>2550</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Tsunoda1"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tsunoda</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Yamane</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Nishizaki</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Tanifuji</surname><given-names>M</given-names></name>
</person-group>             <year>2001</year>             <article-title>Complex objects are represented in macaque inferotemporal cortex by the combination of feature columns.</article-title>             <source>Nat Neurosci</source>             <volume>4</volume>             <fpage>832</fpage>             <lpage>838</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Churchland1"><label>39</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Churchland</surname><given-names>PS</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name>
</person-group>             <year>1992</year>             <source>The Computational Brain</source>             <publisher-loc>Cambridge, Massachusetts</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000532-Rakic1"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rakic</surname><given-names>P</given-names></name>
</person-group>             <year>1988</year>             <article-title>Specification of cerebral cortical areas.</article-title>             <source>Science</source>             <volume>241</volume>             <fpage>170</fpage>             <lpage>176</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Mountcastle1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mountcastle</surname><given-names>VB</given-names></name>
</person-group>             <year>2003</year>             <article-title>Introduction to the special issue on computation in cortical columns.</article-title>             <source>Cerebral Cortex</source>             <volume>13</volume>             <fpage>2</fpage>             <lpage>4</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Thomson1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Thomson</surname><given-names>AM</given-names></name>
<name name-style="western"><surname>Bannister</surname><given-names>AP</given-names></name>
</person-group>             <year>2003</year>             <article-title>Interlaminar connections in the neocortex.</article-title>             <source>Cerebral cortex (New York, NY : 1991)</source>             <volume>13</volume>             <fpage>5</fpage>             <lpage>14</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Bannister1"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bannister</surname><given-names>PA</given-names></name>
</person-group>             <year>2005</year>             <article-title>Inter- and intra-laminar connections of pyramidal cells in the neocortex.</article-title>             <source>Neurosci Res</source>             <volume>53</volume>             <fpage>95</fpage>             <lpage>103</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Thomson2"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Thomson</surname><given-names>AM</given-names></name>
<name name-style="western"><surname>Bannister</surname><given-names>AP</given-names></name>
</person-group>             <year>1998</year>             <article-title>Postsynaptic pyramidal target selection by descending layer iii pyramidal axons: dual intracellular recordings and biocytin filling in slices of rat neocortex.</article-title>             <source>Neuroscience</source>             <volume>84</volume>             <fpage>669</fpage>             <lpage>683</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Thomson3"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Thomson</surname><given-names>AM</given-names></name>
<name name-style="western"><surname>Lamy</surname><given-names>C</given-names></name>
</person-group>             <year>2007</year>             <article-title>Functional maps of neocortical local circuitry.</article-title>             <source>Front Neurosci</source>             <volume>1</volume>             <fpage>19</fpage>             <lpage>42</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Lubke1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lubke</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Feldmeyer</surname><given-names>D</given-names></name>
</person-group>             <year>2007</year>             <article-title>Excitatory signal flow and connectivity in a cortical column: focus on barrel cortex.</article-title>             <source>Brain Struct Funct</source>             <volume>212</volume>             <fpage>3</fpage>             <lpage>17</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Hirsch1"><label>47</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hirsch</surname><given-names>JA</given-names></name>
<name name-style="western"><surname>Martinez</surname><given-names>LM</given-names></name>
</person-group>             <year>2006</year>             <article-title>Laminar processing in the visual cortical column.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>16</volume>             <fpage>377</fpage>             <lpage>384</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Lund1"><label>48</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lund</surname><given-names>JS</given-names></name>
<name name-style="western"><surname>Angelucci</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Bressloff</surname><given-names>PC</given-names></name>
</person-group>             <year>2003</year>             <article-title>Anatomical substrates for functional columns in macaque monkey primary visual cortex.</article-title>             <source>Cerebral cortex (New York, NY : 1991)</source>             <volume>13</volume>             <fpage>15</fpage>             <lpage>24</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Yoshimura1"><label>49</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Yoshimura</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Sato</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Imamura</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Watanabe</surname><given-names>Y</given-names></name>
</person-group>             <year>2000</year>             <article-title>Properties of horizontal and vertical inputs to pyramidal cells in the superficial layers of the cat visual cortex.</article-title>             <source>Journal of Neuroscience</source>             <volume>20</volume>             <fpage>1931</fpage>          </element-citation></ref>
<ref id="pcbi.1000532-Feldmeyer1"><label>50</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Feldmeyer</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Lubke</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Silver</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Sakmann</surname><given-names>B</given-names></name>
</person-group>             <year>2002</year>             <article-title>Synaptic connections between layer 4 spiny neurone-layer 2/3 pyramidal cell pairs in juvenile rat barrel cortex: physiology and anatomy of interlaminar signalling within a cortical column.</article-title>             <source>The Journal of Physiology</source>             <volume>538</volume>             <fpage>803</fpage>             <lpage>822</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Mongillo1"><label>51</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mongillo</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Barak</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Synaptic theory of working memory.</article-title>             <source>Science</source>             <volume>319</volume>             <fpage>1543</fpage>          </element-citation></ref>
<ref id="pcbi.1000532-Feldmeyer2"><label>52</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Feldmeyer</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Lubke</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Sakmann</surname><given-names>B</given-names></name>
</person-group>             <year>2006</year>             <article-title>Efficacy and connectivity of intracolumnar pairs of layer 2/3 pyramidal cells in the barrel cortex of juvenile rats.</article-title>             <source>J Physiol</source>             <volume>575</volume>             <fpage>583</fpage>             <lpage>602</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Holmgren1"><label>53</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Holmgren</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Harkany</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Svennenfors</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Zilberter</surname><given-names>Y</given-names></name>
</person-group>             <year>2003</year>             <article-title>Pyramidal cell communication within local networks in layer 2/3 of rat neocortex.</article-title>             <source>J Physiol</source>             <volume>551</volume>             <fpage>139</fpage>             <lpage>153</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Jones1"><label>54</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Jones</surname><given-names>E</given-names></name>
</person-group>             <year>2001</year>             <article-title>The thalamic matrix and thalamocortical synchrony.</article-title>             <source>Trends in Neurosciences</source>             <volume>24</volume>             <fpage>595</fpage>             <lpage>601</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-LaBerge1"><label>55</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>LaBerge</surname><given-names>D</given-names></name>
</person-group>             <year>2005</year>             <article-title>Sustained attention and apical dendrite activity in recurrent circuits.</article-title>             <source>Brain Research Reviews</source>             <volume>50</volume>             <fpage>86</fpage>             <lpage>99</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Guillery1"><label>56</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Guillery</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Feig</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Lozsadi</surname><given-names>D</given-names></name>
</person-group>             <year>1998</year>             <article-title>Paying attention to the thalamic reticular nucleus.</article-title>             <source>Trends in Neurosciences</source>             <volume>21</volume>             <fpage>28</fpage>             <lpage>31</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Sherman1"><label>57</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sherman</surname><given-names>SM</given-names></name>
<name name-style="western"><surname>Guillery</surname><given-names>RW</given-names></name>
</person-group>             <year>2002</year>             <article-title>The role of the thalamus in the flow of information to the cortex.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>357</volume>             <fpage>1695</fpage>             <lpage>1708</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Sillito1"><label>58</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sillito</surname><given-names>AM</given-names></name>
<name name-style="western"><surname>Cudeiro</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Jones</surname><given-names>HE</given-names></name>
</person-group>             <year>2006</year>             <article-title>Always returning: feedback and sensory processing in visual cortex and thalamus.</article-title>             <source>Trends Neurosci</source>             <volume>29</volume>             <fpage>307</fpage>             <lpage>316</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Rees1"><label>59</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rees</surname><given-names>G</given-names></name>
</person-group>             <year>2009/03/10</year>             <article-title>Visual attention: The thalamus at the centre?</article-title>             <source>Current Biology</source>             <volume>19</volume>             <fpage>R213</fpage>             <lpage>R214</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-FeiFei1"><label>60</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fei-Fei</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Fergus</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Perona</surname><given-names>P</given-names></name>
</person-group>             <year>2006</year>             <article-title>One-shot learning of object categories.</article-title>             <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>             <volume>28</volume>             <fpage>594</fpage>             <lpage>611</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Pinto1"><label>61</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pinto</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Cox</surname><given-names>DD</given-names></name>
<name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name>
</person-group>             <year>2008</year>             <article-title>Why is real-world visual object recognition hard?</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <fpage>e27</fpage>          </element-citation></ref>
<ref id="pcbi.1000532-Bartels1"><label>62</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bartels</surname><given-names>A</given-names></name>
</person-group>             <year>2009</year>             <article-title>Visual Perception: Converging Mechanisms of Attention, Binding, and Segmentation?</article-title>             <source>Current Biology</source>             <volume>19</volume>             <fpage>300</fpage>             <lpage>302</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Kanizsa1"><label>63</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kanizsa</surname><given-names>G</given-names></name>
</person-group>             <year>1976</year>             <article-title>Subjective contours.</article-title>             <source>Sci Am</source>             <volume>234</volume>             <fpage>48</fpage>             <lpage>52</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Lee2"><label>64</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lee</surname><given-names>TS</given-names></name>
<name name-style="western"><surname>Nguyen</surname><given-names>M</given-names></name>
</person-group>             <year>2001</year>             <article-title>Dynamics of subjective contour formation in the early visual cortex.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>98</volume>             <fpage>1907</fpage>             <lpage>1911</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Thorpe1"><label>65</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Thorpe</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Fize</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Marlot</surname><given-names>C</given-names></name>
</person-group>             <year>1996</year>             <article-title>Speed of processing in the human visual system.</article-title>             <source>Nature</source>             <volume>381</volume>             <fpage>520</fpage>             <lpage>522</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Williams1"><label>66</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Williams</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Jacobs</surname><given-names>D</given-names></name>
</person-group>             <year>1997</year>             <article-title>Stochastic completion fields: A neural model of illusory contour shape and salience.</article-title>             <source>Neural Computation</source>             <volume>9</volume>             <fpage>837</fpage>             <lpage>858</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Rubin1"><label>67</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rubin</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Nakayama</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Shapley</surname><given-names>R</given-names></name>
</person-group>             <year>1997</year>             <article-title>Abrupt learning and retinal size specificity in illusory-contour perception.</article-title>             <source>Current Biology</source>             <volume>7</volume>             <fpage>461</fpage>             <lpage>467</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Sallee1"><label>68</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sallee</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name>
</person-group>             <year>2004</year>             <article-title>Learning sparse multiscale image representations.</article-title>             <fpage>1327</fpage>             <lpage>1334</lpage>             <comment>In: Advances in Neural Information Processing Systems. volume 16</comment>          </element-citation></ref>
<ref id="pcbi.1000532-Wersing1"><label>69</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wersing</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Korner</surname><given-names>E</given-names></name>
</person-group>             <year>2003</year>             <article-title>Learning optimized features for hierarchical models of invariant object recognition.</article-title>             <source>Neural computation</source>             <volume>15</volume>             <fpage>1559</fpage>             <lpage>1588</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Wandell1"><label>70</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wandell</surname><given-names>BA</given-names></name>
<name name-style="western"><surname>Dumoulin</surname><given-names>SO</given-names></name>
<name name-style="western"><surname>Brewer</surname><given-names>AA</given-names></name>
</person-group>             <year>2007</year>             <article-title>Visual field maps in human cortex.</article-title>             <source>Neuron</source>             <volume>56</volume>             <fpage>366</fpage>             <lpage>383</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Swindale1"><label>71</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Swindale</surname><given-names>NV</given-names></name>
</person-group>             <year>2000</year>             <article-title>How many maps are there in visual cortex?</article-title>             <source>Cereb Cortex</source>             <volume>10</volume>             <fpage>633</fpage>             <lpage>643</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Kohonen1"><label>72</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kohonen</surname><given-names>T</given-names></name>
</person-group>             <year>1982</year>             <article-title>Self-organized formation of topologically correct feature maps.</article-title>             <source>Biological Cybernetics</source>             <volume>43</volume>             <fpage>59</fpage>             <lpage>69</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Olshausen1"><label>73</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name>
<name name-style="western"><surname>Anderson</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Van Essen</surname><given-names>DC</given-names></name>
</person-group>             <year>1993</year>             <article-title>A neurobiological model of visual attention and pattern recognition based on dynamic routing of information.</article-title>             <source>Journal of Neuroscience</source>             <volume>13</volume>             <fpage>4700</fpage>             <lpage>4719</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Pearl2"><label>74</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pearl</surname><given-names>J</given-names></name>
</person-group>             <year>2000</year>             <source>Causality : Models, Reasoning, and Inference</source>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000532-Guillery2"><label>75</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Guillery</surname><given-names>RW</given-names></name>
<name name-style="western"><surname>Sherman</surname><given-names>SM</given-names></name>
</person-group>             <year>2002</year>             <article-title>The thalamus as a monitor of motor outputs.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>357</volume>             <fpage>1809</fpage>             <lpage>1821</lpage>          </element-citation></ref>
<ref id="pcbi.1000532-Koch1"><label>76</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>
</person-group>             <year>2004</year>             <source>Biophysics of Computation: Information Processing in Single Neurons (Computational Neuroscience Series)</source>             <publisher-loc>New York, NY, USA</publisher-loc>             <publisher-name>Oxford University Press, Inc</publisher-name>          </element-citation></ref>
</ref-list>

</back>
</article>