<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-00746</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005735</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Immunology</subject><subj-group><subject>Immune system proteins</subject><subj-group><subject>Immune receptors</subject><subj-group><subject>Pattern recognition receptors</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Immunology</subject><subj-group><subject>Immune system proteins</subject><subj-group><subject>Immune receptors</subject><subj-group><subject>Pattern recognition receptors</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Proteins</subject><subj-group><subject>Immune system proteins</subject><subj-group><subject>Immune receptors</subject><subj-group><subject>Pattern recognition receptors</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Signal transduction</subject><subj-group><subject>Immune receptors</subject><subj-group><subject>Pattern recognition receptors</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Experimental organism systems</subject><subj-group><subject>Model organisms</subject><subj-group><subject>Drosophila melanogaster</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Model organisms</subject><subj-group><subject>Drosophila melanogaster</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Experimental organism systems</subject><subj-group><subject>Animal models</subject><subj-group><subject>Drosophila melanogaster</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Arthropoda</subject><subj-group><subject>Insects</subject><subj-group><subject>Drosophila</subject><subj-group><subject>Drosophila melanogaster</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuronal tuning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Neural coding in the visual system of <italic>Drosophila melanogaster</italic>: How do small neural populations support visually guided behaviours?</article-title>
<alt-title alt-title-type="running-head">Neural coding of information for visual behaviours in <italic>Drosophila</italic></alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2432-5964</contrib-id>
<name name-style="western">
<surname>Dewar</surname> <given-names>Alex D. M.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Wystrach</surname> <given-names>Antoine</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5503-0467</contrib-id>
<name name-style="western">
<surname>Philippides</surname> <given-names>Andrew</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Graham</surname> <given-names>Paul</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Informatics, University of Sussex, Falmer, Brighton, United Kingdom</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Centre de Recherches sur la Cognition Animale, Centre National de la Recherche Scientifique, Université Paul Sabatier, Toulouse, France</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>School of Life Sciences, University of Sussex, Falmer, Brighton, United Kingdom</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bush</surname> <given-names>Daniel</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University College London, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">andrewop@sussex.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>10</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>10</day>
<month>10</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>10</issue>
<elocation-id>e1005735</elocation-id>
<history>
<date date-type="received">
<day>12</day>
<month>5</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>21</day>
<month>8</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Dewar et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005735"/>
<abstract>
<p>All organisms wishing to survive and reproduce must be able to respond adaptively to a complex, changing world. Yet the computational power available is constrained by biology and evolution, favouring mechanisms that are parsimonious yet robust. Here we investigate the information carried in small populations of visually responsive neurons in <italic>Drosophila melanogaster</italic>. These so-called ‘ring neurons’, projecting to the ellipsoid body of the central complex, are reported to be necessary for complex visual tasks such as pattern recognition and visual navigation. Recently the receptive fields of these neurons have been mapped, allowing us to investigate how well they can support such behaviours. For instance, in a simulation of classic pattern discrimination experiments, we show that the pattern of output from the ring neurons matches observed fly behaviour. However, performance of the neurons (as with flies) is not perfect and can be easily improved with the addition of extra neurons, suggesting the neurons’ receptive fields are not optimised for recognising abstract shapes, a conclusion which casts doubt on cognitive explanations of fly behaviour in pattern recognition assays. Using artificial neural networks, we then assess how easy it is to decode more general information about stimulus shape from the ring neuron population codes. We show that these neurons are well suited for encoding information about size, position and orientation, which are more relevant behavioural parameters for a fly than abstract pattern properties. This leads us to suggest that in order to understand the properties of neural systems, one must consider how perceptual circuits put information at the service of behaviour.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>A general problem in neuroscience is understanding how sensory systems organise information to be at the service of behaviour. Computational approaches can be useful for such studies as they allow one to simulate the sensory experience of a behaving animal whilst considering how sensory information should be encoded. In flies, small subpopulations of identifiable neurons are known to be necessary for particular visual tasks, and the response properties of these populations have now been described in detail. Surprisingly, these populations are small, with only 14 or 28 neurons each, which suggests something of a sensory bottleneck. In this paper, we consider how the population code from these neurons relates to the information required to control specific behaviours. We conclude that, despite previous claims, flies are unlikely to possess a general-purpose pattern-learning ability. However, implicit information about the shape and size of objects, which is necessary for many ecologically important visually guided behaviours, does pass through the sensory bottleneck. These findings show that nervous systems can be particularly economical when specific populations of cells are paired with specific visual behaviours. This is a general-interest finding for computer vision and biomimetics, as well as sensory neuroscience.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000266</institution-id>
<institution>Engineering and Physical Sciences Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>EP/P006094/1</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2432-5964</contrib-id>
<name name-style="western">
<surname>Dewar</surname> <given-names>Alex David McDonald</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000266</institution-id>
<institution>Engineering and Physical Sciences Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>EP/P006094/1</award-id>
<principal-award-recipient>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5503-0467</contrib-id>
<name name-style="western">
<surname>Philippides</surname> <given-names>Andrew</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100003135</institution-id>
<institution>Fondation Fyssen</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name name-style="western">
<surname>Wystrach</surname> <given-names>Antoine</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000268</institution-id>
<institution>Biotechnology and Biological Sciences Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>BB/F015925/1</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2432-5964</contrib-id>
<name name-style="western">
<surname>Dewar</surname> <given-names>Alex David McDonald</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award005">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000268</institution-id>
<institution>Biotechnology and Biological Sciences Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>BB/H013644</award-id>
</award-group>
<award-group id="award006">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100011102</institution-id>
<institution>Seventh Framework Programme</institution>
</institution-wrap>
</funding-source>
<award-id>308943</award-id>
<principal-award-recipient>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5503-0467</contrib-id>
<name name-style="western">
<surname>Philippides</surname> <given-names>Andrew</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>ADMD, PG and AP are funded by EPSRC (grant code: EP/P006094/1; <ext-link ext-link-type="uri" xlink:href="https://www.epsrc.ac.uk/" xlink:type="simple">https://www.epsrc.ac.uk/</ext-link>). AW is funded by the Fyssen Foundation (<ext-link ext-link-type="uri" xlink:href="http://www.fondationfyssen.fr/en/" xlink:type="simple">http://www.fondationfyssen.fr/en/</ext-link>). ADMD and PG have also received funding from the BBSRC (grant codes: BB/F015925/1 and BB/H013644; <ext-link ext-link-type="uri" xlink:href="http://www.bbsrc.ac.uk/" xlink:type="simple">http://www.bbsrc.ac.uk/</ext-link>). AP has also received funding from the European Union’s Seventh Framework Programme for research, technological development and demonstration under grant agreement no. 308943 (<ext-link ext-link-type="uri" xlink:href="https://ec.europa.eu/research/fp7/index_en.cfm" xlink:type="simple">https://ec.europa.eu/research/fp7/index_en.cfm</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="0"/>
<page-count count="21"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2017-10-20</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Data will be made available in full (via figshare, according to the funders’ and the University of Sussex’s requirements) if the manuscript is accepted. All data created during this research are openly available from the University of Sussex data repository (DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25377/sussex.5442346" xlink:type="simple">10.25377/sussex.5442346</ext-link>) at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/Research_data_for_research_paper_Neural_coding_in_the_visual_system_of_Drosophila_melanogaster_How_do_small_neural_populations_support_visually_guided_behaviours_/5442346" xlink:type="simple">https://figshare.com/articles/Research_data_for_research_paper_Neural_coding_in_the_visual_system_of_Drosophila_melanogaster_How_do_small_neural_populations_support_visually_guided_behaviours_/5442346</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>As with many animals, vision plays a key role in a number of behaviours performed by the fruit fly <italic>Drosophila melanogaster</italic>, including mate-recognition [<xref ref-type="bibr" rid="pcbi.1005735.ref001">1</xref>], place homing [<xref ref-type="bibr" rid="pcbi.1005735.ref002">2</xref>], visual course control [<xref ref-type="bibr" rid="pcbi.1005735.ref003">3</xref>], collision-avoidance [<xref ref-type="bibr" rid="pcbi.1005735.ref004">4</xref>], landing [<xref ref-type="bibr" rid="pcbi.1005735.ref004">4</xref>] and escaping a looming object (like a rolled newspaper, for example) [<xref ref-type="bibr" rid="pcbi.1005735.ref005">5</xref>]. The benefit of studying these visually guided behaviours in <italic>Drosophila</italic> is the range of neurogenetic techniques which give a realistic chance of understanding the neural circuits that underpin them. With that goal in mind, we focus on work by Seelig and Jayaraman [<xref ref-type="bibr" rid="pcbi.1005735.ref006">6</xref>] which mapped the receptive fields (RFs) of a set of visually responsive neurons: the ring neurons of the ellipsoid body. These neurons are necessary and sufficient for a range of complex behaviours, including short term spatial memory, pattern discrimination and place memory [<xref ref-type="bibr" rid="pcbi.1005735.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1005735.ref009">9</xref>], and yet are surprisingly small in number. To understand their role in these behaviours, we used modelling to bridge the gap between neurogenetic data and behaviour by evaluating ring neuron responses during simulations of fly experiments. In this way we investigate how small populations of visual neurons in <italic>Drosophila</italic>, which might represent a sensory bottleneck, can still provide behaviourally relevant information.</p>
<p>In laboratory assays, flies show interesting spontaneous visual behaviours. For instance, flies orient towards bar stimuli [<xref ref-type="bibr" rid="pcbi.1005735.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref011">11</xref>] and in a circular arena with two diametrically opposed bars will walk between them until exhaustion [<xref ref-type="bibr" rid="pcbi.1005735.ref012">12</xref>]. The attraction to vertical bars decreases as the bar is shortened and flies are strongly repulsed by small spots [<xref ref-type="bibr" rid="pcbi.1005735.ref013">13</xref>]. In addition, a number of studies have investigated the process of pattern recognition and its neural underpinnings [<xref ref-type="bibr" rid="pcbi.1005735.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref015">15</xref>]. Flies seem to possess a form of pattern memory analogous to the better-studied pattern memory of bees [<xref ref-type="bibr" rid="pcbi.1005735.ref016">16</xref>–<xref ref-type="bibr" rid="pcbi.1005735.ref018">18</xref>]. Interestingly, both bees [<xref ref-type="bibr" rid="pcbi.1005735.ref019">19</xref>] and flies [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>] systematically fail to discriminate certain pattern pairs.</p>
<p>These visual behaviours require the central complex, a major neuropil which comprises the ellipsoid body, the fan-shaped body, the paired noduli and the protocerebral bridge [<xref ref-type="bibr" rid="pcbi.1005735.ref020">20</xref>]. The central complex is thought to be involved primarily in spatial representation, action selection and mediation between visual input and motor output [<xref ref-type="bibr" rid="pcbi.1005735.ref021">21</xref>]. One class of neurons with projections in the ellipsoid body is the ‘ring neurons’, which are known to be involved in certain visual behaviours (R1: place homing [<xref ref-type="bibr" rid="pcbi.1005735.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref023">23</xref>]; R2/R4m: pattern recognition [<xref ref-type="bibr" rid="pcbi.1005735.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref015">15</xref>]; R3/R4: bar direction memory [<xref ref-type="bibr" rid="pcbi.1005735.ref008">8</xref>]). Here we investigate how the ring neurons might contribute to behaviour, by simulating the visual input as it would be processed by this small population of visually responsive cells. In particular, we can address why flies are unable to discriminate certain pattern pairs, whether these subpopulations of neurons are optimised for pattern recognition and, if not, what visually guided behaviours these cells are suited to.</p>
<p>In order to do this, we leverage research which has described the RF properties of two classes of ring neuron in the <italic>Drosophila</italic> ellipsoid body [<xref ref-type="bibr" rid="pcbi.1005735.ref006">6</xref>]. The two subtypes of neuron investigated were the R2 and the R4d ring neurons, of which only 28 and 14, respectively, were responsive to visual stimuli. The cells were found to possess RFs that were large, centred in the ipsilateral portion of the visual field and with forms similar to those of mammalian simple cells [<xref ref-type="bibr" rid="pcbi.1005735.ref024">24</xref>] (for details of how the RFs were estimated, see <xref ref-type="sec" rid="sec010">Materials and methods</xref>). Like simple cells, many of these neurons showed strong orientation tuning and some were sensitive to the direction of motion of stimuli. The ring neuron RFs, however, are much coarser than those of simple cells, far larger and less evenly distributed across the visual field and respond mainly to orientations near the vertical. This suggests that ring neurons might have a less general function than simple cells [<xref ref-type="bibr" rid="pcbi.1005735.ref025">25</xref>]. In mammals, the very large population of simple cells means that small, high-contrast boundaries of any orientation are detected at all points in the visual field. Thus the encoding provided by simple cells preserves visual information and acts as a ‘general purpose’ perceptual network that can feed into a large number of behaviours. In contrast, the coarseness of the ring neuron RFs, allied to the tight relationship between specific behaviours and specific subpopulations of ring neurons, suggests instead that these cells are providing economical visual information that is likely tuned for specific behaviours [<xref ref-type="bibr" rid="pcbi.1005735.ref025">25</xref>].</p>
<p>To investigate such issues, we use a synthetic approach whereby investigations, in simulation, of the information provided by these populations of neurons can be related to behavioural requirements, thus ‘closing the loop’ between brain and behaviour. We show how the population code is well-suited to the spontaneous bar orientation behaviours shown by flies. Similarly, we verify that our population of simulated ring neurons is able to explain the success and failure of the fly to discriminate pairs of patterns. Upon deeper analysis, we demonstrate that certain shape parameters—orientation, size and position—are implicit in the ring neurons’ outputs to a high accuracy, thus providing the information required for a suite of basic fly behaviours. This contrasts with the rather limited ability of ring neuron populations (and flies) to discriminate between abstract shapes, casting doubt on cognitive explanations of fly behaviour in pattern discrimination assays.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>Here we analyse the task-specific information provided by visually responsive ring neurons by simulating their responses during well-known behavioural experiments. To do this we use data from Seelig and Jayaraman [<xref ref-type="bibr" rid="pcbi.1005735.ref006">6</xref>] who used calcium imaging to examine the RFs of ring neurons, whose cell bodies are in specific glomeruli in the lateral triangle. As the RFs of glomeruli are remarkably consistent across flies [<xref ref-type="bibr" rid="pcbi.1005735.ref006">6</xref>], we combine them by averaging across flies to reduce measurement error and obtain sets of ‘canonical’ RFs, which can be thought of as visual filters. The averaging process assumes a certain amount of underlying homogeneity for each glomerulus across flies, which we feel is justified given their similar forms; the advantage of this approach, over one in which we, say, take the RF from a single fly, is that it reduces the inevitable noise that will have been accrued in determining the RFs for individual flies. Additionally, small changes in the averaging process have little effect on the results [<xref ref-type="bibr" rid="pcbi.1005735.ref026">26</xref>]. This process (for details, see <xref ref-type="sec" rid="sec010">Materials and methods</xref>) gave us a set of 28 R2 and 14 R4d filters (14 and 7 on each side of the visual field, respectively). We treat these as simple linear filters, following [<xref ref-type="bibr" rid="pcbi.1005735.ref006">6</xref>], as we are not attempting here to model outputs at the neuronal level. To investigate the visual information that these cells encode, we calculate outputs for a given visual stimulus by convolving it with the averaged ring neuron filters. This gives a population code where the outputs of the set of filters is the encoded ‘representation’ of the current visual stimulus. We interrogate these encodings to understand the information they contain, focusing on the relationships to specific behaviours.</p>
<sec id="sec003">
<title>Orientation towards bar stimuli</title>
<p>We first consider experiments in which flies are presented with bar stimuli, as flies are known to spontaneously orient towards black bars [<xref ref-type="bibr" rid="pcbi.1005735.ref011">11</xref>], aiming for the centres of narrow bars and the edges of wide bars [<xref ref-type="bibr" rid="pcbi.1005735.ref027">27</xref>]. We therefore decided to examine the responses of simulated ring neurons to bars of different widths (<xref ref-type="fig" rid="pcbi.1005735.g001">Fig 1A and 1B</xref>). The summed outputs of the ensembles of ring neurons show peaks to the bars of different widths, which broadly matches experimental results (<xref ref-type="fig" rid="pcbi.1005735.g001">Fig 1B</xref>). For instance, R2 neurons respond maximally to the inside edges of large bars, while peak activity in R4d neurons occurs at bar centres and also at roughly ±90°. While we do not know the details of mechanisms downstream of the ring neurons and hence how their activity is transformed into action, the simulation is an existence proof that the information needed to control the observed behaviour is present in the sparse ring neuron code.</p>
<fig id="pcbi.1005735.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005735.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Simulation of two behavioural experiments with <italic>Drosophila</italic> to examine the outputs of the R2 and R4d RFs.</title>
<p>A: A diagram showing Buridan’s paradigm [<xref ref-type="bibr" rid="pcbi.1005735.ref052">52</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref053">53</xref>]. If a fly is placed in an arena between two large vertical bars, it will walk back and forth until exhaustion. B: The mean output of R2 (blue) and R4d (red) filters to bars of different widths from two points in the arena (indicated in panel A with crosses) across headings. C: The summed outputs of R2 and R4d filters can be used to drive orientation towards a bar stimulus with a simple proportional-integral-derivative (PID) controller. D: A vector field of orientations for a simulated fly driven by the simple PID controller. Note that this is not intended as a descriptive model of how bar attraction in flies operates, but as an illustration of the information latent in the outputs of the ring neurons, hence why many of the ‘flies’ in the vector plot would miss the bar. E: The fly is held tethered in a drum. As the fly attempts to rotate about its yaw-axis, the drum rotates in the opposite direction, thus allowing the fly to select the portion of the pattern in view. By monitoring the fly’s heading, one can surmise whether there is a spontaneous preference for one of the patterns. Whether the fly can learn to head towards one pattern is tested by adding a laser that punishes the fly for facing one of the patterns. Shown inside the drum are the visual RFs for one pair of left- and right-side glomeruli. F and G: The r.m.s. difference in output for R2 RFs as the pattern is rotated. The reference activities are the RF outputs when the simulated flies are at 0°. Patterns with a greater difference in activity at 0° <italic>vs</italic> 90° (indicated by dotted lines) should be more discriminable by flies. For two pairs of patterns we show that there is a much smaller difference in output when the triangles are aligned about the vertical centre of mass (G) than not (F). This mirrors real flies’ performance on this task [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005735.g001" xlink:type="simple"/>
</fig>
<p>We further demonstrate this point by closing the loop between sensory systems and behaviour using a simple model of a fly viewing a bar in which the fly’s heading is controlled by the difference between the summed activation of left and right ring neurons (<xref ref-type="fig" rid="pcbi.1005735.g001">Fig 1C</xref>; see <xref ref-type="sec" rid="sec010">Materials and methods</xref> for details). The simulated fly approaches the bar from different distances, demonstrating centre-aiming when far from the bar and fixation of the edges when it is nearer and the bar’s apparent size is thus greater (<xref ref-type="fig" rid="pcbi.1005735.g001">Fig 1D</xref>). Through this example, we can see how the information present in this small population of visually responsive ring neurons can control a specific behaviour. We now turn to a more complex behaviour: pattern discrimination.</p>
</sec>
<sec id="sec004">
<title>Pattern discrimination in flies and ring neuron population codes</title>
<p>The standard paradigm for testing pattern discrimination involves putting a fly into a closed-loop system where it is tethered inside a drum, on the inside of which are two different visual patterns, alternating every 90°, giving four visual stimuli in total [<xref ref-type="bibr" rid="pcbi.1005735.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref028">28</xref>] (see <xref ref-type="fig" rid="pcbi.1005735.g001">Fig 1E</xref>). As the fly attempts to rotate in one direction, the drum rotates in the other, giving the fly the illusion that it is moving in a stable world. To elicit conditioned behaviour, if the fly faces one of the four pattern stimuli it is punished by a heat beam. Over time, if the fly is able to differentiate the patterns, it should preferentially face the unpunished pattern. This procedure has been used to demonstrate that flies can differentiate stimulus pairs such as upright and inverted ‘T’ shapes, a small and a large square, and many others [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>]. The ability to discriminate patterns in such an assay requires R2 neurons [<xref ref-type="bibr" rid="pcbi.1005735.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref029">29</xref>]. More specifically, synaptic plasticity afforded by <italic>rutabaga</italic> in these neurons is sufficient and necessary for observed pattern learning [<xref ref-type="bibr" rid="pcbi.1005735.ref015">15</xref>]. We therefore investigate the responses of ring neurons in simulations of the classic pattern discrimination paradigm.</p>
<p>To recreate the visual information perceived by flies in such experiments, we simulated the typical experimental flight arena with a fly tethered in the centre. We then examined the output of the ensembles of ring neurons for a fly rotating in the drum and looked at the difference in the activation code when the agent was facing the different patterns of a pair. Our logic is that if the ensemble codes were identical, it would be impossible for the patterns to be discriminated by interrogating the outputs of ring neurons alone. Similarly, the greater the difference in the ring neuron ensemble activation codes when looking at the pattern pairs, the easier they would be to discriminate (<xref ref-type="fig" rid="pcbi.1005735.g001">Fig 1F and 1G</xref>; see <xref ref-type="sec" rid="sec010">Materials and methods</xref> for details). Our discriminability measure is the root mean square (r.m.s.) difference between ensemble outputs when the (virtual) fly faces different azimuths in the drum. In this way, we can compare the ensemble output when the ‘fly’ is oriented at 0° (i.e. with the view centred on one pattern) and the ensemble output at other azimuths (<xref ref-type="fig" rid="pcbi.1005735.g001">Fig 1</xref>). We henceforth treat this as a measure of ‘discriminability’ of patterns, following the experimental work that we are modelling, though of course in reality an animal’s ability to discriminate stimuli is not an absolute value and varies depending on many factors, including task and training procedure [<xref ref-type="bibr" rid="pcbi.1005735.ref030">30</xref>]. The r.m.s. difference, as compared to the view at 0°, rises as the fly rotates in the drum, peaking as it faces the space in between the patterns and dropping to a minimum when facing the centre of the next pattern (<xref ref-type="fig" rid="pcbi.1005735.g001">Fig 1F and 1G</xref>). For some pairs of patterns, there is still an appreciable r.m.s. difference between the codes when facing the centres of each pattern, thus enabling their discrimination. However, in the example of <xref ref-type="fig" rid="pcbi.1005735.g001">Fig 1F and 1G</xref>, if we displace the patterns vertically, we see a drop in the r.m.s. difference between activation codes when the fly fixates the patterns. This is despite the fact that, to the human eye, the patterns still appear very different. Interestingly, the pattern pair in <xref ref-type="fig" rid="pcbi.1005735.g001">Fig 1G</xref> is also harder to discriminate for flies.</p>
<p>In this way, we can use the difference between ensemble codes when flies face the patterns to re-examine the discriminability of pattern pairs tested with flies. One illustrative example is shown in <xref ref-type="fig" rid="pcbi.1005735.g002">Fig 2</xref> (see pattern set <italic>(9)</italic> in <xref ref-type="fig" rid="pcbi.1005735.g003">Fig 3</xref>), which contains pairs of ‘triangles’, one facing up and the other down. <italic>Drosophila</italic> are able to discriminate these pattern pairs when they are aligned along the top and bottom, but not when aligned about the vertical centres of mass [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>]. Looking at the placement and form of the R2 RFs allows us to determine where this difference comes from (<xref ref-type="fig" rid="pcbi.1005735.g002">Fig 2</xref>). The excitatory regions of the RFs fall roughly across the middle of triangles that are not aligned about their vertical centres of mass and therefore the difference in width at this point will lead to differences in activation. If the triangles are offset (<xref ref-type="fig" rid="pcbi.1005735.g002">Fig 2B</xref>) so as to be aligned about their vertical centres of mass, their width will be similar for the regions of peak R2 coverage and the difference in activation will be lower. Thus the failure to discriminate features with an equivalent vertical centre of mass can be explained by the shape of the RFs interacting with the patterns directly. It is not necessary to invoke an additional system that extracts and compares the vertical centres of mass of the patterns.</p>
<fig id="pcbi.1005735.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005735.g002</object-id>
<label>Fig 2</label>
<caption>
<title>R2 cells do not encode detailed shape information.</title>
<p>A–C: The discriminability of pattern pairs can vary greatly, independently of the apparent difference between visual stimuli. A: A pattern pair made up of an upright and an inverted triangle, aligned along their top and bottom. B: Another pattern pair, consisting of the same triangles as in A, but aligned about their vertical centres of mass. C: The difference in activation between 0° and 90° for all R2 RF filters for the triangles from A (white bars) and B (black bars). Discriminability is greater for the triangles in A than in B. The red asterisk marks the output of the ring neuron RF shown in A and B. D and E: Performance improves with number of RFs, for two sets of ‘difficult’ patterns (centre-of-mass-aligned triangles/bars, insets). Bars show mean r.m.s. difference between the activation of 28, 56 and 112 RFs to each pattern. For 28 RFs we use the R2 RFs. For 56 and 112 RFs, one and three extra sets of R2 filters, respectively, are added in random, non-overlapping positions on the visual field (see <xref ref-type="sec" rid="sec010">Materials and methods</xref> for details). As there is some stochasticity in this process, differences are averaged over 1000 trials with error bars showing standard deviation. F and G: We can also generate shapes that appear similar yet produce a large mean difference in RF activation (G, top) or appear different and produce similar RF activations (G, bottom). The stimuli here are ‘blobs’ of the form described in Materials and methods. To generate the stimuli, an optimisation was performed in MATLAB (<monospace>fminsearch</monospace> function) to minimise the ratio of blob difference to difference in activation (G, top) or its inverse (G, bottom). Pairs of stimuli are shown in grey and green whereas in the simulation both are black. F: The corresponding activations of the different R2 RFs to the blobs in G. Two similar patterns give a mean difference in activity of 11.0% and two very different patterns give a mean difference in activity of 5.10%. The dotted line indicates the mean activation for the blob in the upper panel and the dashed line for the blob in the lower.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005735.g002" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005735.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005735.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Outputs of simulated R2 cells and degree of retinal overlap for published pattern pairs.</title>
<p>Difference in R2 RF output partly predicts whether patterns are discriminable by flies, but not whether there is a spontaneous preference. By contrast, retinal overlap predicts whether there is a spontaneous preference, but not whether the patterns will be discriminable. The patterns tested here are drawn from [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>] and are grouped according to the figures in which they appear in that work; the corresponding figure numbers are shown in parentheses. All patterns for which the significance of ‘learning preference’ (‘<inline-formula id="pcbi.1005735.e001"><alternatives><graphic id="pcbi.1005735.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mover><mml:mtext>DCP</mml:mtext> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula>’ in [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>]) was given are included. A: Grey bars indicate that the learning index for the pattern was significant (i.e. <italic>p</italic> &lt; .05 in [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>]). A higher score indicates a greater r.m.s. difference in R2 activity and thus that the pattern was more discriminable by the simulation. Performance on more ‘horizontal’ patterns (e.g. <italic>(3)</italic> and the final three patterns in <italic>(12)</italic>) was poor in the behavioural experiments, but better in simulation. This is perhaps due to the horizontal motion of the patterns in training, as noted in [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>]. B and C: Scatter plots of R2 difference (the ‘RF Model’) and retinal overlap (the ‘Retinotopic Model’) <italic>vs</italic> spontaneous preference (‘<inline-formula id="pcbi.1005735.e002"><alternatives><graphic id="pcbi.1005735.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mover><mml:mtext>SCP</mml:mtext> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula>’ in [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>]) shown in [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>]. A significant correlation was found for the Retinotopic Model (<italic>n</italic> = 29; <italic>ρ</italic> = −0.370509; <italic>p</italic> &lt; .05) but not the RF Model (Spearman’s rank, <italic>n</italic> = 29, <italic>ρ</italic> = .289, <italic>p</italic> = n.s.). D and E: Scatter plots of R2 difference (the ‘RF Model’) and retinal overlap <italic>vs</italic> learning index (<inline-formula id="pcbi.1005735.e003"><alternatives><graphic id="pcbi.1005735.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mover><mml:mtext>DCP</mml:mtext> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula>). A significant correlation was found for the RF Model (Spearman’s rank, <italic>n</italic> = 32, <italic>ρ</italic> = .502, <italic>p</italic> &lt; .005) but not the Retinotopic Model (<italic>n</italic> = 32, <italic>ρ</italic> = −.00960, <italic>p</italic> = n.s.). As two data points on the far right in panel D could be outliers, we reran the analysis excluding these points and found that the correlation was still significant, albeit less so (<italic>n</italic> = 30, <italic>ρ</italic> = .420, <italic>p</italic> &lt; .05).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005735.g003" xlink:type="simple"/>
</fig>
<p>Performance on poorly discriminated patterns can be improved, however, by simply adding more RFs of the same form. <xref ref-type="fig" rid="pcbi.1005735.g002">Fig 2D and 2E</xref> show the increase in performance with number of RFs for two such pattern pairs: triangles and triangularly shaped horizontal bars aligned about the centre of mass (from pattern set <italic>(9)</italic> in <xref ref-type="fig" rid="pcbi.1005735.g003">Fig 3</xref>; see <xref ref-type="sec" rid="sec010">Materials and methods</xref> for details). This demonstrates that the patterns could be discriminated by flies simply with the addition of more RFs centred on other portions of the visual field.</p>
<p>Similarly, pattern set <italic>(2)</italic> in <xref ref-type="fig" rid="pcbi.1005735.g003">Fig 3</xref> gives examples of pattern pairs that are not discriminable by flies and also give only small differences in the outputs of R2 filters. This may seem surprising, given that these patterns appear quite different to human observers and are also very dissimilar if compared retinotopically. Thus we can see that the R2 ring neuron encoding is informationally sparse. Whilst the V1 region of human visual cortex contains neurons representing the full range of orientations across the visual field, R2 neurons have large RFs and poor orientation resolution. Hence, a pattern pair consisting of a diagonal line facing left and a diagonal line facing right, for example, have only a small difference in R2 outputs in our simulation and are also not discriminable by flies. This could, in the light of behavioural experiments alone, be interpreted as evidence that flies do not discriminate patterns on the basis of orientation. A more parsimonious explanation, however, is that the flies are failing because the form of the RFs means that the output code is similar for these particular orientations.</p>
<p>To emphasise the independence of apparent similarity of patterns and the visual encoding from R2 cells, we designed shape pairs that appear similar to humans, but are easily discriminable by the R2 population (white bars in <xref ref-type="fig" rid="pcbi.1005735.g002">Fig 2F</xref>), as well as shape pairs that are considered similar by the R2 population but not by human observers or in terms of retinal overlap (black bars in <xref ref-type="fig" rid="pcbi.1005735.g002">Fig 2F</xref>; see <xref ref-type="sec" rid="sec010">Materials and methods</xref> for details). Despite the similarity between the pairs of patterns, the first is readily discriminable, especially from the outputs of glomeruli 1, 3, 5 and 11, while the second pair—which we easily see as having a different orientation—has very low overall differences across the glomeruli. This shows that the irregular RF shapes can lead to counterintuitive results.</p>
<p>The small population of visually responsive R2 neurons can be thought of as a sensory bottleneck. If the information that passes through this bottleneck is all that a fly has available for pattern discrimination, then we should see a close relationship between the r.m.s. difference in simulated R2 output for a pattern pair and the flies’ ability to learn to discriminate that pair. We thus examined the difference in the outputs of the R2 filters between patterns from pairs drawn from work by Ernst and Heisenberg [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>] (<xref ref-type="fig" rid="pcbi.1005735.g003">Fig 3</xref>). In general, the pattern pairs for which flies show a significant learned discrimination have a greater r.m.s. difference in R2 population activity [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>]. All of the pattern pairs where flies show significant learning (<italic>n</italic> = 8) have R2 r.m.s. differences above the overall mean (<xref ref-type="fig" rid="pcbi.1005735.g003">Fig 3A and 3B</xref>), whereas 13 out of 18 patterns that flies found more difficult to learn had below-average r.m.s. differences (there were nine pattern pairs for which a significance level was not given that were excluded.) Across all pattern pairs, we find a significant correlation between the strength of the learning index reported for flies in [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>] and the r.m.s. difference in R2 activation (Spearman’s rank, <italic>n</italic> = 30, <italic>ρ</italic> = .420, <italic>p</italic> &lt; .05). Of course, these differences could simply result from the apparent similarity of the patterns. Therefore, as a control comparison, we quantified the similarity of pattern pairs based on the degree to which the patterns overlap in a pixel-by-pixel manner (see <xref ref-type="sec" rid="sec010">Materials and methods</xref>). There was no significant correlation with the flies’ learning index over the pattern pairs (Spearman’s rank, <italic>n</italic> = 32, <italic>ρ</italic> = −.068, <italic>p</italic> = n.s.). We additionally looked at the relationship between the two visual similarity metrics (R2 population code and pixelwise retinal overlap) and the degree to which flies show a spontaneous preference (i.e. without any conditioning) for one of the patterns within a pair (<xref ref-type="fig" rid="pcbi.1005735.g003">Fig 3D and 3E</xref>). There was no correlation for R2 population codes (Spearman’s rank, <italic>n</italic> = 29, <italic>ρ</italic> = .289, <italic>p</italic> = n.s.), but for retinal overlap there was a weakly significant correlation (Spearman’s rank, <italic>n</italic> = 29, <italic>ρ</italic> = −.371, <italic>p</italic> &lt; .05). This is consistent with research showing that R2 neurons alone are critical for learned pattern differences [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>], but not spontaneous preferences which, by contrast, seem to result from activity across all subsets of ring neurons [<xref ref-type="bibr" rid="pcbi.1005735.ref031">31</xref>].</p>
<p>There are, however, some discrepancies where the learning performance of flies for a particular pattern pair does not match the r.m.s. difference of our R2 population code. In some cases flies are better at discriminating pairs of patterns that differ along the vertical rather than horizontal axis (set <italic>(3)</italic> <italic>vs</italic> set <italic>(4)</italic>, and the pairs in set <italic>(12)</italic>, marked with red Xs in <xref ref-type="fig" rid="pcbi.1005735.g003">Fig 3</xref>). In contrast, the r.m.s. difference in the R2 population code discriminates horizontal and vertical patterns equally. This is because while our R2 filters are presented with static stimulus pairs to simulate a fly facing the centre of a pattern, for real flies the patterns were moving horizontally but fixed in the vertical axis making it harder for flies to resolve horizontal information [<xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>].</p>
<p>Overall, we have shown that the behavioural performance of flies on a pattern discrimination task is approximated by a simple discriminability metric applied to the population activity of a small number of simulated R2 cells. There were, however, a number of seemingly ‘easy’ pattern pairs which neither flies nor the simulated population of R2 cells, perhaps surprisingly, could discriminate. On further investigation, we found that performance for poorly discriminated pattern pairs could be improved with the addition of extra R2-type RFs. Thus, it seems likely that the pattern discrimination capability of a set of R2-like neurons could easily have been improved over evolutionary time with the simple addition of more cells and we therefore suggest that there must have been little selection pressure specifically for a specialised pattern recognition module in fruit flies.</p>
</sec>
<sec id="sec005">
<title>What information is preserved in this simple neural code?</title>
<p>Information from 3000 ommatidia is funnelled to just 28 R2 and 14 R4d ring neurons, yet these cells are able to support a number of complex behaviours. We have shown how the R2 population code provides sufficient information to discriminate some pattern pairs, and also that, as performance could be improved with the addition of more ring neurons, general-purpose pattern recognition seems unlikely to be the purpose of the ring neuron system. So what information is this system tuned to extract? Examining the pattern pairs which flies and the R2 population were able to discriminate, we see that certain pattern parameters are implicitly coded for in the R2 population. Pattern sets <italic>(6)</italic> and <italic>(9)</italic> (<xref ref-type="fig" rid="pcbi.1005735.g003">Fig 3</xref>) suggest that, for instance, stimulus size and vertical centre of mass are parameters that can be recovered from the R2 population code after this sensory bottleneck.</p>
<p>We now address in more general terms the question of what shape information is implicitly conveyed in the ring neuron population code. To do this, we generated large sets of ellipse-like ‘blob’ stimuli varying in size (specified by major-axis length), position (azimuth and elevation) and orientation. The blob generation procedure was stochastic and so the precise shape of each blob was random and unique (see <xref ref-type="sec" rid="sec010">Materials and methods</xref>). We then trained an artificial neural network (ANN) to recover this shape information from either a raw image of the shape (the control condition) or from the output of the R2/R4d populations on presentation of the blobs. We are using ANNs here as statistical engines interrogating the output of the ring neuron population code to determine if shape information is implicit to the code and has therefore passed through the sensory bottleneck. We first examined whether ANNs could be trained to extract positional information (the elevation and azimuth) of randomly generated blobs. Note that as the blobs are initially aligned about their centres of mass, elevation is equivalent to vertical centre of mass, except where the blobs are partially outside the visual field. The blobs varied along four parameters: elevation (≥ −60° and ≤ 60°), azimuth (≥ −135° and ≤ 0°), orientation (≥ 0° and ≤ 90°) and major-axis length (≥ 12.79° and ≤ 60°). Each parameter had 22 possible values, giving a total of 234,256 (= 22<sup>4</sup>) stimuli. Of these, approximately 40% (<italic>n</italic> = 93,702) were used for training and the remainder (<italic>n</italic> = 140,554) for testing. Results for the test set (<xref ref-type="fig" rid="pcbi.1005735.g004">Fig 4A–4D</xref>) show that ANNs are indeed able to extract information about elevation and azimuth from any of the input types (‘raw view’, ‘R2’, ‘R4d’ or ‘R2 + R4d’). Performance was better with parameter values near the middle: at the extremes, portions of the stimuli lay outside the visual field of the simulated fly, meaning stimuli begin to disappear ‘off the edge’ of the visual field (<xref ref-type="fig" rid="pcbi.1005735.g004">Fig 4A and 4B</xref>), making the task harder (i.e., is this a large object projecting outside the visual field, or a smaller object at the edge?). While performance was best with raw views as inputs (<xref ref-type="fig" rid="pcbi.1005735.g004">Fig 4C and 4D</xref>), positional information could still be reliably extracted from ring neuron outputs. The R2 code performs better than the R4d and the addition of R4d RFs to the R2 code (‘R2 + R4d’), while adding dimensionality, does not improve performance, suggesting that either an R2-like encoding is sufficient to extract positional information, or that the information in the two codes is redundant. Thus small populations of ring neurons retain positional information.</p>
<fig id="pcbi.1005735.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005735.g004</object-id>
<label>Fig 4</label>
<caption>
<title>How much shape information is preserved in the R2 population code?</title>
<p>ANNs were trained to estimate various properties of randomly generated ‘blob’ stimuli (<italic>n</italic> = 93,702) from raw views (3 × 14 = 42 pixels; blue), R2 neurons (<italic>n</italic> = 28; red), R4d neurons (<italic>n</italic> = 14; green) or R2 and R4d neurons (<italic>n</italic> = 42; magenta). Panels A–D show results for networks trained with elevation and azimuth and panels E–H for orientation and size. For each visual input a network was trained with 100 training cycles and average performance with blobs that were not part of the training set was taken. A and B: Plots of elevation and azimuth of the test visual stimuli <italic>vs</italic> the mean network output (<italic>n</italic> = 140,554). The dashed line indicates ideal performance (i.e. <italic>y</italic> = <italic>x</italic>) and the thickness of the lines at each point shows standard error. The possible values of elevation and azimuth were constrained by the size of the fruit fly visual field (approx. 120° × 270°). Within this range there were 22 possible values. C and D: Average network performance (r.m.s. error) for networks trained to recover elevation (C) or azimuth (D) for each type of visual input (colour code as above; <italic>n</italic>(train) = 4259, <italic>n</italic>(test) = 6389). Standard error is shown, but is very small. E and F: Network performance in recovering stimulus orientation and size. Orientation was constrained between 0° and 90°, to avoid the problem of aliasing, and varied with 22 levels. G and H: Average network performance (r.m.s. error) for networks trained to recover orientation (G) or size (H) and for each type of visual input (colour code as previously). Standard error is shown, but is very small.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005735.g004" xlink:type="simple"/>
</fig>
<p>We next trained ANNs to decode information about stimulus orientation and size. The stimuli were random blobs, as before, with the same possible values for elevation, orientation and size. This time, however, azimuthal position was fixed at −90°. The reason for this was that the neural network struggled to encode information about orientation when azimuth also varied, presumably because the centres of the receptive fields—and thus the position on the visual field where they can best extract information—are clustered at around −90°. For this experiment there were therefore 10,648 (= 22<sup>3</sup>) stimuli, of which approximately 40% (<italic>n</italic> = 4259) were used for training and the remainder (<italic>n</italic> = 6389) for testing. The ANNs were able to extract this shape information from both raw images and the ring neuron outputs (<xref ref-type="fig" rid="pcbi.1005735.g004">Fig 4E–4H</xref>). Orientation was the parameter with the highest error score, possibly because its calculation requires a second-order statistic (the covariance of the shape). Nonetheless, both parameters could be simultaneously estimated by an ANN neural network fed with ring neuron outputs.</p>
<p>In summary, we have shown that information about a number of shape properties passes through the bottleneck created by the small number of ring neurons. This indicates that such information is available downstream of the ring neurons for the guidance of behaviour.</p>
</sec>
</sec>
<sec id="sec006" sec-type="conclusions">
<title>Discussion</title>
<p>A general problem in neuroscience is understanding how sensory systems organise information to be at the service of behaviour. Computational approaches can be important in this endeavour, as they allow simulation of the sensory experience of a behaving animal, such that one can investigate how this information is transformed by populations of neurons. In this way, we can relate the details of neural circuitry to theories about the requirements of behaviour. Work by Seelig and Jayaraman [<xref ref-type="bibr" rid="pcbi.1005735.ref006">6</xref>], showing the forms of visual receptive fields for ring neurons projecting to the ellipsoid body of flies, gave us the opportunity to investigate how these neurons transform information and how this information relates to specific behaviours. In particular, we have shown that despite the small size of the neural code, the outputs of the ring neurons can subserve both bar following and limited pattern recognition and also implicitly convey information about shape parameters. We now discuss the implications of our findings.</p>
<sec id="sec007">
<title>Short-term memory for object position in flies</title>
<p>One striking feature of the ring neuron receptive fields is that they are in general tuned to vertically oriented objects. We know that fruit flies are strongly attracted to vertical bars, a finding that has been leveraged across a range of behavioural paradigms (e.g. bar fixation: [<xref ref-type="bibr" rid="pcbi.1005735.ref008">8</xref>]). In one, individual flies are placed into a virtual-reality arena with two vertical stripes 180° apart: flies will typically head back and forth between the two bars repeatedly. Occasionally, when a fly crosses the arena’s midline, the bars disappear and a new bar is presented at 90° to the originals, to which the flies reorient. The new target then also disappears, and the flies resume their initial heading, even though the original bar is no longer visible. This indicates that directional information is stored in short-term memory and updated. Work by Neuser and colleagues [<xref ref-type="bibr" rid="pcbi.1005735.ref008">8</xref>] has shown that R4 (and R3) ring neurons are involved in this spatial orientation memory. We found that both R2 and R4d neurons were responsive to vertical bars of varying widths, mimicking flies’ preference for the edges of larger bars and the centres of narrower ones [<xref ref-type="bibr" rid="pcbi.1005735.ref027">27</xref>]. We also showed that the cells provide sufficient information to guide homing towards a large vertical object and, separately, that the azimuth of bar stimuli makes it through the sensory bottleneck. Taken together, these findings demonstrate a viable role for the small R4d population in the behaviours described above.</p>
<p>The more general role of R4d cells within the central complex is still unknown. There is evidence that R4d neurons are able to act as a ring attractor, maintaining a stable encoding of the fly’s orientation with respect to a landmark [<xref ref-type="bibr" rid="pcbi.1005735.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref032">32</xref>]. Therefore, R4d neurons could be conceived variously as functioning like mammalian head-direction cells [<xref ref-type="bibr" rid="pcbi.1005735.ref033">33</xref>], playing a part in a path integration system [<xref ref-type="bibr" rid="pcbi.1005735.ref008">8</xref>] or in conditioning of visual orientation [<xref ref-type="bibr" rid="pcbi.1005735.ref034">34</xref>]. These possibilities are not mutually exclusive, of course, and their true function (or functions) will become apparent only with a better understanding of the behaviours in which they are involved.</p>
</sec>
<sec id="sec008">
<title>Do flies recognise patterns?</title>
<p><italic>Drosophila</italic> can discriminate patterns differing in size, orientation and elevation and other complex shape parameters, an ability for which R2 cells are critical [<xref ref-type="bibr" rid="pcbi.1005735.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref015">15</xref>]. We have shown that the discriminability of a given pattern pair is predicted by the outputs of the small population of R2 cells, which have coarse receptive fields and therefore do not encode higher-order visual properties explicitly. Does this limited ability of the R2 population (and, of course, the fly) to discriminate patterns suggest that flies might be a good model for the study of a universal perceptual process of pattern recognition, or might limited pattern recognition be an artefact of a perceptual system tuned to other tasks? Any selection pressure on flies’ ability to discriminate patterns (as bees need to do, for instance) would surely have led to a larger R2 population or, possibly, visual input to the mushroom body [<xref ref-type="bibr" rid="pcbi.1005735.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref036">36</xref>], and we can therefore be confident that ring neurons have not been tuned for arbitrary, general-purpose pattern recognition. Accordingly, we must suggest caution if research on flies is used with the aim of understanding the neural basis of pattern recognition or even visual cognition more generally [<xref ref-type="bibr" rid="pcbi.1005735.ref037">37</xref>]. So what behaviours are served by the information that makes it through this sensory bottleneck?</p>
<p>It is interesting to consider to what extent <italic>Drosophila</italic>’s ecological needs are served by general learning mechanisms—such as a capacity to learn arbitrary visual stimuli—and to what extent by domain-specific abilities. For example, bees have a well-attested ability to learn many varied patterns, which presumably derives from a need to learn about flowers [<xref ref-type="bibr" rid="pcbi.1005735.ref038">38</xref>]; it is not apparent, however, that there has been a comparable selection pressure on <italic>Drosophila</italic> for such general-purpose learning. Across the animal kingdom there are many cases where a task-specific heuristic can provide an elegant solution. For example, male fiddler crabs (<italic>Uca pugilator</italic>) treat salient objects above the horizon as predators and everything below as conspecifics [<xref ref-type="bibr" rid="pcbi.1005735.ref039">39</xref>]. Similarly, <italic>Drosophila</italic> have a mechanism to approach bars and to avoid small objects [<xref ref-type="bibr" rid="pcbi.1005735.ref013">13</xref>]; presumably to approach vegetation (for oviposition, etc.) and avoid predators, respectively. In order to fully understand these circuits we need to examine further how flies depend on a balance of innate visual responses versus learned visual information.</p>
<p>So, if the R2s are not truly ‘pattern-recognition cells’, the question remains: what are they for? Though we have not attempted to answer this question here, we have shown that there is <italic>implicit</italic> information about higher-order properties, such as stimulus position, orientation and size, in the RFs’ code, which could drive any number of natural behaviours. For example, elsewhere we have shown that the information content of ring neuron RFs is suitable for place learning and homing [<xref ref-type="bibr" rid="pcbi.1005735.ref026">26</xref>], and although this behaviour in flies involves a subset of ring neurons other than those examined here (R1), it gives an indication of how small populations of coarse, wide-field cells can be used to drive behaviour.</p>
</sec>
<sec id="sec009">
<title>Summary</title>
<p>The goal of this work was to investigate the information encoded in a population of visually responsive ring neurons, in simulations of classic pattern discrimination assays. Our aim was to examine the behavioural uses to which the information encoded in this population of cells could be put by a fruit fly. Of course, a full understanding of these neurons requires detailed knowledge of how they interact with other neural circuits for behaving flies in natural environments. Hence future work needs to address the interaction between brain, behaviour and environment [<xref ref-type="bibr" rid="pcbi.1005735.ref040">40</xref>].</p>
<p>For the brain, a sensible starting point is to ask how ring neurons and the information they carry are integrated in the central complex circuitry. Recent work has shown the presence of a ring attractor network [<xref ref-type="bibr" rid="pcbi.1005735.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1005735.ref042">42</xref>] in the ellipsoid body of the central complex which integrates both visual and proprioceptive information. This circuit is able to retain a heading in short-term memory [<xref ref-type="bibr" rid="pcbi.1005735.ref008">8</xref>] and thus the cells we have modelled could be useful in contributing information about the position of behaviourally relevant objects. Of course, there are many details to be determined, such as the dynamics of neural coding in this circuit and the sensory pathways that lead to the observed receptive fields [<xref ref-type="bibr" rid="pcbi.1005735.ref043">43</xref>]. In the current study we have not considered neural dynamics and have assumed that the information would be extracted as rate codes. While this is a common assumption for models of visual perception (e.g. [<xref ref-type="bibr" rid="pcbi.1005735.ref024">24</xref>]) we note that information could be extracted via a timing code, perhaps even more efficiently—especially if the fly is actively perceiving its environment. Though it is possible to convert from an analogue neural network to a spiking neural network [<xref ref-type="bibr" rid="pcbi.1005735.ref044">44</xref>], more work would be needed to establish this. Finally, the story is complicated further by the sheer variety of behaviours in which these cells have been implicated: for example, different subsets of R2 neurons have also been implicated in an olfactory decision task [<xref ref-type="bibr" rid="pcbi.1005735.ref045">45</xref>] and in sleep drive [<xref ref-type="bibr" rid="pcbi.1005735.ref046">46</xref>].</p>
<p>The sensory ecology of fruit flies is still largely a mystery (see [<xref ref-type="bibr" rid="pcbi.1005735.ref047">47</xref>]), despite the immense promise and productivity of <italic>Drosophila</italic> neuroscience research. We thus know relatively little about ‘natural’ <italic>Drosophila</italic> visual behaviours—in contrast to bees, for which we know much about behaviour but comparatively little about the nervous system. One pertinent example of this is visual pattern recognition, where for bees we have a good understanding of the real-world challenge facing a forager. This has enabled models of pattern recognition to be developed for bees [<xref ref-type="bibr" rid="pcbi.1005735.ref048">48</xref>]. Without a detailed understanding of sensory ecology and the natural behaviour of flies, it is hard to understand what type of pattern vision flies might need. However, some fly behaviours are easier to relate to the natural environment. Flies show an innate attraction to long bar-like objects, on which they might perch [<xref ref-type="bibr" rid="pcbi.1005735.ref013">13</xref>] and in <xref ref-type="fig" rid="pcbi.1005735.g005">Fig 5</xref> we show one example of how behaviourally relevant information is maintained in the output of R4d cells even for complex scenes. Taking this further, we could consider how an active fly in a complex environment might be able to shape its own visual input to give us a better understanding of the true potential of the fly as a pattern discriminator. More generally, it is only with a deeper knowledge of fruit fly ecology that we will be able to close the loop between brain, body and environment and thus obtain a full understanding of the whole system.</p>
<fig id="pcbi.1005735.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005735.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Mean output of R4d RFs as they move azimuthally across two natural panoramic images.</title>
<p>Mean across all R4d RFs is shown in yellow together with the left- and right-side RFs (blue and red dashed lines, respectively) with values normalised to the greatest absolute mean value of left or right RFs across all azimuths and both images (i.e. the <italic>y</italic>-scale is the same for A and B). A: Response of filters to distant trees. Many R4d RFs respond preferentially to the positions of trees, meaning outputs—both mean, left and right—peak in the centre of trees and tend to dip when facing azimuths with more sky or lighter areas. There are minor differences between the direction at which left and right signals peak, but there is sufficient information in the mean responses to allow a fly to head towards large vertical objects from a distance. B: Response of filters to a nearby tree. Mean responses again peak when facing salient, vertically oriented portions of the image (hanging branches) and dip at lighter portions, meaning flies could identify and head towards these regions. While left and right responses peak together in some cases, for the peaks at –120° and 60° there is an asymmetry in the left and right signals which could provide steering information to allow a fly to navigate towards these targets in a similar fashion to the model used in <xref ref-type="fig" rid="pcbi.1005735.g001">Fig 1A–1D</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005735.g005" xlink:type="simple"/>
</fig>
<p>One of the advantages of studying insects is the potential for describing their neural processes with modelling. In this way, simulations can help bridge the gap between biology and behaviour [<xref ref-type="bibr" rid="pcbi.1005735.ref049">49</xref>]. We have shown that the sensory bottleneck produced by small populations of cells is not a barrier to the specific information that is required for particular behaviours. However, this modelling work, and the neuroscience that invited it, do suggest caution when proposing that flies possess general-purpose visual cognition. We thus hope that future experiments, grounded in both the ecological needs of the animal and the information given by neural circuits, will be able to better inform the next generation of models, and vice versa.</p>
</sec>
</sec>
<sec id="sec010" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec011">
<title>Neurogenetic methods used for estimating ring neuron receptive fields</title>
<p>The goal of Seelig and Jayaraman’s work [<xref ref-type="bibr" rid="pcbi.1005735.ref006">6</xref>] was to examine responses of lateral triangle microglomeruli (which house the cell bodies of the ring neurons) to visual stimuli. For this, they employed two-photon calcium imaging to examine the activity of genetically targeted subsets of microglomeruli, the R2 and R3/R4d neurons. Fluorescence was recorded for head-fixed flies held in an arena with a curved display composed of an LED array. In order to map the RFs, the flies were presented with a series of flashing dots at random locations on the visual display; the fine structure of the RFs was then revealed by using white-noise stimuli [<xref ref-type="bibr" rid="pcbi.1005735.ref050">50</xref>]. The accuracy of the estimated receptive fields was then verified by correlating predicted with actual responses to novel bar stimuli (and a high degree of correspondence was found). The predicted responses were calculated by using the RFs as linear filters through convolution and so we follow a similar procedure here.</p>
</sec>
<sec id="sec012">
<title>Turning visual receptive field data into visual filters</title>
<p>To create the visual filters which represent the RFs, we first extract the image representations of the RFs from Seelig and Jayaraman (Extended Data Figure 8 in [<xref ref-type="bibr" rid="pcbi.1005735.ref006">6</xref>]). This gives us images of 112 × 252 pixels for R2 neurons and 88 × 198 pixels for R4d. Given the visual field is taken as 120° × 270°, this corresponds to a resolution of 1.07° and 1.36° per pixel, respectively. As data is given for multiple flies, we averaged the RFs for the different glomeruli across flies (2 ≤ <italic>N</italic>(<italic>R</italic>2) ≤ 6, 4 ≤ <italic>N</italic>(<italic>R</italic>4) ≤ 7). This process is summarised in <xref ref-type="fig" rid="pcbi.1005735.g006">Fig 6</xref>. Each pixel on the extracted image is initially assigned a value ranging from –1 for maximum inhibition to 1 for maximum excitation, based on the values given by the colour scale bars in [<xref ref-type="bibr" rid="pcbi.1005735.ref006">6</xref>]. These images are then thresholded to give a kernel <italic>g</italic>(<italic>i</italic>, <italic>j</italic>):
<disp-formula id="pcbi.1005735.e004"><alternatives><graphic id="pcbi.1005735.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>for</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>R</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>≥</mml:mo> <mml:mi>T</mml:mi> <mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>for</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>R</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>≤</mml:mo> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi> <mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mtext>otherwise.</mml:mtext></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>g</italic>(<italic>i</italic>, <italic>j</italic>) is the (<italic>i</italic>, <italic>j</italic>)th pixel of the kernel, <italic>R</italic><sub><italic>i</italic>,<italic>j</italic></sub> is the (<italic>i</italic>, <italic>j</italic>)th value of the processed RF image and <italic>T</italic> is the threshold value, here 0.25 (<xref ref-type="fig" rid="pcbi.1005735.g006">Fig 6A</xref>).</p>
<fig id="pcbi.1005735.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005735.g006</object-id>
<label>Fig 6</label>
<caption>
<title>The algorithm for obtaining average RFs.</title>
<p>A: The raw image (left; fly glomerulus 1) is thresholded so as to give excitatory and inhibitory regions of uniform intensity (right). The ‘centre’ is then calculated as the centroid of the largest excitatory region (+). B: Aligning two RFs. The new centre is taken as the average of the centre of both RFs and the RFs are then shifted so that the centres are aligned. C: Averaging the RFs for this glomerulus over all flies (<italic>n</italic> = 7), following alignment. Note that this is the left-side version; the right-side version is its mirror. D: Averaged R2 and R4d filters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005735.g006" xlink:type="simple"/>
</fig>
<p>We take the centroid of the largest excitatory region as the ‘centre’ of each of the kernels. The excitatory region is then extracted using MATLAB’s <monospace>bwlabeln</monospace> function (with eight-connectivity) and its centroid, (<italic>x</italic>, <italic>y</italic>), with the <monospace>regionprops</monospace> function. The mean centroid, <inline-formula id="pcbi.1005735.e005"><alternatives><graphic id="pcbi.1005735.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, across flies is then calculated and the kernels are all recentred on this point:
<disp-formula id="pcbi.1005735.e006"><alternatives><graphic id="pcbi.1005735.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>g</mml:mi> <mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>+</mml:mo> <mml:mi>y</mml:mi> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>+</mml:mo> <mml:mi>x</mml:mi> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>for</mml:mtext> <mml:mspace width="4.pt"/><mml:mn>1</mml:mn> <mml:mo>≤</mml:mo> <mml:mi>i</mml:mi> <mml:mo>+</mml:mo> <mml:mi>y</mml:mi> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>≤</mml:mo> <mml:mi>m</mml:mi> <mml:mspace width="4.pt"/><mml:mtext>and</mml:mtext> <mml:mspace width="4.pt"/><mml:mn>1</mml:mn> <mml:mo>≤</mml:mo> <mml:mi>j</mml:mi> <mml:mo>+</mml:mo> <mml:mi>x</mml:mi> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>≤</mml:mo> <mml:mi>n</mml:mi> <mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mtext>otherwise.</mml:mtext></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <inline-formula id="pcbi.1005735.e007"><alternatives><graphic id="pcbi.1005735.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is a recentred kernel (<xref ref-type="fig" rid="pcbi.1005735.g006">Fig 6C</xref>).</p>
<p>In order to calculate the activation for a given RF on presentation of an image the RF must first be resized to have the same number of pixels as the image. This is accomplished by resizing the average RF, <inline-formula id="pcbi.1005735.e008"><alternatives><graphic id="pcbi.1005735.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, using Matlab’s <monospace>imresize</monospace> function with bilinear interpolation and then scaled to [−1, 1]. Finally, the filter is thresholded and the excitatory and inhibitory regions are assigned different normalised values:
<disp-formula id="pcbi.1005735.e009"><alternatives><graphic id="pcbi.1005735.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>K</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>÷</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mtext>exc</mml:mtext></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>for</mml:mtext> <mml:mspace width="4.pt"/><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&gt;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>÷</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mtext>inh</mml:mtext></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>for</mml:mtext> <mml:mspace width="4.pt"/><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mtext>otherwise.</mml:mtext></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>S</italic><sub>exc</sub> and <italic>S</italic><sub>inh</sub> indicate the sums of excitatory and inhibitory pixels, respectively. This method of normalising values has the result that the activation (see below) for an all-white or -black image will be zero. Other normalisation schemes are possible, but the choice is somewhat arbitrary, as we are only interested in the differences in output values. Furthermore, RFs are sensitive to contrast differences, so a zero-sum filter, as seen in edge detectors, is appropriate. Additionally, assigning biologically relevant values is not possible because of a lack of data.</p>
<p>The activation of an average kernel, <italic>K</italic>, to the presentation of a greyscale image, <italic>I</italic>, at rotation <italic>θ</italic>, is then:
<disp-formula id="pcbi.1005735.e010"><alternatives><graphic id="pcbi.1005735.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>K</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover></mml:mstyle> <mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover></mml:mstyle> <mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>K</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>where</mml:mtext> <mml:mspace width="4pt"/><mml:mn>0</mml:mn> <mml:mo>≤</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≤</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>I</italic><sub><italic>i</italic>,<italic>j</italic></sub>(<italic>θ</italic>) and <italic>K</italic><sub><italic>i</italic>,<italic>j</italic></sub> are the (<italic>i</italic>,<italic>j</italic>)th pixels of the image and kernel, respectively.</p>
</sec>
<sec id="sec013">
<title>Replication of behavioural experiments</title>
<p>The equation for describing the bar fixation mechanism shown in <xref ref-type="fig" rid="pcbi.1005735.g001">Fig 1C</xref> is as follows:
<disp-formula id="pcbi.1005735.e011"><alternatives><graphic id="pcbi.1005735.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi> <mml:mtext>turn</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mi>G</mml:mi> <mml:mo>·</mml:mo> <mml:mfrac><mml:mi>π</mml:mi> <mml:mn>4</mml:mn></mml:mfrac> <mml:mo>(</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>K</mml:mi> <mml:mo>∈</mml:mo> <mml:msub><mml:mi mathvariant="bold">G</mml:mi> <mml:mtext>left</mml:mtext></mml:msub></mml:mrow></mml:munder> <mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mi>A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>K</mml:mi> <mml:mo>,</mml:mo> <mml:mn>0</mml:mn><mml:mo>°</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>K</mml:mi> <mml:mo>∈</mml:mo> <mml:msub><mml:mi mathvariant="bold">G</mml:mi> <mml:mtext>right</mml:mtext></mml:msub></mml:mrow></mml:munder> <mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mi>A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>K</mml:mi> <mml:mo>,</mml:mo> <mml:mn>0</mml:mn><mml:mo>°</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>I</italic> is the view of the bar from the agent’s current location and <bold>G</bold><sub>left</sub> and <bold>G</bold><sub>right</sub> are the sets of left- and right-side filters. ‘<italic>G</italic>’ is a parameter to control the gain of the system, and here was set to 2.</p>
<p>For the pattern recognition tasks (see <xref ref-type="fig" rid="pcbi.1005735.g003">Fig 3</xref>), the difference in activation is calculated as follows:
<disp-formula id="pcbi.1005735.e012"><alternatives><graphic id="pcbi.1005735.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>D</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msqrt><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>K</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="bold">G</mml:mi></mml:mrow></mml:munder></mml:mstyle> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>K</mml:mi> <mml:mo>,</mml:mo> <mml:mn>0</mml:mn><mml:mo>°</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>K</mml:mi> <mml:mo>,</mml:mo> <mml:mn>90</mml:mn><mml:mo>°</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:mo>|</mml:mo> <mml:mi mathvariant="bold">G</mml:mi> <mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:msqrt></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <bold>G</bold> is the set of R2 filters, <italic>I</italic> is the current pattern pair and <italic>A</italic>(⋅, ⋅, ⋅) is the activation of the kernel to the pattern, as described in <xref ref-type="disp-formula" rid="pcbi.1005735.e010">Eq 1</xref>. The choice of r.m.s. difference as a difference function is somewhat arbitrary, but r.m.s. difference is commonly used (e.g. [<xref ref-type="bibr" rid="pcbi.1005735.ref051">51</xref>]); alternatively one could use mean absolute difference or mutual information etc., although the choice is not critical as we are looking at relative differences and the RF output code is normalised.</p>
<p>The retinal overlap for two binary patterns, <italic>A</italic> and <italic>B</italic>, is calculated in two steps. Firstly we measure the number of overlapping pixels between <italic>A</italic> and <italic>B</italic>; this value is referred to as <italic>Q</italic>. Next the proportion of pixels which this overlap represents for <italic>A</italic> and <italic>B</italic> is calculated and, finally, we calculate the retinal overlap as the average of these two values:
<disp-formula id="pcbi.1005735.e013"><alternatives><graphic id="pcbi.1005735.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>,</mml:mo> <mml:mi>B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>Q</mml:mi> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>(</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover></mml:mstyle> <mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover></mml:mstyle> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover></mml:mstyle> <mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover></mml:mstyle> <mml:msub><mml:mi>B</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>A</italic><sub><italic>i</italic>,<italic>j</italic></sub> and <italic>B</italic><sub><italic>i</italic>,<italic>j</italic></sub> represent the <italic>i</italic>th, <italic>j</italic>th pixel of patterns <italic>A</italic> and <italic>B</italic>, respectively.</p>
<p>We also carried out simulations of pattern discrimination where the number of R2 RFs was varied between 28, 56 and 112 (<xref ref-type="fig" rid="pcbi.1005735.g002">Fig 2D and 2E</xref>). The ‘28-kernels’ condition simply used the original kernels in their original positions (see <xref ref-type="fig" rid="pcbi.1005735.g006">Fig 6D</xref>). For the 56- and 112-kernel conditions (double and quadruple the number of kernels, respectively), the original kernel types and positions were used for the first 28 kernels for every trial and the remainder were placed in random locations, with the only constraint being that the ‘centre’ of the new kernel could not be within 10° of the centre of any already-placed kernel. Equal numbers of each kernel type (corresponding to each one of the original R2 filters) were used in each condition; that is, each kernel type was repeated twice for the 56-kernel condition (once in the original position and once in a random position) and four times for the 112-kernel condition (once in the original position and three times in a random position). The kernels were also shrunk for the latter two conditions by a factor of <inline-formula id="pcbi.1005735.e014"><alternatives><graphic id="pcbi.1005735.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mfrac><mml:mn>1</mml:mn> <mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mfrac></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005735.e015"><alternatives><graphic id="pcbi.1005735.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac></mml:math></alternatives></inline-formula>, respectively, to keep the sum of the retinal area covered by all kernels constant across conditions, although the shapes of the kernels were otherwise kept the same. As the kernels were in fixed positions for the first condition, only one test was performed; for the other two conditions, 1000 trials were carried out.</p>
</sec>
<sec id="sec014">
<title>Neural networks</title>
<p>The ANNs were implemented using the <monospace>Netlab</monospace> toolbox for MATLAB. All networks were two-layer feedforward networks, with 10 hidden units and a linear activation function for the output units. There were 100 training cycles and optimisation was performed with the scaled conjugate gradient method.</p>
<sec id="sec015">
<title>Random blob stimuli</title>
<p>The stimuli used to train the networks were a series of black ‘blobs’ on a white background. The blobs were based on ellipses with a fixed ratio between the lengths of the major and minor axes (2 : 1), with the radii modified with complex waves:
<disp-formula id="pcbi.1005735.e016"><alternatives><graphic id="pcbi.1005735.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mtext>cos</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>θ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mtext>sin</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>θ</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives></disp-formula>
where <italic>a</italic> is the length of the major axis and <italic>W</italic>(<italic>θ</italic>) is a complex wave defined as:
<disp-formula id="pcbi.1005735.e017"><alternatives><graphic id="pcbi.1005735.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005735.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>W</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:msub><mml:mi>W</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo form="prefix">sin</mml:mo> <mml:mspace width="4pt"/><mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>A</italic><sub><italic>i</italic></sub>, <italic>f</italic><sub><italic>i</italic></sub> and <italic>ϕ</italic><sub><italic>i</italic></sub> describe the maximum amplitude, frequency and phase shift of the wave, <italic>W</italic><sub><italic>i</italic></sub>(<italic>θ</italic>), respectively. This method for generating stimuli allows for a substantial degree of random variation between the stimuli, while not producing shapes that are so irregular as to be unlearnable by a neural network. In these experiments, <italic>A</italic><sub><italic>i</italic></sub>, <italic>f</italic><sub><italic>i</italic></sub> and <italic>ϕ</italic><sub><italic>i</italic></sub> were randomly generated and <italic>n</italic> = 2. <italic>A</italic><sub><italic>i</italic></sub> was a random value from 0 to 1, <italic>f</italic><sub><italic>i</italic></sub> were random integers from 1 to 30 and <italic>ϕ</italic><sub><italic>i</italic></sub> was a random value from 0 to 2<italic>π</italic>.</p>
<p>The blobs were first generated, according to the above equation, as images of 120 × 270 pixels. For the ‘raw view’ stimuli, these images were resized, using MATLAB’s <monospace>imresize</monospace> function, to 3 × 14 pixels, thus giving the same number of inputs as there are R2 and R4d filters (<italic>n</italic> = 42).</p>
</sec>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1005735.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Agrawal</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Safarik</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Dickinson</surname> <given-names>M</given-names></name> (<year>2014</year>) <article-title>The relative roles of vision and chemosensation in mate recognition of <italic>Drosophila melanogaster</italic></article-title>. <source>J Exp Biol</source> <volume>217</volume>: <fpage>2796</fpage>–<lpage>2805</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1242/jeb.105817" xlink:type="simple">10.1242/jeb.105817</ext-link></comment> <object-id pub-id-type="pmid">24902744</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ofstad</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Zuker</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Reiser</surname> <given-names>MB</given-names></name> (<year>2011</year>) <article-title>Visual place learning in <italic>Drosophila melanogaster</italic></article-title>. <source>Nature</source> <volume>474</volume>: <fpage>204</fpage>–<lpage>207</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature10131" xlink:type="simple">10.1038/nature10131</ext-link></comment> <object-id pub-id-type="pmid">21654803</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Borst</surname> <given-names>A</given-names></name> (<year>2014</year>) <article-title>Fly visual course control: behaviour, algorithms and circuits</article-title>. <source>Nat Rev Neurosci</source> <volume>15</volume>: <fpage>590</fpage>–<lpage>599</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn3799" xlink:type="simple">10.1038/nrn3799</ext-link></comment> <object-id pub-id-type="pmid">25116140</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tammero</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Dickinson</surname> <given-names>MH</given-names></name> (<year>2002</year>) <article-title>Collision-avoidance and landing responses are mediated by separate pathways in the fruit fly, <italic>Drosophila melanogaster</italic></article-title>. <source>J Exp Biol</source> <volume>205</volume>: <fpage>2785</fpage>–<lpage>2798</lpage>. <object-id pub-id-type="pmid">12177144</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Card</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Dickinson</surname> <given-names>MH</given-names></name> (<year>2008</year>) <article-title>Visually mediated motor planning in the escape response of <italic>Drosophila</italic></article-title>. <source>Curr Biol</source> <volume>18</volume>: <fpage>1300</fpage>–<lpage>1307</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2008.07.094" xlink:type="simple">10.1016/j.cub.2008.07.094</ext-link></comment> <object-id pub-id-type="pmid">18760606</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Seelig</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Jayaraman</surname> <given-names>V</given-names></name> (<year>2013</year>) <article-title>Feature detection and orientation tuning in the <italic>Drosophila</italic> central complex</article-title>. <source>Nature</source> <volume>503</volume>: <fpage>262</fpage>–<lpage>266</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature12601" xlink:type="simple">10.1038/nature12601</ext-link></comment> <object-id pub-id-type="pmid">24107996</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liu</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Seiler</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Zars</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ito</surname> <given-names>K</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Distinct memory traces for two visual features in the <italic>Drosophila</italic> brain</article-title>. <source>Nature</source> <volume>439</volume>: <fpage>551</fpage>–<lpage>556</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature04381" xlink:type="simple">10.1038/nature04381</ext-link></comment> <object-id pub-id-type="pmid">16452971</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Neuser</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Triphan</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Mronz</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Poeck</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Strauss</surname> <given-names>R</given-names></name> (<year>2008</year>) <article-title>Analysis of a spatial orientation memory in <italic>Drosophila</italic></article-title>. <source>Nature</source> <volume>453</volume>: <fpage>1244</fpage>–<lpage>1248</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature07003" xlink:type="simple">10.1038/nature07003</ext-link></comment> <object-id pub-id-type="pmid">18509336</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Seelig</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Jayaraman</surname> <given-names>V</given-names></name> (<year>2015</year>) <article-title>Neural dynamics for landmark orientation and angular path integration</article-title>. <source>Nature</source> <volume>521</volume>: <fpage>186</fpage>–<lpage>191</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature14446" xlink:type="simple">10.1038/nature14446</ext-link></comment> <object-id pub-id-type="pmid">25971509</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Reichardt</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Wenking</surname> <given-names>H</given-names></name> (<year>1969</year>) <article-title>Optical detection and fixation of objects by fixed flying flies</article-title>. <source>Naturwissenschaften</source> <volume>56</volume>: <fpage>424</fpage>–<lpage>424</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00593644" xlink:type="simple">10.1007/BF00593644</ext-link></comment> <object-id pub-id-type="pmid">5362004</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Götz</surname> <given-names>KG</given-names></name> (<year>1987</year>) <article-title>Course-control, metabolism and wing interference during ultralong tethered flight in <italic>Drosophila melanogaster</italic></article-title>. <source>J Exp Biol</source> <volume>128</volume>: <fpage>35</fpage>–<lpage>46</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005735.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Strauss</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Heisenberg</surname> <given-names>M</given-names></name> (<year>1993</year>) <article-title>A higher control center of locomotor behavior in the <italic>Drosophila</italic> brain</article-title>. <source>J Neurosci</source> <volume>13</volume>: <fpage>1852</fpage>–<lpage>861</lpage>. <object-id pub-id-type="pmid">8478679</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maimon</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Straw</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Dickinson</surname> <given-names>MH</given-names></name> (<year>2008</year>) <article-title>A simple vision-based algorithm for decision making in flying <italic>Drosophila</italic></article-title>. <source>Curr Biol</source> <volume>18</volume>: <fpage>464</fpage>–<lpage>470</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2008.02.054" xlink:type="simple">10.1016/j.cub.2008.02.054</ext-link></comment> <object-id pub-id-type="pmid">18342508</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ernst</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Heisenberg</surname> <given-names>M</given-names></name> (<year>1999</year>) <article-title>The memory template in <italic>Drosophila</italic> pattern vision at the flight simulator</article-title>. <source>Vision Res</source> <volume>39</volume>: <fpage>3920</fpage>–<lpage>3933</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0042-6989(99)00114-5" xlink:type="simple">10.1016/S0042-6989(99)00114-5</ext-link></comment> <object-id pub-id-type="pmid">10748925</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Guo</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Gong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Gong</surname> <given-names>Z</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Differential roles of the fan-shaped body and the ellipsoid body in <italic>Drosophila</italic> visual pattern memory</article-title>. <source>Learn Mem</source> <volume>16</volume>: <fpage>289</fpage>–<lpage>295</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1101/lm.1331809" xlink:type="simple">10.1101/lm.1331809</ext-link></comment> <object-id pub-id-type="pmid">19389914</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>von Frisch</surname> <given-names>K</given-names></name> (<year>1914</year>) <article-title>Der Farbensinn und Formensinn der Biene</article-title>. <source>Zool Jahrb</source> <volume>35</volume>: <fpage>1</fpage>–<lpage>188</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005735.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Giurfa</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Menzel</surname> <given-names>R</given-names></name> (<year>1997</year>) <article-title>Insect visual perception: complex abilities of simple nervous systems</article-title>. <source>Curr Opin Neurobiol</source> <volume>7</volume>: <fpage>505</fpage>–<lpage>513</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0959-4388(97)80030-X" xlink:type="simple">10.1016/S0959-4388(97)80030-X</ext-link></comment> <object-id pub-id-type="pmid">9287201</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref018">
<label>18</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Horridge</surname> <given-names>GA</given-names></name> (<year>2009</year>) <source>What does the honeybee see and how do we know?: A critique of scientific reason</source>. <publisher-loc>Canberra</publisher-loc>: <publisher-name>ANU E Press</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1005735.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Srinivasan</surname> <given-names>MV</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>SW</given-names></name>, <name name-style="western"><surname>Witney</surname> <given-names>K</given-names></name> (<year>1994</year>) <article-title>Visual discrimination of pattern orientation by honeybees: performance and implications for ‘cortical’ processing</article-title>. <source>Phil Trans R Soc B</source> <volume>343</volume>: <fpage>199</fpage>–<lpage>210</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.1994.0021" xlink:type="simple">10.1098/rstb.1994.0021</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Young</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Armstrong</surname> <given-names>JD</given-names></name> (<year>2010</year>) <article-title>Structure of the adult central complex in <italic>Drosophila</italic>: organization of distinct neuronal subsets</article-title>. <source>J Comp Neurol</source> <volume>518</volume>: <fpage>1500</fpage>–<lpage>1524</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/cne.22284" xlink:type="simple">10.1002/cne.22284</ext-link></comment> <object-id pub-id-type="pmid">20187142</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pfeiffer</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Homberg</surname> <given-names>U</given-names></name> (<year>2014</year>) <article-title>Organization and functional roles of the central complex in the insect brain</article-title>. <source>Annu Rev Entomol</source> <volume>59</volume>: <fpage>165</fpage>–<lpage>184</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-ento-011613-162031" xlink:type="simple">10.1146/annurev-ento-011613-162031</ext-link></comment> <object-id pub-id-type="pmid">24160424</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sitaraman</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Zars</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>LaFerriere</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>YC</given-names></name>, <name name-style="western"><surname>Sable-Smith</surname> <given-names>A</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Serotonin is necessary for place memory in <italic>Drosophila</italic></article-title>. <source>Proc Natl Acad Sci USA</source> <volume>105</volume>: <fpage>5579</fpage>–<lpage>5584</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0710168105" xlink:type="simple">10.1073/pnas.0710168105</ext-link></comment> <object-id pub-id-type="pmid">18385379</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sitaraman</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Zars</surname> <given-names>T</given-names></name> (<year>2010</year>) <article-title>Lack of prediction for high-temperature exposures enhances <italic>Drosophila</italic> place learning</article-title>. <source>J Exp Biol</source> <volume>213</volume>: <fpage>4018</fpage>–<lpage>4022</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1242/jeb.050344" xlink:type="simple">10.1242/jeb.050344</ext-link></comment> <object-id pub-id-type="pmid">21075943</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hubel</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Wiesel</surname> <given-names>TN</given-names></name> (<year>1962</year>) <article-title>Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex</article-title>. <source>J Physiol (Lond)</source> <volume>160</volume>: <fpage>106</fpage>–<lpage>154</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1113/jphysiol.1962.sp006837" xlink:type="simple">10.1113/jphysiol.1962.sp006837</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wystrach</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Dewar</surname> <given-names>ADM</given-names></name>, <name name-style="western"><surname>Graham</surname> <given-names>P</given-names></name> (<year>2014</year>) <article-title>Insect vision: emergence of pattern recognition from coarse encoding</article-title>. <source>Curr Biol</source> <volume>24</volume>: <fpage>R78</fpage>–<lpage>R80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2013.11.054" xlink:type="simple">10.1016/j.cub.2013.11.054</ext-link></comment> <object-id pub-id-type="pmid">24456981</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dewar</surname> <given-names>ADM</given-names></name>, <name name-style="western"><surname>Wystrach</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Graham</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Philippides</surname> <given-names>A</given-names></name> (<year>2015</year>) <article-title>Navigation-specific neural coding in the visual system of <italic>Drosophila</italic></article-title>. <source>Biosystems</source> <volume>136</volume>: <fpage>120</fpage>–<lpage>127</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.biosystems.2015.07.008" xlink:type="simple">10.1016/j.biosystems.2015.07.008</ext-link></comment> <object-id pub-id-type="pmid">26310914</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Osorio</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Srinivasan</surname> <given-names>MV</given-names></name>, <name name-style="western"><surname>Pinter</surname> <given-names>RB</given-names></name> (<year>1990</year>) <article-title>What causes edge fixation in walking flies?</article-title> <source>J Exp Biol</source> <volume>149</volume>: <fpage>281</fpage>–<lpage>292</lpage>. <object-id pub-id-type="pmid">2324671</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dill</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wolf</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Heisenberg</surname> <given-names>M</given-names></name> (<year>1993</year>) <article-title>Visual pattern recognition in <italic>Drosophila</italic> involves retinotopic matching</article-title>. <source>Nature</source> <volume>365</volume>: <fpage>751</fpage>–<lpage>753</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/365751a0" xlink:type="simple">10.1038/365751a0</ext-link></comment> <object-id pub-id-type="pmid">8413652</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wang</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Pan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Jiang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Chatzimanolis</surname> <given-names>L</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Visual pattern memory requires <italic>foraging</italic> function in the central complex of <italic>Drosophila</italic></article-title>. <source>Learn Mem</source> <volume>15</volume>: <fpage>133</fpage>–<lpage>142</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1101/lm.873008" xlink:type="simple">10.1101/lm.873008</ext-link></comment> <object-id pub-id-type="pmid">18310460</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dyer</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Chittka</surname> <given-names>L</given-names></name> (<year>2004</year>) <article-title>Bumblebees (<italic>Bombus terrestris</italic>) sacrifice foraging speed to solve difficult colour discrimination tasks</article-title>. <source>J Comp Physiol A</source> <volume>190</volume>: <fpage>759</fpage>–<lpage>763</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00359-004-0547-y" xlink:type="simple">10.1007/s00359-004-0547-y</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Solanki</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Wolf</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Heisenberg</surname> <given-names>M</given-names></name> (<year>2015</year>) <article-title>Central complex and mushroom bodies mediate novelty choice behavior in <italic>Drosophila</italic></article-title>. <source>J Neurogenet</source> <volume>29</volume>: <fpage>1</fpage>–<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3109/01677063.2014.1002661" xlink:type="simple">10.3109/01677063.2014.1002661</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cope</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Sabo</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Vasilaki</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Barron</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Marshall</surname> <given-names>JAR</given-names></name>, <etal>et al</etal>. (<year>2017</year>) <article-title>A computational model of the integration of landmarks and motion in the insect central complex</article-title>. <source>PLOS ONE</source> <volume>12</volume>: <fpage>e0172325</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0172325" xlink:type="simple">10.1371/journal.pone.0172325</ext-link></comment> <object-id pub-id-type="pmid">28241061</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tomchik</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>RL</given-names></name> (<year>2008</year>) <article-title>Behavioural neuroscience: out of sight, but not out of mind</article-title>. <source>Nature</source> <volume>453</volume>: <fpage>1192</fpage>–<lpage>1194</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/4531192a" xlink:type="simple">10.1038/4531192a</ext-link></comment> <object-id pub-id-type="pmid">18580936</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Guo</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Du</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Yuan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gong</surname> <given-names>H</given-names></name>, <etal>et al</etal>. (<year>2015</year>) <article-title>A conditioned visual orientation requires the ellipsoid body in <italic>Drosophila</italic></article-title>. <source>Learn Mem</source> <volume>22</volume>: <fpage>56</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1101/lm.036863.114" xlink:type="simple">10.1101/lm.036863.114</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ehmer</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Gronenberg</surname> <given-names>W</given-names></name> (<year>2002</year>) <article-title>Segregation of visual input to the mushroom bodies in the honeybee (<italic>Apis mellifera</italic>)</article-title>. <source>J Comp Neurol</source> <volume>451</volume>: <fpage>362</fpage>–<lpage>373</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/cne.10355" xlink:type="simple">10.1002/cne.10355</ext-link></comment> <object-id pub-id-type="pmid">12210130</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wolf</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wittig</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Wustmann</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Eyding</surname> <given-names>D</given-names></name>, <etal>et al</etal>. (<year>1998</year>) <article-title><italic>Drosophila</italic> mushroom bodies are dispensable for visual, tactile, and motor learning</article-title>. <source>Learn Mem</source> <volume>5</volume>: <fpage>166</fpage>–<lpage>178</lpage>. <object-id pub-id-type="pmid">10454381</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Menzel</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Giurfa</surname> <given-names>M</given-names></name> (<year>2001</year>) <article-title>Cognitive architecture of a mini-brain: the honeybee</article-title>. <source>Trends Cogn Sci</source> <volume>5</volume>: <fpage>62</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S1364-6613(00)01601-6" xlink:type="simple">10.1016/S1364-6613(00)01601-6</ext-link></comment> <object-id pub-id-type="pmid">11166636</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gould</surname> <given-names>JL</given-names></name> (<year>1985</year>) <article-title>How bees remember flower shapes</article-title>. <source>Science</source> <volume>227</volume>: <fpage>1492</fpage>–<lpage>1494</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.227.4693.1492" xlink:type="simple">10.1126/science.227.4693.1492</ext-link></comment> <object-id pub-id-type="pmid">17777783</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Layne</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Land</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Zeil</surname> <given-names>J</given-names></name> (<year>1997</year>) <article-title>Fiddler crabs use the visual horizon to distinguish predators from conspecifics: a review of the evidence</article-title>. <source>J Mar Biol Assoc UK</source> <volume>77</volume>: <fpage>43</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1017/S0025315400033774" xlink:type="simple">10.1017/S0025315400033774</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chiel</surname> <given-names>HJ</given-names></name>, <name name-style="western"><surname>Beer</surname> <given-names>RD</given-names></name> (<year>1997</year>) <article-title>The brain has a body: adaptive behavior emerges from interactions of nervous system, body and environment</article-title>. <source>Trends Neurosci</source> <volume>20</volume>: <fpage>553</fpage>–<lpage>557</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0166-2236(97)01149-1" xlink:type="simple">10.1016/S0166-2236(97)01149-1</ext-link></comment> <object-id pub-id-type="pmid">9416664</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Green</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Adachi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Shah</surname> <given-names>KK</given-names></name>, <name name-style="western"><surname>Hirokawa</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Magani</surname> <given-names>PS</given-names></name>, <etal>et al</etal>. (<year>2017</year>) <article-title>A neural circuit architecture for angular integration in <italic>Drosophila</italic></article-title>. <source>Nature</source> <volume>546</volume>: <fpage>101</fpage>–<lpage>106</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature22343" xlink:type="simple">10.1038/nature22343</ext-link></comment> <object-id pub-id-type="pmid">28538731</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Turner-Evans</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Wegener</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rouault</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Franconville</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wolff</surname> <given-names>T</given-names></name>, <etal>et al</etal>. (<year>2017</year>) <article-title>Angular velocity integration in a fly heading circuit</article-title>. <source>eLife</source> <volume>6</volume>: <fpage>1</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7554/eLife.23496" xlink:type="simple">10.7554/eLife.23496</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Nern</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Franconville</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Dana</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Schreiter</surname> <given-names>ER</given-names></name>, <etal>et al</etal>. (<year>2017</year>) <article-title>Neural signatures of dynamic stimulus selection in <italic>Drosophila</italic></article-title>. <source>Nat Neurosci</source> <volume>20</volume>: <fpage>1104</fpage>–<lpage>1113</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.4581" xlink:type="simple">10.1038/nn.4581</ext-link></comment> <object-id pub-id-type="pmid">28604683</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref044">
<label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Diehl PU, Neil D, Binas J, Cook M, Liu SC, et al. (2015) Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing. In: Proceedings of the International Joint Conference on Neural Networks. IEEE, pp. 1–8.</mixed-citation>
</ref>
<ref id="pcbi.1005735.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Azanchi</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Kaun</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Heberlein</surname> <given-names>U</given-names></name> (<year>2013</year>) <article-title>Competing dopamine neurons drive oviposition choice for ethanol in <italic>Drosophila</italic></article-title>. <source>Proc Natl Acad Sci USA</source> <volume>110</volume>: <fpage>21153</fpage>–<lpage>21158</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1320208110" xlink:type="simple">10.1073/pnas.1320208110</ext-link></comment> <object-id pub-id-type="pmid">24324162</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liu</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>Tabuchi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>MN</given-names></name> (<year>2016</year>) <article-title>Sleep drive is encoded by neural plastic changes in a dedicated circuit</article-title>. <source>Cell</source> <volume>165</volume>: <fpage>1347</fpage>–<lpage>1360</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cell.2016.04.013" xlink:type="simple">10.1016/j.cell.2016.04.013</ext-link></comment> <object-id pub-id-type="pmid">27212237</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dickinson</surname> <given-names>MH</given-names></name> (<year>2014</year>) <article-title>Death Valley, <italic>Drosophila</italic>, and the Devonian Toolkit</article-title>. <source>Annu Rev Entomol</source> <volume>59</volume>: <fpage>51</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-ento-011613-162041" xlink:type="simple">10.1146/annurev-ento-011613-162041</ext-link></comment> <object-id pub-id-type="pmid">24160432</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Horridge</surname> <given-names>A</given-names></name> (<year>2000</year>) <article-title>Seven experiments on pattern vision of the honeybee, with a model</article-title>. <source>Vision Research</source> <volume>40</volume>: <fpage>2589</fpage>–<lpage>2603</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0042-6989(00)00096-1" xlink:type="simple">10.1016/S0042-6989(00)00096-1</ext-link></comment> <object-id pub-id-type="pmid">10958911</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Webb</surname> <given-names>B</given-names></name> (<year>2009</year>) <article-title>Animals versus animats: Or why not model the real iguana?</article-title> <source>Adapt Behav</source> <volume>17</volume>: <fpage>269</fpage>–<lpage>286</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/1059712309339867" xlink:type="simple">10.1177/1059712309339867</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Weber</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Machens</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Borst</surname> <given-names>A</given-names></name> (<year>2010</year>) <article-title>Spatiotemporal response properties of optic-flow processing neurons</article-title>. <source>Neuron</source> <volume>67</volume>: <fpage>629</fpage>–<lpage>642</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2010.07.017" xlink:type="simple">10.1016/j.neuron.2010.07.017</ext-link></comment> <object-id pub-id-type="pmid">20797539</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zeil</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hofmann</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Chahl</surname> <given-names>JS</given-names></name> (<year>2003</year>) <article-title>Catchment areas of panoramic snapshots in outdoor scenes</article-title>. <source>J Opt Soc Am A</source> <volume>20</volume>: <fpage>450</fpage>–<lpage>469</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1364/JOSAA.20.000450" xlink:type="simple">10.1364/JOSAA.20.000450</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005735.ref052">
<label>52</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Götz</surname> <given-names>KG</given-names></name> (<year>1980</year>) <chapter-title>Visual guidance in <italic>Drosophila</italic></chapter-title>. In: <name name-style="western"><surname>Siddiqi</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Babu</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Hall</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Hall</surname> <given-names>JC</given-names></name>, editors, <source>Development and Neurobiology of <italic>Drosophila</italic></source>, <publisher-name>Springer</publisher-name> <publisher-loc>US</publisher-loc>, volume 16 of <italic>Basic Life Sciences</italic>. pp. <fpage>391</fpage>–<lpage>407</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005735.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bülthoff</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Götz</surname> <given-names>KG</given-names></name>, <name name-style="western"><surname>Herre</surname> <given-names>M</given-names></name> (<year>1982</year>) <article-title>Recurrent inversion of visual orientation in the walking fly, <italic>Drosophila melanogaster</italic></article-title>. <source>J Comp Physiol</source> <volume>148</volume>: <fpage>471</fpage>–<lpage>481</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00619785" xlink:type="simple">10.1007/BF00619785</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>