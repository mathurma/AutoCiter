<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006829</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-01716</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Human performance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Human performance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuronal tuning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Modeling second-order boundary perception: A machine learning approach</article-title>
<alt-title alt-title-type="running-head">Modeling second-order boundary perception</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1568-1390</contrib-id>
<name name-style="western">
<surname>DiMattina</surname>
<given-names>Christopher</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9988-3936</contrib-id>
<name name-style="western">
<surname>Baker</surname>
<given-names>Curtis L.</given-names>
<suffix>Jr.</suffix>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Computational Perception Laboratory, Department of Psychology, Florida Gulf Coast University, Fort Myers, Florida, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>McGill Vision Research Unit, Department of Ophthalmology, McGill University, Montreal, Quebec, Canada</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Einhäuser</surname>
<given-names>Wolfgang</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Technische Universitat Chemnitz, GERMANY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">cdimattina@fgcu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>18</day>
<month>3</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>3</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>3</issue>
<elocation-id>e1006829</elocation-id>
<history>
<date date-type="received">
<day>5</day>
<month>10</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>15</day>
<month>1</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>DiMattina, Baker</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006829"/>
<abstract>
<p>Visual pattern detection and discrimination are essential first steps for scene analysis. Numerous human psychophysical studies have modeled visual pattern detection and discrimination by estimating linear templates for classifying noisy stimuli defined by spatial variations in pixel intensities. However, such methods are poorly suited to understanding sensory processing mechanisms for complex visual stimuli such as second-order boundaries defined by spatial differences in contrast or texture. We introduce a novel machine learning framework for modeling human perception of second-order visual stimuli, using image-computable hierarchical neural network models fit directly to psychophysical trial data. This framework is applied to modeling visual processing of boundaries defined by differences in the contrast of a carrier texture pattern, in two different psychophysical tasks: (1) boundary orientation identification, and (2) fine orientation discrimination. Cross-validation analysis is employed to optimize model hyper-parameters, and demonstrate that these models are able to accurately predict human performance on novel stimulus sets not used for fitting model parameters. We find that, like the ideal observer, human observers take a region-based approach to the orientation identification task, while taking an edge-based approach to the fine orientation discrimination task. How observers integrate contrast modulation across orientation channels is investigated by fitting psychophysical data with two models representing competing hypotheses, revealing a preference for a model which combines multiple orientations at the earliest possible stage. Our results suggest that this machine learning approach has much potential to advance the study of second-order visual processing, and we outline future steps towards generalizing the method to modeling visual segmentation of natural texture boundaries. This study demonstrates how machine learning methodology can be fruitfully applied to psychophysical studies of second-order visual processing.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Many naturally occurring visual boundaries are defined by spatial differences in features other than luminance, for example by differences in texture or contrast. Quantitative models of such “second-order” boundary perception cannot be estimated using the standard regression techniques (known as “classification images”) commonly applied to “first-order”, luminance-defined stimuli. Here we present a novel machine learning approach to modeling second-order boundary perception using hierarchical neural networks. In contrast to previous quantitative studies of second-order boundary perception, we directly estimate network model parameters using psychophysical trial data. We demonstrate that our method can reveal different spatial summation strategies that human observers utilize for different kinds of second-order boundary perception tasks, and can be used to compare competing hypotheses of how contrast modulation is integrated across orientation channels. We outline extensions of the methodology to other kinds of second-order boundaries, including those in natural images.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000038</institution-id>
<institution>Natural Sciences and Engineering Research Council of Canada</institution>
</institution-wrap>
</funding-source>
<award-id>OPG0001978, RGPIN-2017-05292</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9988-3936</contrib-id>
<name name-style="western">
<surname>Baker</surname>
<given-names>Curtis L.</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>Supported by Canadian NSERC Grants OPG0001978 and RGPIN-2017-05292 to C.L.B. and a FGCU Professional Development Grant to C.D. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="13"/>
<table-count count="0"/>
<page-count count="41"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-03-28</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Data are permanently hosted on an FGCU server and can be accessed at <ext-link ext-link-type="uri" xlink:href="https://www.fgcu.edu/contrastexperiment" xlink:type="simple">https://www.fgcu.edu/contrastexperiment</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Many of the most common functions of sensory systems involve detection, identification or discrimination of particular stimuli. For example, a critically important ability of early visual processing is to segment images into distinct regions, which may correspond to cohesive surfaces or objects [<xref ref-type="bibr" rid="pcbi.1006829.ref001">1</xref>]. To better understand the underlying mechanisms, it would be useful to fit biologically plausible models of surface segmentation to psychophysical data, and assess how well such models can account for independent datasets.</p>
<p>A widely used approach to modeling human psychophysics has been to assume that performance can be understood in terms of how well a visual stimulus matches an internal “template”, typically modeled as a linear spatial filter. This approach uses high-dimensional regression models fit to data from tasks in which a subject detects or classifies a sensory stimulus presented with superimposed white noise [<xref ref-type="bibr" rid="pcbi.1006829.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref005">5</xref>], to recover an estimate of the linear filter that best accounts for the data. This system-identification (SI) approach to psychophysics, often called “psychophysical reverse correlation” or “classification image analysis”, parallels neurophysiology studies that fit linear filter models to neural responses elicited by white noise stimuli [<xref ref-type="bibr" rid="pcbi.1006829.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref007">7</xref>]. Psychophysical SI methods have been applied in varied problem domains and yielded valuable insight into perceptual mechanisms [<xref ref-type="bibr" rid="pcbi.1006829.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref013">13</xref>].</p>
<p>One limitation of most previous psychophysical system identification studies has been the use of stimuli that are typically defined by spatial or temporal variations in first-order statistics like luminance or color. However, many natural visual stimuli such as occlusion boundaries (e.g., the top image patch in <xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1a</xref>, showing animal fur occluding a forest floor) are defined by spatial variations in higher-order image statistics, e.g. texture or contrast, as well as simple luminance differences [<xref ref-type="bibr" rid="pcbi.1006829.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref018">18</xref>]. Such “second-order” boundaries (<xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1a</xref>, bottom) have been of particular interest because their detection cannot be explained by simple linear filters [<xref ref-type="bibr" rid="pcbi.1006829.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref020">20</xref>], and consequently human performance cannot be readily modeled using conventional psychophysical SI methods. More generally, there is a need for a psychophysical modeling approach that can handle a broader range of stimuli and tasks.</p>
<fig id="pcbi.1006829.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006829.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Examples of second-order boundary stimuli, with varying texture density and modulation depth.</title>
<p><bold>(a)</bold> <italic>Top</italic>: A natural occlusion boundary formed by animal fur (foreground, lower right) occluding the forest floor (background, upper left). This boundary has similar average luminance on both sides, but clearly visible differences in texture. <italic>Bottom</italic>: A contrast-defined boundary (used in Experiment 3) with identical mean luminance in each region, but different contrasts of the texture elements. In this example the contrast modulation envelope has a left-oblique (-45 deg. w.r.t vertical) orientation. <bold>(b)</bold> Examples of contrast-modulated micropattern stimuli, for three densities of micropatterns and three modulation depths, all having a right-oblique (+45 deg.) boundary orientation. Boundary segmentation is typically easier with increasing modulation depth and micropattern density. <bold>(c)</bold> Schematic illustration of the two psychophysical tasks used here (<italic>orientation identification</italic>: Experiments 1 and 3, <italic>orientation discrimination</italic>: Experiment 2), in both cases two-alternative forced-choice judgements of left- vs. right-oblique boundary orientation. Stimuli shown are representative of those used in Experiments 1 and 2. In the identification task boundaries are oriented at (+/- 45 deg.), and in the discrimination task boundaries are oriented slightly off vertical (+/- 6–7 deg.).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.g001" xlink:type="simple"/>
</fig>
<p>Here we present a novel machine learning method to extend psychophysical SI to more complex naturalistic stimuli such as second-order boundaries. Our general approach is to fit image-computable hierarchical neural network models of second-order vision (<xref ref-type="fig" rid="pcbi.1006829.g002">Fig 2</xref>) directly to psychophysical trial data, similar to recent studies using multi-stage models to characterize nonlinear responses in sensory neurons [<xref ref-type="bibr" rid="pcbi.1006829.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref024">24</xref>]. Our application of this framework generalizes a class of biologically plausible “Filter-Rectify-Filter” (FRF) models that have often been invoked to explain second-order processing [<xref ref-type="bibr" rid="pcbi.1006829.ref020">20</xref>]. However, in contrast to previous general schemes for second-order vision [<xref ref-type="bibr" rid="pcbi.1006829.ref025">25</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref027">27</xref>] and quantitative models [<xref ref-type="bibr" rid="pcbi.1006829.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref029">29</xref>], we take a machine learning approach in which we estimate the defining model parameters by direct fitting to psychophysical trial data. This permits more incisive tests of predictions on novel datasets in order to validate our models, and to evaluate competing models.</p>
<fig id="pcbi.1006829.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006829.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Neural network model implementing a FRF (Filter-Rectify-Filter) arrangement for texture segmentation.</title>
<p>Parameters shown in blue are learned from data, those in black are fixed. Stimulus image <bold>I</bold> is filtered by a bank of first-stage energy filters, resembling V1 complex cells, whose downsampled responses provide a feature vector input <bold>x</bold> to two second-stage filters. An optimization algorithm adjusts connection weights <bold>w</bold><sub>L</sub>, <bold>w</bold><sub>R</sub> (blue lines), producing second-stage filters which are selective for left-oblique (L) or right-oblique (R) contrast-defined boundaries, to give responses consistent with human psychophysical data. The output of each second-stage filter is passed through a nonlinearity <italic>h</italic>(<italic>u</italic>) = |<italic>u</italic>|<sup><italic>α</italic></sup> (blue curve) whose shape parameter α is also estimated from the psychophysical data. Fixed output weights <italic>v</italic><sub>R</sub> = +1, <italic>v</italic><sub>L</sub> = -1 lead to the decision variable <italic>u</italic> = <italic>s</italic><sub>R</sub> − <italic>s</italic><sub>L</sub> + <italic>v</italic><sub>0</sub>, which is input to a sigmoid function to determine the probability of the observer classifying a given boundary as being right-oblique.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.g002" xlink:type="simple"/>
</fig>
<p>We focus on contrast-defined second-order boundaries (<xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1a and 1b</xref>) since they are well studied psychophysically [<xref ref-type="bibr" rid="pcbi.1006829.ref030">30</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref034">34</xref>], and there are neurons in the early mammalian visual cortex that give selective responses to contrast modulation stimuli [<xref ref-type="bibr" rid="pcbi.1006829.ref035">35</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref038">38</xref>]. Performance on two tasks making use of contrast-defined boundaries is examined: (1) identification of boundary orientation, and (2) discrimination between two slightly off-vertical boundaries (<xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1c</xref>). The results demonstrate that subjects utilize spatial information in markedly different ways for these two tasks, employing “region-based” processing for the orientation identification task and “edge-based” processing for the orientation discrimination task, in both cases consistent with an ideal observer.</p>
<p>We further demonstrate the power of this methodology for evaluating competing hypotheses regarding the nature of channel summation within the FRF scheme [<xref ref-type="bibr" rid="pcbi.1006829.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref039">39</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref041">41</xref>] by comparing the ability of two different model architectures to fit human performance on a contrast edge detection task in which the carrier pattern contains elements with two orientations. Post-hoc Bayesian model comparison [<xref ref-type="bibr" rid="pcbi.1006829.ref042">42</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref044">44</xref>] demonstrates a preference for a model in which carrier pattern orientation is integrated across channels by the second-stage filters, consistent with previous psychophysical studies [<xref ref-type="bibr" rid="pcbi.1006829.ref040">40</xref>].</p>
<p>Finally, we discuss potential applications of this methodology to future quantitative studies of first- and second-order cue integration [<xref ref-type="bibr" rid="pcbi.1006829.ref045">45</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref047">47</xref>] and natural boundary detection [<xref ref-type="bibr" rid="pcbi.1006829.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref048">48</xref>]. We suggest that the application of machine learning methodology similar to that presented here can potentially help to better quantify visual processing of natural occlusion boundaries and complex natural visual stimuli more generally.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>Observers performed two different psychophysical tasks making use of contrast-modulated boundary stimuli like those shown in <xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1</xref> (see <xref ref-type="sec" rid="sec028">Methods</xref>). In the first task, orientation-identification (<xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1c</xref>, left), observers determined in which of two oblique orientations (-/+ 45 deg, L/R) a near-threshold contrast boundary was presented, with varying amounts of contrast modulation (Experiments 1 and 3). We also employed an orientation-discrimination task (<xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1c</xref>, right) in which observers indicated the orientation (L/R) of a supra-threshold contrast boundary, whose orientation was perturbed slightly from vertical by varying amounts (-/+ 6–7 deg.) (Experiment 2). In all experiments, stimuli were presented centrally, subtending 4 degrees of visual angle (dva), with envelope phase randomized (0 or 180 deg.) across trials. Trial-wise responses obtained from these experiments were used to fit and evaluate a model (<xref ref-type="fig" rid="pcbi.1006829.g002">Fig 2</xref>) implementing a “Filter-Rectifty-Filter” (FRF) operation [<xref ref-type="bibr" rid="pcbi.1006829.ref020">20</xref>] on each stimulus image, with the goal of characterizing the nature of spatial summation for perception of contrast boundaries, as well as the shape of the second-stage filter nonlinearity. We describe the model in greater depth before presenting the experimental results.</p>
<sec id="sec003">
<title>Modeling framework</title>
<p>The model used to fit psychophysical trial data (<xref ref-type="fig" rid="pcbi.1006829.g002">Fig 2</xref>) employed a first stage comprised of a bank of fine-scale Gabor filters that compute local oriented energy in a manner similar to V1 complex cells [<xref ref-type="bibr" rid="pcbi.1006829.ref049">49</xref>]. The second stage consisted of a pair of filters, representing the two task alternatives (L/R), each instantiated by a weighted sum of the normalized outputs of the first-stage filters. In contrast to the first-stage filters, these second-stage filters analyze the image at a more coarse spatial scale, integrating variations in contrast across the boundary. Finally, the outputs of these two second-stage filters are passed through a static non-linearity <italic>h</italic>(<italic>u</italic>) = |<italic>u</italic>|<sup><italic>α</italic></sup> and then compared to decide on a trial-by-trial basis if a given stimulus boundary appeared more likely to be tilted left or right (L/R), i.e. the probability 0 &lt; P(R) &lt; 1 of the stimulus being classified by the observer as R. The main goal of Experiments 1 and 2 was to estimate the shapes of the second-stage filters. The filter shapes are not specified beforehand, but rather are learned by fitting actual psychophysical data, with the goal of revealing how the observer spatially integrates texture information.</p>
<p>To reduce model parameter space dimensionality, we performed downsampling between the first- and second-stage filters, i.e. pooling of spatially adjacent first-stage filter responses. Here we compared three ways of implementing downsampling within each locality: (1) taking the simple average (AVG), (2) taking the largest value (MAX) [<xref ref-type="bibr" rid="pcbi.1006829.ref050">50</xref>], and (3) taking just one of the values, e.g. the center one (sub-sampling, SUB) [<xref ref-type="bibr" rid="pcbi.1006829.ref026">26</xref>]. In Experiment 1 we considered all three pooling rules; most analyses utilized 16x16 downsampling, but 3 downsampling sizes (22x22, 16x16 and 8x8) were compared. In Experiments 2 and 3 we considered only AVG pooling and a single downsampling size (Experiment 2: 22x22, Experiment 3: 12x12).</p>
<p>We implement the model’s final (L/R) classification in two alternative ways: In the <italic>deterministic</italic> model (DET), if the output of the R filter is larger than that of the L filter (plus the bias term) so that P(R) &gt; 0.5, the stimulus will be classified as R, otherwise it is classified as L. In the <italic>stochastic</italic> model (STO), the output P(R) specifies only the probability of classifying the stimulus as R or L, thus introducing randomness to the model without requiring an additional free parameter.</p>
<p>A common approach to make the estimation problem more tractable is to utilize reasonable prior constraints on the model parameter space [<xref ref-type="bibr" rid="pcbi.1006829.ref051">51</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref053">53</xref>]. Here we use two kinds of Bayesian priors: (1) ridge-regression [<xref ref-type="bibr" rid="pcbi.1006829.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref054">54</xref>] to penalize large weights (<italic>ridge</italic>), and (2) ridge-regression combined with an additional smoothness prior [<xref ref-type="bibr" rid="pcbi.1006829.ref051">51</xref>] to penalize excessively “rough” second-stage filter weight profiles (<italic>ridge + smooth</italic>). Model performance was evaluated by using different randomly selected partitions (called <italic>folds</italic>) of the data for training and testing, a standard practice in machine learning (i.e., <italic>k</italic>-fold cross-validation [<xref ref-type="bibr" rid="pcbi.1006829.ref051">51</xref>]). Additional details of the model are given in Methods.</p>
</sec>
<sec id="sec004">
<title>Experiment 1: Boundary orientation identification</title>
<sec id="sec005">
<title>Stimuli and task</title>
<p>Observers viewed contrast-defined texture boundaries with varying degrees of modulation depth (<xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1b</xref>), and performed forced-choice judgments of whether the boundary was oriented right- or left-oblique (45 degrees clockwise or counterclockwise, <xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1c</xref> left). In Experiment 1-VAR (7 observers), a method of constant stimuli was used to vary contrast modulation depth over a range spanning values near threshold but also including some levels producing near-perfect, or chance, performance. In another version (Experiment 1-FIX, 3 observers) the contrast modulation depth was held constant at a near-threshold level.</p>
</sec>
<sec id="sec006">
<title>Model fit to representative observers</title>
<p><xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3a</xref> shows the results of fitting the model (16x16 downsampling, AVG pooling) to data from one representative observer (AMA) in Experiment 1-VAR. The top panels show estimated second-stage filter weights as image maps across the spatial extent of the stimuli, with pixel color indicating weight values. The black dashed lines in the bottom plots show mean 1-D profiles, obtained by collapsing the 2-D weight maps orthogonally to the boundary orientations, whereas thick solid lines (red = <italic>ridge</italic>, green = <italic>ridge + smooth</italic>) show profiles obtained from over 30 bootstrap-resampled training sets. We see that these mean 1-D profiles are nearly indistinguishable from one another, all resembling a step-like function (see below). While many of the individual 2-D weights can be quite variable (<xref ref-type="supplementary-material" rid="pcbi.1006829.s007">S7 Fig</xref>), the 1-D weight profile magnitudes lie well away from zero (<xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3</xref>, thin dashed lines show +/- 1 SEM). The addition of the smoothing prior (<italic>ridge + smooth</italic>) results in weighting functions that are very similar, but less “noisy” in appearance, consistent with previous results with classification images and receptive field modeling [<xref ref-type="bibr" rid="pcbi.1006829.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref052">52</xref>]. Furthermore, all three pooling rules (AVG, MAX, SUB) yielded consistent 2-D and 1-D weight profiles (<xref ref-type="supplementary-material" rid="pcbi.1006829.s008">S8 Fig</xref>).</p>
<fig id="pcbi.1006829.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006829.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Estimated second-stage filter weights for two representative observers in Experiment 1 (orientation identification).</title>
<p><bold>(a)</bold> Second-stage filter weights learned by the model for representative observer AMA in Experiment 1-VAR (varying modulation depth) for two different priors (left: <italic>ridge</italic>; right: <italic>ridge + smooth</italic>) with 16x16 AVG downsampling. Top panels show the 2-D filter weights (averaged over 4 training folds) and bottom panels show these 2-D weights collapsed into 1-D profiles (black dashed lines) by averaging along the matrix diagonals (left-oblique) or anti-diagonals (right-oblique). Thick lines (red: <italic>ridge</italic>; green: <italic>ridge + smooth</italic>) denote averages over 30 resampled bootstrapped training sets, and thin dashed lines show +/- 1 SEM. <bold>(b)</bold> Same as (a) but for observer JJF in Experiment 1-FIX (fixed, near-threshold modulation depth). <bold>(c)</bold> Results for ideal observer for Experiment 1-VAR. Organization as in (a), (b) except thick black lines denote averages over 4 training folds and thin dashed lines show fits to individual folds.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.g003" xlink:type="simple"/>
</fig>
<p><xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3b</xref> shows the model fit to data from a representative observer (JJF) in Experiment 1-FIX. In this version of the experiment, the contrast modulation depth was kept constant at approximately threshold level for each observer (<xref ref-type="supplementary-material" rid="pcbi.1006829.s024">S1 Table</xref>), similar to most previous work in the classification image literature [<xref ref-type="bibr" rid="pcbi.1006829.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref008">8</xref>]. Here we see very similar results as for the observer in Experiment 1-VAR, also showing a step-like function for this task. As before, this result was robust to the pooling rule (<xref ref-type="supplementary-material" rid="pcbi.1006829.s009">S9 Fig</xref>).</p>
<p>To assess the extent to which observers were employing the same spatial integration as an ideal observer, we fit the model to stimulus category labels rather than observer responses. The 2-D weight maps and 1-D profiles obtained for an ideal observer in Experiment 1-VAR (<xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3c</xref>), reveal that the ideal weight map is a step-like function like that seen in the human results. Similar results for an ideal observer fit to stimulus category labels in Experiment 1-FIX are shown in <xref ref-type="supplementary-material" rid="pcbi.1006829.s010">S10 Fig</xref>.</p>
</sec>
<sec id="sec007">
<title>Edge-based vs. region-based analysis</title>
<p>When finding a boundary between adjacent textures, the visual system might make use only of texture elements close to the boundary (“edge-based”), or it might utilize the entirety of both adjacent texture regions (“region-based”). A number of previous studies have investigated this question for textures defined by orientation differences [<xref ref-type="bibr" rid="pcbi.1006829.ref055">55</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref057">57</xref>], but to the best of our knowledge this issue has not been addressed systematically for contrast-defined textures. The results from two individual observers (<xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3a and 3b</xref>) as well as an ideal observer (<xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3c</xref>) indicate that the entire texture regions on either side of the boundary are contributing to performance, giving step-like 1-D profiles—all indicative of “region-based” processing.</p>
<p>To examine the robustness of this result across observers and for different model variants, <xref ref-type="fig" rid="pcbi.1006829.g004">Fig 4</xref> shows the 1-D averaged profiles of second-stage filter weights for all observers (thin colored lines) and group averages (thick black lines) in Experiment 1-VAR (<xref ref-type="fig" rid="pcbi.1006829.g004">Fig 4a</xref>) and Experiment 1-FIX (<xref ref-type="fig" rid="pcbi.1006829.g004">Fig 4b</xref>). We see that the magnitudes of the 1-D weights are approximately constant as one moves from the diagonal to the boundary, indicative of region-based processing, and that these results are also consistent for all three pooling rules (AVG, MAX, SUB). To explore whether the step-like 1-D profile result is consistent with an ideal observer, plots of the normalized (scaled to unit norm) ideal observer 1-D weight profiles (30 bootstraps) are shown in <xref ref-type="fig" rid="pcbi.1006829.g005">Fig 5a</xref> as thick black lines, with 95% confidence intervals (+/- 1.96 SEM) plotted as dashed black lines. We see that the normalized 1-D weight profiles for individual observers (dashed color lines), and for means over observers (thick red dashed lines) generally lie within the 95% confidence intervals of the ideal observer. Therefore, the performance of the human observers is very much consistent with that of the ideal observer, both of which give similar step-like profiles and thus seem to integrate contrast information across the whole region.</p>
<fig id="pcbi.1006829.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006829.g004</object-id>
<label>Fig 4</label>
<caption>
<title>1-D profiles of the second-stage filter weights for all observers in Experiment 1.</title>
<p><bold>(a)</bold> 1-D profiles for individual observers in Experiment1-VAR (colored lines) plotted with the average across observers (thick black lines) for three sampling rules (AVG, MAX, SUB) and both priors. <bold>(b)</bold> Same as (a) but for Experiment 1-FIX.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.g004" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006829.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006829.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Normalized 1-D profiles of the average second-stage filter weights (30 bootstrapped re-samples).</title>
<p>Thick black lines are for the ideal observer, thin black dashed lines indicate confidence intervals (+/- 1.96 SEM). Colored lines indicate normalized profiles for individual human observers. All models used AVG pooling. <bold>(a)</bold> Experiment 1-VAR (top) and Experiment 1-FIX (bottom). <bold>(b)</bold> Experiment 1-HFC (higher density of smaller micropatterns).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.g005" xlink:type="simple"/>
</fig>
<p>To test whether our finding of region-based processing was robust to increasing the carrier spatial frequency, we ran an additional control version of the experiment on 3 naïve observers (Experiment 1-HFC) in which we reduced the micropattern size by a factor of 4, from 32 to 8 pixels (0.5 dva. to 0.125 dva), while increasing the density (to 8192 micropatterns) but holding overall RMS contrast constant (at 14%). Such a stimulus would potentially engage many more contrast-modulation tuned neurons selective for higher carrier spatial frequencies, thus having receptive fields more localized to the boundary [<xref ref-type="bibr" rid="pcbi.1006829.ref038">38</xref>] and therefore potentially revealing edge-based processing. However, even with the high-frequency carrier we still observed region-based processing (<xref ref-type="fig" rid="pcbi.1006829.g005">Fig 5b</xref>), consistent with an ideal observer.</p>
</sec>
<sec id="sec008">
<title>Second-stage filter nonlinearity</title>
<p>In Experiment 1, the best-fit expansive nonlinearity exponent α for the second-stage filters is greater than unity for all observers, for both the <italic>ridge</italic> and <italic>ridge + smooth</italic> priors. For Experiment 1-VAR with AVG sampling, median α values were around 2 (<italic>ridge</italic>: median = 2.15, min = 1.70, max = 2.85; <italic>ridge + smooth</italic>: median = 1.95, min = 1.70, max = 3.35, N = 7), and for Experiment 1-FIX we obtained median values closer to 1.5 (<italic>ridge</italic>: median = 1.50, min = 1.40, max = 1.60; <italic>ridge + smooth</italic>: median = 1.60, min = 1.60, max = 1.90, N = 3). Similar results were obtained for other downsampling methods (<xref ref-type="supplementary-material" rid="pcbi.1006829.s025">S2</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006829.s026">S3</xref> Tables), and the ideal observer was also best fit with an expansive nonlinearity (AVG; Experiment 1-VAR: <italic>ridge</italic>: 4.00; <italic>ridge + smooth</italic>: 3.30; Experiment 1-FIX: <italic>ridge</italic>: 2.20; <italic>ridge + smooth</italic>: 2.30). Plots of the second-stage nonlinearities for observers AMA, JJF are shown in <xref ref-type="supplementary-material" rid="pcbi.1006829.s011">S11 Fig</xref>. This finding is consistent with previous studies suggesting an accelerating nonlinearity following the second-stage filters (see <xref ref-type="sec" rid="sec020">Discussion</xref>).</p>
</sec>
</sec>
<sec id="sec009">
<title>Analysis of model accuracy</title>
<p>To better assess the meaningfulness of the above model-fitting results, it is important to make quantitative measures of the fitted models’ accuracy in predicting observers’ responses to novel stimuli. Such measures can also address the relative merit of different variants of the estimation procedure (<italic>ridge</italic> vs <italic>ridge + smooth</italic> prior) and of the model details, such as downsampling method (AVG, MAX, SUB) and classification rule (DET, STO). In this section we describe some of our analyses of model performance, and refer to other assessments in the Supplementary Material.</p>
<sec id="sec010">
<title>Psychometric function agreement</title>
<p>To the extent that the model architecture is appropriate and the data are sufficient to obtain accurate estimates of parameters, then the fitted model should be able to predict each observer’s responses. Consequently, we constructed psychometric functions of proportion correct on the orientation identification task as a function of the modulation depth across the boundary for individuals observers in Experiment 1-VAR—three representative observers are shown in <xref ref-type="fig" rid="pcbi.1006829.g006">Fig 6</xref> (AVG downsampling). The model predictions (green, red) on the test datasets (which were not used to train models or optimize hyper-parameters) generally lie within or near the 95% binomial proportion confidence intervals (+/- 1.96 SEM) of observer performance (<xref ref-type="fig" rid="pcbi.1006829.g006">Fig 6</xref>, blue dotted lines). However, the deterministic model (DET, left column) tends to over-predict performance at larger contrast modulations for some observers. Implementing the model stochastically (STO, right column) improved the model-observer agreement.</p>
<fig id="pcbi.1006829.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006829.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Psychometric functions for two versions of model (DET, STO), for three representative human observers (AMA, SEL, VHB) in Experiment 1-VAR.</title>
<p>Blue lines denote human observer performance on the test set (stimuli not used for model estimation) as a function of modulation depth, together with 95% binomial proportion confidence intervals (+/- 1.96 SEM, blue dashed lines). Red and green lines denote model predictions for test stimuli for different Bayesian priors (red: <italic>ridge</italic>; green: <italic>ridge + smooth</italic>). Left column shows model operating deterministically (DET), right column shows model operating stochastically (STO).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.g006" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec011">
<title>Overall performance agreement</title>
<p>Another way to assess how well fitted models account for observer responses is to compare their overall performance (i.e. proportion correct averaged over all modulation depths tested) with observer performance. Such an analysis succinctly characterizes model accuracy for each observer and model variant with a single number (as opposed to an entire psychometric function). <xref ref-type="fig" rid="pcbi.1006829.g007">Fig 7</xref> shows observer and model performance in Experiment 1-VAR for each individual test fold on every observer (7 observers, 4 test folds each = 28 total folds). The deterministic model with AVG pooling (AVG-DET, <xref ref-type="fig" rid="pcbi.1006829.g007">Fig 7a</xref>, top) somewhat tended to over-predict observer performance, with the majority (<italic>ridge</italic>: 18/28; <italic>ridge + smooth</italic>: 23/28) of test folds having significantly different (binomial proportion difference test, 95% CI) proportions correct (<xref ref-type="fig" rid="pcbi.1006829.g007">Fig 7b</xref>, top). Despite the systematic over-prediction of observer performance (by about 6–8%), we still observe significant correlations between predictions and actual performance across observers (<italic>ridge</italic>: <italic>r</italic> = 0.983, <italic>p</italic> &lt; 0.001; <italic>ridge + smooth</italic>: <italic>r</italic> = 0.972, <italic>p</italic> &lt; 0.001, N = 7) and folds (<italic>ridge</italic>: <italic>r</italic> = 0.872, <italic>p</italic> &lt; 0.001; <italic>ridge + smooth</italic>: <italic>r</italic> = 0.806, <italic>p</italic> &lt; 0.001, N = 28).</p>
<fig id="pcbi.1006829.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006829.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Analysis of model accuracy for Experiment 1-VAR.</title>
<p>(<bold>a</bold>) Plots of model vs. observer performance (proportion correct), averaged across observers (left sub-panels, circles, N = 7) or test folds (right sub-panels, solid symbols, N = 28). Observer and model performance were compared on a set of novel test stimuli (N = 500) not used for model estimation or hyper-parameter optimization. <italic>Top</italic>: Deterministic model with AVG (average) downsampling (AVG-DET). Correlation coefficients (r values) are color coded for each choice of Bayesian prior (red: <italic>ridge</italic>; green: <italic>ridge + smooth</italic>). <italic>Middle</italic>: Stochastic model with AVG downsampling (AVG-STO). <italic>Bottom</italic>: Deterministic model with downsampling implemented with subsampling (SUB-DET). (<bold>b</bold>) Difference between observer and model performance for each individual test fold (4 folds per observer) for all models shown in (a). Lines show 95% confidence intervals of the difference (binomial proportion difference test). Colors indicate different choices of Bayesian prior (red: <italic>ridge</italic>; green: <italic>ridge + smooth</italic>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.g007" xlink:type="simple"/>
</fig>
<p>Implementing the classification stochastically produced much better agreement (AVG-STO: <xref ref-type="fig" rid="pcbi.1006829.g007">Fig 7a</xref>, middle), with relatively few (<italic>ridge</italic>: 4/28; <italic>ridge + smooth</italic>: 6/28) test folds having significantly different proportions correct (<xref ref-type="fig" rid="pcbi.1006829.g007">Fig 7b</xref>, middle). As before, strong correlations between model and observer performance are seen across observers (<italic>ridge</italic>: <italic>r</italic> = 0.996, <italic>p</italic> &lt; 0.001; <italic>ridge + smooth</italic>: <italic>r</italic> = 0.997, <italic>p</italic> &lt; 0.001, N = 7) and folds (<italic>ridge</italic>: <italic>r</italic> = 0.890, <italic>p</italic> &lt; 0.001; <italic>ridge + smooth</italic>: <italic>r</italic> = 0.911, <italic>p</italic> &lt; 0.001, N = 28). For the deterministic (DET) model, performance was more accurate for subsampling (SUB-DET: <xref ref-type="fig" rid="pcbi.1006829.g007">Fig 7a</xref>, bottom) than averaging downsampling (AVG-DET: <xref ref-type="fig" rid="pcbi.1006829.g007">Fig 7a</xref>, top) or MAX-DET downsampling (<xref ref-type="supplementary-material" rid="pcbi.1006829.s023">S23 Fig</xref>). In this case, many fewer folds (<italic>ridge</italic>: 10/28; <italic>ridge + smooth</italic>: 8/28) had significantly different proportions of correct responses (<xref ref-type="fig" rid="pcbi.1006829.g007">Fig 7b</xref>, bottom). Similar results were obtained for Experiment 1-FIX (<xref ref-type="supplementary-material" rid="pcbi.1006829.s013">S13 Fig</xref>).</p>
<p>The over-prediction of observer performance for the DET model with AVG + MAX pooling is due to the DET mode of operation not accounting for internal noise on a trial-by-trial basis. By contrast, this noise is accounted for when these model is operating in STO mode, since the model is explicitly optimized to predict the proportion of correct responses at each level. The degradation in model performance for SUB-DET pooling (leading to better agreement with the observer) may be due to the fact that this model is getting less information from the first-stage filters than AVG or MAX pooling, which integrate over all first-stage filters instead of a relatively small subset of them. In an additional analysis of Experiment 1-FIX, we systematically varied the downsampling rate for all three pooling rules (<xref ref-type="supplementary-material" rid="pcbi.1006829.s012">S12 Fig</xref>). We find that when operating in DET mode, for low rates (8x8) the SUB model drastically under-predicts observer performance, while for higher rates (16x16, 22x22) the SUB model performance improves and more closely matches observer performance. By contrast, we see in <xref ref-type="supplementary-material" rid="pcbi.1006829.s012">S12 Fig</xref>. that performance of the DET mode AVG and MAX models consistently over-predicts performance for all down-sampling rates, since regardless of down-sampling rates both pooling rules integrate over all of the first-stage filters rather than a limited subset, making them highly robust to random variations in micropattern placement.</p>
</sec>
<sec id="sec012">
<title>Double-pass analysis</title>
<p>To obtain an upper bound on the performance of the model, we carried out a “double-pass” analysis [<xref ref-type="bibr" rid="pcbi.1006829.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref059">59</xref>] by testing observers twice on the same set of stimuli, and comparing model-observer agreement (proportion of trials with consistent responses) on the double-pass set to observer-observer agreement between the two passes. For 5 observers in Experiment 1-VAR, a fold of 1000 stimuli was used as the double-pass set, and the model was trained on the remaining 3000 stimuli. The results (<xref ref-type="fig" rid="pcbi.1006829.g008">Fig 8</xref>) show that the AVG-DET or MAX-DET models agree with the observer on the two passes, indicating that the observer is self-consistent on the two passes. For AVG-DET, both priors give a small, statistically insignificant (rank-sum test) median difference (<italic>ridge</italic>: median difference = 0.022, <italic>p</italic> = 0.063; <italic>ridge + smooth</italic>: median difference = 0.009, <italic>p</italic> = 0.313, N = 5), with most or all of the (binomial proportion difference) confidence intervals bracketing zero (<italic>ridge</italic>: 4/5; <italic>ridge + smooth</italic>: 5/5). Similarly, MAX-DET gives small median differences (<italic>ridge</italic>: median difference = 0.018, <italic>p</italic> = 0.125; <italic>ridge + smooth</italic>: median difference = 0.02, <italic>p</italic> = 0.313, N = 5) with most confidence intervals (<italic>ridge</italic>: 4/5; <italic>ridge + smooth</italic>: 4/5) including zero. However, SUB-DET (<xref ref-type="fig" rid="pcbi.1006829.g008">Fig 8a</xref>, bottom panels) gives somewhat larger differences (<italic>ridge</italic>: median difference = 0.052, <italic>p</italic> = 0.063; <italic>ridge + smooth</italic>: median difference = 0.024, <italic>p</italic> = 0.063, N = 5), although they fail to reach statistical significance. In the SUB-DET case with the <italic>ridge</italic> prior (red), 0/5 confidence intervals include zero, while 3/5 do so for <italic>ridge + smooth</italic> (green). Implementing the decision rule stochastically (AVG-STO) yielded somewhat poorer double-pass agreement (<xref ref-type="fig" rid="pcbi.1006829.g008">Fig 8b</xref>), although the difference fails to reach significance across the set of observers (<italic>ridge</italic>: median difference = 0.075, <italic>p</italic> = 0.063; <italic>ridge + smooth</italic>: median difference = 0.088, <italic>p</italic> = 0.063, N = 5). For AVG-STO, we find that for both priors, 0/5 confidence intervals contain zero.</p>
<fig id="pcbi.1006829.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006829.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Double-pass analysis for 5 observers in Experiment 1-VAR.</title>
<p>(<bold>a</bold>) Models operating in deterministic (DET) mode for three different downsampling rules. <italic>Left panels</italic>: Observer-observer (horizontal axis) and observer-model (vertical axis) proportion agreement on the double-pass set. Error bars denote 95% binomial proportion confidence intervals. <italic>Right panels</italic>: Difference between observer-observer and observer-model proportion agreement for each of 5 observers. Error bars denote 95% binomial proportion difference confidence intervals. (<bold>b</bold>) Same as (a), but for model with AVG downsampling operating in stochastic mode (AVG-STO).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.g008" xlink:type="simple"/>
</fig>
<p>Why does the stochastic (STO) decision rule (with AVG/MAX pooling) lead to better prediction of observer performance (<xref ref-type="fig" rid="pcbi.1006829.g007">Fig 7</xref>), and yet worse double-pass agreement (<xref ref-type="fig" rid="pcbi.1006829.g008">Fig 8</xref>) than the deterministic (DET) rule? To understand why, suppose for a given right-oblique stimulus the observer gave the response (R) on the first trial, and due to internal noise is 80% likely to respond R on the second trial, yielding a self-consistent response (R, R) on both passes. Suppose that for this stimulus the output of the model is P(R) &gt;0.5. Then the DET model will always return R, and therefore be in agreement with the observer’s choice on the second pass 80% of the time. In contrast, the STO model will return R on only a subset of the trials, which will be uncorrelated with the observer’s responses. Therefore, the STO model will have a lower rate of double-pass agreement, even if it better matches the overall proportion correct.</p>
</sec>
<sec id="sec013">
<title>Decision variable correlation analysis</title>
<p>Although a strong model-observer agreement for the percentage correct is consistent with accurate modeling of the observer’s visual processing, it is not fully compelling. The model and observer might both choose category A or B the same proportion of trials, without necessarily exhibiting any correlation between observer and model responses on a trial-by-trial basis. This situation could arise, for example, if the observer and model were utilizing different stimulus features for their decisions. To more thoroughly quantify model performance, we implemented a <italic>decision variable correlation</italic> (DVC) analysis, which estimates the trial-by-trial correlation between a model’s decision variable and the observer’s hypothesized decision variable [<xref ref-type="bibr" rid="pcbi.1006829.ref060">60</xref>]. This method extends standard signal detection theory by hypothesizing that the joint distribution of model and observer decision variables for each stimulus category (A = left-oblique/L, B = right-oblique/R) can be modeled as bivariate Gaussian distributions having correlation parameters ρ<sub>A</sub>, ρ<sub>B</sub>. Using the proportions of trials on which the observer and model decisions agree for each stimulus category, ρ<sub>A</sub> and ρ<sub>B</sub> are estimated using a simple maximum likelihood procedure [<xref ref-type="bibr" rid="pcbi.1006829.ref060">60</xref>].</p>
<p>DVC analysis is most accurate (as measured by bootstrapped estimates of SEM) for large numbers of trials at a constant stimulus level sufficiently near threshold to give consistent proportions of the four logical possibilities (model says A/B, observer says a/b) [<xref ref-type="bibr" rid="pcbi.1006829.ref060">60</xref>]. Therefore we applied DVC analysis to Experiment 1-FIX, which entailed thousands of trials (6000) at a single stimulus level for each of three observers (CJD, MAK, JJF). The model was trained on 5000 trials and the DVC computed using the remaining 1000 trials, for 6 choices of hold-out fold. Since the two boundary orientations provide independent estimates of the DVC, this provided 12 estimates of DVC from each observer. <xref ref-type="fig" rid="pcbi.1006829.g009">Fig 9</xref> shows the resulting DVC values (plotted in rank order within each set, for graphical clarity) measured for AVG downsampling and both choices of prior. These estimates of DVC are overwhelmingly positive and significantly larger than zero.</p>
<fig id="pcbi.1006829.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006829.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Decision variable correlation analysis.</title>
<p>Decision variable correlation (DVC) computed on 6 test folds (two measurements per fold) for three observers (CJD = blue, MAK = magenta, JJF = black) in Experiment 1-FIX for both priors. DVC values for each observer are sorted by magnitude for visual clarity. Thin lines denote +/-1 SEM (200 bootstrap re-samples).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.g009" xlink:type="simple"/>
</fig>
<p>For AVG downsampling, 32/36 (<italic>ridge</italic>) and 33/36 (<italic>ridge + smooth</italic>) estimates of DVC are significantly (&gt; 1.96 SEM) larger than zero, with DVC mean +/- SD of 0.234 +/- 0.094 (<italic>ridge</italic>) and 0.254 +/- 0.095 for (<italic>ridge + smooth</italic>). MAX downsampling gives similar results, where 33/36 (<italic>ridge</italic>) and 35/36 (<italic>ridge + smooth</italic>) estimates of DVC are significantly greater than zero, with DVC mean +/- SD of 0.245 +/- 0.094 (<italic>ridge</italic>) and 0.263 +/- 0.086 (<italic>ridge + smooth</italic>). Interestingly, SUB downsampling gives weaker correlations with only 20/36 (<italic>ridge</italic>) and 24/36 (<italic>ridge + smooth</italic>) estimates of DVC significantly greater than zero, and DVCs have mean +/- SD of 0.147 +/- 0.082 (<italic>ridge</italic>) and 0.164 +/- 0.085 (<italic>ridge + smooth</italic>).</p>
<p>Therefore, we see that not only are the models and observers in agreement about overall proportions of each category, but they are also correlated in their trial-by-trial decisions, consistent with the interpretation that the models and human observers are performing similar kinds of processing.</p>
</sec>
<sec id="sec014">
<title>Ideal-observer analysis</title>
<p>To compare the spatial summation and task performance of human observers to those of an ideal observer, we trained the model on the true stimulus category labels rather than observer classifications of these stimuli. Notably, the resulting ideal observer models make use of a region-based spatial summation similar to that of human observers (<xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3c</xref>). Examining this more systematically, a comparison of human performance to that of the deterministic (16x16 AVG-DET) ideal observer in Experiment 1-VAR (<xref ref-type="supplementary-material" rid="pcbi.1006829.s014">S14 Fig</xref>) revealed that across 6 observers analyzed, most confidence intervals (binomial proportion difference test, 95% confidence interval) did not contain zero (<italic>ridge</italic>: 6/24; <italic>ridge + smooth</italic>: 3/24), with those containing zero coming mainly from two observers (AMA, SRS). In contrast, for the stochastic ideal observer (AVG-STO) many more confidence intervals contained zero (<italic>ridge</italic>: 11/24; <italic>ridge + smooth</italic>: 10/24), suggesting a better agreement with observer performance. Similar results were obtained for the 3 observers tested in Experiment 1-FIX (<xref ref-type="supplementary-material" rid="pcbi.1006829.s014">S14 Fig</xref>). Comparing human to deterministic ideal observer (AVG-DET) performance, most confidence intervals did not contain zero (<italic>ridge</italic>: 5/18; <italic>ridge + smooth</italic>: 3/18), while for the stochastic ideal observer most did contain zero (<italic>ridge</italic>: 12/18; <italic>ridge + smooth</italic>: 15/18). On the whole, this result is similar to our findings for the models fitted to observer data—i.e. a general tendency for the AVG or MAX models operating deterministically to over-predict performance, while these models operating stochastically seemed to agree reasonably well.</p>
</sec>
<sec id="sec015">
<title>Correlations between observer performance and model parameters</title>
<p>The different human observers exhibited substantial individual differences in overall performance as measured by proportion correct (<xref ref-type="fig" rid="pcbi.1006829.g007">Fig 7</xref>), raising the question whether there might be corresponding variations in the fitted model parameters. We assessed this possibility for the results in Experiment 1-VAR, using the 16x16 AVG model.</p>
<p>Although the nonlinearity was always expansive (exponent α &gt; 1.0), there was no compelling evidence across the set of individuals that the magnitude of α explained individual variations in performance, based on Pearson’s <italic>r</italic> (<italic>ridge</italic>: <italic>r</italic> = -0.458, <italic>p</italic> = 0.302; <italic>ridge + smooth</italic>: <italic>r</italic> = -0.787, <italic>p</italic> = 0.036, N = 7). The significant result for the <italic>ridge + smooth</italic> case was due to the presence of an outlier—computing these correlations using Spearman’s rho revealed no significant effect (<italic>ridge</italic>: <italic>rho</italic> = -0.357, <italic>p</italic> = 0.444; <italic>ridge + smooth</italic>: <italic>rho</italic> = -0.214, <italic>p</italic> = 0.662). Results were similar for MAX and SUB models (<xref ref-type="supplementary-material" rid="pcbi.1006829.s028">S5 Table</xref>).</p>
<p>However, individual variability in observer performance was very strongly related to the magnitudes of the fitted second-stage filter weights. That is, correlating the sum of the norms of the filter weights with overall performance across the 7 observers yielded strong, significant positive correlations (<italic>ridge</italic>: <italic>r</italic> = 0.970, <italic>p</italic> &lt; 0.001; <italic>ridge + smooth</italic>: <italic>r</italic> = 0.993, <italic>p</italic> &lt; 0.001), with similar results for MAX, SUB models (<xref ref-type="supplementary-material" rid="pcbi.1006829.s029">S6 Table</xref>). Consistent with this, the ideal observer had a larger filter norm (<italic>ridge</italic>: 5.67; <italic>ridge + smooth</italic>: 5.37) than the human observers (<italic>ridge</italic>: median = 4.30, min = 2.36, max = 4.59; <italic>ridge + smooth</italic>: median = 4.04, min = 2.27, max = 4.34, N = 7), as revealed by a sign-rank test (<italic>ridge</italic>: <italic>p</italic> = 0.018; <italic>ridge + smooth</italic>: <italic>p</italic> = 0.018).</p>
<p>In our model, the inverse internal noise level is represented implicitly by the magnitudes (vector norms) of the second-stage filters (see <xref ref-type="sec" rid="sec028">Methods</xref> for details). Therefore, we should expect observers with larger second-stage filter norms to also have better double-pass agreement, which is a function of internal noise. We find that there are indeed strong, significant positive correlations between filter norm and double-pass agreement for all pooling rules and choices of prior (AVG—<italic>ridge</italic>: <italic>r</italic> = 0.954, <italic>p</italic> = 0.012, <italic>ridge + smooth</italic>: <italic>r</italic> = 0.942, <italic>p</italic> = 0.017; MAX—<italic>ridge</italic>: <italic>r</italic> = 0.964, <italic>p</italic> = 0.008, <italic>ridge + smooth</italic>: <italic>r</italic> = 0.956, <italic>p</italic> = 0.011; SUB—<italic>ridge</italic>: <italic>r</italic> = 0.940, <italic>p</italic> = 0.017, <italic>ridge + smooth</italic>: <italic>r</italic> = 0.943, <italic>p</italic> = 0.016). Taken together with significant positive correlations between observer performance and double-pass agreement (<italic>r</italic> = 0.949, <italic>p</italic> = 0.014), we conclude that variations in filter norm magnitudes across observers are related to variations in internal noise levels across observers, and this variability in internal noise levels contributes to difference in observer performance.</p>
</sec>
</sec>
<sec id="sec016">
<title>Model consistency</title>
<p>The perceptual filters obtained in Experiment 1 (<xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3</xref>) seem consistent with the idea that observers are monitoring the entire stimulus region for each of the two possible boundary orientations. However, to rule out the possibility that such filters could also be observed if observers were utilizing some different kind of spatial processing, we implemented four alternative varieties of spatial summation as ideal observer models fit to category labels. We then fit the model architecture in <xref ref-type="fig" rid="pcbi.1006829.g002">Fig 2</xref> to data from these ground truth ideal observers in order to assess whether our methodology is <italic>consistent</italic> in the sense that it recovers the actual set of filters generating the data.</p>
<p>One possible alternative spatial summation that could mediate our task would be to simply monitor two adjacent “pizza slice” shaped quadrants defined by the four compass directions (2-slice). For instance, if one is monitoring the North and East slices, a difference in contrast between these slices is diagnostic for a right-oblique boundary. Implementing an ideal observer that utilizes such a subset of the spatial information (<xref ref-type="fig" rid="pcbi.1006829.g010">Fig 10a</xref>, top) and fitting our model architecture shown in <xref ref-type="fig" rid="pcbi.1006829.g002">Fig 2</xref> to this data reveals consistent results: A single filter which monitors two slices only. Implementing an observer who monitors three adjacent slices ("3-slice"), for instance N+E and N+W (<xref ref-type="fig" rid="pcbi.1006829.g010">Fig 10a</xref>, second row) also reveals consistent filters. Another possible spatial processing approach would be to observe all slices but to only monitor for one of the two potential boundary orientations ("1-filter")—simulating this model yields a consistent estimate of a single filter tuned to the monitored boundary (<xref ref-type="fig" rid="pcbi.1006829.g010">Fig 10a</xref>, third row).</p>
<fig id="pcbi.1006829.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006829.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Simulated ground-truth observers (left columns) and filter shapes recovered (right columns) from simulated datasets generated by these observers.</title>
<p><bold>(a)</bold> Ideal observers implementing various sub-optimal spatial filtering models for Experiment 1. <italic>Top row</italic>: Simulated observer monitors two adjacent “pizza slice” shaped regions (2-slice). <italic>Second row</italic>: Simulated observer monitors three adjacent regions (3-slice). <italic>Third row</italic>: Simulated observer only monitors one boundary (1-filter). <italic>Bottom row</italic>: Simulated observer randomly monitors 1 of 4 pairs of informative adjacent pizza slices (2-slice—random). <bold>(b)</bold> Ideal observer implementing a spatial filtering model comprised of two perceptual filters, each monitoring one potential boundary (2-filter). The recovered filters (right) are most similar to those observed in our results (<xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3</xref>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.g010" xlink:type="simple"/>
</fig>
<p>One can in principle observe filters somewhat similar to those we recover in <xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3</xref> by using a sub-optimal spatial processing model which only monitors a single pair of potentially informative slices, chosen at random on each trial. However, such a model leads to perceptual filters which are extremely “noisy” compared to those we find (<xref ref-type="fig" rid="pcbi.1006829.g010">Fig 10a</xref>, bottom row), so it is quite unlikely that our observers are actually using such spatial processing. The simulated ideal observer that produces filters providing the best match to those we obtain from human observers (<xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3</xref>) makes use of two filters, each of which monitors the entire stimulus region for each of the two stimulus possibilities (<xref ref-type="fig" rid="pcbi.1006829.g010">Fig 10b</xref>).</p>
<sec id="sec017">
<title>Summary and interpretation</title>
<p>From our analysis of model accuracy, we may draw several broad conclusions. Firstly, accuracy is not greatly affected by choice of prior, with similar results obtained using the <italic>ridge</italic> and <italic>ridge + smooth</italic> priors. Also, from our analyses we cannot favor a particular choice of pooling rule among those tested (AVG, MAX, SUB). Almost identical overall results are obtained using AVG and MAX pooling, and although in some cases we see lower accuracy using SUB pooling, our results suggest this can be ameliorated by simply increasing the down-sampling rate (<xref ref-type="supplementary-material" rid="pcbi.1006829.s012">S12 Fig</xref>). In Experiment 1-VAR, where the stimulus levels were fixed for all observers (rather than set at their individual thresholds), we observed great variability between observers in both task performance and double-pass agreement. This variability is reflective of differences in internal noise across observers, and is captured by our model. Finally, simulations using ideal observers reveal that our modeling process is consistent, accurately recovering the perceptual filter shapes which generate the data (<xref ref-type="fig" rid="pcbi.1006829.g010">Fig 10</xref>). The strongest agreement with our human results (<xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3</xref>) is obtained by an observer monitoring the whole stimulus region for each of the two possible stimulus alternatives.</p>
</sec>
</sec>
<sec id="sec018">
<title>Experiment 2: Boundary orientation discrimination</title>
<p>The first experiment demonstrated that human observers employ region-based processing for the orientation identification task. To see if our modeling method could also reveal situations where observers employ edge-based processing, we utilized a fine orientation discrimination task (<xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1c</xref>, right). The orientation of a second-order boundary was perturbed slightly (JND at 32% contrast modulation—see <xref ref-type="sec" rid="sec028">Methods</xref>) from vertical, and observers judged the direction of perturbation (i.e. slightly left- vs right-oblique). In one variant of this task (Experiment 2-VAR), contrast modulation was varied in 9 logarithmic steps from 16% to 64% centered at 32% (plus zero modulation) to vary the task difficulty. In a second variant (Experiment 2-FIX), contrast modulation was fixed at 32%. In both variants of Experiment 2, the perturbation of orientation was fixed near its JND for each observer at 32% contrast modulation (CJD: 7 deg., JJF: 6 deg., VHB: 7 deg.). In the data analysis, a higher downsampling factor (22 x 22) was used to better reveal fine spatial details, and only AVG pooling was considered, since Experiment 1 demonstrated substantial robustness of spatial summation to pooling rules (<xref ref-type="supplementary-material" rid="pcbi.1006829.s008">S8</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006829.s009">S9</xref> Figs).</p>
<p><xref ref-type="fig" rid="pcbi.1006829.g011">Fig 11</xref> shows the second-stage filter weights from an observer (JJF) tested in both Experiment 2-VAR (<xref ref-type="fig" rid="pcbi.1006829.g011">Fig 11a</xref>) and Experiment 2-FIX (<xref ref-type="fig" rid="pcbi.1006829.g011">Fig 11b</xref>). Unlike in Experiment 1, here observers employ an “edge-based” summation, assigning substantially more weight to texture regions near the boundary. Bootstrapped significance plots for individual weights are shown in <xref ref-type="supplementary-material" rid="pcbi.1006829.s015">S15 Fig</xref>, and consistent results from other observers are shown in <xref ref-type="supplementary-material" rid="pcbi.1006829.s016">S16 Fig</xref>. The edge-based processing employed by these observers is very similar to that obtained from an ideal observer (<xref ref-type="supplementary-material" rid="pcbi.1006829.s017">S17 Fig</xref>). As with Experiment 1, we find a reasonably strong agreement between model predictions and observed performance on sets of novel data not used for model parameter estimation (<xref ref-type="supplementary-material" rid="pcbi.1006829.s018">S18 Fig</xref>), and find an expansive second-stage filter nonlinearity for Experiment 2 (<xref ref-type="supplementary-material" rid="pcbi.1006829.s030">S7 Table</xref>). Taken together, Experiments 1 and 2 show that our modeling methodology can reveal different kinds of spatial processing that observers may employ for different kinds of psychophysical tasks.</p>
<fig id="pcbi.1006829.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006829.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Second-stage filter weights for one representative observer, JJF, from Experiment 2 (orientation discrimination, <xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1c</xref>, right).</title>
<p><bold>(a)</bold> Experiment 2-VAR. <italic>Top</italic>: Estimated 2-D weight maps averaged over 4 training folds. <italic>Bottom</italic>: 1-D profiles showing mean weight magnitude (thick lines), averaged over 30 bootstrapped re-samplings. Thin dashed lines show +/- 1 SEM. <bold>(b)</bold> Same as (a), but for Experiment 2-FIX.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.g011" xlink:type="simple"/>
</fig>
<p>We also considered the possibility that our results in Experiment 2 (<xref ref-type="fig" rid="pcbi.1006829.g011">Fig 11</xref>) could also be explained by two off-orientation filters defined on the spatial scale of the whole stimulus [<xref ref-type="bibr" rid="pcbi.1006829.ref061">61</xref>]. We defined an ideal observer with two off-orientation filters (+/- 20 deg.) defined on the scale of the stimulus and simulated responses of this observer to the stimuli used in Experiment 2. This analysis yielded very different filters from those we observe in <xref ref-type="fig" rid="pcbi.1006829.g011">Fig 11</xref>, as we simply recovered the original off-orientation filters (<xref ref-type="supplementary-material" rid="pcbi.1006829.s022">S22 Fig</xref>), again demonstrating the consistency of our method (as in <xref ref-type="fig" rid="pcbi.1006829.g010">Fig 10</xref>). Based on these results, we suggest that it is more likely that the relevant neural mechanisms employed in Experiment 2 are contrast-modulation tuned neurons with smaller receptive fields localized to the boundary (see <xref ref-type="sec" rid="sec020">Discussion</xref>).</p>
</sec>
<sec id="sec019">
<title>Experiment 3: Integrating contrast modulation across orientation channels</title>
<p>In Experiments 1 and 2, we considered a particular FRF model architecture and focused on characterizing the spatial processing that observers employ in different second-order vision tasks. However, one of the most powerful applications of this methodology is model comparison—i.e. comparing the ability of alternative models, embodying different assumptions about an FRF model architecture, to account for psychophysical data. Indeed, a major focus of much of the second-order vision literature has been to design experiments whose express goal is to test competing hypotheses of FRF architecture [<xref ref-type="bibr" rid="pcbi.1006829.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref039">39</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref062">62</xref>].</p>
<p>We applied our method to ask how second-stage filters for contrast modulation integrate orientation modulations across multiple kinds of first-stage filters or channels. In many previous computational studies of second-order processing, the FRF models were comprised of second-stage filters, each of which only analyzes a single orientation/spatial frequency channel [<xref ref-type="bibr" rid="pcbi.1006829.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref063">63</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref064">64</xref>], with subsequent “late summation” of their signals. However, other psychophysical studies have suggested “early summation” FRF models, in which second-stage filters integrate contrast modulation across texture channels [<xref ref-type="bibr" rid="pcbi.1006829.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref062">62</xref>].</p>
<p>To address this question systematically, we used a boundary orientation identification task as in Experiment 1, but with the carrier pattern containing two orthogonal orientations of micropatterns (<xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1a</xref>, bottom), which would be processed by distinct first-stage orientation-selective channels. Since the nature of spatial summation is not critical to this question, we used a coarser downsampling factor (12 x 12), and as above, only AVG pooling was used. We considered two models of how information is integrated by the second-stage filters. In a “late summation” Model 1 (<xref ref-type="fig" rid="pcbi.1006829.g012">Fig 12a</xref>, top), each first-stage channel is analyzed by its own pair of second-stage filters (L/R oblique). Alternatively, in an “early summation” Model 2 (<xref ref-type="fig" rid="pcbi.1006829.g012">Fig 12a</xref>, bottom), each second-stage filter receives inputs from both first-order channels. As in Experiments 1 and 2, we find that both the models are in reasonably good agreement with observer performance on the task (<xref ref-type="supplementary-material" rid="pcbi.1006829.s020">S20 Fig</xref>). Also there was reassuring consistency in that both the fitted models exhibited similar region-based weight maps, with an expansive second-order nonlinearity (<xref ref-type="supplementary-material" rid="pcbi.1006829.s031">S8 Table</xref>).</p>
<fig id="pcbi.1006829.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006829.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Model comparison for Experiment 3, boundary orientation identification with textures composed of two kinds of micropatterns.</title>
<p><bold>(a)</bold> Two competing models of FRF architecture fit to psychophysical trial data. <italic>Top</italic>: Model 1 (“late summation”) assumes that each first-stage orientation channel is analyzed by its own pair of (L/R) second-stage filters and then pooled. <italic>Bottom</italic>: Model 2 (“early summation”) assumes that each (L/R) second-stage filter integrates over both first-stage channels. (<bold>b</bold>) Bayesian model comparison making use of all data reveals a strong consistent preference (as measured by the Bayes factor—see text) for Model 2 (“early summation”) for all three observers (blue line and symbols). Thick black line indicates the conventional criterion for a “very strong” preference for Model 2. (<bold>c</bold>) Bootstrapping analysis, in which model likelihood is evaluated on novel data not used for model training, also reveals a preference for Model 2. Plotted points indicate mean +/- SEM for 50 bootstrapped samples. (<bold>d</bold>) Difference between predicted and observed proportions where observer chooses “right oblique” for all observers and both models. Negative modulation depths indicate left-oblique stimuli.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.g012" xlink:type="simple"/>
</fig>
<p>When comparing Models 1 and 2 using standard model comparison techniques such as the Bayes Information Criterion, or BIC [<xref ref-type="bibr" rid="pcbi.1006829.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref044">44</xref>], there is no need to correct for overfitting since both models have the same number of parameters. This is because when parameter space dimensionality is equal, the complexity-penalizing terms of the BIC cancel, so that Δ<sub><italic>BIC</italic></sub> = ln <italic>p</italic><sub>2</sub>(<italic>D</italic>) − ln <italic>p</italic><sub>1</sub>(<italic>D</italic>), where <italic>p</italic><sub><italic>i</italic></sub>(<italic>D</italic>) denotes the likelihood of the data (<italic>D</italic>) under model <italic>i</italic> evaluated at the MAP estimate of the model parameters. That is, when two models are of equal complexity and equally likely <italic>a priori</italic>, we simply choose the one with higher log-likelihood of the data. The difference in log-likelihoods (Δ<sub><italic>BIC</italic></sub>) is identical to the natural log of the Bayes factor <inline-formula id="pcbi.1006829.e001"><alternatives><graphic id="pcbi.1006829.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>. Standard effect size conventions state that when 2 ln <italic>B</italic><sub>21</sub> &gt; 10 (corresponding to &gt; 150-fold higher probability of Model 2), the preference is considered to be “very strong” [<xref ref-type="bibr" rid="pcbi.1006829.ref043">43</xref>]. We see in <xref ref-type="fig" rid="pcbi.1006829.g012">Fig 12b</xref> that for all three observers tested in Experiment 3, the evidence (measured by 2 ln <italic>B</italic><sub>21</sub>) in favor of the orientation opponent model (Model 2) exceeds the standard criterion (10) for a very strong preference. Ideal observer analysis also revealed a very strong preference for the orientation-opponent model fit to stimulus category labels (2 ln <italic>B</italic><sub>21</sub> = 478.4). With a large number of trials, a strong preference for one binomial response model does not require a huge difference in predicting observer performance (<xref ref-type="sec" rid="sec041">Appendix A</xref>). We see in <xref ref-type="fig" rid="pcbi.1006829.g012">Fig 12d</xref> that although Model 2 very consistently does a better job of matching observer performance than Model 1, that the differences between predicted proportions correct are fairly small.</p>
<p>We also wanted to compare the predictive abilities of the two models using a cross-validation approach, by evaluating their agreement with observer performance on data not used for training. Therefore we performed a bootstrap analysis in which both models were fit on randomly selected samples of 4000 stimuli and tested on the remaining 1000 (50 bootstraps). To optimize the generalization performance, each model was trained using the value of the ridge regression hyperparameter λ which yielded the best generalization (<xref ref-type="supplementary-material" rid="pcbi.1006829.s019">S19</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006829.s021">S21</xref> Figs). The results in <xref ref-type="fig" rid="pcbi.1006829.g012">Fig 12c</xref> show a consistent preference across observers for Model 2 (magenta line). Performing a 3 x 2 factorial ANOVA (3 observers, 2 models, 50 observations each) on the test set log-likelihoods revealed significant main effects of both observer (<italic>F</italic><sub>2,294</sub> = 201.085, <italic>p</italic> &lt; 0.001) and model (<italic>F</italic><sub>1,294</sub> = 39.811, <italic>p</italic> &lt; 0.001), but no significant interaction (<italic>F</italic><sub>2,294</sub> = 3.001, <italic>p</italic> = 0.051). In summary, both analysis approaches (<xref ref-type="fig" rid="pcbi.1006829.g012">Fig 12b and 12c</xref>) show a strong and consistent preference for the early summation version (Model 2).</p>
</sec>
</sec>
<sec id="sec020" sec-type="conclusions">
<title>Discussion</title>
<p>Second-order stimuli present a challenge for computational models of visual processing, as they do not contain spatial variations in stimulus intensity and thus cannot be detected by simple linear filtering operations [<xref ref-type="bibr" rid="pcbi.1006829.ref019">19</xref>]. Consequently, standard “classification image” techniques [<xref ref-type="bibr" rid="pcbi.1006829.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref005">5</xref>], which assume a linear spatial summation model, are not readily applicable to studies of second-order visual processing. Here we define a simple hierarchical neural network implementing a Filter-Rectify-Filter (FRF) model of second-order visual processing [<xref ref-type="bibr" rid="pcbi.1006829.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref065">65</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref066">66</xref>]. We estimate its parameters directly from psychophysical trial data, and show that it can accurately predict observer performance on novel test data not used for training. We demonstrate the utility of this modeling approach by testing whether observers segment contrast-defined boundaries using an edge- or region-based processing, finding support for region-based segmentation in a boundary orientation-identification task (Experiment 1), and edge-based processing for a boundary discrimination task (Experiment 2). In addition, we show how this methodology can adjudicate between alternative FRF model architectures (Experiment 3).</p>
<sec id="sec021">
<title>Model complexity vs. biological realism</title>
<p>Psychophysical system identification is critically limited by the modest amount of data that can be collected in an experiment. Although constraints or priors can help alleviate the demand for data [<xref ref-type="bibr" rid="pcbi.1006829.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref067">67</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref068">68</xref>], it is also often necessary to fit simplified models requiring fewer parameters, which omit many known biological details. Here we discuss several modeling simplifications which may need to be addressed when extending this methodology to more complicated second-order stimuli and tasks.</p>
<sec id="sec022">
<title>First-stage filters</title>
<p>In FRF models like those considered here, the initial stage of filtering and rectification is meant to model computations most commonly believed to be taking place in visual area V1 [<xref ref-type="bibr" rid="pcbi.1006829.ref069">69</xref>]. We modeled the first-stage filters using a standard energy model of V1 complex cells with normalization [<xref ref-type="bibr" rid="pcbi.1006829.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref070">70</xref>]. Although for our stimuli a front-end comprised entirely of oriented energy filters was perfectly adequate, for many texture segmentation tasks it may be necessary to model half-wave rectified filters (both even and odd phase) as well. For instance, human observers can segment second-order images using cues of carrier contrast phase and polarity [<xref ref-type="bibr" rid="pcbi.1006829.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref071">71</xref>], which is not possible using only energy filters, whose responses are invariant to phase and polarity. Likewise, it may also be desirable to include center-surround “difference-of-Gaussian” (DOG) filters as well, since early visual cortex has many neurons with non-oriented receptive fields [<xref ref-type="bibr" rid="pcbi.1006829.ref072">72</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref074">74</xref>]. Human psychophysics has indicated that non-oriented filters may potentially serve as a mechanism for polarity-based segmentation [<xref ref-type="bibr" rid="pcbi.1006829.ref041">41</xref>], and computer vision work on natural image segmentation found it advantageous to include non-oriented as well as oriented filters [<xref ref-type="bibr" rid="pcbi.1006829.ref017">17</xref>].</p>
<p>In Experiments 1 and 2, we implemented only a single spatial frequency and orientation channel matched to the carrier, and in Experiment 3 we only considered two such channels. This simplification neglects the possible contributions of off-orientation and off-spatial frequency channels to task performance. Indeed, previous psychophysical studies have challenged the notion that second-order mechanisms are narrowly tuned for a single first-order channel, with many studies suggesting broad bandwidth for carrier spatial frequency [<xref ref-type="bibr" rid="pcbi.1006829.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref075">75</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref076">76</xref>]. On the other hand, single unit neurophysiology has demonstrated neurons that are selective for the carrier spatial frequency of CM stimuli, both in macaque V2 [<xref ref-type="bibr" rid="pcbi.1006829.ref038">38</xref>] and in cat Area 18 [<xref ref-type="bibr" rid="pcbi.1006829.ref077">77</xref>].</p>
</sec>
<sec id="sec023">
<title>Second-stage filters</title>
<p>The main focus of our study was demonstrating an approach to estimate the properties (shapes and nonlinearities) of a set of second-stage filters which detect higher-order visual features. For our task involving contrast-modulated boundaries (<xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1c</xref>), the simplest possible model involves two second-stage filters representing the two potential stimulus categories, much like the model shown in <xref ref-type="fig" rid="pcbi.1006829.g002">Fig 2</xref>. However, for psychophysical tasks involving more complex second-order stimuli such as natural occlusion boundaries (<xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1a</xref>, top) defined by multiple cues which differ on opposite sides of a boundary, it may be necessary to have both a wider variety of first-stage filters as well as multiple kinds of second-stage filters sensitive to different feature combinations (<xref ref-type="fig" rid="pcbi.1006829.g013">Fig 13</xref>). In addition, it may be pertinent to allow direct connections between the first stage of filtering (albeit for much lower spatial frequencies) and the output (decision) stage, since natural region boundaries contain both first- and second-order cues [<xref ref-type="bibr" rid="pcbi.1006829.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref016">16</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref018">18</xref>] which can be integrated in psychophysical tasks [<xref ref-type="bibr" rid="pcbi.1006829.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref078">78</xref>].</p>
<fig id="pcbi.1006829.g013" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006829.g013</object-id>
<label>Fig 13</label>
<caption>
<title>Hypothetical extension of our modeling framework with additional second-stage filters and a more complete implementation of the first-stage filters.</title>
<p>Such a model could potentially be fit to human observer performance on boundary perception tasks involving complex stimuli such as natural occlusion boundaries.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.g013" xlink:type="simple"/>
</fig>
<p>An additional limitation of our model (and many other FRF variants) is the assumption of only a single stage of higher-order filtering beyond the first stage. Recent neurophysiological and fMRI studies have suggested that texture selectivity may develop progressively at successive stages of the visual hierarchy from V1 to V2 to V4 [<xref ref-type="bibr" rid="pcbi.1006829.ref079">79</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref084">84</xref>]. In studies using convolutional neural networks to solve image recognition tasks, texture-sensitive units often develop gradually over several processing layers [<xref ref-type="bibr" rid="pcbi.1006829.ref085">85</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref088">88</xref>]. Furthermore, computer vision work on boundary detection has also found it useful to employ additional “texton” layers of processing [<xref ref-type="bibr" rid="pcbi.1006829.ref017">17</xref>]. Therefore, it may be of interest for future work to consider models having additional intermediate layers of processing.</p>
<p>A number of previous studies have considered how information from an array of first-order visual filters may be pooled for perceptual decisions [<xref ref-type="bibr" rid="pcbi.1006829.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref089">89</xref>]. In our analysis, we considered three different pooling rules inspired by previous work in computational vision science. We found that the spatial processing for the second-order vision tasks we examined was highly robust to choice of pooling rule (<xref ref-type="supplementary-material" rid="pcbi.1006829.s008">S8</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006829.s009">S9</xref> Figs), and that we could model our data accurately with different choices of pooling rule (<xref ref-type="fig" rid="pcbi.1006829.g007">Fig 7</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006829.s012">S12</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006829.s013">S13</xref> Figs). Therefore, we remain agnostic about the “best” model of pooling between the first and second stage filters, and suggest that this may be an interesting topic for future investigation.</p>
<p>Nevertheless, despite these limitations and our numerous modeling simplifications, in practice we found that for the tasks and the stimuli used here our simple FRF model (<xref ref-type="fig" rid="pcbi.1006829.g002">Fig 2</xref>) was able to capture human performance quite well (Figs <xref ref-type="fig" rid="pcbi.1006829.g006">6</xref>–<xref ref-type="fig" rid="pcbi.1006829.g009">9</xref>).</p>
</sec>
</sec>
<sec id="sec024">
<title>Comparison with previous psychophysical modeling</title>
<p>Despite the simplifications made in the present study, it represents a substantial methodological advance over previous efforts to model second-order visual perception. Although previous studies have developed image-computable FRF models, they have generally fixed not only the model architecture but also the shapes of the second-stage filters. The present study demonstrates how the second-stage filter shapes can be “learned” directly from experimental data in order to reveal the nature of spatial integration used in different tasks (Experiments 1 and 2).</p>
<p>Most previous FRF models in human psychophysics studies were conceptual or qualitative, without quantitative fitting of model parameters to data. Two recent studies [<xref ref-type="bibr" rid="pcbi.1006829.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref090">90</xref>] estimated free parameters of an FRF model by fitting threshold data as functions of stimulus parameters. Since thresholds are typically determined at only a few levels of a one-dimensional independent variable, this yields relatively few data points, greatly limiting the number of model parameters one can estimate. In our work, we directly estimate hundreds of model parameters by fitting stimulus-response data from thousands of psychophysical trials, enabling us to characterize the shapes of the second-stage filters.</p>
<p>In addition to extending the FRF modeling literature, our work also complements and extends previous classification image studies, which have generally been applied in the context of first-order vision [<xref ref-type="bibr" rid="pcbi.1006829.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref005">5</xref>]. Although classification images yield image-computable predictive models of observer responses, relatively few studies validate these models by testing their predictive performance on a set of stimuli not used for model estimation. We tested model performance on novel stimuli not used for model estimation using both standard measures, e.g. percentage correct, as well as more advanced methods such as double-pass analysis [<xref ref-type="bibr" rid="pcbi.1006829.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref059">59</xref>] and decision variable correlation [<xref ref-type="bibr" rid="pcbi.1006829.ref060">60</xref>].</p>
<p>Finally, in contrast to most classification image studies, here we systematically address the issue of model comparison. Psychophysical system identification can be formulated in the framework of estimating a generalized linear model [<xref ref-type="bibr" rid="pcbi.1006829.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref067">67</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref068">68</xref>], which is unique up to choice of link function. Therefore, issues of model comparison (beyond coefficient shrinkage) do not naturally arise in this context. However, the FRF models relevant to second-order vision can vary greatly in architectural details (e.g., number of filtering layers, nonlinearities, connectivity, etc.), and model comparison is of great interest. In Experiment 3, we compare two competing FRF architectures making different assumptions about how the second-stage filters integrate across orientation channels, finding a preference for a model with early summation of first-order channels by the second-stage filters, consistent with previous psychophysical work [<xref ref-type="bibr" rid="pcbi.1006829.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref076">76</xref>].</p>
</sec>
<sec id="sec025">
<title>Edge- versus region-based segmentation</title>
<p>We applied our methodology to the question of whether or not observers take a region-based or an edge-based approach to segmenting a contrast boundary (Experiment 1). To the best of our knowledge, our experiment is the first time the issue of edge/region-based processing has been addressed for boundaries defined by differences in contrast, as it has for other second-order boundaries [<xref ref-type="bibr" rid="pcbi.1006829.ref055">55</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref057">57</xref>].</p>
<p>This question is of particular interest in light of several studies that have revealed possible neurophysiological mechanisms for contrast-boundary detection [<xref ref-type="bibr" rid="pcbi.1006829.ref036">36</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref091">91</xref>]. This work has demonstrated a population of contrast-modulation (CM) tuned neurons in early visual cortex, which exhibit band-pass spatial frequency and orientation tuning to second-order modulation envelope frequency, consistent with a Gabor-shaped second-stage filter applied to the outputs of a bank of first-stage filters selectively tuned to the carrier pattern [<xref ref-type="bibr" rid="pcbi.1006829.ref019">19</xref>].</p>
<p>If such CM-responsive neurons were the mechanism responsible for psychophysical detection of CM boundaries, one might expect the 1-D weighting profiles to have a Gabor-like shape, placing greater weight on regions near the boundary. However, we find that this was not the case and that all areas of the image are weighted equally for the orientation-identification task (Experiment 1, Figs <xref ref-type="fig" rid="pcbi.1006829.g003">3</xref> and <xref ref-type="fig" rid="pcbi.1006829.g004">4</xref>). Furthermore, in an additional control manipulation where we increased the carrier frequency in order to make sure CM neurons with smaller RFs would be relatively more engaged (Experiment 1-HFC), we still did not see any change in the type of spatial integration (<xref ref-type="fig" rid="pcbi.1006829.g005">Fig 5</xref>). These findings are consistent with at least three distinct possibilities (not mutually exclusive) for the underlying neural mechanisms. One possibility is that task performance reflects CM-responsive neurons, like those described in V2 with large receptive fields (&gt; 4 dva) encompassing the entire stimulus used in our experiments. Such neurons would prefer a carrier frequency much higher than their envelope frequency, which is plausible since primate V2 CM-responsive neurons can have preferred carrier:envelope ratios of up to 40:1 [<xref ref-type="bibr" rid="pcbi.1006829.ref038">38</xref>]. A second possibility is that psychophysical performance is mediated by a range of Gabor-like second-order mechanisms covering a range of spatial scales, and that our measured 1-d profiles reflect an ensemble of their contributions. Another possibility is that an important role is played by other neurons sensitive to texture patterns, such as those described in ventral stream areas [<xref ref-type="bibr" rid="pcbi.1006829.ref081">81</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref083">83</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref092">92</xref>].</p>
<p>Clearly, the fact that we can readily distinguish non-juxtaposed textures implies that there must be mechanisms for texture representation that operate in the absence of a boundary. However, in some cases texture segmentation can benefit from boundary information, as in the textural Craik-Cornsweet illusion [<xref ref-type="bibr" rid="pcbi.1006829.ref055">55</xref>]. For example, [<xref ref-type="bibr" rid="pcbi.1006829.ref057">57</xref>] demonstrated that direct juxtaposition of textures having different orientations gives rise to local junction or corner cues that can support segmentation. Such enhanced segmentation might be mediated by neurons such as those described by [<xref ref-type="bibr" rid="pcbi.1006829.ref093">93</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref094">94</xref>], which are selective for such cues.</p>
<p>However, other work on texture segmentation has shown a minimal contribution of information along the boundary. A study using natural image patches in a task of discriminating occlusion boundaries from uniform textures found that removing the boundary region had little effect on performance [<xref ref-type="bibr" rid="pcbi.1006829.ref018">18</xref>]. However, performance on natural occlusion edge and junction detection tasks can be seriously degraded when textural information, which is by its nature defined over a region, is removed [<xref ref-type="bibr" rid="pcbi.1006829.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref048">48</xref>]. Similarly, segmentation of complex textures defined by letters shows little effect of boundary information [<xref ref-type="bibr" rid="pcbi.1006829.ref056">56</xref>].</p>
<p>In general, the answer to this question is most likely dependent on the particular stimulus and the nature of the psychophysical task, as indicated by the very different results of our first two experiments. In our view, the question of “edge” versus “region”-based processing should be treated with some caution, since clearly for a very large stimulus extending into the far periphery there will be reduced weighting at the largest eccentricities, since visual acuity falls off sharply outside the fovea. In such a case the real question of interest would be the shape of the fall-off in weight as a function of stimulus eccentricity, and how this fall-off may relate to putative neural codes. Our method is well suited to address such issues for many different kinds of second-order boundaries and tasks, much as standard classification images can be similarly used for first-order boundaries.</p>
<p>We further demonstrate that our modeling methodology is consistent, meaning that it can accurately recover the actual perceptual filters employed in a task, provided the fitted and generating models have corresponding architectures. Simulation of ideal observers implementing sub-optimal spatial processing, e.g. utilizing only limited subsets of the texture, revealed filters which do not match those we observed experimentally (<xref ref-type="fig" rid="pcbi.1006829.g010">Fig 10a</xref>). In contrast, a strong agreement was found with results from simulating an observer monitoring the whole stimulus for each of two possible boundary orientations (<xref ref-type="fig" rid="pcbi.1006829.g010">Fig 10b</xref>), strengthening our interpretation that this is the kind of spatial summation actually being employed.</p>
<p>In Experiment 1 our results showed that human observers use every part of the image to perform the task (<xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3</xref>), consistent with an ideal observer that integrates information from all image regions (<xref ref-type="fig" rid="pcbi.1006829.g010">Fig 10b</xref>). In terms of possible mechanisms, we might therefore expect CM-tuned neurons with larger receptive fields that integrate over the expanse of the stimulus image to be engaged in Experiment 1. The results in Experiment 2 showed that only the texture information near the boundary was being utilized by humans (<xref ref-type="fig" rid="pcbi.1006829.g011">Fig 11</xref>), again consistent with an ideal observer that is indifferent to the stimulus elsewhere. This suggests that the most useful neural substrate in Experiment 2 might be CM-responsive neurons with smaller receptive fields localized near the boundary, that integrate texture only over small extents to either side of the boundary.</p>
<p>We demonstrate using simulation that one cannot explain the filters we observe in Experiment 2 by assuming two large off-orientation receptive fields (<xref ref-type="supplementary-material" rid="pcbi.1006829.s022">S22 Fig</xref>), since otherwise we would have recovered filters with slightly non-vertical orientation rather than filters localized to the vertical (<xref ref-type="fig" rid="pcbi.1006829.g011">Fig 11</xref>). We conjecture that our results are more consistent with the possibility that observers are making use of contrast-modulation tuned neurons having smaller receptive fields localized to the boundary.</p>
<p>In both Experiments 1 and 2 we found that humans behaved as ideal observers with regard to their spatial integration of texture. Such a result should not be regarded as inevitable or having little relevance for questions of neural mechanism. Previous psychophysical studies (including several classification image studies) have demonstrated that humans often do behave ideally in perceptual tasks [<xref ref-type="bibr" rid="pcbi.1006829.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref095">95</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref098">98</xref>] and furthermore such ideal behavior can be suggestive of neuronal mechanisms underlying the behavior [<xref ref-type="bibr" rid="pcbi.1006829.ref099">99</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref101">101</xref>]. However in many other cases, human psychophysical performance does not match that of an ideal observer [<xref ref-type="bibr" rid="pcbi.1006829.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref096">96</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref098">98</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref102">102</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref103">103</xref>]. Interestingly, [<xref ref-type="bibr" rid="pcbi.1006829.ref096">96</xref>] demonstrated that even with the same stimulus, changes in the psychophysical task could alter whether or not human observers behaved ideally. We feel that the demonstration of ideal behavior in Experiments 1 and 2 is significant, and strongly suggests possible neural mechanisms, for example suggesting a greater involvement of neurons with larger CM-responsive receptive fields in the task of Experiment 1, and those with smaller receptive fields in Experiment 2.</p>
<p>In general, the relationship between neural response properties and the perceptual filters obtained in classification image studies is complex [<xref ref-type="bibr" rid="pcbi.1006829.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref104">104</xref>]. Perceptual filters do not generally correspond to fixed, "hard-wired" neural populations, but are most likely actively constructed from available neuronal populations through perceptual learning. Consistent with this idea, previous work has demonstrated that observers’ perceptual summation gets successively closer to that of the ideal observer with practice [<xref ref-type="bibr" rid="pcbi.1006829.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref105">105</xref>]. It would be of great interest for future research to explore the dynamics of the perceptual filtering strategies that observers employ in our tasks.</p>
</sec>
<sec id="sec026">
<title>Integration of heterogeneous first-stage channels</title>
<p>A common ingredient to the various FRF models proposed to account for psychophysical texture segmentation data is a set of fine-scale first-stage filters summed by one or more second-stage filters defined on a much larger spatial scale. However one aspect in which such models differ is whether the individual second-stage filters analyze first-stage filter responses at a single orientation/spatial frequency, or whether they integrate across heterogeneous first-stage channels. In one standard model of FRF processing, each second-stage filter only analyzes the outputs of a single first-stage orientation/spatial frequency channel [<xref ref-type="bibr" rid="pcbi.1006829.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref063">63</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref064">64</xref>]. However a number of psychophysical results have called this model into question. One study using a sub-threshold summation paradigm found support for a model utilizing linear integration across carrier orientation for detecting contrast modulations [<xref ref-type="bibr" rid="pcbi.1006829.ref040">40</xref>]. Another study making use of energy-frequency analysis revealed the existence of orientation-opponent mechanisms in second-order vision [<xref ref-type="bibr" rid="pcbi.1006829.ref062">62</xref>]. Other work has suggested at least partly independent mechanisms for detecting second-order modulations defined by contrast and spatial/frequency orientation, with the suggestion that contrast modulations may be pooled across first-stage channels [<xref ref-type="bibr" rid="pcbi.1006829.ref039">39</xref>].</p>
<p>In Experiment 3 we find support for a model with second-stage filters that integrate across first-stage orientation channels, consistent with the latter psychophysical studies. Although CM-responsive neurons described so far exhibit narrow tuning for carrier orientation [<xref ref-type="bibr" rid="pcbi.1006829.ref038">38</xref>], this discrepancy could be due to the use of grating carriers rather than multi-orientation micropatterns. This suggests that that possible effects of carrier orientation bandwidth might be worth future investigation, both in human psychophysics and for CM-responsive neurons.</p>
<p>In our opinion, Experiment 3 best demonstrates the power of this methodology compared to traditional linear classification image approaches: The ability to explore a much larger space of non-linear perceptual models embodying various assumptions about neural mechanisms. Rather than simply comparing recovered filter shapes to ideal behavior as in most previous classification image studies [<xref ref-type="bibr" rid="pcbi.1006829.ref005">5</xref>], here we directly compare two biologically plausible hypotheses of how contrast modulation is integrated across orientation channels. Our finding of clear support for an early summation model [<xref ref-type="bibr" rid="pcbi.1006829.ref040">40</xref>] suggests that an interesting direction for future neurophysiological investigation would be to examine neural responses to contrast modulations of carrier patterns containing multiple orientations. Such a study would be quite novel, as previous neurophysiological investigations of contrast modulation tuning have only considered a single carrier orientation [<xref ref-type="bibr" rid="pcbi.1006829.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref038">38</xref>].</p>
</sec>
<sec id="sec027">
<title>Conclusions</title>
<p>Statistical machine learning has in recent years become a powerful and widely applied methodology in vision and in computational neuroscience. Our work demonstrates that the same machine learning methodology used to characterize neural coding [<xref ref-type="bibr" rid="pcbi.1006829.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref052">52</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref088">88</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref106">106</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref107">107</xref>] and first-order visual processing [<xref ref-type="bibr" rid="pcbi.1006829.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref051">51</xref>] can be fruitfully applied to extend psychophysical system identification approaches to complex visual stimuli, including second-order boundaries. We anticipate that future applications of this approach may lead to a better understanding of how multiple first- and second-order cues are combined by human observers to detect natural boundaries, and provide insights into neural and computational mechanisms of image region segmentation.</p>
</sec>
</sec>
<sec id="sec028" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec029">
<title>Experimental paradigms</title>
<sec id="sec030">
<title>Stimuli and psychophysical task</title>
<p>Contrast-modulated texture boundary stimuli were created similarly to those of [<xref ref-type="bibr" rid="pcbi.1006829.ref028">28</xref>], by multiplying a large (predominantly low spatial frequency) contrast modulation envelope by a high spatial frequency carrier (texture) pattern (<xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1b</xref>). The modulation envelope was a circular region, half of which had a larger value (giving higher texture contrast) and the other half had a smaller value (producing lower texture contrast). This pair of half-discs was oriented either right oblique (clockwise) or left-oblique (counter-clockwise). The boundary between the two regions was tapered smoothly, as was the outer edge of the circular region, so that it smoothly decayed into the background (see stimulus images in <xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1</xref>). The functional form of the tapering was a half-cosine going from 1 to -1 over a proportion (0.2) of the stimulus width, as used previously [<xref ref-type="bibr" rid="pcbi.1006829.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref090">90</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref108">108</xref>]. Such tapering was a precaution against first-order artifacts.</p>
<p>The carrier patterns were produced by adding odd-phase Gabor micropatterns at random locations in an over-size larger image, which was subsequently cropped to the desired size. Only texture patterns having sufficient uniformity in root-mean-squared (rms) contrast were utilized, following the uniformity criterion of [<xref ref-type="bibr" rid="pcbi.1006829.ref028">28</xref>]. The rms contrast of the carrier patterns was set to 14%, which is 6 dB above previously observed average carrier detection thresholds [<xref ref-type="bibr" rid="pcbi.1006829.ref108">108</xref>]. Examples of right-oblique boundaries with varying carrier mircopattern densities and depths of contrast modulation are shown in <xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1b</xref>. Boundaries between micropattern textures like these have been used in previous psychophysics studies [<xref ref-type="bibr" rid="pcbi.1006829.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref090">90</xref>], and offer flexibility to produce a wide variety of naturalistic texture patterns amenable to parametric variation.</p>
<p>In all experiments, observers performed a single-interval 2AFC task to determine whether a briefly displayed (100 msec) boundary stimulus was contrast-modulated in a right- or left-oblique orientation. Envelope phase was randomized across trials, to preclude its use as a cue. Visual feedback was provided after each trial to maximize observer performance. Data was collected over multiple (4–5) sessions of 30–60 minutes, conducted over several days. An initial training session was dedicated to learning the orientation identification task (+/- 45 degree) by performing a short version (300 trials) at various micropattern densities (256, 1024, 2048 patterns or 512, 1024, 2048 patterns). Consistent with previous studies on second-order edge detection [<xref ref-type="bibr" rid="pcbi.1006829.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref090">90</xref>], we found in preliminary experiments that task performance was best for carrier texture patterns having a high density of micropatterns. Therefore we used relatively high micropattern densities (2048 per 256x256 image for Experiments 1, 2 and 512 for Experiment 3), higher than in previous studies [<xref ref-type="bibr" rid="pcbi.1006829.ref028">28</xref>], with the goal of optimizing observer performance.</p>
</sec>
<sec id="sec031">
<title>Psychophysical observers</title>
<p>Observers were author CJD and 7 members of his research group (adult undergraduate students) who were initially naïve to the experimental purposes. For Experiment 1, 4 additional observers were recruited from two Psychology classes (EXP-3202, PSB-4002) at Florida Gulf Coast University (FGCU). All observers had normal or corrected-to-normal acuity. All observers provided written informed consent and all procedures were approved beforehand by the IRBs at FGCU (Protocol 2014–01) and McGill University (Protocol MED-B-96-279), in accordance with the Declaration of Helsinki.</p>
</sec>
<sec id="sec032">
<title>Displays and experimental control</title>
<p>FGCU experiments were run in one of two windowless vision psychophysics chambers with no light sources other than the computer. Chamber 1 contained a factory-calibrated (gamma-corrected) LCD (Cambridge Research Systems, Display++, 120Hz, 1920x1080 pixels, 100 cd/m<sup>2</sup>). Chamber 2 contained a gamma-corrected CRT (SONY FD-500 Trinitron, 75 Hz, 1024x768 pixels, 59.2 cd/m<sup>2</sup>) driven by a Bits# stimulus processor (Cambridge Research Systems). In each chamber the display was controlled by a Dell OptiPlex 9020 with an NVIDIA GeForce GTX 645 graphics card. Observers viewed the stimuli binocularly, with a chin rest, at a distance of 133 cm, so that the 256x256 stimulus image subtended approximately 4 deg. visual angle (dva). McGill data was collected from author CJD for Experiments 2 and 3. Stimuli were displayed on a CRT (SONY Multiscan G400, 75 Hz, 1024x768 pixels, 53 cd/m<sup>2</sup>) with gamma correction through the color lookup tables. This monitor was driven by an Apple Mac Pro 4,4 (2.26 GHz Quad Core) hosting an NVIDIA GeForce GT 120 graphics card. Binocular viewing was at a distance of 125 cm, so that the stimulus subtended 4 deg, and experiments were conducted in a room with normal lighting. We observed remarkable consistency in our results from the three setups, despite differences in monitor characteristics and room lighting conditions, so all data was pooled for analysis. Experiments were controlled using custom-authored software written in the MATLAB programming language (MathWorks, Inc.), making use of routines from the PsychToolbox Version 3.0.12 [<xref ref-type="bibr" rid="pcbi.1006829.ref109">109</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref111">111</xref>]. To speed up experiments, carrier patterns were pre-computed and stored at 8 bit precision on the host computer. A unique set of pre-computed carrier patterns was used for each experiment, so that a given observer never saw the same carrier pattern more than once.</p>
</sec>
<sec id="sec033">
<title>Experiment 1: Orientation identification</title>
<p>Stimuli were 256 x 256 pixels in size with a carrier pattern comprised of 2048 randomly placed, vertically oriented (0 degrees) Gabor micropattern stimuli (32 pixels or 0.5 dva., 1:1 aspect ratio). In this task, the two stimulus alternatives had right/left-oblique (+/- 45 deg.) boundaries (<xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1b and 1c</xref>). In the main version of Experiment 1, for 7 observers, contrast modulation depths were varied across 9 logarithmically spaced levels from 8% to 32%, centered at 16%, plus 0% modulation (Experiment 1-VAR), using a method of constant stimuli. This set of levels resulted in trials at or near threshold for most observers, while still including some relatively easy trials. After an initial training session, observers each performed a total of 4000 trials (4 blocks of 1000 trials, 100 trials per level) over the course of multiple sessions spaced over several days. For one observer (CJD), 2975 micropatterns were used instead of 2048.</p>
<p>For 3 observers (CJD and naïve observers MAK and JJF), contrast modulation depths were fixed at a value slightly above each observer’s measured threshold (Experiment 1-FIX), with the goal of obtaining near-threshold task performance of approximately 80% correct (<xref ref-type="supplementary-material" rid="pcbi.1006829.s012">S12 Fig</xref>). This is more typical of most classification image literature [<xref ref-type="bibr" rid="pcbi.1006829.ref005">5</xref>], which often fixes stimulus strengths at or near task performance thresholds. These observers performed 6000 trials (6 blocks of 1000 trials) over multiple sessions spaced across several days.</p>
<p>As an additional variant of Experiment 1, we used a carrier texture composed of micropatterns with a much higher spatial frequency (8 pixels or 0.125 dva.) and a higher density (8192), with rms contrast remaining at 14%. Experiment 1-HFC was run on three naïve observers (LJW, JMM, VHB) tested at variable modulation contrasts (same levels as Experiment 1-VAR).</p>
</sec>
<sec id="sec034">
<title>Experiment 2: Fine orientation discrimination</title>
<p>In order to show the general applicability of our methodology to tasks where observers may employ an edge-based spatial summation, we ran a fine orientation discrimination task in which observers determined whether a clearly visible contrast-modulated boundary (same carrier parameters as in Experiment 1) was tilted slightly left- vs right-oblique relative to vertical. Experiment 2 is complementary to Experiment 1, in which the orientation difference is very large (+/- 45 degrees from vertical), while the contrast modulation is near detection threshold, whereas in Experiment 2 the contrast modulation is well above detection threshold, but the orientation difference is very small. This experiment was run on 2 observers, one of whom (JJF) was naïve to the purpose of the experiment. In a preliminary experiment at 32% contrast modulation, a method of constant stimuli was used to measure a just-noticeable-difference (JND) for orientation discrimination (approximately 75% performance) for each observer (JJF: 6 deg., CJD: 7 deg.). Then in subsequent sessions, observers performed the fine orientation discrimination at this value for a set of stimuli whose contrast modulation was varied from 16% to 64% (center = 32%) with a method of constant stimuli. Observers performed 6000 trials of the task over multiple sessions spaced over multiple days (6 blocks of 1000 trials, 100 trials per level). We refer to this experiment as Experiment 2-VAR. For two naïve observers (VHB, JJF) we also ran Experiment 2-FIX, where the orientation difference was fixed at each observer’s JND.</p>
</sec>
<sec id="sec035">
<title>Experiment 3: Orientation identification with a more complex carrier texture</title>
<p>Experiment 3 was the same as Experiment 1-VAR except using a carrier texture containing micropatterns with two orthogonal orientations, as in <xref ref-type="fig" rid="pcbi.1006829.g001">Fig 1a</xref> (bottom). For this experiment the micropatterns were elongated (2:1 aspect ratio instead of 1:1 as in Experiments 1, 2), to give a narrower orientation bandwidth. In addition the density was reduced (512 micropatterns instead of 2048), to create a stronger percept of oriented structure in the carrier texture.</p>
</sec>
</sec>
<sec id="sec036">
<title>Modeling approach</title>
<sec id="sec037">
<title>Modeling framework</title>
<p>Most previous psychophysical SI methods assume that the decision variable arises from a weighted linear sum of the values (e.g. pixel luminances) of the sensory stimulus [<xref ref-type="bibr" rid="pcbi.1006829.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref005">5</xref>], essentially modeling the entire chain of brain mechanisms involved in stimulus-processing and decision-making by a single linear filter followed by a threshold. Here we propose a more general approach which we refer to as <italic>psychophysical network modeling</italic>. In this scheme, the decision variable is a function of the stimulus-dependent activities of a set of "hidden units", each of which may be sensitive to different aspects of the stimulus. This idea closely parallels recent modeling of neural responses by combining the outputs of multiple nonlinear subunits selective for different stimulus features [<xref ref-type="bibr" rid="pcbi.1006829.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1006829.ref024">24</xref>].</p>
<p>Given a classification task for a stimulus <inline-formula id="pcbi.1006829.e002"><alternatives><graphic id="pcbi.1006829.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> which elicits one of two behavioral alternatives (<italic>b</italic> = 1, <italic>b</italic> = −1), one may define a generic psychophysical model as
<disp-formula id="pcbi.1006829.e003">
<alternatives>
<graphic id="pcbi.1006829.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">θ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(M.1)</label>
</disp-formula>
<disp-formula id="pcbi.1006829.e004">
<alternatives>
<graphic id="pcbi.1006829.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(M.2)</label>
</disp-formula>
where <italic>σ</italic> is a psychometric function, <italic>u</italic>(<bold>x</bold>, <bold>θ</bold>) denotes the stimulus-dependent input to the psychometric function whose parameters <bold>θ</bold> characterize the sensory processing model, and <italic>γ</italic> denotes the lapse rate. The stimulus inputs <bold>x</bold> may be either the stimulus parameterization itself (e.g. pixel luminances), <bold>I</bold>, or a transformed version of the stimulus parameterization (<bold>x</bold> = <italic>ϕ</italic>(<bold>I</bold>)). We define a <italic>psychophysical network model</italic> by making the additional assumption that the input <italic>u</italic>(<bold>x</bold>, <bold>θ</bold>) to the psychometric function is some function of the activities <italic>s</italic><sub>1</sub>, ⋯, <italic>s</italic><sub><italic>K</italic></sub> of a set of <italic>K</italic> hidden units, i.e.
<disp-formula id="pcbi.1006829.e005">
<alternatives>
<graphic id="pcbi.1006829.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(M.3)</label>
</disp-formula>
where <bold>v</bold> is a set of parameters characterizing how the <italic>s</italic><sub><italic>i</italic></sub> are combined. Perhaps the simplest combination rule is a weighted linear summation, i.e.</p>
<disp-formula id="pcbi.1006829.e006">
<alternatives>
<graphic id="pcbi.1006829.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub>
</mml:math>
</alternatives>
<label>(M.4)</label>
</disp-formula>
<p>Note that we can have more than 2 hidden units (K &gt; 2) even though there are only two behavioral responses (e.g. model of <xref ref-type="fig" rid="pcbi.1006829.g012">Fig 12a</xref>). We assume that the hidden unit activities are a nonlinear function of the projection of the features <bold>x</bold> onto some filter <bold>w</bold><sub><italic>i</italic></sub>,
<disp-formula id="pcbi.1006829.e007">
<alternatives>
<graphic id="pcbi.1006829.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow></mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(M.5)</label>
</disp-formula>
where <bold>α</bold> parameterizes the shape of the nonlinearity. Combining Eqs (<xref ref-type="disp-formula" rid="pcbi.1006829.e003">M.1</xref>), (<xref ref-type="disp-formula" rid="pcbi.1006829.e006">M.4</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1006829.e007">M.5</xref>) yields a final expression for a psychophysical network model with linear cue combination
<disp-formula id="pcbi.1006829.e008">
<alternatives>
<graphic id="pcbi.1006829.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e008" xlink:type="simple"/>
<mml:math display="block" id="M8">
<mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">θ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow></mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(M.6)</label>
</disp-formula></p>
<p>Various standard models used in machine learning are special cases of (<xref ref-type="disp-formula" rid="pcbi.1006829.e008">M.6</xref>). For instance, standard binary logistic regression [<xref ref-type="bibr" rid="pcbi.1006829.ref044">44</xref>] used for simple yes/no detection corresponds to the case of <italic>γ</italic> = 0 and a single hidden unit whose nonlinearity is a simple linear pass-through (<italic>h</italic>(<italic>u</italic>) = <italic>u</italic>) with fixed output weight <italic>v</italic><sub>1</sub> = 1, so that (<xref ref-type="disp-formula" rid="pcbi.1006829.e008">M.6</xref>) reduces to
<disp-formula id="pcbi.1006829.e009">
<alternatives>
<graphic id="pcbi.1006829.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="bold">θ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow></mml:msubsup><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(M.7)</label>
</disp-formula></p>
<p>Likewise, with <italic>γ</italic> = 0 and the sigmoidal hidden unit nonlinearity <italic>h</italic>(<italic>u</italic>) = 1/(1 + <italic>e</italic><sup>−<italic>u</italic></sup>), this model (<xref ref-type="disp-formula" rid="pcbi.1006829.e008">M.6</xref>) becomes a three-layer connectionist neural network [<xref ref-type="bibr" rid="pcbi.1006829.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref112">112</xref>].</p>
</sec>
<sec id="sec038">
<title>Filter-rectify-filter model</title>
<p>A psychophysical network model for detecting whether a second-order edge is oriented left- or right-oblique is illustrated in <xref ref-type="fig" rid="pcbi.1006829.g002">Fig 2</xref>. In this model, the inputs <bold>x</bold> are the down-sampled outputs of a bank of first-stage energy filters resembling V1 complex cells applied to the stimulus image <bold>I</bold>, a transformation which we represent as <bold>x</bold> = <italic>ϕ</italic>(<bold>I)</bold>. There are two second-stage filters which serve as inputs to the hidden units. One hidden unit (L) learns a set of filter weights (blue lines) to detect a left-oblique edge from the pattern of activity in the first-stage subunits, while the other hidden unit (R) learns a set of filter weights to detect a right-oblique edge. The filter outputs are passed through a rectified power-law nonlinearity <italic>h</italic>(<italic>u</italic>) = |<italic>u</italic>|<sup><italic>α</italic></sup> (blue curve) to generate the hidden unit activities <italic>s</italic><sub><italic>L</italic></sub>, <italic>s</italic><sub><italic>R</italic></sub>. Note it is important for <italic>h</italic>(<italic>u</italic>) to be even-symmetric, i.e. <italic>h</italic>(<italic>u</italic>) = <italic>h</italic>(−<italic>u</italic>), to provide envelope phase-invariance—otherwise the model would require four rather than two hidden units, since in our psychophysical experiment the envelope phase is randomized on each trial.</p>
<p>The hidden unit activities <italic>s</italic><sub><italic>L</italic></sub>, <italic>s</italic><sub><italic>R</italic></sub> are combined, assuming fixed output weights (<italic>v</italic><sub><italic>R</italic></sub> = 1, <italic>v</italic><sub><italic>L</italic></sub> = −1), and a bias term <italic>v</italic><sub>0</sub> is added to obtain a decision variable <italic>u</italic> = <italic>s</italic><sub><italic>R</italic></sub> − <italic>s</italic><sub><italic>L</italic></sub> + <italic>v</italic><sub>0</sub>. The fixed output weights are a consequence of our choice of a power-law nonlinearity, since for all inputs <bold>x</bold>, filters <bold>w</bold> and output weights <italic>v</italic>, the term <italic>v</italic>|<bold>w</bold><sup>T</sup><bold>x</bold>|<sup><italic>α</italic></sup> is identical for all <italic>v</italic>, <bold>w</bold> satisfying <italic>v</italic>∥<bold>w</bold>∥<sup><italic>α</italic></sup> = <italic>C</italic>, where <italic>C</italic> is some constant and ∥<bold>w</bold>∥ denotes the norm of <bold>w</bold>—see [<xref ref-type="bibr" rid="pcbi.1006829.ref113">113</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref114">114</xref>]. Many variants on this basic FRF model architecture are possible, and fitting multiple models to a given dataset could permit us to investigate questions about FRF model architecture [<xref ref-type="bibr" rid="pcbi.1006829.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1006829.ref041">41</xref>].</p>
<p>A model of the form (<xref ref-type="disp-formula" rid="pcbi.1006829.e008">M.6</xref>) arises naturally where there are two signals representing the two stimulus alternatives whose activities are compared to make a decision, for instance, the outputs of the two subunits in <xref ref-type="fig" rid="pcbi.1006829.g002">Fig 2</xref>. Subtracting the activities <italic>s</italic><sub>1</sub> − <italic>s</italic><sub>2</sub> and adding a bias <italic>v</italic><sub>0</sub> and Gaussian internal noise <italic>ε</italic>~<italic>N</italic>(0, <italic>ρ</italic><sup>2</sup>) models the proportion of times behavioral alternative <italic>b</italic> = 1 is chosen as
<disp-formula id="pcbi.1006829.e010">
<alternatives>
<graphic id="pcbi.1006829.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e010" xlink:type="simple"/>
<mml:math display="block" id="M10">
<mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>Φ</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(M.8)</label>
</disp-formula>
where Φ is the cumulative unit normal distribution and <italic>μ</italic> = <italic>s</italic><sub>1</sub> − <italic>s</italic><sub>2</sub> + <italic>v</italic><sub>0</sub>. Using the approximation Φ(<italic>u</italic>) ≈ <italic>σ</italic>(<italic>ku</italic>), where <italic>σ</italic> is the sigmoid function and <italic>k</italic> is optimized to best approximate Φ(<italic>u</italic>), we can re-write (<xref ref-type="disp-formula" rid="pcbi.1006829.e010">M.8</xref>) as
<disp-formula id="pcbi.1006829.e011">
<alternatives>
<graphic id="pcbi.1006829.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(M.9)</label>
</disp-formula>
where <inline-formula id="pcbi.1006829.e012"><alternatives><graphic id="pcbi.1006829.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula> is inversely proportional to the internal noise <italic>ρ</italic>. Since <inline-formula id="pcbi.1006829.e013"><alternatives><graphic id="pcbi.1006829.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow></mml:msubsup><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>, this inverse internal noise parameter <italic>β</italic> is simply absorbed into the magnitudes of the second-stage filter weights <bold>w</bold><sub><italic>i</italic></sub> and bias <italic>v</italic><sub>0</sub>. Therefore, larger filter norms will correspond to larger values of <italic>β</italic> and lower internal noise. In the analysis here, the norms of the <bold>w</bold><sub><italic>i</italic></sub> were not solely determined by the internal noise since they were also constrained by the Bayesian prior. However, we found that the model provided an excellent fit to our psychophysical data without explicitly optimizing an additional internal noise parameter, so with the exception of only one analysis for Experiment 1-FIX (<xref ref-type="supplementary-material" rid="pcbi.1006829.s013">S13 Fig</xref>) we did not optimize a noise parameter.</p>
</sec>
<sec id="sec039">
<title>Model estimation and regularization</title>
<p>Here we simplify notation by combining all model parameters and the lapse rate into the single parameter vector <bold>η</bold>. Given psychophysical stimulus-response data <inline-formula id="pcbi.1006829.e014"><alternatives><graphic id="pcbi.1006829.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, and prior constraints <italic>p</italic><sub>0</sub>(<bold>η</bold>) (possibly uninformative) on the model parameters, we pose the estimation as a machine learning problem of finding the maximum a posteriori (MAP) estimate of <bold>η</bold>, given by:
<disp-formula id="pcbi.1006829.e015">
<alternatives>
<graphic id="pcbi.1006829.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e015" xlink:type="simple"/>
<mml:math display="block" id="M15">
<mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">η</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>argmax</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="bold">η</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mtext>ln</mml:mtext><mml:mspace width="2pt"/><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">η</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext>ln</mml:mtext></mml:mrow></mml:mrow><mml:mspace width="2pt"/><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="bold">η</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mtext>ln</mml:mtext><mml:mspace width="2pt"/><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">η</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(M.10)</label>
</disp-formula></p>
<p>In this framework, the prior constraints <italic>p</italic><sub>0</sub>(<bold>η</bold>) are often used to prevent over-fitting (Bishop, 2006), which can be a serious problem due to the relatively large number of parameters (hundreds or thousands) being estimated from a limited number (generally &lt; 10K) of psychophysical trials. Various techniques have been applied to help reduce the problems of noise and over-fitting, including Bayesian priors [<xref ref-type="bibr" rid="pcbi.1006829.ref051">51</xref>], radial averaging [<xref ref-type="bibr" rid="pcbi.1006829.ref008">8</xref>] or using simplified low-dimensional stimuli [<xref ref-type="bibr" rid="pcbi.1006829.ref010">10</xref>]. Here we considered two different Bayesian methods for regularization of the second-stage filter weights. Under a prior assumption that the weights are from a multivariate Gaussian distribution, a “ridge-regression” penalty [<xref ref-type="bibr" rid="pcbi.1006829.ref044">44</xref>] is applied to each of the second-stage filter weight vectors <bold>w</bold><sub><italic>i</italic></sub> (<xref ref-type="fig" rid="pcbi.1006829.g002">Fig 2</xref>, blue lines)—this encourages small weight magnitudes, to prevent fitting noise. The prior is given by
<disp-formula id="pcbi.1006829.e016">
<alternatives>
<graphic id="pcbi.1006829.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e016" xlink:type="simple"/>
<mml:math display="block" id="M16">
<mml:mtext>ln</mml:mtext><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(M.11)</label>
</disp-formula>
where <italic>w</italic><sub><italic>ij</italic></sub> denotes the <italic>j</italic>-th element of filter <bold>w</bold><sub><italic>i</italic></sub> and the hyperparameter <italic>λ</italic> controls the strength of the penalty (smaller <italic>λ</italic> induces a stronger penalty). Optimizing <italic>λ</italic> using <italic>k</italic>-fold cross-validation [<xref ref-type="bibr" rid="pcbi.1006829.ref044">44</xref>], <italic>λ</italic> = 0.1 − 1.0 provided a good trade-off between fitting to the training data and generalizing to novel data in the test set in Experiment 1—see <xref ref-type="supplementary-material" rid="pcbi.1006829.s001">S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006829.s002">S2</xref> Figs. This parameter was re-optimized for each of the two models in Experiment 3 (which differed in architecture from those used in Experiments 1, 2) using the same procedure (<xref ref-type="supplementary-material" rid="pcbi.1006829.s019">S19 Fig</xref>).</p>
<p>We also considered the effects of an additional prior <italic>q</italic><sub>0</sub>(<bold>w</bold><sub><italic>i</italic></sub>) favoring second-stage weights that form a spatially smooth filter [<xref ref-type="bibr" rid="pcbi.1006829.ref115">115</xref>]. Such a prior was implemented by constructing a matrix <bold>S</bold><sub><bold>2</bold></sub> whose rows are the 2-D discrete Laplace operator
<disp-formula id="pcbi.1006829.e017">
<alternatives>
<graphic id="pcbi.1006829.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e017" xlink:type="simple"/>
<mml:math display="block" id="M17">
<mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>4</mml:mn></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(M.12)</label>
</disp-formula>
centered at every point in a <italic>p</italic> × <italic>p</italic> matrix of zeros and unwrapped into a vector of length <italic>d</italic> = <italic>p</italic><sup>2</sup>, where <italic>p</italic> is the down-sampled image dimensionality. This smoothness prior is specified by
<disp-formula id="pcbi.1006829.e018">
<alternatives>
<graphic id="pcbi.1006829.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e018" xlink:type="simple"/>
<mml:math display="block" id="M18">
<mml:mtext>ln</mml:mtext><mml:mspace width="4pt"/><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>ω</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:msubsup><mml:mi mathvariant="bold">w</mml:mi><mml:mi>i</mml:mi><mml:mtext>T</mml:mtext></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(M.13)</label>
</disp-formula>
with hyper-parameter <italic>ω</italic> controlling the extent to which roughness is penalized (smaller <italic>ω</italic> induces a stronger penality). Optimizing <italic>ω</italic> using <italic>k</italic>-fold cross-validation with <italic>λ</italic> = 1, there was a slight improvement in generalization when the smoothness prior (<xref ref-type="disp-formula" rid="pcbi.1006829.e018">M.13</xref>) was also included, with the value of <italic>ω</italic> = 1 providing a good tradeoff between fitting the data and generalizing to novel test data in Experiment 1 (<xref ref-type="supplementary-material" rid="pcbi.1006829.s003">S3</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006829.s004">S4</xref> Figs). Adding this additional smoothness constraint reduced fitting to the training dataset, but improved generalization of the model to novel data (<xref ref-type="supplementary-material" rid="pcbi.1006829.s005">S5</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006829.s006">S6</xref> Figs).</p>
</sec>
<sec id="sec040">
<title>Numerical optimization</title>
<p>Second-stage filter weights <bold>w</bold><sub><italic>i</italic></sub> and the exponent α of the hidden unit nonlinearity were estimated using alternating coordinate ascent in a manner similar to the EM algorithm [<xref ref-type="bibr" rid="pcbi.1006829.ref116">116</xref>]. Weights were optimized for one filter <bold>w</bold><sub><italic>i</italic></sub> at a time while all other filters <bold>w</bold><sub><italic>j≠i</italic></sub> and the power law exponent α were fixed. The filter weights, starting with randomized initial values, were optimized using a quasi-Newton method (L-BFGS). To estimate the power law exponent α, all of the second-stage filters were fixed and a 1-D line search was performed over the range [0.1,4.0]. The optimizations for <bold>w</bold><sub><italic>i</italic></sub> and α are not log-concave, which means that local maxima may be possible [<xref ref-type="bibr" rid="pcbi.1006829.ref117">117</xref>]. In practice we usually found robust and consistent estimates of model parameters across optimization runs starting from several starting points. We found that sometimes the search converged on a “local maximum” consisting of only a single non-zero filter. Therefore, as an additional precaution, after the first round of optimization (or at least 4 total rounds depending on the experiment) we always re-initialized the search at a point in parameter space where the filter that was smaller in magnitude was set to be mirror-symmetric to the current value of the larger one. The optimization was then allowed to proceed as usual for several more rounds before termination. This procedure had the effect of eliminating any problems with local minima.</p>
<p>In preliminary investigations the estimated lapse rate <italic>γ</italic> was always zero, so this parameter was not included in our final model. Previous work has shown the best estimates of lapse rates when a large number of easy trials are included [<xref ref-type="bibr" rid="pcbi.1006829.ref118">118</xref>]. This was not the case in our study (which focused trials near threshold), nor in the general classification image literature, which tends to use near-threshold stimuli and does not typically estimate lapse rates [<xref ref-type="bibr" rid="pcbi.1006829.ref005">5</xref>].</p>
</sec>
</sec>
</sec>
<sec id="sec041">
<title>Appendix A</title>
<p>Here we derive a simple expression for the Bayes factor for two binomial models. We show using our formula that over large numbers of trials, a small difference in the accuracy of the two models can lead to a very strong preference for the more accurate model, as measured by the Bayes Factor.</p>
<p>Let <italic>M</italic><sub>1</sub>, <italic>M</italic><sub>2</sub> be two binomial models, each predicting for a fixed stimulus, positive response (<italic>r</italic> = 1) probabilities of <italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub> and negative response (<italic>r</italic> = −1) probabilities 1 − <italic>p</italic><sub>1</sub>, 1 − <italic>p</italic><sub>2</sub>. Assume that we perform <italic>n</italic> experimental trials and observe proportion <italic>p</italic>* positive responses and (1 − <italic>p</italic>*) negative responses. Let <italic>D</italic> = <italic>r</italic><sub>1</sub>, …, <italic>r</italic><sub><italic>n</italic></sub> denote the observer responses. The Bayes Factor is
<disp-formula id="pcbi.1006829.e019">
<alternatives>
<graphic id="pcbi.1006829.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e019" xlink:type="simple"/>
<mml:math display="block" id="M19">
<mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(A.1)</label>
</disp-formula>
and taking the natural log we obtain
<disp-formula id="pcbi.1006829.e020">
<alternatives>
<graphic id="pcbi.1006829.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e020" xlink:type="simple"/>
<mml:math display="block" id="M20">
<mml:mrow><mml:mrow><mml:mtext>ln</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mtext>ln</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mtext>ln</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(A.2)</label>
</disp-formula>
where <italic>L</italic><sub><italic>k</italic></sub> = <italic>p</italic>(<italic>D</italic>|<italic>M</italic><sub><italic>k</italic></sub>) for (<italic>k</italic> = 1,2). Expanding the log-likelihoods, we obtain
<disp-formula id="pcbi.1006829.e021">
<alternatives>
<graphic id="pcbi.1006829.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e021" xlink:type="simple"/>
<mml:math display="block" id="M21">
<mml:mrow><mml:mrow><mml:mtext>ln</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mrow><mml:mtext>ln</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mrow><mml:mtext>ln</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(A.3)</label>
</disp-formula>
where <italic>p</italic><sub><italic>k</italic></sub> = <italic>p</italic>(<italic>r</italic> = 1|<italic>M</italic><sub><italic>k</italic></sub>). Since there are <italic>np</italic>* trials with positive responses and <italic>n</italic>(1 − <italic>p</italic>*) trials with negative responses, we may simplify (<xref ref-type="disp-formula" rid="pcbi.1006829.e021">A.3</xref>) to
<disp-formula id="pcbi.1006829.e022">
<alternatives>
<graphic id="pcbi.1006829.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e022" xlink:type="simple"/>
<mml:math display="block" id="M22">
<mml:mrow><mml:mrow><mml:mtext>ln</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mspace width="2pt"/><mml:mrow><mml:mrow><mml:mtext>ln</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mtext>ln</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(A.4)</label>
</disp-formula></p>
<p>Noting that the bracketed term in (<xref ref-type="disp-formula" rid="pcbi.1006829.e022">A.4</xref>) is simply the negative cross-entropy between two binomial models, and using the fact that the cross-entropy <italic>C</italic>(<italic>p</italic>, <italic>q</italic>) relates to the entropy <italic>H</italic>(<italic>p</italic>) and the KL divergence <italic>D</italic><sub><italic>KL</italic></sub>(<italic>p</italic>||<italic>q</italic>) by the relation <italic>C</italic>(<italic>p</italic>, <italic>q</italic>) = <italic>H</italic>(<italic>p</italic>) + <italic>D</italic><sub><italic>KL</italic></sub>(<italic>p</italic>||<italic>q</italic>), we can re-write (<xref ref-type="disp-formula" rid="pcbi.1006829.e022">A.4</xref>) as
<disp-formula id="pcbi.1006829.e023">
<alternatives>
<graphic id="pcbi.1006829.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e023" xlink:type="simple"/>
<mml:math display="block" id="M23">
<mml:mrow><mml:mrow><mml:mtext>ln</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(A.5)</label>
</disp-formula></p>
<p>Plugging (<xref ref-type="disp-formula" rid="pcbi.1006829.e023">A.5</xref>) into (<xref ref-type="disp-formula" rid="pcbi.1006829.e020">A.2</xref>), the natural log of the Bayes factor becomes
<disp-formula id="pcbi.1006829.e024">
<alternatives>
<graphic id="pcbi.1006829.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e024" xlink:type="simple"/>
<mml:math display="block" id="M24">
<mml:mrow><mml:mrow><mml:mtext>ln</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(A.6)</label>
</disp-formula>
and we obtain the final expression
<disp-formula id="pcbi.1006829.e025">
<alternatives>
<graphic id="pcbi.1006829.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006829.e025" xlink:type="simple"/>
<mml:math display="block" id="M25">
<mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(A.7)</label>
</disp-formula></p>
<p>We see from the final result (<xref ref-type="disp-formula" rid="pcbi.1006829.e025">A.7</xref>) that the Bayes factor magnitude is exponential in the number of trials (<italic>n</italic>), meaning that small differences in model performance (measured by the difference in the KL divergences) over a large number of trials can lead to very large values for the Bayes factor. Similarly, we see from (<xref ref-type="disp-formula" rid="pcbi.1006829.e024">A.6</xref>) that the standard model selection criterion 2ln <italic>B</italic><sub>21</sub> [<xref ref-type="bibr" rid="pcbi.1006829.ref043">43</xref>] grows linearly as <italic>n</italic>, meaning that a small difference in performance may accumulate over trials to produce large values of this criterion.</p>
<p>Concretely, consider the case where <italic>p</italic>* = 0.75, <italic>p</italic><sub>1</sub> = 0.73, <italic>p</italic><sub>2</sub> = 0.74 for <italic>n</italic> = 5000 trials. Qualitatively, we see that both models are slightly inaccurate, with Model 2 agreeing with the observer more than Model 1, but with a difference of only 0.01. Applying formula (<xref ref-type="disp-formula" rid="pcbi.1006829.e024">A.6</xref>) gives us ln <italic>B</italic><sub>21</sub> = 3.8458 and therefore 2ln <italic>B</italic><sub>21</sub> = 7.6916, corresponding to a “strong” preference for Model 2 according to standard criteria [<xref ref-type="bibr" rid="pcbi.1006829.ref043">43</xref>]. Re-working this example with a slightly larger difference in performance (0.02) by setting <italic>p</italic><sub>1</sub> = 0.72 yields a “very strong” preference for Model 2 (2ln <italic>B</italic><sub>21</sub> = 20.2224), and a 2.46 × 10<sup>4</sup> fold preference for Model 2. This effect is even more pronounced for predictions of very high (or low) success rates: setting <italic>p</italic>* = 0.95, <italic>p</italic><sub>1</sub> = 0.92, <italic>p</italic><sub>2</sub> = 0.94 yields 2ln <italic>B</italic><sub>21</sub> = 60.4679, greatly exceeding the standard criterion (10) for “very strong” evidence in favor of Model 2.</p>
</sec>
<sec id="sec042">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006829.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Ridge regression hyper-parameter (λ) optimization for the ridge regression prior, for two observers (SRS, AMA) in Experiment 1-VAR.</title>
<p>Optimization was performed using <italic>k</italic>-fold cross-validation (<italic>k</italic> = 4). (<bold>a</bold>) Likelihood of training set (averaged over folds) for various values of λ for different down-sampling sizes and pooling rules. (<bold>b</bold>) Likelihood of validation set (averaged over folds) for various values of λ.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Same as <xref ref-type="supplementary-material" rid="pcbi.1006829.s001">S1 Fig</xref> but for Experiment 1-FIX (observers CJD, MAK).</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Smoothing hyper-parameter ω optimization for two observers (SRS, AMA) in Experiment 1-VAR with ridge regression hyper-parameter λ = 1.</title>
<p>Optimization was performed using <italic>k</italic>-fold cross validation (<italic>k</italic> = 4). (<bold>a</bold>) Likelihood of training set (averaged over folds) for various values of ω for several down-sampling sizes and pooling rules. (<bold>b</bold>) Likelihood of validation set (averaged over folds) for various values of ω.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Same as <xref ref-type="supplementary-material" rid="pcbi.1006829.s003">S3 Fig</xref> but for two observers (CJD, MAK) in Experiment 1-FIX.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Likelihood of training and validation data for models with ridge prior (R) and ridge + smooth priors (R + S) for two observers (SRS, AMA) in Experiment 1-VAR.</title>
<p>We see that for all three pooling rules (AVG, MAX, SUB), addition of the smoothing constraint decreases the fit to the training data (left), but improves generalization to the validation data (right).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Same as <xref ref-type="supplementary-material" rid="pcbi.1006829.s005">S5 Fig</xref> but for two observers (CJD, MAK) in Experiment 1-FIX.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s007" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Individual second-stage filter weights whose magnitudes are 1 (top) or 1.96 (bottom) standard errors above/below zero (shown in yellow), as determined by a bootstrap analysis.</title>
<p>Left columns show <italic>ridge</italic> prior, right show <italic>ridge + smooth</italic> prior. (<bold>a</bold>) Observer AMA in Experiment 1-VAR. (<bold>b</bold>) Observer JJF in Experiment 1-FIX.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s008" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>Second-stage filter weights for observer AMA in Experiment 1-VAR for three pooling rules.</title>
<p>Organization is as in <xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3</xref>, except that in the 1-D plots the thin dashed lines are fits to individual folds (k = 4), and the thick line is the average.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s009" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s009" xlink:type="simple">
<label>S9 Fig</label>
<caption>
<title>Same as <xref ref-type="supplementary-material" rid="pcbi.1006829.s008">S8 Fig</xref> but for observer JJF in Experiment 1-FIX.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s010" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s010" xlink:type="simple">
<label>S10 Fig</label>
<caption>
<title>Same as <xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3c</xref> but for ideal observer in Experiment 1-FIX.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s011" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s011" xlink:type="simple">
<label>S11 Fig</label>
<caption>
<title>Second-stage nonlinearity functions obtained from observers AMA and JJF in Experiment 1-VAR (FIX), for three pooling rules and both priors.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s012" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s012" xlink:type="simple">
<label>S12 Fig</label>
<caption>
<title>Predicted and measured overall performance for all three observers (1 = CJD, 2 = MAK, 3 = JJF) in Experiment 1-FIX, for all three pooling rules and for three downsampling sizes.</title>
<p><bold>(a)</bold> Performance of observers (blue) and models with both priors. <bold>(b)</bold> Difference between observer and model performance.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s013" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s013" xlink:type="simple">
<label>S13 Fig</label>
<caption>
<title>Analysis of model accuracy for Experiment 1-FIX, similar to that of <xref ref-type="fig" rid="pcbi.1006829.g007">Fig 7</xref>.</title>
<p>(<bold>a</bold>) Scatterplots of model vs. observer performance, averaged across observers, for all test folds (N = 18 folds, 6 per observer). We also show the stochastic AVG model with an internal noise parameter, β, optimized to fit observer performance (on the training set, which was not used for testing), denoted AVG-SOP, bottom left panel. Values of the noise parameter β for each observer are shown in <xref ref-type="supplementary-material" rid="pcbi.1006829.s027">S4 Table</xref>. (<bold>b</bold>) Difference between observer and model performance (observer—model) for each individual test fold (3 observers, 6 folds per observer) for all models shown in (a). Lines show 95% confidence intervals of the difference (binomial proportion difference test).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s014" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s014" xlink:type="simple">
<label>S14 Fig</label>
<caption>
<title>Overall performance for ideal observer in two modes of operation (DET—Square symbols, STO—Round symbols) compared with human observer performance for N = 6 observers from Experiment 1-FIX, tested with same (2048 micropattern) stimuli used to train ideal observer.</title>
<p><bold>(a)</bold> Proportion correct for models and observers (blue dots). <bold>(b)</bold> Difference between observer and model performance.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s015" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s015" xlink:type="simple">
<label>S15 Fig</label>
<caption>
<title>Same as <xref ref-type="supplementary-material" rid="pcbi.1006829.s007">S7 Fig</xref> but for Experiment 2-VAR (observer CJD).</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s016" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s016" xlink:type="simple">
<label>S16 Fig</label>
<caption>
<title>Same as <xref ref-type="fig" rid="pcbi.1006829.g011">Fig 11</xref> but for remaining observers in each condition of Experiment 2.</title>
<p><bold>(a)</bold> Experiment 2-VAR. <bold>(b)</bold> Experiment 2-FIX.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s017" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s017" xlink:type="simple">
<label>S17 Fig</label>
<caption>
<title>Ideal observer second-stage filter weights.</title>
<p>Organization as <xref ref-type="fig" rid="pcbi.1006829.g003">Fig 3c</xref>. <bold>(a)</bold> Experiment 2-VAR. <bold>(b)</bold> Experiment 2-FIX.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s018" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s018" xlink:type="simple">
<label>S18 Fig</label>
<caption>
<title>Psychometric functions for observers in Experiment 2-VAR, with model predictions for both priors.</title>
<p>Organization as in <xref ref-type="fig" rid="pcbi.1006829.g006">Fig 6</xref>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s019" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s019" xlink:type="simple">
<label>S19 Fig</label>
<caption>
<title>Same as <xref ref-type="supplementary-material" rid="pcbi.1006829.s001">S1 Fig</xref> for Experiment 3, for models 1 and 2 in <xref ref-type="fig" rid="pcbi.1006829.g012">Fig 12a</xref>.</title>
<p><italic>Top</italic>: Model 1, <italic>Bottom</italic>: Model 2. We see that values of the ridge regression hyper-parameter λ in the range 0.1–1 lead to the best generalization for each model. Model 1 generalized best with λ = 1, Model 2 with λ = 0.1.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s020" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s020" xlink:type="simple">
<label>S20 Fig</label>
<caption>
<title>Psychometric functions for models (red, magenta solid lines) and observers (blue, dashed lines show +/- 1.96 SEM) in Experiment 3.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s021" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s021" xlink:type="simple">
<label>S21 Fig</label>
<caption>
<title>Dependence of generalization performance on values of the hyper-parameter λ.</title>
<p>For each model we performed our generalization analysis (<xref ref-type="fig" rid="pcbi.1006829.g012">Fig 12c</xref>) using its optimal value (<xref ref-type="supplementary-material" rid="pcbi.1006829.s019">S19 Fig</xref>).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s022" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s022" xlink:type="simple">
<label>S22 Fig</label>
<caption>
<title>Filters recovered from fitting simulated data in Experiment 2 created by an ideal observer using two off-orientation filters aligned +/- 20 degrees from vertical (left).</title>
<p>As we see, our method accurately recovers the ground-truth filters, which do not resemble the filters obtained from observers in Experiment 2.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s023" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s023" xlink:type="simple">
<label>S23 Fig</label>
<caption>
<title>Same as <xref ref-type="fig" rid="pcbi.1006829.g007">Fig 7a</xref> but for MAX-DET.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s024" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s024" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Contrast modulation levels for three observers in Experiment 1-FIX.</title>
<p>Levels were chosen for each subject observer individually to attain near-threshold performance (approximately 80% correct).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s025" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s025" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Values of second-stage nonlinearity exponent obtained in Experiment 1-VAR, for the three pooling rules.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s026" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s026" xlink:type="simple">
<label>S3 Table</label>
<caption>
<title>Values of second-stage nonlinearity exponent obtained in Experiment 1-FIX.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s027" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s027" xlink:type="simple">
<label>S4 Table</label>
<caption>
<title>Optimal values of internal noise parameter β for all observers in Experiment 1-FIX (16x16-AVG).</title>
<p>Values were optimized on the training set, which was not used for model evaluation. The same value was used for both priors.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s028" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s028" xlink:type="simple">
<label>S5 Table</label>
<caption>
<title>Correlations (Spearman’s rho), and associated <italic>p</italic>-values, between second-stage nonlinearity exponent (α) and subject performance (N = 7) in Experiment 1-VAR for 16x16 model.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s029" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s029" xlink:type="simple">
<label>S6 Table</label>
<caption>
<title>Correlations (Pearson’s) between second-stage filter norms and subject performance (N = 7) in Experiment 1-VAR for 16x16 model.</title>
<p>For all correlations, <italic>p</italic> &lt; 0.001.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s030" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s030" xlink:type="simple">
<label>S7 Table</label>
<caption>
<title>Values of second-stage nonlinearity exponent (α) obtained in Experiment 2-FIX and Experiment 2-VAR.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006829.s031" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006829.s031" xlink:type="simple">
<label>S8 Table</label>
<caption>
<title>Second-stage nonlinearity exponents (α) for both models in Experiment 3.</title>
<p>Values are obtained by averages over N = 4 training sets.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank the FGCU undergraduates in the Computational Perception Lab for help with data collection, Wilson S. Geisler for code implementing decision-variable correlation, Elizabeth Zavitz for micropattern texture stimuli, and Fred Kingdom for helpful discussions and comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006829.ref001"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Marr, D. Vision. WH Freeman and Company; 1982.</mixed-citation></ref>
<ref id="pcbi.1006829.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahumada</surname> <given-names>AJ</given-names></name>. <article-title>Perceptual classification images from Vernier acuity masked by noise</article-title>. <source>Perception</source>. <year>1996</year>; <volume>25</volume>(<issue>suppl</issue>), <fpage>2</fpage>–<lpage>2</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahumada</surname> <given-names>AJ</given-names></name>. <article-title>Classification image weights and internal noise level estimation</article-title>. <source>Journal of Vision</source>. <year>2002</year>; <volume>2</volume>(<issue>1</issue>), <fpage>8</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neri</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Levi</surname> <given-names>DM</given-names></name>. <article-title>Receptive versus perceptive fields from the reverse-correlation viewpoint</article-title>. <source>Vision Research</source>. <year>2006</year>; <volume>46</volume>(<issue>16</issue>), <fpage>2465</fpage>–<lpage>2474</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2006.02.002" xlink:type="simple">10.1016/j.visres.2006.02.002</ext-link></comment> <object-id pub-id-type="pmid">16542700</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Murray</surname> <given-names>RF</given-names></name>. <article-title>Classification images: A review</article-title>. <source>Journal of Vision</source>. <year>2011</year>; <volume>11</volume>(<issue>5</issue>), <fpage>2</fpage>–<lpage>2</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/11.5.2" xlink:type="simple">10.1167/11.5.2</ext-link></comment> <object-id pub-id-type="pmid">21536726</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeAngelis</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Ohzawa</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Freeman</surname> <given-names>RD</given-names></name>. <article-title>Spatiotemporal organization of simple-cell receptive fields in the cat’s striate cortex. II. Linearity of temporal and spatial summation</article-title>. <source>Journal of Neurophysiology</source>. <year>1993</year>; <volume>69</volume>(<issue>4</issue>), <fpage>1118</fpage>–<lpage>1135</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1993.69.4.1118" xlink:type="simple">10.1152/jn.1993.69.4.1118</ext-link></comment> <object-id pub-id-type="pmid">8492152</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chichilnisky</surname> <given-names>EJ</given-names></name>. <article-title>A simple white noise analysis of neuronal light responses</article-title>. <source>Network: Computation in Neural Systems</source>. <year>2001</year>; <volume>12</volume>(<issue>2</issue>), <fpage>199</fpage>–<lpage>213</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbey</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>. <article-title>Classification image analysis: Estimation and statistical inference for two-alternative forced-choice experiments</article-title>. <source>Journal of Vision</source>. <year>2002</year>; <volume>2</volume>(<issue>1</issue>), <fpage>5</fpage>–<lpage>5</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Shimozaki</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Abbey</surname> <given-names>CK</given-names></name>. <article-title>The footprints of visual attention in the Posner cueing paradigm revealed by classification images</article-title>. <source>Journal of Vision</source>. <year>2002</year>; <volume>2</volume>(<issue>1</issue>), <fpage>3</fpage>–<lpage>3</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Levi</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Klein</surname> <given-names>SA</given-names></name>. <article-title>Perceptual learning improves efficiency by re-tuning the decision ‘template’ for position discrimination</article-title>. <source>Nature Neuroscience</source>. <year>2004</year>; <volume>7</volume>(<issue>2</issue>), <fpage>178</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1183" xlink:type="simple">10.1038/nn1183</ext-link></comment> <object-id pub-id-type="pmid">14730311</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morgenstern</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Elder</surname> <given-names>JH</given-names></name>. <article-title>Local visual energy mechanisms revealed by detection of global patterns</article-title>. <source>Journal of Neuroscience</source>. <year>2012</year>; <volume>32</volume>(<issue>11</issue>), <fpage>3679</fpage>–<lpage>3696</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3881-11.2012" xlink:type="simple">10.1523/JNEUROSCI.3881-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22423090</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McIlhagga</surname> <given-names>WH</given-names></name>, <name name-style="western"><surname>May</surname> <given-names>KA</given-names></name>. <article-title>Optimal edge filters explain human blur detection</article-title>. <source>Journal of Vision</source>. <year>2012</year>; <volume>12</volume>(<issue>10</issue>), <fpage>9</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">22984222</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kurki</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Saarinen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hyvärinen</surname> <given-names>A</given-names></name>. <article-title>Investigating shape perception by classification images</article-title>. <source>Journal of Vision</source>. <year>2014</year>; <volume>14</volume>(<issue>12</issue>), <fpage>24</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/14.12.24" xlink:type="simple">10.1167/14.12.24</ext-link></comment> <object-id pub-id-type="pmid">25342541</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schofield</surname> <given-names>AJ</given-names></name>. <article-title>What does second-order vision see in an image?</article-title>. <source>Perception</source>. <year>2000</year>; <volume>29</volume>(<issue>9</issue>), <fpage>1071</fpage>–<lpage>1086</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1068/p2913" xlink:type="simple">10.1068/p2913</ext-link></comment> <object-id pub-id-type="pmid">11144820</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Konishi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Yuille</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Coughlan</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>SC</given-names></name>. <article-title>Statistical edge detection: Learning and evaluating edge cues</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2003</year>; <volume>25</volume>(<issue>1</issue>), <fpage>57</fpage>–<lpage>74</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name>. (<year>2004</year>). <article-title>First-and second-order information in natural images: a filter-based approach to image statistics</article-title>. <source>JOSA A</source>. 2004; <volume>21</volume>(<issue>6</issue>), <fpage>913</fpage>–<lpage>925</lpage>. <object-id pub-id-type="pmid">15191171</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Martin</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Fowlkes</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Malik</surname> <given-names>J</given-names></name>. <article-title>Learning to detect natural image boundaries using local brightness, color, and texture cues</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2004</year>; <volume>26</volume>(<issue>5</issue>), <fpage>530</fpage>–<lpage>549</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TPAMI.2004.1273918" xlink:type="simple">10.1109/TPAMI.2004.1273918</ext-link></comment> <object-id pub-id-type="pmid">15460277</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiMattina</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fox</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>. <article-title>Detecting natural occlusion boundaries using local cues</article-title>. <source>Journal of Vision</source>. <year>2012</year>; <volume>12</volume>(<issue>13</issue>), <fpage>15</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/12.13.15" xlink:type="simple">10.1167/12.13.15</ext-link></comment> <object-id pub-id-type="pmid">23255731</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name>. <article-title>Central neural mechanisms for detecting second-order motion</article-title>. <source>Current Opinion in Neurobiology</source>. <year>1999</year>; <volume>9</volume>(<issue>4</issue>), <fpage>461</fpage>–<lpage>466</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0959-4388(99)80069-5" xlink:type="simple">10.1016/S0959-4388(99)80069-5</ext-link></comment> <object-id pub-id-type="pmid">10448168</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref020"><label>20</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Graham</surname> <given-names>N</given-names></name>. <chapter-title>Visual Perception of Texture</chapter-title>. In: <name name-style="western"><surname>Chalupa</surname> <given-names>L.M.</given-names></name> and <name name-style="western"><surname>Werner</surname> <given-names>J.S.</given-names></name> (Eds.) <source>The Visual Neurosciences</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>2004</year>, <fpage>1106</fpage>–<lpage>1118</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willmore</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>Prenger</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Neural representation of natural images in visual area V2</article-title>. <source>Journal of Neuroscience</source>. <year>2010</year>; <volume>30</volume>(<issue>6</issue>), <fpage>2102</fpage>–<lpage>2114</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4099-09.2010" xlink:type="simple">10.1523/JNEUROSCI.4099-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20147538</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vintch</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>A convolutional subunit model for neuronal responses in macaque V1</article-title>. <source>Journal of Neuroscience</source>. <year>2015</year>; <volume>35</volume>(<issue>44</issue>), <fpage>14829</fpage>–<lpage>14841</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2815-13.2015" xlink:type="simple">10.1523/JNEUROSCI.2815-13.2015</ext-link></comment> <object-id pub-id-type="pmid">26538653</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harper</surname> <given-names>NS</given-names></name>, <name name-style="western"><surname>Schoppe</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Willmore</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>Cui</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Schnupp</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>. <article-title>Network receptive field modeling reveals extensive integration and multi-feature selectivity in auditory cortical neurons</article-title>. <source>PLoS Computational Biology</source>. <year>2016</year>; <volume>12</volume>(<issue>11</issue>), <fpage>e1005113</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005113" xlink:type="simple">10.1371/journal.pcbi.1005113</ext-link></comment> <object-id pub-id-type="pmid">27835647</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rowekamp</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name>. <article-title>Cross-orientation suppression in visual area V2</article-title>. <source>Nature Communications</source>. <year>2017</year>; <volume>8</volume>, <fpage>15739</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms15739" xlink:type="simple">10.1038/ncomms15739</ext-link></comment> <object-id pub-id-type="pmid">28593941</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Malik</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Perona</surname> <given-names>P</given-names></name>. <article-title>Preattentive texture discrimination with early vision mechanisms</article-title>. <source>JOSA A</source>. <year>1990</year>; <volume>7</volume>(<issue>5</issue>), <fpage>923</fpage>–<lpage>932</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref026"><label>26</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bergen</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>. <chapter-title>Computational modeling of visual texture segregation</chapter-title>. In: <name name-style="western"><surname>Landy</surname> <given-names>M.S.</given-names></name> and <name name-style="western"><surname>Movshon</surname> <given-names>J.A.</given-names></name>, Editors. <source>Computational Models of Visual Processing</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1991</year>. pp. <fpage>253</fpage>–<lpage>271</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graham</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sutter</surname> <given-names>A</given-names></name>. <article-title>Nonlinear processes in spatial-frequency channel models of perceived texture segregation: Effects of sign and amount of contrast</article-title>. <source>Vision Research</source>. <year>1992</year>; <volume>32</volume>(<issue>4</issue>), <fpage>719</fpage>–<lpage>743</lpage>. <object-id pub-id-type="pmid">1413556</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zavitz</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name>. <article-title>Texture sparseness, but not local phase structure, impairs second-order segmentation</article-title>. <source>Vision Research</source>. <year>2013</year>; <volume>91</volume>, <fpage>45</fpage>–<lpage>55</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2013.07.018" xlink:type="simple">10.1016/j.visres.2013.07.018</ext-link></comment> <object-id pub-id-type="pmid">23942289</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Westrick</surname> <given-names>ZM</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>. <article-title>Pooling of first-order inputs in second-order vision</article-title>. <source>Vision Research</source>. <year>2013</year>; <volume>91</volume>, <fpage>108</fpage>–<lpage>117</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2013.08.005" xlink:type="simple">10.1016/j.visres.2013.08.005</ext-link></comment> <object-id pub-id-type="pmid">23994031</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dakin</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Mareschal</surname> <given-names>I</given-names></name>. <article-title>Sensitivity to contrast modulation depends on carrier spatial frequency and orientation</article-title>. <source>Vision Research</source>. <year>2000</year>; <volume>40</volume>(<issue>3</issue>), <fpage>311</fpage>–<lpage>329</lpage>. <object-id pub-id-type="pmid">10793904</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schofield</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Georgeson</surname> <given-names>MA</given-names></name>. <article-title>Sensitivity to modulations of luminance and contrast in visual white noise: Separate mechanisms with similar behaviour</article-title>. <source>Vision Research</source>. <year>1999</year>; <volume>39</volume>(<issue>16</issue>), <fpage>2697</fpage>–<lpage>2716</lpage>. <object-id pub-id-type="pmid">10492831</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schofield</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Georgeson</surname> <given-names>MA</given-names></name>. <article-title>Sensitivity to contrast modulation: The spatial frequency dependence of second-order vision</article-title>. <source>Vision Research</source>. <year>2003</year>; <volume>43</volume>(<issue>3</issue>), <fpage>243</fpage>–<lpage>259</lpage>. <object-id pub-id-type="pmid">12535984</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ledgeway</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>AT</given-names></name>. <article-title>Evidence for separate motion-detecting mechanisms for first-and second-order motion in human vision</article-title>. <source>Vision Research</source>. <year>1994</year>; <volume>34</volume>(<issue>20</issue>), <fpage>2727</fpage>–<lpage>2740</lpage>. <object-id pub-id-type="pmid">7975310</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ellemberg</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Allen</surname> <given-names>HA</given-names></name>, <name name-style="western"><surname>Hess</surname> <given-names>RF</given-names></name>. <article-title>Second-order spatial frequency and orientation channels in human vision</article-title>. <source>Vision Research</source>. <year>2006</year>; <volume>46</volume>(<issue>17</issue>), <fpage>2798</fpage>–<lpage>2803</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2006.01.028" xlink:type="simple">10.1016/j.visres.2006.01.028</ext-link></comment> <object-id pub-id-type="pmid">16542701</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mareschal</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name>. <article-title>A cortical locus for the processing of contrast-defined contours</article-title>. <source>Nature Neuroscience</source>. <year>1998</year>; <volume>1</volume>(<issue>2</issue>), <fpage>150</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/401" xlink:type="simple">10.1038/401</ext-link></comment> <object-id pub-id-type="pmid">10195131</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Ohzawa</surname> <given-names>I</given-names></name>. <article-title>Neural basis for stereopsis from second-order contrast cues</article-title>. <source>Journal of Neuroscience</source>. <year>2006</year>; <volume>26</volume>(<issue>16</issue>), <fpage>4370</fpage>–<lpage>4382</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4379-05.2006" xlink:type="simple">10.1523/JNEUROSCI.4379-05.2006</ext-link></comment> <object-id pub-id-type="pmid">16624957</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rosenberg</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Husson</surname> <given-names>TR</given-names></name>, <name name-style="western"><surname>Issa</surname> <given-names>NP</given-names></name>. <article-title>Subcortical representation of non-Fourier image features</article-title>. <source>Journal of Neuroscience</source>. <year>2010</year>; <volume>30</volume>(<issue>6</issue>), <fpage>1985</fpage>–<lpage>1993</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3258-09.2010" xlink:type="simple">10.1523/JNEUROSCI.3258-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20147527</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Yao</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Yuan</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Talebi</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tan</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Form-cue invariant second-order neuronal responses to contrast modulation in primate area V2</article-title>. <source>Journal of Neuroscience</source>. <year>2014</year>; <volume>34</volume>(<issue>36</issue>), <fpage>12081</fpage>–<lpage>12092</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0211-14.2014" xlink:type="simple">10.1523/JNEUROSCI.0211-14.2014</ext-link></comment> <object-id pub-id-type="pmid">25186753</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kingdom</surname> <given-names>FAA</given-names></name>, <name name-style="western"><surname>Prins</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hayes</surname> <given-names>A</given-names></name>. <article-title>Mechanism independence for texture-modulation detection is consistent with a filter-rectify-filter mechanism</article-title>. <source>Visual Neuroscience</source>. <year>2003</year>; <volume>20</volume>(<issue>1</issue>), <fpage>65</fpage>–<lpage>76</lpage>. <object-id pub-id-type="pmid">12699084</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Motoyoshi</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Nishida</surname> <given-names>SY</given-names></name>. <article-title>Cross-orientation summation in texture segregation</article-title>. <source>Vision Research</source>. <year>2004</year>; <volume>44</volume>(<issue>22</issue>), <fpage>2567</fpage>–<lpage>2576</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2004.05.024" xlink:type="simple">10.1016/j.visres.2004.05.024</ext-link></comment> <object-id pub-id-type="pmid">15358072</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Motoyoshi</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Kingdom</surname> <given-names>FAA</given-names></name>. <article-title>Differential roles of contrast polarity reveal two streams of second-order visual processing</article-title>. <source>Vision Research</source>. <year>2007</year>; <volume>47</volume>(<issue>15</issue>), <fpage>2047</fpage>–<lpage>2054</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2007.03.015" xlink:type="simple">10.1016/j.visres.2007.03.015</ext-link></comment> <object-id pub-id-type="pmid">17555787</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwarz</surname> <given-names>G</given-names></name>. <article-title>Estimating the dimension of a model</article-title>. <source>The Annals of Statistics</source>. <year>1978</year>; <volume>6</volume>(<issue>2</issue>), <fpage>461</fpage>–<lpage>464</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kass</surname> <given-names>RE</given-names></name>, <article-title>Raftery AE. Bayes factors</article-title>. <source>Journal of the American Statistical Association</source>. <year>1995</year>; <volume>90</volume>(<issue>430</issue>), <fpage>773</fpage>–<lpage>795</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref044"><label>44</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bishop</surname> <given-names>CM</given-names></name>. <source>Pattern Recognition and Machine Learning</source>. <publisher-loc>New York</publisher-loc>: <collab>Springer</collab>; <year>2006</year>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Allard</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Faubert</surname> <given-names>J</given-names></name>. <article-title>Double dissociation between first-and second-order processing</article-title>. <source>Vision Research</source>. <year>2007</year>; <volume>47</volume>(<issue>9</issue>), <fpage>1129</fpage>–<lpage>1141</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2007.01.010" xlink:type="simple">10.1016/j.visres.2007.01.010</ext-link></comment> <object-id pub-id-type="pmid">17363024</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saarela</surname> <given-names>TP</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>. <article-title>Combination of texture and color cues in visual segmentation</article-title>. <source>Vision Research</source>. <year>2012</year>; <volume>58</volume>, <fpage>59</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2012.01.019" xlink:type="simple">10.1016/j.visres.2012.01.019</ext-link></comment> <object-id pub-id-type="pmid">22387319</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hutchinson</surname> <given-names>CV</given-names></name>, <name name-style="western"><surname>Ledgeway</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name>. <article-title>Phase-dependent interactions in visual cortex to combinations of first-and second-order stimuli</article-title>. <source>Journal of Neuroscience</source>. <year>2016</year>; <volume>36</volume>(<issue>49</issue>), <fpage>12328</fpage>–<lpage>12337</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1350-16.2016" xlink:type="simple">10.1523/JNEUROSCI.1350-16.2016</ext-link></comment> <object-id pub-id-type="pmid">27927953</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McDermott</surname> <given-names>J</given-names></name>. <article-title>Psychophysics with junctions in real images</article-title>. <source>Perception</source>. <year>2004</year>; <volume>33</volume>(<issue>9</issue>), <fpage>1101</fpage>–<lpage>1127</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1068/p5265" xlink:type="simple">10.1068/p5265</ext-link></comment> <object-id pub-id-type="pmid">15560510</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Adelson</surname> <given-names>EH</given-names></name>, <name name-style="western"><surname>Bergen</surname> <given-names>JR</given-names></name>. <article-title>Spatiotemporal energy models for the perception of motion</article-title>. <source>JOSA A</source>. <year>1985</year>; <volume>2</volume>(<issue>2</issue>), <fpage>284</fpage>–<lpage>299</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref050"><label>50</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Goodfellow</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Courville</surname> <given-names>A</given-names></name>. <source>Deep Learning</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>2016</year>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mineault</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Barthelme</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pack</surname> <given-names>CC</given-names></name>. <article-title>Improved classification images with sparse priors in a smooth basis</article-title>. <source>Journal of Vision</source>. <year>2009</year>; <volume>9</volume>(<issue>10</issue>), <fpage>17</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/9.10.17" xlink:type="simple">10.1167/9.10.17</ext-link></comment> <object-id pub-id-type="pmid">19810798</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname> <given-names>MCK</given-names></name>, <name name-style="western"><surname>David</surname> <given-names>SV</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Complete functional characterization of sensory neurons by system identification</article-title>. <source>Annu. Rev. Neurosci</source>. <year>2006</year>; <volume>29</volume>, <fpage>477</fpage>–<lpage>505</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.neuro.29.051605.113024" xlink:type="simple">10.1146/annurev.neuro.29.051605.113024</ext-link></comment> <object-id pub-id-type="pmid">16776594</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Park</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>. <article-title>Receptive field inference with localized priors</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>.<issue>10</issue> (<issue>2011</issue>): <fpage>e1002219</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002219" xlink:type="simple">10.1371/journal.pcbi.1002219</ext-link></comment> <object-id pub-id-type="pmid">22046110</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoerl</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Kennard</surname> <given-names>RW</given-names></name>. <article-title>Ridge regression: Biased estimation for nonorthogonal problems</article-title>. <source>Technometrics</source>. <year>1970</year>; <volume>12</volume>(<issue>1</issue>), <fpage>55</fpage>–<lpage>67</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nothdurft</surname> <given-names>HC</given-names></name>. <article-title>Sensitivity for structure gradient in texture discrimination tasks</article-title>. <source>Vision Research</source>. <year>1985</year>; <volume>25</volume>(<issue>12</issue>), <fpage>1957</fpage>–<lpage>1968</lpage>. <object-id pub-id-type="pmid">3832621</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gurnsey</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Laundry</surname> <given-names>DS</given-names></name>. <article-title>Texture discrimination with and without abrupt texture gradients</article-title>. <source>Canadian Journal of Psychology/Revue Canadienne de Psychologie</source>. <year>1992</year>; <volume>46</volume>(<issue>2</issue>), <fpage>306</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wolfson</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>. <article-title>Examining edge-and region-based texture analysis mechanisms</article-title>. <source>Vision Research</source>. <year>1998</year>; <volume>38</volume>(<issue>3</issue>), <fpage>439</fpage>–<lpage>446</lpage>. <object-id pub-id-type="pmid">9536367</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neri</surname> <given-names>P</given-names></name>. <article-title>How inherently noisy is human sensory processing?</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2010</year>; <volume>17</volume>(<issue>6</issue>), <fpage>802</fpage>–<lpage>808</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hasan</surname> <given-names>BAS</given-names></name>, <name name-style="western"><surname>Joosten</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Neri</surname> <given-names>P</given-names></name>. <article-title>Estimation of internal noise using double passes: Does it matter how the second pass is delivered?</article-title>. <source>Vision Research</source>. <year>2012</year>; <volume>69</volume>, <fpage>1</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2012.06.014" xlink:type="simple">10.1016/j.visres.2012.06.014</ext-link></comment> <object-id pub-id-type="pmid">22835631</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sebastian</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Decision-variable correlation</article-title>. <source>Journal of Vision</source>. <year>2018</year>; <volume>18</volume>(<issue>4</issue>), <fpage>3</fpage>–3. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/18.4.3" xlink:type="simple">10.1167/18.4.3</ext-link></comment> <object-id pub-id-type="pmid">29614152</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Regan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Beverley</surname> <given-names>KI</given-names></name>. <article-title>Post-adaptation orientation discrimination</article-title>. <source>JOSA A</source>. <year>1985</year>; <volume>2</volume>(<issue>2</issue>), <fpage>147</fpage>–<lpage>155</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Motoyoshi</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Kingdom</surname> <given-names>FAA</given-names></name>. <article-title>Orientation opponency in human vision revealed by energy-frequency analysis</article-title>. <source>Vision Research</source>. <year>2003</year>; <volume>43</volume>(<issue>21</issue>), <fpage>2197</fpage>–<lpage>2205</lpage>. <object-id pub-id-type="pmid">12885374</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname> <given-names>HR</given-names></name>. <article-title>Nonlinear processes in visual pattern discrimination</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1993</year>; <volume>90</volume>(<issue>21</issue>), <fpage>9785</fpage>–<lpage>9790</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Oruç</surname> <given-names>I</given-names></name>. <article-title>Properties of second-order spatial frequency channels</article-title>. <source>Vision Research</source>. <year>2002</year>; <volume>42</volume>(<issue>19</issue>), <fpage>2311</fpage>–<lpage>2329</lpage>. <object-id pub-id-type="pmid">12220586</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bovik</surname> <given-names>AC.</given-names></name>, <name name-style="western"><surname>Clark</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Multichannel texture analysis using localized spatial filters</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>1990</year>; <volume>12</volume>(<issue>1</issue>), <fpage>55</fpage>–<lpage>73</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutter</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Graham</surname> <given-names>N</given-names></name>. <article-title>Contrast and spatial variables in texture segregation: Testing a simple spatial-frequency channels model</article-title>. <source>Perception &amp; Psychophysics</source>. <year>1989</year>; <volume>46</volume>(<issue>4</issue>), <fpage>312</fpage>–<lpage>332</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knoblauch</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>. <article-title>Classification images estimated by generalized additive models</article-title>. <source>Journal of Vision</source>. <year>2008</year>; <volume>8</volume>(<issue>6</issue>), <fpage>344</fpage>–<lpage>344</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knoblauch</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>. <article-title>Estimating classification images with generalized linear and additive models</article-title>. <source>Journal of Vision</source>. <year>2008</year>; <volume>8</volume>(<issue>16</issue>), <fpage>10</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/8.16.10" xlink:type="simple">10.1167/8.16.10</ext-link></comment> <object-id pub-id-type="pmid">19146276</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name>, <name name-style="western"><surname>Mareschal</surname> <given-names>I</given-names></name>. (<year>2001</year>). <article-title>Processing of second-order stimuli in the visual cortex</article-title>. <source>Progress in Brain Research</source>. 2001; <volume>134</volume>, <fpage>171</fpage>–<lpage>191</lpage>. <object-id pub-id-type="pmid">11702543</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>Normalization of cell responses in cat striate cortex</article-title>. <source>Visual Neuroscience</source>. <year>1992</year>; <volume>9</volume>(<issue>2</issue>), <fpage>181</fpage>–<lpage>197</lpage>. <object-id pub-id-type="pmid">1504027</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hansen</surname> <given-names>BC</given-names></name>, <name name-style="western"><surname>Hess</surname> <given-names>RF</given-names></name>. <article-title>The role of spatial phase in texture segmentation and contour integration</article-title>. <source>Journal of Vision</source>. <year>2006</year>; <volume>6</volume>(<issue>5</issue>), <fpage>5</fpage>–<lpage>5</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubel</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Livingstone</surname> <given-names>MS</given-names></name>. <article-title>Complex–unoriented cells in a subregion of primate area 18</article-title>. <source>Nature</source>. <year>1985</year>; <volume>315</volume>(<issue>6017</issue>), <fpage>325</fpage>. <object-id pub-id-type="pmid">2987703</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ringach</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Shapley</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Hawken</surname> <given-names>MJ</given-names></name>. <article-title>Orientation selectivity in macaque V1: diversity and laminar dependence</article-title>. <source>Journal of Neuroscience</source>. <year>2002</year>; <volume>22</volume>(<issue>13</issue>), <fpage>5639</fpage>–<lpage>5651</lpage>. <object-id pub-id-type="pmid">12097515</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Talebi</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name>. <article-title>Natural versus synthetic stimuli for estimating receptive field models: a comparison of predictive robustness</article-title>. <source>Journal of Neuroscience</source>. <year>2012</year>; <volume>32</volume>(<issue>5</issue>), <fpage>1560</fpage>–<lpage>1576</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4661-12.2012" xlink:type="simple">10.1523/JNEUROSCI.4661-12.2012</ext-link></comment> <object-id pub-id-type="pmid">22302799</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graham</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Sutter</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Venkatesan</surname> <given-names>C</given-names></name>. <article-title>Spatial-frequency-and orientation-selectivity of simple and complex channels in region segregation</article-title>. <source>Vision Research</source>. <year>1993</year>; <volume>33</volume>(<issue>14</issue>), <fpage>1893</fpage>–<lpage>1911</lpage>. <object-id pub-id-type="pmid">8249309</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Prins</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kingdom</surname> <given-names>FAA</given-names></name>. <article-title>Orientation-and frequency modulated textures at low depths of modulation are processed by off-orientation and off-frequency texture mechanisms</article-title>. <source>Vision Research</source>. <year>2002</year>; <volume>42</volume>, <fpage>705</fpage>–<lpage>713</lpage>. <object-id pub-id-type="pmid">11888536</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name>. <article-title>Functional organization of envelope-responsive neurons in early visual cortex: Organization of carrier tuning properties</article-title>. <source>Journal of Neuroscience</source>. <year>2012</year>; <volume>32</volume>(<issue>22</issue>), <fpage>7538</fpage>–<lpage>7549</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4662-11.2012" xlink:type="simple">10.1523/JNEUROSCI.4662-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22649232</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rivest</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Cavanagh</surname> <given-names>P</given-names></name>. <article-title>Localizing contours defined by more than one attribute</article-title>. <source>Vision Research</source>. <year>1996</year>; <volume>36</volume>(<issue>1</issue>), <fpage>53</fpage>–<lpage>66</lpage>. <object-id pub-id-type="pmid">8746242</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Freeman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ziemba</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>D.</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA.</given-names></name> (<year>2013</year>). <article-title>A functional and perceptual signature of the second visual area in primates</article-title>. <source>Nature Neuroscience</source>. 2013; <volume>16</volume>(<issue>7</issue>), <fpage>974</fpage>–<lpage>981</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3402" xlink:type="simple">10.1038/nn.3402</ext-link></comment> <object-id pub-id-type="pmid">23685719</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goda</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Tachibana</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Okazawa</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Komatsu</surname> <given-names>H</given-names></name>. <article-title>Representation of the material properties of objects in the visual cortex of nonhuman primates</article-title>. <source>Journal of Neuroscience</source>. <year>2014</year>; <volume>34</volume>(<issue>7</issue>), <fpage>2660</fpage>–<lpage>2673</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2593-13.2014" xlink:type="simple">10.1523/JNEUROSCI.2593-13.2014</ext-link></comment> <object-id pub-id-type="pmid">24523555</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Okazawa</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Tajima</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Komatsu</surname> <given-names>H</given-names></name>. <article-title>Image statistics underlying natural texture selectivity of neurons in macaque V4</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2015</year>; <volume>112</volume>(<issue>4</issue>), <fpage>E351</fpage>–<lpage>E360</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Okazawa</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Tajima</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Komatsu</surname> <given-names>H</given-names></name>. <article-title>Gradual development of visual texture-selective properties between macaque areas V2 and V4</article-title>. <source>Cerebral Cortex</source>. <year>2016</year>; <volume>27</volume>(<issue>10</issue>), <fpage>4867</fpage>–<lpage>4880</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ziemba</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Freeman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Selectivity and tolerance for visual texture in macaque V2</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2016</year>; <volume>113</volume>(<issue>22</issue>), <fpage>E3140</fpage>–<lpage>E3149</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kohler</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Clarke</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Yakovleva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Norcia</surname> <given-names>AM</given-names></name>. <article-title>Representation of maximally regular textures in human visual cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2016</year>; <volume>36</volume>(<issue>3</issue>), <fpage>714</fpage>–<lpage>729</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2962-15.2016" xlink:type="simple">10.1523/JNEUROSCI.2962-15.2016</ext-link></comment> <object-id pub-id-type="pmid">26791203</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Seibert</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year>; <volume>111</volume>(<issue>23</issue>), <fpage>8619</fpage>–<lpage>8624</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title>. <source>PLoS Computational Biology</source>. <year>2014</year>; <volume>10</volume>(<issue>11</issue>), <fpage>e1003915</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003915" xlink:type="simple">10.1371/journal.pcbi.1003915</ext-link></comment> <object-id pub-id-type="pmid">25375136</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref087"><label>87</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Zeiler</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Fergus</surname> <given-names>R</given-names></name>. <chapter-title>Visualizing and understanding convolutional networks</chapter-title>. In <source><italic>European Conference on Computer Vision</italic></source>. <publisher-name>Springer</publisher-name>; <year>2014</year>, pp <fpage>818</fpage>–<lpage>833</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nature Neuroscience</source>. <year>2016</year>; <volume>19</volume>(<issue>3</issue>), <fpage>356</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4244" xlink:type="simple">10.1038/nn.4244</ext-link></comment> <object-id pub-id-type="pmid">26906502</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baker</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Meese</surname> <given-names>TS</given-names></name>. <article-title>Measuring the spatial extent of texture pooling using reverse correlation</article-title>. <source>Vision Research</source>. <year>2014</year>; <volume>97</volume>, <fpage>52</fpage>–<lpage>58</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2014.02.004" xlink:type="simple">10.1016/j.visres.2014.02.004</ext-link></comment> <object-id pub-id-type="pmid">24576749</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zavitz</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name>. <article-title>Higher order image structure enables boundary segmentation in the absence of luminance or contrast cues</article-title>. <source>Journal of Vision</source>. <year>2014</year>; <volume>14</volume>(<issue>4</issue>), <fpage>14</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/14.4.14" xlink:type="simple">10.1167/14.4.14</ext-link></comment> <object-id pub-id-type="pmid">24762950</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gharat</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name>. <article-title>Nonlinear Y-like receptive fields in the early visual cortex: An intermediate stage for building cue-invariant receptive fields from subcortical Y cells</article-title>. <source>Journal of Neuroscience</source>. <year>2016</year>; <volume>37</volume>(<issue>4</issue>), <fpage>998</fpage>–<lpage>1013</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hanazawa</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Komatsu</surname> <given-names>H</given-names></name>. <article-title>Influence of the direction of elemental luminance gradients on the responses of V4 cells to textured surfaces</article-title>. <source>Journal of Neuroscience</source>. <year>2001</year>; <volume>21</volume>(<issue>12</issue>), <fpage>4490</fpage>–<lpage>4497</lpage>. <object-id pub-id-type="pmid">11404436</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hegdé</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Van Essen</surname> <given-names>DC</given-names></name>. <article-title>Selectivity for complex shapes in primate visual area V2</article-title>. <source>Journal of Neuroscience</source>. <year>2000</year>; <volume>20</volume>(<issue>5</issue>), <fpage>RC61</fpage>–<lpage>RC61</lpage>. <object-id pub-id-type="pmid">10684908</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anzai</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Peng</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Van Essen</surname> <given-names>DC</given-names></name>. <article-title>Neurons in monkey visual area V2 encode combinations of orientations</article-title>. <source>Nature Neuroscience</source>. <year>2007</year>; <volume>10</volume>(<issue>10</issue>), <fpage>1313</fpage>–<lpage>1321</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1975" xlink:type="simple">10.1038/nn1975</ext-link></comment> <object-id pub-id-type="pmid">17873872</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref095"><label>95</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>. <year>2002</year>; <volume>415</volume>(<issue>6870</issue>), <fpage>429</fpage>–<lpage>433</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/415429a" xlink:type="simple">10.1038/415429a</ext-link></comment> <object-id pub-id-type="pmid">11807554</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref096"><label>96</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbey</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>. <article-title>Classification images for detection, contrast discrimination, and identification tasks with a common ideal observer</article-title>. <source>Journal of Vision</source>. <year>2006</year>; <volume>6</volume>(<issue>4</issue>), <fpage>4</fpage>–<lpage>4</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref097"><label>97</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McIlhagga</surname> <given-names>WH</given-names></name>. <article-title>Estimates of edge detection filters in human vision</article-title>. <source>Vision Research</source>. <year>2018</year>; <volume>153</volume>, <fpage>30</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2018.09.007" xlink:type="simple">10.1016/j.visres.2018.09.007</ext-link></comment> <object-id pub-id-type="pmid">30291920</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref098"><label>98</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Contributions of ideal observer theory to vision research</article-title>. <source>Vision Research</source>. <year>2011</year>; <volume>51</volume>(<issue>7</issue>), <fpage>771</fpage>–<lpage>781</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2010.09.027" xlink:type="simple">10.1016/j.visres.2010.09.027</ext-link></comment> <object-id pub-id-type="pmid">20920517</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref099"><label>99</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Solomon</surname> <given-names>JA</given-names></name>. <article-title>Noise reveals visual mechanisms of detection and discrimination</article-title>. <source>Journal of Vision</source>. <year>2002</year>; <volume>2</volume>(<issue>1</issue>), <fpage>7</fpage>–<lpage>7</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref100"><label>100</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burge</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Optimal disparity estimation in natural stereo images</article-title>. <source>Journal of Vision</source>. <year>2014</year>; <volume>14</volume>(<issue>2</issue>), <fpage>1</fpage>–<lpage>1</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/14.2.1" xlink:type="simple">10.1167/14.2.1</ext-link></comment> <object-id pub-id-type="pmid">24492596</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref101"><label>101</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burge</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Optimal speed estimation in natural image movies predicts human performance</article-title>. <source>Nature Communications</source>. <year>2015</year>; <volume>6</volume>, <fpage>7900</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms8900" xlink:type="simple">10.1038/ncomms8900</ext-link></comment> <object-id pub-id-type="pmid">26238697</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref102"><label>102</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gold</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>RF</given-names></name>, <name name-style="western"><surname>Bennett</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Sekuler</surname> <given-names>AB</given-names></name>. <article-title>Deriving behavioural receptive fields for visually completed contours</article-title>. <source>Current Biology</source>. <year>2000</year>; <volume>10</volume>(<issue>11</issue>), <fpage>663</fpage>–<lpage>666</lpage>. <object-id pub-id-type="pmid">10837252</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref103"><label>103</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Prins</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kingdom</surname> <given-names>FAA</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name> <article-title>Ecologically valid combinations of first- and second-order surface markings facilitate texture discrimination</article-title>. <source>Vision Research</source>. <year>2007</year>; <volume>47</volume>, <fpage>2281</fpage>–<lpage>2290</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2007.05.003" xlink:type="simple">10.1016/j.visres.2007.05.003</ext-link></comment> <object-id pub-id-type="pmid">17618668</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref104"><label>104</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neri</surname> <given-names>P</given-names></name>. <article-title>Classification images as descriptive statistics</article-title>. <source>Journal of Mathematical Psychology</source>. <year>2018</year>; <volume>82</volume>, <fpage>26</fpage>–<lpage>37</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref105"><label>105</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Klein</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Levi</surname> <given-names>DM</given-names></name>. <article-title>Prolonged perceptual learning of positional acuity in adult amblyopia: perceptual template retuning dynamics</article-title>. <source>Journal of Neuroscience</source>. <year>2008</year>; <volume>28</volume>(<issue>52</issue>), <fpage>14223</fpage>–<lpage>14229</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4271-08.2008" xlink:type="simple">10.1523/JNEUROSCI.4271-08.2008</ext-link></comment> <object-id pub-id-type="pmid">19109504</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref106"><label>106</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep neural networks: a new framework for modeling biological vision and brain information processing</article-title>. <source>Annual Review of Vision Science</source>. <year>2015</year>; <volume>1</volume>, <fpage>417</fpage>–<lpage>446</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-vision-082114-035447" xlink:type="simple">10.1146/annurev-vision-082114-035447</ext-link></comment> <object-id pub-id-type="pmid">28532370</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref107"><label>107</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Antolík</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hofer</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Bednar</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name>. <article-title>Model constrained by visual hierarchy improves prediction of neural responses to natural scenes</article-title>. <source>PLoS Computational Biology</source>. <year>2016</year>; <volume>12</volume>(<issue>6</issue>), <fpage>e1004927</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004927" xlink:type="simple">10.1371/journal.pcbi.1004927</ext-link></comment> <object-id pub-id-type="pmid">27348548</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref108"><label>108</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arsenault</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Yoonessi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name>. <article-title>Higher order texture statistics impair contrast boundary segmentation</article-title>. <source>Journal of Vision</source>. <year>2011</year>; <volume>11</volume>(<issue>10</issue>), <fpage>14</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/11.10.14" xlink:type="simple">10.1167/11.10.14</ext-link></comment> <object-id pub-id-type="pmid">21933932</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref109"><label>109</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brainard</surname> <given-names>DH</given-names></name>. <article-title>The psychophysics toolbox</article-title>. <source>Spatial Vision</source>. <year>1997</year>; <volume>10</volume>:<fpage>433</fpage>–<lpage>436</lpage>. <object-id pub-id-type="pmid">9176952</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref110"><label>110</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pelli</surname> <given-names>DG</given-names></name>. <article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title>. <source>Spatial Vision</source>. <year>1997</year>; <volume>10</volume>:<fpage>437</fpage>–<lpage>442</lpage>. <object-id pub-id-type="pmid">9176953</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref111"><label>111</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kleiner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Brainard</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Pelli</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ingling</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Broussard</surname> <given-names>C</given-names></name>. <article-title>What’s new in Psychtoolbox-3</article-title>. <source>Perception</source>. <year>2007</year>; <volume>36</volume>(<issue>14 suppl</issue>), <fpage>1</fpage>–<lpage>1</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref112"><label>112</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rumelhart</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>RJ</given-names></name>. <article-title>Learning representations by back-propagating errors</article-title>. <source>Nature</source>. <year>1986</year>; <volume>323</volume>(<issue>6088</issue>), <fpage>533</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref113"><label>113</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ran</surname> <given-names>ZY</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>BG</given-names></name>. <article-title>Parameter identifiability in statistical machine learning: a review</article-title>. <source>Neural Computation</source>. <year>2017</year>; <volume>29</volume>(<issue>5</issue>), <fpage>1151</fpage>–<lpage>1203</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00947" xlink:type="simple">10.1162/NECO_a_00947</ext-link></comment> <object-id pub-id-type="pmid">28181880</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref114"><label>114</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiMattina</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>K</given-names></name>. <article-title>How to modify a neural network gradually without changing its input-output functionality</article-title>. <source>Neural Computation</source>. <year>2010</year>; <volume>22</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>47</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2009.05-08-781" xlink:type="simple">10.1162/neco.2009.05-08-781</ext-link></comment> <object-id pub-id-type="pmid">19842986</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref115"><label>115</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willmore</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>Smyth</surname> <given-names>D</given-names></name>. <article-title>Methods for first-order kernel estimation: simple-cell receptive fields from responses to natural scenes</article-title>. <source>Network: Computation in Neural Systems</source>. <year>2003</year>; <volume>14</volume>(<issue>3</issue>), <fpage>553</fpage>–<lpage>577</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref116"><label>116</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dempster</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Laird</surname> <given-names>NM</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>DB</given-names></name>. <article-title>Maximum likelihood from incomplete data via the EM algorithm</article-title>. <source>Journal of the Royal Statistical Society. Series B</source>. <year>1977</year>; <volume>39</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006829.ref117"><label>117</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lewi</surname> <given-names>J</given-names></name>. <article-title>Statistical models for neural encoding, decoding, and optimal stimulus design</article-title>. <source>Progress in Brain Research</source>. <year>2007</year>; <volume>165</volume>, <fpage>493</fpage>–<lpage>507</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0079-6123(06)65031-0" xlink:type="simple">10.1016/S0079-6123(06)65031-0</ext-link></comment> <object-id pub-id-type="pmid">17925266</object-id></mixed-citation></ref>
<ref id="pcbi.1006829.ref118"><label>118</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Prins</surname> <given-names>N</given-names></name>. <article-title>The psychometric function: The lapse rate revisited</article-title>. <source><italic>Journal of Vision</italic></source>. <year>2012</year>; <volume>12</volume>(<issue>6</issue>), <fpage>25</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/12.6.25" xlink:type="simple">10.1167/12.6.25</ext-link></comment> <object-id pub-id-type="pmid">22715196</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>