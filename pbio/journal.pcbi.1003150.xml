<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-01490</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003150</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Decision making</subject></subj-group></subj-group><subj-group><subject>Learning and memory</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Social and behavioral sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A Mixture of Delta-Rules Approximation to Bayesian Inference in Change-Point Problems</article-title>
<alt-title alt-title-type="running-head">Approximate Inference in Change-Point Problems</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Wilson</surname><given-names>Robert C.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Nassar</surname><given-names>Matthew R.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Gold</surname><given-names>Joshua I.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Princeton Neuroscience Institute, Princeton University, Princeton, New Jersey, United States of America</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Department of Neuroscience, University of Pennsylvania, Philadelphia, Pennsylvania, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Behrens</surname><given-names>Tim</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Oxford, United Kingdom</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">rcw2@princeton.edu</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: RCW MRN JIG. Performed the experiments: RCW MRN. Analyzed the data: RCW MRN. Contributed reagents/materials/analysis tools: RCW MRN. Wrote the paper: RCW MRN JIG.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>7</month><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>25</day><month>7</month><year>2013</year></pub-date>
<volume>9</volume>
<issue>7</issue>
<elocation-id>e1003150</elocation-id>
<history>
<date date-type="received"><day>20</day><month>9</month><year>2012</year></date>
<date date-type="accepted"><day>6</day><month>6</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Wilson et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Error-driven learning rules have received considerable attention because of their close relationships to both optimal theory and neurobiological mechanisms. However, basic forms of these rules are effective under only a restricted set of conditions in which the environment is stable. Recent studies have defined optimal solutions to learning problems in more general, potentially unstable, environments, but the relevance of these complex mathematical solutions to how the brain solves these problems remains unclear. Here, we show that one such Bayesian solution can be approximated by a computationally straightforward mixture of simple error-driven ‘Delta’ rules. This simpler model can make effective inferences in a dynamic environment and matches human performance on a predictive-inference task using a mixture of a small number of Delta rules. This model represents an important conceptual advance in our understanding of how the brain can use relatively simple computations to make nearly optimal inferences in a dynamic world.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>The ability to make accurate predictions is important to thrive in a dynamic world. Many predictions, like those made by a stock picker, are based, at least in part, on historical data thought also to reflect future trends. However, when unexpected changes occur, like an abrupt change in the value of a company that affects its stock price, the past can become irrelevant and we must rapidly update our beliefs. Previous research has shown that, under certain conditions, human predictions are similar to those of mathematical, ideal-observer models that make accurate predictions in the presence of change-points. Despite this progress, these models require superhuman feats of memory and computation and thus are unlikely to be implemented directly in the brain. In this work, we address this conundrum by developing an approximation to the ideal-observer model that drastically reduces the computational load with only a minimal cost in performance. We show that this model better explains human behavior than other models, including the optimal model, and suggest it as a biologically plausible model for learning and prediction.</p>
</abstract>
<funding-group><funding-statement>This work was supported by grants NIH R01 EY015260 (to JIG) and F21 MH093099 (to MRN). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="18"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Decisions are often guided by beliefs about the probability and utility of potential outcomes. These beliefs are learned through past experiences that, in stable environments, can be used to generate accurate predictions. However, in dynamic environments, changes can occur that render past experiences irrelevant for predicting future outcomes. For example, after a change in government, historical tax rates may no longer be a reliable predictor of future tax rates. Thus, an important challenge faced by a decision-maker is to identify and respond to environmental change-points, corresponding to when previous beliefs should be abandoned and new beliefs should be formed.</p>
<p>A toy example of such a situation is shown in <xref ref-type="fig" rid="pcbi-1003150-g001">figure 1A</xref>, where we plot the price of a fictional stock over time. In this example, the stock price on a given day (red dots) is generated by sampling from a Gaussian distribution with variance $1 and a mean (dashed black line) that starts at $10 before changing abruptly to $20 at a change-point, perhaps caused by the favorable resolution of a court case. A trader only sees the stock price and not the underlying mean but has to make predictions about the stock price on the next day.</p>
<fig id="pcbi-1003150-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003150.g001</object-id><label>Figure 1</label><caption>
<title>An example change-point problem.</title>
<p>(A) This example has a single change-point at time 20. (B) The Delta rule model with learning rate parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e001" xlink:type="simple"/></inline-formula> performs well before the change-point but poorly immediately afterwards. (C) The Delta rule model with learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e002" xlink:type="simple"/></inline-formula> responds quickly to the change-point but has noisier estimates overall. (D) The full Bayesian model dynamically adapts its learning rate to minimize error overall. (E) Our approximate model shows similar performance to the Bayesian model but is implemented at a fraction of the computational cost and in a biologically plausible manner.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003150.g001" position="float" xlink:type="simple"/></fig>
<p>One common strategy for computing this prediction is based on the Delta rule:<disp-formula id="pcbi.1003150.e003"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e003" xlink:type="simple"/><label>(1)</label></disp-formula>According to this rule, an observation, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e004" xlink:type="simple"/></inline-formula>, is used to update an existing prediction, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e005" xlink:type="simple"/></inline-formula>, based on the learning rate, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e006" xlink:type="simple"/></inline-formula> and the prediction error, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e007" xlink:type="simple"/></inline-formula>. Despite its simplicity, this learning rule can provide effective solutions to a wide range of machine-learning problems <xref ref-type="bibr" rid="pcbi.1003150-Bertsekas1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003150-Sutton1">[2]</xref>. In certain forms, it can also account for numerous behavioral findings that are thought to depend on prediction-error signals represented in brainstem dopaminergic neurons, their inputs from the lateral habenula, and their targets in the basal ganglia and the anterior cingulate cortex <xref ref-type="bibr" rid="pcbi.1003150-Rescorla1">[3]</xref>–<xref ref-type="bibr" rid="pcbi.1003150-Hayden1">[15]</xref>.</p>
<p>Unfortunately, this rule does not perform particularly well in the presence of change-points. We illustrate this problem with a toy example in <xref ref-type="fig" rid="pcbi-1003150-g001">figure 1B and C</xref>. In panel B, we plot the predictions of this model for the toy data set when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e008" xlink:type="simple"/></inline-formula> is set to 0.2. In this case, the algorithm does an excellent job of computing the mean stock value before the change-point. However, it takes a long time to adjust its predictions after the change-point, undervaluing the stock for several days. In <xref ref-type="fig" rid="pcbi-1003150-g001">figure 1C</xref>, we plot the predictions of the model when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e009" xlink:type="simple"/></inline-formula>. In this case, the model responds rapidly to the change-point but has larger errors during periods of stability.</p>
<p>One way around this problem is to dynamically update the learning rate on a trial-by-trial basis between zero, indicating that no weight is given to the last observed outcome, and one, indicating that the prediction is equal to the last outcome <xref ref-type="bibr" rid="pcbi.1003150-Behrens1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003150-Nassar1">[17]</xref>. During periods of stability, a decreasing learning rate can match the current belief to the average outcome. After change-points, a high learning rate shifts beliefs away from historical data and towards more recent, and more relevant, outcomes.</p>
<p>These adaptive dynamics are captured by Bayesian ideal-observer models that determine the rate of learning based on the statistics of change-points and the observed data <xref ref-type="bibr" rid="pcbi.1003150-Adams1">[18]</xref>–<xref ref-type="bibr" rid="pcbi.1003150-Wilson1">[20]</xref>. An example of the behavior of the Bayesian model is shown in <xref ref-type="fig" rid="pcbi-1003150-g001">figure 1D</xref>. In this case, the model uses a low learning rate in periods of stability to make predictions that are very close to the mean, then changes to a high learning rate after a change-point to adapt more quickly to the new circumstances.</p>
<p>Recent experimental work has shown that human subjects adaptively adjust learning rates in dynamic environments in a manner that is qualitatively consistent with these algorithms <xref ref-type="bibr" rid="pcbi.1003150-Behrens1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003150-Nassar1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003150-Krugel1">[21]</xref>. However, it is unlikely that subjects are basing these adjustments on a direct neural implementation of the Bayesian algorithms, which are complex and computationally demanding. Thus, in this paper we ask two questions: 1) Is there a simpler, general algorithm capable of adaptively adjusting its learning rate in the presence of change-points? And 2) Does the new model better explain human behavioral data than either the full Bayesian model or a simple Delta rule? We address these questions by developing a simple approximation to the full Bayesian model. In contrast to earlier work that used a single Delta rule with an adaptive learning rate <xref ref-type="bibr" rid="pcbi.1003150-Nassar1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003150-Krugel1">[21]</xref>, our model uses a mixture of biologically plausible Delta rules, each with its own, fixed learning rate, to adapt its behavior in the presence of change-points. We show that the model provides a better match to human performance than the other models. We conclude with a discussion of the biological plausibility of our model, which we propose as a general model of human learning.</p>
</sec><sec id="s2" sec-type="methods">
<title>Methods</title>
<sec id="s2a">
<title>Ethics statement</title>
<p>Human subject protocols were approved by the University of Pennsylvania internal review board. Informed consent was given by all participants prior to taking part in the study.</p>
</sec><sec id="s2b">
<title>Change-point processes</title>
<p>To familiarize readers with change-point processes and the Bayesian model, we first review these topics in some detail and then turn our attention to the reduced model.</p>
<p>In this paper we are concerned with data generated from change-point processes. An example of such a process generating Gaussian data is given in <xref ref-type="fig" rid="pcbi-1003150-g002">figure 2</xref>. We start by defining a hazard rate, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e010" xlink:type="simple"/></inline-formula>, that in the general case can be variable over time but for our purposes is assumed to be constant. Change-point locations are then generated by sampling from a Bernoulli distribution with this hazard rate, such that the probability of a change-point occurring at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e011" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e012" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003150-g002">figure 2A</xref>). In between change-points, in periods we term ‘epochs,’ the generative parameters of the data are constant. Within each epoch, the values of the generative parameters, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e013" xlink:type="simple"/></inline-formula>, are sampled from a prior distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e014" xlink:type="simple"/></inline-formula>, for some hyper-parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e015" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e016" xlink:type="simple"/></inline-formula> that will be described in more detail in the following sections. For the Gaussian example, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e017" xlink:type="simple"/></inline-formula> is simply the mean of the Gaussian at each time point. We generate this mean for each epoch (<xref ref-type="fig" rid="pcbi-1003150-g002">figure 2B</xref>) by sampling from the prior distribution shown in <xref ref-type="fig" rid="pcbi-1003150-g002">figure 2C</xref>. Finally, we sample the data points at each time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e018" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e019" xlink:type="simple"/></inline-formula> from the generative distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e020" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003150-g002">figure 2D and E</xref>).</p>
<fig id="pcbi-1003150-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003150.g002</object-id><label>Figure 2</label><caption>
<title>An example of the generative process behind a change-point data set with Gaussian data.</title>
<p>(A) First, the change-point locations (grey lines) are sampled from a Bernoulli process with known hazard rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e021" xlink:type="simple"/></inline-formula> (in this case, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e022" xlink:type="simple"/></inline-formula>). (B) Next, the mean of the Gaussian distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e023" xlink:type="simple"/></inline-formula>, is sampled from the prior distribution defined by parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e024" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e025" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e026" xlink:type="simple"/></inline-formula>, (C) for each epoch between change-points (in this case, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e027" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e028" xlink:type="simple"/></inline-formula>). (D) Finally, the data points at each time step (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e029" xlink:type="simple"/></inline-formula>) are sampled from a Gaussian distribution with the current mean and a variance of 1, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e030" xlink:type="simple"/></inline-formula>, shown in (E) for the mean of the last epoch.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003150.g002" position="float" xlink:type="simple"/></fig></sec><sec id="s2c">
<title>Full Bayesian model</title>
<p>The goal of the full Bayesian model <xref ref-type="bibr" rid="pcbi.1003150-Adams1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003150-Fearnhead1">[19]</xref> is to make accurate predictions in the presence of change-points. This model infers the predictive distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e031" xlink:type="simple"/></inline-formula>, over the next data point, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e032" xlink:type="simple"/></inline-formula>, given the data observed up to time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e033" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e034" xlink:type="simple"/></inline-formula>.</p>
<p>In the case where the change-point locations are known, computing the predictive distribution is straightforward. In particular, because the parameters of the generative distribution are resampled independently at a change-point (more technically, the change-points separate the data into product partitions <xref ref-type="bibr" rid="pcbi.1003150-Barry1">[22]</xref>) only data seen since the last change-point are relevant for predicting the future. Therefore, if we define the run-length at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e035" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e036" xlink:type="simple"/></inline-formula>, as the number of time steps since the last change-point, we can write<disp-formula id="pcbi.1003150.e037"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e037" xlink:type="simple"/><label>(2)</label></disp-formula>where we have introduced the shorthand <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e038" xlink:type="simple"/></inline-formula> to denote the predictive distribution given the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e039" xlink:type="simple"/></inline-formula> time points. Assuming that our generative distribution is parameterized by parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e040" xlink:type="simple"/></inline-formula>, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e041" xlink:type="simple"/></inline-formula> is straightforward to write down (at least formally) as the marginal over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e042" xlink:type="simple"/></inline-formula><disp-formula id="pcbi.1003150.e043"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e043" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e044" xlink:type="simple"/></inline-formula> is the inferred distribution over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e045" xlink:type="simple"/></inline-formula> given the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e046" xlink:type="simple"/></inline-formula> time points, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e047" xlink:type="simple"/></inline-formula> is the likelihood of the data given the generative parameters.</p>
<p>When the change-point locations are unknown the situation is more complex. In particular we need to compute a probability distribution over all possible values for the run-length given the observed data. This distribution is called the run-length distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e048" xlink:type="simple"/></inline-formula>. Once we have the run-length distribution, we can compute the predictive distribution in the following way. First we compute the expected run-length on the next trial, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e049" xlink:type="simple"/></inline-formula>; i.e.,<disp-formula id="pcbi.1003150.e050"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e050" xlink:type="simple"/><label>(4)</label></disp-formula>where the sum is over all possible values of the run-length at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e051" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e052" xlink:type="simple"/></inline-formula> is the change-point prior that describes the dynamics of the run-length over time. In particular, because the run-length either increases by one, with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e053" xlink:type="simple"/></inline-formula> in between change-points, or decreases to zero, with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e054" xlink:type="simple"/></inline-formula> at a change-point, the change-point prior, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e055" xlink:type="simple"/></inline-formula>, takes the following form<disp-formula id="pcbi.1003150.e056"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e056" xlink:type="simple"/><label>(5)</label></disp-formula>Given the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e057" xlink:type="simple"/></inline-formula>, we can then compute the predictive distribution of the data on the next trial, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e058" xlink:type="simple"/></inline-formula> in the following manner,<disp-formula id="pcbi.1003150.e059"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e059" xlink:type="simple"/><label>(6)</label></disp-formula>where the sum is over all possible values of the run-length at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e060" xlink:type="simple"/></inline-formula>.</p>
<p>All that then remains is to compute the run-length distribution itself, which can be done recursively using Bayes' rule<disp-formula id="pcbi.1003150.e061"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e061" xlink:type="simple"/><label>(7)</label></disp-formula>Substituting in the form of the change-point prior for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e062" xlink:type="simple"/></inline-formula> we get<disp-formula id="pcbi.1003150.e063"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e063" xlink:type="simple"/><label>(8)</label></disp-formula>Thus for each value of the run-length, all but two of the of the terms in <xref ref-type="disp-formula" rid="pcbi.1003150.e061">equation 7</xref> vanish and the algorithm has complexity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e064" xlink:type="simple"/></inline-formula> computations per timestep. Unfortunately, although this is a substantial improvement compared to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e065" xlink:type="simple"/></inline-formula> complexity of a more naïve change-point model, this computation is still quite demanding. In principle, the total number of run-lengths we must consider is infinite, because we must allow for the possibility that a change-point occurred at any time in the past. In practice, however, it is usual to introduce a maximum run-length, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e066" xlink:type="simple"/></inline-formula>, and define the change-point prior here to be<disp-formula id="pcbi.1003150.e067"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e067" xlink:type="simple"/><label>(9)</label></disp-formula>With this procedure, the complexity of the computation is bounded but still can remain dauntingly high.</p>
<sec id="s2c1">
<title>Efficient solution for exponential families</title>
<p>This inference algorithm is particularly well suited to problems that involve exponential family distributions (such as the Gaussian, Bernoulli, or Laplace distributions) with a conjugate prior <xref ref-type="bibr" rid="pcbi.1003150-Wainwright1">[23]</xref>. For these cases, the predictive distribution given the run-length, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e068" xlink:type="simple"/></inline-formula>, can be represented with a finite number of parameters, called sufficient statistics, that are easily updated when new data arrive.</p>
<p>Specifically, we assume that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e069" xlink:type="simple"/></inline-formula> is sampled from a distribution with parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e070" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e071" xlink:type="simple"/></inline-formula>, which can be related to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e072" xlink:type="simple"/></inline-formula> as<disp-formula id="pcbi.1003150.e073"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e073" xlink:type="simple"/><label>(10)</label></disp-formula>If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e074" xlink:type="simple"/></inline-formula> is an exponential family distribution and we assume a conjugate prior, then this equation is relatively straightforward to compute. Specifically we assume that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e075" xlink:type="simple"/></inline-formula> has the form<disp-formula id="pcbi.1003150.e076"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e076" xlink:type="simple"/><label>(11)</label></disp-formula>where the forms of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e077" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e078" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e079" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e080" xlink:type="simple"/></inline-formula> determine the specific type of exponential family distribution. For example, for a Gaussian distribution with unknown mean, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e081" xlink:type="simple"/></inline-formula>, and known variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e082" xlink:type="simple"/></inline-formula>, we have<disp-formula id="pcbi.1003150.e083"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e083" xlink:type="simple"/><label>(12)</label></disp-formula>We further assume that the generative parameters, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e084" xlink:type="simple"/></inline-formula>, are resampled at each change-point from a conjugate prior distribution of the form<disp-formula id="pcbi.1003150.e085"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e085" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e086" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e087" xlink:type="simple"/></inline-formula> are the prior hyperparameters and the forms of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e088" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e089" xlink:type="simple"/></inline-formula> determine the nature of the prior distribution.</p>
<p>For example, for a Gaussian prior distribution over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e090" xlink:type="simple"/></inline-formula> with standard deviation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e091" xlink:type="simple"/></inline-formula> and mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e092" xlink:type="simple"/></inline-formula>, we set<disp-formula id="pcbi.1003150.e093"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e093" xlink:type="simple"/><label>(14)</label></disp-formula>With this conjugate prior, the posterior distribution over the parameters given the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e094" xlink:type="simple"/></inline-formula> data points, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e095" xlink:type="simple"/></inline-formula>, has the same form as the prior, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e096" xlink:type="simple"/></inline-formula> and we can write<disp-formula id="pcbi.1003150.e097"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e097" xlink:type="simple"/><label>(15)</label></disp-formula>This posterior distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e098" xlink:type="simple"/></inline-formula> (and thus also the likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e099" xlink:type="simple"/></inline-formula> by <xref ref-type="disp-formula" rid="pcbi.1003150.e073">equation 10</xref>), is parameterized by the sufficient statistics <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e100" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e101" xlink:type="simple"/></inline-formula>. Crucially, these statistics are straightforward to compute, as follows<disp-formula id="pcbi.1003150.e102"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e102" xlink:type="simple"/><label>(16)</label></disp-formula>and<disp-formula id="pcbi.1003150.e103"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e103" xlink:type="simple"/><label>(17)</label></disp-formula>Thus, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e104" xlink:type="simple"/></inline-formula> is constant for a given run-length, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e105" xlink:type="simple"/></inline-formula> computes a running sum of the most recent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e106" xlink:type="simple"/></inline-formula> data points (transformed by function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e107" xlink:type="simple"/></inline-formula>).</p>
<p>It is useful to write the equation for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e108" xlink:type="simple"/></inline-formula> as an update rule; that is, in terms of the sufficient statistics at an earlier time point. In particular, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e109" xlink:type="simple"/></inline-formula>, we can write the update in terms of the sufficient statistic at the previous time point and run-length; i.e.,<disp-formula id="pcbi.1003150.e110"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e110" xlink:type="simple"/><label>(18)</label></disp-formula>Dividing through by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e111" xlink:type="simple"/></inline-formula> gives a Delta-rule update for the mean, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e112" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003150.e113"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e113" xlink:type="simple"/><label>(19)</label></disp-formula>Note that in this case the learning rate, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e114" xlink:type="simple"/></inline-formula>, decays as the run-length increases.</p>
</sec><sec id="s2c2">
<title>Graphical interpretation</title>
<p>The previous sections showed that, for conjugate exponential distributions, the Bayesian model needs to keep track of only the run-length distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e115" xlink:type="simple"/></inline-formula>, and the sufficient statistics, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e116" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e117" xlink:type="simple"/></inline-formula>, for each run-length to fully compute the predictive distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e118" xlink:type="simple"/></inline-formula>. This algorithm also has an intuitive interpretation in terms of message passing on a graph (<xref ref-type="fig" rid="pcbi-1003150-g003">Figure 3A</xref>). Each node in this graph represents a run-length, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e119" xlink:type="simple"/></inline-formula>, with two properties: 1) the sufficient statistics, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e120" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e121" xlink:type="simple"/></inline-formula>, associated with that run-length, and 2) a ‘weight’ representing the probability that the run-length at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e122" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e123" xlink:type="simple"/></inline-formula>; i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e124" xlink:type="simple"/></inline-formula>. The weights of the nodes are computed by passing messages along the edges of the graph. Specifically, each node, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e125" xlink:type="simple"/></inline-formula>, sends out two messages: an ‘increasing’ message to node <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e126" xlink:type="simple"/></inline-formula> that corresponds to an increase in run-length if no change-point occurred, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e127" xlink:type="simple"/></inline-formula>, and 2) a ‘change-point’ message, to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e128" xlink:type="simple"/></inline-formula>, corresponding to a decrease in run-length at a change-point, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e129" xlink:type="simple"/></inline-formula>. The weight of node <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e130" xlink:type="simple"/></inline-formula> is then updated by summing all of the incoming messages and multiplying it by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e131" xlink:type="simple"/></inline-formula>, which implements <xref ref-type="disp-formula" rid="pcbi.1003150.e063">equation 8</xref>.</p>
<fig id="pcbi-1003150-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003150.g003</object-id><label>Figure 3</label><caption>
<title>Schematic of algorithms.</title>
<p>(A) Full Bayesian model. (B) Approximate model.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003150.g003" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s2d">
<title>Reduced model</title>
<p>Despite the elegance of the full Bayesian algorithm, it is complex, requiring a memory of a large number (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e132" xlink:type="simple"/></inline-formula>) of different run-lengths, which, in the worst case, is equivalent to keeping track of all the past data. Thus, it seems an unlikely model of human cognition, and a key question is whether comparable predictive performance can be achieved with a simpler, more biologically plausible algorithm. Here we introduce an approximation to the full model that addresses these issues. First we reduce the model's complexity by removing nodes from the update graph (<xref ref-type="fig" rid="pcbi-1003150-g003">Figure 3</xref>). Then we transform the update equation for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e133" xlink:type="simple"/></inline-formula> into a Delta-rule update equation in which the sufficient statistic on each node updates independently of the other nodes. The resulting algorithm is a biologically plausible mixture of Delta-rules that is able to flexibly adapt its overall learning rate in the presence of change-points and whose performance is comparable with that of the full Bayesian model at a fraction of the computational cost. Below we derive new update equations for the sufficient statistics and the weights of each new node for this reduced model.</p>
<p>To more easily distinguish the full and reduced models, we use <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e134" xlink:type="simple"/></inline-formula> to denote run-length in the reduced model and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e135" xlink:type="simple"/></inline-formula> to denote run-length in the full model. Thus, the reduced model has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e136" xlink:type="simple"/></inline-formula> nodes, where node <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e137" xlink:type="simple"/></inline-formula> has run-length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e138" xlink:type="simple"/></inline-formula>. The set of run-lengths, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e139" xlink:type="simple"/></inline-formula>, are ordered such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e140" xlink:type="simple"/></inline-formula>. Unlike the full model, the run-lengths in the reduced model can take on non-integer values, which allows greater flexibility.</p>
<p>The first step in our approximation is to remove nodes from the update graph. This step reduces the memory demands of the algorithm but also requires us to change the update rule for the sufficient statistic and the form of the change-point prior.</p>
<p>Consider a node with run-length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e141" xlink:type="simple"/></inline-formula>. In the full Bayesian model, the sufficient statistic for this node would be<disp-formula id="pcbi.1003150.e142"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e142" xlink:type="simple"/><label>(20)</label></disp-formula>Note that this form of the update relies on having computed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e143" xlink:type="simple"/></inline-formula>, which is the sufficient statistic at run length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e144" xlink:type="simple"/></inline-formula>. In the full Bayesian model, this procedure is straightforward because all possible run-lengths are represented. In contrast, the reduced model includes only a subset of possible run-lengths, and thus a node with run-length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e145" xlink:type="simple"/></inline-formula> will not exist for some values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e146" xlink:type="simple"/></inline-formula>. Therefore, the reduced model must include a new method for updating the sufficient statistic and a new form of the change-point prior.</p>
<p>We first note that another way of writing the update for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e147" xlink:type="simple"/></inline-formula> is as<disp-formula id="pcbi.1003150.e148"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e148" xlink:type="simple"/><label>(21)</label></disp-formula>This sliding-window update equation depends only on information available at node <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e149" xlink:type="simple"/></inline-formula> and thus does not rely on knowing the sufficient statistic at node <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e150" xlink:type="simple"/></inline-formula>. However, this update also has a high memory demand because, to update the sliding window, we have to subtract <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e151" xlink:type="simple"/></inline-formula>, which we can only do if we keep track of the previous <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e152" xlink:type="simple"/></inline-formula> data points on each node.</p>
<p>In our model, we remove the dependence on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e153" xlink:type="simple"/></inline-formula>, and hence the additional memory demands, by taking the average of <xref ref-type="disp-formula" rid="pcbi.1003150.e148">equation 21</xref>. This procedure leads to a memoryless (yet approximate) form of the update equation for each node. In particular, if we take the average of <xref ref-type="disp-formula" rid="pcbi.1003150.e148">equation 21</xref> with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e154" xlink:type="simple"/></inline-formula>, we have<disp-formula id="pcbi.1003150.e155"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e155" xlink:type="simple"/><label>(22)</label></disp-formula>where we have introduced <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e156" xlink:type="simple"/></inline-formula> as the Delta-rule's approximation to the mean sufficient statistic and<disp-formula id="pcbi.1003150.e157"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e157" xlink:type="simple"/><label>(23)</label></disp-formula>as the mean of the node. Dividing <xref ref-type="disp-formula" rid="pcbi.1003150.e148">equation 21</xref> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e158" xlink:type="simple"/></inline-formula> gives us the following form of the update for the mean<disp-formula id="pcbi.1003150.e159"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e159" xlink:type="simple"/><label>(24)</label></disp-formula>Note that this equation for the update of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e160" xlink:type="simple"/></inline-formula> is a Delta rule, just like <xref ref-type="disp-formula" rid="pcbi.1003150.e003">equation 1</xref>, with a fixed learning rate, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e161" xlink:type="simple"/></inline-formula>. Thus, the reduced model simply has to keep track of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e162" xlink:type="simple"/></inline-formula> for each node and update it using only the most recent data point. This form of update rule also allows us to interpret non-integer values of the run-length, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e163" xlink:type="simple"/></inline-formula>, in terms of changes in the learning rate of the Delta rule on a continuum. In <xref ref-type="fig" rid="pcbi-1003150-g004">figure 4</xref> we show the effect of this approximation on the extent to which past data points are used to compute the mean of each node. The sliding window rule computes the average across the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e164" xlink:type="simple"/></inline-formula> data points, ignoring all previous data. In contrast, the Delta rule computes a weighted average using an exponential that decays over time, which tends to slightly under-emphasize the contributions of recent data and over-emphasize the contributions of distant data relative to the sliding window.</p>
<fig id="pcbi-1003150-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003150.g004</object-id><label>Figure 4</label><caption>
<title>Comparison of the extent to which the sliding window and Delta rule updates weigh past information for different run-lengths.</title>
<p>(A) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e165" xlink:type="simple"/></inline-formula>, (B) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e166" xlink:type="simple"/></inline-formula> and (C) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e167" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003150.g004" position="float" xlink:type="simple"/></fig>
<p>Reducing the number of nodes in the model also requires us to change how we update the weights of each node. In particular the update for the weights, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e168" xlink:type="simple"/></inline-formula>, is given as<disp-formula id="pcbi.1003150.e169"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e169" xlink:type="simple"/><label>(25)</label></disp-formula>This equation is similar to <xref ref-type="disp-formula" rid="pcbi.1003150.e061">equation 7</xref> but differs in the number of run-lengths available. Crucially, this difference requires an adjustment to the change-point prior. The adjusted prior should approximate the full change-point prior (<xref ref-type="disp-formula" rid="pcbi.1003150.e056">Eq. 5</xref>) as closely as possible. Recall that the full prior captures the fact that the run-length either decreases to zero if there is a change-point (with prior probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e170" xlink:type="simple"/></inline-formula>) or increases by one if there is no change-point (with prior probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e171" xlink:type="simple"/></inline-formula>).</p>
<p>To see how to compute this adjusted prior in the reduced model, we first decompose the change-point prior into two terms corresponding to the possibility that a change-point will occur or not; i.e.,<disp-formula id="pcbi.1003150.e172"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e172" xlink:type="simple"/><label>(26)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e173" xlink:type="simple"/></inline-formula> is the probability that the run-length is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e174" xlink:type="simple"/></inline-formula> given that there was a change-point and that the previous run-length was <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e175" xlink:type="simple"/></inline-formula>. Similarly <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e176" xlink:type="simple"/></inline-formula> is the probability that the run-length is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e177" xlink:type="simple"/></inline-formula> given that the previous run-length was <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e178" xlink:type="simple"/></inline-formula> and there was not a change-point.</p>
<p>The change-point case is straightforward, because a change-point always results in a transition to the shortest run-length; i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e179" xlink:type="simple"/></inline-formula> is zero, except when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e180" xlink:type="simple"/></inline-formula> when it takes value 1.</p>
<p>The no change-point case, however, is more difficult. In the full model the run-length increases by 1 when there is no change-point, thus we would like to have<disp-formula id="pcbi.1003150.e181"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e181" xlink:type="simple"/><label>(27)</label></disp-formula>However, because the nodes have variable spacing in the reduced model, this form is not possible as there may be no node with a run-length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e182" xlink:type="simple"/></inline-formula>. We thus seek an approximation such that the prior defines an average increase in run-length of 1 if there is not a change-point. That is, we require<disp-formula id="pcbi.1003150.e183"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e183" xlink:type="simple"/><label>(28)</label></disp-formula>For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e184" xlink:type="simple"/></inline-formula> we can match this expectation exactly by setting<disp-formula id="pcbi.1003150.e185"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e185" xlink:type="simple"/><label>(29)</label></disp-formula>For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e186" xlink:type="simple"/></inline-formula> we approximate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e187" xlink:type="simple"/></inline-formula> using<disp-formula id="pcbi.1003150.e188"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e188" xlink:type="simple"/><label>(30)</label></disp-formula>In this case we do not match the expected increase in run-length. For the final node, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e189" xlink:type="simple"/></inline-formula>, it is impossible to transition to a longer run-length and so we simply have a self transition with probability 1; i.e.,<disp-formula id="pcbi.1003150.e190"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e190" xlink:type="simple"/><label>(31)</label></disp-formula>Taken together with <xref ref-type="disp-formula" rid="pcbi.1003150.e172">equation 26</xref>, <xref ref-type="disp-formula" rid="pcbi.1003150.e185">equations 29</xref>, <xref ref-type="disp-formula" rid="pcbi.1003150.e188">30</xref> and <xref ref-type="disp-formula" rid="pcbi.1003150.e190">31</xref> define the change-point prior in the reduced model.</p>
<p>Like the full Bayesian model, our reduced model also has a graphical interpretation. Again each node, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e191" xlink:type="simple"/></inline-formula>, keeps track of two quantities: 1) the mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e192" xlink:type="simple"/></inline-formula>, computed according to <xref ref-type="disp-formula" rid="pcbi.1003150.e159">equation 24</xref>, and 2) the weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e193" xlink:type="simple"/></inline-formula>. As in the full model, the weights are computed by passing messages along the edge of the graph. However, the structure of the graph is slightly different, with no increasing message being sent by node <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e194" xlink:type="simple"/></inline-formula> and an extra ‘self’ message from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e195" xlink:type="simple"/></inline-formula> to itself. The increasing message has weight<disp-formula id="pcbi.1003150.e196"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e196" xlink:type="simple"/><label>(32)</label></disp-formula>the self message has weight<disp-formula id="pcbi.1003150.e197"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e197" xlink:type="simple"/><label>(33)</label></disp-formula>and the change-point message has weight<disp-formula id="pcbi.1003150.e198"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e198" xlink:type="simple"/><label>(34)</label></disp-formula>Finally the new weight for each node is computed by summing all of the incoming messages to implement <xref ref-type="disp-formula" rid="pcbi.1003150.e169">equation 25</xref>.</p>
</sec></sec><sec id="s3">
<title>Results</title>
<p>In this section we present the results of simple simulations comparing the reduced and full models, investigate the error between the reduced model's predictions and the ground truth and use our model to fit human behavior on a simple prediction task with change-points.</p>
<sec id="s3a">
<title>Simulations</title>
<p>First we consider the simplest cases of one and two nodes with Gaussian data. These cases have particularly simple update rules, and their output is easy to understand. We then consider the more general case of many nodes to show how the reduced model retains many of the useful properties of the full model, such as keeping track of an approximate run-length distribution and being able to handle different kinds of data.</p>
<sec id="s3a1">
<title>One and two nodes</title>
<p>To better understand the model it is useful to consider the special cases of one and two nodes with Gaussian data. When there is only one node, the model has only one run-length, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e199" xlink:type="simple"/></inline-formula>. The update for the mean of this single node is given by<disp-formula id="pcbi.1003150.e200"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e200" xlink:type="simple"/><label>(35)</label></disp-formula>where we have used the fact that, for Gaussian data with a known variance, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e201" xlink:type="simple"/></inline-formula>, we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e202" xlink:type="simple"/></inline-formula>. This update rule is, of course, equivalent to a simple Delta rule with a fixed learning rate. Because there is only one node, computing the run-length distribution is trivial, as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e203" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e204" xlink:type="simple"/></inline-formula> and thus the predictions of this model are simply the mean of the single Delta rule.</p>
<p>In the two-node case the model has two nodes with run-lengths <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e205" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e206" xlink:type="simple"/></inline-formula>. The means of these nodes update according to independent Delta rules<disp-formula id="pcbi.1003150.e207"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e207" xlink:type="simple"/><label>(36)</label></disp-formula>The prediction of the two-node model is given as the weighted sum of these two nodes<disp-formula id="pcbi.1003150.e208"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e208" xlink:type="simple"/><label>(37)</label></disp-formula>where the weights, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e209" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e210" xlink:type="simple"/></inline-formula>, are the components of the run-length distribution that update according to <xref ref-type="disp-formula" rid="pcbi.1003150.e169">equation 25</xref>. For node 1, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e211" xlink:type="simple"/></inline-formula> updates as<disp-formula id="pcbi.1003150.e212"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e212" xlink:type="simple"/><label>(38)</label></disp-formula>where we have used the fact that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e213" xlink:type="simple"/></inline-formula> because the run-length distribution is normalized. For node 2, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e214" xlink:type="simple"/></inline-formula> updates as<disp-formula id="pcbi.1003150.e215"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e215" xlink:type="simple"/><label>(39)</label></disp-formula>Thus, for the two-node case, the run-length distribution is closely tied to the likelihood of the data for each of the nodes, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e216" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e217" xlink:type="simple"/></inline-formula>. These likelihoods are computed in a straightforward manner given the mean and run-length of each node. For Gaussian data these likelihoods take the form<disp-formula id="pcbi.1003150.e218"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e218" xlink:type="simple"/><label>(40)</label></disp-formula></p>
<p>An illustration of the output of the one and two node models is shown in <xref ref-type="fig" rid="pcbi-1003150-g005">figure 5A</xref>. This figure shows the predictions of one- and two-node models when faced with a relatively simple change-point task. To generate this figure, the one-node model had a single run-length, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e219" xlink:type="simple"/></inline-formula>, whereas the two-node model had two run-lengths, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e220" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e221" xlink:type="simple"/></inline-formula>. The hazard rate in each model was set to 0.1, and the noise standard deviation, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e222" xlink:type="simple"/></inline-formula>, was set at 0.5. The two-node model is much better able to adapt to the change-point than the one-node model. <xref ref-type="fig" rid="pcbi-1003150-g005">Figure 5B</xref> shows the evolving weights of the two nodes, determined from the run-length distribution. Before the change-point, the model has a high weight on the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e223" xlink:type="simple"/></inline-formula> node and a low weight on the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e224" xlink:type="simple"/></inline-formula> node. At the change-point, this trend reverses abruptly but then returns after the model stabilizes to the mean of the new data.</p>
<fig id="pcbi-1003150-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003150.g005</object-id><label>Figure 5</label><caption>
<title>Output of one- and two- node models on a simple change-point task.</title>
<p>(A) Predictions from the one- and two-node models. (B) Evolution of the node weights for the two-node model.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003150.g005" position="float" xlink:type="simple"/></fig></sec><sec id="s3a2">
<title>Many nodes</title>
<p>Here we illustrate the utility of the approximate algorithm to solve simulated change-point problems using three different types of generative distribution. The first is a Bernoulli process with a piecewise constant rate, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e225" xlink:type="simple"/></inline-formula>, (<xref ref-type="fig" rid="pcbi-1003150-g006">Figure 6A</xref>) in which the generative distribution takes the following exponential family form<disp-formula id="pcbi.1003150.e226"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e226" xlink:type="simple"/><label>(41)</label></disp-formula>and with a uniform prior distribution defined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e227" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e228" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1003150-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003150.g006</object-id><label>Figure 6</label><caption>
<title>Examples comparing estimates and run-length distributions from the full Bayesian model and our reduced approximation.</title>
<p>These comparisons are made for Bernoulli data (A, D, G), Gaussian data with unknown mean (B,E,F) and Gaussian data with a constant mean but unknown variance (C, F, I). (A, B, C) input data (grey), model estimates (blue: full model; red: reduced model), and the ground truth generative parameter (mean for A and B, standard deviation in C; dashed black line). Run-length distributions computed for the full model (D, E, F) and reduced model (G, H, I) are shown for each of the examples.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003150.g006" position="float" xlink:type="simple"/></fig>
<p>The second is a Gaussian distribution with known standard deviation, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e229" xlink:type="simple"/></inline-formula> = 5, but unknown mean (<xref ref-type="fig" rid="pcbi-1003150-g006">Figure 6B</xref>). In this case, the generative distribution takes on the following exponential family form<disp-formula id="pcbi.1003150.e230"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e230" xlink:type="simple"/><label>(42)</label></disp-formula>with prior hyperparameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e231" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e232" xlink:type="simple"/></inline-formula>.</p>
<p>The third is a Gaussian distribution with a known mean, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e233" xlink:type="simple"/></inline-formula> = 0, and a changing standard deviation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e234" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003150-g006">Figure 6C</xref>). In this case, the generative distribution takes on the following exponential family form<disp-formula id="pcbi.1003150.e235"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e235" xlink:type="simple"/><label>(43)</label></disp-formula>with prior hyper parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e236" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e237" xlink:type="simple"/></inline-formula>.</p>
<p>For all three cases, both the full and reduced models used a fixed hazard rate (equal to 0.05 for the first and third cases, 0.025 for the second case). The reduced models used as initial sufficient statistics <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e238" xlink:type="simple"/></inline-formula> for case 1 and 2 and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e239" xlink:type="simple"/></inline-formula> in case 3, and had 18 nodes spaced logarithmically between 1 and 100.</p>
<p>In <xref ref-type="fig" rid="pcbi-1003150-g006">figure 6</xref>, the top row shows the true value of the parameter of interest for the generative process (the Bernoulli rate in panel A, the mean in panel B, and the standard deviation in panel C), the generated data, and the inferred value of the parameter from the full (blue) and reduced (red) models. For all three cases, there is a close correspondence between the values inferred by the full and reduced models. For the Bernoulli case, the full model has an average mean squared error (relative to ground truth) of 0.037 versus 0.041 for the reduced model. For the Gaussian case with known variance the mean squared errors are 13.9 for the full model and 16.4 for the reduced model. For the Gaussian case with known variance the errors are 4.3 and 6.2 respectively. We also show the run-length distributions inferred by both models (middle and bottom rows), which are more sparsely sampled by the reduced models but still pick up the major trends seen in the full model.</p>
<p>For these examples, we used more nodes in the reduced model than were necessary to solve these problems effectively, because this approach allowed us to better visualize the run-length distribution in the reduced model and to facilitate comparison with the full model. In the next section, we explore the relationship between the effectiveness of the reduced model and its number of nodes in more detail.</p>
</sec></sec><sec id="s3b">
<title>Performance of the reduced model relative to ground truth</title>
<p>Here we derive an approximate, but analytic, expression for the average discrepancy between the predictions made by the reduced model and the ground truth generative parameters. We then use this result to compute approximately optimal node arrangements for a variety of conditions and investigate how the error varies as a function of the parameters in the model.</p>
<sec id="s3b1">
<title>Analytic expression for error</title>
<p>Although there are many measures we could use to quantify the error between the approximation and the ground truth, for reasons of analytic tractability, we focus here on the squared error. More specifically, we compute the expected value, over data and time, of the squared error between the predictive mean of the reduced model, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e240" xlink:type="simple"/></inline-formula>, and the ground truth mean on the next time step, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e241" xlink:type="simple"/></inline-formula>; i.e.,<disp-formula id="pcbi.1003150.e242"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e242" xlink:type="simple"/><label>(44)</label></disp-formula>Because our model is a mixture model, the mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e243" xlink:type="simple"/></inline-formula> is given by<disp-formula id="pcbi.1003150.e244"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e244" xlink:type="simple"/><label>(45)</label></disp-formula>For notational convenience we drop the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e245" xlink:type="simple"/></inline-formula> subscripts and refer to node <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e246" xlink:type="simple"/></inline-formula> simply by its subscript <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e247" xlink:type="simple"/></inline-formula>, and we write <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e248" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e249" xlink:type="simple"/></inline-formula>. We also refer to the learning rate of node <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e250" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e251" xlink:type="simple"/></inline-formula>. Finally, we refer to the set of nodes in the reduced model as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e252" xlink:type="simple"/></inline-formula>, such that the above equation, in our new notation, becomes<disp-formula id="pcbi.1003150.e253"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e253" xlink:type="simple"/><label>(46)</label></disp-formula>Substituting this expression into <xref ref-type="disp-formula" rid="pcbi.1003150.e242">equation 44</xref> for the error we get<disp-formula id="pcbi.1003150.e254"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e254" xlink:type="simple"/><label>(47)</label></disp-formula>Here we have made a mean-field approximation along the lines of<disp-formula id="pcbi.1003150.e255"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e255" xlink:type="simple"/><label>(48)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e256" xlink:type="simple"/></inline-formula> is the average run-length distribution over the reduced model. This assumption is clearly not strictly true, because the weights of the two nodes are driven by at least some of the same data points. Accordingly, this approximation breaks down under certain conditions. For example, when change-point locations are known exactly, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e257" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e258" xlink:type="simple"/></inline-formula> are strongly correlated, because if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e259" xlink:type="simple"/></inline-formula>, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e260" xlink:type="simple"/></inline-formula> is necessarily zero. Thus, under these conditions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e261" xlink:type="simple"/></inline-formula> is only non-zero when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e262" xlink:type="simple"/></inline-formula>, which is not true in the approximation. However, in noisy environments, change-point locations are rarely known exactly and this approximation is far less problematic. As we show below, the approximation provided a reasonably close match to the actual squared error measured from simulations for both Bernoulli and Gaussian data.</p>
<p><xref ref-type="disp-formula" rid="pcbi.1003150.e254">Equations 47</xref> and <xref ref-type="disp-formula" rid="pcbi.1003150.e255">48</xref> imply that, to compute the error, we need to compute four quantities: the averages over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e263" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e264" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e265" xlink:type="simple"/></inline-formula>, in addition to the expected run-length distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e266" xlink:type="simple"/></inline-formula>. A full derivation of these terms is presented in the Supplementary Material; here we focus on presenting how this error varies with model parameters in the specific cases of Bernoulli and Gaussian data. To facilitate comparison between these two different data types, we compute the error relative to the variance of the prior distribution over the data,<disp-formula id="pcbi.1003150.e267"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e267" xlink:type="simple"/><label>(49)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e268" xlink:type="simple"/></inline-formula> is the prior over the data given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e269" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e270" xlink:type="simple"/></inline-formula> is the mean squared error if the algorithm simply predicted the mean of the prior distribution at each time step. Thus the ‘relative error,’ <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e271" xlink:type="simple"/></inline-formula>, takes a value of one when the algorithm picks the mean of the prior distribution, which is the limiting case as the learning rate approaches zero.</p>
</sec><sec id="s3b2">
<title>Error for one node</title>
<p>We first consider how the relative error varies as a function of hazard rate and learning rate for a model with just one node (<xref ref-type="fig" rid="pcbi-1003150-g007">figure 7</xref>). The one-node case is useful because we can easily visualize the results and, because in this case the run-length distribution has only one non-zero term, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e272" xlink:type="simple"/></inline-formula>, the expression for the error is exact. <xref ref-type="fig" rid="pcbi-1003150-g007">Figures 7A and B</xref> consider Bernoulli data with a uniform prior (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e273" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e274" xlink:type="simple"/></inline-formula> = 1). For different settings of the hazard rate, there is a unique learning rate (which is bounded between 0 and 1) that minimizes the error. The value of this optimal learning rate tends to increase as a function of increasing hazard rate, except at high hazard rates when it decreases to near zero. This decrease at high hazard rates is due to the fact that when a change happens on nearly every trial, the best guess is the mean of the prior distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e275" xlink:type="simple"/></inline-formula>, which is better learned with a smaller learning rate that averages over multiple change-points.</p>
<fig id="pcbi-1003150-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003150.g007</object-id><label>Figure 7</label><caption>
<title>Error and optimal learning rates from the one-node model.</title>
<p>(A, B) Bernoulli data, (C, D) Gaussian data. (A, C) Error (normalized by the variance of the prior, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e276" xlink:type="simple"/></inline-formula>) as a function of learning rate for four different hazard rates, as indicated. (B, D) Optimal learning rate, corresponding to the lowest relative error, as a function of hazard rate.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003150.g007" position="float" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1003150-g007">Figure 7C and D</xref> consider a Gaussian distribution with unknown mean and known variance (using parameters that match the experimental setup: standard deviation = 10, prior parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e277" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e278" xlink:type="simple"/></inline-formula>). These plots show the same qualitative pattern as the Bernoulli case, except that the relative error is smaller and the optimal learning rate varies over a wider range. This variability results from the fact that the costs involved in making a wrong prediction can be much higher in the Gaussian case (because of the larger variance) than the Bernoulli case, in which the maximal error is between −1 and 1.</p>
</sec><sec id="s3b3">
<title>Error for multiple nodes</title>
<p>Next we consider the case of multiple nodes. <xref ref-type="fig" rid="pcbi-1003150-g008">Figure 8</xref> shows the optimal learning rates as a function of hazard rate for the reduced model with 1–3 nodes for Bernoulli (panels A–C) and Gaussian (panels D–F) data. In the Bernoulli case, going to two nodes adds a second, larger learning rate that shows the same non-monotonic dependence on hazard rate as with one node. However, the hazard rate at which the smaller learning rate goes to zero is lower than in the one-node case. For three nodes, the relationship between optimal learning rate and hazard rate is more complicated. We see numerical instability in the optimization procedure at low hazard rate, caused by the presence of several distinct local minima. We also see complex behavior at higher hazard rates, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e279" xlink:type="simple"/></inline-formula> as the smallest learning rate goes to zero, the behavior of the other two learning rates changes dramatically. Similar results were obtained for the Gaussian case except that for three nodes, the optimal node positions become degenerate as the highest two learning rates converge for intermediate values of the hazard rate.</p>
<fig id="pcbi-1003150-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003150.g008</object-id><label>Figure 8</label><caption>
<title>Optimal learning rates.</title>
<p>These learning rates correspond to the lowest relative error (see <xref ref-type="fig" rid="pcbi-1003150-g007">figure 7</xref>), as a function of hazard rate and number of nodes. (A–C), Bernoulli case with 1 (A), 2 (B), or 3 (C) nodes. (D–F), Gaussian case with 1 (D), 2 (E), or 3 (F) nodes.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003150.g008" position="float" xlink:type="simple"/></fig>
<p>In <xref ref-type="fig" rid="pcbi-1003150-g009">figure 9</xref> we show the relative error as a function of hazard rate at the optimal learning rate settings computed both from simulation and our analytic expression. The close agreement between theory and simulation provides some justification for the approximations we used. More generally, we see that the relative error increases with hazard rate and decreases slightly with more nodes. The biggest improvement in performance comes from increasing from one to two nodes.</p>
<fig id="pcbi-1003150-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003150.g009</object-id><label>Figure 9</label><caption>
<title>Error (normalized by the variance of the prior, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e280" xlink:type="simple"/></inline-formula>) as a function of hazard rate for the reduced model at the optimal parameter settings.</title>
<p>The solid black lines correspond to the approximate error computed using the theory, the grey dots correspond to the average error computed from simulations. (A–C), Bernoulli case with 1 (A), 2 (B), or 3 (C) nodes. (D–F), Gaussian case with 1 (D), 2 (E), or 3 (F) nodes.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003150.g009" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s3c">
<title>Fits to experimental data</title>
<p>In this section, we ask how well our model describes human behavior by fitting versions of the model to behavioral data from a predictive-inference task <xref ref-type="bibr" rid="pcbi.1003150-Nassar2">[24]</xref>. Briefly, in this task, 30 human subjects (19 female, 11 male) were shown a sequence of numbers between 0 and 300 that were generated by a Gaussian change-point process. This process had a mean that was randomly sampled at every change-point and a standard deviation that was constant (set to either 5 or 10) for blocks of 200 trials. Samples were constrained to be between 0 and 300 by keeping the generative means away from these bounds (the generative means were sampled from uniform distribution [from 40 to 260]) and resampling the small fraction of samples outside of this range until they lay within the range. The hazard rate was set at 0.1 except for the first three trials following a change-point, in which case the hazard rate was zero.</p>
<p>The subjects were required to predict the next number in the sequence and obtained more reward the closer their predictions were to the actual outcome. In particular, subjects were required to minimize the mean absolute error between prediction and outcome, which we denote <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e281" xlink:type="simple"/></inline-formula>. Because prediction errors depended substantially on the specific sequence of numbers generated for the given session, the exact conversion between error and monetary reward was computed by comparing performance with two benchmarks: a lower benchmark (LB) and an higher benchmark (HB). The LB was computed as the mean absolute difference between sequential generated numbers. The HB was the mean difference between mean of the generative distribution on the previous trial and the generated number. Payout was then computed as follows:<disp-formula id="pcbi.1003150.e282"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e282" xlink:type="simple"/><label>(50)</label></disp-formula></p>
<p>A benefit of this task design is that the effective learning rates used by subjects on a trial-by-trial basis can be computed in terms of their predictions following each observed outcome, using the relationships in <xref ref-type="disp-formula" rid="pcbi.1003150.e003">equation 1</xref>. Our previous studies indicated that these learning rates varied systematically as a function of properties of the generative process, including its standard deviation and the occurrence of change-points <xref ref-type="bibr" rid="pcbi.1003150-Nassar1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003150-Nassar2">[24]</xref>.</p>
<p>To better understand the computational basis for these behavioral findings, we compared five different inference models: the full Bayesian model (‘full’), the reduced model with 1 to 3 nodes and the approximately Bayesian model of Nassar et al <xref ref-type="bibr" rid="pcbi.1003150-Nassar1">[17]</xref>. The Nassar et al model instantiates an alternative hypothesis to the mixture of fixed Delta rules by using a single Delta rule with a single, adaptive learning rate to approximate Bayesian inference.</p>
<p>On each trial, each of these models, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e283" xlink:type="simple"/></inline-formula>, produces a prediction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e284" xlink:type="simple"/></inline-formula> about the location of the next data point. To simulate the effects of decision noise, we assume that the subjects' reported predictions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e285" xlink:type="simple"/></inline-formula>, are subject to noise, such that<disp-formula id="pcbi.1003150.e286"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e286" xlink:type="simple"/><label>(51)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e287" xlink:type="simple"/></inline-formula> is sampled from a Gaussian distribution with mean 0 and standard deviation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e288" xlink:type="simple"/></inline-formula> that we fit as a free parameter for all models.</p>
<p>In addition to this noise parameter, we fit the following free parameters for each model: The full model and the model of Nassar et al. have a hazard rate as their only other parameter, the one-node model has a single learning rate and the remaining models with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e289" xlink:type="simple"/></inline-formula> nodes (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e290" xlink:type="simple"/></inline-formula>) have a hazard rate as well as the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e291" xlink:type="simple"/></inline-formula> learning rates.</p>
<p>Our fits identified the model parameters that maximized the log likelihood of the observed human predictions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e292" xlink:type="simple"/></inline-formula>, given each of the models, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e293" xlink:type="simple"/></inline-formula>, which is given by<disp-formula id="pcbi.1003150.e294"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e294" xlink:type="simple"/><label>(52)</label></disp-formula>We used the maximum likelihood value to approximate the log Bayesian evidence, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e295" xlink:type="simple"/></inline-formula> for each model using the standard Bayesian information criterion (BIC) approximation <xref ref-type="bibr" rid="pcbi.1003150-Schwarz1">[25]</xref>, which takes into account the different numbers of parameters in the different models; i.e.,<disp-formula id="pcbi.1003150.e296"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003150.e296" xlink:type="simple"/><label>(53)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e297" xlink:type="simple"/></inline-formula> is the number of free parameters in model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e298" xlink:type="simple"/></inline-formula>.</p>
<p>Models were then compared at the group level using the Bayesian method of Stephan et al. <xref ref-type="bibr" rid="pcbi.1003150-Stephan1">[26]</xref>. Briefly, this method aggregates the evidence from each of the models for each of the subjects to estimate two measures of model fit. The first, which we refer to as the ‘model probability’, is an estimate of how likely it is that a given model generated the data from a randomly chosen subject. The second, termed the ‘exceedance probability’, is the probability that one model is more likely than any of the others to have generated the behavior of all of the subjects.</p>
<p>An important question when interpreting the model fits is the extent to which the different models are identifiable using these analyses. In particular we are interested in the extent to which different models can be separated on the basis of their behavior and the accuracy with which the parameters of each model can be fit.</p>
<p>The question of model identifiability is addressed in <xref ref-type="fig" rid="pcbi-1003150-g010">figure 10</xref>, where we plot two confusion matrices showing the model probability (A) and the exceedance probability (B) for simulated data. These matrices were generated using simulations that matched the human-subjects experiments, with the same values of the observed stimuli, the same number of trials per experiment and the same parameter settings as found by fitting the human data. Ideally, both confusion matrices should be the identity matrix, indicating that data fit to model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e299" xlink:type="simple"/></inline-formula> is always generated by model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e300" xlink:type="simple"/></inline-formula> and never by any other model (e.g., <xref ref-type="bibr" rid="pcbi.1003150-Steyvers1">[27]</xref>). However, because of noise in the data and the limited number of trials in the experiment, it is often the case that not all of the models are completely separable. In the present case, there is good separation for the Nassar et al., full, 1-node, and 2-node models and reasonable separation between the 3-node model and others. When we extended this analysis to include 4- and 5-node models, we found that they were indistinguishable from the 3-node model. Thus, these models are not included in our analyses, and we consider the ‘3-node model’ to represent a model with 3 or more nodes. Note that the confusion matrix showing the exceedance probability (<xref ref-type="fig" rid="pcbi-1003150-g010">figure 10B</xref>) is closer to diagonal than the model probability confusion matrix (<xref ref-type="fig" rid="pcbi-1003150-g010">figure 10A</xref>). This result reflects the fact that exceedance probability is computed at the group level (i.e., that all the simulated data sets were generated by model M), whereas model probability computes the chance that any given simulation is best by model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e301" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1003150-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003150.g010</object-id><label>Figure 10</label><caption>
<title>Confusion matrices.</title>
<p>(A) The confusion matrix of model probability, the estimated fraction of data simulated according to one model that is fit to each of the models. (B) The confusion matrix of exceedance probability, the estimated probability at the group level that a given model has generated all the data.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003150.g010" position="float" xlink:type="simple"/></fig>
<p>To address the question of parameter estimability, we computed correlations between the simulated parameters and the parameter values recovered by the fitting procedure for each of the models. There was strong correspondence between the simulated and fit parameter values for all of the models and all correlations were significant (see supplementary <xref ref-type="supplementary-material" rid="pcbi.1003150.s002">table S1</xref>).</p>
<p>The 3-node model most effectively describes the human data (<xref ref-type="fig" rid="pcbi-1003150-g011">Figure 11</xref>), producing slightly better fits than the model of Nassar et al. at the group level. <xref ref-type="fig" rid="pcbi-1003150-g011">Figure 11A</xref> shows model probability, the estimated probability that any given subject is best fit by each of the models. This measure showed a slight preference for the 3-node model over the model of Nassar et al. <xref ref-type="fig" rid="pcbi-1003150-g011">Figure 11B</xref> shows the exceedance probability for each of the models, the probability that each of the models best fits the data at the group level. Because this measure aggregates across the group it magnifies the differences between the models and showed a clearer preference for the 3-node model. <xref ref-type="table" rid="pcbi-1003150-t001">Table 1</xref> reports the means of the corresponding fit parameters for each of the models (see also supplementary <xref ref-type="supplementary-material" rid="pcbi.1003150.s001">figure S1</xref> for plots of the full distributions of the fit parameters). Consistent with the optimal parameters derived in the previous section (<xref ref-type="fig" rid="pcbi-1003150-g009">figure 9E</xref>), for the 2- and 3-node models, the learning rate of the 1st node is close to one (mean ∼0.95).</p>
<fig id="pcbi-1003150-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003150.g011</object-id><label>Figure 11</label><caption>
<title>Results of the model-fitting procedure.</title>
<p>(A) The model probability for each of the five models. This measure reports the estimated probability that a given subject will be best fit by each of the models. (B) The exceedance probability for each of the five models. This measure reports the probability that each of the models best explains the data from all subjects.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003150.g011" position="float" xlink:type="simple"/></fig><table-wrap id="pcbi-1003150-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003150.t001</object-id><label>Table 1</label><caption>
<title>Table of mean fit parameter values for all models ± s.e.m.</title>
</caption><alternatives><graphic id="pcbi-1003150-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003150.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Model</td>
<td align="left" rowspan="1" colspan="1">hazard rate, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e302" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">decision noise, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e303" xlink:type="simple"/></inline-formula></td>
<td align="center" rowspan="1" colspan="1">learning rate(s), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e304" xlink:type="simple"/></inline-formula></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Nassar et al.</td>
<td align="left" rowspan="1" colspan="1">0.45±0.04</td>
<td align="left" rowspan="1" colspan="1">8.35±0.87</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">full</td>
<td align="left" rowspan="1" colspan="1">0.04±0.01</td>
<td align="left" rowspan="1" colspan="1">20.22±0.53</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">1 node</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">8.7±0.72</td>
<td align="left" rowspan="1" colspan="1">0.88±0.01</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2 nodes</td>
<td align="left" rowspan="1" colspan="1">0.27±0.04</td>
<td align="left" rowspan="1" colspan="1">7.26±0.66</td>
<td align="left" rowspan="1" colspan="1">0.94±0.01</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="justify" rowspan="1" colspan="1">0.53±0.03</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">3 nodes</td>
<td align="left" rowspan="1" colspan="1">0.12±0.03</td>
<td align="left" rowspan="1" colspan="1">6.97±0.66</td>
<td align="left" rowspan="1" colspan="1">0.96±0.01</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.82±0.03</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.52±0.03</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap></sec></sec><sec id="s4">
<title>Discussion</title>
<p>The world is an ever-changing place. Humans and animals must recognize these changes to make accurate predictions and good decisions. In this paper, we considered dynamic worlds in which periods of stability are interrupted by abrupt change-points that render the past irrelevant for predicting the future. Previous experimental work has shown that humans modulate their behavior in the presence of such change-points in a way that is qualitatively consistent with Bayesian models of change-point detection. However, these models appear to be too computationally demanding to be implemented directly in the brain. Thus we asked two questions: 1) Is there a simple and general algorithm capable of making good predictions in the presence of change-points? And 2) Does this algorithm explain human behavior? In this section we discuss the extent to which we have answered these questions, followed by a discussion of the question that motivated this work: Is this algorithm biologically plausible? Throughout we consider the broader implications of our answers and potential avenues for future research.</p>
<sec id="s4a">
<title>Does the reduced model make good predictions?</title>
<p>To address this question, we derived an approximation to the Bayesian model based on a mixture of Delta rules, each implemented in a separate ‘node’ of a connected graph. In this reduced model, each Delta rule has its own, fixed learning rate. The overall prediction is generated by computing a weighted sum of the predictions from each node. Because only a small number of nodes are required, the model is substantially less complex than the full Bayesian model. Qualitatively, the outputs of the reduced and full Bayesian models share many features, including the ability to quickly increase the learning rate following a change-point and reduce it during periods of stability. These features were apparent for the reduced model even with a small number of (2 or 3) nodes. Thus, effective solutions to change-point problems can be achieved with minimal computational cost.</p>
<p>For future work, it would be interesting to consider other generative distributions, such as a Gaussian with unknown mean and variance or multidimensional data (e.g., multidimensional Gaussians) to better assess the generality of this solution. In principle, these extensions should be straightforward to deal with in the current model, which would simply require the sufficient statistic <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e305" xlink:type="simple"/></inline-formula> to be a vector instead of a scalar. Another obvious extension would be to consider generative parameters that drift over time (perhaps in addition to abrupt changes at change-points) or a hazard rate that changes as a function of run-length and/or time.</p>
</sec><sec id="s4b">
<title>Does the reduced model explain human behavior?</title>
<p>To address this question, we used a model-based analysis of human behavior on a prediction task with change-points. The reduced model fit the behavioral data better than either the full Bayesian model or a single learning-rate Delta rule. Our fits also suggest that a three-node model can in many cases be sufficient to explain human performance on the task. However, our experiment did not have the power to distinguish models with more that three nodes. Thus, although the results imply that the three-node model is better than the other models we tested, we cannot rule out the possibility that humans use significantly more that three learning rates.</p>
<p>Despite this qualification, it is an intriguing idea that the brain might use just a handful of learning rates. Our theoretical analysis suggests that this scheme would yield only a small cost in performance for the variety of different problems considered here. In this regard, our model can be seen as complementary to recent work showing that in many probabilistic-inference problems faced by humans <xref ref-type="bibr" rid="pcbi.1003150-Vul1">[28]</xref> and pigeons <xref ref-type="bibr" rid="pcbi.1003150-Daw1">[29]</xref>, as few as just one sample from the posterior can be enough to generate good solutions.</p>
<p>It is also interesting to note that, for models with more than one node, the fastest learning rate was always close to one. Such a high learning rate corresponds to a Delta rule that does not integrate any information over time and simply uses the last outcome to form a prediction. This qualitative difference in the behavior of the fastest node could indicate a very different underlying process such as working memory for the last trial as is proposed in <xref ref-type="bibr" rid="pcbi.1003150-Collins1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1003150-Collins2">[31]</xref>.</p>
<p>One situation in which many nodes would be advantageous is the case in which the hazard rate changes as a function of run-length. In this case, only having a few run-lengths available would be problematic, because the changing hazard rate would be difficult to represent. Experiments designed to measure the effects of variable hazard rates on the ability to make predictions might therefore be able to distinguish whether multiple Delta rules are indeed present.</p>
</sec><sec id="s4c">
<title>Is the reduced model biologically plausible?</title>
<p>The question of biological plausibility is always difficult to answer in computational neuroscience. This difficulty is especially true when the focus of the model is at the algorithmic level and is not directly tied to a specific neural architecture, like in this study. Nevertheless, one useful approach to help guide an answer to this question is to associate key components of the algorithm to known neurobiological mechanisms. Here we support the biological plausibility of our reduced model by showing that signatures of all the elements necessary to implement it have been observed in neural data.</p>
<p>In the reduced model, the update of each node uses a simple Delta rule with a fixed learning rate. The ‘Delta’ of such an update rule corresponds to a prediction error, correlates of which have been found throughout the brain, including notably brainstem dopaminergic neurons and their targets, and have been used extensively to model behavioral data <xref ref-type="bibr" rid="pcbi.1003150-Rescorla1">[3]</xref>–<xref ref-type="bibr" rid="pcbi.1003150-Hayden1">[15]</xref>.</p>
<p>More recently, several studies have also shown evidence for representations of different learning rates, as required by the model. Human subjects performing a statistical-learning task used a pair of learning rates, one fast and one slow, that were associated with BOLD activity in two different brain areas, with the hippocampus responsible for slow learning and the striatum for fast learning <xref ref-type="bibr" rid="pcbi.1003150-Bornstein1">[32]</xref>. A related fMRI study showed different temporal integration in one network of brain areas including the amygdala versus another, more sensory network <xref ref-type="bibr" rid="pcbi.1003150-Glscher1">[33]</xref>. Complementary work at the neural level found a reservoir of many different learning rates in three brain regions (anterior cingulate cortex, dorsolateral prefrontal cortex, and the lateral intraparietal area) of monkeys performing a competitive game <xref ref-type="bibr" rid="pcbi.1003150-Bernacchia1">[34]</xref>. Likewise, neural correlates of different learning rates have been identified in each of the ventral tegmental area and habenula <xref ref-type="bibr" rid="pcbi.1003150-BrombergMartin1">[35]</xref>. Finally, outside of the reward system, other fMRI studies using scrambled movies have found evidence for temporal receptive fields of increasingly long time scales (equivalent to decreasingly small learning rates) up the sensory processing hierarchy <xref ref-type="bibr" rid="pcbi.1003150-Hasson1">[36]</xref>.</p>
<p>Applied to our model, these results suggest that each node is implemented in a distinct, although not necessarily anatomically separated, population of neurons. For our task and the above-referenced studies, in which trials last on the order of seconds, we speculate that the mean of a node is encoded in persistent firing of neurons. Alternatively, for tasks requiring learning over longer timescales, other mechanisms such as changes in synaptic weights might play key roles in these computations.</p>
<p>Our model also depends on the run-length distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003150.e306" xlink:type="simple"/></inline-formula>. Functionally, this distribution serves as a weighting function, determining how each of the different nodes (corresponding to different run lengths) contributes to the final prediction. In this regard, the run-length distribution can be thought of as an attentional filter, similar to mechanisms of spatial or feature-based attention, evident in multiple brain regions that enhance the output of certain signals and suppress others. For longer timescales, this kind of weighting process might have analogies to certain mechanisms of perceptual decision-making that involve the readout of appropriate sensory neurons <xref ref-type="bibr" rid="pcbi.1003150-Gold1">[37]</xref>. Intriguingly, these readout mechanisms are thought to be shaped by experience – governed by a Delta-rule learning process – to ultimately enhance the most reliable sensory outputs and suppress the others <xref ref-type="bibr" rid="pcbi.1003150-Law1">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1003150-Law2">[39]</xref>. We speculate that a similar process might help select, from a reservoir of nodes with different learning rates, those that can most effectively solve a particular task.</p>
<p>The brain must also solve another challenge to directly implement the run-length distribution in our model. In particular, the update equation for the weights (<xref ref-type="disp-formula" rid="pcbi.1003150.e169">Eq. 25</xref>) includes a constant of proportionality that serves to normalize the probability distribution. On a computer, ensuring that the run-length distribution is normalized is relatively straightforward: after the update we just divide by the sum of the node weights. In the brain, this procedure requires some kind of global divisive normalization among all areas coding different nodes. While such divisive normalization is thought to occur in the brain <xref ref-type="bibr" rid="pcbi.1003150-Heeger1">[40]</xref>, it may be more difficult to implement over different brain regions that are far apart.</p>
<sec id="s4c1">
<title>Mixture of Delta rules versus direct modulation of learning rate</title>
<p>An alternative account of variability in learning rates is that the brain uses a single Delta rule whose learning rate is modulated directly. This kind of model has been used previously to explain certain behavioral and imaging results in the context of change-point tasks <xref ref-type="bibr" rid="pcbi.1003150-Nassar1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003150-Krugel1">[21]</xref>. A leading candidate for this role is the neuromodulator norepinephrine (NE), which is released from the locus coeruleus (LC) and has been proposed to encode the unexpected uncertainty associated with change-points <xref ref-type="bibr" rid="pcbi.1003150-Yu1">[41]</xref>. The wide-ranging projections of LC, which include most cortical and subcortical structures, and the neuromodulatory properties of NE, which adapts the gain of neural response functions <xref ref-type="bibr" rid="pcbi.1003150-ServanSchreiber1">[42]</xref>, make this system ideally suited to deliver a global signal such as the learning rate. Control of LC could come from top-down projections from anterior cingulate cortex <xref ref-type="bibr" rid="pcbi.1003150-Behrens1">[16]</xref>, amygdala <xref ref-type="bibr" rid="pcbi.1003150-Li1">[43]</xref>, and posterior cingulate cortex <xref ref-type="bibr" rid="pcbi.1003150-Pearson1">[44]</xref>, all of which have been proposed to encode learning rate.</p>
<p>Indirect evidence for this account comes from putative correlates of LC activity such as pupil dilation <xref ref-type="bibr" rid="pcbi.1003150-Li1">[43]</xref> and skin conductance response <xref ref-type="bibr" rid="pcbi.1003150-Li1">[43]</xref> that have been found to correlate with observed learning rate. However, such results are also consistent with our model if we assume that LC signals shifts in attentional focus to Delta rules with shorter learning rates, or a modified version of our model in which the learning rates of the different nodes adapt.</p>
<p>Our model-based analysis of behavioral data provides some evidence in favor of the present model over the fixed learning rate model of Nassar et al. However, because the experiment was not specifically designed to tease apart these two alternatives, and we did not consider every possible implementation of a variable learning rate model, the result should be treated with caution. To fully distinguish between these two accounts will require careful experimentation to determine whether the learning rate of individual neurons (using recordings from animals) or whole brain areas (using fMRI in humans) are variable or are fixed.</p>
</sec></sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003150.s001" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003150.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p>Histograms of fit parameter values for all models. Each column represents a model, with the name of the model given at the top. Each row represents a single variable going, in order from top to bottom: hazard rate, decision noise standard deviation, learning rate 1, learning rate 2 and learning rate 3. Where a particular model does not have a particular parameter that box is left empty.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003150.s002" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003150.s002" position="float" xlink:type="simple"><label>Table S1</label><caption>
<p>Table showing correlation coefficient between simulated and fit parameter values.</p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003150.s003" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003150.s003" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p>Derivation of error relative ground truth.</p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1003150-Bertsekas1"><label>1</label>
<mixed-citation publication-type="other" xlink:type="simple">Bertsekas D, Tsitsiklis JN (1996) Neurodynamic Programming. Belmont, NJ: Athena Scientific.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Sutton1"><label>2</label>
<mixed-citation publication-type="other" xlink:type="simple">Sutton RS, Barto AG (1998) Reinforcement Learning : An Introduction. Cambridge, Massachusetts: The MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Rescorla1"><label>3</label>
<mixed-citation publication-type="other" xlink:type="simple">Rescorla RA,Wagner AR (1972) A Theory of Pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement. In: Black AH, Prokasy WF, editors, Classical conditioning II: current research and theory. New York: Appleton Century Crofts. chapter 3. pp. 64–99.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Miller1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miller</surname><given-names>RR</given-names></name>, <name name-style="western"><surname>Barnet</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Grahame</surname><given-names>NJ</given-names></name> (<year>1995</year>) <article-title>Assessment of the Rescolra-Wagner model</article-title>. <source>Psychological Bulletin</source> <volume>117</volume>: <fpage>363</fpage>–<lpage>386</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Schultz1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name> (<year>1997</year>) <article-title>A Neural Substrate of Prediction and Reward</article-title>. <source>Science</source> <volume>275</volume>: <fpage>1593</fpage>–<lpage>1599</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Holroyd1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Holroyd</surname><given-names>CB</given-names></name>, <name name-style="western"><surname>Coles</surname><given-names>MGH</given-names></name> (<year>2002</year>) <article-title>The Neural Basis of Human Error Processing: Reinforcement Learning, Dopamine, and the Error-Related Negativity</article-title>. <source>Psychological Review</source> <volume>109</volume>: <fpage>679</fpage>–<lpage>709</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Doherty1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doherty</surname><given-names>JO</given-names></name>, <name name-style="western"><surname>Critchley</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Deichmann</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name> (<year>2003</year>) <article-title>Dissociating Valence of Outcome from Behavioral Control in Human Orbital and Ventral Prefrontal Cortices</article-title>. <source>The Journal of Neuroscience</source> <volume>23</volume>: <fpage>7931</fpage>–<lpage>7939</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Brown1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Braver</surname><given-names>TS</given-names></name> (<year>2005</year>) <article-title>Learned Predictions of Error Likelihood in the Anterior Cingulate Cortex</article-title>. <source>Science</source> <volume>307</volume>: <fpage>1118</fpage>–<lpage>1121</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Debener1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Debener</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Ullsperger</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Siegel</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Fiehler</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Cramon</surname><given-names>DYV</given-names></name>, <etal>et al</etal>. (<year>2005</year>) <article-title>Trial-by-Trial Coupling of Concurrent Electroencephalogram and Functional Magnetic Resonance Imaging Identifies the Dynamics of Performance Monitoring</article-title>. <source>The Journal of Neuroscience</source> <volume>25</volume>: <fpage>11730</fpage>–<lpage>11737</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Seo1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seo</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>D</given-names></name> (<year>2007</year>) <article-title>Temporal Filtering of Reward Signals in the Dorsal Anterior Cingulate Cortex during a Mixed-Strategy Game</article-title>. <source>The Journal of Neuroscience</source> <volume>27</volume>: <fpage>8366</fpage>–<lpage>8377</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Matsumoto1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Matsumoto</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name> (<year>2007</year>) <article-title>Lateral habenula as a source of negative reward signals in dopamine neurons</article-title>. <source>Nature</source> <volume>447</volume>: <fpage>1111</fpage>–<lpage>1115</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Matsumoto2"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Matsumoto</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Matsumoto</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Abe</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Tanaka</surname><given-names>K</given-names></name> (<year>2007</year>) <article-title>Medial prefrontal cell activity signaling prediction errors of action values</article-title>. <source>Nature Neuroscience</source> <volume>10</volume>: <fpage>647</fpage>–<lpage>656</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Kennerley1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kennerley</surname><given-names>SW</given-names></name>, <name name-style="western"><surname>Behrens</surname><given-names>TEJ</given-names></name>, <name name-style="western"><surname>Wallis</surname><given-names>JD</given-names></name> (<year>2011</year>) <article-title>Double dissociation of value computations in orbitofrontal and anterior cingulate neurons</article-title>. <source>Nature Neuroscience</source> <volume>14</volume>: <fpage>1581</fpage>–<lpage>1589</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Silvetti1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Silvetti</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Seurinck</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Verguts</surname><given-names>T</given-names></name> (<year>2011</year>) <article-title>Value and prediction error in medial frontal cortex: integrating the single-unit and systems levels of analysis</article-title>. <source>Frontiers in Human Neuroscience</source> <volume>5</volume>: <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Hayden1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hayden</surname><given-names>BY</given-names></name>, <name name-style="western"><surname>Pearson</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Platt</surname><given-names>ML</given-names></name> (<year>2011</year>) <article-title>Neuronal basis of sequential foraging decisions in a patchy environment</article-title>. <source>Nature Neuroscience</source> <volume>14</volume>: <fpage>933</fpage>–<lpage>939</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Behrens1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Behrens</surname><given-names>TEJ</given-names></name>, <name name-style="western"><surname>Woolrich</surname><given-names>MW</given-names></name>, <name name-style="western"><surname>Walton</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>Rushworth</surname><given-names>MFS</given-names></name> (<year>2007</year>) <article-title>Learning the value of information in an uncertain world</article-title>. <source>Nature Neuroscience</source> <volume>10</volume>: <fpage>1214</fpage>–<lpage>1221</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Nassar1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nassar</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Wilson</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Heasly</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Gold</surname><given-names>JI</given-names></name> (<year>2010</year>) <article-title>An Approximately Bayesian Delta-Rule Model Explains the Dynamics of Belief Updating in a Changing Environment</article-title>. <source>The Journal of Neuroscience</source> <volume>30</volume>: <fpage>12366</fpage>–<lpage>12378</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Adams1"><label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">Adams RP, Mackay DJC (2007) Bayesian Online Changepoint Detection. Technical report, Cambridge University, Cambridge.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Fearnhead1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fearnhead</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>Z</given-names></name> (<year>2007</year>) <article-title>On-line inference for multiple changepoint problems</article-title>. <source>J R Statist Soc B</source> <volume>69</volume>: <fpage>589</fpage>–<lpage>605</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Wilson1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Nassar</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Gold</surname><given-names>JI</given-names></name> (<year>2010</year>) <article-title>Bayesian Online Learning of the Hazard Rate in Change-Point Problems</article-title>. <source>Neural Computation</source> <volume>2476</volume>: <fpage>2452</fpage>–<lpage>2476</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Krugel1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krugel</surname><given-names>LK</given-names></name>, <name name-style="western"><surname>Biele</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Mohr</surname><given-names>PNC</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>SC</given-names></name>, <name name-style="western"><surname>Heekeren</surname><given-names>HR</given-names></name> (<year>2009</year>) <article-title>Genetic variation in dopaminergic neuromodulation inuences the ability to rapidly and exibly</article-title>. <source>PNAS</source> <volume>106</volume>: <fpage>17951</fpage>–<lpage>17956</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Barry1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barry</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Hartigan</surname><given-names>D</given-names></name> (<year>1992</year>) <article-title>Product Partition Models for Change Point Problems</article-title>. <source>The Annals of Statistics</source> <volume>20</volume>: <fpage>260</fpage>–<lpage>279</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Wainwright1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wainwright</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name> (<year>2008</year>) <article-title>Graphical Models, Exponential Families, and Variational Inference</article-title>. <source>Machine Learning</source> <volume>1</volume>: <fpage>1</fpage>–<lpage>305</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Nassar2"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nassar</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Rumsey</surname><given-names>KM</given-names></name>, <name name-style="western"><surname>Wilson</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Parikh</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Heasly</surname><given-names>B</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Rational regulation of learning dynamics by pupil-linked arousal systems</article-title>. <source>Nature Neuroscience</source> <volume>15</volume>: <fpage>1040</fpage>–<lpage>1046</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Schwarz1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwarz</surname><given-names>G</given-names></name> (<year>1978</year>) <article-title>Estimating the dimension of a model</article-title>. <source>The Annals of Statistics</source> <volume>6</volume>: <fpage>461</fpage>–<lpage>464</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Stephan1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Penny</surname><given-names>WD</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Moran</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2009</year>) <article-title>Bayesian model selection for group studies</article-title>. <source>Neuroimage</source> <volume>46</volume>: <fpage>1004</fpage>–<lpage>17</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Steyvers1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Steyvers</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Wagenmakers</surname><given-names>EJ</given-names></name> (<year>2009</year>) <article-title>A Bayesian analysis of human decision-making on bandit problems</article-title>. <source>Journal of Mathematical Psychology</source> <volume>53</volume>: <fpage>168</fpage>–<lpage>179</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Vul1"><label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Vul E, Goodman ND, Griffiths TL, Tenenbaum JB (2008) One and Done? Optimal Decisions From Very Few Samples. In: Proceedings of the 31st Annual Conference of the Cognitive Science Society.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Daw1"><label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Daw ND, Courville AC (2008) The pigeon as particle filter. In: Platt J, Koller D, Singer Y, Roweis S, editors, Advances in Neural Information Processing Systems 20, Cambridge, MA: MIT Press. pp. 369–376.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Collins1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collins</surname><given-names>AGE</given-names></name>, <name name-style="western"><surname>Frank</surname><given-names>MJ</given-names></name> (<year>2012</year>) <article-title>How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis</article-title>. <source>Eur J Neurosci</source> <volume>35</volume>: <fpage>1024</fpage>–<lpage>35</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Collins2"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collins</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Koechlin</surname><given-names>E</given-names></name> (<year>2012</year>) <article-title>Reasoning, learning, and creativity: frontal lobe function and human decision-making</article-title>. <source>PLoS Biol</source> <volume>10</volume>: <fpage>e1001293</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Bornstein1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bornstein</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name> (<year>2012</year>) <article-title>Dissociating hippocampal and striatal contributions to sequential prediction learning</article-title>. <source>European Journal of Neuroscience</source> <volume>35</volume>: <fpage>1011</fpage>–<lpage>1023</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Glscher1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gläscher</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Büchel</surname><given-names>C</given-names></name> (<year>2005</year>) <article-title>Formal learning theory dissociates brain regions with different temporal integration</article-title>. <source>Neuron</source> <volume>47</volume>: <fpage>295</fpage>–<lpage>306</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Bernacchia1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bernacchia</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Seo</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name> (<year>2011</year>) <article-title>A reservoir of time constants for memory traces in cortical neurons</article-title>. <source>Nature Neuroscience</source> <volume>14</volume>: <fpage>366</fpage>–<lpage>372</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-BrombergMartin1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bromberg-Martin</surname><given-names>ES</given-names></name>, <name name-style="western"><surname>Matsumoto</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Nakahara</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name> (<year>2010</year>) <article-title>Multiple Timescales of Memory in Lateral Habenula and Dopamine Neurons</article-title>. <source>Neuron</source> <volume>67</volume>: <fpage>499</fpage>–<lpage>510</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Hasson1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hasson</surname><given-names>U</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Vallines</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Heeger</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Rubin</surname><given-names>N</given-names></name> (<year>2008</year>) <article-title>A Hierarchy of Temporal Receptive Windows in Human Cortex</article-title>. <source>Journal of Neuroscience</source> <volume>28</volume>: <fpage>2539</fpage>–<lpage>2550</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Gold1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gold</surname><given-names>JI</given-names></name>, <name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name> (<year>2007</year>) <article-title>The neural basis of decision making</article-title>. <source>Annu Rev Neurosci</source> <volume>30</volume>: <fpage>535</fpage>–<lpage>74</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Law1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Law</surname><given-names>CT</given-names></name>, <name name-style="western"><surname>Gold</surname><given-names>JI</given-names></name> (<year>2008</year>) <article-title>Neural correlates of perceptual learning in a sensory-motor, but not a sensory, cortical area</article-title>. <source>Nat Neurosci</source> <volume>11</volume>: <fpage>505</fpage>–<lpage>13</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Law2"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Law</surname><given-names>CT</given-names></name>, <name name-style="western"><surname>Gold</surname><given-names>JI</given-names></name> (<year>2009</year>) <article-title>Reinforcement learning can account for associative and perceptual learning on a visual-decision task</article-title>. <source>Nat Neurosci</source> <volume>12</volume>: <fpage>655</fpage>–<lpage>63</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Heeger1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Heeger</surname><given-names>D</given-names></name> (<year>1993</year>) <article-title>Modeling simple-cell direction selectivity with normalized, half-squared, linear operators</article-title>. <source>Journal of Neurophysiology</source> <volume>70</volume>: <fpage>1885</fpage>–<lpage>1898</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Yu1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>2005</year>) <article-title>Uncertainty, Neuromodulation, and Attention</article-title>. <source>Neuron</source> <volume>46</volume>: <fpage>681</fpage>–<lpage>692</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-ServanSchreiber1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Servan-Schreiber</surname><given-names>AD</given-names></name>, <name name-style="western"><surname>Printz</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Cohen</surname><given-names>JD</given-names></name> (<year>1990</year>) <article-title>Reports A Network Model of Catecholamine Effects: Gain, Signal-to-Noise Ratio, and Behavior</article-title>. <source>Science</source> <volume>249</volume>: <fpage>892</fpage>–<lpage>895</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Li1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Schiller</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Phelps</surname><given-names>EA</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name> (<year>2011</year>) <article-title>Differential roles of human striatum and amygdala in associative learning</article-title>. <source>Nature Neuroscience</source> <volume>14</volume>: <fpage>1250</fpage>–<lpage>1252</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003150-Pearson1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pearson</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Heilbronner</surname><given-names>SR</given-names></name>, <name name-style="western"><surname>Barack</surname><given-names>DL</given-names></name>, <name name-style="western"><surname>Hayden</surname><given-names>BY</given-names></name>, <name name-style="western"><surname>Platt</surname><given-names>ML</given-names></name> (<year>2011</year>) <article-title>Posterior cingulate cortex: adapting behavior to a changing world</article-title>. <source>Trends in Cognitive Sciences</source> <volume>15</volume>: <fpage>143</fpage>–<lpage>151</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>