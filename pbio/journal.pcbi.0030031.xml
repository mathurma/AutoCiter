<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN"><front><journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="publisher">pcbi</journal-id><journal-id journal-id-type="flc">plcb</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1371/journal.pcbi.0030031</article-id><article-id pub-id-type="publisher-id">06-PLCB-RA-0472R2</article-id><article-id pub-id-type="sici">plcb-03-02-08</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology</subject><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="System Taxonomy"><subject>Primates</subject></subj-group></article-categories><title-group><article-title>Unsupervised Learning of Visual Features through Spike Timing Dependent Plasticity</article-title><alt-title alt-title-type="running-head">STDP-Based Visual Feature Learning</alt-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Masquelier</surname><given-names>Timothée</given-names></name><xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref><xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref><xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Thorpe</surname><given-names>Simon J</given-names></name><xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref><xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref></contrib></contrib-group><aff id="aff1">
				<label>1</label><addr-line> Centre de Recherche Cerveau et Cognition, Centre National de la Recherche Scientifique, Université Paul Sabatier, Faculté de Médecine de Rangueil, Toulouse, France
			</addr-line></aff><aff id="aff2">
				<label>2</label><addr-line> SpikeNet Technology SARL, Labege, France
			</addr-line></aff><contrib-group><contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes><fn fn-type="con" id="ack1"><p>TM and SJT conceived and designed the experiments, TM performed the experiments and analyzed the data, and TM and SJT wrote the paper.</p></fn><corresp id="cor1">* To whom correspondence should be addressed. E-mail: <email xlink:type="simple">timothee.masquelier@alum.mit.edu</email></corresp><fn fn-type="conflict" id="ack3"><p> The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="ppub"><month>2</month><year>2007</year></pub-date><pub-date pub-type="epub"><day>16</day><month>2</month><year>2007</year></pub-date><pub-date pub-type="epreprint"><day>2</day><month>1</month><year>2007</year></pub-date><volume>3</volume><issue>2</issue><elocation-id>e31</elocation-id><history><date date-type="received"><day>10</day><month>11</month><year>2006</year></date><date date-type="accepted"><day>2</day><month>1</month><year>2007</year></date></history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2007</copyright-year><copyright-holder>Masquelier and Thorpe</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract><p>Spike timing dependent plasticity (STDP) is a learning rule that modifies synaptic strength as a function of the relative timing of pre- and postsynaptic spikes. When a neuron is repeatedly presented with similar inputs, STDP is known to have the effect of concentrating high synaptic weights on afferents that systematically fire early, while postsynaptic spike latencies decrease. Here we use this learning rule in an asynchronous feedforward spiking neural network that mimics the ventral visual pathway and shows that when the network is presented with natural images, selectivity to intermediate-complexity visual features emerges. Those features, which correspond to prototypical patterns that are both salient and consistently present in the images, are highly informative and enable robust object recognition, as demonstrated on various classification tasks. Taken together, these results show that temporal codes may be a key to understanding the phenomenal processing speed achieved by the visual system and that STDP can lead to fast and selective responses.</p></abstract><abstract abstract-type="summary"><title>Author Summary</title><sec id="st1"><title/><p>The paper describes a new biologically plausible mechanism for generating intermediate-level visual representations using an unsupervised learning scheme. These representations can then be used very effectively to perform categorization tasks using natural images. While the basic hierarchical architecture of the system is fairly similar to a number of other recent proposals, the key differences lie in the level of description that is used—individual neurons and spikes—and in the sort of coding scheme involved. Essentially, we have found that a combination of a temporal coding scheme where the most strongly activated neurons fire first with spike timing dependent plasticity leads to a situation where neurons in higher order visual areas will gradually become selective to frequently occurring feature combinations. At the same time, their responses become more and more rapid. We firmly believe that such mechanisms are a key to understanding the remarkable efficiency of the primate visual system.</p></sec></abstract><funding-group><funding-statement>This research was supported by CNRS, STREP Decisions-in-Motion (IST-027198), and SpikeNet Technology SARL.</funding-statement></funding-group><counts><page-count count="11"/></counts><!--===== Restructure custom-meta-wrap to custom-meta-group =====--><custom-meta-group><custom-meta><meta-name>citation</meta-name><meta-value>Masquelier T, Thorpe SJ (2007) Unsupervised learning of visual features through spike timing dependent plasticity. PLoS Comput Biol 3(2): e31. DOI: <ext-link ext-link-type="doi" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.0030031" xlink:type="simple">10.1371/journal.pcbi.0030031</ext-link></meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Temporal constraints pose a major challenge to models of object recognition in cortex. When two images are simultaneously flashed to the left and right of fixation, human subjects can make reliable saccades to the side where there is a target animal in as little as 120–130 ms [<xref ref-type="bibr" rid="pcbi-0030031-b001">1</xref>]. If we allow 20–30 ms for motor delays in the oculomotor system, this implies that the underlying visual processing can be done in 100 ms or less. In monkeys, recent recordings from inferotemporal cortex (IT) showed that spike counts over time bins as small as 12.5 ms (which produce essentially a binary vector with either ones or zeros) and only about 100 ms after stimulus onset contain remarkably accurate information about the nature of a visual stimulus [<xref ref-type="bibr" rid="pcbi-0030031-b002">2</xref>]. This sort of rapid processing presumably depends on the ability of the visual system to learn to recognize familiar visual forms in an unsupervised manner. Exactly how this learning occurs constitutes a major challenge for theoretical neuroscience. Here we explored the capacity of simple feedforward network architectures that have two key features. First, when stimulated with a flashed visual stimulus, the neurons in the various layers of the system fire asynchronously, with the most strongly activated neurons firing first—a mechanism that has been shown to efficiently encode image information [<xref ref-type="bibr" rid="pcbi-0030031-b003">3</xref>]. Second, neurons at later stages of the system implement spike timing dependent plasticity (STDP), which is known to have the effect of concentrating high synaptic weights on afferents that systematically fire early [<xref ref-type="bibr" rid="pcbi-0030031-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0030031-b005">5</xref>]. We demonstrate that when such a hierarchical system is repeatedly presented with natural images, these intermediate-level neurons will naturally become selective to patterns that are reliably present in the input, while their latencies decrease, leading to both fast and informative responses. This process occurs in an entirely unsupervised way, but we then show that these intermediate features are able to support categorization.</p><p>Our network belongs to the family of feedforward hierarchical convolutional networks, as in [<xref ref-type="bibr" rid="pcbi-0030031-b006">6</xref>–<xref ref-type="bibr" rid="pcbi-0030031-b010">10</xref>]. To be precise, its architecture is inspired from Serre, Wolf, and Poggio's model of object recognition [<xref ref-type="bibr" rid="pcbi-0030031-b006">6</xref>], a model that itself extends HMAX [<xref ref-type="bibr" rid="pcbi-0030031-b007">7</xref>] and performs remarkably well with natural images. Like them, in an attempt to model the increasing complexity and invariance observed along the ventral pathway [<xref ref-type="bibr" rid="pcbi-0030031-b011">11</xref>,<xref ref-type="bibr" rid="pcbi-0030031-b012">12</xref>], we use a four-layer hierarchy (S1–C1–S2–C2) in which simple cells (S) gain their selectivity from a linear sum operation, while complex cells (C) gain invariance from a nonlinear max pooling operation (see <xref ref-type="fig" rid="pcbi-0030031-g001">Figure 1</xref> and <xref ref-type="sec" rid="s4">Methods</xref> for a complete description of our model).</p><fig id="pcbi-0030031-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030031.g001</object-id><label>Figure 1</label><caption><title>Overview of the Five-Layer Feedforward Spiking Neural Network</title><p>As in HMAX [<xref ref-type="bibr" rid="pcbi-0030031-b007">7</xref>], we alternate simple cells that gain selectivity through a sum operation, and complex cells that gain shift and scale invariance through a max operation (which simply consists of propagating the first received spike). Cells are organized in retinotopic maps until the S2 layer (inclusive). S1 cells detect edges. C1 maps subsample S1 maps by taking the maximum response over a square neighborhood. S2 cells are selective to intermediate-complexity visual features, defined as a combination of oriented edges (here we symbolically represented an eye detector and a mouth detector). There is one S1–C1–S2 pathway for each processing scale (not represented). Then C2 cells take the maximum response of S2 cells over all positions and scales and are thus shift- and scale-invariant. Finally, a classification is done based on the C2 cells' responses (here we symbolically represented a face/nonface classifier). In the brain, equivalents of S1 cells may be in V1, S2 cells in V1–V2, S2 cells in V4–PIT, C2 cells in AIT, and the final classifier in PFC. This paper focuses on the learning of C1 to S2 synaptic connections through STDP.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030031.g001" xlink:type="simple"/></fig><p>Nevertheless, our network does not only rely on static nonlinearities: it uses spiking neurons and operates in the temporal domain. At each stage, the time to first spike with respect to stimulus onset (or, to be precise, the rank of the first spike in the spike train, as we will see later) is supposed to be the “key variable,” that is, the variable that contains information and that is indeed read out and processed by downstream neurons. When presented with an image, the first layer's S1 cells, emulating V1 simple cells, detect edges with four preferred orientations, and the more strongly a cell is activated, the earlier it fires. This intensity–latency conversion is in accordance with recordings in V1 showing that response latency decreases with the stimulus contrast [<xref ref-type="bibr" rid="pcbi-0030031-b013">13</xref>,<xref ref-type="bibr" rid="pcbi-0030031-b014">14</xref>] and with the proximity between the stimulus orientation and the cell's preferred orientation [<xref ref-type="bibr" rid="pcbi-0030031-b015">15</xref>]. It has already been shown how such orientation selectivity can emerge in V1 by applying STDP on spike trains coming from retinal ON- and OFF-center cells [<xref ref-type="bibr" rid="pcbi-0030031-b016">16</xref>], so we started our model from V1 orientation-selective cells. We also limit the number of spikes at this stage by introducing competition between S1 cells through a one-winner-take-all mechanism: at a given location—corresponding to one cortical column—only the spike corresponding to the best matching orientation is propagated (sparsity is thus 25% at this stage). Note that <italic>k</italic>-winner-take-all mechanisms are easy to implement in the temporal domain using inhibitory GABA interneurons [<xref ref-type="bibr" rid="pcbi-0030031-b017">17</xref>].</p><p>These S1 spikes are then propagated asynchronously through the feedforward network of integrate-and-fire neurons. Note that within this time-to-first-spike framework, the maximum operation of complex cells simply consists of propagating the first spike emitted by a given group of afferents [<xref ref-type="bibr" rid="pcbi-0030031-b018">18</xref>]. This can be done efficiently with an integrate-and-fire neuron with low threshold that has synaptic connections from all neurons in the group.</p><p>Images are processed one by one, and we limit activity to at most one spike per neuron, that is, only the initial spike wave is propagated. Before presenting a new image, every neuron's potential is reset to zero. We process various scaled versions of the input image (with the same filter size). There is one S1–C1–S2 pathway for each processing scale (not represented on <xref ref-type="fig" rid="pcbi-0030031-g001">Figure 1</xref>). This results in S2 cells with various receptive field sizes (see <xref ref-type="sec" rid="s4">Methods</xref>). Then C2 cells take the maximum response (i.e., first spike) of S2 cells over all positions and scales, leading to position and scale invariant responses.</p><p>This paper explains how STDP can set the C1–S2 synaptic connections, leading to intermediate-complexity visual features, whose equivalent in the brain may be in V4 or IT. STDP is a learning rule that modifies the strength of a neuron's synapses as a function of the precise temporal relations between pre- and postsynaptic spikes: an excitatory synapse receiving a spike before a postsynaptic one is emitted is potentiated (long-term potentiation) whereas its strength is weakened the other way around (long-term depression) [<xref ref-type="bibr" rid="pcbi-0030031-b019">19</xref>]. The amount of modification depends on the delay between these two events: maximal when pre- and postsynaptic spikes are close together, and the effects gradually decrease and disappear with intervals in excess of a few tens of milliseconds [<xref ref-type="bibr" rid="pcbi-0030031-b020">20</xref>–<xref ref-type="bibr" rid="pcbi-0030031-b022">22</xref>]. Note that STDP is in agreement with Hebb's postulate because presynaptic neurons that fired slightly before the postsynaptic neuron are those that “took part in firing it.” Here we used a simplified STDP rule where the weight modification does not depend on the delay between pre- and postsynaptic spikes, and the time window is supposed to cover the whole spike wave (see <xref ref-type="sec" rid="s4">Methods</xref>). We also use 0 and 1 as “soft bounds” (see <xref ref-type="sec" rid="s4">Methods</xref>), ensuring the synapses remain excitatory. Several authors have studied the effect of STDP with Poisson spike trains [<xref ref-type="bibr" rid="pcbi-0030031-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0030031-b023">23</xref>]. Here, we demonstrate STDP's remarkable ability to detect statistical regularities in terms of earliest firing afferent patterns within visual spike trains, despite their very high dimensionality inherent to natural images.</p><p>Visual stimuli are presented sequentially, and the resulting spike waves are propagated through to the S2 layer, where STDP is used. We use restricted receptive fields (i.e., S2 cells only integrate spikes from an <italic>s</italic> × <italic>s</italic> square neighborhood in the C1 maps corresponding to one given processing scale) and weight-sharing (i.e., each <italic>prototype</italic> S2 cell is duplicated in retinotopic maps and at all scales). Starting with a random weight matrix (size = 4 × <italic>s</italic> × <italic>s</italic>), we present the first visual stimuli. Duplicated cells are all integrating the spike train and compete with each other. If no cell reaches its threshold, nothing happens and we process the next image. Otherwise for each prototype the first duplicate to reach its threshold is the winner. A one-winner-take-all mechanism prevents the other duplicated cells from firing. The winner thus fires and the STDP rule is triggered. Its weight matrix is updated, and the change in weights is duplicated at all positions and scales. This allows the system to learn patterns despite changes in position and size in the training examples. We also use local inhibition between different prototype cells: when a cell fires at a given position and scale, it prevents all other cells from firing later at the same scale and within an <italic>s</italic>/2 × <italic>s</italic>/2 square neighborhood of the firing position. This competition, only used in the learning phase, prevents all the cells from learning the same pattern. Instead, the cell population self-organizes, each cell trying to learn a distinct pattern so as to cover the whole variability of the inputs.</p><p>If the stimuli have visual features in common (which should be the case if, for example, they contain similar objects), the STDP process will extract them. That is, for some cells we will observe convergence of the synaptic weights (by saturation), which end up being either close to 0 or to 1. During the convergence process, synapses compete for control of the timing of postsynaptic spikes [<xref ref-type="bibr" rid="pcbi-0030031-b004">4</xref>]. The winning synapses are those through which the earliest spikes arrive (on average) [<xref ref-type="bibr" rid="pcbi-0030031-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0030031-b005">5</xref>], and this is true even in the presence of jitter and spontaneous activity [<xref ref-type="bibr" rid="pcbi-0030031-b005">5</xref>] (although the model presented in this paper is fully deterministic). This “preference” for the earliest spikes is a key point since the earliest spikes, which correspond in our framework to the most salient regions of an image, have been shown to be the most informative [<xref ref-type="bibr" rid="pcbi-0030031-b003">3</xref>]. During the learning, the postsynaptic spike latency decreases [<xref ref-type="bibr" rid="pcbi-0030031-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0030031-b005">5</xref>,<xref ref-type="bibr" rid="pcbi-0030031-b024">24</xref>]. After convergence, the responses become selective (in terms of latency) [<xref ref-type="bibr" rid="pcbi-0030031-b005">5</xref>] to visual features of intermediate complexity similar to the features used in earlier work [<xref ref-type="bibr" rid="pcbi-0030031-b008">8</xref>]. Features can now be defined as clusters of afferents that are consistently among the earliest to fire. STDP detects these kinds of statistical regularities among the spike trains and creates one unit for each distinct pattern.</p></sec><sec id="s2"><title>Results</title><p>We evaluated our STDP-based learning algorithm on two California Institute of Technology datasets, one containing faces and the other motorbikes, and a distractor set containing backgrounds, all available at <ext-link ext-link-type="uri" xlink:href="http://www.vision.caltech.edu" xlink:type="simple">http://www.vision.caltech.edu</ext-link> (see <xref ref-type="fig" rid="pcbi-0030031-g002">Figure 2</xref> for sample pictures). Note that most of the images are not segmented. Each dataset was split into a training set, used in the learning phase, and a testing set, not seen during the learning phase but used afterward to evaluate the performance on novel images. This standard cross-validation procedure allows the measurement of the system's ability to <italic>generalize,</italic> as opposed to learning the specific training examples. The splits used were the same as Fergus, Perona, and Zisserman [<xref ref-type="bibr" rid="pcbi-0030031-b025">25</xref>]. All images were rescaled to be 300 pixels in height (preserving the aspect ratio) and converted to grayscale values.</p><fig id="pcbi-0030031-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030031.g002</object-id><label>Figure 2</label><caption><title>Sample Pictures from the Caltech Datasets</title><p>The top row shows examples of faces (all unsegmented), the middle row shows examples of motorbikes (some are segmented, others are not), and the bottom row shows examples of distractors.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030031.g002" xlink:type="simple"/></fig><p>We first applied our unsupervised STDP-based algorithm on the face and motorbike training examples (separately), presented in random order, to build two sets of ten class-specific C2 features. Each C2 cell has one preferred input, defined as a combination of edges (represented by C1 cells). Note that many gray-level images may lead to this combination of edges because of the local max operation of C1 cells and because we lose the “polarity” information (i.e., which side of the edge is darker). However, we can reconstruct a representation of the set of preferred images by convolving the weight matrix with a set of kernels representing oriented bars. Since we start with random weight matrices, at the beginning of the learning process the reconstructed preferred stimuli do not make much sense. But as the cells learn, structured representations emerge, and we are usually able to identify the nature of the cells' preferred stimuli. <xref ref-type="fig" rid="pcbi-0030031-g003">Figures 3</xref> and <xref ref-type="fig" rid="pcbi-0030031-g004">4</xref> show the reconstructions at various stages of learning for the face and motorbike datasets, respectively. We stopped the learning after 10,000 presentations.</p><fig id="pcbi-0030031-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030031.g003</object-id><label>Figure 3</label><caption><title>Evolution of Reconstructions for Face Features</title><p>At the top is the number of postsynaptic spikes emitted. Starting from random preferred stimuli, cells detect statistical regularities among the input visual spike trains after a few hundred discharges and progressively develop selectivity to those patterns. A few hundred more discharges are needed to reach a stable state. Furthermore, the population of cells self-organizes, with each cell effectively trying to learn a distinct pattern so as to cover the whole variability of the inputs.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030031.g003" xlink:type="simple"/></fig><fig id="pcbi-0030031-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030031.g004</object-id><label>Figure 4</label><caption><title>Evolution of Reconstructions for Motorbike Features</title></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030031.g004" xlink:type="simple"/></fig><p>Then we turned off the STDP rule and tested these STDP-obtained features' ability to support face/nonface and motorbike/nonmotorbike classification. This paper focuses more on feature extraction than on sophisticated classification methods, so we first used a very simple decision rule based on the number of C2 cells that fired with each test image, on which a threshold is applied. Such a mechanism could be easily implemented in the brain. The threshold was set at the equilibrium point (i.e., when the false positive rate equals the missed rate). In <xref ref-type="table" rid="pcbi-0030031-t001">Table 1</xref> we report good classification results with this “simple-count” scheme in terms of area under the receiver operator characteristic (ROC) and the performance rate at equilibrium point.</p><table-wrap content-type="2col" id="pcbi-0030031-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030031.t001</object-id><label>Table 1</label><caption><p>Classification Results</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030031.t001" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb1col1" align="left" charoff="0" char=""/><col id="tb1col2" align="left" charoff="0" char=""/><col id="tb1col3" align="left" charoff="0" char=""/><col id="tb1col4" align="left" charoff="0" char=""/><col id="tb1col5" align="left" charoff="0" char=""/><col id="tb1col6" align="left" charoff="0" char=""/><col id="tb1col7" align="left" charoff="0" char=""/><col id="tb1col8" align="left" charoff="0" char=""/><col id="tb1col9" align="left" charoff="0" char=""/></colgroup><thead><tr><td align="left" rowspan="2"><hr/>Model</td><td colspan="2"><hr/>STDP Features (Simple Count)</td><td colspan="2"><hr/>STDP Features (Potential &plus; RBF)</td><td colspan="2"><hr/>Hebbian Features</td><td colspan="2"><hr/>Serre, Wolf, and Poggio</td></tr><tr><td><hr/>Equilibrium Point</td><td><hr/>ROC</td><td><hr/>Equilibrium Point</td><td><hr/>ROC</td><td><hr/>Equilibrium Point</td><td><hr/>ROC</td><td><hr/>Equilibrium Point</td><td><hr/>ROC</td></tr></thead><tbody><tr><td>Faces</td><td>96.5</td><td>99.1</td><td>99.1</td><td>100.0</td><td>96.9</td><td>99.7</td><td>98.2</td><td>99.8</td></tr><tr><td>Motorbikes</td><td>95.4</td><td>98.4</td><td>97.8</td><td>99.7</td><td>96.5</td><td>99.3</td><td>98</td><td>99.8</td></tr></tbody></table> --><!-- --></table-wrap><p>We also evaluated a more complicated classification scheme. C2 cells' thresholds were supposed to be infinite, and we measured the final potentials they reached after having integrated the whole spike train generated by the image. This final potential can be seen as the number of early spikes in common between a current input and a stored prototype (this contrasts with HMAX and extensions [<xref ref-type="bibr" rid="pcbi-0030031-b006">6</xref>,<xref ref-type="bibr" rid="pcbi-0030031-b007">7</xref>,<xref ref-type="bibr" rid="pcbi-0030031-b026">26</xref>], where a Euclidian distance or a normalized dot product is used to measure the difference between a stored prototype and a current input). Note that this potential is contrast invariant: a change in contrast will shift all the latencies but will preserve the spike order. The final potentials reached with the training examples were used to train a radial basis function (RBF) classifier (see <xref ref-type="sec" rid="s4">Methods</xref>). We chose this classifier because linear combination of Gaussian-tuned units is hypothesized to be a key mechanism for generalization in the visual system [<xref ref-type="bibr" rid="pcbi-0030031-b027">27</xref>]. We then evaluated the RBF on the testing sets. As can be seen in <xref ref-type="table" rid="pcbi-0030031-t001">Table 1</xref>, performance with this “potential + RBF” scheme was better.</p><p>Using only ten STDP-learnt features, we reached on those two classes a performance that is comparable to that of Serre, Wolf, and Poggio's model, which itself is close to the best state-of-the-art computer vision systems [<xref ref-type="bibr" rid="pcbi-0030031-b006">6</xref>]. However, their system is more generic. Classes with more intraclass variability (for example, animals) appear to pose a problem with our approach because a lot of training examples (say a few tens) of a given feature type are needed for the STDP process to learn it properly.</p><p>Our approach leads to the extraction of a small set (here ten) of highly informative class-specific features. This is in contrast with Serre et al.'s approach where many more (usually about a thousand) features are used. Their sets are more generic and are suitable for many different classes [<xref ref-type="bibr" rid="pcbi-0030031-b006">6</xref>]. They rely on the final classifier to “select” diagnostic features and appropriately weight them for a given classification task. Here, STDP will naturally focus on what is common to the positive training set, that is, target object features. The background is generally not learned (at least not in priority), since backgrounds are almost always too different from one image to another for the STDP process to converge. Thus, we directly extract diagnostic features, and we can obtain reasonably good classification results using only a threshold on the number of detected features. Furthermore, as STDP performs vector quantization from multiple examples as opposed to “one-shot learning,” it will not learn the noise, nor anything too specific to a given example, with the result that it will tend to learn archetypical features.</p><p>Another key point is the natural trend of the algorithm to learn salient regions, simply because they correspond to the earliest spikes, with the result that neurons whose receptive fields cover salient regions are likely to reach their threshold (and trigger the STDP rule) before neurons “looking” at other regions. This contrasts with more classical competitive learning approaches, where input normalization helps different input patterns to be equally effective in the learning process [<xref ref-type="bibr" rid="pcbi-0030031-b028">28</xref>]. Note that “salient” means within our network “with well-defined contrasted edges,” but saliency is a more generic concept of local differences, for example, in intensity, color, or orientations as in the model of Itti, Koch, and Niebur [<xref ref-type="bibr" rid="pcbi-0030031-b029">29</xref>]. We could use other types of S1 cells to detect other types of saliency, and, provided we apply the same intensity–latency conversion, STDP would still focus on the most salient regions. Saliency is known to drive attention (see [<xref ref-type="bibr" rid="pcbi-0030031-b030">30</xref>] for a review). Our model predicts that it also drives the learning. Future experimental work will test this prediction.</p><p>Of course, in real life we are unlikely to see many examples of a given category in a row. That is why we performed a second simulation, where 20 C2 cells were presented with the face, motorbike, and background training pictures in random order, and the STDP rule was applied. <xref ref-type="fig" rid="pcbi-0030031-g005">Figure 5</xref> shows all the reconstructions for this mixed simulation after 20,000 presentations. We see that the 20 cells self-organized, some of them having developed selectivity to face features, and others to motorbike features. Interestingly, during the learning process the cells rapidly showed a preference for one category. After a certain degree of selectivity had been reached, the face-feature learning was not influenced by the presentation of motorbikes (and vice versa), simply because face cells will not fire (and trigger the STDP rule) on motorbikes. Again we tested the quality of these features with a (multiclass) classification task, using an RBF network and a “one-versus-all” approach (see <xref ref-type="sec" rid="s4">Methods</xref>). As before, we tested two implementations: one based on “binary detections + RBF” and one based on “potential + RBF”. Note that a simple detection count cannot work here, as we need at least some supervised learning to know which feature (or feature combination) is diagnostic (or antidiagnostic) of which class. <xref ref-type="table" rid="pcbi-0030031-t002">Table 2</xref> shows the confusion matrices obtained on the testing sets for both implementations, leading, respectively, to 95.0% and 97.7% of correct classifications on average. It is worth mentioning that the “potential + RBF” system perfectly discriminated between faces and motorbikes—although both were presented in the unsupervised STDP-based learning phase.</p><fig id="pcbi-0030031-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030031.g005</object-id><label>Figure 5</label><caption><title>Final Reconstructions for the 20 Features in the Mixed Case</title><p>The 20 cells self-organized, some having developed selectivity to face features, and some to motorbike features.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030031.g005" xlink:type="simple"/></fig><table-wrap content-type="2col" id="pcbi-0030031-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030031.t002</object-id><label>Table 2</label><caption><p>Confusion Matrices</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030031.t002" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb2col1" align="left" charoff="0" char=""/><col id="tb2col2" align="left" charoff="0" char=""/><col id="tb2col3" align="left" charoff="0" char=""/><col id="tb2col4" align="left" charoff="0" char=""/><col id="tb2col5" align="left" charoff="0" char=""/><col id="tb2col6" align="left" charoff="0" char=""/><col id="tb2col7" align="left" charoff="0" char=""/><col id="tb2col8" align="left" charoff="0" char=""/><col id="tb2col9" align="left" charoff="0" char=""/><col id="tb2col10" align="left" charoff="0" char=""/></colgroup><thead><tr><td align="left" rowspan="2"><hr/>Predicted with:</td><td colspan="3"><hr/>STDP Features (Binary Detections)</td><td colspan="3"><hr/>STDP Features (Potential)</td><td colspan="3"><hr/>Hebbian Features</td></tr><tr><td><hr/>Face</td><td><hr/>Motorbike</td><td><hr/>Background</td><td><hr/>Face</td><td><hr/>Motorbike</td><td><hr/>Background</td><td><hr/>Face</td><td><hr/>Motorbike</td><td><hr/>Background</td></tr></thead><tbody><tr><td>Actual Face</td><td>97.2</td><td>0.5</td><td>2.3</td><td>98.2</td><td>0</td><td>1.8</td><td>97.7</td><td>0</td><td>2.3</td></tr><tr><td>Actual Motorbike</td><td>0</td><td>95.3</td><td>4.8</td><td>0</td><td>97.5</td><td>2.5</td><td>0.3</td><td>96.3</td><td>3.5</td></tr><tr><td>Actual Background</td><td>3.1</td><td>4.4</td><td>92.4</td><td>0.4</td><td>2.2</td><td>97.3</td><td>4.9</td><td>3.6</td><td>91.6</td></tr></tbody></table> --><!-- --></table-wrap><p>A third type of simulation was run to illustrate the STDP learning process. For these simulations, only three C2 cells and four processing scales (71%, 50%, 35%, and 25%) were used. We let at most one cell fire at each processing scale. The rest of the parameters were strictly identical to the other simulations (see <xref ref-type="sec" rid="s4">Methods</xref>). <xref ref-type="supplementary-material" rid="pcbi-0030031-sv001">Videos S1</xref>–<xref ref-type="supplementary-material" rid="pcbi-0030031-sv003">S3</xref> illustrate the STDP learning process with, respectively, faces, motorbikes, and a mix of faces, motorbikes, and background pictures. It can be seen that after convergence the STDP feature showed a good tradeoff between selectivity (very few false alarms) and invariance (most of the targets were recognized).</p><p>An interesting control is to compare the STDP learning rule with a more standard hebbian rule in this precise framework. For this purpose, we converted the spike trains coming from C1 cells into a vector of (real-valued) C1 activities <italic>X</italic><sub>C1</sub>, supposed to correspond to firing rates (see <xref ref-type="sec" rid="s4">Methods</xref>). Each S2 cell was no longer modeled at the integrate-and-fire level but was supposed to respond with a (static) firing rate <italic>Y</italic><sub>S2</sub> given by the normalized dot product:
				<disp-formula id="pcbi-0030031-e001"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030031.e001" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&sdot;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math> --></disp-formula>where <italic>W</italic><sub>S2</sub> is the synaptic weight vector of the S2 cell (see <xref ref-type="sec" rid="s4">Methods</xref>).
			</p><p>The S2 cells still competed with each other, but the <italic>k</italic>-winner-take-all mechanisms now selected the cells with the highest firing rates (instead of the first one to fire). Only the cells whose firing rates reached a certain threshold were considered in the competition (see <xref ref-type="sec" rid="s4">Methods</xref>). The winners now triggered the following modified hebbian rule (instead of STDP):
				<disp-formula id="pcbi-0030031-e002"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030031.e002" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>&delta;</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&equals;</mml:mo><mml:mi>a</mml:mi><mml:mo>&middot;</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&middot;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>where a decay term has been added to keep the weight vector bounded (however, the rule is still local, unlike an explicit weight normalization). Note that this precaution was not needed in the STDP case because competition between synapse naturally bounds the weight vector [<xref ref-type="bibr" rid="pcbi-0030031-b004">4</xref>]. The rest of the network is strictly identical to the STDP case.
			</p><p><xref ref-type="fig" rid="pcbi-0030031-g006">Figure 6</xref> shows the reconstruction of the preferred stimuli for the ten C2 cells after 10,000 presentations for the face stimuli (<xref ref-type="fig" rid="pcbi-0030031-g006">Figure 6</xref>, top) and the motorbikes stimuli (<xref ref-type="fig" rid="pcbi-0030031-g006">Figure 6</xref>, top). Again we can usually recognize the face and motorbike parts to which the cells became selective (even though the reconstructions look fuzzier than in the STDP case because the final weights are more graded). We also tested the ability of these hebbian-obtained features to support face/nonface and motorbike/nonmotorbike classification once fed into an RBF, and the results are shown in <xref ref-type="table" rid="pcbi-0030031-t001">Table 1</xref> (last column). We also evaluated the hebbian features with the multiclass setup. Twenty cells were presented with the same mix of face, motorbike, and background pictures as before. <xref ref-type="fig" rid="pcbi-0030031-g007">Figure 7</xref> shows the final reconstructions after 20,000 presentations, and <xref ref-type="table" rid="pcbi-0030031-t002">Table 2</xref> shows the confusion matrix (last columns).</p><fig id="pcbi-0030031-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030031.g006</object-id><label>Figure 6</label><caption><title>Hebbian Learning</title><p>(Top) Final reconstructions for the ten face features.</p><p>(Bottom) The ten motorbike features.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030031.g006" xlink:type="simple"/></fig><fig id="pcbi-0030031-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030031.g007</object-id><label>Figure 7</label><caption><title>Hebbian Learning: Final Reconstructions for the 20 Features in the Mixed Case</title><p>As with STDP-based learning, the 20 cells self-organized, some having developed selectivity to face features, and some to motorbike features.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030031.g007" xlink:type="simple"/></fig><p>The main conclusion is that the modified hebbian rule is also able to extract pertinent features for classification (although performance on these tests appears to be slightly worse). This is not very surprising as STDP can be seen as a hebbian rule transposed in the temporal domain, but it was worth checking. Where STDP would detect (and create selectivity to) sets of units that are consistently among the first one to fire, the hebbian rule detects (and creates selectivity to) sets of units that consistently have the highest firing rates. However, we believe the temporal framework is a better description of what really happens at the neuronal level, at least in ultrarapid categorization tasks. Furthermore, STDP also explains how the system becomes faster and faster with training, since the neurons learn to decode the first information available at their afferents' level (see also <xref ref-type="sec" rid="s3">Discussion</xref>).</p></sec><sec id="s3"><title>Discussion</title><p>While the ability of hierarchical feedforward networks to support classification is now reasonably well established (e.g., [<xref ref-type="bibr" rid="pcbi-0030031-b006">6</xref>–<xref ref-type="bibr" rid="pcbi-0030031-b008">8</xref>,<xref ref-type="bibr" rid="pcbi-0030031-b010">10</xref>]), how intermediate-complexity features can be learned remains an open problem, especially with cluttered images. In the original HMAX model, S2 features were not learned but were manually hardwired [<xref ref-type="bibr" rid="pcbi-0030031-b007">7</xref>]. Later versions used huge sets of random crops (say 1,000) taken from natural images and used these crops to “imprint” S2 cells [<xref ref-type="bibr" rid="pcbi-0030031-b006">6</xref>]. This approach works well but is costly since redundancy is very high between features, and many features are irrelevant for most (if not all) of the tasks. To select only pertinent features for a given task, Ullman proposed an interesting criterion based on mutual information [<xref ref-type="bibr" rid="pcbi-0030031-b008">8</xref>], leaving the question of possible neural implementation open. LeCun showed how visual features in a convolutional network could be learned in a supervised manner using back-propagation [<xref ref-type="bibr" rid="pcbi-0030031-b010">10</xref>], without claiming this algorithm was biologically plausible. Although we may occasionally use supervised learning to create a set of features suitable for a particular recognition task, it seems unrealistic that we need to do that each time we learn a new class. Here we took another approach: one layer with unsupervised competitive learning is used as input for a second layer with supervised learning. Note that this kind of hybrid scheme has been found to learn much faster than a two-layer backpropagation network [<xref ref-type="bibr" rid="pcbi-0030031-b028">28</xref>].</p><p>Our approach is a bottom-up one: instead of intuiting good image-processing schemes and discussing their eventual neural correlates, we took known biological phenomena that occur at the neuronal level, namely integrate-and-fire and STDP, and observed where they could lead at a more integrated level. The role of the simulations with natural images is thus to provide a “plausibility proof” that such mechanisms could be implemented in the brain.</p><p>However, we have made four main simplifications. The first one was to propagate input stimuli one by one. This may correspond to what happens when an image is flashed in an ultrarapid categorization paradigm [<xref ref-type="bibr" rid="pcbi-0030031-b001">1</xref>], but normal visual perception is an ongoing process. However, every 200 ms or 300 ms we typically perform a saccade. The processing of each of these discrete “chunks” seems to be optimized for rapid execution [<xref ref-type="bibr" rid="pcbi-0030031-b031">31</xref>], and we suggest that much can be done with the feedforward propagation of a single spike wave. Furthermore, even when fixating, our eyes are continuously making microsaccades that could again result in repetitive waves of activation. This idea is in accordance with electrophysiological recordings showing that V1 neuron activity is correlated with microsaccades [<xref ref-type="bibr" rid="pcbi-0030031-b032">32</xref>]. Here we assumed the successive waves did not interfere, which does not seem too unreasonable given that the neuronal time constants (integration, leak, STDP window) are in the range of a few tens of milliseconds whereas the interval between saccades and microsaccades is substantially longer. It is also possible that extraretinal signals suppress interference by shutting down any remaining activity before propagating the next wave. Note that this simplification allows us to use nonleaky integrate-and-fire neurons and an infinite STDP time window. More generally, as proposed by Hopfield [<xref ref-type="bibr" rid="pcbi-0030031-b033">33</xref>], waves could be generated by population oscillations that would fire one cell at a time in advance of the maximum of the oscillation, which increases with the inputs the cell received. This idea is in accordance with recordings in area 17 of cat visual cortex showing that suboptimal cells reveal a systematic phase lag relative to optimally stimulated cells [<xref ref-type="bibr" rid="pcbi-0030031-b034">34</xref>].</p><p>The second simplification we have made is to use only five layers (including the classification layer), whereas processing in the ventral stream involves many more layers (probably about ten), and complexity increases more slowly than suggested here. However, STDP as a way to combine simple features into more complex representations, based on statistical regularities among earliest spike patterns, seems to be a very efficient learning rule and could be involved at all stages.</p><p>The third main simplification we have made consists of using restricted receptive fields and weight sharing, as do most of the bio-inspired hierarchical networks [<xref ref-type="bibr" rid="pcbi-0030031-b006">6</xref>–<xref ref-type="bibr" rid="pcbi-0030031-b010">10</xref>] (networks using these techniques are called <italic>convolutional networks</italic>). We built shift and scale invariance by structure (and not by training) by duplicating S1, C1, and S2 cells at all positions and scales. This is a way to reduce the number of free parameters (and therefore the VC dimension [<xref ref-type="bibr" rid="pcbi-0030031-b035">35</xref>]) of the network by incorporating prior information into the network design: responses should be scale- and shift-invariant. This greatly reduces the number of training examples needed. Note that this technique of weight sharing could be applied to other transformations than shifting and scaling, for instance, rotation and symmetry. However, it is difficult to believe that the brain could really use weight sharing since, as noted by Földiák [<xref ref-type="bibr" rid="pcbi-0030031-b036">36</xref>], updating the weights of all the simple units connected to the same complex unit is a nonlocal operation. Instead, he suggested that at least the low-level features could be learned locally and independently. Subsequently, cells with similar preferred stimulus may connect adaptively to the same complex cell, possibly by detecting correlation across time thanks to a trace rule [<xref ref-type="bibr" rid="pcbi-0030031-b036">36</xref>]. Wallis, Rolls, and Milward successfully implemented this sort of mechanism in a multilayered hierarchical network called Vis-Net [<xref ref-type="bibr" rid="pcbi-0030031-b037">37</xref>,<xref ref-type="bibr" rid="pcbi-0030031-b038">38</xref>]; however, performance after learning objects from unsegmented natural images was poor [<xref ref-type="bibr" rid="pcbi-0030031-b039">39</xref>]. Future work will evaluate the use of local learning and adaptative complex pooling in our network, instead of exact weight sharing. Learning will be much slower but should lead to similar STDP features. Note that it seems that monkeys can recognize high-level objects at scales and positions that have not been experienced previously [<xref ref-type="bibr" rid="pcbi-0030031-b002">2</xref>,<xref ref-type="bibr" rid="pcbi-0030031-b040">40</xref>]. It could be that in the brain local learning and adaptative complex pooling are used up to a certain level of complexity, but not for high-level objects. These high-level objects could be represented with a combination of simpler features that would already be shift- and scale-invariant. As a result, there would be less need for spatially specific representations for high-level objects.</p><p>The last main simplification we have made is to ignore both feedback loops and top-down influences. While normal, everyday vision extensively uses feedback loops, the temporal constraints almost certainly rule them out in an ultrarapid categorization task [<xref ref-type="bibr" rid="pcbi-0030031-b041">41</xref>]. The same cannot be said about the top-down signals, which do not depend directly on inputs. For example, there is experimental evidence that the selectivity to the “relevant” features for a given recognition task can be enhanced in IT [<xref ref-type="bibr" rid="pcbi-0030031-b042">42</xref>] and in V4 [<xref ref-type="bibr" rid="pcbi-0030031-b043">43</xref>], possibly thanks to a top-down signal coming from the prefrontal cortex, thought to be involved in the categorization process. These effects, for example, modeled by Szabo et al. [<xref ref-type="bibr" rid="pcbi-0030031-b044">44</xref>], are not taken into account here.</p><p>Despite these four simplifications, we think our model captures two key mechanisms used by the visual system for rapid object recognition. The first one is the importance of the first spikes for rapidly encoding the most important information about a visual stimulus. Given the number of stages involved in high-level recognition and the short latencies of selective responses recorded in monkeys' IT [<xref ref-type="bibr" rid="pcbi-0030031-b002">2</xref>], the time window available for each neuron to perform its computation is probably about 10–20 ms [<xref ref-type="bibr" rid="pcbi-0030031-b045">45</xref>] and will rarely contain more than one or two spikes. The only thing that matters for a neuron is whether an afferent fires early enough so that the presynaptic spike falls in the critical time window, while later spikes cannot be used for ultrarapid categorization. At this point (but only at this point), we have to consider two hypotheses: either presynaptic spike times are completely stochastic (for example, drawn from a Poisson distribution), or they are somewhat reliable. The first hypothesis causes problems since the first presynaptic spikes (again the only ones taken into account) will correspond to a subset of the afferents that is essentially random, and will not contain much information about their real activities [<xref ref-type="bibr" rid="pcbi-0030031-b046">46</xref>]. A solution to this problem is to use populations of redundant neurons (with similar selectivity) to ensure the first presynaptic spikes do correspond on average to the most active populations of afferents. In this work we took the second hypothesis, assuming the time to first spike of the afferents (or, to be precise, their firing order) was reliable and did reflect a level of activity. This second hypothesis receives experimental support. For example, recent recordings in monkeys show that IT neurons' responses in terms of spike count <italic>close to stimulus onset</italic> (100–150 ms time bin) seem to be too reliable to be fit by a typical Poisson firing rate model [<xref ref-type="bibr" rid="pcbi-0030031-b047">47</xref>]. Another recent electrophysiological study in monkeys showed that IT cell's latencies do contain information about the nature of a visual stimulus [<xref ref-type="bibr" rid="pcbi-0030031-b048">48</xref>]. There is also experimental evidence for precise spike time responses in V1 and in many other neuronal systems (see [<xref ref-type="bibr" rid="pcbi-0030031-b049">49</xref>] for a review).</p><p>Very interestingly, STDP provides an efficient way to develop selectivity to first spike patterns, as shown in this work. After convergence, the potential reached by an STDP neuron is linked to the number of early spikes in common between the current input and a stored prototype. This “early spike” versus “later spike” neural code (while the spike order within each bin does not matter) has not only been proven robust enough to perform object recognition in natural images but is fast to read out: an accurate response can be produced when only the earliest afferents have fired. The use of such a mechanism at each stage of the ventral stream could account for the phenomenal processing speed achieved by the visual system.</p></sec><sec id="s4"><title>Materials and Methods</title><p>Here is a detailed description of the network, the STDP model, and the classification methods.</p><sec id="s4a"><title>S1 cells.</title><p>S1 cells detect edges by performing a convolution on the input images. We are using 5 × 5 convolution kernels, which roughly correspond to Gabor filters with wavelength of 5 (i.e., the kernel contains one period), effective width 2, and four preferred orientations: π/8, π/4 + π/8, π/2 + π/8, and 3π/4 + π/8 (π/8 is there to avoid focusing on horizontal and vertical edges, which are seldom diagnostic). We apply those filters to five scaled versions of the original image: 100%, 71%, 50%, 35%, and 25%. There are thus 4 × 5 = 20 S1 maps. S1 cells emit spikes with a latency that is inversely proportional to the absolute value of the convolution (the response is thus invariant to an image negative operation). We also limit activity at this stage: at a given processing scale and location, only the spike corresponding to the best matching orientation is propagated.</p></sec><sec id="s4b"><title>C1 cells.</title><p>C1 cells propagate the first spike emitted by S1 cells in a 7 × 7 square of a given S1 map (which corresponds to one preferred orientation and one processing scale). Two adjacent C1 cells in a C1 map correspond to two 7 × 7 squares of S1 cells shifted by six S1 cells (and thus overlap of one S1 row). C1 maps thus subsample S1 maps. To be precise, neglecting the side effects, there are 6 × 6 = 36 times fewer C1 cells than S1 cells. As proposed by Riesenhuber and Poggio [<xref ref-type="bibr" rid="pcbi-0030031-b007">7</xref>], this maximum operation is a biologically plausible way to gain local shift invariance. From an image processing point of view, it is a way to perform subsampling within retinotopic maps without flattening high spatial frequency peaks (as would be the case with local averaging).</p><p>We also use a local lateral inhibition mechanism at this stage: when a C1 cell emits a spike, it increases the latency of its neighbors within an 11 × 11 square in the map with the same preferred orientation and the same scale. The percentage of latency increase decreases linearly with the distance from the spike, from 15% to 5%. As a result, if a region is clearly dominated by one orientation, cells will inhibit each other and the spike train will be globally late and thus unlikely to be “selected” by STDP.</p></sec><sec id="s4c"><title>S2 cells.</title><p>S2 cells correspond to intermediate-complexity visual features. Here we used ten prototype S2 cell types, and 20 in the mixed simulation. Each prototype cell is duplicated in five maps (weight sharing), each map corresponding to one processing scale. Within those maps, the S2 cells can integrate spikes only from the four C1 maps of the corresponding processing scale. The receptive field size is 16 × 16 C1 cells (neglecting the side effects; this leads to 96 × 96 S1 cells, and the corresponding receptive field size in the original image is [96 / processing scale]<sup>2</sup>). C1–S2 synaptic connections are set by STDP.</p><p>Note that we did not use a leakage term. In the brain, by progressively resetting membrane potentials toward their resting levels, leakiness will decrease the interference between two successive spike waves. In our model we process spike waves one by one and reset all the potentials before each propagation, and so leaks are not needed.</p><p>Finally, activity is limited at this stage: a <italic>k</italic>-winner-take-all strategy ensures at most two cells that can fire for each processing scale. This mechanism, only used in the learning phase, helps the cells to learn patterns with different real sizes. Without it, there is a natural bias toward “small” patterns (i.e., large scales), simply because corresponding maps are larger, and so likeliness of firing with random weights at the beginning of the STDP process is higher.</p></sec><sec id="s4d"><title>C2 cells.</title><p>Those cells take for each prototype the maximum response (i.e., first spike) of corresponding S2 cells over all positions and processing scales, leading to ten shift- and scale-invariant cells (20 in the mixed case).</p></sec><sec id="s4e"><title>STDP model.</title><p>We used a simplified STDP rule:
					<disp-formula id="pcbi-0030031-e003"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030031.e003" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mrow><mml:mo>&lcub;</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>&Delta;</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&equals;</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mo>&plus;</mml:mo></mml:msup><mml:mo>.</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&le;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>&Delta;</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&equals;</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mo>&minus;</mml:mo></mml:msup><mml:mo>.</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math> --></disp-formula>where <italic>i</italic> and <italic>j</italic> refer, respectively, to the post- and presynaptic neurons, <italic>t<sub>i</sub></italic> and <italic>t<sub>j</sub></italic> are the corresponding spike times, Δ<italic>w<sub>ij</sub></italic> is the synaptic weight modification, and <italic>a</italic><sup>+</sup> and <italic>a</italic><sup>−</sup> are two parameters specifying the amount of change. Note that the weight change does not depend on the exact <italic>t<sub>i</sub></italic> − <italic>t<sub>j</sub></italic> value, but only on its sign. We also used an infinite time window. These simplifications are equivalent to assuming that the intensity–latency conversion of S1 cells compresses the whole spike wave in a relatively short time interval (say, 20–30 ms), so that all presynaptic spikes necessarily fall close to the postsynaptic spike time, and the change decrease becomes negligible. In the brain, this change decrease and the limited time window are crucial: they prevent different spike waves coming from different stimuli from interfering in the learning process. In our model, we propagate stimuli one by one, so these mechanisms are not needed. Note that with this simplified STDP rule only the <italic>order</italic> of the spikes matters, not their precise timings. As a result, the intensity–latency conversion function of S1 cells has no impact, and any monotonously decreasing function gives the same results.
				</p><p>The multiplicative term <italic>w<sub>ij</sub></italic> · (1 − <italic>w<sub>ij</sub></italic>) ensures the weight remains in the range [0,1] (excitatory synapses) and implements a soft bound effect: when the weight approaches a bound, weight changes tend toward zero.</p><p>We also applied long-term depression to synapses through which no presynaptic spike arrived, exactly as if a presynaptic spike had arrived after the postsynaptic one. This is useful to eliminate the noise due to original random weights on synapses through which presynaptic spikes never arrive.</p><p> As the STDP learning progresses, we increase <italic>a</italic><sup>+</sup> and <inline-formula id="pcbi-0030031-ex001"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030031.ex001" xlink:type="simple"/></inline-formula>
					 To be precise, we start with <italic>a</italic><sup>+</sup> = 2<sup>−6</sup> and multiply the value by 2 every 400 postsynaptic spikes, until a maximum value of 2<sup>−2</sup>. <italic>a</italic><sup>−</sup> is adjusted so as to keep a fixed <italic>a</italic><sup>+</sup>/<italic>a</italic><sup>−</sup> ratio (−4/3). This allows us to accelerate convergence when the preferred stimulus is somewhat “locked,” whereas directly using high learning rates with the random initial weights leads to erratic results.
				</p><p>We used a threshold of 64 (= 1/4 × 16 × 16). Initial weights are randomly generated, with mean 0.8 and standard deviation 0.05.</p></sec><sec id="s4f"><title>Classification setup.</title><p>We used an RBF network. In the brain, this classification step may be done in the PFC using the outputs of IT. Let <italic>X</italic> be the vector of C2 responses (containing either binary detections with the first implementation or final potentials with the second one). This kind of classifier computes an expression of the form:
					<disp-formula id="pcbi-0030031-e004"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030031.e004" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&sdot;</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&minus;</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>&sigma;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>and then classifies based on whether or not <italic>f</italic>(<italic>X</italic>) reaches a threshold. Supervised learning at this stage involves adjusting the synaptic weights <italic>c</italic> so as to minimize a (regularized) error on the training set [<xref ref-type="bibr" rid="pcbi-0030031-b027">27</xref>]. The <italic>X<sub>i</sub></italic> correspond to C2 responses for some training examples (1/4 of the training set randomly selected). The full training set was used to learn the <italic>c<sub>i</sub></italic>. We used <italic>σ</italic> = 2 and <italic>λ</italic> = 10<sup>−12</sup> (regularization parameter).
				</p><p>The multiclass case was handled with a “one-versus-all approach.” If <italic>n</italic> is the number of classes (here, three), <italic>n</italic> RBF classifiers of the kind “class I” versus “all other classes” are trained. At the time of testing, each one of the <italic>n</italic> classifiers emits a (real-valued) prediction that is linked to the probability of the image belonging to its category. The assigned category is the one that corresponds to the highest prediction value.</p></sec><sec id="s4g"><title>Hebbian learning.</title><p>The spike trains coming from C1 cells were converted into real-valued activities (supposed to correspond to firing rates) by taking the inverse of the first spikes' latencies (note that these activities do not correspond exactly to the convolution values because of the local lateral inhibition mechanism of layer C1). The activities (or firing rates) of S2 units were computed as:
					<disp-formula id="pcbi-0030031-e005"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030031.e005" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&sdot;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math> --></disp-formula>where <italic>W</italic><sub>S2</sub> is the synaptic weight vector of the S2 cell. Note that the normalization causes an S2 cell to respond maximally when the input vector <italic>X</italic><sub>C1</sub> is collinear to its weight vector <italic>W</italic><sub>S2</sub> (neural circuits for such normalization have been proposed in [<xref ref-type="bibr" rid="pcbi-0030031-b027">27</xref>]). Hence <italic>W</italic><sub>S2</sub> (or any vector collinear to it) is the preferred stimulus of the S2 cell. With another stimulus <italic>X</italic><sub>C1</sub> the response is proportional to the cosine between <italic>W</italic><sub>S2</sub> and <italic>X</italic><sub>C1</sub>. This kind of tuning has been used in extensions of HMAX [<xref ref-type="bibr" rid="pcbi-0030031-b026">26</xref>]. It is similar to the Gaussian tuning of the original HMAX [<xref ref-type="bibr" rid="pcbi-0030031-b007">7</xref>], but it is invariant to the norm of the input (i.e., multiplying the input activities by 2 has no effect on the response), which allows us to remain contrast-invariant (see also [<xref ref-type="bibr" rid="pcbi-0030031-b026">26</xref>] for a comparison between the two kinds of tuning).
				</p><p>Only the cells whose activities were above a threshold were considered in the competition process. It was found useful to use individual adaptative thresholds: each time a cell was among the winners, its threshold was set to 0.91 times its activity (this value was tuned to get approximately the same number of weight updates as with STDP). The competition mechanism was exactly the same as before, except that it selected the most active units and not the first one to fire. The winners' weight vectors were updated with the following modified hebbian rule:
					<disp-formula id="pcbi-0030031-e006"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030031.e006" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>&delta;</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&equals;</mml:mo><mml:mi>a</mml:mi><mml:mo>&middot;</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&middot;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math> --></disp-formula>
				</p><p><italic>a</italic> is the learning rate. It was found useful to start with a small learning rate (0.002) and to geometrically increase it every ten iterations. The geometric ratio was set to reach a learning rate of 0.02 after 2,000 iterations, after which the learning rate stayed constant.</p></sec><sec id="s4h"><title>Differences from the model of Serre, Wolf, and Poggio.</title><p>Here we summarize the differences between our model and their model [<xref ref-type="bibr" rid="pcbi-0030031-b006">6</xref>] in terms of architecture (leaving the questions of learning and temporal code aside).</p><p>We process various scaled versions of the input image (with the same filter size), instead of using various filter sizes on the original image: S1 level, only the best matching orientation is propagated; C1 level, we use lateral inhibition (see above); S2 level, the similarity between a current input and the stored prototype is linked to the number of early spikes in common between the corresponding spike trains, while Serre et al. use the Euclidian distance between the corresponding patches of C1 activities.</p><p>We used an RBF network and not a Support Vector Machine.</p></sec></sec><sec id="s5"><title>Supporting Information</title><supplementary-material id="pcbi-0030031-sv001" mimetype="video/quicktime" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030031.sv001" xlink:type="simple"><label>Video S1</label><caption><title>Face-Feature Learning</title><p>Here we presented the face-training examples in random order, propagated the corresponding spike waves, and applied the STDP rule. At the top of the screen, the input image is shown, with red, green, or blue squares indicating the receptive fields of the cells that fired (if any). At the bottom of the screen, we reconstructed the preferred stimuli of the three C2 cells. Above each reconstruction, the number of postsynaptic spikes emitted is shown with the corresponding color. The red, green, and blue cells develop selectivity to a view of, respectively, the bust, the head, and the face.</p><p>(3.3 MB MOV)</p></caption></supplementary-material><supplementary-material id="pcbi-0030031-sv002" mimetype="video/quicktime" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030031.sv002" xlink:type="simple"><label>Video S2</label><caption><title>Motorbike-Feature Learning</title><p>The red cell becomes selective to the front part of a motorbike, while the green and blue cells both become selective to the wheels.</p><p>(6.8 MB MOV)</p></caption></supplementary-material><supplementary-material id="pcbi-0030031-sv003" mimetype="video/quicktime" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030031.sv003" xlink:type="simple"><label>Video S3</label><caption><title>Mixed Case</title><p>The training set consisted of 200 face pictures, 200 motorbike pictures, and 200 background pictures. Notice that the red cell becomes selective to faces and the blue cell to heads, while the green cell illustrates how a given feature (round shape) can be shared by two categories.</p><p>(7.6 MB MOV)</p></caption></supplementary-material></sec></body><back><ack><p>We thank Thomas Serre and Rufin VanRullen for reading the manuscript and making comments.</p></ack><glossary><title>Abbreviations</title><def-list><def-item><term>IT</term><def><p>inferotemporal cortex</p></def></def-item><def-item><term>RBF</term><def><p>radial basis function</p></def></def-item><def-item><term>ROC</term><def><p>receiver operator characteristic</p></def></def-item><def-item><term>STDP</term><def><p>spike timing dependent plasticity</p></def></def-item></def-list></glossary><ref-list><title>References</title><ref id="pcbi-0030031-b001"><label>1</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Kirchner</surname><given-names>H</given-names></name><name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group>
					<year>2006</year>
					<article-title>Ultra-rapid object detection with saccadic eye movements: Visual processing speed revisited.</article-title>
					<source>Vision Res</source>
					<volume>46</volume>
					<fpage>1762</fpage>
					<lpage>1776</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b002"><label>2</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hung</surname><given-names>CP</given-names></name><name name-style="western"><surname>Kreiman</surname><given-names>G</given-names></name><name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name><name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group>
					<year>2005</year>
					<article-title>Fast readout of object identity from macaque inferior temporal cortex.</article-title>
					<source>Science</source>
					<volume>310</volume>
					<fpage>863</fpage>
					<lpage>866</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b003"><label>3</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>VanRullen</surname><given-names>R</given-names></name><name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group>
					<year>2001</year>
					<article-title>Rate coding versus temporal order coding: What the retinal ganglion cells tell the visual cortex.</article-title>
					<source>Neural Comput</source>
					<volume>13</volume>
					<fpage>1255</fpage>
					<lpage>1283</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b004"><label>4</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>S</given-names></name><name name-style="western"><surname>Miller</surname><given-names>KD</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>
					<year>2000</year>
					<article-title>Competitive hebbian learning through spike-timing–dependent synaptic plasticity.</article-title>
					<source>Nat Neurosci</source>
					<volume>3</volume>
					<fpage>919</fpage>
					<lpage>926</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b005"><label>5</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Guyonneau</surname><given-names>R</given-names></name><name name-style="western"><surname>VanRullen</surname><given-names>R</given-names></name><name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group>
					<year>2005</year>
					<article-title>Neurons tune to the earliest spikes through STDP.</article-title>
					<source>Neural Comput</source>
					<volume>17</volume>
					<fpage>859</fpage>
					<lpage>879</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b006"><label>6</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Serre</surname><given-names>T</given-names></name><name name-style="western"><surname>Wolf</surname><given-names>L</given-names></name><name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name></person-group>
					<year>2005</year>
					<article-title>Object recognition with features inspired by visual cortex.</article-title>
					<source>CVPR</source>
					<volume>2</volume>
					<fpage>994</fpage>
					<lpage>1000</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b007"><label>7</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name><name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name></person-group>
					<year>1999</year>
					<article-title>Hierarchical models of object recognition in cortex.</article-title>
					<source>Nat Neurosci</source>
					<volume>2</volume>
					<fpage>1019</fpage>
					<lpage>1025</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b008"><label>8</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Ullman</surname><given-names>S</given-names></name><name name-style="western"><surname>Vidal-Naquet</surname><given-names>M</given-names></name><name name-style="western"><surname>Sali</surname><given-names>E</given-names></name></person-group>
					<year>2002</year>
					<article-title>Visual features of intermediate complexity and their use in classification.</article-title>
					<source>Nat Neurosci</source>
					<volume>5</volume>
					<fpage>682</fpage>
					<lpage>687</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b009"><label>9</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Fukushima</surname><given-names>K</given-names></name></person-group>
					<year>1980</year>
					<article-title>Neocognitron: A self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position.</article-title>
					<source>Biol Cybern</source>
					<volume>36</volume>
					<fpage>193</fpage>
					<lpage>202</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b010"><label>10</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name></person-group>
					<year>1995</year>
					<article-title>Convolutional networks for images, speech, and time series.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Arbib</surname><given-names>MA</given-names></name></person-group>
					<source>The handbook of brain theory and neural networks</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<fpage>255</fpage>
					<lpage>258</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b011"><label>11</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Kobatake</surname><given-names>E</given-names></name><name name-style="western"><surname>Tanaka</surname><given-names>K</given-names></name></person-group>
					<year>1994</year>
					<article-title>Neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex.</article-title>
					<source>J Neurophysiol</source>
					<volume>71</volume>
					<fpage>856</fpage>
					<lpage>867</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b012"><label>12</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Oram</surname><given-names>MW</given-names></name><name name-style="western"><surname>Perrett</surname><given-names>DI</given-names></name></person-group>
					<year>1994</year>
					<article-title>Modeling visual recognition from neurobiological constraints.</article-title>
					<source>Neural Networks</source>
					<volume>7</volume>
					<fpage>945</fpage>
					<lpage>972</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b013"><label>13</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Albrecht</surname><given-names>DG</given-names></name><name name-style="western"><surname>Geisler</surname><given-names>WS</given-names></name><name name-style="western"><surname>Frazor</surname><given-names>RA</given-names></name><name name-style="western"><surname>Crane</surname><given-names>AM</given-names></name></person-group>
					<year>2002</year>
					<article-title>Visual cortex neurons of monkeys and cats: Temporal dynamics of the contrast response function.</article-title>
					<source>J Neurophysiol</source>
					<volume>88</volume>
					<fpage>888</fpage>
					<lpage>913</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b014"><label>14</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gawne</surname><given-names>TJ</given-names></name><name name-style="western"><surname>Kjaer</surname><given-names>TW</given-names></name><name name-style="western"><surname>Richmond</surname><given-names>BJ</given-names></name></person-group>
					<year>1996</year>
					<article-title>Latency: Another potential code for feature binding in striate cortex.</article-title>
					<source>J Neurophysiol</source>
					<volume>76</volume>
					<fpage>1356</fpage>
					<lpage>1360</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b015"><label>15</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Celebrini</surname><given-names>S</given-names></name><name name-style="western"><surname>Thorpe</surname><given-names>S</given-names></name><name name-style="western"><surname>Trotter</surname><given-names>Y</given-names></name><name name-style="western"><surname>Imbert</surname><given-names>M</given-names></name></person-group>
					<year>1993</year>
					<article-title>Dynamics of orientation coding in area V 1 of the awake primate.</article-title>
					<source>Vis Neurosci</source>
					<volume>10</volume>
					<fpage>811</fpage>
					<lpage>825</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b016"><label>16</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Delorme</surname><given-names>A</given-names></name><name name-style="western"><surname>Perrinet</surname><given-names>L</given-names></name><name name-style="western"><surname>Samuelides</surname><given-names>M</given-names></name><name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group>
					<year>2000</year>
					<article-title>Networks of Integrate-and-Fire-Neurons using Rank Order Coding B: Spike Timing Dependent Plasticity and Emergence of Orientation Selectivity.</article-title>
					<source>Neurocomputing</source>
					<volume>38–40</volume>
					<fpage>539</fpage>
					<lpage>545</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b017"><label>17</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group>
					<year>1990</year>
					<article-title>Spike arrival times: A highly efficient coding scheme for neural networks.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Eckmiller</surname><given-names>R</given-names></name><name name-style="western"><surname>Hartmann</surname><given-names>G</given-names></name><name name-style="western"><surname>Hauske</surname><given-names>G</given-names></name></person-group>
					<source>Parallel processing in neural systems and computers</source>
					<publisher-loc>Amsterdam</publisher-loc>
					<publisher-name>Elsevier</publisher-name>
					<fpage>91</fpage>
					<lpage>94</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b018"><label>18</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Rousselet</surname><given-names>GA</given-names></name><name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Fabre-Thorpe</surname><given-names>M</given-names></name></person-group>
					<year>2003</year>
					<article-title>Taking the MAX from neuronal responses.</article-title>
					<source>Trends Cogn Sci</source>
					<volume>7</volume>
					<fpage>99</fpage>
					<lpage>102</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b019"><label>19</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name><name name-style="western"><surname>Lubke</surname><given-names>J</given-names></name><name name-style="western"><surname>Frotscher</surname><given-names>M</given-names></name><name name-style="western"><surname>Sakmann</surname><given-names>B</given-names></name></person-group>
					<year>1997</year>
					<article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs.</article-title>
					<source>Science</source>
					<volume>275</volume>
					<fpage>213</fpage>
					<lpage>215</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b020"><label>20</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bi</surname><given-names>GQ</given-names></name><name name-style="western"><surname>Poo</surname><given-names>MM</given-names></name></person-group>
					<year>1998</year>
					<article-title>Synaptic modifications in cultured hippocampal neurons: Dependence on spike timing, synaptic strength, and postsynaptic cell type.</article-title>
					<source>J Neurosci</source>
					<volume>18</volume>
					<fpage>10464</fpage>
					<lpage>10472</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b021"><label>21</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>LI</given-names></name><name name-style="western"><surname>Tao</surname><given-names>HW</given-names></name><name name-style="western"><surname>Holt</surname><given-names>CE</given-names></name><name name-style="western"><surname>Harris</surname><given-names>WA</given-names></name><name name-style="western"><surname>Poo</surname><given-names>M</given-names></name></person-group>
					<year>1998</year>
					<article-title>A critical window for cooperation and competition among developing retinotectal synapses.</article-title>
					<source>Nature</source>
					<volume>395</volume>
					<fpage>37</fpage>
					<lpage>44</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b022"><label>22</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Feldman</surname><given-names>DE</given-names></name></person-group>
					<year>2000</year>
					<article-title>Timing-based LTP and LTD at vertical inputs to layer II /III pyramidal cells in rat barrel cortex.</article-title>
					<source>Neuron</source>
					<volume>27</volume>
					<fpage>45</fpage>
					<lpage>56</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b023"><label>23</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>VanRossum</surname><given-names>MCW</given-names></name><name name-style="western"><surname>Bi</surname><given-names>GQ</given-names></name><name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name></person-group>
					<year>2000</year>
					<article-title>Stable Hebbian learning from spike timing-dependent plasticity.</article-title>
					<source>J Neurosci</source>
					<volume>20</volume>
					<fpage>8812</fpage>
					<lpage>8821</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b024"><label>24</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name><name name-style="western"><surname>Kistler</surname><given-names>WM</given-names></name></person-group>
					<year>2002</year>
					<source>Learning to be fast: Spiking neuron models</source>
					<publisher-name>Cambridge University Press</publisher-name>
					<fpage>421</fpage>
					<lpage>432</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b025"><label>25</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Fergus</surname><given-names>R</given-names></name><name name-style="western"><surname>Perona</surname><given-names>P</given-names></name><name name-style="western"><surname>Zisserman</surname><given-names>A</given-names></name></person-group>
					<year>2003</year>
					<article-title>Object class recognition by unsupervised scale-invariant learning.</article-title>
					<source>CVPR</source>
					<volume>2</volume>
					<fpage>264</fpage>
					<lpage>271</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b026"><label>26</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Serre</surname><given-names>T</given-names></name><name name-style="western"><surname>Kouh</surname><given-names>M</given-names></name><name name-style="western"><surname>Cadieu</surname><given-names>C</given-names></name><name name-style="western"><surname>Knoblich</surname><given-names>U</given-names></name><name name-style="western"><surname>Kreiman</surname><given-names>G</given-names></name><etal/></person-group>
					<year>2005</year>
					<article-title>A theory of object recognition: Computations and circuits in the feedforward path of the ventral stream in primate visual cortex.</article-title>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>Massachusetts Institute of Technology</publisher-name>
					<source>CBCL Paper #259/AI Memo #2005–036</source>
				</element-citation></ref><ref id="pcbi-0030031-b027"><label>27</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name><name name-style="western"><surname>Bizzi</surname><given-names>E</given-names></name></person-group>
					<year>2004</year>
					<article-title>Generalization in vision and motor control.</article-title>
					<source>Nature</source>
					<volume>431</volume>
					<fpage>768</fpage>
					<lpage>774</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b028"><label>28</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name><name name-style="western"><surname>Deco</surname><given-names>G</given-names></name></person-group>
					<year>2002</year>
					<source>Computational neuroscience of vision</source>
					<publisher-loc>Oxford</publisher-loc>
					<publisher-name>Oxford University Press</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">592</size>
				</element-citation></ref><ref id="pcbi-0030031-b029"><label>29</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Itti</surname><given-names>L</given-names></name><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name><name name-style="western"><surname>Niebur</surname><given-names>E</given-names></name></person-group>
					<year>1998</year>
					<article-title>A model of saliency-based visual attention for rapid scene analysis.</article-title>
					<source>IEEE Trans Pattern Anal Mach Intell</source>
					<volume>20</volume>
					<fpage>1254</fpage>
					<lpage>1259</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b030"><label>30</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Treue</surname><given-names>S</given-names></name></person-group>
					<year>2003</year>
					<article-title>Abstract visual attention: The where, what, how and why of saliency.</article-title>
					<source>Curr Opin Neurobiol</source>
					<volume>13</volume>
					<fpage>428</fpage>
					<lpage>432</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b031"><label>31</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Uchida</surname><given-names>N</given-names></name><name name-style="western"><surname>Kepecs</surname><given-names>A</given-names></name><name name-style="western"><surname>Mainen</surname><given-names>ZF</given-names></name></person-group>
					<year>2006</year>
					<article-title>Seeing at a glance, smelling in a whiff: Rapid forms of perceptual decision making.</article-title>
					<source>Nat Rev Neurosci</source>
					<volume>7</volume>
					<fpage>485</fpage>
					<lpage>491</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b032"><label>32</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Martinez-Conde</surname><given-names>S</given-names></name><name name-style="western"><surname>Macknik</surname><given-names>SL</given-names></name><name name-style="western"><surname>Hubel</surname><given-names>DH</given-names></name></person-group>
					<year>2000</year>
					<article-title>Microsaccadic eye movements and firing of single cells in the striate cortex of macaque monkeys.</article-title>
					<source>Nat Neurosci</source>
					<volume>3</volume>
					<fpage>251</fpage>
					<lpage>258</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b033"><label>33</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group>
					<year>1995</year>
					<article-title>Pattern recognition computation using action potential timing for stimulus representation.</article-title>
					<source>Nature</source>
					<volume>376</volume>
					<fpage>33</fpage>
					<lpage>36</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b034"><label>34</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>König</surname><given-names>P</given-names></name><name name-style="western"><surname>Engel</surname><given-names>AK</given-names></name><name name-style="western"><surname>Roelfsema</surname><given-names>PR</given-names></name><name name-style="western"><surname>Singer</surname><given-names>W</given-names></name></person-group>
					<year>1995</year>
					<article-title>How precise is neuronal synchronization?</article-title>
					<source>Neural Comput</source>
					<volume>7</volume>
					<fpage>469</fpage>
					<lpage>485</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b035"><label>35</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Vapnik</surname><given-names>VN</given-names></name><name name-style="western"><surname>Chervonenkis</surname><given-names>AY</given-names></name></person-group>
					<year>1971</year>
					<article-title>On the uniform convergence of relative frequencies of events to their probabilities.</article-title>
					<source>Theor Probab Appl</source>
					<volume>17</volume>
					<fpage>264</fpage>
					<lpage>280</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b036"><label>36</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Földiák</surname><given-names>P</given-names></name></person-group>
					<year>1991</year>
					<article-title>Learning invariance from transformation sequences.</article-title>
					<source>Neural Comput</source>
					<volume>3</volume>
					<fpage>194</fpage>
					<lpage>200</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b037"><label>37</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wallis</surname><given-names>G</given-names></name><name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name></person-group>
					<year>1997</year>
					<article-title>Invariant face and object recognition in the visual system.</article-title>
					<source>Prog Neurobiol</source>
					<volume>51</volume>
					<fpage>167</fpage>
					<lpage>194</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b038"><label>38</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name><name name-style="western"><surname>Milward</surname><given-names>T</given-names></name></person-group>
					<year>2000</year>
					<article-title>A model of invariant object recognition in the visual system: Learning rules, activation functions, lateral inhibition, and information-based performance measures.</article-title>
					<source>Neural Comput</source>
					<volume>12</volume>
					<fpage>2547</fpage>
					<lpage>2572</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b039"><label>39</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Stringer</surname><given-names>SM</given-names></name><name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name></person-group>
					<year>2000</year>
					<article-title>Position invariant recognition in the visual system with cluttered environments.</article-title>
					<source>Neural Networks</source>
					<volume>13</volume>
					<fpage>305</fpage>
					<lpage>315</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b040"><label>40</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name><name name-style="western"><surname>Pauls</surname><given-names>J</given-names></name><name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name></person-group>
					<year>1995</year>
					<article-title>Shape representation in the inferior temporal cortex of monkeys.</article-title>
					<source>Curr Biol</source>
					<volume>5</volume>
					<fpage>552</fpage>
					<lpage>563</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b041"><label>41</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Thorpe</surname><given-names>S</given-names></name><name name-style="western"><surname>Fize</surname><given-names>D</given-names></name><name name-style="western"><surname>Marlot</surname><given-names>C</given-names></name></person-group>
					<year>1996</year>
					<article-title>Speed of processing in the human visual system.</article-title>
					<source>Nature</source>
					<volume>381</volume>
					<fpage>520</fpage>
					<lpage>522</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b042"><label>42</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sigala</surname><given-names>N</given-names></name><name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name></person-group>
					<year>2002</year>
					<article-title>Visual categorization shapes feature selectivity in the primate temporal cortex.</article-title>
					<source>Nature</source>
					<volume>415</volume>
					<fpage>318</fpage>
					<lpage>320</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b043"><label>43</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bichot</surname><given-names>NP</given-names></name><name name-style="western"><surname>Rossi</surname><given-names>AF</given-names></name><name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name></person-group>
					<year>2005</year>
					<article-title>Parallel and serial neural mechanisms for visual search in macaque area v4.</article-title>
					<source>Science</source>
					<volume>308</volume>
					<fpage>529</fpage>
					<lpage>534</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b044"><label>44</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Szabo</surname><given-names>M</given-names></name><name name-style="western"><surname>Stetter</surname><given-names>M</given-names></name><name name-style="western"><surname>Deco</surname><given-names>G</given-names></name><name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name><name name-style="western"><surname>Giudice</surname><given-names>PD</given-names></name><etal/></person-group>
					<year>2006</year>
					<article-title>Learning to attend: Modeling the shaping of selectivity in infero-temporal cortex in a categorization task.</article-title>
					<source>Biol Cybern</source>
					<volume>94</volume>
					<fpage>351</fpage>
					<lpage>365</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b045"><label>45</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Imbert</surname><given-names>M</given-names></name></person-group>
					<year>1989</year>
					<article-title>Biological constraints on connectionist modelling.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Pfeifer</surname><given-names>R</given-names></name><name name-style="western"><surname>Schreter</surname><given-names>Z</given-names></name><name name-style="western"><surname>Fogelman-Soulié</surname><given-names>F</given-names></name><name name-style="western"><surname>Steels</surname><given-names>L</given-names></name></person-group>
					<source>Connectionism in perspective</source>
					<publisher-loc>Amsterdam</publisher-loc>
					<publisher-name>Elsevier</publisher-name>
					<fpage>63</fpage>
					<lpage>92</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b046"><label>46</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gautrais</surname><given-names>J</given-names></name><name name-style="western"><surname>Thorpe</surname><given-names>S</given-names></name></person-group>
					<year>1998</year>
					<article-title>Rate coding versus temporal order coding: A theoretical approach.</article-title>
					<source>Biosystems</source>
					<volume>48</volume>
					<fpage>57</fpage>
					<lpage>65</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b047"><label>47</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Amarasingham</surname><given-names>A</given-names></name><name name-style="western"><surname>Chen</surname><given-names>TL</given-names></name><name name-style="western"><surname>Geman</surname><given-names>S</given-names></name><name name-style="western"><surname>Harrison</surname><given-names>MT</given-names></name><name name-style="western"><surname>Sheinberg</surname><given-names>DL</given-names></name></person-group>
					<year>2006</year>
					<article-title>Spike count reliability and the Poisson hypothesis.</article-title>
					<source>J Neurosci</source>
					<volume>26</volume>
					<fpage>801</fpage>
					<lpage>809</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b048"><label>48</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Kiani</surname><given-names>R</given-names></name><name name-style="western"><surname>Esteky</surname><given-names>H</given-names></name><name name-style="western"><surname>Tanaka</surname><given-names>K</given-names></name></person-group>
					<year>2005</year>
					<article-title>Differences in onset latency of macaque inferotemporal neural responses to primate and non-primate faces.</article-title>
					<source>J Neurophysiol</source>
					<volume>94</volume>
					<fpage>1587</fpage>
					<lpage>1596</lpage>
				</element-citation></ref><ref id="pcbi-0030031-b049"><label>49</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>VanRullen</surname><given-names>R</given-names></name><name name-style="western"><surname>Guyonneau</surname><given-names>R</given-names></name><name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group>
					<year>2005</year>
					<article-title>Spike times make sense.</article-title>
					<source>Trends Neurosci</source>
					<volume>28</volume>
					<fpage>1</fpage>
					<lpage>4</lpage>
				</element-citation></ref></ref-list></back></article>