<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-01808</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005781</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Gene expression</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Oncology</subject><subj-group><subject>Cancers and neoplasms</subject><subj-group><subject>Breast tumors</subject><subj-group><subject>Breast cancer</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Chromosome biology</subject><subj-group><subject>Chromatin</subject><subj-group><subject>Chromatin modification</subject><subj-group><subject>DNA methylation</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Epigenetics</subject><subj-group><subject>Chromatin</subject><subj-group><subject>Chromatin modification</subject><subj-group><subject>DNA methylation</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Gene expression</subject><subj-group><subject>Chromatin</subject><subj-group><subject>Chromatin modification</subject><subj-group><subject>DNA methylation</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>DNA</subject><subj-group><subject>DNA modification</subject><subj-group><subject>DNA methylation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Nucleic acids</subject><subj-group><subject>DNA</subject><subj-group><subject>DNA modification</subject><subj-group><subject>DNA methylation</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Epigenetics</subject><subj-group><subject>DNA modification</subject><subj-group><subject>DNA methylation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Gene expression</subject><subj-group><subject>DNA modification</subject><subj-group><subject>DNA methylation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Gene expression</subject><subj-group><subject>Gene regulation</subject><subj-group><subject>MicroRNAs</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Nucleic acids</subject><subj-group><subject>RNA</subject><subj-group><subject>Non-coding RNA</subject><subj-group><subject>MicroRNAs</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Nephrology</subject><subj-group><subject>Renal cancer</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Clustering algorithms</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Clustering algorithms</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Clusternomics: Integrative context-dependent clustering for heterogeneous datasets</article-title>
<alt-title alt-title-type="running-head">Clusternomics: Integrative context-dependent clustering for heterogeneous datasets</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3707-0657</contrib-id>
<name name-style="western">
<surname>Gabasova</surname> <given-names>Evelina</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="fn" rid="currentaff001"><sup>¤</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7762-6760</contrib-id>
<name name-style="western">
<surname>Reid</surname> <given-names>John</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="fn" rid="econtrib001"><sup>‡</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Wernisch</surname> <given-names>Lorenz</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="fn" rid="econtrib001"><sup>‡</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>MRC Biostatistics Unit, University of Cambridge, Cambridge, United Kingdom</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Morris</surname> <given-names>Quaid</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Toronto, CANADA</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<label>¤</label> <p>Current address: MRC Cancer Unit, University of Cambridge, Cambridge, United Kingdom</p>
</fn>
<fn fn-type="other" id="econtrib001">
<p>‡These authors share last authorship on this work.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">evelina@evelinag.com</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>10</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>16</day>
<month>10</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>10</issue>
<elocation-id>e1005781</elocation-id>
<history>
<date date-type="received">
<day>4</day>
<month>11</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>18</day>
<month>9</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Gabasova et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005781"/>
<abstract>
<p>Integrative clustering is used to identify groups of samples by jointly analysing multiple datasets describing the same set of biological samples, such as gene expression, copy number, methylation etc. Most existing algorithms for integrative clustering assume that there is a shared consistent set of clusters across all datasets, and most of the data samples follow this structure. However in practice, the structure across heterogeneous datasets can be more varied, with clusters being joined in some datasets and separated in others. In this paper, we present a probabilistic clustering method to identify groups across datasets that do not share the same cluster structure. The proposed algorithm, Clusternomics, identifies groups of samples that share their global behaviour across heterogeneous datasets. The algorithm models clusters on the level of individual datasets, while also extracting global structure that arises from the local cluster assignments. Clusters on both the local and the global level are modelled using a hierarchical Dirichlet mixture model to identify structure on both levels. We evaluated the model both on simulated and on real-world datasets. The simulated data exemplifies datasets with varying degrees of common structure. In such a setting Clusternomics outperforms existing algorithms for integrative and consensus clustering. In a real-world application, we used the algorithm for cancer subtyping, identifying subtypes of cancer from heterogeneous datasets. We applied the algorithm to TCGA breast cancer dataset, integrating gene expression, miRNA expression, DNA methylation and proteomics. The algorithm extracted clinically meaningful clusters with significantly different survival probabilities. We also evaluated the algorithm on lung and kidney cancer TCGA datasets with high dimensionality, again showing clinically significant results and scalability of the algorithm.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Integrative clustering is the task of identifying groups of samples by combining information from several datasets. An example of this task is cancer subtyping, where we cluster tumour samples based on several datasets, such as gene expression, proteomics and others. Most existing algorithms assume that all such datasets share a similar cluster structure, with samples outside these clusters treated as noise. The structure can, however, be much more heterogeneous: some meaningful clusters may appear only in some datasets. In the paper, we introduce the Clusternomics algorithm that identifies groups of samples across heterogeneous datasets. It models both cluster structure of individual datasets, and the global structure that appears as a combination of local structures. The algorithm uses probabilistic modelling to identify the groups and share information across the local and global levels. We evaluated the algorithm on both simulated and real world datasets, where the algorithm found clinically significant clusters with different survival outcomes.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000265</institution-id>
<institution>Medical Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>MC U105260799</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Wernisch</surname> <given-names>Lorenz</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>EG, JR and LW were funded by the UK Medical Research Council (Grant 573 Ref MC U105260799). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="23"/>
<table-count count="0"/>
<page-count count="29"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2017-10-26</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The data were originally downloaded from The Cancer Genome Atlas (TCGA). Direct links to download the datasets used in the paper are available from <ext-link ext-link-type="uri" xlink:href="https://github.com/evelinag/clusternomics" xlink:type="simple">https://github.com/evelinag/clusternomics</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<disp-quote><p>This is a <italic>PLOS Computational Biology</italic> Methods paper.</p></disp-quote>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Current growth in high-throughput analysis methods in bioinformatics gives access to many different types of data measuring behaviour in complex biological systems. It is now possible to observe multiple levels of a complex biological process simultaneously. For example, we can look at DNA copy number changes, gene expression patterns, epigenetic features such as DNA methylation, and protein expression—all giving a view of different aspects of the underlying process. Analysis of such data is non-trivial because of the need to integrate information from the different levels. Integrative methods that look at the combined effects of several levels of biological processes in the cell have the potential to identify more features that lead to complex phenotypes.</p>
<p>We introduce a novel algorithm for integrative clustering for heterogeneous multilevel data: context-dependent clustering. In general, integrative clustering is the task of identifying clusters in a set of related datasets across the same samples.</p>
<p>As a motivating example, we use the problem of cancer subtyping. Cancer is a heterogeneous process where even cancers originating from the same tissue behave differently in relation to their original driver mutations [<xref ref-type="bibr" rid="pcbi.1005781.ref001">1</xref>]. Cancer subtyping is the task of identifying types of cancer which are typically associated with different patient outcomes, therapy responses or tumour aggressiveness. Finding such cancer subtypes then allows identification of the differences between their molecular behaviour. By integrating different levels of data we can get a better understanding of the interplay of different steps in cancer biochemical pathways.</p>
<p>In the cancer subtyping problem, we use integrative clustering to characterise different types of cancer, based on the tumours’ genomic and other omic profiles. The tumour samples are simultaneously analysed using different technologies, for example gene expression, methylation, and sequencing, yielding a set of related datasets. Integrative clustering looks for a partitioning of tumour samples based on their exhibited behaviour across the datasets.</p>
<p>Compared to standard clustering methods, the main challenges in integrative clustering come from the fact that individual datasets are not directly comparable: each of them describes a different aspect of the underlying biological process. Datasets may even have different data types—continuous gene expression values, binary presence/absence of chromatin modifications or even time-series observations and clinical markers.</p>
<p>Because each data set originates from a different context, clusters obtained from different datasets are not consistent in general. Cluster membership and/or the number of clusters may differ between datasets. This discrepancy originates both from noise and from biological heterogeneity. For example, Ovaska et al. [<xref ref-type="bibr" rid="pcbi.1005781.ref002">2</xref>] report unexpectedly poor concordance between gene amplification, expression of the genes from the amplicons, and patient survival in a study on childhood brain tumour glioblastoma multiforme. Kristensen et al. [<xref ref-type="bibr" rid="pcbi.1005781.ref003">3</xref>] analysed results from dataset-specific clustering analyses on different types of omic data and note that only a minority of the samples were grouped together across all datasets.</p>
<p>These examples show that the notion of a cluster itself is ambiguous in the context of integrative clustering. Most existing methods aim to extract a common cluster structure that is shared across all datasets. However, due to the multiple different sources of heterogeneity, biases and high level of noise, a single cluster structure that would be common to all datasets may not exist or it may not be identifiable in some of the datasets. The assumption of a common cluster structure is well suited to model only the processes that show consistent behaviours across all datasets. However, this assumption is limiting in modelling complex heterogeneous processes such as cancer.</p>
<p>Using a simple example from breast cancer research, both increase in copy number of the HER2 gene, and increase in expression of the corresponding protein are correlated with poor prognosis in patients [<xref ref-type="bibr" rid="pcbi.1005781.ref001">1</xref>]. One would naively expect that the oncogene copy number amplification leads to over-expression of its protein. However, the relation is not always this straightforward: some tumours over-express HER2 even without the original gene amplification [<xref ref-type="bibr" rid="pcbi.1005781.ref001">1</xref>]. The over-expression is apparently achieved through a mechanism other than a change in gene copy number. However, the samples would not be distinguishable based on gene expression data alone, but can be separated by taking genomic information into account as well.</p>
<p>By assuming a single common cluster structure, these heterogeneous situations are difficult to model. A set of samples may systematically belong to two distinct clusters in one dataset (copy number aberration) and to a single joint cluster in a second dataset (mRNA gene expression). This example shows that real biological processes display different behaviours in different contexts, which leads to different cluster arrangements.</p>
<p>The existing algorithms for integrative clustering assume that there is only a single common cluster structure across all analysed datasets. Individual samples then either follow this structure or they are considered to be a noise. This treatment of inconsistent samples is suitable for technical noise which does not bear biological significance, but it can miss cases where the difference is biologically meaningful.</p>
<p>The proposed context-dependent clustering model assumes instead that a single biological process (such as cancer) looks and behaves differently in different <italic>contexts</italic>. A context might represent different stages of the process (early versus late carcinogenesis), or different data domains (DNA modifications, mRNA expression, etc.).</p>
<sec id="sec002">
<title>Existing methods assume a common structure</title>
<p>Here we look at some of the existing approaches to integrative clustering and how they use the assumption of a common clustering structure.</p>
<p>The iCluster algorithm [<xref ref-type="bibr" rid="pcbi.1005781.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1005781.ref005">5</xref>] uses a Gaussian latent variable model to infer clusters. It assumes that there is a common set of latent cluster membership variables across all datasets. Differences in structure between different datasets are accounted for only via individual noise terms, which correspond to within-dataset variances. iCluster uses the <italic>k</italic>-means algorithm to extract the actual cluster assignments given latent variable values.</p>
<p>Using this approach, it is possible to obtain clusters where the data are well separated across all datasets. However, if two clusters are joined together in one dataset and separate in another dataset, this leads to ambiguous latent variable values and iCluster fails to infer the correct partitioning. Also, using <italic>k</italic>-means for cluster inference from latent variable values makes the algorithm strongly dependent on the correct specification of the number of clusters. We demonstrate this behaviour on a simulated dataset later in the paper.</p>
<p>A similar algorithm to iCluster is the Joint and Individual Clustering (JIC) [<xref ref-type="bibr" rid="pcbi.1005781.ref006">6</xref>]. This algorithm uses the connection between principal component analysis and <italic>k</italic>-means clustering to integrate data from multiple datasets and infer both joint clusters and dataset-specific clusters. The dataset-specific clusters are estimated so that they are independent of the joint clusters, and thus capture independent structure that exists only within individual datasets. This algorithm still uses the assumption of one global cluster structure across the datasets, which is augmented by additional cluster structures within individual datasets that are independent of the overall global structure.</p>
<p>Bayesian consensus clustering (BCC) [<xref ref-type="bibr" rid="pcbi.1005781.ref007">7</xref>] relaxes the assumption of a common cluster structure by allowing different datasets to follow individual <italic>local</italic> clustering models. Apart from the local clusters, BCC introduces a <italic>global</italic> cluster structure as well. This depends on the local clusters through a parameter <italic>α</italic>, which regulates how much the local clusters correspond to the global clusters. The split between global and local structure brings flexibility in modelling inconsistent cluster structures across many datasets. The consensus global cluster assignments then define a common clustering based on partial agreement between context datasets.</p>
<p>However, this again assumes that there is a common set of clusters which are exhibited in a similar way across all datasets. Deviances from this structure are considered to be dataset-specific noise, which might lead to omission of any additional structure that is present in the data.</p>
<p>Multiple Dataset Integration (MDI) [<xref ref-type="bibr" rid="pcbi.1005781.ref008">8</xref>] uses a different approach. This model does not assume a common clustering but it looks only at pair-wise relations between datasets. In this formulation, every dataset can have a flexible set of clusters, and a pairs of samples are considered <italic>fused</italic>, if they belong to the same set of pairs of clusters across datasets. This limits the interpretability of a solution. Although the model allows enough flexibility for combinations of clusters, it does not explicitly encourage any sharing of clusters across more than pairs of datasets.</p>
<p>Another approach is taken by the Similarity Network Fusion (SNF) [<xref ref-type="bibr" rid="pcbi.1005781.ref009">9</xref>]. This algorithm uses an iterative approach on dataset-specific similarity networks that intensifies strong similarities and diminishes weak similarities across samples. This again implies the existence of a common structure that is consistent across the datasets.</p>
</sec>
<sec id="sec003">
<title>Context-dependent integrative clustering</title>
<p>We introduce the context-dependent clustering model to address the issues outlined in the introduction. Compared to existing integrative clustering algorithms, the proposed model does not generally assume a single partitioning of the data that is consistent across heterogeneous datasets.</p>
<p>If a process is allowed to follow different cluster structures across several contexts, we have to modify our notion of clustering. We assume that there is a set of clusters within each context (dataset), but these clusters do not generally correspond directly to each other. However, some dependency between the clusters is expected:</p>
<list list-type="order">
<list-item>
<p>Clustering structure in one context should influence clustering in other contexts. If two samples are clustered together in one context, they should be more likely to be clustered together in other contexts as well.</p>
</list-item>
<list-item>
<p>Different degrees of dependence should be allowed between clusters across contexts. For example, datasets may not share the same numbers of clusters. We should be able to model cases where all contexts share the same cluster structure, as well as cases where the clusters are completely independent in the different contexts.</p>
</list-item>
</list>
<p>Using the example of the HER2 oncogene from the Introduction, there are two levels of complexity we can analyse. If we consider only tumour samples which over-express HER2, the first level is formed by the individual contexts: there are two clusters in DNA amplification dataset (amplified or neutral), and one cluster when we consider mRNA expression of HER2 (over-expressed). If we assumed a globally consistent cluster structure, we would have to either artificially split the cluster of samples which over-express HER2, or artificially join the two clusters with different copy number aberrations. When we look at the problem from the <italic>global</italic> level, we get two groups of samples that behave in a different way across contexts. The behaviours are defined by different <italic>combinations</italic> of context-specific clusters. The concepts are illustrated in <xref ref-type="fig" rid="pcbi.1005781.g001">Fig 1</xref>.</p>
<fig id="pcbi.1005781.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Illustration of heterogeneous cluster structures in two contexts (datasets).</title>
<p>Each context corresponds to different data source (gene expression, ribosome profiling, proteomics etc.) describing the same set of biological samples. In the first context, there are two distinct clusters on the local level. In the second context, there is only a single local cluster. From the overall perspective, there are two global clusters defined by the combined behaviour across the two contexts.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g001" xlink:type="simple"/>
</fig>
<p>The context-dependent clustering model as proposed here uses a Bayesian clustering framework to infer both the local structure within each dataset, as well as the global structure which arises from the combination of cluster assignments. We use probabilistic framework, with hierarchical Dirichlet mixture models to model clusters across the contexts. We fit the model using Gibbs sampling. Details of the model formulation and inference algorithms are presented in the Methods section.</p>
<p>In the next section we focus on a comparison of the proposed algorithm with alternative context dependent clustering approaches on simulated as well as on real-world datasets.</p>
</sec>
</sec>
<sec id="sec004" sec-type="results">
<title>Results</title>
<p>We evaluate the proposed context-dependent clustering model both on simulated dataset to compare its performance with other currently used methods, and on real world datasets. The simulated example demonstrates that the proposed model identifies clustering structure within datasets where the dependence of individual contexts varies. The applications to the real world datasets (breast cancer, lung cancer and kidney cancer) from The Cancer Genome Atlas (TCGA) show that the algorithm identifies biologically meaningful structure, which is characterized by significantly different survival outcomes. Finally, we demonstrate the robustness of the results to changes in parameter settings.</p>
<sec id="sec005">
<title>Case study: Simulated datasets with heterogeneous structure</title>
<p>In this section, we look at how the proposed context-dependent clustering method performs in a situation where there is no single common clustering structure. We compare its performance with popular alternative clustering algorithms on a set of simulated data with various degrees of dependence between contexts.</p>
<p>The simulated data are formed by two 1-dimensional datasets, which represent a dataset with two contexts. Each context contains two clusters: both clusters are normally distributed with unit variance but with different means. Cluster 1 in both contexts is centred at -2 and Cluster 2 is centred at 2. The two clusters are well separated and they do not overlap significantly. We combine the clusters to simulate different degrees of independence between context-specific cluster structures.</p>
<p>We generate 100 data sets each with 200 samples according to the following procedure. For each data set we sample a mixture probability <italic>p</italic> uniformly from the interval (0, 0.5). For the first group of 100 samples we sample each of the two context values independently from <inline-formula id="pcbi.1005781.e001"><alternatives><graphic id="pcbi.1005781.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> or from <inline-formula id="pcbi.1005781.e002"><alternatives><graphic id="pcbi.1005781.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> with probability 1 − <italic>p</italic> and <italic>p</italic>, respectively, that is, since <italic>p</italic> &lt; 0.5 mostly from Cluster 1, but occasionally also from Cluster 2. For the next group of 100 samples we sample the each of two context values independently from <inline-formula id="pcbi.1005781.e003"><alternatives><graphic id="pcbi.1005781.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> or from <inline-formula id="pcbi.1005781.e004"><alternatives><graphic id="pcbi.1005781.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> with probability <italic>p</italic> and 1 − <italic>p</italic>, that is, mostly from Cluster 2 this time, but occasionally from Cluster 1.</p>
<p>For mixture probabilities <italic>p</italic> close to 0, the clusterings of the two contexts largely agree by putting the first 100 samples into Cluster 1, and the rest into Cluster 2, leading to two global clusters. At the other extreme, if <italic>p</italic> is close to 0.5, cluster assignments are more or less random and there is little agreement between contexts, leading to four global clusters.</p>
<p>
<xref ref-type="fig" rid="pcbi.1005781.g002">Fig 2</xref> shows data sampled from the two opposing scenarios.</p>
<fig id="pcbi.1005781.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Example of the simulated data for <italic>p</italic> = 0 and <italic>p</italic> = 0.5, which show different degrees of dependence.</title>
<p>The <italic>x</italic> axis corresponds to the data in the first dataset (context), the <italic>y</italic> axis represents the data in the second dataset (context). The two subfigures show the two extreme situations: (a) For <italic>p</italic> = 0, we get two global clusters. Cluster membership is fully dependent on each other in both datasets. (b) For <italic>p</italic> = 0.5, we get four global clusters, where cluster membership in one dataset is fully independent on cluster membership in the second dataset.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g002" xlink:type="simple"/>
</fig>
<p>If the data distributions are fully dependent across the contexts, we get two global clusters (<xref ref-type="fig" rid="pcbi.1005781.g002">Fig 2a</xref>). Because the data are assigned to the same cluster in both contexts, there is a single common cluster structure. On the other hand, if the two contexts are independent, we get four global clusters (<xref ref-type="fig" rid="pcbi.1005781.g002">Fig 2b</xref>) which correspond to the combinations of context-specific cluster assignments. This scenario is difficult to model if we look at individual contexts separately.</p>
<p>As we discussed above, most existing algorithms assume a common set of clusters that are shared across all contexts, with only a limited deviation from this structure. The simulated data illustrate what happens when this assumption does not hold, i.e. when there are different degrees of dependence between the contexts. We compare clusternomics with existing algorithms to see how they adapt to different scenarios where the assumption of a single common cluster structure is not satisfied.</p>
<sec id="sec006">
<title>Results</title>
<p>We use the simulated data to evaluate the following integrative clustering algorithms:</p>
<list list-type="order">
<list-item>
<p>The proposed Context-Dependent Clustering (Clusternomics)</p>
</list-item>
<list-item>
<p>Bayesian Consensus Clustering (BCC) [<xref ref-type="bibr" rid="pcbi.1005781.ref007">7</xref>]</p>
</list-item>
<list-item>
<p>Multiple Dataset Integration (MDI) [<xref ref-type="bibr" rid="pcbi.1005781.ref008">8</xref>]</p>
</list-item>
<list-item>
<p>iCluster [<xref ref-type="bibr" rid="pcbi.1005781.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1005781.ref005">5</xref>]</p>
</list-item>
<list-item>
<p>Similarity Network Fusion [<xref ref-type="bibr" rid="pcbi.1005781.ref009">9</xref>]</p>
</list-item>
</list>
<p>Methods 1, 2 and 3 are Bayesian probabilistic methods fitted using MCMC sampling, 4 uses a latent variable model fitted with an EM algorithm combined with <italic>k</italic>-means clustering and finally 5 is an algorithm based on summarised distances between samples.</p>
<p>We ran the listed methods on the 100 simulated datasets. Details on the settings of the individual algorithms are given in <xref ref-type="supplementary-material" rid="pcbi.1005781.s001">S1 Appendix</xref>. The results are only comparable on the level of global clustering due to the properties of the algorithms themselves: SNF and iCluster only compute the global clustering across all contexts. BCC and MDI have a notion of local clustering within each context, but they do not allow the number of clusters to differ between the global and local level. All methods except for MDI use the number of global clusters as their input: we set this equal to 4.</p>
<p>We use the adjusted Rand Index (ARI) [<xref ref-type="bibr" rid="pcbi.1005781.ref010">10</xref>] to compare the true cluster assignments used to generate the data, and the results estimated by each algorithm. The ARI measures agreement between two partitions of the same set of samples; the value of 1 corresponds to complete agreement between two cluster assignments, and 0 means the agreement between partitions is caused by chance. <xref ref-type="fig" rid="pcbi.1005781.g003">Fig 3</xref> shows ARI values comparing the different algorithms for the 100 simulated datasets.</p>
<fig id="pcbi.1005781.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g003</object-id>
<label>Fig 3</label>
<caption>
<title>ARI comparing global clustering of simulated datasets for varying values of <italic>p</italic> (see <xref ref-type="fig" rid="pcbi.1005781.g002">Fig 2</xref>).</title>
<p>Each point corresponds to the corresponding algorithm applied to one dataset, the plot shows also the loess curve for each method. Higher values correspond to better agreement between the estimated cluster assignments and the true cluster membership.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g003" xlink:type="simple"/>
</fig>
<p>For small values of <italic>p</italic>, which correspond to two fully dependent global clusters, all probabilistic methods Clusternomics, BCC and MDI perform similarly well. This scenario best fits the assumptions behind BCC and MDI algorithms that there is a common shared cluster structure, which corresponds to their concept of a <italic>global cluster</italic>. For higher values of <italic>p</italic>, which represent higher degrees of independence of clusters between contexts, Clusternomics and iCluster have the best performance. Since the evaluated algorithms other than Clusternomics all assume some form of a shared global clustering structure across contexts, they are at a disadvantage in this simulation scenario. Consequently, only Clusternomics is able to recover the underlying cluster structure across all different values of <italic>p</italic>.</p>
<p>The disappointing performance of MDI is caused by the algorithm allocating all the samples to a single cluster for higher values of <italic>p</italic>. One should note, however, that the MDI algorithm was disadvantaged in this setting because it infers the number of clusters from the data instead of using a pre-specified value.</p>
<p>The iCluster and SNF algorithms were also disadvantaged specifically for small values of <italic>p</italic> because these algorithms use <italic>k</italic>-means and spectral clustering respectively to extract the number of clusters. This makes them very sensitive to mis-specification. As an illustration, <xref ref-type="fig" rid="pcbi.1005781.g004">Fig 4</xref> shows the behaviour of all analysed algorithms when we set the number of global clusters to 5, and allowed up to 3 clusters in every context in the Clusternomics algorithm. We can see that the change in the number of clusters does not affect probabilistic algorithms but iCluster and SNF are strongly dependent on this setting.</p>
<fig id="pcbi.1005781.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g004</object-id>
<label>Fig 4</label>
<caption>
<title>ARI comparing global clustering of simulated datasets with misspecified number of clusters for varying values of <italic>p</italic>, when we set the number of global clusters to 5.</title>
<p>Each point corresponds to a corresponding algorithm applied to one dataset, the plot shows also the loess curve for each method. Higher values correspond to better agreement between the estimated cluster assignments and the true cluster membership.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g004" xlink:type="simple"/>
</fig>
<p>In real world applications, it is often very difficult to estimate the number of clusters correctly. Although Dirichlet process-type methods are inconsistent in terms of the true number of clusters [<xref ref-type="bibr" rid="pcbi.1005781.ref011">11</xref>], their results are generally consistent in the number of large clusters that appear in the data.</p>
<p>We also provide an additional comparison in <xref ref-type="supplementary-material" rid="pcbi.1005781.s001">S1 Appendix</xref> with an ad-hoc integrative clustering, where we first cluster data in each context individually, and then we construct <italic>global</italic> clusters manually as a combination of <italic>local</italic> cluster assignments. For the small simulated dataset, the results of this ad-hoc integration are equivalent to Clusternomics. They differ in larger real-world applications, where the probabilistic integrative model encourages data points from different contexts to share global clusters, as opposed to crude manual construction of global clusters.</p>
</sec>
</sec>
<sec id="sec007">
<title>Case study: Discovering subtypes in breast cancer</title>
<p>In this section we apply the context-dependent clustering to a breast cancer dataset obtained from The Cancer Genome Atlas (TCGA) database. The dataset contains measurements of 348 patients diagnosed with breast cancer, and it comprises of four different data types: DNA methylation for 574 probes, RNA gene expression for 645 genes, expression for 423 microRNA molecules and reverse-phase protein array (RPPA) measurements for 171 proteins.</p>
<p>The dataset was originally presented in Koboldt et al. [<xref ref-type="bibr" rid="pcbi.1005781.ref012">12</xref>], and it was also previously used to evaluate integrative clustering methods in Lock and Dunson [<xref ref-type="bibr" rid="pcbi.1005781.ref007">7</xref>]. An advantage of the dataset is that it also contains clinical information on the patients including their survival. This information can be used to validate the clustering results.</p>
<p>We used the proposed context-dependent clustering algorithm to identify subtypes of breast cancer across the four data contexts. We applied the proposed algorithm with several different settings of both the <italic>global</italic> number of clusters, and the <italic>local</italic> context-specific number of clusters. The details of the algorithm’s settings are provided in <xref ref-type="supplementary-material" rid="pcbi.1005781.s001">S1 Appendix</xref>.</p>
<p>To provide an overview of the results, we first look at a specific clustering obtained from the model to illustrate the working of the algorithm. Then we look at consistency of the results with respect to varying number of clusters, and also at results from two other cancer datasets from TCGA.</p>
<p>To illustrate the results from the model, we use an example with the number of local clusters set to 3 in all the four contexts (gene expression, DNA methylation, miRNA expression and RPPA) and 18 global clusters. The number of global clusters acts as an upper bound on the number of clusters that can be represented in the data by the clustering model. The number of clusters that the model identified in the presented result was 16. <xref ref-type="fig" rid="pcbi.1005781.g005">Fig 5</xref> shows the size distribution of these 16 clusters. There are several larger clusters and a larger number of smaller clusters.</p>
<fig id="pcbi.1005781.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Sizes of global clusters identified in the breast cancer dataset from TCGA, using the model with 3 context-specific clusters and up to 18 global clusters.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g005" xlink:type="simple"/>
</fig>
<p>The global combinatorial clusters identified by the model showed clinical significance in terms of survival probabilities. <xref ref-type="fig" rid="pcbi.1005781.g006">Fig 6</xref> shows the different survival curves corresponding to each of the clusters. The survival probabilities in each cluster are significantly different with <italic>p</italic> = 0.038 using the log-rank test with a null hypothesis that assumes that the survival rates are the same. Here, identification of the global structure helps clustering within individual datasets. Looking at the local cluster structure within each dataset, the differences in cluster survival curves are not individually significant. By looking at the overall global structure, we can identify groups that are characterised by different survival outcomes.</p>
<fig id="pcbi.1005781.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Survival curves for global clusters in the breast cancer dataset from TCGA, using the model with 3 context-specific clusters and up to 18 global clusters.</title>
<p>The differences between the survival curves are significant with <italic>p</italic> = 0.0382 using the log-rank test.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g006" xlink:type="simple"/>
</fig>
<p><xref ref-type="fig" rid="pcbi.1005781.g007">Fig 7</xref> shows the global clusters as they appear in the four individual contexts in the first two principal components. The clusters are better defined for the gene expression and the protein assay context. However, the samples corresponding to each cluster also occupy distinguishable regions in the remaining contexts. Because the plots show only the first two principal components, we also provide an overview of variance captured by these components in <xref ref-type="supplementary-material" rid="pcbi.1005781.s001">S1 Appendix</xref>.</p>
<fig id="pcbi.1005781.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g007</object-id>
<label>Fig 7</label>
<caption>
<title>PCA projection of the global clusters in individual contexts in the breast cancer dataset from TCGA, from the model with 3 context-specific clusters and up to 18 global clusters.</title>
<p>The colours correspond to the colours used in the survival curves in <xref ref-type="fig" rid="pcbi.1005781.g006">Fig 6</xref>. (a) Gene expression context. (b) DNA methylation context. (c) miRNA expression context. (d) RPPA context.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g007" xlink:type="simple"/>
</fig>
<p>In contrast to the global clusters in Figs <xref ref-type="fig" rid="pcbi.1005781.g007">7</xref> and <xref ref-type="fig" rid="pcbi.1005781.g008">8</xref> shows the inferred local clusters within each dataset. The clusters are well defined in all contexts except the miRNA expression dataset, where the model did not identify any structure. However, by sharing information from other contexts we see that some of the global clusters are distinguishable in this context as well. For example, in <xref ref-type="fig" rid="pcbi.1005781.g007">Fig 7c</xref> Cluster 3 (violet) occupies a distinguishable region in the miRNA context although the dataset itself does not show any pronounced clustering tendency within the first two principal components. This indicates that the identified clusters represent some feature of the underlying process even within datasets that, by themselves, contain only a vague structure.</p>
<fig id="pcbi.1005781.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g008</object-id>
<label>Fig 8</label>
<caption>
<title>PCA projection of the local clusters identified in individual contexts in the breast cancer dataset from TCGA, from the model with 3 context-specific clusters and up to 18 global clusters.</title>
<p>(a) Context 1 represents the gene expression dataset which contains three local clusters. (b) Context 2 represents the DNA methylation dataset and contains two local clusters. (c) Context 3 represents the miRNA expression dataset with only 1 cluster. (d) Context 4 corresponds to the RPPA dataset, which contains three local clusters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g008" xlink:type="simple"/>
</fig>
<p>To highlight some of the features of the presented context-dependent clustering model, we look at two clusters in more detail, namely clusters 1 and 6. <xref ref-type="fig" rid="pcbi.1005781.g009">Fig 9</xref> shows the two highlighted clusters. We can see that they are similar in all contexts except gene expression, where they belong to different clusters. This is the type of structure where two clusters are merged into one in some of the contexts and separate in other contexts, which we illustrated at the beginning of this chapter using the example of the HER2 gene. This type of structure relates to the interplay of different levels in the underlying biological process.</p>
<fig id="pcbi.1005781.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g009</object-id>
<label>Fig 9</label>
<caption>
<title>PCA projections of the global clusters identified in individual contexts in the breast cancer dataset from TCGA.</title>
<p>The two highlighted clusters differ only in the gene expression context but they are merged in the other contexts. (a) Context 1 represents the gene expression dataset where the two clusters are separate. (b) Context 2 represents the DNA methylation dataset. (c) Context 3 represents the miRNA expression dataset. (d) Context 4 corresponds to the RPPA dataset.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g009" xlink:type="simple"/>
</fig>
<p>The differences between the two clusters are also biologically relevant. <xref ref-type="fig" rid="pcbi.1005781.g010">Fig 10</xref> shows the highlighted survival curves for the two clusters. The green cluster (Cluster 6) contains 40 samples and has significantly different survival outcomes from the dark blue cluster (Cluster 1) containing 19 patient samples, with a log-rank test <italic>p</italic>-value of 0.012.</p>
<fig id="pcbi.1005781.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Survival curves for clusters in <xref ref-type="fig" rid="pcbi.1005781.g009">Fig 9</xref>.</title>
<p>The highlighted clusters have different survival probabilities with <italic>p</italic> = 0.012 under the log-rank survival model.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g010" xlink:type="simple"/>
</fig>
<p>The presented result from the model shows that the identified structure is clinically relevant. The assumptions behind the Clusternomics algorithm also help identify combined global clusters that would be missed by other algorithms, as demonstrated on the simulated datasets.</p>
</sec>
<sec id="sec008">
<title>Stability of inferred clusters</title>
<p>The illustration of the results looked at a specific size of the model. The algorithm also allows the user to specify the number of clusters both on the local context-specific level, and on the global level. We fitted the model with varying numbers of clusters on both levels and we look at the stability of results with respect to different parameter settings. The results show that the estimated global clustering structure is stable when the number of clusters is large enough to give the model sufficient flexibility to fit the data.</p>
<p>
<xref ref-type="fig" rid="pcbi.1005781.g011">Fig 11</xref> shows a comparison of log likelihoods of the fitted models for 10 different numbers of global clusters. The maximum of the log likelihood corresponds to the model with 18 global clusters that we highlighted as an example in the previous section. The same model also corresponds to the lowest <italic>p</italic>-value with respect to the differences in the survival function, as shown in <xref ref-type="fig" rid="pcbi.1005781.g011">Fig 11b</xref>.</p>
<fig id="pcbi.1005781.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Log likelihoods (a) and survival <italic>p</italic>-values (b) of models with different numbers of global clusters.</title>
<p>The first significant difference in survival corresponds to the model with the highest log likelihood.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g011" xlink:type="simple"/>
</fig>
<p>The number of global clusters specified in the algorithm serves only as the upper limit on the number of occupied clusters. To investigate this relation we also look at the number of clusters that are actually occupied for different settings of the number of global clusters. <xref ref-type="fig" rid="pcbi.1005781.g012">Fig 12</xref> shows the posterior average number of clusters that had samples assigned to them. The figure shows both the average total number of occupied clusters across Gibbs samples for each run, and the average number of <italic>large</italic> clusters (clusters that have at least five samples assigned to them). The number of large clusters is more reflective of the actual underlying structure, because Dirichlet mixture models with large number of components are inconsistent with respect to the true number of clusters [<xref ref-type="bibr" rid="pcbi.1005781.ref011">11</xref>]. <xref ref-type="fig" rid="pcbi.1005781.g012">Fig 12</xref> shows that both the number of larger clusters and the number of all clusters is stable between the different settings, when the model is saturated (with 18 or more global clusters) and provided that the corresponding chain reached convergence. The only exception is the solution with 45 global clusters where the algorithm identified an alternative clustering solution, with lower likelihood. In this case the probabilistic algorithm remained in a local optimum. We provide additional analysis of convergence of the presented algorithm in <xref ref-type="supplementary-material" rid="pcbi.1005781.s001">S1 Appendix</xref>.</p>
<fig id="pcbi.1005781.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Average number of occupied clusters across different numbers of global clusters.</title>
<p>The number of clusters is the average of the posterior number of global clusters that have any samples assigned to them across the MCMC iterations. The figure shows both the total number of occupied clusters and the number of clusters that have more than 5 samples assigned to them.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g012" xlink:type="simple"/>
</fig>
<p>Even though the number of estimated clusters changes with parameter settings, the cluster structures might still be very similar. In order to explore these similarities, the ARI between pairs of clusterings of different sizes is displayed graphically in <xref ref-type="fig" rid="pcbi.1005781.g013">Fig 13</xref>. As the area of high ARI values (that is, similarity of clusterings) in the upper right corner indicates, the solution with 18 global clusters is a part of a larger group of solutions with similar cluster assignments. The result with 18 global clusters corresponds to the maximal log likelihood also corresponds to the smallest number of global clusters for which the results stabilise and then remain consistent for larger numbers of global clusters. The result with 45 global clusters is again an outlier.</p>
<fig id="pcbi.1005781.g013" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g013</object-id>
<label>Fig 13</label>
<caption>
<title>Consistency between global clustering results for different number of global clusters with 3 context-specific clusters, as measured by the ARI.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g013" xlink:type="simple"/>
</fig>
<p>
<xref ref-type="fig" rid="pcbi.1005781.g014">Fig 14</xref> shows the pairwise ARI values for local cluster assignments. The local assignments become stable at various sizes of the model. Generally, the larger number of possible global clusters give the model more flexibility to model cluster structures in individual datasets, as opposed to modelling a common global structure. Again, 18 global clusters represent the parameter setting where all the context-specific clusters converge to similar cluster assignments.</p>
<fig id="pcbi.1005781.g014" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g014</object-id>
<label>Fig 14</label>
<caption>
<title>Consistency between local clustering results for different number of global clusters with 3 context-specific clusters, as measured by the ARI.</title>
<p>The ARI values show several local optima. (a) Gene expression context. (b) DNA methylation context. (c) miRNA context. (d) RPPA context.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g014" xlink:type="simple"/>
</fig>
<p>We also look at the effect of setting different numbers of local clusters. We look in detail at 3, 4 and 5 local clusters in every local context-specific dataset. <xref ref-type="fig" rid="pcbi.1005781.g015">Fig 15</xref> shows the agreement between the results as measured by the ARI. All the cluster assignments are relatively consistent given different model sizes with high pairwise ARI values.</p>
<fig id="pcbi.1005781.g015" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g015</object-id>
<label>Fig 15</label>
<caption>
<title>Consistency between global clustering results for different number of local context-specific clusters, as measured by the ARI.</title>
<p>The compared models were trained with 18 global clusters and 3 to 5 context-specific clusters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g015" xlink:type="simple"/>
</fig>
<p>In general, the number of clusters tends to saturate for larger sizes of the model. Because the model asymptotically approaches a Dirichlet process when the number of global/local clusters is large, the model automatically infers the number of clusters that is needed to represent the data (see the <xref ref-type="sec" rid="sec010">Methods</xref> section for details). However, smaller sizes of the model are more computationally efficient in real-world scenarios. To infer a reliable clustering of real-world data, it is necessary to explore several different settings of the model’s parameters, where stability of the clustering can serve as an indicator of clustering tendencies in the dataset [<xref ref-type="bibr" rid="pcbi.1005781.ref013">13</xref>].</p>
<p>To assist with selecting the number of clusters, the package <italic>clusternomics</italic> also provides the Deviance Information Criterion (DIC, [<xref ref-type="bibr" rid="pcbi.1005781.ref014">14</xref>]). DIC is a criterion for model selection for Bayesian models, combining posterior likelihood with penalty for model complexity. Number of clusters should be chosen so that the value of DIC is minimized. <xref ref-type="fig" rid="pcbi.1005781.g016">Fig 16</xref> shows the DIC values for the number of global clusters for the breast cancer dataset. Based on this measure, the optimal number of clusters is 18, which is also the value where the cluster results become stable.</p>
<fig id="pcbi.1005781.g016" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g016</object-id>
<label>Fig 16</label>
<caption>
<title>Deviance information criterion (DIC) as a method for selecting number of clusters.</title>
<p>The plot shows the DIC for a range of numbers of global clusters when the number of local clusters is set to three. The DIC is minimized for 18 global clusters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g016" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec009">
<title>Case study: Prognostic clusters in lung and kidney cancer</title>
<p>To evaluate the general utility of the model, we also examined two additional smaller cancer datasets from The Cancer Genome Atlas repository:</p>
<list list-type="bullet">
<list-item>
<p>Lung cancer samples from 106 patients with 3 contexts: gene expression (12, 042 genes), DNA methylation (23,074 loci) and miRNA expression (352 miRNAs)</p>
</list-item>
<list-item>
<p>Kidney cancer samples from 122 patients with 3 contexts: gene expression (17,899 genes), DNA methylation (24,960 loci) and miRNA expression (329 miRNAs)</p>
</list-item>
</list>
<p specific-use="continuation">The datasets were normalised to zero mean and unit variance for every feature, but the total size of the data was not reduced. For both datasets we again fit models with 3 to 5 local clusters in each context and varying numbers of global clusters.</p>
<p>In this case, the size of the problem in terms of number of genes and DNA methylation loci is problematic for the iCluster algorithm. For larger problems with more features it becomes increasingly memory intensive (at least 60 GB of memory were required to run iCluster on the lung dataset, and the algorithm did not terminate in a reasonable period of time).</p>
<p>Figs <xref ref-type="fig" rid="pcbi.1005781.g017">17</xref> and <xref ref-type="fig" rid="pcbi.1005781.g018">18</xref> show the summary of the results on the lung cancer dataset, using the clinical survival information. The differences in survival prospects in the clusters are statistically significant for all clusterings of this dataset. The consistency of results reveals that there are three versions of stable cluster assignments across the models of different sizes. Figs <xref ref-type="fig" rid="pcbi.1005781.g019">19</xref> and <xref ref-type="fig" rid="pcbi.1005781.g020">20</xref> show similar results for the kidney cancer dataset. Here, the survival <italic>p</italic>-values drop below the significance threshold when the cluster assignments become more stable and consistent. In both cancer datasets, the Clusternomics algorithm identified clinically relevant clusters.</p>
<fig id="pcbi.1005781.g017" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g017</object-id>
<label>Fig 17</label>
<caption>
<title>Consistency of results and survival <italic>p</italic>-values for clusters identified in the lung cancer dataset, with a range of numbers of global clusters and 3 local clusters.</title>
<p>(a) Consistency of results with respect to the ARI between different settings of numbers of global clusters. (b) <italic>p</italic>-values corresponding to the different numbers of global clusters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g017" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005781.g018" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g018</object-id>
<label>Fig 18</label>
<caption>
<title>Deviance information criterion (DIC) for selecting number of clusters in the lung cancer dataset.</title>
<p>The plot shows the DIC for a range of numbers of global clusters when the number of local clusters is set to three. The DIC is minimized for 53 global clusters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g018" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005781.g019" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g019</object-id>
<label>Fig 19</label>
<caption>
<title>Consistency of results and survival <italic>p</italic>-values for clusters identified in the kidney cancer dataset, with a range of numbers of global clusters and 3 local clusters.</title>
<p>(a) Consistency of results with respect to the ARI between different settings of numbers of global clusters. (b) <italic>p</italic>-values corresponding to the different numbers of global clusters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g019" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005781.g020" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g020</object-id>
<label>Fig 20</label>
<caption>
<title>Deviance information criterion (DIC) as a method for selecting number of clusters in the kidney cancer dataset.</title>
<p>The plot shows the DIC for a range of numbers of global clusters when the number of local clusters is set to three. The DIC is minimized for 16 global clusters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g020" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec010" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec011">
<title>The context-dependent clustering model</title>
<p>The context-dependent clustering model explicitly represents both the local clusters within each dataset (local context), and the global structure that emerges when looking at the combination of cluster assignments across the individual datasets.</p>
<p>When we consider a local structure within several datasets, each dataset has its own context-specific set of clusters. When we look at the combination of the context-specific clusters, we get a combined structure which defines clusters on the <italic>global</italic> level while keeping information about cluster membership on the <italic>local</italic> level. For example, if one context (dataset) contains two clusters of samples labelled 1 and 2, and second dataset contains three clusters, labelled <italic>A</italic>, <italic>B</italic> and <italic>C</italic>, we get a combined structure with six potential global clusters where each cluster corresponds a combination of assignments on the local level:
<disp-formula id="pcbi.1005781.e005"><alternatives><graphic id="pcbi.1005781.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>B</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mi>B</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>Fig <xref ref-type="fig" rid="pcbi.1005781.g021">21</xref> provides a schematic illustration of the concepts. Using this formulation, we can model groups of data that are joined in one context and separated in another context, because they correspond to different global clusters.</p>
<fig id="pcbi.1005781.g021" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g021</object-id>
<label>Fig 21</label>
<caption>
<title>Illustration of the concepts of <italic>global</italic> and <italic>local</italic> clusters.</title>
<p>The first dataset contains two clusters 1 and 2, the second dataset contains three clusters, <italic>A</italic>, <italic>B</italic> and <italic>C</italic>. The combined structure contains six potential global clusters that correspond to combinations of assignments on the local context level.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g021" xlink:type="simple"/>
</fig>
<p>We use this intuition to develop the context-dependent clustering model, which is based on a Bayesian probabilistic clustering. The algorithm is based on Dirichlet mixture models [<xref ref-type="bibr" rid="pcbi.1005781.ref015">15</xref>] and their infinite generalisation, the Dirichlet process mixture model [<xref ref-type="bibr" rid="pcbi.1005781.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005781.ref017">17</xref>].</p>
<p>The probabilistic model accounts both for probabilities of individual samples belonging to a specific cluster on the local level, and for the global probabilities of samples belonging to a global cluster. The global clusters are modelled as a combination of local clusters, and bound together by a Bayesian hierarchical model. This assures that a local cluster assignment changes the posterior probabilities of corresponding global cluster assignments.</p>
<p>Continuing with the two dataset example, when a sample is assigned to cluster 1 in the first dataset, it increases the posterior probability of the cluster within the first dataset, but it also increases the probability of global clusters (1, <italic>A</italic>), (1, <italic>B</italic>) and (1, <italic>C</italic>). This dependence is defined by the Bayesian hierarchical model, and encodes the objectives stated in the Introduction.</p>
</sec>
<sec id="sec012">
<title>Model description</title>
<p>In this section we introduce two alternative Bayesian probabilistic models for context-specific clustering. The first model looks explicitly at all possible combinations of local clusters which define the global combinatorial clusters. The second formulation of context-dependent clustering only models a restricted set of combinations of local clusters, that are data-driven.</p>
<p>Both models are asymptotically equivalent (for details see <xref ref-type="supplementary-material" rid="pcbi.1005781.s001">S1 Appendix</xref>). The first formulation of the model provides an intuition that leads to the second formulation of the model. Because the second formulation uses only a smaller number of local clusters, it is more computationally efficient than the first formulation, and it was therefore used to compute the results presented in the Results section.</p>
<p>We introduce the following notation: <bold><italic>x</italic></bold><sub><italic>n</italic></sub>, <italic>n</italic> = 1, …, <italic>N</italic>, are data items where <bold><italic>x</italic></bold><sub><italic>n</italic></sub> is composed of a set of observed values coming from contexts <italic>c</italic> = 1, …, <italic>C</italic>:
<disp-formula id="pcbi.1005781.e006"><alternatives><graphic id="pcbi.1005781.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
For example, for <italic>C</italic> = 2, <bold><italic>x</italic></bold><sub><italic>n</italic></sub> may represent a tumour sample from patient <italic>n</italic> with gene expression values <inline-formula id="pcbi.1005781.e007"><alternatives><graphic id="pcbi.1005781.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and DNA copy number states <inline-formula id="pcbi.1005781.e008"><alternatives><graphic id="pcbi.1005781.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>.</p>
<sec id="sec013">
<title>Basic Dirichlet mixture model</title>
<p>The basis of the integrative hierarchical model is standard Bayesian model-based clustering with Dirichlet prior [<xref ref-type="bibr" rid="pcbi.1005781.ref015">15</xref>]. This basic model is used to cluster data within each context <italic>c</italic>.</p>
<p>This clustering model has been previously applied to gene expression studies, see for example Medvedovic et al. 2002 [<xref ref-type="bibr" rid="pcbi.1005781.ref018">18</xref>]. Lock and Dunson 2013 [<xref ref-type="bibr" rid="pcbi.1005781.ref007">7</xref>] used the same model as the basis of their integrative BCC model.</p>
<p><italic>K</italic><sup>(<italic>c</italic>)</sup> is a fixed parameter for the maximum number of clusters in dataset <italic>c</italic> and <inline-formula id="pcbi.1005781.e009"><alternatives><graphic id="pcbi.1005781.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is an indicator variable which defines the cluster assignment of sample <inline-formula id="pcbi.1005781.e010"><alternatives><graphic id="pcbi.1005781.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1005781.e011"><alternatives><graphic id="pcbi.1005781.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>∈</mml:mo> <mml:mo>{</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The probability of a sample belonging to a cluster <italic>k</italic> in context <italic>c</italic> is <inline-formula id="pcbi.1005781.e012"><alternatives><graphic id="pcbi.1005781.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msubsup><mml:mi>π</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>,
<disp-formula id="pcbi.1005781.e013"><alternatives><graphic id="pcbi.1005781.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mrow><mml:msubsup><mml:mi>π</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula>
Values <inline-formula id="pcbi.1005781.e014"><alternatives><graphic id="pcbi.1005781.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>π</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>π</mml:mi> <mml:mrow><mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> define the weights of each mixture component and follow a Dirichlet distribution with concentration parameter <italic>α</italic><sub>0</sub> <disp-formula id="pcbi.1005781.e015"><alternatives><graphic id="pcbi.1005781.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>∼</mml:mo> <mml:mtext>Dirichlet</mml:mtext> <mml:mo>(</mml:mo> <mml:mfrac><mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfrac> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mfrac><mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
We define a finite mixture distribution for samples <inline-formula id="pcbi.1005781.e016"><alternatives><graphic id="pcbi.1005781.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> as
<disp-formula id="pcbi.1005781.e017"><alternatives><graphic id="pcbi.1005781.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:munderover> <mml:msubsup><mml:mi>π</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mspace width="0.277778em"/><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <inline-formula id="pcbi.1005781.e018"><alternatives><graphic id="pcbi.1005781.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> defines a probability distribution for sample <inline-formula id="pcbi.1005781.e019"><alternatives><graphic id="pcbi.1005781.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> under mixture component <italic>k</italic><sup>(<italic>c</italic>)</sup>, with parameters <inline-formula id="pcbi.1005781.e020"><alternatives><graphic id="pcbi.1005781.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>.</p>
<p>The distributions in the mixture model for each context <italic>c</italic> are summarised as follows:
<disp-formula id="pcbi.1005781.e021"><alternatives><graphic id="pcbi.1005781.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="0.277778em"/><mml:mtext>Dirichlet</mml:mtext> <mml:mo>(</mml:mo> <mml:mfrac><mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfrac> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mfrac><mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="0.277778em"/><mml:mtext>Categorical</mml:mtext> <mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="0.277778em"/><mml:msup><mml:mi>H</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>∣</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>k</mml:mi> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="0.277778em"/><mml:msup><mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>H</italic><sup>(<italic>c</italic>)</sup> is some base prior distribution for parameters of each mixture component; <italic>F</italic><sup>(<italic>c</italic>)</sup> is a probability distribution for samples given parameters <inline-formula id="pcbi.1005781.e022"><alternatives><graphic id="pcbi.1005781.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>. Note that the parameters <italic>θ</italic><sup>(<italic>c</italic>)</sup> and data distribution <italic>F</italic><sup>(<italic>c</italic>)</sup> depend on the context <italic>c</italic> and therefore can be modelled differently for different contexts. In general, we can also use different concentration parameters <italic>α</italic><sub><italic>c</italic></sub> for each context.</p>
</sec>
</sec>
<sec id="sec014">
<title>First model formulation: Fully combinatorial model</title>
<p>Using the basic model presented in the preceding section we can now construct a composite model for integrative clustering, which we call Context-Dependent Clustering (CDC). To keep the notation simple, we first present the model for only two contexts, <italic>c</italic> ∈ {1, 2}. We start by specifying two mixture distributions as defined in the previous section, one for each context. Each context has its own mixture weights <bold><italic>π</italic></bold><sup>(1)</sup> and <bold><italic>π</italic></bold><sup>(2)</sup>, with symmetric Dirichlet priors:
<disp-formula id="pcbi.1005781.e023"><alternatives><graphic id="pcbi.1005781.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi mathvariant="bold-italic">π</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mo>∼</mml:mo> <mml:mtext>Dirichlet</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msup><mml:mi mathvariant="bold-italic">π</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mo>∼</mml:mo> <mml:mtext>Dirichlet</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
These two mixture distributions form the basis of <italic>local</italic> clustering within each context. We link the two distributions together using a third mixture distribution, also with a Dirichlet prior over the mixture weights <bold><italic>ρ</italic></bold>. This represents the <italic>global</italic> mixture distribution, defined over the outer product of <bold><italic>π</italic></bold><sup>(1)</sup> and <bold><italic>π</italic></bold><sup>(2)</sup>:
<disp-formula id="pcbi.1005781.e024"><alternatives><graphic id="pcbi.1005781.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mrow><mml:mi mathvariant="bold-italic">ρ</mml:mi> <mml:mo>∼</mml:mo> <mml:mtext>Dirichlet</mml:mtext> <mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:mspace width="0.277778em"/><mml:mtext>vec</mml:mtext> <mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>⊗</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
Here the outer product of <bold><italic>π</italic></bold><sup>(1)</sup> and <bold><italic>π</italic></bold><sup>(2)</sup> is
<disp-formula id="pcbi.1005781.e025"><alternatives><graphic id="pcbi.1005781.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">π</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mo>⊗</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">π</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">π</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">π</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow/></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>π</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:msubsup><mml:mi>π</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:msubsup><mml:mi>π</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:msubsup><mml:mi>π</mml:mi> <mml:mn>2</mml:mn> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mtd> <mml:mtd><mml:mo>…</mml:mo></mml:mtd> <mml:mtd><mml:mrow><mml:msubsup><mml:mi>π</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:msubsup><mml:mi>π</mml:mi> <mml:mi>K</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>π</mml:mi> <mml:mn>2</mml:mn> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:msubsup><mml:mi>π</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:msubsup><mml:mi>π</mml:mi> <mml:mn>2</mml:mn> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:msubsup><mml:mi>π</mml:mi> <mml:mn>2</mml:mn> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mtd> <mml:mtd><mml:mo>…</mml:mo></mml:mtd> <mml:mtd><mml:mrow><mml:msubsup><mml:mi>π</mml:mi> <mml:mn>2</mml:mn> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:msubsup><mml:mi>π</mml:mi> <mml:mi>K</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋱</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>π</mml:mi> <mml:mi>K</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:msubsup><mml:mi>π</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:msubsup><mml:mi>π</mml:mi> <mml:mi>K</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:msubsup><mml:mi>π</mml:mi> <mml:mn>2</mml:mn> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mtd> <mml:mtd><mml:mo>…</mml:mo></mml:mtd> <mml:mtd><mml:mrow><mml:msubsup><mml:mi>π</mml:mi> <mml:mi>K</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:msubsup><mml:mi>π</mml:mi> <mml:mi>K</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
The vec operation takes a matrix and stacks its columns on top of each other to form one column vector.</p>
<p>We use the outer product matrix in a vectorised form as the basis for the (non-symmetric) concentration parameters of the Dirichlet distribution over <italic>global</italic> mixture weights <bold><italic>ρ</italic></bold>. Each element of <bold><italic>ρ</italic></bold> corresponds to a specific pair of local cluster probabilities <inline-formula id="pcbi.1005781.e026"><alternatives><graphic id="pcbi.1005781.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:msubsup><mml:mi>π</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005781.e027"><alternatives><graphic id="pcbi.1005781.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:msubsup><mml:mi>π</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>. This effectively creates a mixture model over all possible combinations of cluster assignments on the level of individual contexts. The prior probability of a data item being simultaneously assigned into cluster <italic>k</italic> in the first context and cluster <italic>l</italic> in the second context corresponds to the element <italic>s</italic> in the <bold><italic>ρ</italic></bold> vector which originated from the row <italic>k</italic> and column <italic>l</italic> in the outer product matrix <xref ref-type="disp-formula" rid="pcbi.1005781.e025">(2)</xref>:
<disp-formula id="pcbi.1005781.e028"><alternatives><graphic id="pcbi.1005781.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mi>ρ</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mspace width="2.em"/><mml:mspace width="4.pt"/><mml:mtext>where</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
<p>We also define a composite cluster indicator variable <bold><italic>z</italic></bold> for each sample that represents a pair of <italic>z</italic><sup>(<italic>c</italic>)</sup> values, one for each context <italic>c</italic>:
<disp-formula id="pcbi.1005781.e029"><alternatives><graphic id="pcbi.1005781.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mspace width="1pt"/><mml:mtext>Categorical</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">ρ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mspace width="1pt"/><mml:mo>=</mml:mo><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mspace width="1pt"/><mml:mo>∣</mml:mo><mml:mspace width="1pt"/><mml:msubsup><mml:mi>z</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mspace width="1pt"/><mml:msup><mml:mi>F</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:mspace width="1pt"/><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow> <mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>By fitting this model to a dataset, we obtain both local clusters on the level of the original datasets (contexts) and the global composite clusters that represent different combinations of individual local cluster assignments. Local clusters for context <italic>c</italic> can be obtained by taking a projection of the <bold><italic>z</italic></bold> indicator variable onto the context <italic>c</italic>.</p>
<p>Going back to the HER2 oncogene example in the Introduction, the model explicitly represents the local clusters with respect to DNA copy number changes and mRNA expression, while forming two global clusters with respect to the overall behaviour.</p>
<p>The model satisfies the objectives presented in the Introduction. In the posterior, assignments into a context-specific cluster affect the posterior distribution of <italic>π</italic><sup>(<italic>c</italic>)</sup>, which in turn affects the probabilities of all global combinatorial clusters through the hierarchical model. For example if we look at the outer product matrix <xref ref-type="disp-formula" rid="pcbi.1005781.e025">(2)</xref> which is a part of the prior for global mixture weights <bold><italic>ρ</italic></bold>, change in the value of <inline-formula id="pcbi.1005781.e030"><alternatives><graphic id="pcbi.1005781.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:msubsup><mml:mi>π</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> changes values of the whole <italic>k</italic>-th row in the matrix.</p>
<p>The model also represents different degrees of dependence by allowing any combination of cluster assignments across contexts. When there is a single common cluster structure across the two contexts, the occupied clusters will be concentrated along the diagonal of the probability matrix <xref ref-type="disp-formula" rid="pcbi.1005781.e025">(2)</xref>.</p>
<p>For <italic>C</italic> &gt; 2, the outer product of <inline-formula id="pcbi.1005781.e031"><alternatives><graphic id="pcbi.1005781.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msup><mml:mi>π</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> generalises into a tensor product. Each element of <bold><italic>ρ</italic></bold> represents the probability of a <italic>C</italic>-tuple of cluster assignments, i.e. a specific combination of cluster assignments in specific contexts. To summarise the model, we give a general formulation for <italic>C</italic> contexts.
<disp-formula id="pcbi.1005781.e032"><alternatives><graphic id="pcbi.1005781.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e032" xlink:type="simple"/><mml:math display="block" id="M32"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>∣</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="0.277778em"/><mml:mtext>Dirichlet</mml:mtext> <mml:mo>(</mml:mo> <mml:mfrac><mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfrac> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mfrac><mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold-italic">ρ</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>,</mml:mo> <mml:mo>{</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>}</mml:mo> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="0.277778em"/><mml:mtext>Dirichlet</mml:mtext> <mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>{</mml:mo> <mml:munder><mml:mo>⊗</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mi>C</mml:mi></mml:mrow></mml:munder> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>}</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>∣</mml:mo> <mml:mi mathvariant="bold-italic">ρ</mml:mi> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="0.277778em"/><mml:mtext>Categorical</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">ρ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="0.277778em"/><mml:msup><mml:mi>H</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>∣</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>k</mml:mi> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="0.277778em"/><mml:msup><mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mrow><mml:mo>|</mml:mo> <mml:mspace width="0.277778em"/></mml:mrow> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula></p>
<p>
<xref ref-type="fig" rid="pcbi.1005781.g022">Fig 22</xref> shows the graphical model for the full context-dependent clustering model.</p>
<fig id="pcbi.1005781.g022" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g022</object-id>
<label>Fig 22</label>
<caption>
<title>Graphical model representation of the fully combinatorial context-dependent clustering model.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g022" xlink:type="simple"/>
</fig>
<p>In general, the number of clusters <italic>K</italic><sup>(<italic>c</italic>)</sup> can be different for each context <italic>c</italic>. Given that the number of clusters in each dataset is <inline-formula id="pcbi.1005781.e033"><alternatives><graphic id="pcbi.1005781.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>K</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, the total length of <bold><italic>ρ</italic></bold> parameter vector is <inline-formula id="pcbi.1005781.e034"><alternatives><graphic id="pcbi.1005781.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mi>C</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> because the model represents all possible combinations of cluster assignments. This yields a very large number of potential combinatorial clusters. However, only a small number of clusters is actually represented in the data and many of the clusters remain empty as shown by Rousseau et al. [<xref ref-type="bibr" rid="pcbi.1005781.ref019">19</xref>]. Also, by using small values for the concentration parameter <italic>γ</italic> &lt; 1, we encourage the data to be concentrated in only a small number of global mixture components.</p>
<p>This model explicitly represents all the possible combinations of cluster assignments, which may not be desirable in real-world applications, where the model may use some of the structure to capture technical noise present in the data. Also, although the number of clusters that are occupied is smaller in the posterior, the model size grows exponentially with the number of contexts. In the next section, we look at an alternative model that circumvents this limitation.</p>
</sec>
<sec id="sec015">
<title>Second model formulation: Decoupled combinatorial model</title>
<p>To avoid the large number of potential cluster combinations in the previous model, we decouple the number of context-specific clusters <italic>K</italic><sup>(<italic>c</italic>)</sup>, <italic>c</italic> = 1, …, <italic>C</italic>, and the number of global clusters. First a mixture distribution over <italic>S</italic> global clusters is defined similarly to the finite Dirichlet mixture model [<xref ref-type="bibr" rid="pcbi.1005781.ref015">15</xref>].
<disp-formula id="pcbi.1005781.e035"><alternatives><graphic id="pcbi.1005781.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold-italic">ρ</mml:mi> <mml:mo>∣</mml:mo> <mml:msub><mml:mi>γ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="0.277778em"/><mml:mtext>Dirichlet</mml:mtext> <mml:mo>(</mml:mo> <mml:mfrac><mml:msub><mml:mi>γ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mi>S</mml:mi></mml:mfrac> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mfrac><mml:msub><mml:mi>γ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mi>S</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>z</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>∣</mml:mo> <mml:mi>ρ</mml:mi> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="0.277778em"/><mml:mtext>Categorical</mml:mtext> <mml:mo>(</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <bold><italic>ρ</italic></bold> are the mixture weights and <italic>z</italic><sub><italic>n</italic></sub> are standard cluster assignment indicator variables. We combine context-specific clusters with the global clusters using the following method: we specify context mixture distributions and a set of assignment variables <inline-formula id="pcbi.1005781.e036"><alternatives><graphic id="pcbi.1005781.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:msubsup><mml:mi>k</mml:mi> <mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> that associate global cluster <italic>s</italic> = 1, …, <italic>S</italic> with context-specific clusters:
<disp-formula id="pcbi.1005781.e037"><alternatives><graphic id="pcbi.1005781.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">π</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>∣</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="0.277778em"/><mml:mtext>Dirichlet</mml:mtext> <mml:mo>(</mml:mo> <mml:mfrac><mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfrac> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mfrac><mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>s</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>∣</mml:mo> <mml:msup><mml:mi>π</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="0.277778em"/><mml:mtext>Categorical</mml:mtext> <mml:mo>(</mml:mo> <mml:msup><mml:mi>π</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula></p>
<p>In this formulation, <inline-formula id="pcbi.1005781.e038"><alternatives><graphic id="pcbi.1005781.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mrow><mml:msubsup><mml:mi>k</mml:mi> <mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>∈</mml:mo> <mml:mo>{</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> assigns the <italic>s</italic>-th global cluster to a specific local cluster in context <italic>c</italic>. One could say that <inline-formula id="pcbi.1005781.e039"><alternatives><graphic id="pcbi.1005781.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>k</mml:mi> <mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> are the coordinates of the global cluster in terms of the local cluster identifiers. The variables <italic>z</italic><sub><italic>n</italic></sub> then assign samples to global clusters. This way, data are represented by a mixture of context-specific cluster combinations. In the previous model <xref ref-type="disp-formula" rid="pcbi.1005781.e032">(3)</xref> the mapping of global clusters to local clusters was implicit, because each combination of context clusters mapped to a unique global cluster. In this model, the mapping is probabilistic and forms a part of the model.</p>
<p>Note that compared to the previous model, we also have to specify the number of potential global clusters <italic>S</italic> which is no longer determined by the number of clusters within each context. This circumvents the problem of large dimensionality of the space of potential cluster combinations across contexts.</p>
<p>To summarise, the complete model can be written as
<disp-formula id="pcbi.1005781.e040"><alternatives><graphic id="pcbi.1005781.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e040" xlink:type="simple"/><mml:math display="block" id="M40"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold-italic">ρ</mml:mi> <mml:mo>∣</mml:mo> <mml:msub><mml:mi>γ</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="1pt"/><mml:mtext>Dirichlet</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>γ</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow> <mml:mi>S</mml:mi></mml:mfrac> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>γ</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow> <mml:mi>S</mml:mi></mml:mfrac></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>z</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>∣</mml:mo> <mml:mi>ρ</mml:mi></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="1pt"/><mml:mtext>Categorical</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">π</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mo>∣</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="1pt"/><mml:mtext>Dirichlet</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>k</mml:mi> <mml:mi>s</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:mo>∣</mml:mo> <mml:msup><mml:mi>π</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="1pt"/><mml:mtext>Categorical</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msup><mml:mi>π</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>l</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:mo>∣</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">H</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="1pt"/><mml:msup><mml:mi mathvariant="bold-italic">H</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:mo>∣</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msubsup><mml:mi>k</mml:mi> <mml:mi>s</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>S</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msub><mml:mi>θ</mml:mi> <mml:mi>l</mml:mi></mml:msub></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo> <mml:mspace width="1pt"/><mml:msup><mml:mi>F</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msubsup><mml:mi>x</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:mo>∣</mml:mo> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mrow><mml:msub><mml:mi>k</mml:mi> <mml:mrow><mml:msub><mml:mi>z</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula></p>
<p><xref ref-type="fig" rid="pcbi.1005781.g023">Fig 23</xref> shows the graphical representation of this model.</p>
<fig id="pcbi.1005781.g023" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005781.g023</object-id>
<label>Fig 23</label>
<caption>
<title>Graphical model representation of the decoupled context-dependent integrative clustering model.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.g023" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec016">
<title>Inference in the model and implementation</title>
<p>We derived Gibbs sampling inference algorithms for both formulations of the context-dependent clustering model, details and the inference equations can be found in <xref ref-type="supplementary-material" rid="pcbi.1005781.s001">S1 Appendix</xref>.</p>
<p>For the fully combinatorial version of the model, computational complexity of each iteration of the Gibbs sampling algorithm is <inline-formula id="pcbi.1005781.e041"><alternatives><graphic id="pcbi.1005781.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mi>C</mml:mi> <mml:msubsup><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:msubsup> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, additionally multiplied by the complexity of evaluating the data likelihood <italic>F</italic><sup>(<italic>c</italic>)</sup> for each context. We also derived approximate variational inference updates for this version of the model, which achieves faster convergence than Gibbs sampling. Details on variational inference in the model are also available in <xref ref-type="supplementary-material" rid="pcbi.1005781.s001">S1 Appendix</xref>.</p>
<p>For the decoupled formulation of the model, the computational complexity of each Gibbs sampling iteration is <inline-formula id="pcbi.1005781.e042"><alternatives><graphic id="pcbi.1005781.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mi>C</mml:mi> <mml:mi>S</mml:mi> <mml:mo>+</mml:mo> <mml:mi>S</mml:mi> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:msubsup> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, again additionally increased by the complexity of evaluating data likelihood <italic>F</italic><sup>(<italic>c</italic>)</sup>. Compared to the fully combinatorial model, complexity of this algorithm is lower for <inline-formula id="pcbi.1005781.e043"><alternatives><graphic id="pcbi.1005781.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005781.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:mi>S</mml:mi> <mml:mo>&lt;</mml:mo> <mml:msubsup><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:msubsup> <mml:msup><mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> because of the decoupled representation.</p>
<p>Implementation of the decoupled version of the context-dependent clustering model, which was used to produce the results presented in the Results section, is available as the R package <monospace>clusternomics</monospace> from CRAN.</p>
</sec>
</sec>
<sec id="sec017" sec-type="conclusions">
<title>Discussion</title>
<p>To summarize, in this paper we proposed a clustering algorithm for integrative analysis of heterogeneous datasets. We described the probabilistic model behind the algorithm, which is closely related to the hierarchical Dirichlet process [<xref ref-type="bibr" rid="pcbi.1005781.ref020">20</xref>]. The proposed context-dependent clustering algorithm models both the local structure within each dataset, and the global structure which arises from combinations of dataset-specific clusters. This form of model enables modelling of heterogeneous related datasets that do not share the same structure.</p>
<p>We described two representations of the model which are equivalent in their limit. The first full model makes the assumptions behind the model explicit and represents all possible combinations of context-specific cluster assignments. Given the number of clusters in each context <italic>K</italic><sup>(<italic>c</italic>)</sup>, the model is a mixture model over all possible combinations of cluster assignments in individual contexts.</p>
<p>The second type of representation is the decoupled CDC model which allows us to specify the number of global clusters <italic>S</italic> that are identified in the data separately from the number of context-dependent clusters <italic>K</italic><sup>(<italic>c</italic>)</sup>. For a large number of global clusters <italic>S</italic> the model is equivalent to the full model. However, the number of global clusters allows us to additionally tune the resulting cluster structure. For smaller numbers of clusters <italic>S</italic>, the global and local cluster structures are forced to be more similar and the algorithm enforces a common cluster structure across all datasets. For larger numbers of global clusters, the model has more flexibility to model the local structure within each dataset.</p>
<p>We evaluated the proposed model both on simulated data and on a set of real-world cancer datasets. The simulated data revealed that other algorithms for integrative clustering do not model situations where there is a varying degree of dependence of cluster structures across multiple datasets.</p>
<p>We also evaluated the proposed context-dependent clustering model on the breast cancer dataset which includes four different contexts, and additionally on two datasets studying lung and kidney cancer. The model successfully identified clinically meaningful clusters as measured by the survival probabilities for each global cluster. We evaluated the decoupled clustering model over a number of possible global clusters. Generally, the best clustering results are obtained when the cluster structure stabilises as measured by the ARI. Senbabaoglu et al. [<xref ref-type="bibr" rid="pcbi.1005781.ref013">13</xref>] note that in real-world datasets there may be many different numbers of clusters that are equally highly plausible. The comparison of clustering results based on their agreement identifies the model sizes that lead to similar sets of cluster assignments. Each group then corresponds to an alternative interpretation of the data.</p>
<p>Overall, the model uses different assumptions about the cluster structure than other currently used integrative clustering algorithms. By dropping the assumption of a single common cluster structure, the model identifies both the local structure of individual datasets, and a global structure that combines the local clusters.</p>
</sec>
<sec id="sec018">
<title>Supporting information</title>
<supplementary-material id="pcbi.1005781.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005781.s001" xlink:type="simple">
<label>S1 Appendix</label>
<caption>
<title>Model and implementation details.</title>
<p>Details on the model, implementation, algorithm setting and MCMC convergence.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors would like to thank David L. Wild and Richard E. Turner for helpful comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005781.ref001">
<label>1</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Weinberg</surname> <given-names>R</given-names></name>. <source>The Biology of Cancer</source>. <edition>Second edi ed</edition>. <publisher-name>Garland Science</publisher-name>; <year>2013</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005781.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ovaska</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Laakso</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Haapa-Paananen</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Louhimo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Aittomäki</surname> <given-names>V</given-names></name>, <etal>et al</etal>. <article-title>Large-scale data integration framework provides a comprehensive view on glioblastoma multiforme</article-title>. <source>Genome medicine</source>. <year>2010</year>;<volume>2</volume>(<issue>9</issue>):<fpage>65</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/gm186" xlink:type="simple">10.1186/gm186</ext-link></comment> <object-id pub-id-type="pmid">20822536</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kristensen</surname> <given-names>VN</given-names></name>, <name name-style="western"><surname>Lingjæ rde</surname> <given-names>OC</given-names></name>, <name name-style="western"><surname>Russnes</surname> <given-names>HG</given-names></name>, <name name-style="western"><surname>Vollan</surname> <given-names>HKM</given-names></name>, <name name-style="western"><surname>Frigessi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bø rresen Dale</surname> <given-names>AL</given-names></name>. <article-title>Principles and methods of integrative genomic analyses in cancer</article-title>. <source>Nature reviews Cancer</source>. <year>2014</year>;<volume>14</volume>(<issue>5</issue>):<fpage>299</fpage>–<lpage>313</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrc3721" xlink:type="simple">10.1038/nrc3721</ext-link></comment> <object-id pub-id-type="pmid">24759209</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Olshen</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Ladanyi</surname> <given-names>M</given-names></name>. <article-title>Integrative clustering of multiple genomic data types using a joint latent variable model with application to breast and lung cancer subtype analysis</article-title>. <source>Bioinformatics (Oxford, England)</source>. <year>2009</year> <month>Nov</month>;<volume>25</volume>(<issue>22</issue>):<fpage>2906</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btp543" xlink:type="simple">10.1093/bioinformatics/btp543</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Mo</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Seshan</surname> <given-names>VE</given-names></name>, <name name-style="western"><surname>Olshen</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Huse</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Integrative subtype discovery in glioblastoma using iCluster</article-title>. <source>PLoS ONE</source>. <year>2012</year>;<volume>7</volume>(<issue>4</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0035236" xlink:type="simple">10.1371/journal.pone.0035236</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hellton</surname> <given-names>KH</given-names></name>, <name name-style="western"><surname>Thoresen</surname> <given-names>M</given-names></name>. <article-title>Integrative clustering of high-dimensional data with joint and individual clusters</article-title>. <source>Biostatistics</source>. <year>2016</year>;<volume>17</volume>(<issue>3</issue>):<fpage>537</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/biostatistics/kxw005" xlink:type="simple">10.1093/biostatistics/kxw005</ext-link></comment> <object-id pub-id-type="pmid">26917056</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lock</surname> <given-names>EF</given-names></name>, <name name-style="western"><surname>Dunson</surname> <given-names>DB</given-names></name>. <article-title>Bayesian consensus clustering</article-title>. <source>Bioinformatics</source>. <year>2013</year>;<volume>29</volume>(<issue>20</issue>):<fpage>2610</fpage>–<lpage>2616</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btt425" xlink:type="simple">10.1093/bioinformatics/btt425</ext-link></comment> <object-id pub-id-type="pmid">23990412</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kirk</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Griffin</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Savage</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Wild</surname> <given-names>DL</given-names></name>. <article-title>Bayesian correlated clustering to integrate multiple datasets</article-title>. <source>Bioinformatics</source>. <year>2012</year>;<volume>28</volume>(<issue>24</issue>):<fpage>3290</fpage>–<lpage>3297</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/bts595" xlink:type="simple">10.1093/bioinformatics/bts595</ext-link></comment> <object-id pub-id-type="pmid">23047558</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wang</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Mezlini</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Demir</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Fiume</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Brudno</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Similarity network fusion for aggregating data types on a genomic scale</article-title>. <source>Nature methods</source>. <year>2014</year>;<volume>11</volume>(<issue>3</issue>):<fpage>333</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.2810" xlink:type="simple">10.1038/nmeth.2810</ext-link></comment> <object-id pub-id-type="pmid">24464287</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hubert</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Arabie</surname> <given-names>P</given-names></name>. <article-title>Comparing partitions</article-title>. <source>Journal of Classification</source>. <year>1985</year>;<volume>2</volume>(<issue>1</issue>):<fpage>193</fpage>–<lpage>218</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF01908075" xlink:type="simple">10.1007/BF01908075</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Harrison</surname> <given-names>MT</given-names></name>. <article-title>A simple example of Dirichlet process mixture inconsistency for the number of components</article-title>. <source>Advances in Neural Information Processing Systems (NIPS)</source>. <year>2013</year>;<volume>26</volume>.</mixed-citation>
</ref>
<ref id="pcbi.1005781.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<collab>The Cancer Genome Atlas Network</collab>. <article-title>Comprehensive molecular portraits of human breast tumours</article-title>. <source>Nature</source>. <year>2012</year>;<volume>490</volume>(<issue>7418</issue>):<fpage>61</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature11412" xlink:type="simple">10.1038/nature11412</ext-link></comment> <object-id pub-id-type="pmid">23000897</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Senbabaoglu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Michailidis</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>JZ</given-names></name>. <article-title>Critical limitations of consensus clustering in class discovery</article-title>. <source>Sci Rep</source>. <year>2014</year> <month>Aug</month>;<volume>4</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/srep06207" xlink:type="simple">10.1038/srep06207</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Spiegelhalter</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Best</surname> <given-names>NG</given-names></name>, <name name-style="western"><surname>Carlin</surname> <given-names>BP</given-names></name>, <name name-style="western"><surname>Van Der Linde</surname> <given-names>A</given-names></name>. <article-title>Bayesian measures of model complexity and fit</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>2002</year>;<volume>64</volume>(<issue>4</issue>):<fpage>583</fpage>–<lpage>639</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/1467-9868.00353" xlink:type="simple">10.1111/1467-9868.00353</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Green</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Richardson</surname> <given-names>S</given-names></name>. <article-title>Modelling heterogeneity with and without the Dirichlet process</article-title>. <source>Scandinavian journal of statistics</source>. <year>2001</year>;. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/1467-9469.00242" xlink:type="simple">10.1111/1467-9469.00242</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ferguson</surname> <given-names>TS</given-names></name>. <article-title>A Bayesian Analysis of Some Nonparametric Problems</article-title>. <source>The Annals of Statistics</source>. <year>1973</year>;<volume>1</volume>(<issue>2</issue>):<fpage>209</fpage>–<lpage>230</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/aos/1176342360" xlink:type="simple">10.1214/aos/1176342360</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Antoniak</surname> <given-names>CE</given-names></name>. <article-title>Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems</article-title>. <source>The Annals of Statistics</source>. <year>1974</year>;<volume>2</volume>(<issue>6</issue>):<fpage>1152</fpage>–<lpage>1174</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/aos/1176342871" xlink:type="simple">10.1214/aos/1176342871</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Medvedovic</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sivaganesan</surname> <given-names>S</given-names></name>. <article-title>Bayesian infinite mixture model based clustering of gene expression profiles</article-title>. <source>Bioinformatics</source>. <year>2002</year> <month>Sep</month>;<volume>18</volume>(<issue>9</issue>):<fpage>1194</fpage>–<lpage>1206</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/18.9.1194" xlink:type="simple">10.1093/bioinformatics/18.9.1194</ext-link></comment> <object-id pub-id-type="pmid">12217911</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rousseau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mengersen</surname> <given-names>K</given-names></name>. <article-title>Asymptotic behaviour of the posterior distribution in overfitted mixture models</article-title>. <source>Journal of the Royal Statistical Society Series B: Statistical Methodology</source>. <year>2011</year>;<volume>73</volume>(<issue>5</issue>):<fpage>689</fpage>–<lpage>710</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-9868.2011.00781.x" xlink:type="simple">10.1111/j.1467-9868.2011.00781.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005781.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Teh</surname> <given-names>YW</given-names></name>, <name name-style="western"><surname>Jordan</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Beal</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Blei</surname> <given-names>DM</given-names></name>. <source>Hierarchical Dirichlet processes</source>; <year>2006</year>. <volume>476</volume>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>