<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="other" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id>
<journal-title-group>
<journal-title>PLOS Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pbio.3000127</article-id>
<article-id pub-id-type="publisher-id">PBIOLOGY-D-18-01005</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Perspective</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Behavioral ecology</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Behavioral ecology</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Behavioral ecology</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Ecology</subject><subj-group><subject>Behavioral ecology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Ecology and environmental sciences</subject><subj-group><subject>Ecology</subject><subj-group><subject>Behavioral ecology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical data</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Science policy</subject><subj-group><subject>Research integrity</subject><subj-group><subject>Publication ethics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Physiological parameters</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Physiological parameters</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Research design</subject><subj-group><subject>Experimental design</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Economics</subject><subj-group><subject>Labor economics</subject><subj-group><subject>Employment</subject><subj-group><subject>Careers</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Metaanalysis</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Metaanalysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical models</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Evidence that nonsignificant results are sometimes preferred: Reverse <italic>P</italic>-hacking or selective reporting?</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5414-0242</contrib-id>
<name name-style="western">
<surname>Chuard</surname>
<given-names>Pierre J. C.</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="fn" rid="econtrib001"><sup>‡</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9058-9220</contrib-id>
<name name-style="western">
<surname>Vrtílek</surname>
<given-names>Milan</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="fn" rid="econtrib001"><sup>‡</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Head</surname>
<given-names>Megan L.</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9221-2788</contrib-id>
<name name-style="western">
<surname>Jennions</surname>
<given-names>Michael D.</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Division of Ecology and Evolution, Research School of Biology, The Australian National University, Canberra, Australia</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Department of Biological Sciences, Bishop's University, Sherbrooke, Canada</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="other" id="econtrib001">
<p>‡ These authors share joint first authorship on this work.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">michael.jennions@anu.edu.au</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>25</day>
<month>1</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>1</month>
<year>2019</year>
</pub-date>
<volume>17</volume>
<issue>1</issue>
<elocation-id>e3000127</elocation-id>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Chuard et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pbio.3000127"/>
<abstract>
<p>There is increased concern about poor scientific practices arising from an excessive focus on <italic>P</italic>-values. Two particularly worrisome practices are selective reporting of significant results and ‘<italic>P</italic>-hacking’. The latter is the manipulation of data collection, usage, or analyses to obtain statistically significant outcomes. Here, we introduce the novel, to our knowledge, concepts of selective reporting of nonsignificant results and ‘reverse <italic>P</italic>-hacking’ whereby researchers ensure that tests produce a nonsignificant result. We test whether these practices occur in experiments in which researchers randomly assign subjects to treatment and control groups to minimise differences in confounding variables that might affect the focal outcome. By chance alone, 5% of tests for a group difference in confounding variables should yield a significant result (<italic>P</italic> &lt; 0.05). If researchers less often report significant findings and/or reverse <italic>P</italic>-hack to avoid significant outcomes that undermine the ethos that experimental and control groups only differ with respect to actively manipulated variables, we expect significant results from tests for group differences to be under-represented in the literature. We surveyed the behavioural ecology literature and found significantly more nonsignificant <italic>P</italic>-values reported for tests of group differences in potentially confounding variables than the expected 95% (<italic>P</italic> = 0.005; <italic>N</italic> = 250 studies). This novel, to our knowledge, publication bias could result from selective reporting of nonsignificant results and/or from reverse <italic>P</italic>-hacking. We encourage others to test for a bias toward publishing nonsignificant results in the equivalent context in their own research discipline.</p>
</abstract>
<abstract abstract-type="toc">
<p>There is concern that the scientific literature is biased towards reporting statistically significant results. By contrast, this Perspective article presents evidence for an unusual situation in which there is a systematic bias towards publishing non-significant findings.</p>
</abstract>
<funding-group>
<funding-statement>PC and MV were funded by Australia Awards-Endeavour Fellowships (IDs 6114_2017 and 6558_2018, respectively) awarded by the Australian Government (URL: <ext-link ext-link-type="uri" xlink:href="https://internationaleducation.gov.au/Endeavour%20program/Scholarships-and-Fellowships/Pages/default.aspx" xlink:type="simple">https://internationaleducation.gov.au/Endeavour%20program/Scholarships-and-Fellowships/Pages/default.aspx</ext-link>); and MH and MJ were funded by the Australian Research Council (DP160100285 and FT160100149) (URL: <ext-link ext-link-type="uri" xlink:href="https://www.arc.gov.au/grants/discovery-program" xlink:type="simple">https://www.arc.gov.au/grants/discovery-program</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="1"/>
<table-count count="0"/>
<page-count count="7"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-02-06</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The publication record of academics is a key factor determining funding and career success [<xref ref-type="bibr" rid="pbio.3000127.ref001">1</xref>]. This constant pressure to publish, and to do so in high-impact journals that strongly favour significant results [<xref ref-type="bibr" rid="pbio.3000127.ref002">2</xref>], has consequences for the extent to which published scientific studies reflect the outcome of all original research. Key studies in many fields are not replicated [<xref ref-type="bibr" rid="pbio.3000127.ref003">3</xref>], and if they are, the original results are often not reproduced [<xref ref-type="bibr" rid="pbio.3000127.ref004">4</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref005">5</xref>]. The latter phenomenon is widely known as the replication crisis [<xref ref-type="bibr" rid="pbio.3000127.ref005">5</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref006">6</xref>]. One reason posited for low replicability is the existence of a publication bias that favours reporting significant findings and suppressing nonsignificant ones [<xref ref-type="bibr" rid="pbio.3000127.ref007">7</xref>].</p>
<p>There are two main types of publication bias. First, selection bias occurs when studies either go unpublished (‘file-drawer effect’) or statistical tests are not reported within published studies (selective reporting) (<xref ref-type="fig" rid="pbio.3000127.g001">Fig 1A</xref>). It often reflects a tendency by authors, reviewers, and editors, respectively, to preferentially submit, recommend, and accept studies with statistically significant findings (usually <italic>P</italic> &lt; 0.05) for publication [<xref ref-type="bibr" rid="pbio.3000127.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref008">8</xref>]. It results in artificially increased estimates of average effect sizes because some conducted studies with smaller effect sizes (i.e., larger <italic>P</italic>-values for a given sample size) are missing from the scientific literature and excluded from meta-analyses or synthetic reviews. Second, <italic>P</italic>-hacking occurs when effect sizes are inflated because of how data are collected or analysed (<xref ref-type="fig" rid="pbio.3000127.g001">Fig 1B</xref>). It involves the active, although not necessarily conscious, manipulation of data, data collection, or statistical tests [<xref ref-type="bibr" rid="pbio.3000127.ref009">9</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref010">10</xref>] to obtain a statistically significant result for a favoured explanatory variable. This can occur by, for example, stopping a study as soon as a significant result is obtained rather than collecting a prespecified sample size, removal of outliers, and inclusion or exclusion of terms from an initially chosen statistical model. Although there are methods to detect missing, unpublished results that arise because of selection bias (for example, the use of funnel plots of effect sizes that should be symmetric) [<xref ref-type="bibr" rid="pbio.3000127.ref011">11</xref>], correcting for <italic>P</italic>-hacking is notoriously difficult. This is partly because the various types of <italic>P</italic>-hacking practices will have different effects on the distribution of <italic>P</italic>-values that result when a pool of studies are considered, some of which have been <italic>P</italic>-hacked [<xref ref-type="bibr" rid="pbio.3000127.ref012">12</xref>].</p>
<fig id="pbio.3000127.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000127.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Frequency distribution of numerical <italic>P</italic>-values (<italic>P</italic>-curve).</title>
<p>Two hypothetical examples of <italic>P</italic>-curves show the expected distribution if (a) selection bias is present or (b) certain types of reverse <italic>P</italic>-hacking occur (see [<xref ref-type="bibr" rid="pbio.3000127.ref012">12</xref>]). Data collected from articles testing the effect of confounding variables between treatment(s) and control groups are shown in (c). In (c), darker red points represent a higher frequency of <italic>P</italic>-values/bin among the randomised datasets. The horizontal red line denotes the theoretical even distribution across 20 bins (188 articles in 20 bins = 9.4 articles/bin). The grey rectangles emphasize intervals of interest in the two hypothetical scenarios. We only used <italic>P</italic>-values presented to at least two decimal places. The full data file of the 1,805 articles consulted and the 250 from which data were extracted and the <italic>R</italic> scripts used for the analysis and to generate the figures are available at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/s/f3bb7dfefdaa8976d3a1" xlink:type="simple">https://figshare.com/s/f3bb7dfefdaa8976d3a1</ext-link> or <ext-link ext-link-type="uri" xlink:href="https://osf.io/au7yc/" xlink:type="simple">https://osf.io/au7yc/</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000127.g001" xlink:type="simple"/>
</fig>
<p>Well-conducted experimental studies allow researchers to reduce the effect of confounding factors that might otherwise hinder the ability to attribute causal effects to manipulated variables. We define a confounding variable as an unmanipulated variable that might affect the interpretation of an experiment if it differs between control and treatment groups or among different treatment groups. For example, when studying the effect of manipulating testosterone levels on male mating success, male body size could be considered a confounding variable because larger males tend to be more attractive to females in many species [<xref ref-type="bibr" rid="pbio.3000127.ref013">13</xref>]. There are two main approaches to minimise the effect of confounding variables in experiments: balanced designs [<xref ref-type="bibr" rid="pbio.3000127.ref014">14</xref>] and randomisation [<xref ref-type="bibr" rid="pbio.3000127.ref015">15</xref>]. In a balanced design, a key confounding variable is purposefully distributed across treatment groups to avoid differences in group means. This is usually done by sequentially assigning subjects to treatment groups based on their rank for the confounding variable (for example, from the largest to the smallest individual). Randomisation, on the other hand, is when subjects are randomly assigned to different treatment groups. Whichever approach investigators use, one might then expect them to test for differences among groups in the confounding variable to ensure and demonstrate that they have successfully eliminated it as a source of between-group variation in the focal outcome variable [<xref ref-type="bibr" rid="pbio.3000127.ref016">16</xref>]. Of course, by chance alone, the difference among groups in a confounding variable can be sufficiently large that it is statistically significant. Given random assignment of subjects, the probability of significance is 5% if we use the typical significance level of 0.05 employed in biological studies [<xref ref-type="bibr" rid="pbio.3000127.ref017">17</xref>]. The probability of significance for the main confounding variable is far smaller for experiments with a deliberately balanced design [<xref ref-type="bibr" rid="pbio.3000127.ref014">14</xref>], although, of course, other confounding variables that are uncorrelated with the main confounding variable are still being randomly assigned. Randomisation is therefore less effective than a balanced design, but it is still the most common approach to assigning subjects to experiments in ecology [<xref ref-type="bibr" rid="pbio.3000127.ref018">18</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref019">19</xref>] and is the gold standard in randomised control trials in medicine [<xref ref-type="bibr" rid="pbio.3000127.ref017">17</xref>].</p>
<p>Significant among-group differences in confounding variables are seen to potentially undermine the conclusions of an experimental study [<xref ref-type="bibr" rid="pbio.3000127.ref016">16</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref017">17</xref>]. Researchers might therefore be tempted not to report significant differences in confounding variables between treatment(s) and control(s) to avoid criticism from reviewers. This could lead to selective reporting of nonsignificance tests. Or researchers might manipulate data collection or analyses (for example, by including or removing some subjects from a dataset) until group differences in confounding variables become statistically nonsignificant. We refer to this type of active data manipulation as ‘reverse <italic>P</italic>-hacking’. Here, we are specifically interested in those forms of <italic>P</italic>-hacking that will lead to slight shifts in <italic>P</italic>-values with cumulative testing until the ‘desired’ threshold is exceeded (see [<xref ref-type="bibr" rid="pbio.3000127.ref009">9</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref012">12</xref>]) (see <xref ref-type="fig" rid="pbio.3000127.g001">Fig 1</xref>).</p>
<p>We test for the presence of selective reporting of nonsignificance and/or reverse <italic>P</italic>-hacking, using articles about behavioural ecology. We chose this discipline because it is the one with which we are most familiar, which facilitated identifying suitable papers. If researchers honestly report all conducted statistical tests for confounding variables between treatment and control groups after random assignment of subjects, we expect 5% to report a statistically significant difference.</p>
</sec>
<sec id="sec002">
<title>Assessing the extent of selective reporting of negative results and reverse <italic>P</italic>-hacking in the literature</title>
<p>We preregistered our study design (<ext-link ext-link-type="uri" xlink:href="https://osf.io/xg6ja/" xlink:type="simple">https://osf.io/xg6ja/</ext-link>), and subsequent modifications were explicitly noted. The full details of the database are provided in <xref ref-type="supplementary-material" rid="pbio.3000127.s001">S1 Text</xref>. In brief, we extracted data from 250 of 1,805 articles examined from three top behavioural ecology journals (1990–2018) that reported on experimental studies. We recorded all <italic>P</italic>-values or statements about the significance of results from tests of whether the mean value of confounding variables differed between treatment(s) and control groups. To be conservative, we treated an article as ‘significant’ if it contained at least one significant outcome. We then tested the null hypothesis that 5% of articles that test for such a difference will report a significant <italic>P</italic>-value (i.e., <italic>P</italic> &lt; 0.05) (<xref ref-type="fig" rid="pbio.3000127.g001">Fig 1A</xref>). To investigate reverse <italic>P</italic>-hacking, we examined <italic>P</italic>-curves (<xref ref-type="fig" rid="pbio.3000127.g001">Fig 1B</xref>) [<xref ref-type="bibr" rid="pbio.3000127.ref010">10</xref>], using all <italic>P</italic>-values presented to two or more decimal places (188 articles, 427 <italic>P</italic>-values). We tested whether there are more <italic>P</italic>-values in the bin closest to the significance threshold (0.05 to &lt; 0.10) than in the next bin (0.10 to &lt; 0.15) (<xref ref-type="fig" rid="pbio.3000127.g001">Fig 1B</xref> [<xref ref-type="bibr" rid="pbio.3000127.ref009">9</xref>]; but see [<xref ref-type="bibr" rid="pbio.3000127.ref012">12</xref>]). We iteratively performed the test by randomly drawing one <italic>P</italic>-value per article. We present the median <italic>P</italic>-value.</p>
<p>The 250 articles with usable data contained 510 tests. Of these articles, only four (1.6%) reported at least one significant difference between the treatment and control group in the mean value of a confounding variable. This is significantly fewer articles than the expected 5% (i.e., 12.5 articles) that should occur in the absence of publication bias (exact binomial test: CI = 0–0.036, <italic>P</italic> = 0.005). Furthermore, two of the four articles with significant test outcomes also had several nonsignificant test outcomes (one article had two tests, and the other had eight tests). If we repeatedly randomly sampled a single test per article, this leads to an even lower presence of significant findings (predominantly only two articles [0.8%] with significant tests, CI = 0–0.025, <italic>P</italic> &lt; 0.001).</p>
<p>The number of <italic>P</italic>-values in the 0.05 to &lt; 0.10 bin immediately above the significant threshold was not significantly larger than the number in the adjacent 0.10 to &lt; 0.15 bin (exact binomial tests, median <italic>P</italic> = 0.989, standard deviation [SD] = 0.036, <xref ref-type="fig" rid="pbio.3000127.g001">Fig 1C</xref>). The pattern predicted given the type of reverse <italic>P</italic>-hacking we were testing for (see [<xref ref-type="bibr" rid="pbio.3000127.ref009">9</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref012">12</xref>]) was therefore not seen (see prediction in <xref ref-type="fig" rid="pbio.3000127.g001">Fig 1A</xref>). Instead, in addition to there being significantly fewer than expected <italic>P</italic>-values below 0.05 (see Methods for analysis below), there were equally few <italic>P</italic>-values in the 0.05 to 0.10 bin (<xref ref-type="fig" rid="pbio.3000127.g001">Fig 1C</xref>). Based on this pattern (see <xref ref-type="fig" rid="pbio.3000127.g001">Fig 1B</xref>), there therefore appears to be selective reporting. However, because there are fewer than expected studies with <italic>P</italic>-values below 0.10 rather than 0.05, the pattern suggests a bias against reporting <italic>P</italic>-values below 0.10.</p>
<p>There was no difference in the year of publication between articles that did or did not report at least one significant difference between the treatment and control group in the mean value of a confounding variable (independent sample <italic>t</italic> test: <italic>t</italic><sub>247</sub> = 0.408, <italic>P</italic> = 0.342; significant: year 2004.5 ± 3.1, nonsignificant: year 2005.9 ± 0.5; mean ± SE, <italic>n</italic> = 4, 246). This test has little statistical power (15%) to detect a medium-strength effect (<italic>d</italic> = 0.5) [<xref ref-type="bibr" rid="pbio.3000127.ref020">20</xref>], but we include it because it was part of our preregistered study design.</p>
<p>Failure to identify a confounding variable can be a serious flaw if it has a strong causal effect on the focal outcome of an experiment (but see [<xref ref-type="bibr" rid="pbio.3000127.ref017">17</xref>]). Depending on the direction of a difference in the confounding variable between the control and treatment group, it could either inflate or obscure the true effect of an experimentally manipulated variable. This is most likely to be an issue when the confounding variable is strongly related to the measured outcome of an experiment (for example, body size is correlated with life span in dogs). We compiled <italic>P</italic>-values (or statements about significance) from statistical tests of differences between treatment and control groups in the mean value of confounding variables. Given no true effect because subjects are randomly assigned to groups, 5% of tests should report a significant difference when α = 0.05 (<xref ref-type="fig" rid="pbio.3000127.g001">Fig 1A</xref>). We found, however, that fewer tests than expected reported a significant difference (<italic>P</italic> = 0.005). Many studies have documented a selection bias against nonsignificant results [<xref ref-type="bibr" rid="pbio.3000127.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref004">4</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref011">11</xref>], but, to our knowledge, ours is the first to present evidence of a selection bias for nonsignificant results.</p>
</sec>
<sec id="sec003">
<title>Why are there so few significant results?</title>
<p>Selection bias can arise because of a file-drawer effect (i.e., studies go unpublished) or selective reporting. Although the former is possible, we suspect that researchers would still attempt to publish studies even if a confounding variable differed significantly between the control and treatment group. Reviewers might, however, occasionally block publication on the grounds that the allocation of subjects means that causal inferences about a manipulated variable are no longer possible (although one could, of course, statistically control for the confounding variable). We therefore focus on selective reporting as the main reason for a dearth of significant results. While selectively reporting goes against recent calls for research transparency (for example, [<xref ref-type="bibr" rid="pbio.3000127.ref020">20</xref>]), the failure to report the outcome of a test need not mean that investigators purposefully omitted results that undermine a study’s validity. Based on reading the Methods of many articles, it is clear that some studies did not report a test for a difference between treatment(s) and control groups in a confounding variable despite the data existing (i.e., confounding variables were measured). We estimate there were up to 371 such studies, but we note that this is a subjective claim, and we may have incorrectly classified some articles. A lack of test results could have occurred because the relevant statistical test was not performed or because the test output was not reported. It is impossible to differentiate between these possibilities, unless there is selective reporting such that missing <italic>P</italic>-values are unevenly distributed between 0 and 1.</p>
<p>The fact we observed too few studies with <italic>P</italic>-values less than 0.05 implicates the selective reporting of nonsignificant results. The magnitude of this type of bias could, however, be underestimated (and might sometimes even go undetected) when examining <italic>P</italic>-curves if there is additional under-reporting of precise, nonsignificant <italic>P</italic>-values. Omission of nonsignificant results might stem from a perception that they are uninteresting [<xref ref-type="bibr" rid="pbio.3000127.ref002">2</xref>, <xref ref-type="bibr" rid="pbio.3000127.ref004">4</xref>, <xref ref-type="bibr" rid="pbio.3000127.ref006">6</xref>, <xref ref-type="bibr" rid="pbio.3000127.ref021">21</xref>]. Indeed, some studies in our dataset tested up to 17 confounding variables and then provided statements like ‘all NS’ or ‘all <italic>P</italic> &gt; X’ rather than precise <italic>P</italic>-values. These results had to be excluded from the <italic>P</italic>-curve, which required that <italic>P</italic>-values were stated to two decimal places. The net effect of the inclusion of results reported as ‘<italic>P</italic> &gt; 0.05’ or ‘NS’ would be to increase the frequency of studies with <italic>P</italic>-values above 0.05. In addition, in ‘<italic>P</italic> greater than’ statements, the most frequent <italic>P</italic>-value used was 0.05. This might further explain why we observed so few exact <italic>P</italic>-values in the 0.05 to &lt; 0.10 bin (<xref ref-type="fig" rid="pbio.3000127.g001">Fig 1C</xref>). Researchers might prefer not to provide exact <italic>P</italic>-values when they lie close to 0.05 (i.e., below 0.10) because such findings are often described, albeit erroneously, as ‘marginally significant’. Crucially, however, if all nonsignificant results are under-reported, as occurs with conventional selective reporting, the <italic>P</italic>-curve would reveal an excess of significant <italic>P</italic>-values. We found the opposite. There is high incentive to publish completed experiments, and we suggest that not reporting significant tests for a confounding variable is the least laborious way to handle such an unwelcome result.</p>
<p>Reverse <italic>P</italic>-hacking is another reason for fewer than expected significant results, even if all <italic>P</italic>-values that are calculated are then published. It is, however, a challenge to determine whether it is occurring (see responses to [<xref ref-type="bibr" rid="pbio.3000127.ref009">9</xref>]). Our approach to test for its existence can be illustrated by considering the complementary phenomenon of <italic>P</italic>-hacking when researchers are rewarded for obtaining significant results. Theory states that a <italic>P</italic>-curve is flat if there is no true effect and right skewed if there is a nonzero true effect (i.e., small <italic>P</italic>-values are more common). One can test for <italic>P</italic>-hacking by only looking at significant <italic>P</italic>-values because within this portion of the <italic>P</italic>-curve, there should be no selection bias. One can then test if the curve here is left skewed, with fewer highly significant than barely significant results, indicative of <italic>P</italic>-hacking [<xref ref-type="bibr" rid="pbio.3000127.ref010">10</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref022">22</xref>]. This would, however, require extreme levels of <italic>P</italic>-hacking if there is actually a nonzero true effect. As such, it has been suggested that one can simply test if there are more <italic>P</italic>-values immediately below 0.05 than further away (for example, 0.045 to &lt; 0.050 versus 0.040 to &lt; 0.045) as evidence for <italic>P</italic>-hacking (for example, [<xref ref-type="bibr" rid="pbio.3000127.ref009">9</xref>]; but see [<xref ref-type="bibr" rid="pbio.3000127.ref012">12</xref>]).</p>
<p>Here, we pursued a similar finer-scale <italic>P</italic>-curve analysis to test for reverse <italic>P</italic>-hacking. We compared the frequency of <italic>P</italic>-values in two equal-sized bins immediately above the 0.05 threshold (<xref ref-type="fig" rid="pbio.3000127.g001">Fig 1B and 1C</xref>). We predicted that there would be more in the 0.05 to &lt; 0.10 bin than the 0.10 to &lt; 0.15 bin. This assumes that an initially significant <italic>P</italic>-value is incrementally shifted toward a larger value as, for example, individual data points are selectively added or removed from a large dataset. If a researcher stops reverse <italic>P</italic>-hacking once the observed <italic>P</italic>-value becomes nonsignificant, it is then likely to fall into the 0.05 to &lt; 0.10 bin. This prediction was not supported. There were actually fewer <italic>P</italic>-values in the 0.05 to &lt; 0.10 than 0.10 to &lt; 0.15 bins (<xref ref-type="fig" rid="pbio.3000127.g001">Fig 1C</xref>). In hindsight, we suggest that many forms of reverse <italic>P</italic>-hacking will shift initially significant values well beyond the 0.05 to &lt; 0.10 bin, and the exact distribution of reverse <italic>P</italic>-hacked values is thus hard to predict. A comparable reservation has been made about the distribution of <italic>P</italic>-hacked values (i.e., that they will not preferentially lie immediately below 0.05; [<xref ref-type="bibr" rid="pbio.3000127.ref012">12</xref>]). It is intriguing to note, however, that we also documented too few values in the 0.05 to &lt; 0.10 bin (<xref ref-type="fig" rid="pbio.3000127.g001">Fig 1C</xref>). These values are often, albeit incorrectly, described as ‘marginally significant’, raising the possibility that they are disliked by researchers wanting to present an image of their experiment as having eliminated differences in confounding variables between control and treatment groups. Selection bias and/or reverse <italic>P</italic>-hacking could therefore explain the general lack of <italic>P</italic>-values below 0.10.</p>
</sec>
<sec id="sec004">
<title>Summary and conclusions</title>
<p>Although there was a strong signal of missing <italic>P</italic>-values below 0.10, we cannot determine whether this is due to selective reporting, reverse <italic>P</italic>-hacking, or both. The most parsimonious explanation is selective reporting, but we cannot exclude reverse <italic>P</italic>-hacking. We collected data from behavioural ecology articles based on our own expertise in this discipline, but there is no a priori reason to believe that behavioural ecologists treat confounding variables any differently than researchers in other disciplines (for example, [<xref ref-type="bibr" rid="pbio.3000127.ref016">16</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref017">17</xref>]). Regardless of the proximate mechanism, we have provided further evidence by taking a novel, to our knowledge, approach that poor research practices arise from an obsession with <italic>P</italic>-values and discriminating between ‘significant’ and ‘nonsignificant’ findings (see also [<xref ref-type="bibr" rid="pbio.3000127.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref004">4</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref006">6</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref009">9</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref011">11</xref>,<xref ref-type="bibr" rid="pbio.3000127.ref022">22</xref>]). A focus on <italic>P</italic>-values rather than effect sizes is clearly problematic. We encourage others to test for reverse <italic>P</italic>-hacking and/or a selection bias against nonsignificant results in their own research fields in equivalent tests of confounding variables. This will allow us to assess the generality of the pattern we observed. Finally, it should be noted that the interpretation of recent studies is unlikely to be affected by group differences in confounding variables. Earlier studies often used univariate tests (for example, <italic>t</italic> tests or Mann–Whitney U tests) that ignored major confounding variables (i.e., implicitly assumed that they did not differ between the control and treatments group). In contrast, recent studies usually add potential confounding variables as covariates that are ‘corrected for’ before examining the effect of an experimental intervention.</p>
</sec>
<sec id="sec005">
<title>Supporting information</title>
<supplementary-material id="pbio.3000127.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000127.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Contains details of the preregistration of the study, links to data files and an R script, and details of the protocol for data collection and analysis.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank our colleagues for helpful feedback.</p>
</ack>
<fn-group>
<fn fn-type="other" id="fn001">
<p><bold>Provenance:</bold> Not commissioned; externally peer reviewed.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="pbio.3000127.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grimes</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Bauch</surname> <given-names>CT</given-names></name>, <name name-style="western"><surname>Ioannidis</surname> <given-names>JPA</given-names></name>. <article-title>Modelling science trustworthiness under publish or perish pressure</article-title>. <source>Royal Society Open Science</source> <year>2018</year>; <volume>5</volume>: <fpage>171511</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rsos.171511" xlink:type="simple">10.1098/rsos.171511</ext-link></comment> <object-id pub-id-type="pmid">29410855</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fanelli</surname> <given-names>D.</given-names></name> <article-title>Negative results are disappearing from most disciplines and countries</article-title>. <source>Scientometrics</source> <year>2012</year>;<volume>90</volume>: <fpage>891</fpage>–<lpage>904</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11192-011-0494-7" xlink:type="simple">10.1007/s11192-011-0494-7</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000127.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kelly</surname> <given-names>CD</given-names></name>. <article-title>Replicating empirical research in behavioral ecology: How and why it should be done but rarely ever is</article-title>. <source>Q Rev Biol</source>. <year>2006</year>;<volume>81</volume>: <fpage>221</fpage>–<lpage>236</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1086/506236" xlink:type="simple">10.1086/506236</ext-link></comment> <object-id pub-id-type="pmid">17051829</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ioannidis</surname> <given-names>JPA</given-names></name>. <article-title>Why most published research findings are false</article-title>. <source>PLoS Med</source>. <year>2005</year>;<volume>2</volume>(<issue>8</issue>): <fpage>e124</fpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pmed.0020124" xlink:type="simple">10.1371/journal.pmed.0020124</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000127.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><collab>Open Science Collaboration</collab>. 2015. <article-title>Estimating the reproducibility of psychological science</article-title>. <source>Science</source> <year>2015</year>;<volume>349</volume>: aac4716. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.aac4716" xlink:type="simple">10.1126/science.aac4716</ext-link></comment> <object-id pub-id-type="pmid">26315443</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pashler</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>CR</given-names></name>. <article-title>Is the replicability crisis overblown? Three arguments examined</article-title>. <source>Perspect Psychol Sci</source>. <year>2012</year>;<volume>7</volume>: <fpage>531</fpage>–<lpage>536</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/1745691612463401" xlink:type="simple">10.1177/1745691612463401</ext-link></comment> <object-id pub-id-type="pmid">26168109</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Forstmeier</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Wagenmakers</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Parker</surname> <given-names>TH</given-names></name>. <article-title>Detecting and avoiding likely false-positive findings—a practical guide</article-title>. <source>Biol Rev Camb Philos Soc</source>. <year>2017</year>;<volume>92</volume>: <fpage>1941</fpage>–<lpage>1968</lpage>. Epub 2016/11/24. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/brv.12315" xlink:type="simple">10.1111/brv.12315</ext-link></comment> <object-id pub-id-type="pmid">27879038</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rosenthal</surname> <given-names>R</given-names></name>. <article-title>The "file drawer problem" and tolerance for null results</article-title>. <source>Psychological Bulletin</source>. <year>1979</year>;<volume>86</volume>: <fpage>638</fpage>–<lpage>641</lpage>.</mixed-citation></ref>
<ref id="pbio.3000127.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Head</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Holman</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Lanfear</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Kahn</surname> <given-names>AT</given-names></name>, <name name-style="western"><surname>Jennions</surname> <given-names>MD</given-names></name>. <article-title>The extent and consequences of p-hacking in science</article-title>. <source>PLoS Biol</source>. <year>2015</year>;<volume>13</volume>(<issue>3</issue>): <fpage>e1002106</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1002106" xlink:type="simple">10.1371/journal.pbio.1002106</ext-link></comment> <object-id pub-id-type="pmid">25768323</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simonsohn</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>LD</given-names></name>, <name name-style="western"><surname>Simmons</surname> <given-names>JP</given-names></name>. <article-title>P-Curve: A key to the file-drawer</article-title>. <source>J Exp Psychol-Gen</source>. <year>2014</year>;<volume>143</volume>: <fpage>534</fpage>–<lpage>547</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0033242" xlink:type="simple">10.1037/a0033242</ext-link></comment> <object-id pub-id-type="pmid">23855496</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Song F</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Gilbody</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Duley</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sutton</surname> <given-names>AJ</given-names></name>. <article-title>Publication and related biases</article-title>. <source>Health technology assessment</source>. <year>2000</year>;<volume>4</volume>: <fpage>1</fpage>–<lpage>115</lpage>.</mixed-citation></ref>
<ref id="pbio.3000127.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bishop</surname> <given-names>DVM</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>PA</given-names></name>. <article-title>Problems in using <italic>p</italic>-curve analysis and text-mining to detect rate of <italic>p</italic>-hacking and evidential value</article-title>. <source>PeerJ</source> <year>2016</year>;<volume>4</volume>: <fpage>e1715</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7717/peerj.1715" xlink:type="simple">10.7717/peerj.1715</ext-link></comment> <object-id pub-id-type="pmid">26925335</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cotton</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fowler</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Pomiankowski</surname> <given-names>A</given-names></name>. <article-title>Do sexual ornaments demonstrate heightened condition-dependent expression as predicted by the handicap hypothesis?</article-title> <source>Proc R Soc B-Biol Sci</source>. <year>2004</year>;<volume>271</volume>: <fpage>771</fpage>–<lpage>783</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rspb.2004.2688" xlink:type="simple">10.1098/rspb.2004.2688</ext-link></comment> <object-id pub-id-type="pmid">15255094</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><collab>Student</collab>. <article-title>Comparison between balanced and random arrangements of field plots</article-title>. <source>Biometrika</source>. <year>1938</year>;<volume>29</volume>: <fpage>363</fpage>–<lpage>378</lpage>.</mixed-citation></ref>
<ref id="pbio.3000127.ref015"><label>15</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Fisher</surname> <given-names>RA</given-names></name>. <source>Design of Experiments</source>. <edition>8th ed</edition>. <publisher-loc>New York</publisher-loc>: <publisher-name>Hafner Publishing Company</publisher-name>; <year>1971</year>. 27 p.</mixed-citation></ref>
<ref id="pbio.3000127.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knol</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Groenwold</surname> <given-names>RHH</given-names></name>, <name name-style="western"><surname>Grobbee</surname> <given-names>DE</given-names></name>. <article-title>P-values in baseline tables of randomised controlled trials are inappropriate but still common in high impact journals</article-title>. <source>European Journal of Preventive Cardiology</source>. <year>2012</year>; <volume>19</volume>: <fpage>231</fpage>–<lpage>232</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/1741826711421688" xlink:type="simple">10.1177/1741826711421688</ext-link></comment> <object-id pub-id-type="pmid">22512015</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stang</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Baethge</surname> <given-names>C</given-names></name>. <article-title>Imbalance p values for baseline covariates in randomized controlled trials: a last resort for the use of p values? A pro and contra debate</article-title>. <source>Clinical epidemiology</source>. <year>2018</year>;<volume>10</volume>: <fpage>531</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2147/CLEP.S161508" xlink:type="simple">10.2147/CLEP.S161508</ext-link></comment> <object-id pub-id-type="pmid">29773956</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>ANH</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Pawley</surname> <given-names>MDM</given-names></name>. <article-title>Could ecologists be more random? Straightforward alternatives to haphazard spatial sampling</article-title>. <source>Ecography</source>. <year>2017</year>;<volume>40</volume>: <fpage>1251</fpage>–<lpage>1255</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/ecog.02821" xlink:type="simple">10.1111/ecog.02821</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000127.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balint</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Marton</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Schatz</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>During</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Grossart</surname> <given-names>HP</given-names></name>. <article-title>Proper experimental design requires randomization/balancing of molecular ecology experiments</article-title>. <source>Ecol Evol</source>. <year>2018</year>;<volume>8</volume>: <fpage>1786</fpage>–<lpage>1793</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/ece3.3687" xlink:type="simple">10.1002/ece3.3687</ext-link></comment> <object-id pub-id-type="pmid">29435253</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parker</surname> <given-names>TH</given-names></name>, <name name-style="western"><surname>Forstmeier</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Koricheva</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Fidler</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Hadfield</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Chee</surname> <given-names>YE</given-names></name>, <etal>et al</etal>. <article-title>Transparency in ecology and evolution: real problems, real solutions</article-title>. <source>Trends Ecol Evol</source>. <year>2016</year>;<volume>31</volume>: <fpage>711</fpage>–<lpage>719</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tree.2016.07.002" xlink:type="simple">10.1016/j.tree.2016.07.002</ext-link></comment> <object-id pub-id-type="pmid">27461041</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pritschet</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Powell</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Horne</surname> <given-names>Z</given-names></name>. <article-title>Marginally significant effects as evidence for hypotheses: changing attitudes over four decades</article-title>. <source>Psychol Sci</source>. <year>2016</year>;<volume>27</volume>: <fpage>1036</fpage>–<lpage>1042</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0956797616645672" xlink:type="simple">10.1177/0956797616645672</ext-link></comment> <object-id pub-id-type="pmid">27207874</object-id></mixed-citation></ref>
<ref id="pbio.3000127.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simonsohn</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>LD</given-names></name>, <name name-style="western"><surname>Simmons</surname> <given-names>JP</given-names></name>. <article-title>p-Curve and effect size: correcting for publication bias using only significant results</article-title>. <source>Perspect Psychol Sci</source>. <year>2014</year>;<volume>9</volume>: <fpage>666</fpage>–<lpage>681</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/1745691614553988" xlink:type="simple">10.1177/1745691614553988</ext-link></comment> <object-id pub-id-type="pmid">26186117</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>