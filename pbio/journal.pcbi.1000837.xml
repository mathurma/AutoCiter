<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">10-PLCB-RA-1660R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000837</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology/Literature Analysis</subject></subj-group></article-categories><title-group><article-title>A Comprehensive Benchmark of Kernel Methods to Extract Protein–Protein Interactions from Literature</article-title><alt-title alt-title-type="running-head">Kernels for Protein-Protein Interaction Mining</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Tikk</surname><given-names>Domonkos</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Thomas</surname><given-names>Philippe</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Palaga</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Hakenberg</surname><given-names>Jörg</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Leser</surname><given-names>Ulf</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Knowledge Management in Bioinformatics, Computer Science Department, Humboldt-Universität zu Berlin, Berlin, Germany</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Department of Telecommunications and Media Informatics, Budapest University of Technology and Economics, Budapest, Hungary</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Department of Computer Science and Engineering, Arizona State University, Tempe, Arizona, United States of America</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Rzhetsky</surname><given-names>Andrey</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">University of Chicago, United States of America</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">tikk@informatik.hu-berlin.de</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: DT UL. Performed the experiments: DT PT PP. Analyzed the data: DT PT PP. Wrote the paper: DT PT JH UL.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>7</month><year>2010</year></pub-date><pub-date pub-type="epub"><day>1</day><month>7</month><year>2010</year></pub-date><volume>6</volume><issue>7</issue><elocation-id>e1000837</elocation-id><history>
<date date-type="received"><day>15</day><month>1</month><year>2010</year></date>
<date date-type="accepted"><day>27</day><month>5</month><year>2010</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2010</copyright-year><copyright-holder>Tikk et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>The most important way of conveying new findings in biomedical research is scientific publication. Extraction of protein–protein interactions (PPIs) reported in scientific publications is one of the core topics of text mining in the life sciences. Recently, a new class of such methods has been proposed - convolution kernels that identify PPIs using deep parses of sentences. However, comparing published results of different PPI extraction methods is impossible due to the use of different evaluation corpora, different evaluation metrics, different tuning procedures, etc. In this paper, we study whether the reported performance metrics are robust across different corpora and learning settings and whether the use of deep parsing actually leads to an increase in extraction quality. Our ultimate goal is to identify the one method that performs best in real-life scenarios, where information extraction is performed on unseen text and not on specifically prepared evaluation data. We performed a comprehensive benchmarking of nine different methods for PPI extraction that use convolution kernels on rich linguistic information. Methods were evaluated on five different public corpora using cross-validation, cross-learning, and cross-corpus evaluation. Our study confirms that kernels using dependency trees generally outperform kernels based on syntax trees. However, our study also shows that only the best kernel methods can compete with a simple rule-based approach when the evaluation prevents information leakage between training and test corpora. Our results further reveal that the F-score of many approaches drops significantly if no corpus-specific parameter optimization is applied and that methods reaching a good AUC score often perform much worse in terms of F-score. We conclude that for most kernels no sensible estimation of PPI extraction performance on new text is possible, given the current heterogeneity in evaluation data. Nevertheless, our study shows that three kernels are clearly superior to the other methods.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>The most important way of conveying new findings in biomedical research is scientific publication. In turn, the most recent and most important findings can only be found by carefully reading the scientific literature, which becomes more and more of a problem because of the enormous number of published articles. This situation has led to the development of various computational approaches to the automatic extraction of important facts from articles, mostly concentrating on the recognition of protein names and on interactions between proteins (PPI). However, so far there is little agreement on which methods perform best for which task. Our paper reports on an extensive comparison of nine recent PPI extraction tools. We studied their performance in various settings on a set of five different text collections containing articles describing PPIs, which for the first time allows for an unbiased comparison of their respective effectiveness. Our results show that the tools' performance depends largely on the collection they are trained on and the collection they are then evaluated on, which means that extrapolating their measured performance to arbitrary text is still highly problematic. We also show that certain classes of methods for extracting PPIs are clearly superior to other classes.</p>
</abstract><funding-group><funding-statement>DT is supported by the Alexander-von-Humboldt Foundation (<ext-link ext-link-type="uri" xlink:href="http://www.humboldt-foundation.de/web/home.html" xlink:type="simple">http://www.humboldt-foundation.de/web/home.html</ext-link>). PT is supported by the Federal Ministriy of Education and Research, Germany (BMBF, <ext-link ext-link-type="uri" xlink:href="http://www.bmbf.de/en/1398.php" xlink:type="simple">http://www.bmbf.de/en/1398.php</ext-link>), grant no 0315417B. JH acknowledges support by Arizona State University (<ext-link ext-link-type="uri" xlink:href="http://www.asu.edu/" xlink:type="simple">http://www.asu.edu/</ext-link>) and Science Foundation Arizona (<ext-link ext-link-type="uri" xlink:href="http://www.sfaz.org/" xlink:type="simple">http://www.sfaz.org/</ext-link>). PP was supported by the Max-Planck-Gesellschaft (<ext-link ext-link-type="uri" xlink:href="http://www.mpg.de/english/portal/index.html" xlink:type="simple">http://www.mpg.de/english/portal/index.html</ext-link>) under project TM-REG. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="19"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Protein-protein interactions (PPIs) are integral to virtually all cellular processes, such as metabolism, signaling, regulation, and proliferation. Collecting data on individual interactions is crucial for understanding these processes at a systems biology level <xref ref-type="bibr" rid="pcbi.1000837-Hoffmann1">[1]</xref>. Known PPIs help to predict the function of yet uncharacterized proteins, for instance using conserved PPI networks <xref ref-type="bibr" rid="pcbi.1000837-Jaeger1">[2]</xref> or proximity in a PPI network <xref ref-type="bibr" rid="pcbi.1000837-Jiang1">[3]</xref>. Networks can be generated from molecular interaction data and are useful for multiple purposes, such as identification of functional modules <xref ref-type="bibr" rid="pcbi.1000837-Spirin1">[4]</xref> or finding novel associations between genes and diseases <xref ref-type="bibr" rid="pcbi.1000837-Ideker1">[5]</xref>.</p>
<p>Several approaches are in use to study interactions in large- or small-scale experiments. Among the techniques most often used are two-hybrid screens, mass spectrometry, and tandem affinity purification <xref ref-type="bibr" rid="pcbi.1000837-Lalonde1">[6]</xref>. Results of high-throughput techniques (such as two-hybrid screens and mass spectrometry) usually are published in tabular form and can be imported by renowned PPI databases quickly. These techniques are prone to produce comparably large numbers of false positives <xref ref-type="bibr" rid="pcbi.1000837-Sprinzak1">[7]</xref>. Other techniques, such as co-immunoprecipitation, cross-linking, or rate-zonal centrifugation, produce more reliable results but are small-scale; these are typically used to verify interesting yet putative interactions, possibly first hypothesized during large-scale experiments <xref ref-type="bibr" rid="pcbi.1000837-Miernyk1">[8]</xref>. Only now, authors started to submit results directly to PPI databases in a regular manner, oftentimes as a step required by publishers to ensure quality.</p>
<p>Taking into account the great wealth of PPI data that was published before the advent of PPI databases, it becomes clear that still much valuable data is available only in text. Turning this information into a structured form is a costly task that has to be performed by human experts <xref ref-type="bibr" rid="pcbi.1000837-Chatraryamontri1">[9]</xref>. Recent years have seen a steep increase in the number of techniques that aim to alleviate this task by applying computational methods, especially machine learning and statistical natural language processing <xref ref-type="bibr" rid="pcbi.1000837-Winnenburg1">[10]</xref>. Such tools are not only used to populate PPI databases, but their output is often also used directly as independent input to biological data mining (see, e.g., <xref ref-type="bibr" rid="pcbi.1000837-zgr1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1000837-Lage1">[12]</xref>).</p>
<p>Several techniques for extracting protein-protein interactions from text have been proposed (cf. Related Work). Unfortunately, the reported results differ widely. While early works reported fabulous results of over 90% precision and recall <xref ref-type="bibr" rid="pcbi.1000837-Proux1">[13]</xref>, the recent BioCreative II.5 community challenge led to results at the opposite edge of the quality range, with the best system performing just above 30% F-measure <xref ref-type="bibr" rid="pcbi.1000837-Leitner1">[14]</xref>. Much of these differences can be accounted to the fact that some evaluations work on corpora that have proteins already annotated, while others include recognition and identification of proteins as a subtask <xref ref-type="bibr" rid="pcbi.1000837-Kabiljo1">[15]</xref>. However, even within the same setting, the spread of reported results remains large. Since there also is a lack of unbiased benchmarks of published systems, a potential end user currently is left rather uncertain about which tool to use and which quality to expect when working with new texts, and published experiences often are rather negative <xref ref-type="bibr" rid="pcbi.1000837-Giles1">[16]</xref>.</p>
<p>In this paper, we give an unbiased and comprehensive benchmark of a large set of PPI extraction methods. We concentrate on a fairly recent class of algorithms which usually is summarized with the term <italic>kernel methods</italic> <xref ref-type="bibr" rid="pcbi.1000837-Airola1">[17]</xref>–<xref ref-type="bibr" rid="pcbi.1000837-Niu1">[33]</xref>. In a nutshell, these methods work as follows. First, they require a training corpus consisting of labeled sentences, some of which contain PPIs, some contain non-interacting proteins, and some contain only one or no protein. The exact information that later should be extracted must be known, that is, usually the pair of proteins that interact. All sentences in the training corpus are transformed into representations that try to best capture properties of how the interaction is expressed (or not for negative examples). The simplest such representation is the set of words that occur in the sentence; more complex representations are syntax trees (also called constituent trees), capturing the syntactic structure of the sentence, and dependency graphs, which represent the main grammatical entities and their relationships to each other (see <xref ref-type="fig" rid="pcbi-1000837-g001">Figures 1</xref> and <xref ref-type="fig" rid="pcbi-1000837-g002">2</xref>). The set of structured representations together with the PPIs are analyzed by a kernel-based learner (mostly an SVM), which learns a model of how PPIs typically are expressed. Every new sentence that should be analyzed must be turned into the same representation, which is then classified by the kernel method.</p>
<fig id="pcbi-1000837-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.g001</object-id><label>Figure 1</label><caption>
<title>Syntax tree parse generated by the Charniak–Lease parser.</title>
<p>The syntax tree parse of the example sentence <italic>SsgG transcription also requires the DNA binding protein GerE</italic>. Under the parse tree we show its substructures used by the subtree, subset tree, partial tree, and spectrum tree kernels.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.g001" xlink:type="simple"/></fig><fig id="pcbi-1000837-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.g002</object-id><label>Figure 2</label><caption>
<title>Dependency tree parse generated by the Stanford parser.</title>
<p>The dependency tree parse of the example sentence <italic>SsgG transcription also requires the DNA binding protein GerE</italic>. Some substructures (paths) generated from the parse tree for kernels. We showed in red the shortest path between the two proteins (in blue), which is used by kBSPS, cosine similarity and edit distance, and all-path graphs kernels. APG kernel also uses the links outside the shortest path, but with lower weights (0.3 vs. 0.9).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.g002" xlink:type="simple"/></fig>
<p>Central to the learning and the classification phases is a so-called kernel function. Simply speaking, a kernel function is a function that takes the representation of two sentences and computes their similarity. Kernel-based approaches to PPI extraction—and especially those working with <italic>convolution kernels</italic>—have shown high predictive accuracy and occupied top ranks in relevant CASP-style community challenges <xref ref-type="bibr" rid="pcbi.1000837-Kim2">[34]</xref>. Consequently, the number of suggested methods has grown quite a bit, differing mostly in the representation they use and in the particular kernel they apply. The reported results differ largely and are difficult to compare, as often different corpora are used together with different ways of defining and measuring quality.</p>
<p>In this paper, we provide a comprehensive benchmark of nine kernel-based methods for relationship extraction from natural text (all substantially different approaches that were available as programs from a list of around 20 methods we considered). We tested each method in various scenarios on five different corpora. The transformation of the sentences in the corpora were performed using state-of-the-art parser software, in particular, the latest release of the Charniak–Lease parser for constituent trees and the Stanford Parser for dependency graphs. We show how publicly available kernels compare to each other in three scenarios: document-level 10-fold cross-validation (CV), cross-learning (CL), and cross-corpus (CC) settings. We also introduce a new and very fast kernel, kBSPS, and demonstrate that it is highly competitive.</p>
<p>We see our work as a continuation of similar benchmarks that have recently shed some light on the state-of-the-art of selected phases in the PPI extraction pipeline; in particular, these are the work on the performance of different constituent and dependency parsers <xref ref-type="bibr" rid="pcbi.1000837-Clegg1">[35]</xref>; on evaluation metrics and the influence of corpus properties on PPI quality <xref ref-type="bibr" rid="pcbi.1000837-Pyysalo1">[36]</xref>; an analysis of the impact of parsers on PPI performance <xref ref-type="bibr" rid="pcbi.1000837-Miyao1">[37]</xref>; and a recent study on the performance of different classes of features <xref ref-type="bibr" rid="pcbi.1000837-Fayruzov1">[38]</xref>.</p>
<sec id="s1a">
<title>Related Work</title>
<p>A number of different techniques have been proposed to solve the problem of extracting interactions between proteins in natural language text. These can be roughly sorted into one of three classes: co-occurrence, pattern matching, and machine learning. We briefly review these methods here for completeness; see <xref ref-type="bibr" rid="pcbi.1000837-Zhou1">[39]</xref> for a recent survey. We describe kernel-based methods in more detail in <xref ref-type="sec" rid="s2">Methods</xref>.</p>
<p>A common baseline method for relationship extraction is to assume a relationship between each pair of entities that co-occur in the same piece of text (e.g., <xref ref-type="bibr" rid="pcbi.1000837-Pyysalo1">[36]</xref>). This “piece of text” is usually restricted to single sentences, but can also be a phrase, a paragraph, or a whole document. The underlying assumption is that whenever (two or more) entities are mentioned together, a semantic relation holds between them. However, the semantic relation does not necessarily mean that the entities interact; consequently, the kind of relation might not match what is sought. In the case of co-occurring proteins, only a fraction of sentences will discuss actual interactions between them. As an example, in the AIMed corpus (see Corpora), only 17% of all sentence-level protein pairs describe protein-protein interactions. Accordingly, precision is often low, but can be improved by additional filtering steps, such as aggregation of single PPI at the corpus level <xref ref-type="bibr" rid="pcbi.1000837-Bunescu2">[19]</xref>, removal of sentences matching certain lexico-syntactic patterns <xref ref-type="bibr" rid="pcbi.1000837-Rinaldi1">[40]</xref>, or requiring the occurrence of an additional “interaction word” from a fixed list between the two proteins <xref ref-type="bibr" rid="pcbi.1000837-Kabiljo1">[15]</xref>.</p>
<p>The second common approach is pattern matching. SUISEKI was one of the first systems to use hand-crafted regular expressions to encode phrases that typically express protein-protein interactions, using part-of-speech and word lists <xref ref-type="bibr" rid="pcbi.1000837-Blaschke1">[41]</xref>. Overall, they found that a set of about 40 manually derived patterns yields high precision, but achieves only low recall. <xref ref-type="bibr" rid="pcbi.1000837-Hunter1">[42]</xref> proposed OpenDMAP, a framework for template matching, which is backed by ontological resources to represent slots and potential slot fillers, etc. With 78 hand-crafted templates, they achieve an F-score of 29% on the BioCreative 2 IPS test set <xref ref-type="bibr" rid="pcbi.1000837-Krallinger1">[43]</xref>, which was the best at the time of the competition. <xref ref-type="bibr" rid="pcbi.1000837-Hao1">[44]</xref> showed that patterns can be generated automatically using manually annotated sentences that are abstracted into patterns. A<sc>li</sc>B<sc>aba</sc> goes a step further in deriving patterns from automatically generated training data <xref ref-type="bibr" rid="pcbi.1000837-Hakenberg1">[45]</xref>. The fact that automatically generated patterns usually yield high precision but low individual recall is made up by this method by generating thousands of patterns. On the BioCreative 2 IPS test set, this method achieves an F-score of around 24% without any corpus-specific tuning <xref ref-type="bibr" rid="pcbi.1000837-Hakenberg1">[45]</xref>. The third category of approaches use machine learning, for instance, Bayesian network approaches <xref ref-type="bibr" rid="pcbi.1000837-Chowdhary1">[46]</xref> or maximum-entropy-based methods <xref ref-type="bibr" rid="pcbi.1000837-Sun1">[47]</xref>. The later can be set up as a two-step classification scenario, first judging sentences for relevance to discussing protein-protein interactions, and then classifying each candidate pair of proteins in such sentences. Using half of the BioCreative 1 PPI corpus each for training and testing, the approach yields an accuracy of 81.9% when using both steps, and 81.2% when using the second step only. As ML-based methods are the focus of our paper, we will discuss more closely related work in the next sections.</p>
</sec></sec><sec id="s2" sec-type="methods">
<title>Methods</title>
<p>In this section, we describe in detail the kernels we evaluated, the corpora and how we used them as gold standards, the measures we computed, and the parameter settings we used and how they were obtained. We believe that such a level of detail is necessary to compare different methods in a fair and unbiased manner. Note that our evaluation often produces results that are far from those published by other authors (see <xref ref-type="sec" rid="s3">Results</xref>), which only underlines the importance of a clear statement regarding evaluation methods.</p>
<sec id="s2a">
<title>Parsers</title>
<p>The effect of using different parsers and parse representations for the task of extracting protein-protein interactions has been investigated in <xref ref-type="bibr" rid="pcbi.1000837-Miyao2">[48]</xref>. In that study, the authors measured the accuracy improvements in PPI extraction when the parser output was incorporated as statistical features of the applied machine learning classifier. Their experiments showed that the investigated parsers are very similar concerning their influence on accuracy.</p>
<p>For our experiments we selected the Charniak–Lease re-ranking parser (<ext-link ext-link-type="uri" xlink:href="ftp://ftp.cs.brown.edu/pub/nlparser/reranking-parserAug06.tar.gz" xlink:type="simple">ftp://ftp.cs.brown.edu/pub/nlparser/reranking-parserAug06.tar.gz</ext-link>) as syntax parser, since several authors <xref ref-type="bibr" rid="pcbi.1000837-Clegg1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1000837-Pyysalo2">[49]</xref> found it as the best in recent evaluations. We used the latest official release (Aug 2006 version) with the improved self-trained biomedical model <xref ref-type="bibr" rid="pcbi.1000837-McClosky1">[50]</xref> using GENIA parse trees. We also performed experiments using the newer pre-release version of the same parser (courtesy of David McClosky, Eugene Charniak and Mark Johnson), and with model files trained exclusively on a news corpus and another trained on both news and PubMed abstracts <xref ref-type="bibr" rid="pcbi.1000837-McClosky2">[51]</xref>. However, differences in results were insignificant, and therefore we omit them for brevity. We used the Stanford conversion tool (<ext-link ext-link-type="uri" xlink:href="http://nlp.stanford.edu/software/lex-parser.shtml" xlink:type="simple">http://nlp.stanford.edu/software/lex-parser.shtml</ext-link>) to obtain dependency graphs from the Charniak–Lease syntax tree parses. When explaining various kernel functions we will make use of the syntax tree (<xref ref-type="fig" rid="pcbi-1000837-g001">Figure 1</xref>) and the dependency tree (<xref ref-type="fig" rid="pcbi-1000837-g002">Figure 2</xref>) of the sentence “SsgG transcription also requires the DNA binding protein GerE,” as generated by the aforementioned parsers and tools.</p>
</sec><sec id="s2b">
<title>Classification with Kernels</title>
<p>A support vector machine (SVM) is a classifier that, given a set of training examples, finds the linear (hyper)plane that separates positive and negative examples with the largest possible margin <xref ref-type="bibr" rid="pcbi.1000837-Joachims1">[52]</xref>. The training examples that lie closest to the hyperplane are the support vectors. If the two sets are not linearly separable, kernel functions can transform the problem space to a nonlinear, often higher dimensional space, in which the problem might be separable <xref ref-type="bibr" rid="pcbi.1000837-Schlkopf1">[53]</xref>. The kernel is a similarity function that maps a pair of instances to their similarity score: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e001" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e002" xlink:type="simple"/></inline-formula> is the feature space in which the instances are represented. Given a finite set of instances, the kernel can be represented by a similarity matrix that contains all pairwise similarity scores. Kernels can be easily computed with inner products between instances without explicit feature handling that permits of the use of high dimensional feature spaces such as the rich structured representation of graphs or trees.</p>
<p>In our experiments we make use of SVM implementations where the training is performed by a convex quadratic programming (QP) task. Additionally, as proposed by its authors <xref ref-type="bibr" rid="pcbi.1000837-Airola1">[17]</xref>, the all-path graph kernel was also trained with sparse regularized least squares (RLS) <xref ref-type="bibr" rid="pcbi.1000837-Rifkin1">[54]</xref>, which requires to solve a single system of linear equations. In practice, various flavors of SVMs have been described <xref ref-type="bibr" rid="pcbi.1000837-WintersHilt1">[55]</xref>; they differ, for instance, in their training algorithm, parameter set, or representation of features. Furthermore, several freely available implementations exist, among which SVM<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e003" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1000837-Joachims1">[52]</xref> and LIBSVM <xref ref-type="bibr" rid="pcbi.1000837-Chang1">[56]</xref> probably are the most renowned ones. Both can be adapted to special needs—such as working with linguistic structures—by providing an option to integrate user-defined kernel functions. There are two alternatives for the integration of a kernel functions. In SVM<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e004" xlink:type="simple"/></inline-formula> one can code his own kernel function that accepts the corresponding instance representation (with option -t 4 and a self-implemented kernel.h). LIBSVM supports the use of pre-computed kernels, i.e., the kernel function is passed to the SVM learner as a Gram-matrix, containing the pairwise similarity of all instances. Most of the kernels we experimented with use the SVM<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e005" xlink:type="simple"/></inline-formula> implementation, except for the shallow linguistic kernel that uses LIBSVM. To be able to measure the AUC score (see Evaluation Methods) we had to apply changes in the LIBSVM code to retrieve not just the class label, but also the value of the prediction.</p>
</sec><sec id="s2c">
<title>Kernel Methods</title>
<p>The kernels introduced in this section are mostly convolution kernels <xref ref-type="bibr" rid="pcbi.1000837-Haussler1">[57]</xref>, i.e., they make use of the structure of the instances (in our case, syntax trees or dependency graphs of sentences). Their main idea is to quantify the similarity of two instances through counting the similarities of their substructures; however, there have been many proposals on to how to do this in the best way. We include into our experiments all publicly available approaches that make use of different kernel functions we are aware of. We were able to obtain nine out of the about 20 considered kernels (see <xref ref-type="supplementary-material" rid="pcbi.1000837.s001">Tables S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1000837.s002">S2</xref>), either from a publicly available download, or upon request from the respective authors (details about kernel packages are in <xref ref-type="supplementary-material" rid="pcbi.1000837.s003">Table S3</xref>, all software source code are available with installation instruction on our website <ext-link ext-link-type="uri" xlink:href="http://informatik.hu-berlin.de/forschung/gebiete/wbi/ppi-benchmark" xlink:type="simple">http://informatik.hu-berlin.de/forschung/gebiete/wbi/ppi-benchmark</ext-link>).</p>
<p>Most of these kernels have been specifically designed to extract PPI from text or have been successfully applied to this task. Exceptions are subtree, partial tree and spectrum tree kernels which to our knowledge were not tested for PPI extraction before. Next, we will very briefly introduce their underlying principles (see also <xref ref-type="supplementary-material" rid="pcbi.1000837.s001">Table S1</xref> for an overview).</p>
<sec id="s2c1">
<title>Shallow linguistic kernel (SL)</title>
<p>From all kernels we tested, this is the only one that exclusively uses shallow parsing information <xref ref-type="bibr" rid="pcbi.1000837-Giuliano1">[23]</xref>. We included it to contrast its performance from the more complex convolution kernels. The kernel is defined as the sum of two kernels, the global and the local context kernels. The feature set of the <italic>global context kernel</italic> is based on the words occurring in the sentence <italic>fore-between</italic>, <italic>between</italic> and <italic>between-after</italic> relative to the pair of investigated proteins. Based on this, three term frequency vectors are created according to the bag-of-words paradigm. The global kernel is then obtained as the count of common words in the three vectors obtained from the two compared sentences. The <italic>local context kernel</italic> uses surface (capitalization, punctuation, numerals) and shallow linguistic (POS-tag, lemma) features generated from tokens left and right to proteins of the protein pair (the size of the window is adjustable). The similarity of the generated pairs of left and right feature vectors is calculated using scalar product.</p>
</sec><sec id="s2c2">
<title>Subtree kernel (ST)</title>
<p>The next four kernels use the syntax tree representation of sentences (see <xref ref-type="fig" rid="pcbi-1000837-g001">Figure 1</xref>). They differ in the definition of extracted substructures. The <italic>subtree kernel</italic> considers all common subtrees in the syntax tree representation of two compared sentences <xref ref-type="bibr" rid="pcbi.1000837-Vishwanathan1">[30]</xref>. Therein, a subtree is a node with all its descendants in the tree (see again <xref ref-type="fig" rid="pcbi-1000837-g001">Figure 1</xref>). Two subtrees are identical if the node labels and order of children are identical for all nodes.</p>
</sec><sec id="s2c3">
<title>Subset tree kernel (SST)</title>
<p>The <italic>subset tree kernel</italic> relaxes the constraint that all descendants, including leaves, must always be included in the substructures <xref ref-type="bibr" rid="pcbi.1000837-Collins1">[20]</xref>. It retains the constraint that grammatical rules must not be broken. For a given tree node, either none or all of its children must be included in the resulting subset tree (see <xref ref-type="fig" rid="pcbi-1000837-g001">Figure 1</xref>). As for the ST kernel, the order of child nodes matters.</p>
</sec><sec id="s2c4">
<title>Partial tree kernel (PT)</title>
<p>The <italic>partial tree kernel</italic> is the most permissive syntax-tree-based kernel we considered <xref ref-type="bibr" rid="pcbi.1000837-Moschitti1">[28]</xref>. It allows virtually any tree substructures; the only constraint that is kept is that the order of child nodes must be identical (see <xref ref-type="fig" rid="pcbi-1000837-g001">Figure 1</xref>).</p>
</sec><sec id="s2c5">
<title>Spectrum tree kernel (SpT)</title>
<p>The <italic>spectrum tree kernel</italic> focuses on simpler syntax-tree substructures than those discussed so far. It compares all vertex-walks (v-walks), sequences of edge-connected syntax tree nodes, of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e006" xlink:type="simple"/></inline-formula> (also known as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e007" xlink:type="simple"/></inline-formula>-grams, <xref ref-type="bibr" rid="pcbi.1000837-Kuboyama1">[58]</xref>). Note that the orientation of edges is important: the vertex-walks <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e008" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e009" xlink:type="simple"/></inline-formula> are thus not identical (see <xref ref-type="fig" rid="pcbi-1000837-g001">Figure 1</xref>).</p>
</sec><sec id="s2c6">
<title><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e010" xlink:type="simple"/></inline-formula>-band shortest path spectrum kernel (kBSPS)</title>
<p>In <xref ref-type="bibr" rid="pcbi.1000837-Palaga1">[29]</xref>, we proposed a new kernel function that is an extension of the SpT kernel. As it was not published before, we explain it here in more detail. kBSPS combines three ideas: First, the syntax-tree-based SpT kernel is adapted to dependency graphs. Second, the definition of v-walk is extended and when comparing two v-walks, certain mismatches are allowed. Third, it considers not only the shortest path between two proteins in the graph (as many others do), but also adds neighboring nodes. The first two extensions work as follows. The kBSPS kernel first includes edge labels into v-walks, which determine the dependency type of a relationship (see <xref ref-type="fig" rid="pcbi-1000837-g002">Figure 2</xref>). For consistency, the length of such v-walks remains the number of included nodes, i.e., edges are not counted into the length. Vertex-walks of dependency graphs contain on average more surface tokens than syntax tree v-walks, because the latter contain surface tokens only in leaves, of which at most two may be present in any syntax tree v-walk. Since the variation in surface tokens is much larger than in internal nodes of syntax trees, a tolerant matching is necessary to allow for linguistic variation. This tolerant matching distinguishes three types of nodes: dependency types (D), candidate entities (E), and other surface tokens (L). Mismatches/matches are then scored differently depending on the type of nodes (determined by appropriate parameters). When two v-walks are compared, a tolerated mismatch assigns score 0 only to the given node in the v-walk, while an untolerated mismatch sets the entire similarity score to 0 (see examples in <xref ref-type="fig" rid="pcbi-1000837-g003">Figure 3</xref>). The third extension changes the substructures that are compared by representing them as v-walks. Instead of using all v-walks of the dependency graph, kBSPS starts from only considering those one lying on the shortest path between the investigated entity pair. It is widely acknowledged that tokens on this path carry most information regarding their relationship; however, in some cases, interacting words are outside this scope, like in “<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e011" xlink:type="simple"/></inline-formula> is an <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e012" xlink:type="simple"/></inline-formula> binding protein.” Therefore, optionally, kBSPS also adds all nodes within distance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e013" xlink:type="simple"/></inline-formula> from the shortest path of the investigated entity pair. The resulting subgraph is called <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e014" xlink:type="simple"/></inline-formula>-band shortest path of a pair <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e015" xlink:type="simple"/></inline-formula>. Finally, the similarity of two entity pairs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e016" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e017" xlink:type="simple"/></inline-formula> is calculated as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e018" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e019" xlink:type="simple"/></inline-formula> is the set of v-walks of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e020" xlink:type="simple"/></inline-formula> generated from the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e021" xlink:type="simple"/></inline-formula>-band shortest path of pair <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e022" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e023" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e024" xlink:type="simple"/></inline-formula> control the range of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e025" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e026" xlink:type="simple"/></inline-formula> is the tolerant matching score (defined in Supporting Information, <xref ref-type="supplementary-material" rid="pcbi.1000837.s010">Text S1</xref>, exemplified in <xref ref-type="fig" rid="pcbi-1000837-g003">Figure 3</xref>).</p>
<fig id="pcbi-1000837-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.g003</object-id><label>Figure 3</label><caption>
<title>Examples of tolerant matching.</title>
<p>L, E mismatches are tolerated (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e027" xlink:type="simple"/></inline-formula>), D mismatches are untolerated (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e028" xlink:type="simple"/></inline-formula>); similarity weights are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e029" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e030" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e031" xlink:type="simple"/></inline-formula>. For kBSPS, we use default values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e032" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e033" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e034" xlink:type="simple"/></inline-formula> for the kernel.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.g003" xlink:type="simple"/></fig></sec><sec id="s2c7">
<title>Cosine similarity kernel (cosine)</title>
<p>In <xref ref-type="bibr" rid="pcbi.1000837-Erkan1">[22]</xref>, the authors define two kernel functions based on the cosine similarity and the edit distance among the shortest paths between protein names in a dependency tree parse (see <xref ref-type="fig" rid="pcbi-1000837-g002">Figure 2</xref>). Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e035" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e036" xlink:type="simple"/></inline-formula> be two such shortest paths between two pairs of analyzed entities. The <italic>cosine similarity kernel</italic> calculates the angle between the representation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e037" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e038" xlink:type="simple"/></inline-formula> as vectors of term frequencies in a vector space. Basically cosine counts the number of common terms of the two paths, normalized by the length of the paths.</p>
</sec><sec id="s2c8">
<title>Edit distance kernel (edit)</title>
<p>The drawback of the cosine similarity for textual data is its order-independence. The <italic>edit distance kernel</italic>, also proposed in <xref ref-type="bibr" rid="pcbi.1000837-Erkan1">[22]</xref>, overcomes this issue. Therein, the distance between two paths is defined as the edit distance between them, i.e., the minimal number of operations (deletion, insertion, substitution at word level) needed to transform one path into the other, normalized by the length of the longer path. This measure is converted into a similarity measure using:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e039" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e040" xlink:type="simple"/></inline-formula> is a parameter.</p>
</sec><sec id="s2c9">
<title>All-paths graph kernel (APG)</title>
<p>The <italic>all-paths graph kernel</italic> <xref ref-type="bibr" rid="pcbi.1000837-Airola1">[17]</xref> counts weighted shared paths of all possible lengths. Paths are generated both from the dependency parse and from the surface word sequence of the sentence. Path weights are determined by dependencies weights which are the higher the shorter the distance of the dependency to the shortest path between the candidate entities is. One peculiarity of Airola's method is the usage of the sparse regularized least squares (RLS) method (instead of standard SVM), which is a state-of-the-art kernel-based machine learning method that scales very well with very large training sets. For comparison, we also trained APG kernel with SVM.</p>
</sec><sec id="s2c10">
<title>Other kernels</title>
<p>In the literature, several further kernel-based approaches to relationship extraction were proposed. We give a brief survey of them below. Note that most of these kernels are either unavailable as programs or very similar to at least one of those we selected for our benchmark (see also <xref ref-type="supplementary-material" rid="pcbi.1000837.s002">Table S2</xref>).</p>
<p>In <xref ref-type="bibr" rid="pcbi.1000837-Kim1">[25]</xref> predicate, walk, dependency, and hybrid kernels are proposed, each operating on dependency trees extended with shallow linguistic and gazetteer information. The <italic>walk kernel</italic> showed the best performance. It generates vertex-walks and edge-walks (edge-based counterpart of v-walks) of fixed length two on syntactic (POS) and lexical (token) level along the shortest path between the analyzed entities. A polynomial SVM kernel was applied to calculate the similarity between vectors. The idea of Kim was developed further in <xref ref-type="bibr" rid="pcbi.1000837-VanLandeghem1">[26]</xref> by augmenting the original feature set with additional sentence characteristics, for example, word stems of all tokens and shortest path length. Since the feature set can get pretty large (10k+ features), feature selection is applied before training. Both kernels were unavailable.</p>
<p>In <xref ref-type="bibr" rid="pcbi.1000837-Culotta1">[21]</xref>, a kernel that used subtrees of dependency trees is proposed. The nodes of the dependency tree were augmented with various syntactic and semantic features. A kernel function was applied to compare subtrees, calculating the common contiguous or sparse subsequences of nodes, which incorporated a similarity function for the augmented features. A similar kernel function was proposed in <xref ref-type="bibr" rid="pcbi.1000837-Zelenko1">[32]</xref>, albeit with a smaller feature set. The source code for these kernels is not publicly available.</p>
<p>The general sparse subsequence kernel for relation extraction <xref ref-type="bibr" rid="pcbi.1000837-Bunescu2">[19]</xref> calculates the total number of weighted subsequences of a given length between two strings. Sentences are represented by fore-between, between, and between-after sequences relative to the investigated entity pair. The sequences can be defined over various alphabets, such as set of words, POS tags, or broader word classes. This kernel is similar to SL kernel proposed in <xref ref-type="bibr" rid="pcbi.1000837-Giuliano1">[23]</xref>. SL kernel uses similar feature sets and it is computationally much more effective, though order-independent kernel.</p>
<p>A mixture of previous approaches was proposed in <xref ref-type="bibr" rid="pcbi.1000837-Wang1">[31]</xref>, called <italic>convolution dependency path kernel</italic>, which combined the beneficial high recall of subsequence kernels with the reduced feature space using syntactic information of shortest path of dependency trees. The combined kernel applied the subsequence kernel on the shortest dependency paths, which makes it very similar to the method of <xref ref-type="bibr" rid="pcbi.1000837-Culotta1">[21]</xref>. The source code of the kernel was not available.</p>
<p>In <xref ref-type="bibr" rid="pcbi.1000837-Miwa1">[27]</xref>, a combined multiple layers of syntactic information is proposed. A bag-of-words kernel, a subset tree kernel <xref ref-type="bibr" rid="pcbi.1000837-Moschitti1">[28]</xref>, and an APG kernel <xref ref-type="bibr" rid="pcbi.1000837-Airola1">[17]</xref> were used together with dependency parses and deep parses. The kernels were combined simply through summing the normalized values of each kernel for each parse. The hybrid kernel is currently not available.</p>
<p>In <xref ref-type="bibr" rid="pcbi.1000837-Katrenko1">[24]</xref>, the authors used the Smith–Waterman distance function when comparing two string sequences. Their <italic>local alignment kernel</italic> was then defined as the sum of SW scores on all possible alignments between the strings. To compute SW distance, a substitution matrix should be initialized with the pairwise similarity of any two words. The matrix elements were estimated by distributional similarity measures calculated on a large independent corpus, which is very costly in terms of time. The source of the approach is currently not available.</p>
</sec></sec><sec id="s2d">
<title>Corpora</title>
<p>There is no widely accepted definition of the concept of PPI, i.e., what should be annotated as PPI in text, therefore methods evaluated on different PPI-annotated corpora are difficult to compare. In <xref ref-type="bibr" rid="pcbi.1000837-Pyysalo1">[36]</xref>, a thorough analysis of five freely available PPI-annotated resources, namely AIMed <xref ref-type="bibr" rid="pcbi.1000837-Bunescu1">[18]</xref>, BioInfer <xref ref-type="bibr" rid="pcbi.1000837-Pyysalo3">[59]</xref>, HPRD50 <xref ref-type="bibr" rid="pcbi.1000837-Fundel1">[60]</xref>, IEPA <xref ref-type="bibr" rid="pcbi.1000837-Ding1">[61]</xref>, and LLL <xref ref-type="bibr" rid="pcbi.1000837-Nedellec1">[62]</xref>, was performed. Some basic statistics of the corpora can be found in <xref ref-type="table" rid="pcbi-1000837-t001">Table 1</xref>. Although all of these corpora carry information about named entities and all annotate PPIs, there are many aspects in which the corpora show significant differences. Corpora differ in quite a few aspects, for instance, the scope of annotated entities varies (typically proteins and genes, some also RNAs, but IEPA only chemicals), the coverage of entities is not always complete, some corpora specify the direction of interactions, just to name a few. As “greatest common factor” among the notions of PPI, in <xref ref-type="bibr" rid="pcbi.1000837-Pyysalo1">[36]</xref> it is suggested to use only the information on undirected, untyped interactions (among a few other constraints) for evaluation purposes. We also followed this suggestion.</p>
<table-wrap id="pcbi-1000837-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.t001</object-id><label>Table 1</label><caption>
<title>Basic statistics of the 5 corpora used for kernel evaluation.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000837-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Corpus</td>
<td align="left" colspan="1" rowspan="1">Sentences</td>
<td align="left" colspan="1" rowspan="1">Positive pairs</td>
<td align="left" colspan="1" rowspan="1">Negative pairs</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">AIMed</td>
<td align="left" colspan="1" rowspan="1">1955</td>
<td align="left" colspan="1" rowspan="1">1000</td>
<td align="left" colspan="1" rowspan="1">4834</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">BioInfer</td>
<td align="left" colspan="1" rowspan="1">1100</td>
<td align="left" colspan="1" rowspan="1">2534</td>
<td align="left" colspan="1" rowspan="1">7132</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">HPRD50</td>
<td align="left" colspan="1" rowspan="1">145</td>
<td align="left" colspan="1" rowspan="1">163</td>
<td align="left" colspan="1" rowspan="1">270</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">IEPA</td>
<td align="left" colspan="1" rowspan="1">486</td>
<td align="left" colspan="1" rowspan="1">335</td>
<td align="left" colspan="1" rowspan="1">482</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">LLL</td>
<td align="left" colspan="1" rowspan="1">77</td>
<td align="left" colspan="1" rowspan="1">164</td>
<td align="left" colspan="1" rowspan="1">166</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt101"><label/><p>Pairs are checked for (orderless) uniqueness; self-interacting proteins are excluded.</p></fn></table-wrap-foot></table-wrap>
<p>In the same study, an XML-based format was also defined for annotating PPIs, called <italic>PPI learning format</italic>. The authors transformed all five aforementioned corpora into this format, which we reuse. The general structure of the learning format is shown in <xref ref-type="fig" rid="pcbi-1000837-g004">Figure 4</xref>. Each corpus consists of documents, and documents consist of sentences. The sentence text is located in the attribute <italic>text</italic>. The actual annotation of named entities and their relations is encoded through <italic>entity</italic> and <italic>pair</italic> elements. The position of an entity in the sentence text is specified in the <italic>charOffset</italic> attribute.</p>
<fig id="pcbi-1000837-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.g004</object-id><label>Figure 4</label><caption>
<title>The general structure of the learning format.</title>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.g004" xlink:type="simple"/></fig>
<p>The presence or absence of a relation is marked on the level of named entity pairs, not on the level of sentences (cf. attribute <italic>interaction</italic> of <italic>pair</italic> in <xref ref-type="fig" rid="pcbi-1000837-g004">Figure 4</xref>), which enables the annotation of multiple entity pairs per sentence. For instance, in the sentence in <xref ref-type="fig" rid="pcbi-1000837-g004">Figure 4</xref>, there is a relation between entities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e041" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e042" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e043" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e044" xlink:type="simple"/></inline-formula>, whereas there is no relation between entities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e045" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e046" xlink:type="simple"/></inline-formula>. Consequently, the learning examples used by a classifier correspond to entity pairs rather than to sentences. The learning format also provides means for expressing token boundaries and dependency parses of sentences, and it allows to store several alternative tokenizations and parses for a given sentence.</p>
</sec><sec id="s2e">
<title>Evaluation Methods</title>
<p>We use various performance measures to evaluate kernel-based classifiers for PPI extraction. On one hand, we report on the standard evaluation measures: precision, recall, and F<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e047" xlink:type="simple"/></inline-formula>-score. F-score has been criticized recently as inadequate for PPI extraction because of its sensitivity to the ratio of positive/negative examples in the training set <xref ref-type="bibr" rid="pcbi.1000837-Airola1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1000837-Pyysalo1">[36]</xref>. Therefore, we also report on the AUC measure (area under the receiver operating characteristics curve) of the methods, which is invariant to the class distribution in the data sets. We evaluated all kernel methods in three different settings: Cross-validation, cross-learning, and cross-corpus. None of these is new; cross-validation still seems to be the current de facto standard in PPI extraction, cross-learning was proposed in <xref ref-type="bibr" rid="pcbi.1000837-VanLandeghem1">[26]</xref>, and cross-corpus was, for instance, used in <xref ref-type="bibr" rid="pcbi.1000837-Kabiljo1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1000837-Airola1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1000837-Miwa2">[63]</xref>.</p>
<sec id="s2e1">
<title>Cross-Validation (CV)</title>
<p>In this setting, we train and test each kernel on the same corpus using document-level 10-fold cross-validation. We refrain from using the also frequently mentioned instance-level splitting, in which every sentence containing more than two protein names may appear, though with different labeling, both in the training and the test sets. This is a clear case of information leakage and compromises the evaluation results. Its impact on PPI results is higher than in many other domains, since in PPI corpora sentences very often contain more than two protein names. We employ the document-level splits that were used by Airola and many others, which allow direct comparison of the results. We indicate the standard deviation of the averaged 10-fold cross-validation values.</p>
</sec><sec id="s2e2">
<title>Cross-Learning (CL)</title>
<p>Although the document-level 10-fold cross-validation became the de facto standard of PPI relation extraction evaluation, it is also somewhat biased, because the training and the test data sets have very similar corpus characteristics. It was shown <xref ref-type="bibr" rid="pcbi.1000837-Pyysalo1">[36]</xref> that the different positive/negative interaction pair distribution of the five benchmark corpora accounts for a substantial part of the diversity of the performance of approaches. Since the ultimate goal of PPI extraction is the identification of PPIs in biomedical texts with unknown characteristics, we performed experiments with learning across corpora, where the training and test data sets are drawn from different distributions. In CL experiments, we train on the ensemble of four corpora and test on the fifth one.</p>
</sec><sec id="s2e3">
<title>Cross-Corpus (CC)</title>
<p>Finally, in CC experiments, we train the model on one corpus and then test on the other four corpora.</p>
<p>Apart from measuring the quality of the extractions, we also looked at the time it takes to classify the corpora. Whenever the texts to be analyzed are large, classification time may be the decisive factor to choose a method. However, we did not take particular measures to obtain perfect run times (eliminating all concurrent processes on the machines), so our times should only be considered as rough estimates. We should also mention that all the tested software are prototypes where the efficiency of implementations may significantly differ. Nevertheless, these figures should be good indicators of what can be expected when using the kernels out-of-the-box. Note that all methods we analyzed also require extra time (in addition to classification) to parse sentences.</p>
</sec></sec><sec id="s2f">
<title>Experimental Setup</title>
<sec id="s2f1">
<title>Entity blinding</title>
<p>All corpora we use for evaluation have all entities readily annotated. This means that our results only measure the performance of PPI extraction and are not influenced by problems of named entity recognition. However, to produce the right format for the kernel methods, we apply entity blinding, that is, we replace named entity occurrences with a generic string. Entity blinding is usually applied in relation extraction systems to (1) inform the classifier about the location of the NEs; (2) ensure the generality of the learned model, since classifiers should work for any entity in the given context. Before doing that we had to resolve the entity–token mismatch problem.</p>
<p>Syntax and dependency parsers work on token-based representation of sentence text being the output of the tokenization, also encoded in the learning format. Entities, however, may not match directly contiguous token sequences; this phenomenon has to be resolved for enabling the entity-based referencing of PPIs. Practically all combinations of entailment and overlapping occur in text: one entity may spread over several tokens or correspond merely to a part of a token, and there may exist several named entities in one token. We depicted some examples of the entity–token mismatch phenomenon in <xref ref-type="fig" rid="pcbi-1000837-g005">Figure 5</xref>.</p>
<fig id="pcbi-1000837-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.g005</object-id><label>Figure 5</label><caption>
<title>Learning format pitfalls (sentence BioInfer.d77.s0).</title>
<p>(1) Named entities may overlap. The string Arp2/3 contains two named entities, namely Arp2 and Arp3. (2) An entity may spread over multiple noncontiguous text ranges. The entity Arp3 paragraph spreads over two ranges [0–2] and [5–5]. (3) Such noncontiguous and overlapping entities may constitute a relation, such as in The Arp2/3 complex….</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.g005" xlink:type="simple"/></fig>
<p>In order to overcome these difficulties and adopt a clear entity–token mapping concept, we apply the following strategy: every token that at least partly overlaps with an entity is marked as entity. Entity blinding is performed as follows: A sentence with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e048" xlink:type="simple"/></inline-formula> entities contains <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e049" xlink:type="simple"/></inline-formula> possibly differently labeled entity pairs (see <xref ref-type="fig" rid="pcbi-1000837-g004">Figures 4</xref>+<xref ref-type="fig" rid="pcbi-1000837-g005">5</xref>). For each entity pair of the sentence, we replicate the sentence and create a separate learning example. In order to distinguish entities of the learning example from other entities, we label all tokens of the entity pair under consideration as _ENT_1_ and _ENT_2_, respectively, while we label the others as _ENT_. In case of overlapping entities (cf. <xref ref-type="fig" rid="pcbi-1000837-g005">Figure 5</xref>), we use the special label _ENT_1_AND_2_ for the token including both entities; this strategy was also applied in <xref ref-type="bibr" rid="pcbi.1000837-Airola1">[17]</xref>.</p>
</sec><sec id="s2f2">
<title>Constituent tree parses</title>
<p>Since some of the selected kernel methods, namely ST, SST, PT and SpT kernels are defined for syntax trees, we injected the syntax tree parses into the learning format. The terminal symbols of the syntax tree parses (i.e., tokens) were mapped to the character offsets of the original sentence text. This was necessary for the entity blinding in the constituent tree parse. Finally, the parses were formatted so that they comply with the expectations of the given kernel's implementation (the extended corpus files are available at our web site).</p>
</sec><sec id="s2f3">
<title>Parameter optimization</title>
<p>All evaluated methods have several parameters whose setting has significant impact on the performance. To achieve best results, authors often apply an exhaustive systematic parameter search—a multidimensional fine-grained grid search for myriads of parameter combinations—for each corpus they evaluate on. However, results obtained in this way cannot be expected to be the same as for other corpora or for new texts, where such an optimization is not possible. In this study, we take the role of an end-user which has a completely new text and wants to choose a PPI extraction method to extract all PPIs from this text. Which parameters should this user apply?</p>
<p>Ideally, one could simply use the default parameters of the kernels, leaving the choice of best settings to the authors of the kernels. This was our initial idea, which we had to abandon for two reasons: (1) for some syntax-tree-based kernels (ST, SST, PT), the default regularization parameter of the learner, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e050" xlink:type="simple"/></inline-formula>, often produced <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e051" xlink:type="simple"/></inline-formula>% F-score; (2) for APG there is no explicit default parameterization. As a compromise, we resorted to a coarse-grained grid parameter search only on a small set of important parameters (see <xref ref-type="supplementary-material" rid="pcbi.1000837.s004">Table S4</xref>). We selected the best average setting as the <italic>de facto</italic> default setting for each kernel. We did not perform separate optimization runs for AUC and F-score, thus reported data always belong to the same experiment.</p>
<p>Also in CC evaluation, optimization geared towards the test-corpus may improve the performance. As shown in <xref ref-type="bibr" rid="pcbi.1000837-Airola1">[17, Tables 3 and 4]</xref>, the F-score can raise tremendously (sometimes by 50 points) when the APG-based classifier is optimized with a threshold according to the ratio of positive/negative pairs in the test corpus. We refrained from using such an optimization technique at CC evaluation, because again such information is not available in a real world application.</p>
</sec></sec></sec><sec id="s3">
<title>Results</title>
<p>We performed a thorough evaluation of nine different methods for extracting protein-protein interactions from text on five different, publicly available and manually annotated corpora. All methods we studied classify each pair of proteins in a sentence using a kernel function. The methods differ widely in their individual definition of this kernel function (comparing all subtrees, all subsets, all paths,…), use different classifiers, and make use of different types of information (shallow linguistic information, syntax trees or dependency graphs).</p>
<p>We report results in three different scenarios. In <italic>cross-validation</italic>, each corpus is treated independently from each other. Reported results are the average over a document-level 10-fold cross-validation per corpus. Even though this strategy is the de facto way of evaluating PPI extraction systems, its results cannot be safely extrapolated to the application of a method on completely new text, as the model that is learned overfits to the particular corpus. In <italic>cross-learning</italic>, training and test data come from different corpora altogether. We report results on five experiments, where in each experiment each method was trained on four corpora and tested on the fifth. This strategy should produce results that are much more likely to hold also on unseen texts. A variation of this strategy is <italic>cross-corpus</italic>, where we always train on one corpus and evaluate on the other four. Obviously, one expects worse results in CC than in CL, as the diversity of training data is reduced, while the heterogeneity in the test data is increased.</p>
<sec id="s3a">
<title>Cross-Validation</title>
<p><xref ref-type="table" rid="pcbi-1000837-t002">Table 2</xref> and <xref ref-type="fig" rid="pcbi-1000837-g006">Figure 6</xref> give results of CV on a per-corpus basis. In the table, for SL, kBSPS, cosine, edit, and APG kernels we provide both our own measurements and the ones published in the respective original paper. We also ran APG with SVM. Recall that, to closely imitate the real word scenario, we did not perform a systematic parameter tuning (see <xref ref-type="sec" rid="s2">Methods</xref>). <xref ref-type="table" rid="pcbi-1000837-t002">Table 2</xref> also contains results for rich-feature-vector-based kernel <xref ref-type="bibr" rid="pcbi.1000837-VanLandeghem1">[26]</xref> and hybrid kernel <xref ref-type="bibr" rid="pcbi.1000837-Miwa2">[63]</xref>, which are both not covered in our evaluation. As a baseline, we additionally give precision/recall/F-score values for the sentence based co-occurrence methods and the rule-based RelEx <xref ref-type="bibr" rid="pcbi.1000837-Fundel1">[60]</xref>.</p>
<fig id="pcbi-1000837-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.g006</object-id><label>Figure 6</label><caption>
<title>AUC, F-score, precision and recall values with CV evaluation, including standard deviation measured on the 10 folds.</title>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.g006" xlink:type="simple"/></fig><table-wrap id="pcbi-1000837-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.t002</object-id><label>Table 2</label><caption>
<title>10-fold document-level CV results.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000837-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.t002" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Kernel</td>
<td align="left" colspan="4" rowspan="1">AIMed</td>
<td align="left" colspan="4" rowspan="1">BioInfer</td>
<td align="left" colspan="4" rowspan="1">HPRD50</td>
<td align="left" colspan="4" rowspan="1">IEPA</td>
<td align="left" colspan="4" rowspan="1">LLL</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">SL</td>
<td align="left" colspan="1" rowspan="1">83.5</td>
<td align="left" colspan="1" rowspan="1">47.5</td>
<td align="left" colspan="1" rowspan="1"><bold>65.5</bold></td>
<td align="left" colspan="1" rowspan="1">54.5</td>
<td align="left" colspan="1" rowspan="1"><bold>81.1</bold></td>
<td align="left" colspan="1" rowspan="1">55.1</td>
<td align="left" colspan="1" rowspan="1">66.5</td>
<td align="left" colspan="1" rowspan="1"><bold>60.0</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>80.0</bold></td>
<td align="left" colspan="1" rowspan="1">64.4</td>
<td align="left" colspan="1" rowspan="1">67.0</td>
<td align="left" colspan="1" rowspan="1">64.2</td>
<td align="left" colspan="1" rowspan="1">81.1</td>
<td align="left" colspan="1" rowspan="1">69.5</td>
<td align="left" colspan="1" rowspan="1">71.2</td>
<td align="left" colspan="1" rowspan="1">69.3</td>
<td align="left" colspan="1" rowspan="1">81.2</td>
<td align="left" colspan="1" rowspan="1">69.0</td>
<td align="left" colspan="1" rowspan="1">85.3</td>
<td align="left" colspan="1" rowspan="1">74.5</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ST</td>
<td align="left" colspan="1" rowspan="1">68.9</td>
<td align="left" colspan="1" rowspan="1">40.3</td>
<td align="left" colspan="1" rowspan="1">25.5</td>
<td align="left" colspan="1" rowspan="1">30.9</td>
<td align="left" colspan="1" rowspan="1">74.2</td>
<td align="left" colspan="1" rowspan="1">46.8</td>
<td align="left" colspan="1" rowspan="1">60.0</td>
<td align="left" colspan="1" rowspan="1">52.2</td>
<td align="left" colspan="1" rowspan="1">63.3</td>
<td align="left" colspan="1" rowspan="1">49.7</td>
<td align="left" colspan="1" rowspan="1">67.8</td>
<td align="left" colspan="1" rowspan="1">54.5</td>
<td align="left" colspan="1" rowspan="1">75.8</td>
<td align="left" colspan="1" rowspan="1">59.4</td>
<td align="left" colspan="1" rowspan="1">75.6</td>
<td align="left" colspan="1" rowspan="1">65.9</td>
<td align="left" colspan="1" rowspan="1">69.0</td>
<td align="left" colspan="1" rowspan="1">55.9</td>
<td align="left" colspan="1" rowspan="1"><bold>100.</bold></td>
<td align="left" colspan="1" rowspan="1">70.3</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SST</td>
<td align="left" colspan="1" rowspan="1">68.9</td>
<td align="left" colspan="1" rowspan="1">42.6</td>
<td align="left" colspan="1" rowspan="1">19.4</td>
<td align="left" colspan="1" rowspan="1">26.2</td>
<td align="left" colspan="1" rowspan="1">73.6</td>
<td align="left" colspan="1" rowspan="1">47.0</td>
<td align="left" colspan="1" rowspan="1">54.3</td>
<td align="left" colspan="1" rowspan="1">50.1</td>
<td align="left" colspan="1" rowspan="1">62.2</td>
<td align="left" colspan="1" rowspan="1">48.1</td>
<td align="left" colspan="1" rowspan="1">63.8</td>
<td align="left" colspan="1" rowspan="1">52.2</td>
<td align="left" colspan="1" rowspan="1">72.4</td>
<td align="left" colspan="1" rowspan="1">54.8</td>
<td align="left" colspan="1" rowspan="1">76.9</td>
<td align="left" colspan="1" rowspan="1">63.4</td>
<td align="left" colspan="1" rowspan="1">63.8</td>
<td align="left" colspan="1" rowspan="1">55.9</td>
<td align="left" colspan="1" rowspan="1"><bold>100.</bold></td>
<td align="left" colspan="1" rowspan="1">70.3</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PT</td>
<td align="left" colspan="1" rowspan="1">68.5</td>
<td align="left" colspan="1" rowspan="1">39.2</td>
<td align="left" colspan="1" rowspan="1">31.9</td>
<td align="left" colspan="1" rowspan="1">34.6</td>
<td align="left" colspan="1" rowspan="1">73.8</td>
<td align="left" colspan="1" rowspan="1">45.3</td>
<td align="left" colspan="1" rowspan="1">58.1</td>
<td align="left" colspan="1" rowspan="1">50.5</td>
<td align="left" colspan="1" rowspan="1">65.2</td>
<td align="left" colspan="1" rowspan="1">54.9</td>
<td align="left" colspan="1" rowspan="1">56.7</td>
<td align="left" colspan="1" rowspan="1">52.4</td>
<td align="left" colspan="1" rowspan="1">73.1</td>
<td align="left" colspan="1" rowspan="1">63.1</td>
<td align="left" colspan="1" rowspan="1">66.3</td>
<td align="left" colspan="1" rowspan="1">63.8</td>
<td align="left" colspan="1" rowspan="1">66.7</td>
<td align="left" colspan="1" rowspan="1">56.2</td>
<td align="left" colspan="1" rowspan="1">97.3</td>
<td align="left" colspan="1" rowspan="1">69.3</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SpT</td>
<td align="left" colspan="1" rowspan="1">66.1</td>
<td align="left" colspan="1" rowspan="1">33.0</td>
<td align="left" colspan="1" rowspan="1">25.5</td>
<td align="left" colspan="1" rowspan="1">27.3</td>
<td align="left" colspan="1" rowspan="1">74.1</td>
<td align="left" colspan="1" rowspan="1">44.0</td>
<td align="left" colspan="1" rowspan="1"><bold>68.2</bold></td>
<td align="left" colspan="1" rowspan="1">53.4</td>
<td align="left" colspan="1" rowspan="1">65.7</td>
<td align="left" colspan="1" rowspan="1">49.3</td>
<td align="left" colspan="1" rowspan="1">71.7</td>
<td align="left" colspan="1" rowspan="1">56.4</td>
<td align="left" colspan="1" rowspan="1">75.9</td>
<td align="left" colspan="1" rowspan="1">54.5</td>
<td align="left" colspan="1" rowspan="1">81.8</td>
<td align="left" colspan="1" rowspan="1">64.7</td>
<td align="left" colspan="1" rowspan="1">50.0</td>
<td align="left" colspan="1" rowspan="1">55.9</td>
<td align="left" colspan="1" rowspan="1"><bold>100.</bold></td>
<td align="left" colspan="1" rowspan="1">70.3</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">kBSPS</td>
<td align="left" colspan="1" rowspan="1">75.1</td>
<td align="left" colspan="1" rowspan="1">50.1</td>
<td align="left" colspan="1" rowspan="1">41.4</td>
<td align="left" colspan="1" rowspan="1">44.6</td>
<td align="left" colspan="1" rowspan="1">75.2</td>
<td align="left" colspan="1" rowspan="1">49.9</td>
<td align="left" colspan="1" rowspan="1">61.8</td>
<td align="left" colspan="1" rowspan="1">55.1</td>
<td align="left" colspan="1" rowspan="1">79.3</td>
<td align="left" colspan="1" rowspan="1">62.2</td>
<td align="left" colspan="1" rowspan="1"><bold>87.1</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>71.0</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>83.2</bold></td>
<td align="left" colspan="1" rowspan="1">58.8</td>
<td align="left" colspan="1" rowspan="1"><bold>89.7</bold></td>
<td align="left" colspan="1" rowspan="1">70.5</td>
<td align="left" colspan="1" rowspan="1">84.3</td>
<td align="left" colspan="1" rowspan="1">69.3</td>
<td align="left" colspan="1" rowspan="1">93.2</td>
<td align="left" colspan="1" rowspan="1"><bold>78.1</bold></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">cosine</td>
<td align="left" colspan="1" rowspan="1">70.5</td>
<td align="left" colspan="1" rowspan="1">43.6</td>
<td align="left" colspan="1" rowspan="1">39.4</td>
<td align="left" colspan="1" rowspan="1">40.9</td>
<td align="left" colspan="1" rowspan="1">66.1</td>
<td align="left" colspan="1" rowspan="1">44.8</td>
<td align="left" colspan="1" rowspan="1">44.0</td>
<td align="left" colspan="1" rowspan="1">44.1</td>
<td align="left" colspan="1" rowspan="1">74.8</td>
<td align="left" colspan="1" rowspan="1">59.0</td>
<td align="left" colspan="1" rowspan="1">67.2</td>
<td align="left" colspan="1" rowspan="1">61.2</td>
<td align="left" colspan="1" rowspan="1">75.5</td>
<td align="left" colspan="1" rowspan="1">61.3</td>
<td align="left" colspan="1" rowspan="1">68.4</td>
<td align="left" colspan="1" rowspan="1">64.1</td>
<td align="left" colspan="1" rowspan="1">75.2</td>
<td align="left" colspan="1" rowspan="1">70.2</td>
<td align="left" colspan="1" rowspan="1">81.7</td>
<td align="left" colspan="1" rowspan="1">73.8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">edit</td>
<td align="left" colspan="1" rowspan="1">75.2</td>
<td align="left" colspan="1" rowspan="1"><bold>68.8</bold></td>
<td align="left" colspan="1" rowspan="1">27.7</td>
<td align="left" colspan="1" rowspan="1">39.0</td>
<td align="left" colspan="1" rowspan="1">67.4</td>
<td align="left" colspan="1" rowspan="1">50.4</td>
<td align="left" colspan="1" rowspan="1">39.2</td>
<td align="left" colspan="1" rowspan="1">43.8</td>
<td align="left" colspan="1" rowspan="1">79.2</td>
<td align="left" colspan="1" rowspan="1">71.3</td>
<td align="left" colspan="1" rowspan="1">45.2</td>
<td align="left" colspan="1" rowspan="1">53.3</td>
<td align="left" colspan="1" rowspan="1">80.2</td>
<td align="left" colspan="1" rowspan="1"><bold>77.2</bold></td>
<td align="left" colspan="1" rowspan="1">60.2</td>
<td align="left" colspan="1" rowspan="1">67.1</td>
<td align="left" colspan="1" rowspan="1"><bold>87.5</bold></td>
<td align="left" colspan="1" rowspan="1">68.0</td>
<td align="left" colspan="1" rowspan="1">98.0</td>
<td align="left" colspan="1" rowspan="1"><bold>78.4</bold></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">APG</td>
<td align="left" colspan="1" rowspan="1"><bold>84.6</bold></td>
<td align="left" colspan="1" rowspan="1">59.9</td>
<td align="left" colspan="1" rowspan="1">53.6</td>
<td align="left" colspan="1" rowspan="1"><bold>56.2</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>81.5</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>60.2</bold></td>
<td align="left" colspan="1" rowspan="1">61.3</td>
<td align="left" colspan="1" rowspan="1"><bold>60.7</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>80.9</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>68.2</bold></td>
<td align="left" colspan="1" rowspan="1">69.8</td>
<td align="left" colspan="1" rowspan="1">67.8</td>
<td align="left" colspan="1" rowspan="1"><bold>83.9</bold></td>
<td align="left" colspan="1" rowspan="1">66.6</td>
<td align="left" colspan="1" rowspan="1">82.6</td>
<td align="left" colspan="1" rowspan="1"><bold>73.1</bold></td>
<td align="left" colspan="1" rowspan="1">83.5</td>
<td align="left" colspan="1" rowspan="1"><bold>71.3</bold></td>
<td align="left" colspan="1" rowspan="1">91</td>
<td align="left" colspan="1" rowspan="1"><bold>78.1</bold></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">APG (with SVM)</td>
<td align="left" colspan="1" rowspan="1">71.2</td>
<td align="left" colspan="1" rowspan="1">62.9</td>
<td align="left" colspan="1" rowspan="1">48.9</td>
<td align="left" colspan="1" rowspan="1">54.7</td>
<td align="left" colspan="1" rowspan="1">73.9</td>
<td align="left" colspan="1" rowspan="1"><bold>60.2</bold></td>
<td align="left" colspan="1" rowspan="1">63.4</td>
<td align="left" colspan="1" rowspan="1"><bold>61.6</bold></td>
<td align="left" colspan="1" rowspan="1">74.1</td>
<td align="left" colspan="1" rowspan="1">65.4</td>
<td align="left" colspan="1" rowspan="1">72.5</td>
<td align="left" colspan="1" rowspan="1">67.5</td>
<td align="left" colspan="1" rowspan="1">76.2</td>
<td align="left" colspan="1" rowspan="1">71.0</td>
<td align="left" colspan="1" rowspan="1">75.1</td>
<td align="left" colspan="1" rowspan="1">72.1</td>
<td align="left" colspan="1" rowspan="1">74.9</td>
<td align="left" colspan="1" rowspan="1"><bold>70.9</bold></td>
<td align="left" colspan="1" rowspan="1">95.4</td>
<td align="left" colspan="1" rowspan="1"><bold>79.7</bold></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SL <xref ref-type="bibr" rid="pcbi.1000837-Giuliano1">[23]</xref></td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">60.9</td>
<td align="left" colspan="1" rowspan="1">57.2</td>
<td align="left" colspan="1" rowspan="1">59.0</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">kBSPS <xref ref-type="bibr" rid="pcbi.1000837-Palaga1">[29]</xref></td>
<td align="left" colspan="1" rowspan="1">67.2</td>
<td align="left" colspan="1" rowspan="1">49.4</td>
<td align="left" colspan="1" rowspan="1">44.7</td>
<td align="left" colspan="1" rowspan="1">46.1</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">76.9</td>
<td align="left" colspan="1" rowspan="1">66.7</td>
<td align="left" colspan="1" rowspan="1">80.2</td>
<td align="left" colspan="1" rowspan="1">70.9</td>
<td align="left" colspan="1" rowspan="1">75.8</td>
<td align="left" colspan="1" rowspan="1">70.4</td>
<td align="left" colspan="1" rowspan="1">73.0</td>
<td align="left" colspan="1" rowspan="1">70.8</td>
<td align="left" colspan="1" rowspan="1">78.5</td>
<td align="left" colspan="1" rowspan="1">76.8</td>
<td align="left" colspan="1" rowspan="1">91.8</td>
<td align="left" colspan="1" rowspan="1">82.2</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">cosine <xref ref-type="bibr" rid="pcbi.1000837-Erkan1">[22]</xref><xref ref-type="table-fn" rid="nt103">†</xref></td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">62.0</td>
<td align="left" colspan="1" rowspan="1">55.0</td>
<td align="left" colspan="1" rowspan="1">58.1</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">edit <xref ref-type="bibr" rid="pcbi.1000837-Erkan1">[22]</xref><xref ref-type="table-fn" rid="nt103">†</xref></td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">77.5</td>
<td align="left" colspan="1" rowspan="1">43.5</td>
<td align="left" colspan="1" rowspan="1">55.6</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">APG <xref ref-type="bibr" rid="pcbi.1000837-Airola1">[17]</xref></td>
<td align="left" colspan="1" rowspan="1">84.8</td>
<td align="left" colspan="1" rowspan="1">52.9</td>
<td align="left" colspan="1" rowspan="1">61.8</td>
<td align="left" colspan="1" rowspan="1">56.4</td>
<td align="left" colspan="1" rowspan="1">81.9</td>
<td align="left" colspan="1" rowspan="1">56.7</td>
<td align="left" colspan="1" rowspan="1">67.2</td>
<td align="left" colspan="1" rowspan="1">61.3</td>
<td align="left" colspan="1" rowspan="1">79.7</td>
<td align="left" colspan="1" rowspan="1">64.3</td>
<td align="left" colspan="1" rowspan="1">65.8</td>
<td align="left" colspan="1" rowspan="1">63.4</td>
<td align="left" colspan="1" rowspan="1">85.1</td>
<td align="left" colspan="1" rowspan="1">69.6</td>
<td align="left" colspan="1" rowspan="1">82.7</td>
<td align="left" colspan="1" rowspan="1">75.1</td>
<td align="left" colspan="1" rowspan="1">83.4</td>
<td align="left" colspan="1" rowspan="1">72.5</td>
<td align="left" colspan="1" rowspan="1">82.2</td>
<td align="left" colspan="1" rowspan="1">76.8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">rich-feature-based <xref ref-type="bibr" rid="pcbi.1000837-VanLandeghem1">[26]</xref></td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">49.0</td>
<td align="left" colspan="1" rowspan="1">44.0</td>
<td align="left" colspan="1" rowspan="1">46.0</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">60.0</td>
<td align="left" colspan="1" rowspan="1">51.0</td>
<td align="left" colspan="1" rowspan="1">55.0</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">64.0</td>
<td align="left" colspan="1" rowspan="1">70.0</td>
<td align="left" colspan="1" rowspan="1">67.0</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">72.0</td>
<td align="left" colspan="1" rowspan="1">73.0</td>
<td align="left" colspan="1" rowspan="1">73.0</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">hybrid <xref ref-type="bibr" rid="pcbi.1000837-Miwa2">[63]</xref></td>
<td align="left" colspan="1" rowspan="1">86.8</td>
<td align="left" colspan="1" rowspan="1">55.0</td>
<td align="left" colspan="1" rowspan="1">68.8</td>
<td align="left" colspan="1" rowspan="1">60.8</td>
<td align="left" colspan="1" rowspan="1">85.9</td>
<td align="left" colspan="1" rowspan="1">65.7</td>
<td align="left" colspan="1" rowspan="1">71.1</td>
<td align="left" colspan="1" rowspan="1">68.1</td>
<td align="left" colspan="1" rowspan="1">82.2</td>
<td align="left" colspan="1" rowspan="1">68.5</td>
<td align="left" colspan="1" rowspan="1">76.1</td>
<td align="left" colspan="1" rowspan="1">70.9</td>
<td align="left" colspan="1" rowspan="1">84.4</td>
<td align="left" colspan="1" rowspan="1">67.5</td>
<td align="left" colspan="1" rowspan="1">78.6</td>
<td align="left" colspan="1" rowspan="1">71.7</td>
<td align="left" colspan="1" rowspan="1">86.3</td>
<td align="left" colspan="1" rowspan="1">77.6</td>
<td align="left" colspan="1" rowspan="1">86.0</td>
<td align="left" colspan="1" rowspan="1">80.1</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">co-occ. <xref ref-type="bibr" rid="pcbi.1000837-Airola1">[17]</xref></td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">17.8</td>
<td align="left" colspan="1" rowspan="1">100.</td>
<td align="left" colspan="1" rowspan="1">30.1</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">26.6</td>
<td align="left" colspan="1" rowspan="1">100.</td>
<td align="left" colspan="1" rowspan="1">41.7</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">38.9</td>
<td align="left" colspan="1" rowspan="1">100.</td>
<td align="left" colspan="1" rowspan="1">55.4</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">40.8</td>
<td align="left" colspan="1" rowspan="1">100.</td>
<td align="left" colspan="1" rowspan="1">57.6</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">55.9</td>
<td align="left" colspan="1" rowspan="1">100.</td>
<td align="left" colspan="1" rowspan="1">70.3</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RelEx <xref ref-type="bibr" rid="pcbi.1000837-Pyysalo1">[36]</xref></td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">40.0</td>
<td align="left" colspan="1" rowspan="1">50.0</td>
<td align="left" colspan="1" rowspan="1">44.0</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">39.0</td>
<td align="left" colspan="1" rowspan="1">45.0</td>
<td align="left" colspan="1" rowspan="1">41.0</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">76.0</td>
<td align="left" colspan="1" rowspan="1">64.0</td>
<td align="left" colspan="1" rowspan="1">69.0</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">74.0</td>
<td align="left" colspan="1" rowspan="1">61.0</td>
<td align="left" colspan="1" rowspan="1">67.0</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">82.0</td>
<td align="left" colspan="1" rowspan="1">72.0</td>
<td align="left" colspan="1" rowspan="1">77.0</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt102"><label/><p>The first two blocks contain the results of our evaluation, the third block contains corresponding results of kernel approaches from the literature, and the third block shows some non-kernel-based baselines. Bold typeface shows our best results for a particular corpus (differences under 1 base point are ignored). AUC, precision, recall, and F<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e052" xlink:type="simple"/></inline-formula>-score in percent.</p></fn><fn id="nt103"><label/><p>† instance-level CV.</p></fn></table-wrap-foot></table-wrap>
<p><xref ref-type="table" rid="pcbi-1000837-t002">Table 2</xref> shows that we often could not reproduce results reported by the authors. However, we want to emphasize that our study is the first to provide an unbiased comparison of different methods where each method was presented exactly the same training and test data and where the same tuning procedures were used (see <xref ref-type="sec" rid="s2">Methods</xref>). The differences may have different reasons. First, evaluation strategies differ (different splits or document- vs. instance-level CV). Second, parameter tuning was different. Third, corpora were treated differently. We provide examples below.</p>
<p>In case of the AIMed corpus, there are different interpretations regarding the number of interacting and non-interacting pairs <xref ref-type="bibr" rid="pcbi.1000837-Stre1">[64]</xref>. The learning format we applied contains 1000 positive and 4834 negative examples (cf. <xref ref-type="table" rid="pcbi-1000837-t001">Table 1</xref>), while in <xref ref-type="bibr" rid="pcbi.1000837-Giuliano1">[23]</xref> (SL kernel) 8 more positive and 200 fewer negative examples are reported. If the entity blinding is performed only partially, that can also affect the performance of the learner. Using the same learning format as in our paper, with the shallow linguistic kernel of <xref ref-type="bibr" rid="pcbi.1000837-Giuliano1">[23]</xref> an F-score of 52.4% was achieved, which is actually somewhat worse than our result of 54.5%.</p>
<p>In case of the cosine and the edit kernels, the figures reported in the original paper were achieved with instance-level CV (personal communication, not mentioned in the original paper). As noted earlier in the literature <xref ref-type="bibr" rid="pcbi.1000837-Airola1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1000837-Stre1">[64]</xref>, this strategy increases F-score significantly (on AIMed by 18%) but relies on information leakage.</p>
<p>We account for smaller differences in F-score to the fact that we used different parameter optimization than in the original works. This is, for instance, the case for kBSPS (our own implementation) and APG. However, recall that parameter tuning always carries the danger of overfitting to the training data. The relative performance of different kernels in our results should be fairly robust due to the usage of the same tuning strategy for all kernels, while better results can be achieved by performing further corpus-specific tuning. Interestingly, for APG, we obtained better F-score and AUC values than the published ones for two of the five corpora.</p>
<p>Based on the results in <xref ref-type="table" rid="pcbi-1000837-t002">Table 2</xref>, we can roughly divide the kernels into three groups. Syntax-tree-based kernels (ST, SST, PT, SpT) oftentimes are just on par with the co-occurrence approach in terms of F-score. They are clearly better than co-occurrence only on BioInfer and IEPA. On the very small LLL, their results practically coincide with co-occurrence. The second group consists of cosine and edit. These two usually outperform co-occurrence (in some cases significantly), but their performance does not exceed the one of the rule-based RelEx method in terms of F-score. The cosine kernel on average delivers better F-scores, while the edit kernel gives higher AUC values. Both of the former groups are outperformed by APG, SL and kBSPS. The figures show that there is only an insignificant difference among APG and SL on the more important larger corpora (AIMed and BioInfer), while on the three smaller ones (HPRD50, IEPA, and LLL) SL has slightly lower scores when compared to APG and kBSPS kernels. Note that when APG is trained with SVM its AUC score drops below average (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e053" xlink:type="simple"/></inline-formula> on AIMed and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e054" xlink:type="simple"/></inline-formula> on BioInfer) while its F-score remains among the best. These three kernels clearly outperform the rule-based RelEx on AIMed and BioInfer, and are slightly better on average on the other corpora.</p>
</sec><sec id="s3b">
<title>Cross-Learning</title>
<p><xref ref-type="table" rid="pcbi-1000837-t003">Table 3</xref> and <xref ref-type="fig" rid="pcbi-1000837-g007">Figure 7</xref> show our results for CL performance. Because the training on the ensemble of four corpora generally takes much longer time, we computed results only for the fastest out of the four syntax-tree-based kernels (SpT), since all of them performed similarly low in the CV setting. This trend is confirmed, as SpT also here performs considerably worse than all other tested methods. We also looked for CL results in the literature. Beside the results of the combined kernel proposed in <xref ref-type="bibr" rid="pcbi.1000837-Kim1">[25]</xref> (numbers showed in the table are taken from <xref ref-type="bibr" rid="pcbi.1000837-Fayruzov1">[38]</xref>), the only one we could find were produced without the BioInfer corpus <xref ref-type="bibr" rid="pcbi.1000837-VanLandeghem1">[26]</xref>. This means that classifiers were trained only on three corpora. Since BioInfer contains the largest number of entity pairs, these numbers are not directly comparable to ours and therefore omitted.</p>
<fig id="pcbi-1000837-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.g007</object-id><label>Figure 7</label><caption>
<title>AUC, F-score, precision and recall values with CL evaluation.</title>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.g007" xlink:type="simple"/></fig><table-wrap id="pcbi-1000837-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.t003</object-id><label>Table 3</label><caption>
<title>Cross-learning results.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000837-t003-3" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.t003" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Kernel</td>
<td align="left" colspan="4" rowspan="1">AIMed</td>
<td align="left" colspan="4" rowspan="1">BioInfer</td>
<td align="left" colspan="4" rowspan="1">HPRD50</td>
<td align="left" colspan="4" rowspan="1">IEPA</td>
<td align="left" colspan="4" rowspan="1">LLL</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">SL</td>
<td align="left" colspan="1" rowspan="1"><bold>77.5</bold></td>
<td align="left" colspan="1" rowspan="1">28.3</td>
<td align="left" colspan="1" rowspan="1"><bold>86.6</bold></td>
<td align="left" colspan="1" rowspan="1">42.6</td>
<td align="left" colspan="1" rowspan="1"><bold>74.9</bold></td>
<td align="left" colspan="1" rowspan="1">62.8</td>
<td align="left" colspan="1" rowspan="1">36.5</td>
<td align="left" colspan="1" rowspan="1">46.2</td>
<td align="left" colspan="1" rowspan="1">78.0</td>
<td align="left" colspan="1" rowspan="1">56.9</td>
<td align="left" colspan="1" rowspan="1">68.7</td>
<td align="left" colspan="1" rowspan="1">62.2</td>
<td align="left" colspan="1" rowspan="1">75.6</td>
<td align="left" colspan="1" rowspan="1">71.0</td>
<td align="left" colspan="1" rowspan="1">52.5</td>
<td align="left" colspan="1" rowspan="1">60.4</td>
<td align="left" colspan="1" rowspan="1">79.5</td>
<td align="left" colspan="1" rowspan="1">79.0</td>
<td align="left" colspan="1" rowspan="1">57.3</td>
<td align="left" colspan="1" rowspan="1">66.4</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SpT</td>
<td align="left" colspan="1" rowspan="1">56.8</td>
<td align="left" colspan="1" rowspan="1">20.3</td>
<td align="left" colspan="1" rowspan="1">48.4</td>
<td align="left" colspan="1" rowspan="1">28.6</td>
<td align="left" colspan="1" rowspan="1">64.2</td>
<td align="left" colspan="1" rowspan="1">38.9</td>
<td align="left" colspan="1" rowspan="1"><bold>48.0</bold></td>
<td align="left" colspan="1" rowspan="1">43.0</td>
<td align="left" colspan="1" rowspan="1">60.4</td>
<td align="left" colspan="1" rowspan="1">44.7</td>
<td align="left" colspan="1" rowspan="1"><bold>77.3</bold></td>
<td align="left" colspan="1" rowspan="1">56.6</td>
<td align="left" colspan="1" rowspan="1">54.2</td>
<td align="left" colspan="1" rowspan="1">41.6</td>
<td align="left" colspan="1" rowspan="1">19.6</td>
<td align="left" colspan="1" rowspan="1">15.5</td>
<td align="left" colspan="1" rowspan="1">50.5</td>
<td align="left" colspan="1" rowspan="1">48.2</td>
<td align="left" colspan="1" rowspan="1"><bold>83.5</bold></td>
<td align="left" colspan="1" rowspan="1">61.2</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">kBSPS</td>
<td align="left" colspan="1" rowspan="1">72.1</td>
<td align="left" colspan="1" rowspan="1">28.6</td>
<td align="left" colspan="1" rowspan="1">68.0</td>
<td align="left" colspan="1" rowspan="1">40.3</td>
<td align="left" colspan="1" rowspan="1">73.3</td>
<td align="left" colspan="1" rowspan="1">62.2</td>
<td align="left" colspan="1" rowspan="1">38.5</td>
<td align="left" colspan="1" rowspan="1"><bold>47.6</bold></td>
<td align="left" colspan="1" rowspan="1">78.3</td>
<td align="left" colspan="1" rowspan="1">61.7</td>
<td align="left" colspan="1" rowspan="1">74.2</td>
<td align="left" colspan="1" rowspan="1">67.4</td>
<td align="left" colspan="1" rowspan="1">81.0</td>
<td align="left" colspan="1" rowspan="1">72.8</td>
<td align="left" colspan="1" rowspan="1"><bold>68.7</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>70.7</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>86.8</bold></td>
<td align="left" colspan="1" rowspan="1">83.7</td>
<td align="left" colspan="1" rowspan="1">75.0</td>
<td align="left" colspan="1" rowspan="1"><bold>79.1</bold></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">cosine</td>
<td align="left" colspan="1" rowspan="1">65.4</td>
<td align="left" colspan="1" rowspan="1">27.5</td>
<td align="left" colspan="1" rowspan="1">59.1</td>
<td align="left" colspan="1" rowspan="1">37.6</td>
<td align="left" colspan="1" rowspan="1">61.3</td>
<td align="left" colspan="1" rowspan="1">42.1</td>
<td align="left" colspan="1" rowspan="1">32.2</td>
<td align="left" colspan="1" rowspan="1">36.5</td>
<td align="left" colspan="1" rowspan="1">71.2</td>
<td align="left" colspan="1" rowspan="1">63.0</td>
<td align="left" colspan="1" rowspan="1">56.4</td>
<td align="left" colspan="1" rowspan="1">59.6</td>
<td align="left" colspan="1" rowspan="1">57.0</td>
<td align="left" colspan="1" rowspan="1">46.3</td>
<td align="left" colspan="1" rowspan="1">31.6</td>
<td align="left" colspan="1" rowspan="1">37.6</td>
<td align="left" colspan="1" rowspan="1">66.9</td>
<td align="left" colspan="1" rowspan="1">80.3</td>
<td align="left" colspan="1" rowspan="1">37.2</td>
<td align="left" colspan="1" rowspan="1">50.8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">edit</td>
<td align="left" colspan="1" rowspan="1">62.8</td>
<td align="left" colspan="1" rowspan="1">26.8</td>
<td align="left" colspan="1" rowspan="1">59.7</td>
<td align="left" colspan="1" rowspan="1">37.0</td>
<td align="left" colspan="1" rowspan="1">61.0</td>
<td align="left" colspan="1" rowspan="1">53.0</td>
<td align="left" colspan="1" rowspan="1">22.7</td>
<td align="left" colspan="1" rowspan="1">31.7</td>
<td align="left" colspan="1" rowspan="1">60.7</td>
<td align="left" colspan="1" rowspan="1">58.1</td>
<td align="left" colspan="1" rowspan="1">55.2</td>
<td align="left" colspan="1" rowspan="1">56.6</td>
<td align="left" colspan="1" rowspan="1">62.1</td>
<td align="left" colspan="1" rowspan="1">58.1</td>
<td align="left" colspan="1" rowspan="1">45.1</td>
<td align="left" colspan="1" rowspan="1">50.8</td>
<td align="left" colspan="1" rowspan="1">57.6</td>
<td align="left" colspan="1" rowspan="1">68.1</td>
<td align="left" colspan="1" rowspan="1">48.2</td>
<td align="left" colspan="1" rowspan="1">56.4</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">APG</td>
<td align="left" colspan="1" rowspan="1"><bold>77.6</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>30.5</bold></td>
<td align="left" colspan="1" rowspan="1">77.5</td>
<td align="left" colspan="1" rowspan="1"><bold>43.8</bold></td>
<td align="left" colspan="1" rowspan="1">69.6</td>
<td align="left" colspan="1" rowspan="1">58.1</td>
<td align="left" colspan="1" rowspan="1">29.4</td>
<td align="left" colspan="1" rowspan="1">39.1</td>
<td align="left" colspan="1" rowspan="1"><bold>84.0</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>64.2</bold></td>
<td align="left" colspan="1" rowspan="1">76.1</td>
<td align="left" colspan="1" rowspan="1"><bold>69.7</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>82.4</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>78.5</bold></td>
<td align="left" colspan="1" rowspan="1">48.1</td>
<td align="left" colspan="1" rowspan="1">59.6</td>
<td align="left" colspan="1" rowspan="1"><bold>86.5</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>86.4</bold></td>
<td align="left" colspan="1" rowspan="1">62.2</td>
<td align="left" colspan="1" rowspan="1">72.3</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Fayruzov <italic>et al.</italic></td>
<td align="left" colspan="1" rowspan="1">72.0</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">40.0</td>
<td align="left" colspan="1" rowspan="1">70.0</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">31.0</td>
<td align="left" colspan="1" rowspan="1">75.0</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">56.0</td>
<td align="left" colspan="1" rowspan="1">68.0</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">29.0</td>
<td align="left" colspan="1" rowspan="1">74.0</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">39.0</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt104"><label/><p>Classifiers are trained on the ensemble of four corpora and tested on the fifth one. Rows correspond to test corpora. Best results are typeset in bold (differences under 1 base point are ignored). We show for reference the results with the combined full kernel of <xref ref-type="bibr" rid="pcbi.1000837-Kim1">[25]</xref>, taken from <xref ref-type="bibr" rid="pcbi.1000837-Fayruzov1">[38]</xref>. AUC, precision, recall, and F<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e055" xlink:type="simple"/></inline-formula>-score in percent.</p></fn></table-wrap-foot></table-wrap>
<p>The overall trend from CV to CL confirms our expectation. Performance results drop significantly, sometimes by more than 15 points. The most stable is the kBSPS kernel (average drop AUC: 1.12, F: 2.84); in a few cases CL outperforms CV results (also seen with APG on HPRD). The SL and APG kernels show a modest drop in AUC (4.5 and 2.82), which gets larger by F-score (9.28 and 10.22). Cosine and edit suffer from the most significant drops.</p>
<p>We can form two groups of kernels based on their CL performance. The first consists of SpT, cosine, and edit—supposedly other syntax-tree-based kernels belong here as well. SpT is clearly the worst in this comparison. Two outlier corpora are BioInfer and IEPA: on the former SpT is on par with other kernels, while on the latter it achieves very low value due to the extremely low recall. Cosine and edit are just somewhat better than SpT, particularly on AIMed and IEPA. Their AUC scores are mostly just above 60%, and their F-scores outperform the co-occurrence methods only on AIMed. On IEPA and LLL, all three F-scores are inferior to the co-occurrence baseline.</p>
<p>The other group consists of SL, kBSPS, and APG kernels. The SL kernel produced the least divergent values across the five corpora in terms of both major evaluation measures. It shows performance comparable with the best kernels on the two larger corpora, but is somewhat inferior on the three smaller ones. The AUC values of our kBSPS kernel are improved with decreasing size of the test corpus, and are comparable on most corpora with the SL and APG kernel, except for AIMed (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000837.e056" xlink:type="simple"/></inline-formula>5%). For F-scores, the size dependent tendency is somewhat similar, but here the kBSPS kernel outperforms the other kernels on three corpora, with a remarkable margin of 8–10% on IEPA and LLL. APG results are comparable or better for AUC than the ones of kBSPS and SL kernel, except on BioInfer. It achieved the best F-score value on AIMed and HPRD50, but on the other three corpora its performance is clearly below kBSPS.</p>
<p>Finally, it is interesting to compare the performance of the better group with RelEx, the rule-based baseline (which requires no learning at all). We can see that on most corpora, only the best kernel-based method is comparable with RelEx, and except on BioInfer, the difference is a mere few percent.</p>
</sec><sec id="s3c">
<title>Cross-Corpus Evaluation</title>
<p><xref ref-type="table" rid="pcbi-1000837-t004">Table 4</xref> and <xref ref-type="fig" rid="pcbi-1000837-g008">Figure 8</xref> show cross-corpus results for classifiers trained on AIMed and BioInfer for some selected kernels. Results for all other kernels and for classifiers trained on HPRD50, IEPA and LLL can be found in <xref ref-type="supplementary-material" rid="pcbi.1000837.s005">Table S5</xref>.</p>
<fig id="pcbi-1000837-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.g008</object-id><label>Figure 8</label><caption>
<title>AUC, F-score, precision and recall values with CC evaluation trained on AIMed and BioInfer.</title>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.g008" xlink:type="simple"/></fig><table-wrap id="pcbi-1000837-t004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.t004</object-id><label>Table 4</label><caption>
<title>Cross-corpus results trained on AIMed and BioInfer.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000837-t004-4" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.t004" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Kernel</td>
<td align="left" colspan="1" rowspan="1">Training corpus</td>
<td align="left" colspan="4" rowspan="1">AIMed</td>
<td align="left" colspan="4" rowspan="1">BioInfer</td>
<td align="left" colspan="4" rowspan="1">HPRD50</td>
<td align="left" colspan="4" rowspan="1">IEPA</td>
<td align="left" colspan="4" rowspan="1">LLL</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">F</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">SL</td>
<td align="left" colspan="1" rowspan="1">AIMed</td>
<td align="left" colspan="1" rowspan="1">(83.5)</td>
<td align="left" colspan="1" rowspan="1">(47.5)</td>
<td align="left" colspan="1" rowspan="1">(65.5)</td>
<td align="left" colspan="1" rowspan="1">(54.5)</td>
<td align="left" colspan="1" rowspan="1"><bold>73.1</bold></td>
<td align="left" colspan="1" rowspan="1">66.8</td>
<td align="left" colspan="1" rowspan="1"><bold>29.2</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>40.6</bold></td>
<td align="left" colspan="1" rowspan="1">72.9</td>
<td align="left" colspan="1" rowspan="1">61.7</td>
<td align="left" colspan="1" rowspan="1">56.4</td>
<td align="left" colspan="1" rowspan="1">59.0</td>
<td align="left" colspan="1" rowspan="1">68.8</td>
<td align="left" colspan="1" rowspan="1">66.3</td>
<td align="left" colspan="1" rowspan="1">15.8</td>
<td align="left" colspan="1" rowspan="1">25.5</td>
<td align="left" colspan="1" rowspan="1">72.6</td>
<td align="left" colspan="1" rowspan="1">86.4</td>
<td align="left" colspan="1" rowspan="1">23.2</td>
<td align="left" colspan="1" rowspan="1">36.5</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">BioInfer</td>
<td align="left" colspan="1" rowspan="1"><bold>76.8</bold></td>
<td align="left" colspan="1" rowspan="1">27.2</td>
<td align="left" colspan="1" rowspan="1"><bold>87.1</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>41.5</bold></td>
<td align="left" colspan="1" rowspan="1">(81.1)</td>
<td align="left" colspan="1" rowspan="1">(55.1)</td>
<td align="left" colspan="1" rowspan="1">(66.5)</td>
<td align="left" colspan="1" rowspan="1">(60.0)</td>
<td align="left" colspan="1" rowspan="1">74.8</td>
<td align="left" colspan="1" rowspan="1">51.0</td>
<td align="left" colspan="1" rowspan="1">78.5</td>
<td align="left" colspan="1" rowspan="1">61.8</td>
<td align="left" colspan="1" rowspan="1">76.6</td>
<td align="left" colspan="1" rowspan="1">63.3</td>
<td align="left" colspan="1" rowspan="1">64.8</td>
<td align="left" colspan="1" rowspan="1">64.0</td>
<td align="left" colspan="1" rowspan="1">80.5</td>
<td align="left" colspan="1" rowspan="1">71.5</td>
<td align="left" colspan="1" rowspan="1">78.0</td>
<td align="left" colspan="1" rowspan="1">74.6</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SpT</td>
<td align="left" colspan="1" rowspan="1">AIMed</td>
<td align="left" colspan="1" rowspan="1">(66.1)</td>
<td align="left" colspan="1" rowspan="1">(33.0)</td>
<td align="left" colspan="1" rowspan="1">(25.5)</td>
<td align="left" colspan="1" rowspan="1">(27.3)</td>
<td align="left" colspan="1" rowspan="1">69.5</td>
<td align="left" colspan="1" rowspan="1">48.4</td>
<td align="left" colspan="1" rowspan="1">16.3</td>
<td align="left" colspan="1" rowspan="1">24.3</td>
<td align="left" colspan="1" rowspan="1">60.0</td>
<td align="left" colspan="1" rowspan="1">47.1</td>
<td align="left" colspan="1" rowspan="1">39.9</td>
<td align="left" colspan="1" rowspan="1">43.2</td>
<td align="left" colspan="1" rowspan="1">67.9</td>
<td align="left" colspan="1" rowspan="1">59.7</td>
<td align="left" colspan="1" rowspan="1">11.0</td>
<td align="left" colspan="1" rowspan="1">18.6</td>
<td align="left" colspan="1" rowspan="1">57.0</td>
<td align="left" colspan="1" rowspan="1">72.7</td>
<td align="left" colspan="1" rowspan="1">29.8</td>
<td align="left" colspan="1" rowspan="1">17.2</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">BioInfer</td>
<td align="left" colspan="1" rowspan="1">65.3</td>
<td align="left" colspan="1" rowspan="1">22.3</td>
<td align="left" colspan="1" rowspan="1">77.8</td>
<td align="left" colspan="1" rowspan="1">34.7</td>
<td align="left" colspan="1" rowspan="1">(74.1)</td>
<td align="left" colspan="1" rowspan="1">(44.0)</td>
<td align="left" colspan="1" rowspan="1">(68.2)</td>
<td align="left" colspan="1" rowspan="1">(53.4)</td>
<td align="left" colspan="1" rowspan="1">57.2</td>
<td align="left" colspan="1" rowspan="1">41.4</td>
<td align="left" colspan="1" rowspan="1">67.5</td>
<td align="left" colspan="1" rowspan="1">51.3</td>
<td align="left" colspan="1" rowspan="1">69.9</td>
<td align="left" colspan="1" rowspan="1">61.2</td>
<td align="left" colspan="1" rowspan="1">52.2</td>
<td align="left" colspan="1" rowspan="1">56.4</td>
<td align="left" colspan="1" rowspan="1">55.7</td>
<td align="left" colspan="1" rowspan="1">54.2</td>
<td align="left" colspan="1" rowspan="1">62.8</td>
<td align="left" colspan="1" rowspan="1">58.2</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">kBSPS</td>
<td align="left" colspan="1" rowspan="1">AIMed</td>
<td align="left" colspan="1" rowspan="1">(75.1)</td>
<td align="left" colspan="1" rowspan="1">(50.1)</td>
<td align="left" colspan="1" rowspan="1">(41.4)</td>
<td align="left" colspan="1" rowspan="1">(44.6)</td>
<td align="left" colspan="1" rowspan="1">69.9</td>
<td align="left" colspan="1" rowspan="1">71.6</td>
<td align="left" colspan="1" rowspan="1">15.0</td>
<td align="left" colspan="1" rowspan="1">24.8</td>
<td align="left" colspan="1" rowspan="1">76.8</td>
<td align="left" colspan="1" rowspan="1">77.5</td>
<td align="left" colspan="1" rowspan="1">38.0</td>
<td align="left" colspan="1" rowspan="1">51.0</td>
<td align="left" colspan="1" rowspan="1">73.6</td>
<td align="left" colspan="1" rowspan="1">66.7</td>
<td align="left" colspan="1" rowspan="1">25.4</td>
<td align="left" colspan="1" rowspan="1">29.9</td>
<td align="left" colspan="1" rowspan="1">75.1</td>
<td align="left" colspan="1" rowspan="1">85.7</td>
<td align="left" colspan="1" rowspan="1">27.3</td>
<td align="left" colspan="1" rowspan="1">13.5</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">BioInfer</td>
<td align="left" colspan="1" rowspan="1">71.8</td>
<td align="left" colspan="1" rowspan="1"><bold>29.1</bold></td>
<td align="left" colspan="1" rowspan="1">65.6</td>
<td align="left" colspan="1" rowspan="1">40.3</td>
<td align="left" colspan="1" rowspan="1">(75.2)</td>
<td align="left" colspan="1" rowspan="1">(49.9)</td>
<td align="left" colspan="1" rowspan="1">(61.8)</td>
<td align="left" colspan="1" rowspan="1">(55.1)</td>
<td align="left" colspan="1" rowspan="1"><bold>77.7</bold></td>
<td align="left" colspan="1" rowspan="1">61.0</td>
<td align="left" colspan="1" rowspan="1">81.6</td>
<td align="left" colspan="1" rowspan="1"><bold>69.8</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>81.5</bold></td>
<td align="left" colspan="1" rowspan="1">67.4</td>
<td align="left" colspan="1" rowspan="1">78.2</td>
<td align="left" colspan="1" rowspan="1"><bold>72.4</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>85.1</bold></td>
<td align="left" colspan="1" rowspan="1">76.8</td>
<td align="left" colspan="1" rowspan="1"><bold>84.8</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>80.6</bold></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">edit</td>
<td align="left" colspan="1" rowspan="1">AIMed</td>
<td align="left" colspan="1" rowspan="1">(75.2)</td>
<td align="left" colspan="1" rowspan="1">(68.8)</td>
<td align="left" colspan="1" rowspan="1">(27.7)</td>
<td align="left" colspan="1" rowspan="1">(39.0)</td>
<td align="left" colspan="1" rowspan="1">67.5</td>
<td align="left" colspan="1" rowspan="1"><bold>86.4</bold></td>
<td align="left" colspan="1" rowspan="1">28.8</td>
<td align="left" colspan="1" rowspan="1">15.9</td>
<td align="left" colspan="1" rowspan="1"><bold>78.1</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>87.0</bold></td>
<td align="left" colspan="1" rowspan="1">24.5</td>
<td align="left" colspan="1" rowspan="1">38.3</td>
<td align="left" colspan="1" rowspan="1">71.1</td>
<td align="left" colspan="1" rowspan="1"><bold>92.9</bold></td>
<td align="left" colspan="1" rowspan="1">23.9</td>
<td align="left" colspan="1" rowspan="1">27.5</td>
<td align="left" colspan="1" rowspan="1">73.2</td>
<td align="left" colspan="1" rowspan="1">75.0</td>
<td align="left" colspan="1" rowspan="1">21.8</td>
<td align="left" colspan="1" rowspan="1">3.6</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">BioInfer</td>
<td align="left" colspan="1" rowspan="1">66.9</td>
<td align="left" colspan="1" rowspan="1"><bold>30.0</bold></td>
<td align="left" colspan="1" rowspan="1">58.4</td>
<td align="left" colspan="1" rowspan="1">39.6</td>
<td align="left" colspan="1" rowspan="1">(67.4)</td>
<td align="left" colspan="1" rowspan="1">(50.4)</td>
<td align="left" colspan="1" rowspan="1">(39.2)</td>
<td align="left" colspan="1" rowspan="1">(43.8)</td>
<td align="left" colspan="1" rowspan="1">72.7</td>
<td align="left" colspan="1" rowspan="1">59.4</td>
<td align="left" colspan="1" rowspan="1">65.6</td>
<td align="left" colspan="1" rowspan="1">62.4</td>
<td align="left" colspan="1" rowspan="1">69.3</td>
<td align="left" colspan="1" rowspan="1">61.1</td>
<td align="left" colspan="1" rowspan="1">55.8</td>
<td align="left" colspan="1" rowspan="1">58.4</td>
<td align="left" colspan="1" rowspan="1">66.9</td>
<td align="left" colspan="1" rowspan="1">69.0</td>
<td align="left" colspan="1" rowspan="1">54.3</td>
<td align="left" colspan="1" rowspan="1">60.8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">APG</td>
<td align="left" colspan="1" rowspan="1">AIMed</td>
<td align="left" colspan="1" rowspan="1">(84.6)</td>
<td align="left" colspan="1" rowspan="1">(59.9)</td>
<td align="left" colspan="1" rowspan="1">(53.6)</td>
<td align="left" colspan="1" rowspan="1">(56.2)</td>
<td align="left" colspan="1" rowspan="1">66.0</td>
<td align="left" colspan="1" rowspan="1">56.5</td>
<td align="left" colspan="1" rowspan="1">14.0</td>
<td align="left" colspan="1" rowspan="1">22.5</td>
<td align="left" colspan="1" rowspan="1"><bold>77.7</bold></td>
<td align="left" colspan="1" rowspan="1">74.1</td>
<td align="left" colspan="1" rowspan="1">52.8</td>
<td align="left" colspan="1" rowspan="1">61.6</td>
<td align="left" colspan="1" rowspan="1">73.1</td>
<td align="left" colspan="1" rowspan="1">69.2</td>
<td align="left" colspan="1" rowspan="1">13.4</td>
<td align="left" colspan="1" rowspan="1">22.5</td>
<td align="left" colspan="1" rowspan="1">82.7</td>
<td align="left" colspan="1" rowspan="1"><bold>88.9</bold></td>
<td align="left" colspan="1" rowspan="1">29.8</td>
<td align="left" colspan="1" rowspan="1">17.6</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">BioInfer</td>
<td align="left" colspan="1" rowspan="1">71.2</td>
<td align="left" colspan="1" rowspan="1">24.7</td>
<td align="left" colspan="1" rowspan="1">81.8</td>
<td align="left" colspan="1" rowspan="1">37.9</td>
<td align="left" colspan="1" rowspan="1">(81.5)</td>
<td align="left" colspan="1" rowspan="1">(60.2)</td>
<td align="left" colspan="1" rowspan="1">(61.3)</td>
<td align="left" colspan="1" rowspan="1">(60.7)</td>
<td align="left" colspan="1" rowspan="1">76.0</td>
<td align="left" colspan="1" rowspan="1">49.3</td>
<td align="left" colspan="1" rowspan="1"><bold>84.0</bold></td>
<td align="left" colspan="1" rowspan="1">62.1</td>
<td align="left" colspan="1" rowspan="1"><bold>81.4</bold></td>
<td align="left" colspan="1" rowspan="1">61.7</td>
<td align="left" colspan="1" rowspan="1"><bold>82.7</bold></td>
<td align="left" colspan="1" rowspan="1">70.7</td>
<td align="left" colspan="1" rowspan="1">82.0</td>
<td align="left" colspan="1" rowspan="1">69.0</td>
<td align="left" colspan="1" rowspan="1"><bold>85.4</bold></td>
<td align="left" colspan="1" rowspan="1">76.3</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt105"><label/><p>CC results trained on the 3 smaller corpora are shown in the Supplement, <xref ref-type="supplementary-material" rid="pcbi.1000837.s005">Table S5</xref>. Classifiers are trained on one corpus and tested on the other four corpora. Rows correspond to the training corpora and columns to test corpora. For reference, cross-validated results are shown in parentheses. Bold typeface highlights overall best results per corpus (differences under 1 base point are ignored).</p></fn></table-wrap-foot></table-wrap>
<p>Overall, our expectation that average CC performance would be worse than CL performance because of the smaller size of training data was in general not confirmed. On the one hand, the average performance measured across all four possible training corpora drops for SL, kBSPS, and APG kernels (the magnitude of the drop increases in this order), while it increases for SpT and edit, so the difference between the performance of these groups shrinks. On the other hand, the average CC F-score belonging to the best training corpus is somewhat better than the average CL F-score also for SL, kBSPS and APG, while AUC decreases slightly.</p>
<p>The CC results show large performance differences for most kernels depending on the training corpus. From cross-corpus evaluation, we can estimate which corpora is the best resource from a generalization perspective. We rank each training corpus for each kernel and average these numbers to obtain an overall rank (<xref ref-type="supplementary-material" rid="pcbi.1000837.s006">Table S6</xref>, <xref ref-type="fig" rid="pcbi-1000837-g009">Figure 9</xref>). This ranking only roughly reflects the size of the corpora. BioInfer, containing the most PPI pairs, gives the best performance with most kernels, and its overall rank calculated over the five kernels is 2.1 (AUC) and 1.3 (F). Surprisingly, systems trained on IEPA perform on average quite well, though IEPA is an order of magnitude smaller than AIMed or BioInfer. In contrast, AIMed is, despite its size, only the third best corpus in terms of AUC and by far the worst for F-score. This does not mean that AIMed is a bad choice for training, but only that differs from the other corpora: the ratio of positive/negative examples is the smallest, and it has the largest fraction of sentences with no interactions.</p>
<fig id="pcbi-1000837-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.g009</object-id><label>Figure 9</label><caption>
<title>Overall ranking of the 5 corpora from the generality perspective in terms of the main performance measures based on the CL evaluation.</title>
<p>The ranking are calculated as the average of rankings on the 5 selected kernels (see <xref ref-type="supplementary-material" rid="pcbi.1000837.s006">Table S6</xref>).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.g009" xlink:type="simple"/></fig></sec></sec><sec id="s4">
<title>Discussion</title>
<p>We performed a systematic benchmark of nine different methods for the extraction of protein-protein-interactions from text using five different evaluation corpora. All figures we report were produced using locally installed, trained, and tuned systems (the packages are available in the online appendix). In almost all cases, our results in cross-validation are well in-line with those published in the respective original papers; only in some cases we observed differences larger than 2%, and those could be attributed to different evaluation methods and different tuning procedures (see <xref ref-type="sec" rid="s3">Results</xref>, Cross-validation). In contrast to cross-validation, our results regarding cross-learning and cross-corpus settings mostly cannot be compared to those of others as such numbers do not exist.</p>
<sec id="s4a">
<title>Relative Performance of Kernels</title>
<p>Taking all our results into account (summarized in <xref ref-type="table" rid="pcbi-1000837-t005">Table 5</xref>), we can safely state that APG, SL and kBSPS kernels are superior to the other methods we tested. APG provides on average the best AUC scores over all experiments when trained with the AUC-optimized sparse RLS. Its AUC scores drop significantly when trained with SVM. There is only one experiment where APG-RLS is outperformed by another method by a clear margin (CL on BioInfer). SL and kBSPS are on par in CL evaluation, while SL is slightly more accurate at CV. The ranking of kernels based on F-score is more diverse. At the more important CL evaluation, the clear advantage of APG observed at CV vanishes against kBSPS. Similarly, SL produces significantly better F-score at CV than at CL evaluation. Only these top-3 performing kernels outperform the rule-based RelEx approach (recall that RelEx's classification model is corpus-independent and thus can be used as baseline in all evaluation settings), at least in CV evaluation. When the more realistic CL evaluation is used, the best methods only just reach or marginally overcome RelEx's accuracy.</p>
<table-wrap id="pcbi-1000837-t005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.t005</object-id><label>Table 5</label><caption>
<title>Comparison of CV, CL, and CC results of selected kernels.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000837-t005-5" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.t005" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="2" rowspan="1">AIMed</td>
<td align="left" colspan="2" rowspan="1">BioInfer</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Kernel</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">AUC</td>
<td align="left" colspan="1" rowspan="1">F</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">CV/CL/CC</td>
<td align="left" colspan="1" rowspan="1">CV/CL/CC</td>
<td align="left" colspan="1" rowspan="1">CV/CL/CC</td>
<td align="left" colspan="1" rowspan="1">CV/CL/CC</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">SL</td>
<td align="left" colspan="1" rowspan="1">83.5/<bold>77.5</bold>/<bold>76.8</bold></td>
<td align="left" colspan="1" rowspan="1">54.5/42.6/<bold>41.5</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>81.1</bold>/<bold>74.9</bold>/<bold>73.1</bold></td>
<td align="left" colspan="1" rowspan="1"><bold>60.0</bold>/46.2/<bold>40.6</bold></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SpT</td>
<td align="left" colspan="1" rowspan="1">66.1/56.8/65.3</td>
<td align="left" colspan="1" rowspan="1">27.3/28.6/34.7</td>
<td align="left" colspan="1" rowspan="1">74.1/64.2/69.5</td>
<td align="left" colspan="1" rowspan="1">53.4/43.0/24.3</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">kBSPS</td>
<td align="left" colspan="1" rowspan="1">75.1/72.1/71.8</td>
<td align="left" colspan="1" rowspan="1">44.6/40.3/40.3</td>
<td align="left" colspan="1" rowspan="1">75.2/73.3/69.9</td>
<td align="left" colspan="1" rowspan="1">55.1/<bold>47.6</bold>/24.8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">edit</td>
<td align="left" colspan="1" rowspan="1">75.2/62.8/66.9</td>
<td align="left" colspan="1" rowspan="1">39.0/37.0/39.6</td>
<td align="left" colspan="1" rowspan="1">67.4/61.0/67.5</td>
<td align="left" colspan="1" rowspan="1">43.8/31.7/15.9</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">APG</td>
<td align="left" colspan="1" rowspan="1"><bold>84.6</bold>/<bold>77.6</bold>/71.2</td>
<td align="left" colspan="1" rowspan="1"><bold>56.2</bold>/<bold>43.8</bold>/37.9</td>
<td align="left" colspan="1" rowspan="1"><bold>81.5</bold>/69.6/66.0</td>
<td align="left" colspan="1" rowspan="1"><bold>60.7</bold>/39.1/22.5</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt106"><label/><p>CC results for AIMed (resp. BioInfer) are obtained with classifier trained on BioInfer (resp. AIMed).</p></fn></table-wrap-foot></table-wrap>
<p>The performance of the other six kernels is clearly weaker. Kernels using syntax trees are on par with simple co-occurrence for CV, and their performance decreases drastically at CL evaluation. Cosine and edit kernels are slightly better than co-occurrence in CV, but their performance also drops significantly in CL evaluation.</p>
<p><xref ref-type="table" rid="pcbi-1000837-t005">Table 5</xref> clearly shows that performance drops considerably for all kernels from CV to CL (see also <xref ref-type="fig" rid="pcbi-1000837-g010">Figure 10</xref>). There also is strong tendency to a worse performance when switching from CL to CC in terms of F-score, but this tendency has many exceptions for AUC. The general decrease in performance can be attributed both to overlearning on the training set (in other words missing generalization capability) and to the significant differences among corpus characteristics. The magnitude of the decrease varies by kernels (CV to CL): From the top-3 kernels, kBSPS has the least decline, while SL shows the largest drop. APG has particularly low scores on the BioInfer corpus compared to other experiments. This corpus exhibits the average largest drop from CV to CL, which can be explained by the fact that it has the largest number of PPI pairs and that a remarkable portion of those pairs is uncovered by the patterns of other corpora.</p>
<fig id="pcbi-1000837-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000837.g010</object-id><label>Figure 10</label><caption>
<title>Performance comparison of SL, SpT, kBSPS, edit and APG kernels across CV, CL and CC evaluations.</title>
<p>AUC and F-score values on AIMed and BioInfer. CC values are obtained with training on the other large corpus, though, eventually training on a smaller corpora may yield better results.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.g010" xlink:type="simple"/></fig></sec><sec id="s4b">
<title>Diversity of Corpora</title>
<p>The performance of machine-learning methods to PPI extraction largely depends on the specific relationship between the training data and the data the method is used on later (test data). If these two data sets exhibit large differences, then evaluation results obtained using only the training data will be much different than those obtained when using the trained model on the test data. Differences can be, among other, the style of writing, the frequency of certain linguistic phenomena, or the level of technical detail in the texts. For the case of PPI, important differences are the ratio between sentences containing a PPI and those that do not, or the implicit understanding of what actually is a PPI—this might, for instance, include or exclude temporary interactions, protein transport, functional association only hinting, yet not proving a physical contact etc; see <xref ref-type="bibr" rid="pcbi.1000837-Pyysalo1">[36, Table 1]</xref> for more details.</p>
<p>Our experiments in CL and CC setting show, in accordance with results obtained by others <xref ref-type="bibr" rid="pcbi.1000837-Pyysalo1">[36]</xref>, that the five corpora used for evaluation indeed have different characteristics. The main source of differences stems from the different ratio of positive pairs to negative pairs. The AIMed corpus has the largest fraction of negative sentences; accordingly, models trained on this corpus are more conservative in predicting PPIs, which leads to a lower recall when those models are applied on corpora with a smaller fraction of negative sentences. Both CL and CC evaluation clearly confirm this behavior, giving the AIMed corpus a bit of an outsider status. To further test this hypothesis, we repeated the CL experiment discarding AIMed from the learning pool. This leads to a significant increase in average F-measures (see details in Supplement; <xref ref-type="supplementary-material" rid="pcbi.1000837.s011">Text S2</xref> and <xref ref-type="supplementary-material" rid="pcbi.1000837.s007">Table S7</xref>), confirming the special role of AIMed.</p>
<p>However, also the other corpora are not homogeneous. This becomes especially clear when comparing CV results with those from CL and CC evaluations. As explained before, in CV all characteristics of the test corpus are also present in the training corpus and are thus learned by the algorithms; in contrast, in CL and CC this is not the case. The relatively large differences in the obtained performance measures indicate that different corpora have notably different characteristics. As any new texts that PPI extraction algorithms would be applied on would have unknown characteristics, we conclude that only the performance we measured for CL and CC can be expected on such texts.</p>
</sec><sec id="s4c">
<title>Shallow Parsing versus Syntax Trees versus Dependency Graphs</title>
<p>We evaluated kernels based on shallow linguistic features, syntax tree, dependency graph, and mixtures of these three. Our results clearly show that syntax trees are less useful than the other representations. Recall that syntax trees contain no explicit information about the semantic relations of connected nodes, which apparently is crucial in a relation extraction task. For the other types of data, the picture is less clear.</p>
<p>Several authors claimed that using more types of information yields better performance <xref ref-type="bibr" rid="pcbi.1000837-Miyao1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1000837-Miwa2">[63]</xref>. Our experiments only partially confirm this claim. In cross-validation results, the APG kernel, which combines multiple sources of sentence information, shows the best performance among all analyzed kernels in terms of both AUC (only with RLS) and F-score. However, this advantage shrinks (AUC) or vanishes (F-score) for cross-corpus and cross-learning evaluation when compared to the pure dependency graph-based kBSPS. We conclude that using more information in first place helps in becoming more corpus-specific. However, this situation might be different if larger training corpora were available.</p>
<p>In contrast to APG and kBSPS kernel, the SL kernel does not use any deep parse information. Nevertheless, it produces results comparable with APG and better than kBSPS for cross-validation. Its superiority over kBSPS vanishes for cross-learning, however. This change may be attributed to the decreasing usefulness of shallow linguistic features—including word sequences—when the model is trained on a more heterogeneous corpus.</p>
<p>Our results also show that the descriptive power of dependency graph parses can only be exploited when combined with an appropriate kernel. Cosine and edit kernels are unable to efficiently capture the features from dependency graphs. In case of the former, the shortcoming may be accounted to the fact that cosine does not take the word order into account. The handicap of the latter can be explained by weighting scheme applied at path distance calculation: its uniform, grammar-independent weighting disregards grammatical rules and structures, and thus the semantics of the underlying text.</p>
</sec><sec id="s4d">
<title>AUC or F-Measure?</title>
<p>Recently, some authors criticized the F-score as performance measure, because it is very sensitive to the ratio of positive/negative pairs in the corpus <xref ref-type="bibr" rid="pcbi.1000837-Airola1">[17]</xref>, , and it is less stable to parameter modifications than AUC. Our experiments confirm both statements. The standard deviation of AUC in CV across the five corpora ranges between 1.34 and 7.45 (F-score: 8.36–24.04). The SL and APG kernels are the most stable ones, while SpT and edit kernels belong to the other extreme in terms of both measures. <xref ref-type="fig" rid="pcbi-1000837-g006">Figure 6</xref> depicts the robustness on corpus level for cross-validation. We can observe that the larger the corpus the smaller the standard deviation, independently from the applied kernel.</p>
<p>On the other hand, one must keep in mind that AUC is a statement about the general capabilities of a PPI extraction method that must not be confused with its expected performance on a concrete problem. For a concrete task, a concrete set of parameters has to be chosen, while AUC expresses a measure over a range of parameter settings. When a user wants to analyze a set of documents, one probably can safely advise her to prefer kernels with higher average AUC measure, but the achieved performance will depend very much on the concrete parameters chosen. We also show via the APG-SVM experiment that the AUC score depends very much on the learning algorithm of the classifier, and only partially on the kernel. Therefore, the (less stable) F-score actually gives a better picture on the expected performance on new texts.</p>
</sec><sec id="s4e">
<title>Robustness against Parameter Setting</title>
<p>We investigated the robustness of the different kernels against parameter settings. To this end, we performed exhaustive, fine-grained parameter optimization for selected tasks and measured the difference to the parameter setting used in the benchmark. The resulting picture is quite heterogeneous.</p>
<p>SL kernel in principle has a number of parameters, but the implementation we were provided with from the authors always uses a default setting (which yields sound results). Therefore, we could not test robustness of SL in terms of parameter settings.</p>
<p>When using task-specific parameter tuning at CV for syntax-tree-based kernels, an improvement of 3 (5) points can be achieved on AUC (F-score). The magnitude of improvement is larger on CL, but the figures remain low. On the other hand, with improper parameter setting, the F-score may drop drastically, even to 0. Overall, syntax-tree-based kernels behave very sensitive to parameter setting.</p>
<p>A fine-grained parameter tuning improves kBSPS results only insignificantly (1–3 points of improvement both AUC and F). A similar small drop can be observed at CV evaluation if the parameters are selected improperly, while at CL evaluation the drop gets larger and reaches 10–15 points F-score. Consequently, we can state that kBSPS is fairly robust to parameter selection.</p>
<p>Cosine and edit show significantly better (high 60s/low 70s) AUC values with task-specific parameter tuning at CL evaluation, but those settings cause a dramatic F-score decrease (cosine: 20–25, edit 6–12 points). At CV evaluation, the trend is similar, but the extent of changes is smaller. As a summary, cosine and edit also should be considered as sensitive to parameter settings.</p>
<p>The performance of APG hardly changes (1–2 points) if the parameters are set differently (CV). The F-score drop is somewhat larger at CL. On the other hand, a major F-score drop can be observed when the threshold parameter is not optimized. When trained with SVM, APG becomes even more sensitive to the right selection of parameters.</p>
</sec><sec id="s4f">
<title>Classification Time</title>
<p>The runtime of a kernel-based method has two main components. First, the linguistic structures have to be generated. Previous experiments show <xref ref-type="bibr" rid="pcbi.1000837-Miyao1">[37, Table 2]</xref> that dependency parsers can be about an order of magnitude faster than syntax parsers and shallow parsing is about 1.5 order of magnitude faster than dependency parsing (see <xref ref-type="bibr" rid="pcbi.1000837-Ravichandran1">[65, Table 2]</xref>). Second, the substructures used by the kernels have to be determined and the classifier has to be applied.</p>
<p>We give an overview of the theoretical complexity of each kernel in the Supporting Information (<xref ref-type="supplementary-material" rid="pcbi.1000837.s012">Text S3</xref>). Actual runtimes are probably more interesting, as the complexity of an algorithm can be distorted to a large degree by the quality of its implementation. We show in <xref ref-type="supplementary-material" rid="pcbi.1000837.s009">Table S9</xref> averaged training and test times for each corpus for CV settings. Note that these figures do not contain the time it takes to parse a sentence; thus, real runtimes would be much higher for all kernels except SL. The APG with its cubic complexity clearly has the longest training time, but the classifier is fast. PT kernel generates the most syntax tree substructures and is an order of magnitude slower both in terms of training and classification time. We can also see that kernels with linear complexity exhibit very different runtimes. Among them kBSPS clearly is the fastest both at training and classification.</p>
<p>Runtime is a strong argument when it comes to the application of a PPI extraction method on large corpora. Consider the top-3 kernels AGP, SL, and kBSPS. When applied to all of Medline with its approximately 120M sentences, one would expect runtimes of 45, 141, and 4 days, respectively, on a single processor and I/O stream. Taking also into account the computation of shallow parses and dependency trees (on average 4 ms and 130 ms per sentence, respectively), times change to 226, 147, and 185 days, thus the formerly existing large differences almost vanish. Clearly, the exact times depend on the hardware that is used, but the ratios should stay roughly the same. The figures imply that an application of kernel based methods to PPI extraction on large corpora is only possible if considerable computational resources are available.</p>
</sec><sec id="s4g">
<title>Summary Kernel-by-Kernel</title>
<p>The SL kernel uses only shallow linguistic information plus the usual bag-of-word features. Taking parse time into account, this kernel is the fastest among all we tested. Despite its simplicity, its performance is remarkable. It is on par with the best kernels in most of the evaluation settings, and yields particularly good results in CV — all with default parameter settings. Furthermore, its performance is the most robust on the two larger corpora across CV, CL and CC evaluation in terms of both AUC and F-score.</p>
<p>Syntax-tree-based kernels (ST, SST, PT, SpT) fail to achieve comparative performance. Their performance hardly reaches the baseline even at CV evaluation. They are also very sensitive to the parameter setting and have a long runtime. Results are very sensitive to the particular training/test corpora and therefore cannot be extrapolated safely to new texts.</p>
<p>The kBSPS kernel achieves an overall very good performance, particularly in the more important CL and CC evaluations. Its performance decreases the least when CL evaluation is used instead of CV. It is very robust against parameter settings and achieves very good results with default parameters. Furthermore, it is by far the fastest kernel among all that use rich linguistic information.</p>
<p>Cosine and edit kernels, though using dependency trees, show significantly worse performance than the top-3 kernels. They are also very sensitive to the parameter settings. Their runtime is the double compared to other dependency tree kernels. In <xref ref-type="bibr" rid="pcbi.1000837-Erkan1">[22]</xref> Erkan proposed to train these kernels with transductive SVM <xref ref-type="bibr" rid="pcbi.1000837-Joachims2">[66]</xref>, however the performance gain is dubious (see <xref ref-type="supplementary-material" rid="pcbi.1000837.s008">Table S8</xref>), while increases tremendously the training time.</p>
<p>APG shows the best performance at CV setting, but its superiority vanishes on the more important CL and CC settings. It uses a different learner than other kernels, which optimizes for AUC. Consequently, its AUC results are the best, but its F-score values are also good (CV, and partly CL). Recall when APG is trained with SVM its AUC performance drops significantly compared to APG-RLS. This reflects the fact that RLS specifically optimizes for AUC; in turn, one can expect other kernels to also obtain better results when RLS learning would be applied. APG is rather sensitive to evaluation settings, where is exhibits the largest drop among top-3 kernels. It is robust to parameter settings except the threshold for the RLS procedure, but becomes very sensitive to parameters when trained with SVM. The classification is pretty fast, but with the necessary preprocessing, it becomes the slowest of the top-3 kernels.</p>
</sec><sec id="s4h">
<title>Conclusion</title>
<p>We investigated nine kernel-based methods for the extraction of PPIs from scientific texts. We studied how these methods behave in different evaluation settings, using different parameter sets, and on different gold standard corpora. We showed that even the best performing kernels, requiring extensive parameter optimizations and large training corpora, cannot be considered as significantly better than a simple rule-based method which does not need any training at all and has essentially no parameters to tune. We also showed that the characteristic features of PPIs can be extracted much more efficiently by kernels based on dependency tree parses than by those based on syntax tree parses. Interestingly, the SL kernel, using only shallow linguistic analysis, is almost as good as the best dependency-based kernels. We pointed out that the advantage of APG kernel, using multiple representations as features, vanishes in a realistic evaluation scenario when compared to the simpler kBSPS and SL kernels.</p>
<p>The ultimate goal of this study was to select the best PPI extraction method for real applications and to generate performance estimates for this method (and others) on new text. We state that this goal was not achieved for mostly two reasons. First, the performance of the methods we studied is very sensitive to parameter settings, evaluation method, and evaluation corpus. Best scores are only achieved when settings are optimized against a gold standard kernel—something that is not possible on unseen text. Our results reveal that some methods apparently are better than others, but a clear-cut winner is not detectable given the bandwidth of results. Second, the heterogeneity between corpora leads to extremely heterogeneous evaluation results, showing that all methods strongly adapt to the training set, and that, in turn, the existing training corpora are not large or not general enough to capture the characteristics of the respective other corpora. This implies that any extrapolation of the observed scores (AUC or F-score) to unseen texts is questionable.</p>
<p>We believe that these findings call for a number of actions. First, there is a strong need to create larger and better characterized evaluation corpora. Second, we think that there is also a need to complement the currently predominant approach, treating all interactions as equally important, with more specific extraction tasks. To this end, it is important to create specialized corpora, such as those for the extraction of regulation events or for protein complex formation. The more specific a question is, the simpler it is to create representative corpora, leading to better models, often higher extraction performance and better comparability of methods. For instance, works like <xref ref-type="bibr" rid="pcbi.1000837-Saric1">[67]</xref> on extraction of gene regulation or <xref ref-type="bibr" rid="pcbi.1000837-Hu1">[68]</xref> on extraction of phosphorylation events report much higher accuracies than those current achievable in the general PPI task. Third, there is a severe lack in studies measuring real-life performance of PPI extraction methods, circumventing the usage of gold standards by, for instance, user surveys with biological experts. Last but not least, our result also show that rule-based methods still make an excellent stand when compared to machine-learning based approaches as soon as specific evaluation settings are left behind.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1000837.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.s001" xlink:type="simple"><label>Table S1</label><caption>
<p>Overview of the evaluated kernels. Overview of the nine kernels evaluated in the paper.</p>
<p>(0.07 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000837.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.s002" xlink:type="simple"><label>Table S2</label><caption>
<p>Other kernels considered. Overview of other kernel based methods in the literature that we did not tested in the paper.</p>
<p>(0.06 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000837.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.s003" xlink:type="simple"><label>Table S3</label><caption>
<p>Overview of the usability of the different kernels. Some details on the nine evaluated kernels: availability of the algorithm and documentation, type of learning software.</p>
<p>(0.06 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000837.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.s004" xlink:type="simple"><label>Table S4</label><caption>
<p>Overview of our parameter selection strategy. Overview of our parameter selection strategy used in the paper. We provide a coarse description of parameter ranges and best parameters for each kernel and evaluation setting.</p>
<p>(0.07 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000837.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.s005" xlink:type="simple"><label>Table S5</label><caption>
<p>Cross-corpus results. Full table of cross-corpus results trained on all 5 corpora and evaluated on all nine kernels.</p>
<p>(0.08 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000837.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.s006" xlink:type="simple"><label>Table S6</label><caption>
<p>Ranking of corpora at CC evaluation based on their AUC and F-score values. We ranked the corpora from the generality perspective, i.e. how general the systems are trained on specific corpora. The evaluation is based on their AUC and F-score values at CC evaluation.</p>
<p>(0.07 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000837.s007" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.s007" xlink:type="simple"><label>Table S7</label><caption>
<p>Cross-learning experiments with some selected kernel performed on 4 corpora (all but AIMed). Cross-learning experiments with some selected kernel performed on 4 corpora (all but AIMed). Classifiers are trained on the ensemble of three corpora and tested on the forth one.</p>
<p>(0.07 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000837.s008" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.s008" xlink:type="simple"><label>Table S8</label><caption>
<p>CV results with transductive SVM for kBSPS, edit, cosine kernels. Results with the transductive learning strategy for some selected kernels.</p>
<p>(0.06 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000837.s009" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.s009" xlink:type="simple"><label>Table S9</label><caption>
<p>Average runtime of training and test processes, and runtime estimates on entire Medline. Average runtime of training and test processes per corpus measured over all cross-validation experiments for each kernel (not including the parsing time at pre-processing), and rough runtime estimates on the entire Medline.</p>
<p>(0.06 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000837.s010" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.s010" xlink:type="simple"><label>Text S1</label><caption>
<p>Similarity function in kBSPS kernel. Definition of similarity score used in kBSPS kernel.</p>
<p>(0.07 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000837.s011" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.s011" xlink:type="simple"><label>Text S2</label><caption>
<p>Additional experiments. We provide here details of two additional experiments. (1) Cross-learning (CL) without AIMed, that is systems are trained on 3 corpora and tested on the fourth one. (2) Models trained with transductive SVM.</p>
<p>(0.05 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000837.s012" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000837.s012" xlink:type="simple"><label>Text S3</label><caption>
<p>Theoretical complexity of kernels. We provide here details on the computational complexity of kernels.</p>
<p>(0.04 MB PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank all authors of the kernel methods we discuss for providing code and numerous hints on how to install and use the systems. We particularly thank Antti Airola for intensive discussions on benchmarking PPI extraction in general and in numerous special cases. We also thank the anonymous reviewers for their valuable comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1000837-Hoffmann1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hoffmann</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Krallinger</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Andres</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Tamames</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Blaschke</surname><given-names>C</given-names></name>
<etal/></person-group>             <year>2005</year>             <article-title>Text mining for metabolic pathways, signaling cascades, and protein networks.</article-title>             <source>Sci STKE 2005</source>             <fpage>pe21</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Jaeger1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Jaeger</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Gaudan</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Leser</surname><given-names>U</given-names></name>
<name name-style="western"><surname>Rebholz-Schuhmann</surname><given-names>D</given-names></name>
</person-group>             <year>2008</year>             <article-title>Integrating protein-protein interactions and text mining for protein function prediction.</article-title>             <source>BMC Bioinformatics</source>             <volume>9</volume>             <supplement>Suppl 8</supplement>             <fpage>S2</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Jiang1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Jiang</surname><given-names>X</given-names></name>
<name name-style="western"><surname>Nariai</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Steffen</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Kasif</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Kolaczyk</surname><given-names>ED</given-names></name>
</person-group>             <year>2008</year>             <article-title>Integration of relational and hierarchical network information for protein function prediction.</article-title>             <source>BMC Bioinformatics</source>             <volume>9</volume>             <fpage>350</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Spirin1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Spirin</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Mirny</surname><given-names>LA</given-names></name>
</person-group>             <year>2003</year>             <article-title>Protein complexes and functional modules in molecular networks.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>100</volume>             <fpage>12123</fpage>             <lpage>8</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Ideker1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ideker</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Sharan</surname><given-names>R</given-names></name>
</person-group>             <year>2008</year>             <article-title>Protein networks in disease.</article-title>             <source>Genome Res</source>             <volume>18</volume>             <fpage>644</fpage>             <lpage>652</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Lalonde1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lalonde</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Ehrhardt</surname><given-names>DW</given-names></name>
<name name-style="western"><surname>Loqué</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Chen</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Rhee</surname><given-names>SY</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>Molecular and cellular approaches for the detection of protein-protein interactions: latest techniques and current limitations.</article-title>             <source>Plant J</source>             <volume>53</volume>             <fpage>610</fpage>             <lpage>35</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Sprinzak1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sprinzak</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Sattath</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Margalit</surname><given-names>H</given-names></name>
</person-group>             <year>2003</year>             <article-title>How reliable are experimental protein-protein interaction data?</article-title>             <source>J Mol Biol</source>             <volume>327</volume>             <fpage>919</fpage>             <lpage>923</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Miernyk1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Miernyk</surname><given-names>JA</given-names></name>
<name name-style="western"><surname>Thelen</surname><given-names>JJ</given-names></name>
</person-group>             <year>2008</year>             <article-title>Biochemical approaches for discovering protein-protein interactions.</article-title>             <source>Plant J</source>             <volume>53</volume>             <fpage>597</fpage>             <lpage>609</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Chatraryamontri1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chatr-aryamontri</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Ceol</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Palazzi</surname><given-names>LM</given-names></name>
<name name-style="western"><surname>Nardelli</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Schneider</surname><given-names>MV</given-names></name>
<etal/></person-group>             <year>2007</year>             <article-title>MINT: the Molecular INTeraction database.</article-title>             <source>Nucleic Acids Res</source>             <volume>35</volume>             <fpage>D572</fpage>             <lpage>D574</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Winnenburg1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Winnenburg</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Wächter</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Plake</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Doms</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Schroeder</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Facts from text: can text mining help to scale-up high-quality manual curation of gene products with ontologies?</article-title>             <source>Brief Bioinform</source>             <volume>9</volume>             <fpage>466</fpage>             <lpage>478</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-zgr1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Özgür</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Vu</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Erkan</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Radev</surname><given-names>DR</given-names></name>
</person-group>             <year>2008</year>             <article-title>Identifying gene-disease associations using centrality on a literature mined gene-interaction network.</article-title>             <source>Bioinformatics</source>             <volume>24</volume>             <fpage>i277</fpage>             <lpage>285</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Lage1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lage</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Karlberg</surname><given-names>EO</given-names></name>
<name name-style="western"><surname>Størling</surname><given-names>ZM</given-names></name>
<name name-style="western"><surname>Ólason</surname><given-names>PI</given-names></name>
<name name-style="western"><surname>Pedersen</surname><given-names>AG</given-names></name>
<etal/></person-group>             <year>2007</year>             <article-title>A human phenome-interactome network of protein complexes implicated in genetic disorders.</article-title>             <source>Nat Biotechnol</source>             <volume>25</volume>             <fpage>309</fpage>             <lpage>16</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Proux1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Proux</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Rechenmann</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Julliard</surname><given-names>L</given-names></name>
</person-group>             <year>2000</year>             <article-title>A pragmatic information extraction strategy for gathering data on genetic interactions.</article-title>             <source>Proc Int Conf Intell Syst Mol Biol</source>             <volume>8</volume>             <fpage>279</fpage>             <lpage>85</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Leitner1"><label>14</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Leitner</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Hirschman</surname><given-names>L</given-names></name>
</person-group>             <year>2009</year>             <article-title>Biocreative ii.5: Evaluation and ensemble system performance.</article-title>             <source>Proc BioCreative II.5</source>             <publisher-loc>Madrid, Spain</publisher-loc> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">20</size>           </element-citation></ref>
<ref id="pcbi.1000837-Kabiljo1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kabiljo</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Clegg</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Shepherd</surname><given-names>A</given-names></name>
</person-group>             <year>2009</year>             <article-title>A realistic assessment of methods for extracting gene/protein interactions from free text.</article-title>             <source>BMC Bioinformatics</source>             <volume>10</volume>             <fpage>233</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Giles1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Giles</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Wren</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>Large-scale directional relationship extraction and resolution.</article-title>             <source>BMC Bioinformatics</source>             <volume>9</volume>             <fpage>S11</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Airola1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Airola</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Pyysalo</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Björne</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Pahikkala</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Ginter</surname><given-names>F</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>All-paths graph kernel for protein-protein interaction extraction with evaluation of cross-corpus learning.</article-title>             <source>BMC Bioinformatics</source>             <volume>9</volume>             <fpage>S2</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Bunescu1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bunescu</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Ge</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Kate</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>Marcotte</surname><given-names>EM</given-names></name>
<name name-style="western"><surname>Mooney</surname><given-names>RJ</given-names></name>
<etal/></person-group>             <year>2005</year>             <article-title>Comparative experiments on learning information extractors for proteins and their interactions.</article-title>             <source>Artif Intell Med</source>             <volume>33</volume>             <fpage>139</fpage>             <lpage>155</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Bunescu2"><label>19</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bunescu</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Mooney</surname><given-names>R</given-names></name>
</person-group>             <year>2006</year>             <article-title>Subsequence kernels for relation extraction.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Weiss</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Platt</surname><given-names>J</given-names></name>
</person-group>             <source>Advances in Neural Information Processing Systems 18</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>171</fpage>             <lpage>178</lpage>             <comment>URL <ext-link ext-link-type="uri" xlink:href="http://books.nips.cc/papers/files/nips18/NIPS2005_0450.pdf" xlink:type="simple">http://books.nips.cc/papers/files/nips18/NIPS2005_0450.pdf</ext-link></comment>          </element-citation></ref>
<ref id="pcbi.1000837-Collins1"><label>20</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Collins</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Duffy</surname><given-names>N</given-names></name>
</person-group>             <year>2001</year>             <article-title>Convolution kernels for natural language.</article-title>             <source>Proc. of Neural Information Processing Systems (NIPS'01)</source>             <publisher-loc>Vancouver, BC, Canada</publisher-loc>             <fpage>625</fpage>             <lpage>632</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Culotta1"><label>21</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Culotta</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Sorensen</surname><given-names>JS</given-names></name>
</person-group>             <year>2004</year>             <article-title>Dependency tree kernels for relation extraction.</article-title>             <source>Proc. of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL'04)</source>             <publisher-loc>Barcelona, Spain</publisher-loc>             <fpage>423</fpage>             <lpage>429</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Erkan1"><label>22</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Erkan</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Özgür</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Radev</surname><given-names>DR</given-names></name>
</person-group>             <year>2007</year>             <article-title>Semi-supervised classification for extracting protein interaction sentences using dependency parsing.</article-title>             <source>Proc. of the 2007 Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</source>             <publisher-loc>Prague, Czech Republic</publisher-loc>             <fpage>228</fpage>             <lpage>237</lpage>             <comment>URL <ext-link ext-link-type="uri" xlink:href="http://www.aclweb.org/anthology/D/D07/D07-1024" xlink:type="simple">http://www.aclweb.org/anthology/D/D07/D07-1024</ext-link></comment>          </element-citation></ref>
<ref id="pcbi.1000837-Giuliano1"><label>23</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Giuliano</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Lavelli</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Romano</surname><given-names>L</given-names></name>
</person-group>             <year>2006</year>             <article-title>Exploiting shallow linguistic information for relation extraction from biomedical literature.</article-title>             <source>Proc. of the 11st Conf. of the European Chapter of the Association for Computational Linguistics (EACL'06)</source>             <publisher-loc>Trento, Italy</publisher-loc>             <publisher-name>The Association for Computer Linguistics</publisher-name>             <fpage>401</fpage>             <lpage>408</lpage>             <comment>URL <ext-link ext-link-type="uri" xlink:href="http://acl.ldc.upenn.edu/E/E06/E06-1051.pdf" xlink:type="simple">http://acl.ldc.upenn.edu/E/E06/E06-1051.pdf</ext-link></comment>          </element-citation></ref>
<ref id="pcbi.1000837-Katrenko1"><label>24</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Katrenko</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Adriaans</surname><given-names>P</given-names></name>
</person-group>             <year>2008</year>             <article-title>A local alignment kernel in the context of NLP.</article-title>             <source>Proc. of the 22nd Int. Conf. on Computational Linguistics (Coling 2008)</source>             <publisher-loc>Manchester, UK</publisher-loc>             <fpage>417</fpage>             <lpage>424</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Kim1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kim</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Yoon</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Yang</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>Kernel approaches for genic interaction extraction.</article-title>             <source>Bioinformatics</source>             <volume>24</volume>             <fpage>118</fpage>             <lpage>126</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-VanLandeghem1"><label>26</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Van Landeghem</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Saeys</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>De Baets</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Van de Peer</surname><given-names>Y</given-names></name>
</person-group>             <year>2008</year>             <article-title>Extracting protein-protein interactions from text using rich feature vectors and feature selection.</article-title>             <source>Proc. of 3rd Int. Symp. on Semantic Mining in Biomedicine (SMBM'08)</source>             <publisher-loc>Turku, Finnland</publisher-loc>             <fpage>77</fpage>             <lpage>84</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Miwa1"><label>27</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Miwa</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Sætre</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Miyao</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Ohta</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Tsujii</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>Combining multiple layers of syntactic information for protein-protein interaction extraction.</article-title>             <source>Proc. of 3rd Int. Symp. on Semantic Mining in Biomedicine (SMBM'08)</source>             <publisher-loc>Turku, Finnland</publisher-loc>             <fpage>101</fpage>             <lpage>108</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Moschitti1"><label>28</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Moschitti</surname><given-names>A</given-names></name>
</person-group>             <year>2006</year>             <article-title>Efficient convolution kernels for dependency and constituent syntactic trees.</article-title>             <source>Proc. of The 17th European Conf. on Machine Learning</source>             <publisher-loc>Berlin, Germany</publisher-loc>             <fpage>318</fpage>             <lpage>329</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Palaga1"><label>29</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Palaga</surname><given-names>P</given-names></name>
</person-group>             <year>2009</year>             <article-title>Extracting Relations from Biomedical Texts Using Syntactic Information.</article-title>             <comment>Master's Thesis, Technische Universität Berlin</comment>          </element-citation></ref>
<ref id="pcbi.1000837-Vishwanathan1"><label>30</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Vishwanathan</surname><given-names>SVN</given-names></name>
<name name-style="western"><surname>Smola</surname><given-names>AJ</given-names></name>
</person-group>             <year>2002</year>             <article-title>Fast kernels for string and tree matching.</article-title>             <source>Proc. of Neural Information Processing Systems (NIPS'02)</source>             <publisher-loc>Vancouver, BC, Canada</publisher-loc>             <fpage>569</fpage>             <lpage>576</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Wang1"><label>31</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wang</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>A re-examination of dependency path kernels for relation extraction.</article-title>             <source>Proc. of the 3rd Int. Conf. on Natural Language Processing (IJCNLP'08)</source>             <publisher-loc>Hyderabad, India</publisher-loc>             <fpage>841</fpage>             <lpage>846</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Zelenko1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zelenko</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Aone</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Richardella</surname><given-names>A</given-names></name>
</person-group>             <year>2003</year>             <article-title>Kernel methods for relation extraction.</article-title>             <source>J Mach Learn Res</source>             <volume>3</volume>             <fpage>1083</fpage>             <lpage>1106</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Niu1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Niu</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Otasek</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Jurisica</surname><given-names>I</given-names></name>
</person-group>             <year>2010</year>             <article-title>Evaluation of linguistic features useful in extraction of interactions from PubMed; Application to annotating known, high-throughput and predicted interactions in I2D.</article-title>             <source>Bioinformatics</source>             <volume>26</volume>             <fpage>111</fpage>             <lpage>119</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Kim2"><label>34</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kim</surname><given-names>JD</given-names></name>
<name name-style="western"><surname>Ohta</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Pyysalo</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Kano</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Tsujii</surname><given-names>J</given-names></name>
</person-group>             <year>2009</year>             <article-title>Overview of BioNLP'09 shared task on event extraction.</article-title>             <source>Proc. of the BioNLP 2009 Workshop Companion Volume for Shared Task</source>             <publisher-loc>Boulder, CO, USA</publisher-loc>             <fpage>1</fpage>             <lpage>9</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Clegg1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Clegg</surname><given-names>AB</given-names></name>
<name name-style="western"><surname>Shepherd</surname><given-names>AJ</given-names></name>
</person-group>             <year>2007</year>             <article-title>Benchmarking natural-language parsers for biological applications using dependency graphs.</article-title>             <source>BMC Bioinformatics</source>             <volume>8</volume>             <fpage>24</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Pyysalo1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pyysalo</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Airola</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Heimonen</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Björne</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Ginter</surname><given-names>F</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>Comparative analysis of five protein-protein interaction corpora.</article-title>             <source>BMC Bioinformatics</source>             <volume>9</volume>             <supplement>Suppl 3</supplement>             <fpage>S6</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Miyao1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Miyao</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Sagae</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Saetre</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Matsuzaki</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Tsujii</surname><given-names>J</given-names></name>
</person-group>             <year>2009</year>             <article-title>Evaluating contributions of natural language parsers to protein-protein interaction extraction.</article-title>             <source>Bioinformatics</source>             <volume>25</volume>             <fpage>394</fpage>             <lpage>400</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Fayruzov1"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fayruzov</surname><given-names>T</given-names></name>
<name name-style="western"><surname>De Cock</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Cornelis</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Hoste</surname><given-names>V</given-names></name>
</person-group>             <year>2009</year>             <article-title>Linguistic feature analysis for protein interaction extraction.</article-title>             <source>BMC Bioinformatics</source>             <volume>10</volume>             <fpage>374</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Zhou1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zhou</surname><given-names>D</given-names></name>
<name name-style="western"><surname>He</surname><given-names>Y</given-names></name>
</person-group>             <year>2008</year>             <article-title>Extracting interactions between proteins from the literature.</article-title>             <source>J Biomed Inform</source>             <volume>41</volume>             <fpage>393</fpage>             <lpage>407</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Rinaldi1"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rinaldi</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Kappeler</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Kaljurand</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Schneider</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Klenner</surname><given-names>M</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>OntoGene in BioCreative II.</article-title>             <source>Genome Biol</source>             <volume>9</volume>             <fpage>S13</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Blaschke1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Blaschke</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Valencia</surname><given-names>A</given-names></name>
</person-group>             <year>2002</year>             <article-title>The frame-based module of the SUISEKI information extraction system.</article-title>             <source>IEEE Intell Syst</source>             <volume>17</volume>             <fpage>14</fpage>             <lpage>20</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Hunter1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hunter</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Lu</surname><given-names>Z</given-names></name>
<name name-style="western"><surname>Firby</surname><given-names>J</given-names></name>
<name name-style="western"><surname>B</surname><given-names>WA</given-names><suffix>Jr</suffix></name>
<name name-style="western"><surname>Johnson</surname><given-names>HL</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>OpenDMAP: An open source, ontology-driven concept analysis engine, with applications to capturing knowledge regarding protein transport, protein interactions and cell-type-specific gene expression.</article-title>             <source>BMC Bioinformatics</source>             <volume>9</volume>             <fpage>78</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Krallinger1"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Krallinger</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Leitner</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Rodriguez-Penagos</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Valencia</surname><given-names>A</given-names></name>
</person-group>             <year>2008</year>             <article-title>Overview of the protein-protein interaction annotation extraction task of BioCreative II.</article-title>             <source>Genome Biol</source>             <volume>9</volume>             <fpage>S4</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Hao1"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hao</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Zhu</surname><given-names>X</given-names></name>
<name name-style="western"><surname>Huang</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Li</surname><given-names>M</given-names></name>
</person-group>             <year>2005</year>             <article-title>Discovering patterns to extract protein-protein interactions from the literature: Part II.</article-title>             <source>Bioinformatics</source>             <volume>21</volume>             <fpage>3294</fpage>             <lpage>3300</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Hakenberg1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hakenberg</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Plake</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Royer</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Strobelt</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Leser</surname><given-names>U</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>Gene mention normalization and interaction extraction with context models and sentence motifs.</article-title>             <source>Genome Biol</source>             <volume>9</volume>             <fpage>S14</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Chowdhary1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chowdhary</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Zhang</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Liu</surname><given-names>JS</given-names></name>
</person-group>             <year>2009</year>             <article-title>Bayesian inference of protein-protein interactions from biological literature.</article-title>             <source>Bioinformatics</source>             <volume>25</volume>             <fpage>1536</fpage>             <lpage>1542</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Sun1"><label>47</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sun</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Lin</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Wang</surname><given-names>X</given-names></name>
<name name-style="western"><surname>Guan</surname><given-names>Y</given-names></name>
</person-group>             <year>2007</year>             <article-title>Using maximum entropy model to extract protein-protein interaction information from biomedical literature.</article-title>             <source>Advanced Intelligent Computing Theories and Applications, Springer, number 4681 in LNCS</source>             <fpage>730</fpage>             <lpage>737</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Miyao2"><label>48</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Miyao</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Sætre</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Sagae</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Matsuzaki</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Tsujii</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>Task-oriented evaluation of syntactic parsers and their representations.</article-title>             <source>Proc. of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL'08: HLT)</source>             <fpage>46</fpage>             <lpage>54</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Pyysalo2"><label>49</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pyysalo</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Ginter</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Laippala</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Haverinen</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Heimonen</surname><given-names>J</given-names></name>
<etal/></person-group>             <year>2007</year>             <article-title>On the unification of syntactic annotations under the Stanford dependency scheme: A case study on BioInfer and GENIA.</article-title>             <source>Biological, translational, and clinical language processing</source>             <publisher-loc>Prague, Czech Republic</publisher-loc>             <fpage>25</fpage>             <lpage>32</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-McClosky1"><label>50</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>McClosky</surname><given-names>D</given-names></name>
</person-group>             <year>2009</year>             <article-title>Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing.</article-title>             <comment>Ph.D. thesis, Department of Computer Science, Brown University</comment>          </element-citation></ref>
<ref id="pcbi.1000837-McClosky2"><label>51</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>McClosky</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Charniak</surname><given-names>E</given-names></name>
</person-group>             <year>2008</year>             <article-title>Self-training for biomedical parsing.</article-title>             <source>Proc. of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL'08: HLT)</source>             <publisher-loc>Columbus, OH, USA</publisher-loc>             <fpage>101</fpage>             <lpage>104</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Joachims1"><label>52</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Joachims</surname><given-names>T</given-names></name>
</person-group>             <year>1999</year>             <source>Making large-scale support vector machine learning practical, Advances in kernel methods: support vector learning</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000837-Schlkopf1"><label>53</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="editor">
<name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Burges</surname><given-names>CJC</given-names></name>
<name name-style="western"><surname>Smola</surname><given-names>AJ</given-names></name>
</person-group>             <year>1999</year>             <source>Advances in kernel methods: support vector learning</source>             <publisher-name>The MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000837-Rifkin1"><label>54</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rifkin</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Yeo</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name>
</person-group>             <year>2003</year>             <article-title>Regularized least-squares classification.</article-title>             <source>Nato Science Series Sub Series III Computer and Systems Sciences</source>             <volume>190</volume>             <fpage>131</fpage>             <lpage>154</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-WintersHilt1"><label>55</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Winters-Hilt</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Yelundur</surname><given-names>A</given-names></name>
<name name-style="western"><surname>McChesney</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Landry</surname><given-names>M</given-names></name>
</person-group>             <year>2006</year>             <article-title>Support vector machine implementations for classification &amp; clustering.</article-title>             <source>BMC Bioinformatics</source>             <volume>7</volume>             <fpage>S4</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Chang1"><label>56</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chang</surname><given-names>CC</given-names></name>
<name name-style="western"><surname>Lin</surname><given-names>CJ</given-names></name>
</person-group>             <year>2001</year>             <article-title>LIBSVM: a library for support vector machines.</article-title>             <comment>Software available at <ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/cjlin/libsvm" xlink:type="simple">http://www.csie.ntu.edu.tw/cjlin/libsvm</ext-link></comment>          </element-citation></ref>
<ref id="pcbi.1000837-Haussler1"><label>57</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Haussler</surname><given-names>D</given-names></name>
</person-group>             <year>1999</year>             <article-title>Convolution kernels on discrete structures.</article-title>             <comment>Technical Report UCS-CRL-99-10, University of California at Santa Cruz, Santa Cruz, CA, USA</comment>          </element-citation></ref>
<ref id="pcbi.1000837-Kuboyama1"><label>58</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kuboyama</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Hirata</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Kashima</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Aoki-Kinoshita</surname><given-names>KF</given-names></name>
<name name-style="western"><surname>Yasuda</surname><given-names>H</given-names></name>
</person-group>             <year>2007</year>             <article-title>A spectrum tree kernel.</article-title>             <source>Information and Media Technologies</source>             <volume>2</volume>             <fpage>292</fpage>             <lpage>299</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Pyysalo3"><label>59</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pyysalo</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Ginter</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Heimonen</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Bjorne</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Boberg</surname><given-names>J</given-names></name>
<etal/></person-group>             <year>2007</year>             <article-title>BioInfer: a corpus for information extraction in the biomedical domain.</article-title>             <source>BMC Bioinformatics</source>             <volume>8</volume>             <fpage>50</fpage>          </element-citation></ref>
<ref id="pcbi.1000837-Fundel1"><label>60</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fundel</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Küffner</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Zimmer</surname><given-names>R</given-names></name>
</person-group>             <year>2007</year>             <article-title>RelEx – relation extraction using dependency parse trees.</article-title>             <source>Bioinformatics</source>             <volume>23</volume>             <fpage>365</fpage>             <lpage>371</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Ding1"><label>61</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ding</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Berleant</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Nettleton</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Wurtele</surname><given-names>E</given-names></name>
</person-group>             <year>2002</year>             <article-title>Mining Medline: abstracts, sentences, or phrases?</article-title>             <source>Pac Symp Biocomput</source>             <fpage>326</fpage>             <lpage>337</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Nedellec1"><label>62</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nedellec</surname><given-names>C</given-names></name>
</person-group>             <year>2005</year>             <article-title>Learning language in logic-genic interaction extraction challenge.</article-title>             <source>Proc. of the ICML05 workshop: Learning Language in Logic (LLL'05)</source>             <publisher-loc>Bonn, Germany</publisher-loc>             <fpage>97</fpage>             <lpage>99</lpage>             <comment>volume 18</comment>          </element-citation></ref>
<ref id="pcbi.1000837-Miwa2"><label>63</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Miwa</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Sætre</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Miyao</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Tsujii</surname><given-names>J</given-names></name>
</person-group>             <year>2009</year>             <article-title>Protein-protein interaction extraction by leveraging multiple kernels and parsers.</article-title>             <source>Int J Med Inform</source>             <volume>18</volume>             <fpage>e39</fpage>             <lpage>e46</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Stre1"><label>64</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sætre</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Sagae</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Tsujii</surname><given-names>J</given-names></name>
</person-group>             <year>2007</year>             <article-title>Syntactic features for protein-protein interaction extraction.</article-title>             <source>Proc. Int Symp on Languages in Biology and Medicine (LBM'07)</source>             <publisher-loc>Singapore</publisher-loc>          </element-citation></ref>
<ref id="pcbi.1000837-Ravichandran1"><label>65</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ravichandran</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Pantel</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Hovy</surname><given-names>E</given-names></name>
</person-group>             <year>2004</year>             <article-title>The terascale challenge.</article-title>             <source>Proc. of KDD Workshop on Mining for and from the Semantic Web (MSW-04)</source>             <publisher-loc>Seattle, WA, USA</publisher-loc>             <fpage>1</fpage>             <lpage>11</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Joachims2"><label>66</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Joachims</surname><given-names>T</given-names></name>
</person-group>             <year>1999</year>             <article-title>Transductive inference for text classification using support vector machines.</article-title>             <source>Proc. of the 16th Int. Conf. on Machine Learning (ICML'99)</source>             <fpage>200</fpage>             <lpage>209</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Saric1"><label>67</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Saric</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Jensen</surname><given-names>LJ</given-names></name>
<name name-style="western"><surname>Ouzounova</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Rojas</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Bork</surname><given-names>P</given-names></name>
</person-group>             <year>2006</year>             <article-title>Extraction of regulatory gene/protein networks from medline.</article-title>             <source>Bioinformatics</source>             <volume>22</volume>             <fpage>645</fpage>             <lpage>650</lpage>          </element-citation></ref>
<ref id="pcbi.1000837-Hu1"><label>68</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hu</surname><given-names>ZZ</given-names></name>
<name name-style="western"><surname>Narayanaswamy</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Ravikumar</surname><given-names>KE</given-names></name>
<name name-style="western"><surname>Vijay-Shanker</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Wu</surname><given-names>CH</given-names></name>
</person-group>             <year>2005</year>             <article-title>Literature mining and database annotation of protein phosphorylation using a rule-based system.</article-title>             <source>Bioinformatics</source>             <volume>21</volume>             <fpage>2759</fpage>             <lpage>65</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>