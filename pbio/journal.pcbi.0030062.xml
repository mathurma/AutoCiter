<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN"><front><journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="publisher">pcbi</journal-id><journal-id journal-id-type="allenpress-id">plcb</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1371/journal.pcbi.0030062</article-id><article-id pub-id-type="publisher-id">06-PLCB-RA-0501R2</article-id><article-id pub-id-type="sici">plcb-03-04-04</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology</subject><subject>Neuroscience</subject><subject>Neuroscience</subject><subject>Neuroscience</subject><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="System Taxonomy"><subject>Primates</subject></subj-group></article-categories><title-group><article-title>Psychophysical Tests of the Hypothesis of a Bottom-Up Saliency Map in Primary Visual Cortex</article-title><alt-title alt-title-type="running-head">Psychophysical Tests of the V1 Saliency Map</alt-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Zhaoping</surname><given-names>Li</given-names></name><xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref><xref ref-type="aff" rid="aff1"/></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>May</surname><given-names>Keith A</given-names></name><xref ref-type="aff" rid="aff1"/></contrib></contrib-group><aff id="aff1">
        <label>1</label>
        <addr-line>Department of Psychology, University College London, London, United Kingdom</addr-line>
      </aff><contrib-group><contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes><fn fn-type="con" id="ack1"><p>LZ conceived and designed the experiments. Both KAM and LZ performed the experiments and analyzed the data, with KAM focusing on the main body of the experiments and data analysis, and LZ on the pilot experiments and on the later parts of the experiments and data analysis. While LZ led the manuscript writing, KAM contributed significantly in the editing and revision process.</p></fn><corresp id="cor1">* To whom correspondence should be addressed. E-mail: <email xlink:type="simple">z.li@ucl.ac.uk</email></corresp><fn fn-type="conflict" id="ack3"><p> The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="ppub"><month>4</month><year>2007</year></pub-date><pub-date pub-type="epub"><day>6</day><month>4</month><year>2007</year></pub-date><pub-date pub-type="epreprint"><day>20</day><month>2</month><year>2007</year></pub-date><volume>3</volume><issue>4</issue><elocation-id>e62</elocation-id><history><date date-type="received"><day>30</day><month>11</month><year>2006</year></date><date date-type="accepted"><day>16</day><month>2</month><year>2007</year></date></history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2007</copyright-year><copyright-holder>Zhaoping and May</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract><p>A unique vertical bar among horizontal bars is salient and pops out perceptually. Physiological data have suggested that mechanisms in the primary visual cortex (V1) contribute to the high saliency of such a unique basic feature, but indicated little regarding whether V1 plays an essential or peripheral role in input-driven or bottom-up saliency. Meanwhile, a biologically based V1 model has suggested that V1 mechanisms can also explain bottom-up saliencies beyond the pop-out of basic features, such as the low saliency of a unique conjunction feature such as a red vertical bar among red horizontal and green vertical bars, under the hypothesis that the bottom-up saliency at any location is signaled by the activity of the most active cell responding to it regardless of the cell's preferred features such as color and orientation. The model can account for phenomena such as the difficulties in conjunction feature search, asymmetries in visual search, and how background irregularities affect ease of search. In this paper, we report nontrivial predictions from the V1 saliency hypothesis, and their psychophysical tests and confirmations. The prediction that most clearly distinguishes the V1 saliency hypothesis from other models is that task-irrelevant features could interfere in visual search or segmentation tasks which rely significantly on bottom-up saliency. For instance, irrelevant colors can interfere in an orientation-based task, and the presence of horizontal and vertical bars can impair performance in a task based on oblique bars. Furthermore, properties of the intracortical interactions and neural selectivities in V1 predict specific emergent phenomena associated with visual grouping. Our findings support the idea that a bottom-up saliency map can be at a lower visual area than traditionally expected, with implications for top-down selection mechanisms.</p></abstract><abstract abstract-type="summary"><title>Author Summary</title><sec id="st1"><title/><p>Only a fraction of visual input can be selected for attentional scrutiny, often by focusing on a limited extent of the visual space. The selected location is often determined by the bottom-up visual inputs rather than the top-down intentions. For example, a red dot among green ones automatically attracts attention and is said to be salient. Physiological data have suggested that the primary visual cortex (V1) in the brain contributes to creating such bottom-up saliencies from visual inputs, but indicated little on whether V1 plays an essential or peripheral role in creating a saliency map of the input space to guide attention. Traditional psychological frameworks, based mainly on behavioral data, have implicated higher-level brain areas for the saliency map. Recently, it has been hypothesized that V1 creates this saliency map, such that the image location whose visual input evokes the highest response among all V1 output neurons is most likely selected from a visual scene for attentional processing. This paper derives nontrivial predictions from this hypothesis and presents their psychophysical tests and confirmations. Our findings suggest that bottom-up saliency is computed at a lower brain area than previously expected, and have implications on top-down attentional mechanisms.</p></sec></abstract><funding-group><funding-statement>Work was supported in part by the Gatsby Charitable Foundation and by grant GR/R87642/01 from the UK Research Council.</funding-statement></funding-group><counts><page-count count="18"/></counts><!--===== Restructure custom-meta-wrap to custom-meta-group =====--><custom-meta-group><custom-meta><meta-name>citation</meta-name><meta-value>Zhaoping L, May KA (2007) Psychophysical tests of the hypothesis of a bottom-up saliency map in primary visual cortex. PLoS Comput Biol 3(4): e62. doi:<ext-link ext-link-type="doi" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.0030062" xlink:type="simple">10.1371/journal.pcbi.0030062</ext-link></meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Visual selection of inputs for detailed, attentive, processing often occurs in a bottom-up or stimulus driven manner, particularly in selections immediately or very soon after visual stimulus onset [<xref ref-type="bibr" rid="pcbi-0030062-b001">1</xref>–<xref ref-type="bibr" rid="pcbi-0030062-b003">3</xref>]. For instance, a vertical bar among horizontal ones or a red dot among green ones perceptually pops out automatically to attract attention [<xref ref-type="bibr" rid="pcbi-0030062-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b005">5</xref>], and is said to be highly salient pre-attentively. Physiologically, a neuron in the primary visual cortex (V1) gives a higher response to its preferred feature, e.g., a specific orientation, color, or motion direction, within its receptive field (RF) when this feature is unique within the display, rather than when it is one of the elements in a homogenous background [<xref ref-type="bibr" rid="pcbi-0030062-b006">6</xref>–<xref ref-type="bibr" rid="pcbi-0030062-b012">12</xref>]. This is the case even when the animal is under anesthesia [<xref ref-type="bibr" rid="pcbi-0030062-b009">9</xref>], suggesting bottom-up mechanisms. This occurs because the neuron's response to its preferred feature is often suppressed when this stimulus is surrounded by stimuli of the same or similar features. Such contextual influences, termed iso-feature suppression, and iso-orientation suppression in particular, are mediated by intracortical connections between nearby V1 neurons [<xref ref-type="bibr" rid="pcbi-0030062-b013">13</xref>–<xref ref-type="bibr" rid="pcbi-0030062-b015">15</xref>]. The same mechanisms also make V1 cells respond more vigorously to an oriented bar when it is at the border, rather than at the middle, of a homogeneous orientation texture, as physiologically observed [<xref ref-type="bibr" rid="pcbi-0030062-b010">10</xref>], since the bar has fewer iso-orientation neighbors at the border. These observations have prompted suggestions that V1 mechanisms contribute to bottom-up saliency for pop-out features like the unique orientation singleton or the bar at an orientation texture border (e.g., [<xref ref-type="bibr" rid="pcbi-0030062-b006">6</xref>–<xref ref-type="bibr" rid="pcbi-0030062-b010">10</xref>]). This is consistent with observations that highly salient inputs can bias responses in extrastriate areas receiving inputs from V1 [<xref ref-type="bibr" rid="pcbi-0030062-b016">16</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b017">17</xref>].</p><p>Behavioral studies have examined bottom-up saliencies extensively in visual search and segmentation tasks [<xref ref-type="bibr" rid="pcbi-0030062-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b018">18</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b019">19</xref>], showing more complex, subtle, and general situations beyond basic feature pop-outs. For instance, a unique feature conjunction, e.g., a red vertical bar as a color-orientation conjunction, is typically less salient and requires longer search times; ease of searches can change with target-distractor swaps; and target salience decreases with background irregularities. However, few physiological recordings in V1 have used stimuli of comparable complexity, leaving it open how generally V1 mechanisms contribute to bottom-up saliency.</p><p>Meanwhile, a model of contextual influences in V1 [<xref ref-type="bibr" rid="pcbi-0030062-b020">20</xref>–<xref ref-type="bibr" rid="pcbi-0030062-b023">23</xref>], including iso-feature suppression and colinear facilitation [<xref ref-type="bibr" rid="pcbi-0030062-b024">24</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b025">25</xref>], has demonstrated that V1 mechanisms can plausibly explain these complex behaviors mentioned above, assuming that the V1 cell with the highest response to a target determines its salience and thus the ease of a task. Accordingly, V1 has been proposed to create a bottom-up saliency map, such that the RF location of the most active V1 cell is most likely selected for further detailed processing [<xref ref-type="bibr" rid="pcbi-0030062-b020">20</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b023">23</xref>]. We call this proposal the V1 saliency hypothesis. This hypothesis is consistent with the observation that microstimulation of a V1 cell can drive saccades, via superior colliculus, to the corresponding RF location [<xref ref-type="bibr" rid="pcbi-0030062-b026">26</xref>], and that higher V1 responses correlate with shorter RTs to saccades to the corresponding RFs [<xref ref-type="bibr" rid="pcbi-0030062-b027">27</xref>]. It can be clearly expressed algebraically. Let (<italic>O</italic><sub>1</sub>,<italic>O</italic><sub>2</sub>,…<italic>O</italic><sub>M</sub>) denote outputs or responses from V1 output cells indexed by <italic>i</italic> = 1, 2,...<italic>M</italic>, and let the RFs of these cells cover locations (<italic>x</italic><sub>1</sub>,<italic>x</italic><sub>2</sub>,…<italic>x<sub>M</sub></italic>), respectively, then the location selected by bottom-up mechanisms is <italic>◯</italic> = <italic>x<sub>î</sub></italic> where <italic>î</italic> is the index of the most responsive V1 cell (mathematically, <italic>î</italic> = argmax<italic><sub>i</sub>O<sub>i</sub></italic>). It is then clear that (1) the saliency SMAP(<italic>x</italic>) at a visual location <italic>x</italic> increases with the response level of the most active V1 cell responding to it,
				<disp-formula id="pcbi-0030062-e001"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030062.e001" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mtext>SMAP</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext>&ensp;increases with</mml:mtext><mml:msub><mml:mrow><mml:mi>max</mml:mi><mml:mo></mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>O</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext>&thinsp;given an input scene</mml:mtext></mml:mrow></mml:math> --></disp-formula>and the less-activated cells responding to the same location do not contribute, regardless of the feature preferences of the cells; and (2) the highest response to a particular location is compared with the highest responses to other locations to determine the saliency of this location, since only the RF location of the most activated V1 cell is the most likely selected (mathematically, the selected location is <italic>◯</italic> = argmax<italic><sub>x</sub></italic>SMAP(<italic>x</italic>))). As salience merely serves to order the priority of inputs to be selected for further processing, only the order of the salience is relevant [<xref ref-type="bibr" rid="pcbi-0030062-b023">23</xref>]. However, for convenience we could write <xref ref-type="disp-formula" rid="pcbi-0030062-e001">Equation 1</xref> as SMAP(<italic>x</italic>) = [<inline-formula id="pcbi-0030062-ex001"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex001" xlink:type="simple"/></inline-formula>
				] /[<inline-formula id="pcbi-0030062-ex002"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex002" xlink:type="simple"/></inline-formula>
				], or simply SMAP(<italic>x</italic>) = <inline-formula id="pcbi-0030062-ex003"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex003" xlink:type="simple"/></inline-formula>
				 Note that the interpretation of <italic>x<sub>i</sub></italic> = <italic>x</italic> is that the RF of cell <italic>i</italic> covers location <italic>x</italic> or is centered near <italic>x</italic>.
			</p><p>In a recent physiological experiment, Hegde and Felleman [<xref ref-type="bibr" rid="pcbi-0030062-b028">28</xref>] used visual stimuli composed of colored and oriented bars resembling those used in experiments on visual search. In some stimuli the target popped out easily (e.g., the target had a different color or orientation from all the background elements), whereas in others, the target was more difficult to detect, and did not pop out (e.g., a color-orientation conjunction search, where the target is defined by a specific combination of orientation and color). They found that the responses of the V1 cells, which are tuned to both orientation and color to some degree, to the pop-out targets were not necessarily higher than responses to non-pop-out targets, and thus raising doubts regarding whether bottom-up saliency is generated in V1. However, these doubts do not disprove the V1 saliency hypothesis since the hypothesis does not predict that the responses to pop-out targets in some particular input images would be higher than the responses to non-pop-out targets in other input images. For a target to pop out, the response to the target should be substantially higher than the responses to all the background elements. The absolute level of the response to the target is irrelevant: what matters is the relative activations evoked by the target and background. Since Hegde and Felleman [<xref ref-type="bibr" rid="pcbi-0030062-b028">28</xref>] did not measure the responses to the background elements, their findings do not tell us whether V1 activities contribute to saliency. It is likely that the responses to the background elements were higher for the conjunction search stimuli, because each background element differed greatly from many of its neighbors, and, as for the target, there would have been weak iso-feature suppression on neurons responding to the background elements. On the other hand, each background element in the pop-out stimuli always had at least one feature (color or orientation) the same as all of its neighbors, so iso-feature suppression would have reduced the responses to the background elements, making them substantially lower than the response to the target. Meanwhile, it remains difficult to test the V1 saliency hypothesis physiologically when the input stimuli are more complex than those of the singleton pop-out conditions.</p><p>Psychophysical experiments provide an alternative means to ascertain V1′s role in bottom-up salience. While previous works [<xref ref-type="bibr" rid="pcbi-0030062-b020">20</xref>–<xref ref-type="bibr" rid="pcbi-0030062-b023">23</xref>] have shown that the V1 mechanisms can plausibly explain the commonly known behavioral data on visual search and segmentation, it is important to generate from the V1 saliency hypothesis behavioral predictions that are hitherto unknown experimentally so as to test the hypothesis behaviorally. This hypothesis testing is very feasible for the following reasons. There are few free parameters in the V1 saliency hypothesis since (1) most of the relevant physiological mechanisms in V1 are established experimental facts that can be modeled but not arbitrarily distorted, and (2) the only theoretical input is the hypothesis that the RF location of the most responsive V1 cell to a scene is the most likely selected. Consequently, the predictions from this hypothesis can be made precise, making the hypothesis falsifiable. One such psychophysical test confirming a prediction has been reported recently [<xref ref-type="bibr" rid="pcbi-0030062-b029">29</xref>]. The current work aims to test the hypothesis more systematically, by providing nontrivial predictions that are more indicative of the particular nature of the V1 saliency hypothesis and the V1 mechanisms.</p><p>For our purpose, we first review the relevant V1 mechanisms in the rest of the Introduction section. The Results section reports the derivations and tests of the predictions. The Discussion section will discuss related issues and implications of our findings, discuss possible alternative explanations for the data, and compare the V1 saliency hypothesis with traditional saliency models [<xref ref-type="bibr" rid="pcbi-0030062-b018">18</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b019">19</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b030">30</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b031">31</xref>] that were motivated more by the behavioral data [<xref ref-type="bibr" rid="pcbi-0030062-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b005">5</xref>] than by their physiological basis.</p><p>The relevant V1 mechanisms for the saliency hypothesis are the RFs and contextual influences. Each V1 cell [<xref ref-type="bibr" rid="pcbi-0030062-b032">32</xref>] responds only to a stimulus within its classical receptive field (CRF). Input at one location <italic>x</italic> evokes responses (<italic>O<sub>i</sub></italic>,<italic>O<sub>j</sub></italic>…) from multiple V1 cells <italic>i, j,</italic>… having overlapping RFs covering <italic>x</italic>. Each cell is tuned to one or more particular features including orientation, color, motion direction, size, and depth, and increases its response monotonically with the input strength and resemblance of the stimulus to its preferred feature. We call cells tuned to more than one feature dimension conjunctive cells [<xref ref-type="bibr" rid="pcbi-0030062-b023">23</xref>]; e.g., a vertical rightward conjunctive cell is simultaneously tuned to rightward motion and vertical orientation [<xref ref-type="bibr" rid="pcbi-0030062-b032">32</xref>], a red horizontal cell to red color and horizontal orientation [<xref ref-type="bibr" rid="pcbi-0030062-b033">33</xref>]. Hence, for instance, a red vertical bar could evoke responses from a vertical tuned cell, a red tuned cell, a red vertical conjunctive cell, and another cell preferring orientation two degrees from vertical but having an orientation tuning width of 15°, etc. The V1 saliency hypothesis states that the saliency of a visual location is dictated by the response of the most active cell responding to it [<xref ref-type="bibr" rid="pcbi-0030062-b020">20</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b023">23</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b034">34</xref>], <inline-formula id="pcbi-0030062-ex004"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex004" xlink:type="simple"/></inline-formula>
				, rather than the sum of the responses <inline-formula id="pcbi-0030062-ex005"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex005" xlink:type="simple"/></inline-formula>
				 to this location. This makes the selection easy and fast, since it can be done by a single operation to find the most active V1 cell (<inline-formula id="pcbi-0030062-ex006"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex006" xlink:type="simple"/></inline-formula>
				) responding to any location and any feature(s). We will refer to saliency by the maximum response, <inline-formula id="pcbi-0030062-ex007"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex007" xlink:type="simple"/></inline-formula>
				 as the MAX rule, to saliency by the summed response <inline-formula id="pcbi-0030062-ex008"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex008" xlink:type="simple"/></inline-formula>
				 as the SUM rule. It will be clear later that the SUM rule is not supported, or is less supported by data, nor is it favored by computational considerations (see <xref ref-type="sec" rid="s3">Discussion</xref>).
			</p><p>Meanwhile, intracortical interactions between neurons make a V1 cell's response context-dependent, a necessary condition for signaling saliency, since, e.g., a red item is salient in a green but not in a red context. The dominant contextual influence is the iso-feature suppression mentioned earlier, so that a cell responding to its preferred feature will be suppressed when there are surrounding inputs of the same or similar feature. Given that each input location will evoke responses from many V1 cells, and that responses are context-dependent, the highest response to each location to determine saliency will also be context-dependent. For example, the saliency of a red vertical bar could be signaled by the vertical tuned cell when it is surrounded by red horizontal bars, since the red tuned cell is suppressed through iso-color suppression by other red tuned cells responding to the context. However, when the context contains green vertical bars, its saliency will be signaled by the red tuned cells. In another context, the red vertical conjunctive cell could be signaling the saliency. This is natural since saliency is meant to be context-dependent.</p><p>Additional contextual influences, weaker than the iso-feature suppression, are also induced by the intracortical interactions in V1. One is the colinear facilitation to a cell's response to an optimally oriented bar when a contextual bar is aligned to this bar as if they are both segments of a smooth contour [<xref ref-type="bibr" rid="pcbi-0030062-b024">24</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b025">25</xref>]. Hence, iso-orientation interaction, including both iso-orientation suppression and colinear facilitation, is not isotropic. Another contextual influence is the general, feature-unspecific, surround suppression to a cell's response by activities in nearby cells regardless of their feature preferences [<xref ref-type="bibr" rid="pcbi-0030062-b006">6</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b007">7</xref>]. This causes reduced responses by contextual inputs of any features, and interactions between nearby V1 cells tuned to different features.</p><p>The most immediate and indicative prediction from the hypothesis is that task-irrelevant features can interfere in tasks that rely significantly on saliency. This is because at each location, only the response of the most activated V1 cell determines the saliency. In particular, if cells responding to task-irrelevant features dictate saliencies at some spatial locations, the task-relevant features become “invisible” for saliency at these locations. Consequently, visual attention is misled to task-irrelevant locations, causing delay in task completion. Second, different V1 processes for different feature dimensions are predicted to lead to asymmetric interactions between features for saliency. Third, the spatial or global phenomena often associated with visual grouping are predicted. This is because the intracortical interactions depend on the relative spatial relationship between input features, particularly in a non-isotropic manner for orientation features, making saliency sensitive to spatial configurations, in addition to the densities, of inputs. These broad categories of predictions will be elaborated in the next section in various specific predictions, together with their psychophysical tests.</p></sec><sec id="s2"><title>Results</title><p>For visual tasks in which saliency plays a dominant or significant role, the transform from visual input to behavioral response, particularly in terms of the RT in performing a task, via V1 and other neural mechanisms, can be simplistically and phenomenologically modeled as follows for clarity of presentation.
				<disp-formula id="pcbi-0030062-e002"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030062.e002" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn><mml:mtext>responses&ensp;</mml:mtext><mml:mi mathvariant="bold">O</mml:mi><mml:mo>&equals;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:msub><mml:mi>O</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>visual input</mml:mtext><mml:mi mathvariant="bold">I</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">&alpha;</mml:mi><mml:mo>&equals;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>&alpha;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&alpha;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math> --></disp-formula>
				<disp-formula id="pcbi-0030062-e003"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030062.e003" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mtext>The saliency map SMAP</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&prop;</mml:mo><mml:msub><mml:mrow><mml:mi>max</mml:mi><mml:mo></mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>O</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math> --></disp-formula>
				<disp-formula id="pcbi-0030062-e004"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030062.e004" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mtext>RT</mml:mtext><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>response</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>SMAP</mml:mtext><mml:mo>;</mml:mo><mml:mi>&beta;</mml:mi><mml:mo>&equals;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>&beta;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&beta;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math> --></disp-formula>where <italic>f</italic><sub><italic>v</italic>1</sub>(.) models the transform from visual input <bold>I</bold> to V1 responses <bold>O</bold> via neural mechanisms parameterized by <bold>α</bold> describing V1′s RFs and intracortical interactions, while <italic>f</italic><sub>response</sub>(.) models the transform from the saliency map SMAP to RT via the processes parameterized by <bold><italic>β</italic></bold> modeling decision making, motor responses, and other factors beyond bottom-up saliency. Without quantitative knowledge of <bold><italic>β</italic></bold><italic>,</italic> it is sufficient for our purpose to assume a monotonic transform <italic>f</italic><sub>response</sub>(.) that gives a shorter RT to a higher saliency value at the task-relevant location, since more salient locations are more quickly selected. This is of course assuming that the RT is dominated by the time for visual selection by saliency, or that the additional time taken after visual selection and before the task response, say indicated by button press, is a roughly constant quantity that does not vary sufficiently with the different stimuli being compared in any particular experiment. For our goal to test the saliency hypothesis, we will select stimuli such that this assumption is practically valid (see <xref ref-type="sec" rid="s3">Discussion</xref>). Hence, all our predictions are qualitative; i.e., we predict a longer RT in one visual search task than that in another rather than the quantitative differences in these RTs. This does not mean that our predictions will be vague or inadequate for testing the V1 saliency hypothesis, since the predictions will be very precise by explicitly stating which tasks should require longer RTs than which other tasks, making them indicative of V1 mechanisms. Meanwhile, the qualitativeness makes the predictions robust and insensitive to variations in quantitative details parameterized by <bold>α</bold> of the underlying V1 mechanisms, such as the quantitative strengths of the lateral connections, provided that the qualitative facts of the V1 neural mechanisms are fixed or determined. Therefore, as will be clear below, our predictions can be derived and comprehensible merely from our qualitative knowledge of a few facts about V1; e.g., that neurons are tuned to their preferred features, that iso-feature suppression is the dominant form of contextual influences, that V1 cells tuned to color have larger RFs than cells tuned to orientation, etc, without resorting to quantitative model analysis or simulations which would only affect the quantitative but not the qualitative outcomes. Meanwhile, although one could quantitatively fit the model to behavioral RTs by tuning the parameters <bold>α</bold> and <bold><italic>β</italic></bold> (within the qualitative range), it adds no value since model fitting is typically possible given enough parameters, nor is it within the scope of this paper to construct a detailed simulation model that, for this purpose, would have to be more complex than the available V1 model for contextual influences [<xref ref-type="bibr" rid="pcbi-0030062-b021">21</xref>–<xref ref-type="bibr" rid="pcbi-0030062-b023">23</xref>]. Hence, we do not include quantitative model simulations in this study, which is only aimed at deriving and testing our qualitative predictions.
			</p><sec id="s2a"><title>Interference by Task-Irrelevant Features</title><p>Consider stimuli having two different features at each location, one task-relevant and the other task-irrelevant. For convenience, we call the V1 responses to the task-relevant and -irrelevant stimuli, relevant and irrelevant responses, respectively, and from the relevant and irrelevant neurons, respectively. If the irrelevant response(s) is stronger than the relevant response(s) at a particular location, this location's salience is dictated by the irrelevant response(s) according to the V1 saliency hypothesis, and the task-relevant features become “invisible” for saliency. In visual search and segmentation tasks that rely significantly on saliency to attract attention to the target or texture border, the task-irrelevant features are predicted to interfere with the task by directing attention irrelevantly or ineffectively.</p><p><xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref> shows the texture patterns (<xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>A–<xref ref-type="fig" rid="pcbi-0030062-g001">1</xref>C) to illustrate this prediction. Pattern A has a salient border between two iso-orientation textures of left oblique and right oblique bars, respectively, activating two populations of neurons each for one of the two orientations. Pattern B is a uniform texture of alternating horizontal and vertical bars, evoking responses from another two groups of neurons for horizontal and vertical orientations, respectively. When all bars are of the same contrast, the neural response from the corresponding neurons to each bar would be the same (ignoring neural noise) if there were no intracortical interactions giving rise to contextual influences. With iso-orientation suppression, neurons responding to the texture border bars in pattern A are more active than neurons responding to other bars in pattern A; this is because they receive iso-orientation suppression from fewer active neighboring neurons, since there are fewer neighboring bars of the same orientation. For ease of explanation, let us say the highest neural responses to a border bar and a background bar are ten and five spikes/second, respectively. This V1 response pattern makes the border more salient, so it pops out in a texture-segmentation task. Each bar in pattern B has the same number of iso-orientation neighbors as a texture border bar in pattern A, so it evokes a comparable level of (highest) V1 response, i.e., ten spikes/second, to that evoked by a border bar in pattern A. If patterns A and B are superimposed, to give pattern C, the composite pattern will activate all neurons responding to patterns A and B, each neuron responding approximately as it does to A or B alone (for simplicity, we omitted the general suppression between neurons tuned to different orientations, without changing our conclusion, see below). According to the V1 saliency hypothesis, the saliency at each texture element location is dictated by the most activated neuron there. Since the (relevant) response to each element of pattern A is lower than or equal to the (irrelevant) response to the corresponding element of pattern B, the saliency at each element location in pattern C is the same as for B, so there is no texture border highlight in such a composite stimulus, making texture segmentation difficult.</p><fig id="pcbi-0030062-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030062.g001</object-id><label>Figure 1</label><caption><title>Prediction of Interference by Task-Irrelevant Features, and Its Psychophysical Test</title><p>(A–C) Schematics of texture stimuli (extending continuously in all directions beyond the portions shown), each followed by schematic illustrations of its V1 responses, in which the orientation and thickness of a bar denote the preferred orientation and response level, respectively, of the activated neuron. Each V1 response pattern is followed below by a saliency map, in which the size of a disk, denoting saliency, corresponds to the response of the most activated neuron at the texture element location. The orientation contrasts at the texture border in (A) and everywhere in (B) lead to less suppressed responses to the stimulus bars since these bars have fewer iso-orientation neighbours to evoke iso-orientation suppression. The composite stimulus (C), made by superposing (A) and (B), is predicted to be difficult to segment, since the task-irrelevant features from (B) interfere with the task-relevant features from (A), giving no saliency highlights to the texture border.</p><p>(D,E) RTs (differently colored data points denote different subjects) for texture segmentation and visual search tasks testing the prediction. For each subject, RT for the composite condition is significantly higher (<italic>p</italic> &lt; 0.001). In all experiments in this paper, stimuli consist of 22 rows × 30 columns of items (of single or double bars) on a regular grid with unit distance 1.6° of visual angle.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030062.g001" xlink:type="simple"/></fig><p>For simplicity in our explanation, our analysis above included only the dominant form of contextual influence, the iso-feature suppression, but not the less dominant form of the contextual influence, the general surround suppression and colinear facilitation. Including the weaker forms of contextual influences, as in the real V1 or our model simulations [<xref ref-type="bibr" rid="pcbi-0030062-b021">21</xref>–<xref ref-type="bibr" rid="pcbi-0030062-b023">23</xref>], does not change our prediction here. So, for instance, general surround suppression between local neurons tuned to different orientations should reduce each neuron's response to pattern C from that to pattern A or B alone. Hence, the (highest) responses to the task-relevant bars in pattern C may be, say, eight and four spikes/second, respectively, at the border and background. Meanwhile, the responses to the task-irrelevant bars in pattern C should be, say, roughly eight spikes/second everywhere, leading to the same prediction of interference. In the rest of this paper, for ease of explanation without loss of generality or change of conclusions, we include only the dominant iso-feature suppression in our description of the contextual influences, and ignore the weaker or less dominant colinear facilitation and general surround suppression unless their inclusion makes a qualitative or relevant difference (as we will see in the section Emergent Grouping of Orientation Features by Spatial Configurations). For the same reason, our arguments do not detail the much weaker responses from cells not as responsive to the stimuli concerned, such as responses from motion direction selective cells to a nonmoving stimulus, or the response from a cell tuned to 22.5° to a texture element in pattern C composed of two intersecting bars oriented at 0° and 45°, respectively. (Jointly, the two bars resemble a single bar oriented at 22.5° only at a scale much larger or coarser than their own. Thus, the most activated cell tuned to 22.5° would have a larger RF, much of which would contain no (contrast or luminance) stimulus, leading to a response weaker than cells preferring both the scale and the orientation of the individual bars.) This is because these additional but nondominant responses at each location are “invisible” to saliency by the V1 saliency hypothesis and thus do not affect our conclusions.</p><p><xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>D shows that segmenting the composite texture C indeed takes much longer than segmenting the task-relevant component texture A, confirming the prediction. The RTs were taken in a task when subjects had to report the location of the texture border, as to the left or right of display center, as quickly as possible. (The actual stimuli used are larger, see <xref ref-type="sec" rid="s4">Materials and Methods</xref>.) In pattern C, the task-irrelevant horizontal and vertical features from component pattern B interfere with segmentation by relevant orientations from pattern A. Since pattern B has spatially uniform saliency values, the interference is not due to the noisy saliencies of the background [<xref ref-type="bibr" rid="pcbi-0030062-b019">19</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b035">35</xref>].</p><p>One may wonder whether each composite texture element in <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>C may be perceived by its average orientation at each location, see <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>F, thereby making the relevant orientation feature noisy to impair performance. <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>E demonstrates by our control experiment that this would not have caused as much impairment; RT for this stimulus is at least 37% shorter than that for the composite stimulus.</p><fig id="pcbi-0030062-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030062.g002</object-id><label>Figure 2</label><caption><title>Further Illustrations To Understand Interference by Task-Irrelevant Features</title><p>(A–C) As in <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>, the schematics of texture stimuli of various feature contrasts in task-relevant and -irrelevant features.</p><p>(D) Like (A), except that each bar is 10° from vertical, reducing orientation contrast to 20°.</p><p>(F) Derived from (C) by replacing each texture element of two intersecting bars by one bar whose orientation is the average of the original two intersecting bars.</p><p>(G–I) Derived from (A–C) by reducing the orientation contrast (to 20°) in the interfering bars, each is 10° from horizontal.</p><p>(J–L) Derived from (G–I) by reducing the task-relevant contrast to 20°.</p><p>(E) Plots the normalized RTs for three subjects, DY, EW, and TT, on stimuli (A,D,F,C,I,L) randomly interleaved within a session. Each normalized RT is obtained by dividing the actual RT by the RT (which are 471, 490, and 528 ms, respectively, for subjects DY, EW, and TT) of the same subject for stimulus (A).</p><p>For each subject, RT for (C) is significantly (<italic>p</italic> &lt; 0.001) higher than that for (A,D,F,I) by at least 95%, 56%, 59%, and 29%, respectively. Matched sample <italic>t</italic>-test across subjects shows no significant difference (<italic>p</italic> = 0.99) between RTs for stimuli (C) and (L).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030062.g002" xlink:type="simple"/></fig><p>If one makes the visual search analog of the texture segmentation tasks in <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>, by changing stimulus <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>A (and consequently stimulus <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>C) such that only one target of left- (or right-) tilted bar is in a background of right- (or left-) tilted bars, qualitatively the same result (<xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>E) is obtained. Note that the visual search task may be viewed as the extreme case of the texture-segmentation task when one texture region has only one texture element.</p><p>Note that, if saliency were computed by the SUM rule <inline-formula id="pcbi-0030062-ex009"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex009" xlink:type="simple"/></inline-formula>
					 (rather than the MAX rule) to sum the responses <italic>O<sub>i</sub></italic> from cells preferring different orientations at a visual location <italic>x,</italic> interference would not be predicted since the summed responses at the border would be greater than those in the background, preserving the border highlight. Here, the texture border highlight <italic>H<sub>border</sub></italic> (for visual selection) is measured by the difference <italic>H<sub>border</sub></italic> = <italic>R<sub>border</sub></italic> − <italic>R<sub>ground</sub></italic> between the (summed or maxed) response <italic>R<sub>border</sub></italic> to the texture border and the response <italic>R<sub>ground</sub></italic> to the background (where response <italic>R<sub>x</sub></italic> at location <italic>x</italic> means <inline-formula id="pcbi-0030062-ex010"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex010" xlink:type="simple"/></inline-formula>
					or <inline-formula id="pcbi-0030062-ex011"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex011" xlink:type="simple"/></inline-formula>
					, under the SUM or MAX rule, respectively). This is justified by the assumption that the visual selection is by the winner-take-all of the responses <italic>R<sub>x</sub></italic> in visual space <italic>x,</italic> hence the priority of selecting the texture border is measured by how much this response difference is compared with the level of noises in the responses. Consequently, the SUM rule applied to our example of response values gives the same border highlight <italic>H<sub>border</sub></italic> = 5 spikes/second with or without the task-irrelevant bars, while the MAX rule gives <italic>H<sub>border</sub></italic> = 0 and 5 spikes/second, respectively. If the border highlight is measured more conservatively by the ratio <italic>H<sub>border</sub></italic> = <italic>R<sub>border</sub>/R<sub>ground</sub></italic> (when a ratio <italic>H<sub>border</sub></italic> = 1 means no border highlight), then the SUM rule predicts, in our particular example, <italic>H<sub>border</sub></italic> = (10 + 10)/(5 + 10) = 4/3 with the irrelevant bars, and <italic>H<sub>border</sub></italic> = 10/5 = 2 without, and thus some degree of interference. However, we argue below that even this measure of <italic>H<sub>border</sub></italic> by the response ratio makes the SUM rule less plausible. Behavioral and physiological data suggest that, as long as the saliency highlight is above the just-noticable difference (JND, [<xref ref-type="bibr" rid="pcbi-0030062-b036">36</xref>]), a reduction in <italic>H<sub>border</sub></italic> should not increase RT as dramatically as observed in our data. In particular, previous findings [<xref ref-type="bibr" rid="pcbi-0030062-b036">36</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b037">37</xref>] and our data (in <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>E) suggest that the ease of detecting an orientation contrast (assessed using RT) does not reduce by more than a small fraction when the orientation contrast is reduced, say, from 90° to 20° as in <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>A and <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>D [<xref ref-type="bibr" rid="pcbi-0030062-b036">36</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b037">37</xref>], even though physiological V1 responses [<xref ref-type="bibr" rid="pcbi-0030062-b038">38</xref>] to these orientation contrasts suggest that a 90° orientation contrast would give a highlight of <italic>H</italic><sub>90°</sub> ∼ 2.25 and a 20° contrast would give <italic>H</italic><sub>20°</sub> ∼ 1.25 using the ratio measurement for highlights. (Jones et al. [<xref ref-type="bibr" rid="pcbi-0030062-b038">38</xref>] illustrated that the V1 response to a 90° and 20° orientation contrast, respectively, can be 45 and 25 spikes/second, respectively, over a background response of 20 spikes/second.) Hence, the very long RT in our texture segmentation with interference implies that the border should have a highlight <italic>H<sub>border</sub></italic> ≈ 1 or below the JND, while a very easy segmentation without interference implies that the border should have <italic>H<sub>border</sub></italic> ≫ 1. If <italic>O<sub>border</sub></italic> and <italic>O<sub>ground</sub></italic> are the relevant responses to the border and background bars, respectively, for our stimulus, and since <italic>O<sub>border</sub></italic> also approximates the irrelevant response, then applying the SUM rule gives border highlight <italic>H<sub>border</sub></italic> = 2<italic>O<sub>border</sub></italic>/(<italic>O<sub>border</sub></italic> + <italic>O<sub>ground</sub></italic>) and <italic>O<sub>border</sub></italic>/<italic>O<sub>ground</sub>,</italic> with and without interference, respectively. Our RT data thus require that <italic>O<sub>border</sub></italic>/<italic>O<sub>ground</sub></italic> ≫ 1 and 2<italic>O<sub>border</sub></italic>/(<italic>O<sub>border</sub></italic> + <italic>O<sub>ground</sub></italic>) ≈ 1 should be satisfied simultaneously—this is difficult since <italic>O<sub>border</sub></italic>/<italic><sub>Oground</sub></italic> &gt; 2 means 2<italic>O<sub>border</sub></italic>/(<italic>O<sub>border</sub></italic> + <italic>O<sub>ground</sub></italic>) &gt; 4/3, and a larger <italic>O<sub>border</sub></italic>/<italic>O<sub>ground</sub></italic> would give a larger 2<italic>O<sub>border</sub></italic>/(<italic>O<sub>border</sub></italic> + <italic>O<sub>ground</sub></italic>), making the SUM rule less plausible. Meanwhile, the MAX rule gives a border highlight <italic>H<sub>border</sub></italic> = <italic>O<sub>border</sub></italic>/<italic>O<sub>border</sub></italic> = 1 with interference and <italic>H<sub>border</sub></italic> = <italic>O<sub>border</sub></italic>/<italic>O<sub>ground</sub></italic> &gt; 1 without. These observations strongly favor the MAX over the SUM rule, and we will show more data to differentiate the two rules later.
				</p><p>From our analysis above, we can see that the V1 saliency hypothesis also predicts a decrease of the interference if the irrelevant feature contrast is reduced, as demonstrated when comparing <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>G–<xref ref-type="fig" rid="pcbi-0030062-g002">2</xref>I with <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>A–<xref ref-type="fig" rid="pcbi-0030062-g002">2</xref>C, and confirmed in our data (<xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>E). The neighboring irrelevant bars in <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>I are more similarly oriented, inducing stronger iso-feature suppression between them, and decreasing their evoked responses, say, from ten to seven spikes/second. (Although colinear facilitation is increased by this stimulus change, since iso-orientation suppression dominates colinear facilitation physiologically, the net effect is decreased responses to all the task-irrelevant bars.) Consequently, the relevant texture border highlights are no longer submerged by the irrelevant responses. The degree of interference would be much weaker, though still nonzero, since the irrelevant responses (of seven spikes/second) still dominate the relevant responses (of five spikes/second) in the background, reducing the relative degree of border highlight from five to three spikes/second. Analogously, interference can be increased by decreasing task-relevant contrast, as demonstrated by comparing <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>J–<xref ref-type="fig" rid="pcbi-0030062-g002">2</xref>L and <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>G–<xref ref-type="fig" rid="pcbi-0030062-g002">2</xref>I, and confirmed in our data (<xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>E). Reducing the relevant contrast makes the relevant responses to the texture border weaker, say from ten to seven spikes/second, making these responses more vulnerable to being submerged by the irrelevant responses. Consequently, interference is stronger in <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>L than in <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>I. Essentially, the existence and strength of the interference depend on the relative response levels to the task-relevant and -irrelevant features, and these response levels depend on the corresponding feature contrasts and direct input strengths. When the relevant responses dictate saliency everywhere and their response values or overall response pattern are little affected by the existence or absence of the irrelevant stimuli, there should be little interference. Conversely, when the irrelevant responses dictate saliency everywhere, interference for visual selection is strongest. When the relevant responses dictate the saliency value at the location of the texture border or visual search target but not in the background of our stimuli, the degree of interference is intermediate. In both <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>C and <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>L, the irrelevant responses (approximately) dictate the saliency everywhere, so the texture borders are predicted to be equally nonsalient. This is confirmed across subjects in our data (<xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>E), although there is a large variation between subjects, perhaps because the bottom-up saliency is so weak in these two stimuli that subject specific top-down factors contribute significantly to the RTs.</p></sec><sec id="s2b"><title>The Color-Orientation Asymmetry in Interference</title><p>Can task-irrelevant features from another feature dimension interfere? <xref ref-type="fig" rid="pcbi-0030062-g003">Figure 3</xref>A illustrates orientation segmentation with irrelevant color contrasts. As in <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>, the irrelevant color contrast increases the responses to the color features since the iso-color suppression is reduced. At each location, the response to color could then compete with the response to the relevant orientation feature to dictate the saliency. In <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>C, the task-irrelevant features interfere because they evoke higher responses than the relevant features, as made clear by demonstrations in <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>. Hence, whether color can interfere with orientation or vice versa depends on the relative levels of V1 responses to these two feature types. Color and orientation are processed differently by V1 in two aspects. First, cells tuned to color, more than cells tuned to orientation, are usually in V1′s cytochrome oxidase–stained blobs which are associated with higher metabolic and neural activities [<xref ref-type="bibr" rid="pcbi-0030062-b039">39</xref>]. Second, cells tuned to color have larger RFs [<xref ref-type="bibr" rid="pcbi-0030062-b033">33</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b040">40</xref>]; hence, they are activated more by larger patches of color. In contrast, larger texture patches of oriented bars can activate more orientation-tuned cells, but do not make individual orientation-tuned cells more active. Meanwhile, in the stimulus for color segmentation (e.g., <xref ref-type="fig" rid="pcbi-0030062-g003">Figure 3</xref>B), each color texture region is large so that color-tuned cells are most effectively activated, making their responses easily the dominant ones. Consequently, the V1 saliency hypothesis predicts: (1) task-irrelevant colors are more likely to interfere with orientation than the reverse; (2) irrelevant color contrast from larger color patches can disrupt an orientation-based task more effectively than that from smaller color patches; and (3) the degree of interference by irrelevant orientation in color-based task will not vary with the patch size of the orientation texture.</p><fig id="pcbi-0030062-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030062.g003</object-id><label>Figure 3</label><caption><title>Interference between Orientation and Color, with Schematic Illustrations (Top [A,B]), and Stimuli/Data (Bottom [C–J])</title><p>(A) Orientation segmentation with irrelevant color.</p><p>(B) Color segmentation with irrelevant orientation.</p><p>(A,B) Larger patch sizes of irrelevant color gives stronger interference, but larger patch sizes of irrelevant orientation do not make interference stronger.</p><p>(C–E) Small portions of the actual experimental stimuli for orientation segmentation, without color contrast (C) or with irrelevant color contrast in 1 × 1 (D) or 2 × 2 (E) blocks. All bars had color saturation <italic>s<sub>uv</sub></italic> = 1, and were ±5° from horizontal.</p><p>(F) Normalized RTs for (C–E) for four subjects (different colors indicate different subjects). The “no”, “1 × 1”, and “2 × 2” on the horizontal axis mark stimulus conditions for (C–E), i.e., with no or n × n blocks of irrelevant features. The RT for condition “2 × 2” is significantly longer (<italic>p</italic> &lt; 0.05) than that for “no” in all subjects, and than that of “1 × 1” in three out of four subjects. By matched sample <italic>t</italic>-test across subjects, mean RTs are significantly longer in “2 × 2” than that in “no” (<italic>p</italic> = 0.008) and than that in “1 × 1” (<italic>p</italic> = 0.042). Each RT is normalized by dividing by the subject's mean RT for the “no” condition, which for the four subjects (AP, FE, LZ, NG) are 1170, 975, 539, and 1107 ms, respectively.</p><p>(G–J) Color segmentation, analogous to (C–F), with stimulus bars oriented ±45° and of color saturation <italic>s<sub>uv</sub></italic> = 0.5. Matched sample <italic>t</italic>-test across subjects showed no significant difference between RTs in different conditions. Only two out of four subjects had their RT significantly higher (<italic>p</italic> &lt; 0.05) in interfering than in no interfering conditions. The un-normalized mean RTs of the four subjects (ASL, FE, LZ, NG) in “no” condition are: 650, 432, 430, 446 ms, respectively.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030062.g003" xlink:type="simple"/></fig><p>These predictions are apparent when viewing <xref ref-type="fig" rid="pcbi-0030062-g003">Figure 3</xref>A and <xref ref-type="fig" rid="pcbi-0030062-g003">3</xref>B. They are confirmed by RT data for our texture segmentation task, shown in <xref ref-type="fig" rid="pcbi-0030062-g003">Figure 3</xref>C–<xref ref-type="fig" rid="pcbi-0030062-g003">3</xref>J. Irrelevant color contrast can indeed raise RT in orientation segmentation, but is effective only for sufficiently large color patches. In contrast, irrelevant orientation contrast does not increase RT in color segmentation regardless of the sizes of the orientation patches. In <xref ref-type="fig" rid="pcbi-0030062-g003">Figure 3</xref>C–<xref ref-type="fig" rid="pcbi-0030062-g003">3</xref>E, the irrelevant color patches are small, activating the color-tuned cells less effectively. However, interference occurs under small orientation contrast which reduces responses to relevant features (as demonstrated in <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>). Larger color patches can enable interference even to a 90° orientation contrast at the texture border, as apparent in <xref ref-type="fig" rid="pcbi-0030062-g003">Figure 3</xref>A, and has been observed by Snowden [<xref ref-type="bibr" rid="pcbi-0030062-b041">41</xref>]. In Snowden's design, the texture bars were randomly rather than regularly assigned one of two iso-luminant, task-irrelevant colors, giving randomly small and larger sizes of the color patches. The larger color patches made task-irrelevant locations salient to interfere with the orientation segmentation task. Previously, the V1 saliency hypothesis predicted that Snowden's interference should become stronger when there are more irrelevant color categories; e.g., each bar could assume one of three rather than two different colors. This is because more color categories further reduce the number of iso-color neighbors for each colored bar and thus the iso-color suppression, increasing responses to irrelevant color. This prediction was subsequently confirmed [<xref ref-type="bibr" rid="pcbi-0030062-b029">29</xref>].</p><p>In <xref ref-type="fig" rid="pcbi-0030062-g003">Figure 3</xref>G–<xref ref-type="fig" rid="pcbi-0030062-g003">3</xref>I, the relevant color contrast was made small to facilitate interference by irrelevant orientation, though unsuccessfully. Our additional data showed that orientation does not significantly interfere with color-based segmentation even when the color contrast was reduced further. The patch sizes, of 1 × 1 and 2 × 2, of the irrelevant orientation textures ensure that each bar in these patches evoke the same levels of responses, since each has the same number of iso-orientation neighbours (this would not hold when the patch size is 3 × 3 or larger). Such an irrelevant stimulus pattern evokes a spatially uniform level of irrelevant responses, thus ensuring that interference cannot possibly arise from non-uniform or noisy response levels to the background [<xref ref-type="bibr" rid="pcbi-0030062-b019">19</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b035">35</xref>]. Patch sizes for irrelevant colors in <xref ref-type="fig" rid="pcbi-0030062-g003">Figure 3</xref>C–<xref ref-type="fig" rid="pcbi-0030062-g003">3</xref>E were made to match those of irrelevant orientations in <xref ref-type="fig" rid="pcbi-0030062-g003">Figure 3</xref>G–<xref ref-type="fig" rid="pcbi-0030062-g003">3</xref>I, so as to compare saliency effects by color and orientation features. Note that, as discussed in the section Interference by Task-Irrelevant Features, the SUM rule would predict the same interference only if saliency highlight <italic>H<sub>border</sub></italic> is measured by the ratio between responses to the border and background. With this measure of <italic>H<sub>border</sub></italic>, our data in this subsection, showing that the interference only increases RT by a small fraction, cannot sufficiently differentiate the MAX from the SUM rule.</p></sec><sec id="s2c"><title>Advantage for Color-Orientation Double Feature but Not Orientation–Orientation Double Feature</title><p>A visual location can be salient due to two simultaneous feature contrasts. For instance, at the texture border between a texture of green, right-tilted bars and another texture of pink, left-tilted bars, in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>C, both the color and orientation contrast could make the border salient. We say that the texture border has a color-orientation double-feature contrast. Analogously, a texture border of an orientation–orientation double contrast, and the corresponding borders of single-orientation contrasts, can be made as in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>E–<xref ref-type="fig" rid="pcbi-0030062-g004">4</xref>G. We can ask whether the saliency of a texture border with a double-feature contrast can be higher than both of those of the corresponding single-feature–contrast texture borders. We show below that the V1 saliency hypothesis predicts a likely “yes” for color-orientation double feature but a definite “no” for orientation–orientation double feature.</p><fig id="pcbi-0030062-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030062.g004</object-id><label>Figure 4</label><caption><title>Small Portions of Actual Stimuli and Data in the Test of the Predictions of Saliency Advantage in Color-Orientation Double Feature (Left, [A–D]) and the Lack of It in Orientation–Orientation Double Feature (Right [E–H])</title><p>(A–C) Texture segmentation stimuli by color contrast, or orientation contrast, or by double color–orientation contrast.</p><p>(D) Normalized RTs for the stimulus conditions (A–C). Normalization for each subject is by whichever is the shorter mean RT (which for the subjects AL, AB, RK, and ZS are, respectively, 651, 888, 821, and 634) of the two single-feature contrast conditions. All stimulus bars had color saturation <italic>s<sub>uv</sub> =</italic> 0.2, and were <italic>±</italic>7.5° from horizontal. All subjects had their RT for the double-feature condition significantly shorter (<italic>p</italic> &lt; 0.001) than those of both single-feature conditions.</p><p>(E–G) Texture-segmentation stimuli by single- or double-orientation contrast, each oblique bar is <italic>±</italic>20° from vertical in (E) and <italic>±</italic>20° from horizontal in (F), and (G) is made by superposing the task-relevant bars in (E) and (F).</p><p>(H) Normalized RTs for the stimulus conditions (E–G) (analogous to [D]). The shorter mean RT among the two single-feature conditions are, for four subjects (LZ, EW, LJ, KC), 493, 688, 549, 998 ms, respectively. None of the subjects had RT for (G) lower than the minimum of the RT for (E) and (F). Averaged over the subjects, the mean normalized RT for the double-orientation feature in (G) is significantly longer (<italic>p</italic> &lt; 0.01) than that for the color-orientation double feature in (C).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030062.g004" xlink:type="simple"/></fig><p>V1 has color-orientation conjunctive cells that are tuned to both color and orientation, though their tuning to either feature is typically not as sharp as that of the single feature–tuned cells [<xref ref-type="bibr" rid="pcbi-0030062-b033">33</xref>]. Hence, a colored bar can activate a color-tuned cell, an orientation-tuned cell, and a color-orientation conjunctive cell, with cell outputs <italic>O<sub>c</sub>, O<sub>o</sub>,</italic> and <italic>O<sub>co</sub></italic>, respectively. The highest response max(<italic>O<sub>c</sub></italic>,<italic>O<sub>o</sub></italic>,<italic>O<sub>co</sub></italic>) from these cells should dictate the saliency of the bar's location. Let the triplet of response be <inline-formula id="pcbi-0030062-ex012"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex012" xlink:type="simple"/></inline-formula>
					 at an orientation texture border, <inline-formula id="pcbi-0030062-ex013"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex013" xlink:type="simple"/></inline-formula>
					 at a color border, and <inline-formula id="pcbi-0030062-ex014"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex014" xlink:type="simple"/></inline-formula>
					 at a color-orientation double-feature border. Due to iso-feature suppression, responses of a single feature cell is higher with than without its feature contrast, i.e., <inline-formula id="pcbi-0030062-ex015"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex015" xlink:type="simple"/></inline-formula>
					 and <inline-formula id="pcbi-0030062-ex016"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex016" xlink:type="simple"/></inline-formula>
					. The single-feature cells also have comparable responses with or without feature contrasts in other dimensions, i.e., <inline-formula id="pcbi-0030062-ex017"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex017" xlink:type="simple"/></inline-formula>
					 and <inline-formula id="pcbi-0030062-ex018"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex018" xlink:type="simple"/></inline-formula>
					. Meanwhile, the conjunctive cell should have a higher response at a double than a single feature border, i.e., <inline-formula id="pcbi-0030062-ex019"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex019" xlink:type="simple"/></inline-formula>
					 and <inline-formula id="pcbi-0030062-ex020"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex020" xlink:type="simple"/></inline-formula>
					, since it has fewer neighboring conjunctive cells responding to the same color and same orientation. The maximum <inline-formula id="pcbi-0030062-ex021"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex021" xlink:type="simple"/></inline-formula>
					 could be <inline-formula id="pcbi-0030062-ex022"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex022" xlink:type="simple"/></inline-formula>
					, or <inline-formula id="pcbi-0030062-ex023"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex023" xlink:type="simple"/></inline-formula>
					 to dictate the saliency of the double-feature border. Without detailed knowledge, we expect that it is likely that, in at least some nonzero percentage of many trials, <inline-formula id="pcbi-0030062-ex024"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex024" xlink:type="simple"/></inline-formula>
					 is the dictating response, and when this happens, <inline-formula id="pcbi-0030062-ex025"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex025" xlink:type="simple"/></inline-formula>
					 is larger than all responses from all cells to both single-feature contrasts. Consequently, averaged over trials, the double-feature border is likely more salient than both of the single-feature borders and thus should require a shorter RT to detect. In contrast, there are no V1 cells tuned conjunctively to two different orientations; hence, a double orientation–orientation border definitely cannot be more salient than both of the two single-orientation borders.
				</p><p>The above considerations have omitted the general suppression between cells tuned to different features. When this is taken into account, the single feature–tuned cells should respond less vigorously to a double feature than to the corresponding effective single feature contrast. This means, for instance, <inline-formula id="pcbi-0030062-ex026"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex026" xlink:type="simple"/></inline-formula>
					≲<inline-formula id="pcbi-0030062-ex027"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex027" xlink:type="simple"/></inline-formula>
					 and <inline-formula id="pcbi-0030062-ex028"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex028" xlink:type="simple"/></inline-formula>
					≲<inline-formula id="pcbi-0030062-ex029"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex029" xlink:type="simple"/></inline-formula>
					. This is because general suppression grows with the overall level of local neural activities. This level is higher with double-feature stimuli which activate some neurons more, e.g., when <inline-formula id="pcbi-0030062-ex030"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex030" xlink:type="simple"/></inline-formula>
					 and <inline-formula id="pcbi-0030062-ex031"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex031" xlink:type="simple"/></inline-formula>
					 (at the texture border). In the color-orientation double-feature case, <inline-formula id="pcbi-0030062-ex032"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex032" xlink:type="simple"/></inline-formula>
					≲<inline-formula id="pcbi-0030062-ex033"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex033" xlink:type="simple"/></inline-formula>
					 and <inline-formula id="pcbi-0030062-ex034"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex034" xlink:type="simple"/></inline-formula>
					≲<inline-formula id="pcbi-0030062-ex035"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex035" xlink:type="simple"/></inline-formula>
					 mean that <inline-formula id="pcbi-0030062-ex036"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex036" xlink:type="simple"/></inline-formula>
					 could not guarantee that <inline-formula id="pcbi-0030062-ex037"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex037" xlink:type="simple"/></inline-formula>
					 must be larger than all neural responses to both of the single feature borders. This consideration could somewhat weaken or compromise the double-feature advantage for the color-orientation case, and should make the double-orientation contrast less salient than the more salient one of the two single-orientation contrast conditions. In any case, the double-feature advantage in the color-orientation condition should be stronger than that of the orientation–orientation condition.
				</p><p>These predictions are indeed confirmed in the RT data. As shown in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>D and <xref ref-type="fig" rid="pcbi-0030062-g004">4</xref>H, the RT to locate a color-orientation double-contrast border <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>C is shorter than both RTs to locate the two single-feature borders <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>A and <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>B. Meanwhile, the RT to locate a double-orientation contrast of <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>G is no shorter than the shorter one of the two RTs to locate the two single-orientation contrast borders <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>E and <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>F. The same conclusion is reached (unpublished data) if the irrelevant bars in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>E or <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>F, respectively, have the same orientation as one of the relevant bars in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>F or <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>E, respectively. Note that, to manifest the double feature advantage, the RTs for the single-feature tasks should not be too short, since RT cannot be shorter than a certain limit for each subject. To avoid this RT floor effect, we have chosen sufficiently small feature contrasts to make RTs for the single-feature conditions longer than 450 ms for experienced subjects and even longer for inexperienced subjects.</p><p>Nothdurft [<xref ref-type="bibr" rid="pcbi-0030062-b042">42</xref>] also showed the saliency advantage of the double-feature contrast in color orientation. The shortening of RT by feature doubling can be viewed phenomenologically as a violation of a race model which models the task's RT as the outcome of a race between two response decision making processes by color and orientation features, respectively. This violation has been used to account for the double-feature advantage in RT also observed in visual search tasks when the search target differs in both color and orientation from uniform distractors observed previously [<xref ref-type="bibr" rid="pcbi-0030062-b043">43</xref>], and in our own data (<xref ref-type="table" rid="pcbi-0030062-t001">Table 1</xref>A). In our framework, we could interpret the RT for color-orientation double feature as a result from a race between three neural groups—the color-tuned, the orientation-tuned, and the conjunctive cells.</p><table-wrap content-type="1col" id="pcbi-0030062-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030062.t001</object-id><label>Table 1</label><caption><p>RTs (ms) in Visual Search for Unique Color and/or Orientation, Corresponding to Those in <xref ref-type="fig" rid="pcbi-0030062-g003">Figures 3</xref> and <xref ref-type="fig" rid="pcbi-0030062-g004">4</xref></p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030062.t001" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb1col1" align="left" charoff="0" char=""/><col id="tb1col2" align="left" charoff="0" char=""/><col id="tb1col3" align="left" charoff="0" char=""/><col id="tb1col4" align="left" charoff="0" char=""/><col id="tb1col5" align="left" charoff="0" char=""/><col id="tb1col6" align="left" charoff="0" char=""/><col id="tb1col7" align="left" charoff="0" char=""/><col id="tb1col8" align="left" charoff="0" char=""/><col id="tb1col9" align="left" charoff="0" char=""/><col id="tb1col10" align="left" charoff="0" char=""/><col id="tb1col11" align="left" charoff="0" char=""/><col id="tb1col12" align="left" charoff="0" char=""/></colgroup><thead><tr><td align="left">Subjects</td><td colspan="3"><hr/>(A) Single or Double Color- Orientation Contrast Search, Analogous to <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>A&ndash;<xref ref-type="fig" rid="pcbi-0030062-g004">4</xref>D</td><td colspan="3"><hr/>(B) Single or Double Orientation Contrast Search, Analogous to <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>E&ndash;<xref ref-type="fig" rid="pcbi-0030062-g004">4</xref>H</td><td colspan="2"><hr/>(C) Irrelevant Orientation in Color Search, Analogous to <xref ref-type="fig" rid="pcbi-0030062-g003">Figure 3</xref>G&ndash;<xref ref-type="fig" rid="pcbi-0030062-g003">3</xref>J</td><td colspan="3"><hr/>(D) Irrelevant Color in Orientation Search, Analogous to <xref ref-type="fig" rid="pcbi-0030062-g003">Figure 3</xref>C&ndash;<xref ref-type="fig" rid="pcbi-0030062-g003">3</xref>F</td></tr><tr><td><hr/></td><td><hr/>Color</td><td><hr/>Orientation</td><td><hr/>Color and Orientation</td><td><hr/>Single Contrast 1, as in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>E</td><td><hr/>Single Contrast 2, as in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>F</td><td><hr/>Double Contrast, as in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>G</td><td><hr/>No Irrelevant Contrast</td><td><hr/>1 &times; 1 Orientation Blocks</td><td><hr/>No Irrelevant Contrast</td><td><hr/>1 &times; 1 Color Blocks</td><td><hr/>2 &times;2 Color Blocks</td></tr></thead><tbody><tr><td>AP</td><td>512 &plusmn; 8 (1)</td><td>1378 &plusmn; 71 (1)</td><td>496 &plusmn; 7 (1)</td><td></td><td></td><td></td><td>804 &plusmn; 30 (0)</td><td>771 &plusmn; 29 (0)</td><td>&thinsp;&thinsp;811 &plusmn; 30 (0)</td><td>&thinsp;&thinsp;854 &plusmn; 38 (0)</td><td>&thinsp;&thinsp;872 &plusmn; 29 (0)</td></tr><tr><td>FE</td><td>529 &plusmn; 12 (1)</td><td>1509 &plusmn; 103 (3)</td><td>497 &plusmn; 12 (0)</td><td></td><td></td><td></td><td>506 &plusmn; 12 (5)</td><td>526 &plusmn; 12 (0)</td><td>1,048 &plusmn; 37 (0)</td><td>1,111 &plusmn; 34 (0)</td><td>1,249 &plusmn; 45 (2)</td></tr><tr><td>LZ</td><td>494 &plusmn; 11 (3)</td><td>&thinsp;&hairsp;846 &plusmn; 37 (4)</td><td>471 &plusmn; 7 (0)</td><td>732 &plusmn; 23 (1)</td><td>689 &plusmn; 18 (3)</td><td>731 &plusmn; 22 (1)</td><td>805 &plusmn; 26 (1)</td><td>893 &plusmn; 35 (5)</td><td>&thinsp;&thinsp;557 &plusmn; 13 (1)</td><td>&thinsp;&thinsp;625 &plusmn; 22 (1)</td><td>&thinsp;&thinsp;632 &plusmn; 21 (1)</td></tr><tr><td>NG</td><td>592 &plusmn; 29 (2)</td><td>&thinsp;&hairsp;808 &plusmn; 34 (4)</td><td>540 &plusmn; 19 (0)</td><td></td><td></td><td></td><td>644 &plusmn; 33 (1)</td><td>677 &plusmn; 34 (3)</td><td>&thinsp;&thinsp;681 &plusmn; 22 (1)</td><td>&thinsp;&thinsp;746 &plusmn; 27 (3)</td><td>&thinsp;&thinsp;734 &plusmn; 31 (1)</td></tr><tr><td>EW</td><td></td><td></td><td></td><td>688 &plusmn; 15 (0)</td><td>786 &plusmn; 20 (1)</td><td>671 &plusmn; 18 (2)</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table> --><!-- <table-wrap-foot><fn id="nt101"><p>Each data entry is: RT &plusmn; its standard error (percentage error rate).</p></fn><fn id="nt102"><p>In (A), orientation of background bars: <italic>&plusmn;</italic>45&deg; from vertical, orientation contrast: <italic>&plusmn;</italic>18&deg;, <italic>s<sub>uv</sub></italic> &equals; 1.5. In (B), stimuli are the visual search versions of <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>E&ndash;<xref ref-type="fig" rid="pcbi-0030062-g004">4</xref>G. In (A) and (B), normalized RT (normalized as in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>) for the double feature contrast is significantly (<italic>p</italic> &lt; 0.05) longer in (A) than that in (B). In (C), luminance of bars &equals; 1 cd/m<sup>2</sup>, <italic>s<sub>uv</sub></italic> &equals; 1.5, bar orientation: <italic>&plusmn;</italic>20&deg; from vertical or horizontal, irrelevant orientation contrast is 90&deg;. No significant difference (<italic>p &equals;</italic> 0.36) between RTs with and without irrelevant feature contrasts. In (D), orientation of background/target bars: &plusmn;/&mnplus;81&deg; from vertical, <italic>s<sub>uv</sub></italic> &equals; 1.5, RTs for stimuli with irrelevant color contrast (of either condition) are significantly longer (<italic>p</italic> &lt; 0.034) than those for stimuli without irrelevant color contrasts.</p></fn></table-wrap-foot> --></table-wrap><p>It is notable that the findings in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>H cannot be predicted from the SUM rule. With single- or double-orientation contrast, the (summed) responses to the background bars are approximately unchanged, since the iso-orientation suppression between various bars is roughly unchanged. Meanwhile, the total (summed) response to the border is larger when the border has double-orientation contrast (even considering the general, feature unspecific, suppression between neurons). Hence, the SUM rule would predict that the double-orientation contrast border is more salient than the single-contrast one, regardless of whether one measures the border highlight <italic>H<sub>border</sub></italic> by the difference or ratio between the summed response to the texture border and that to the background.</p></sec><sec id="s2d"><title>Emergent Grouping of Orientation Features by Spatial Configurations</title><p>Combining iso-orientation suppression and colinear facilitation, contextual influences between oriented bars depend non-isotropically on spatial relationships between the bars. Thus, spatial configurations of the bars can influence saliency in ways that cannot be simply determined by densities of the bars, and properties often associated with grouping can emerge. Patterns A–G in <xref ref-type="fig" rid="pcbi-0030062-g005">Figure 5</xref>A–<xref ref-type="fig" rid="pcbi-0030062-g005">5</xref>G are examples of these, and the RT to segment each texture will be denoted as RT<sub>A</sub>, RT<sub>B</sub>, … , RT<sub>G</sub>. Patterns A and B both have a 90° orientation contrast between two orientation textures. However, the texture border in B seems more salient. Patterns C and D are both made by adding, to A and B, respectively, task-irrelevant bars ±45° relative to the task-relevant bars and containing a 90° irrelevant orientation contrast. However, the interference is stronger in C than in D. Patterns E and G differ from C by having zero orientation contrast among the irrelevant bars, pattern F differs from D analogously. As demonstrated in <xref ref-type="fig" rid="pcbi-0030062-g002">Figure 2</xref>, the interference in E and G should thus be much weaker than that in C, and that in F much weaker than that in D. The irrelevant bars are horizontal in E and vertical in G, on the same original pattern A containing only the ±45° oblique bars. Nevertheless, segmentation seems easier in E than in G. These peculiar observations all seem to relate to what is often called visual “grouping” of elements by their spatial configurations, and can in fact be predicted from the V1 saliency hypothesis when considering that the contextual influences between oriented bars are non-isotropic. To see this, we need to abandon the simplification used so far to approximate contextual influences by only the dominant component—iso-feature suppression. Specifically, we now include in the contextual influences the subtler components: (1) facilitation between neurons responding to colinear neighboring bars and (2) general feature-unspecific surround suppression between nearby neurons tuned to any features.</p><fig id="pcbi-0030062-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030062.g005</object-id><label>Figure 5</label><caption><title>Demonstration and Testing the Predictions on Spatial Grouping</title><p>(A–G) Portions of different stimulus patterns used in the segmentation experiments. Each row starts with an original stimulus (left) without task-irrelevant bars, followed by stimuli when various task-irrelevant bars are superposed on the original.</p><p>(H) RT data when different stimulus conditions are randomly interleaved in experimental sessions. The un-normalized mean RT for four subjects (AP, FE, LZ, NG) in condition (A) are: 493, 465, 363, 351 ms. For each subject, it is statistically significant that RT<sub>C</sub> &gt; RT<sub>A</sub> (<italic>p</italic> &lt; 0.0005), RT<sub>D</sub> &gt; RT<sub>B</sub> (<italic>p</italic> &lt; 0.02), RT<sub>A</sub> &gt; RT<sub>B</sub> (<italic>p</italic> &lt; 0.05), RT<sub>A</sub> &lt; RT<sub>E</sub>, RT<sub>G</sub> (<italic>p</italic> &lt; 0.0005), RT<sub>D</sub> &gt; RT<sub>F</sub>, RT<sub>C</sub> &gt; RT<sub>E</sub>, RT<sub>G</sub> (<italic>p</italic> &lt; 0.02). In three out of four subjects, RT<sub>E</sub> &lt; RT<sub>G</sub> (<italic>p</italic> &lt; 0.01), and in two out of four subjects, RT<sub>B</sub> &lt; RT<sub>F</sub> (<italic>p</italic> &lt; 0.0005). Meanwhile, by matched sample <italic>t</italic>-tests across subjects, the mean RT values between any two conditions are significantly different (<italic>p</italic> smaller than values ranging from 0.0001 to 0.04).</p><p>(I) Schematics of responses from relevant (red) and irrelevant (blue) neurons, with (solid curves) and without (dot-dashed curves) considering general suppressions, for situations in (E–G). Interference from the irrelevant features arises from the spatial peaks in their responses away from the texture border.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030062.g005" xlink:type="simple"/></fig><p>Due to colinear facilitation, a vertical border bar in pattern B is salient not only because a neuron responding to it experiences weaker iso-orientation suppression, but also because it additionally enjoys full colinear facilitation due to the colinear contextual bars, whereas a horizontal border bar in B, or an oblique border bar in A, has only half as many colinear neighbors. Hence, in an orientation texture, the vertical border bars in B, and in general colinear border bars parallel to a texture border, are more salient than border bars not parallel to the border given the same orientation contrast at the border. Hence, if the highest response to each border bar in A is ten spikes/second, then the highest response to each border bar in B could be, say, 15 spikes/second. Indeed, RT<sub>B</sub> &lt; RT<sub>A</sub>, as shown in <xref ref-type="fig" rid="pcbi-0030062-g005">Figure 5</xref>H. (Wolfson and Landy [<xref ref-type="bibr" rid="pcbi-0030062-b044">44</xref>] observed a related phenomenon, more details in Li [<xref ref-type="bibr" rid="pcbi-0030062-b022">22</xref>]). Furthermore, the highly salient vertical border bars make segmentation less susceptible to interference by task-irrelevant features, since their evoked responses are more likely dominating to dictate salience. Hence, interference in D is much weaker than in C, even though the task-irrelevant orientation contrast is 90° in both C and D. Indeed, RT<sub>D</sub> &lt; RT<sub>C</sub> (<xref ref-type="fig" rid="pcbi-0030062-g005">Figure 5</xref>H), although RT<sub>D</sub> is still significantly longer than RT<sub>B</sub> without interference. All these are not due to any special status of the vertical orientation of the border bars in B and D, for rotating the whole stimulus patterns would not eliminate the effects. Similarly, when the task-irrelevant bars are uniformly oriented, as in patterns E and G (for A) and F (for B), the border in F is more salient than those in E and G, as confirmed by RT<sub>F</sub> &lt; RT<sub>E</sub> and RT<sub>G</sub>.</p><p>The “protruding through” of the vertical border bars in D likely triggers the sensation of the (task-irrelevant) oblique bars as grouped or belonging to a separate (transparent) surface. This sensation arises more readily when viewing the stimulus in a leisurely manner rather than in the hurried manner of an RT task. Based on the arguments that one usually perceives the “what” after perceiving the “where” of visual inputs [<xref ref-type="bibr" rid="pcbi-0030062-b045">45</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b046">46</xref>], we believe that this grouping arises from processes subsequent to the V1 saliency processing. Specifically, the highly salient vertical border bars are likely to define a boundary of a surface. Since the oblique bars are neither confined within the boundary nor occluded by the surface, they have to be inferred as belonging to another, overlaying (transparent), surface.</p><p>Given no orientation contrast between the task-irrelevant bars in E–G, the iso-orientation suppression among the irrelevant bars is much stronger than that in C and D, and is in fact comparable in strength to that among the task-relevant bars sufficiently away from the texture border. Hence, the responses to the task-relevant and -irrelevant bars are comparable in the background, and no interference would be predicted if we ignored general surround suppression between the relevant and irrelevant bars (detailed below). Indeed, RT<sub>E</sub>, RT<sub>G</sub> ≪ RT<sub>C</sub>, and RT<sub>F</sub> &lt; RT<sub>D</sub>.</p><p>However, the existence of general surround suppression introduces a small degree of interference, making RT<sub>E</sub>, RT<sub>G</sub> &gt; RT<sub>A</sub>, and RT<sub>F</sub> &gt; RT<sub>B</sub>. Consider E for example, let us say that, without considering the general surround suppression, the relevant responses are ten spikes/second and five spikes/second at the border and background, respectively, and the irrelevant responses are five spikes/second everywhere. The general surround suppression enables nearby neurons to suppress each other regardless of their feature preferences. Hence, spatial variations in the relevant responses cause complementary spatial variations in the irrelevant responses (even though the irrelevant inputs are spatially homogeneous); see <xref ref-type="fig" rid="pcbi-0030062-g005">Figure 5</xref>I for a schematic illustration. For convenience, denote the relevant and irrelevant responses at the border as <italic>O<sub>border</sub></italic>(<italic>r</italic>) and <italic>O<sub>border</sub></italic>(<italic>ir</italic>) respectively, and as <italic>O<sub>near</sub></italic>(<italic>r</italic>) and <italic>O<sub>near</sub></italic>(<italic>ir</italic>)<italic>,</italic> respectively, at locations near but somewhat away from the border. The strongest general suppression is from <italic>O<sub>border</sub></italic>(<italic>r</italic>) to <italic>O<sub>border</sub></italic>(<italic>ir</italic>)<italic>,</italic> reducing <italic>O<sub>border</sub></italic>(<italic>ir</italic>) to, say, four spikes/second. This reduction in turn causes a reduction of iso-orientation suppression on the irrelevant responses <italic>O<sub>near</sub></italic>(<italic>ir</italic>)<italic>,</italic> thus increasing <italic>O<sub>near</sub></italic>(<italic>ir</italic>) to, say, six spikes/second. The increase in <italic>O<sub>near</sub></italic>(<italic>ir</italic>) is also partly due to a weaker general suppression from <italic>O<sub>near</sub></italic>(<italic>r</italic>) (which is weaker than the relevant responses sufficiently away from the border because of the extra strong iso-orientation suppression from the very strong border responses <italic>O<sub>border</sub></italic>(<italic>r</italic>) [<xref ref-type="bibr" rid="pcbi-0030062-b047">47</xref>]). Mutual (iso-orientation) suppression between the irrelevant neurons is a positive feedback process that amplifies any response difference. Hence, the difference between <italic>O<sub>border</sub></italic>(<italic>ir</italic>) and <italic>O<sub>near</sub></italic>(<italic>ir</italic>) is amplified so that, say, <italic>O<sub>border</sub></italic>(<italic>ir</italic>) = 3 and <italic>O<sub>near</sub></italic>(<italic>ir</italic>) = 7 spikes/seconds, respectively. Therefore, <italic>O<sub>near</sub></italic>(<italic>ir</italic>) dominates <italic>O<sub>near</sub></italic>(<italic>r)</italic> somewhat away from the border, dictating and increasing the local saliency. As a result, the relative saliency of the border is reduced and some degree of interference arises, causing RT<sub>E</sub> &gt; RT<sub>A</sub>. The same argument leads similarly to conclusions RT<sub>G</sub> &gt; RT<sub>A</sub> and RT<sub>F</sub> &gt; RT<sub>B</sub>, as seen in our data (<xref ref-type="fig" rid="pcbi-0030062-g005">Figure 5</xref>H). If colinear facilitation is not considered, the degree of interference in E and G should be identical, predicting RT<sub>E</sub> = RT<sub>G</sub>. As explained below, considering colinear facilitation additionally will predict RT<sub>E</sub> &lt; RT<sub>G</sub>, as seen in our data for three out of four subjects (<xref ref-type="fig" rid="pcbi-0030062-g005">Figure 5</xref>H). Stimuli E and G differ in the direction of the colinear facilitation between the irrelevant bars. The direction is across the border in E but along the border in G, and, unlike iso-orientation suppression, facilitation tends to equalize responses <italic>O<sub>near</sub>(ir)</italic> and <italic>O<sub>border</sub>(ir)</italic> to the colinear bars. This reduces the spatial variation of the irrelevant responses across the border in E such that, say<italic>, O<sub>border</sub>(ir)</italic> = 4 and <italic>O<sub>near</sub>(ir)</italic> = 6 spikes/second, thus reducing the interference.</p><p>The SUM rule (over V1′s neural responses) would predict qualitatively the same directions of RT variations between conditions in this section only when the texture border highlight <italic>H<sub>border</sub></italic> is measured by the ratio rather than by the difference between the (summed) response to the border and that to the background. However, using the same argument as in the section Interference by Task-Irrelevant Features, our quantitative data would make the SUM rule even more implausible than it is in that section (since, using the notations from that section, we note that <italic>O<sub>ground</sub></italic> approximates the irrelevant responses in E and G, whose weak interference would require a constraint of <italic>H<sub>border</sub></italic> = (<italic>O<sub>border</sub></italic> + <italic>O<sub>ground</sub></italic>)/2<italic>O<sub>ground</sub></italic> &gt; 1 + <italic>δ</italic> with <italic>δ</italic> ≫ 0, in addition to the other stringent constraints in that section that made the SUM rule less plausible).</p><p>We also carried out experiments in visual search tasks analogous to those in <xref ref-type="fig" rid="pcbi-0030062-g003">Figures 3</xref>–<xref ref-type="fig" rid="pcbi-0030062-g005">5</xref>, as we did in <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>E analogous to <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>D. Qualitatively the same results as those in <xref ref-type="fig" rid="pcbi-0030062-g003">Figures 3</xref> and <xref ref-type="fig" rid="pcbi-0030062-g004">4</xref> were found; see <xref ref-type="table" rid="pcbi-0030062-t001">Table 1</xref>. For visual search conditions corresponding to those in <xref ref-type="fig" rid="pcbi-0030062-g005">Figure 5</xref>, however, since there were no elongated texture borders in the stimuli, grouping effects arising from the colinear border, or as the result of the elongated texture border, are not predicted, and indeed, not reflected in the data; see <xref ref-type="table" rid="pcbi-0030062-t002">Table 2</xref>. This confirmed additionally that saliency is sensitive to spatial configurations of input items in the manner prescribed by V1 mechanisms.</p><table-wrap content-type="1col" id="pcbi-0030062-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030062.t002</object-id><label>Table 2</label><caption><p>RTs (ms) for Visual Search for Unique Orientation, Corresponding to Data in <xref ref-type="fig" rid="pcbi-0030062-g005">Figure 5</xref>H</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030062.t002" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb2col1" align="left" charoff="0" char=""/><col id="tb2col2" align="left" charoff="0" char=""/><col id="tb2col3" align="left" charoff="0" char=""/><col id="tb2col4" align="left" charoff="0" char=""/><col id="tb2col5" align="char" charoff="0" char="plusmn"/><col id="tb2col6" align="char" charoff="0" char="plusmn"/></colgroup><thead><tr><td align="left" rowspan="2"><hr/>Conditions</td><td colspan="5"><hr/>Subjects</td></tr><tr><td><hr/>AP</td><td><hr/>FE</td><td><hr/>LZ</td><td><hr/>NG</td><td><hr/>ASL</td></tr></thead><tbody><tr><td>(A)</td><td>&thinsp;&thinsp;485 &plusmn; 8 (0.00)</td><td>&thinsp;&thinsp;478 &plusmn; 6 (0.00)</td><td>363 &plusmn; 2 (0.00)</td><td>366 &plusmn; 3 (1.04)</td><td>&thinsp;&thinsp;621 &plusmn; 19 (0.00</td></tr><tr><td>(B)</td><td>&thinsp;&thinsp;479 &plusmn; 9 (0.00)</td><td>&thinsp;&thinsp;462 &plusmn; 6 (0.00)</td><td>360 &plusmn; 2 (0.00</td><td>&thinsp;&thinsp;364 &plusmn; 3 (0.00)</td><td>&thinsp;&thinsp;592 &plusmn; 16 (1.04)</td></tr><tr><td>(C)</td><td>3,179 &plusmn; 199 (6.25)</td><td>2,755 &plusmn; 280 (5.21)</td><td>988 &plusmn; 50 (3.12)</td><td>1,209 &plusmn; 62 (2.08)</td><td>2,238 &plusmn; 136 (11.46)</td></tr><tr><td>(D)</td><td>1,295 &plusmn; 71 (1.04)</td><td>1,090 &plusmn; 53 (5.21)</td><td>889 &plusmn; 31 (3.12)</td><td>&thinsp;&thinsp;665 &plusmn; 22 (2.08)</td><td>1,410 &plusmn; 74 (4.17)</td></tr><tr><td>(E)</td><td>&thinsp;&thinsp;623 &plusmn; 20 (0.00)</td><td>&thinsp;&thinsp;707 &plusmn; 19 (0.00)</td><td>437 &plusmn; 9 (1.04)</td><td>&thinsp;&thinsp;432 &plusmn; 7 (1.04)</td><td>&thinsp;&thinsp;838 &plusmn; 35 (0.00)</td></tr><tr><td>(F)</td><td>&thinsp;&thinsp;642 &plusmn; 20 (0.00)</td><td>&thinsp;&thinsp;743 &plusmn; 21 (0.00)</td><td>481 &plusmn; 12 (3.12)</td><td>&thinsp;&thinsp;456 &plusmn; 9 (2.08)</td><td>&thinsp;&thinsp;959 &plusmn; 40 (1.04)</td></tr><tr><td>(G)</td><td>&thinsp;&thinsp;610 &plusmn; 21 (0.00)</td><td>&thinsp;&thinsp;680 &plusmn; 23 (0.00)</td><td>443 &plusmn; 10 (2.08)</td><td>&thinsp;&thinsp;459 &plusmn; 12 (2.08)</td><td>1,042 &plusmn; 48 (3.12)</td></tr></tbody></table> --><!-- <table-wrap-foot><fn id="nt201"><p>Stimulus conditions (A&ndash;G) are, respectively, the visual search versions of the stimulus conditions (A&ndash;G) in <xref ref-type="fig" rid="pcbi-0030062-g005">Figure 5</xref>. For each subject, no significant difference between RT<sub>A</sub> and RT<sub>B</sub> (<italic>p &gt;</italic> 0.05). Irrelevant bars in (C&ndash;G) increase RT significantly (<italic>p</italic> &lt; 0.01). All subjects as a group, no significant difference between RT<sub>E</sub> and RT<sub>G</sub> (<italic>p</italic> &equals; 0.38); RT<sub>C</sub> &gt; RT<sub>D</sub> significantly (<italic>p</italic> &lt; 0.02); RT<sub>C</sub>, RT<sub>D</sub> &gt; RT<sub>E</sub>, RT<sub>F</sub>, RT<sub>G</sub> significantly (<italic>p</italic> &lt; 0.01). Each data entry is: RT &plusmn; its standard error (percentage error rate).</p></fn></table-wrap-foot> --></table-wrap></sec></sec><sec id="s3"><title>Discussion</title><p>In summary, we tested and confirmed several predictions from the hypothesis of a bottom-up saliency map in V1. All these predictions are explicit since they rely on the known V1 mechanisms and an explicit assumption of a MAX rule, <inline-formula id="pcbi-0030062-ex038"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex038" xlink:type="simple"/></inline-formula>
				; i.e., among all responses <italic>O<sub>i</sub></italic> to a location <italic>x,</italic> only the most active V1 cell responding to this location determines its saliency. In particular, the predicted interference by task-irrelevant features and the lack of saliency advantage for orientation–orientation double features are specific to this hypothesis since they arise from the MAX rule. The predictions of color-orientation asymmetry in interference, the violation in the RT for color-orientation double feature of a race model between color and orientation features, the increased interference by larger color patches, and the grouping by spatial configurations, stem one way or another from specific V1 mechanisms. Hence, our experiments provided direct behavioral test and support of the hypothesis.
			</p><p>As mentioned in the Interference by Task-Irrelevant Features, the predicted and observed interference by irrelevant features, particularly those in <xref ref-type="fig" rid="pcbi-0030062-g001">Figures 1</xref> and <xref ref-type="fig" rid="pcbi-0030062-g002">2</xref>, cannot be explained by any background “noise” introduced by the irrelevant features [<xref ref-type="bibr" rid="pcbi-0030062-b019">19</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b035">35</xref>], since the irrelevant features in our stimuli have a spatially regular configuration and thus would by themselves evoke a spatially uniform or non-noisy response.</p><p>The V1 saliency hypothesis does not specify which cortical areas read out the saliency map. A likely candidate is the superior colliculus which receives input from V1 and directs eye movements [<xref ref-type="bibr" rid="pcbi-0030062-b048">48</xref>]. Indeed, microstimulation of V1 makes monkeys saccade to the RF location of the stimulated cell [<xref ref-type="bibr" rid="pcbi-0030062-b026">26</xref>], and such saccades are believed to be mediated by the superior colliculus.</p><p>While our experiments support the V1 saliency hypothesis, the hypothesis itself does not exclude the possibility that other visual areas contribute additionally to the computation of bottom-up saliency. Indeed, the superior colliculus receives inputs also from other visual areas [<xref ref-type="bibr" rid="pcbi-0030062-b048">48</xref>]. For instance, Lee et al. [<xref ref-type="bibr" rid="pcbi-0030062-b049">49</xref>] showed that pop-out of an item due to its unique lighting direction is associated more with higher neural activities in V2 than those in V1. It is not inconceivable that V1′s contribution to bottom-up saliency is mainly for the time duration immediately after exposure to the visual inputs. With a longer latency, especially for inputs when V1 signals alone are too equivocal to select the salient winner within that time duration, it is likely that the contribution from higher visual areas will increase. This is a question that can be answered empirically through additional experiments (e.g., [<xref ref-type="bibr" rid="pcbi-0030062-b050">50</xref>]) beyond the scope of this paper. These contributions from higher visual areas to bottom-up saliency are in addition to the top-down selection mechanisms that further involve mostly higher visual areas [<xref ref-type="bibr" rid="pcbi-0030062-b051">51</xref>–<xref ref-type="bibr" rid="pcbi-0030062-b053">53</xref>]. The feature-blind nature of the bottom-up V1 selection also does not prevent top-down selection and attentional processing from being feature selective [<xref ref-type="bibr" rid="pcbi-0030062-b018">18</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b054">54</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b055">55</xref>], so that, for example, the texture border in <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>C could be located through feature scrutiny or recognition rather than saliency.</p><p>It is notable that while we assume that our RT data are adequate to test bottom-up saliency mechanisms, our stimuli remained displayed until the subjects responded by button press, i.e., for a duration longer than the time necessary for neural signals to propagate to higher level brain areas and feedback to V1. Although physiological observations [<xref ref-type="bibr" rid="pcbi-0030062-b056">56</xref>] indicate that preparation for motor responses contribute a long latency and variations in RTs, our work needs to be followed up in the future to further validate our hopeful assumption that our RT data sufficiently manifest bottom-up saliency to be adequate for our purpose. We argue that to probe the bottom-up processing behaviorally, requiring subjects to respond to a visual stimulus (which stays on before the response) as soon as possible, is one of the most suitable methods. We believe that this method should be more suitable than an alternative method to present stimulus briefly, with, or especially without, requiring the subjects to respond as soon as possible. After all, turning off the visual display does not prevent the neural signals evoked by the turned-off display from being propagated to and processed by higher visual areas [<xref ref-type="bibr" rid="pcbi-0030062-b057">57</xref>], and, if anything, it reduces the weight of stimulus-driven or bottom-up activities relative to the internal brain activities. Indeed, it is not uncommon for subjects to experience in RT tasks that they could not cancel their erroneous responses in time even though the error was realized way before the response completion and at the initiation of the response according to EEG data [<xref ref-type="bibr" rid="pcbi-0030062-b058">58</xref>], suggesting that the commands for the responses were issued considerably before the completion of the responses.</p><p>Traditionally, there have been other frameworks for visual saliency [<xref ref-type="bibr" rid="pcbi-0030062-b018">18</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b019">19</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b030">30</xref>], mainly motivated by and developed from behavioral data [<xref ref-type="bibr" rid="pcbi-0030062-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b005">5</xref>] when there was less knowledge of their physiological basis. Focusing on their bottom-up aspect, these frameworks can be paraphrased as follows. Visual inputs are analyzed by separate feature maps, e.g., red feature map, green feature map, vertical, horizontal, left-tilt, and right-tilt feature maps, etc., in several basic feature dimensions such as orientation, color, and motion direction. The activation of each input feature in its feature map decreases roughly with the number of the neighboring input items sharing the same feature. Hence, in an image of a vertical bar among horizontal bars, the vertical bar evokes a higher activation in the vertical feature map than that by each of the many horizontal bars in the horizontal map. The activations in separate feature maps are summed to produce a master saliency map. Accordingly, the vertical bar produces the highest activation at its location in this master map and attracts visual selection. The traditional theories have been subsequently made more explicit and implemented by computer algorithms [<xref ref-type="bibr" rid="pcbi-0030062-b031">31</xref>]. When applied to the stimulus in <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>C, it becomes clear that the traditional theories correspond to the SUM rule <inline-formula id="pcbi-0030062-ex039"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030062.ex039" xlink:type="simple"/></inline-formula>
				 for saliency determination when different responses <italic>O<sub>i</sub></italic> to different orientations at the same location <italic>x</italic> represent responses from different feature maps. As argued, our data (in the sections Interference from Task-Irrelevant Features, The Color Orientation Asymmetry in Interference, and Emergent Grouping of Orientation Features by Spatial Configurations) on interference by task-irrelevant features are incompatible with or unfavorable for the SUM rule, and our data (in the section Advantage for Color-Orientation Double Feature but Not Orientation–Orientation Double Feature) on the lack of advantage for the double-orientation contrast are contrary to the SUM rule. Many of our predictions from the V1 saliency hypothesis, such as the color-orientation asymmetry in the section The Color Orientation Asymmetry in Interference and the section Advantage for Color-Orientation Double Feature but Not Orientation–Orientation Double Feature, and the emergent grouping phenomenon in the section Emergent Grouping of Orientation Features by Spatial Configuration arise specifically from V1 mechanisms, and could not be predicted by traditional frameworks without adding additional mechanisms or parameters. The traditional framework also contrasted with the V1 saliency hypothesis by implying that the saliency map should be in higher-level cortical areas where neurons are untuned to features, motivating physiological experiments searching for saliency correlates in areas such as the lateral intraparietal area which, downstream from V1, could reflect bottom-up saliences in its neural activities [<xref ref-type="bibr" rid="pcbi-0030062-b059">59</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b060">60</xref>]. Nevertheless, the traditional frameworks have provided an overall characterization of previous behavioral data on bottom-up saliency. These behavioral data provided part of the basis on which the V1 theory of saliency was previously developed and tested by computational modeling [<xref ref-type="bibr" rid="pcbi-0030062-b020">20</xref>–<xref ref-type="bibr" rid="pcbi-0030062-b023">23</xref>].
			</p><p>One may seek alternative explanations for our observations predicted by the V1 saliency hypothesis. For instance, to explain interference in <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>C, one may assign a new feature type to “two bars crossing each other at 45°,” so that each texture element has a feature value (orientation) of this new feature type. Then, each texture region in <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>C is a checkerboard pattern of two different feature values of this feature type. So the segmentation could be more difficult in <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>C, just like it could be more difficult to segment a texture of “ABABAB” from another of “CDCDCD” in a stimulus pattern “ABABABABABCDCDCDCDCD” than to segment “AAA” from “CCC” in “AAAAAACCCCCC.” This approach of creating new feature types to explain hitherto unexplained data could of course be extended to accommodate other new data. So for instance, new stimuli can easily be made such that new feature types may have to include other double feature conjunctions (e.g., color-orientation conjunction), triple, quadruple, and other multiple feature conjunctions, or even complex stimuli like faces, and it is not clear how long this list of new feature types needs to be. Meanwhile, the V1 saliency hypothesis is a more parsimonious account since it is sufficient to explain all the data in our experiments without evoking additional free parameters or mechanisms. It was also used to explain visual searches for, e.g., a cross among bars or an ellipse among circles without any detectors for crosses or circles/ellipses [<xref ref-type="bibr" rid="pcbi-0030062-b020">20</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b023">23</xref>]. Hence, we aim to explain the most data by the fewest necessary assumptions or parameters. Additionally, the V1 saliency hypothesis is a neurally based account. When additional data reveal the limitation of V1 for bottom-up saliency, searches for additional mechanisms for bottom-up saliency can be guided by following the neural basis suggested by the visual pathways and the cortical circuit in the brain [<xref ref-type="bibr" rid="pcbi-0030062-b048">48</xref>].</p><p>Computationally, bottom-up visual saliency serves to guide visual selection or attention to a spatial location to give further processing of the input at that location. Therefore, by nature of its definition, bottom-up visual saliency is computed before the input objects are identified, recognized, or decoded from the population of (V1) neural responses to various primitive features and their combinations. More explicitly, recognition or decoding from (V1) responses requires knowing <italic>both</italic> the response levels <italic>and</italic> the preferred features of the responding neurons, while saliency computation requires only the former. Hence, saliency computation is less sophisticated than object identification; it can thus be achieved more quickly (this is consistent with previous observations and arguments that segmenting or knowing “where is the input” is before or faster than classifying “what is the input” [<xref ref-type="bibr" rid="pcbi-0030062-b045">45</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b046">46</xref>]), as well as more easily impaired or susceptible to noise. On the one hand, the noise susceptibility can be seen as a weakness or a price paid for a faster computation; on the other, a more complete computation at the bottom-up selection level would render the subsequent, attentive, processing more redundant. This is particularly relevant when considering whether the MAX rule or the SUM rule, or some other rule (such as a response power summation rule) in between these two extremes, is more suitable for saliency computation. The MAX rule to guide selection can be easily implemented in a fast and feature-blind manner, in which a saliency map readout area (e.g., the superior colliculus) can simply treat the neural responses in V1 as values in a universal currency bidding for visual selection, to select (stochastically or deterministically) the RF location of the highest bidding neuron [<xref ref-type="bibr" rid="pcbi-0030062-b034">34</xref>]. The SUM rule, or for the same reason the intermediate rule, is much more complicated to implement. The RFs of many (V1) neurons covering a given location are typically non-identically shaped and/or sized, and many are only partially overlapping. It would be nontrivial to compute how to sum the responses from these neurons, whether to sum them linearly or nonlinearly, and whether to sum them with equal or non-equal weights of which values. More importantly, we should realize that these responses should not be assumed as being evoked by the same visual object—imagine an image location around a green leaf floating on a golden pond above an underlying dark fish—deciding whether and how to sum the response of a green tuned cell and that of a vertical tuned cell (which could be responding to the water ripple, the leaf, or the fish) would likely require assigning the green feature and the vertical feature to their respective owner objects, i.e., to solve the feature-binding problem. A good solution to this assignment or summation problem would be close to solving the object-identification problem, making the subsequent attentive processing, after selection by saliency, redundant. These computational considerations against the SUM rule are also in line with the finding that statistical properties of natural scenes also favor the MAX rule [<xref ref-type="bibr" rid="pcbi-0030062-b061">61</xref>]. While our psychophysical data also favor the MAX over the SUM rule, it is currently difficult to test conclusively whether our data could be better explained by an intermediate rule. This is because, with the saliency map SMAP, RT = <italic>f</italic>(SMAP, <bold><italic>β</italic></bold>) (see <xref ref-type="disp-formula" rid="pcbi-0030062-e004">Equation 4</xref>) depend on decision making and motor response processes parameterized by <bold><italic>β</italic></bold>. Let us say that, given V1 responses <bold><italic>O</italic></bold><italic>,</italic> the saliency map is, generalizing from <xref ref-type="disp-formula" rid="pcbi-0030062-e003">Equation 3</xref>, SMAP = SMAP(<bold><italic>O</italic></bold>, γ), where γ is a parameter indicating whether SMAP is made by the MAX rule or its softer version as an intermediate between MAX and SUM. Then, without precise (quantitative) details of <bold><italic>O</italic></bold> and <bold>β</bold>, γ cannot be quantitatively determined. Nevertheless, our data in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>H favor a MAX rather than an intermediate rule for the following reasons. The response level to each background texture bar in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>E–<xref ref-type="fig" rid="pcbi-0030062-g004">4</xref>G is roughly the same among the three stimulus conditions, regardless of whether the bar is relevant or irrelevant, since each bar experiences roughly the same level of iso-orientation suppression. Meanwhile, let the relevant and irrelevant responses to the border bars be <italic>O<sub>E</sub></italic>(<italic>r</italic>) and <italic>O<sub>E</sub></italic>(<italic>ir</italic>)<italic>,</italic> respectively, for <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>E, and <italic>O<sub>F</sub></italic>(<italic>r</italic>) and <italic>O<sub>F</sub></italic>(<italic>ir</italic>)<italic>,</italic> respectively, for <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>F. Then the responses to the two sets of border bars in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>G are approximately <italic>O<sub>E</sub></italic>(<italic>r</italic>) and <italic>O<sub>F</sub></italic>(<italic>r</italic>), ignoring, as an approximation, the effect of increased level of general surround suppression due to an increased level of local neural activities. Since both <italic>O<sub>E</sub></italic>(<italic>r</italic>) and <italic>O<sub>F</sub></italic>(<italic>r</italic>) are larger than both <italic>O<sub>E</sub></italic>(<italic>ir</italic>) and <italic>O<sub>F</sub></italic>(<italic>ir</italic>), an intermediate rule (unlike the MAX rule) combining the responses to two border bars would yield a higher saliency for the border in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>G than for those in <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>E and <xref ref-type="fig" rid="pcbi-0030062-g004">Figure 4</xref>F, contrary to our data. This argument, however, cannot conclusively reject the intermediate rule, especially one that closely resembles the MAX rule, since our approximation to omit the effect of the change in general surround suppression may not hold.</p><p>Due to the difference between the computation for saliency and that for discrimination, it is not possible to predict discrimination performance from visual saliency. In particular, visual saliency computation could not predict subjects' sensitivities, e.g., their d prime values, to discriminate between two texture regions (or to discriminate the texture border from the background). In our stimuli, the differences between texture elements in different texture regions are far above the discrimination threshold with or without task-irrelevant features. Thus, if instead of an RT task, subjects performed texture discrimination without time pressure in their responses, their performance will not be sensitive to the presence of the irrelevant features (even for briefly presented stimuli) since the task essentially probes the visual process for discrimination rather than saliency. Therefore, our experiments to measure RT in a visual segmentation or search task, requiring subjects to respond quickly regarding “where” rather than “what” about the visual input by pressing a button located congruently with “where,” using trivially discriminable stimuli, are designed to probe bottom-up saliency rather than the subsequent object recognition (identification) or discrimination performance. This design assumes that a higher saliency of the texture border or the search target makes its selection easier and thus faster, manifesting in a shorter RT. This is why our findings in RTs cannot be explained by models of texture discrimination (e.g., [<xref ref-type="bibr" rid="pcbi-0030062-b062">62</xref>]), which are based on discriminating or identifying texture features, i.e., based on visual processing after visual selection by saliency. While our subjects gave different RTs to different stimuli, their response error rates are typically very small (&lt;5%) to all stimuli—as our RT task is not to measure discrimination sensitivities (or d prime values). For the same reason, if one were to explain the interference in <xref ref-type="fig" rid="pcbi-0030062-g001">Figure 1</xref>C by the noise added by the task-irrelevant features, this feature noise would not be strong enough to sufficiently affect the error rate, since the feature differences (between those of the irrelevant and relevant features) are many times larger than the just-noticeable feature difference for feature discrimination. Of course, some visual search tasks, especially those using hardly discriminable stimuli, rely more on the recognition and/or less on bottom-up saliency computation. These tasks, while interesting to study for other purposes, would not be suitable for testing hypotheses on the bottom-up saliency, and we expect that cortical areas beyond V1 would be more involved for them and would have to read out from V1 the preferred features (labeled lines) <italic>and</italic> activities of more <italic>and</italic> less active neurons (i.e., beyond reading out the SMAP).</p><p>Our observations are related to Gestalt principles of perceptual organization and many previous observations of visual grouping and emergent properties [<xref ref-type="bibr" rid="pcbi-0030062-b063">63</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b064">64</xref>]. This suggests that V1 mechanisms could be the neural basis for many grouping phenomena, as has been shown in some examples [<xref ref-type="bibr" rid="pcbi-0030062-b047">47</xref>,<xref ref-type="bibr" rid="pcbi-0030062-b065">65</xref>]. For instance, the main Gestalt principle of grouping by similarity is related to iso-feature suppression in V1, since iso-feature suppression, responsible for feature singleton pop-out, also makes a region of items of similar features less salient apart from the region border, which bounds, and induces the perception of, the region as a whole. Similarly, the principle of grouping by proximity is related to the finite length of the intracortical connections in V1 for contextual influences, and the principle of grouping by good continuation is related to the colinear facilitation in V1. Pomerantz [<xref ref-type="bibr" rid="pcbi-0030062-b063">63</xref>] showed that certain features, particularly ones involving spatial properties such as orientation, interact in complex ways to produce emergent perceptual configurations that are not simply the sum of parts. One of his notable examples of what is termed “configuration superiority effect” is shown in <xref ref-type="fig" rid="pcbi-0030062-g006">Figure 6</xref>. One stimulus of a left-tilted bar among three right-tilted bars becomes a composite stimulus of a triangle among three arrows, when a non-informative stimulus of four identical “L”-shaped items is added. As a result, the triangle is easier to detect among the arrows than the left-tilted bar among right-tilt ones in the original stimulus, as if the triangle is an emergent new feature. This superiority effect by spatial configurations of bars, the opposite of interference by irrelevant features in our data, could be accounted for by the following mechanism beyond V1. The added irrelevant “L”s made the target triangle shape unique, while the original target bar was a rotated version of the bar distractors. It was recently shown [<xref ref-type="bibr" rid="pcbi-0030062-b066">66</xref>] that, when the bottom-up saliency is not sufficiently high (as manifested in the longer-than-1,000-ms RTs in Pomerantz's data, likely due to a small set size), object rotational invariance between target and distractors could introduce object-to-feature interference to drastically prolong RT. This interference is because the original target, identically shaped as distractors, is confused as a distractor object. Whereas Gestalt principles and many psychological studies of emergent phenomena have provided excellent summaries and descriptions of a wealth of data, the V1 mechanisms provide explanations behind at least some of these data.</p><fig id="pcbi-0030062-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0030062.g006</object-id><label>Figure 6</label><caption><title>Illustration of Pomerantz's Configuration Superiority Effect</title><p>The triangle is easier to detect among the three arrow shapes in the composite stimulus, than the left-tilted bar among the right-tilted bars in the original stimulus. Identical shape of the target and distractor bars in the original stimulus could lead to confusion and longer RT.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030062.g006" xlink:type="simple"/></fig><p>Meanwhile, the psychological data in the literature, including the vast wealth of data on visual grouping, can in turn predict the physiology and anatomy of V1 through the V1 saliency hypothesis, thus providing opportunities to further test the hypothesis through physiological/anatomical experiments. Such tests should help to explore the potentials and the limitations of the V1 mechanisms to explain the bottom-up selection factors. For example, knowing that color-orientation conjunctive search is difficult (e.g., [<xref ref-type="bibr" rid="pcbi-0030062-b037">37</xref>], searching for a red vertical target among red horizontal and green vertical distractors) and that color-orientation double feature is advantageous allow us to predict that, in V1, intracortical (disynaptic) suppressive connections should link conjunctive cells with other cells preferring either the same color and/or the same orientation. Data by Hegde and Felleman [<xref ref-type="bibr" rid="pcbi-0030062-b028">28</xref>] are consistent with this prediction, although more direct and systematic tests of the prediction are desirable. Similarly, the ease to search for a unique motion–orientation (or motion–form) conjunction predicts [<xref ref-type="bibr" rid="pcbi-0030062-b023">23</xref>] that V1 cells tuned to motion–orientation conjunctions tend to connect to other cells preferring both the same orientation and the same motion direction.</p><p>The V1 mechanisms for bottom-up saliency also have implications for mechanisms of top-down attention. First, if V1 creates a bottom-up saliency map for visual selection, then it would not be surprising that subsequent cortical areas/stages receiving input from V1 should manifest much interaction between bottom-up and top-down selectional and attentional factors. Second, by the V1 saliency hypothesis, the most active V1 cell attracts attention automatically to its RF location. This cell may be tuned to one or a few feature dimensions. Its response does not provide information about other feature dimensions to which it is un-tuned. Thus, such a bottom-up selection does not bind different features at the same location, and the top-down attention may have to bind the features subsequently [<xref ref-type="bibr" rid="pcbi-0030062-b004">4</xref>]. Meanwhile, the conjunctive cells in V1 bind two (or more) features at the same location into a single cell by default (which may or may not be veridical). This suggests that top-down attentional mechanisms are required to determine, from the responses of the conjunctive and nonconjunctive cells, not only the relative strengths of the two features, but also whether the two features belong to the same objects or whether the two features need to be unbound. Our findings reported here should motivate researchers in new directions for research into the mechanisms and frameworks of bottom-up and top-down attentional selection, and post-selectional processes for problems including feature binding.</p></sec><sec id="s4"><title>Materials and Methods</title><sec id="s4a"><title>Stimuli.</title><p>In all our experiments, each stimulus pattern had 22 rows × 30 columns of items (of single or double bars) on a regular grid with unit distance 1.6° of visual angle. Each bar was a white (CIE illuminant C), 1.2 × 0.12 degree rectangle (for experiments in orientation feature dimensions only), or a colored 1.2 × 0.24 degree rectangle (for experiments involving color and orientation features). All bars had a luminance of 14 cd/m<sup>2</sup> unless otherwise stated, and the background was black. The colored bars were green or pink specified by their CIE 1976 coordinates (u′,v′), with hue angles <italic>h<sub>uv</sub></italic> = 130° or 310°, respectively, where tan(<italic>h<sub>uv</sub></italic>) = (<italic>v</italic>′ – <italic>v<sub>n</sub></italic>′)/(<italic>u</italic>′ – <italic>u<sub>n</sub></italic>′), and (<italic>u<sub>n</sub></italic>′, <italic>v<sub>n</sub></italic>′) are the coordinates of CIE illuminant C (0.201, 0.461). All bars within a stimulus had the same saturation
					<disp-formula id="pcbi-0030062-eq001"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030062.eq001" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>&equals;</mml:mo><mml:mn>13</mml:mn><mml:msqrt><mml:mrow><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>n</mml:mi><mml:mo>&prime;</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>&plus;</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>n</mml:mi><mml:mo>&prime;</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow><mml:mtext>.</mml:mtext></mml:math> --></disp-formula>For segmentation experiments, the vertical texture border between two texture regions was located randomly left or right, at 7, 9, or 11 interelement distances laterally from the display centre. Stimuli in search tasks were made analogously to those in texture-segmentation tasks, by reducing one of the two texture regions into a single target item. In each trial, the target was positioned randomly in one of the middle 14 rows; given the target's row number, its column number was such that the target was positioned randomly left or right, as close as possible to 16.8° of visual angle from the display centre. The noncoloured bars are oriented either as specified in captions of the figures and tables presented, or are oriented horizontally, vertically, or ±45° from vertical. The color and orientation of the target or left texture region in each trial were randomly green or pink (for colored stimuli) and left- or right-tilted (or horizontal or vertical) in the relevant orientations.
				</p></sec><sec id="s4b"><title>Subjects.</title><p>Subjects are adults with normal or corrected to normal vision, and they are identified by letters, such as LZ, in the figures and tables. Most subjects are naive to the purpose of the study, except for LZ (one of the authors), LJ, and ASL. Some subjects are more experienced at RT tasks than others. AP, FE, LZ, NG, and ASL participated in more experiments than others (such as KC, DY, and EW) who only participated in one or a few experiments.</p></sec><sec id="s4c"><title>Procedure and data analysis.</title><p>Subjects were instructed to fixate centrally until stimulus onset, to freely move their eyes afterward, and to press a left or right key (located to their left or right hand side) using their left or right hand, respectively, quickly and accurately to indicate whether the target or texture border (present in each trial) was in the left or right half of the display. The stimulus pattern stayed after onset until the subject's response. There were 96 trials per subject per stimulus conditions shown. Average RTs were calculated (and shown in the figures and tables) excluding trials that were erroneous or had an RT outside three standard deviations from the mean. The number of such excluded trials was usually less than 5% of the total for each subject and condition, and our results did not change qualitatively even when we included all trials in calculating RTs or considered the speed–accuracy tradeoff in performances. The error bars shown are standard errors. The experiments were carried out in a dark room. Within each figure plot, and each part (A, B, C, etc.) of <xref ref-type="table" rid="pcbi-0030062-t001">Table 1</xref> or <xref ref-type="table" rid="pcbi-0030062-t002">Table 2</xref>, all the stimulus conditions were randomly interleaved within an experimental session such that the subjects could not predict before each trial which stimulus condition would appear. For texture segmentation, the subjects were told to locate the border between two textures regardless of the difference (e.g., whether in color or orientation or both) between the two textures. For visual search, the subjects were told to locate the target which had a unique feature (such as orientation, color, or both, regardless of which orientation(s) and/or which color), i.e., the odd one out, within the display. The subjects were shown examples of the relevant stimulus conditions to understand the task before the data-taking. Experiments (e.g., the one for <xref ref-type="fig" rid="pcbi-0030062-g005">Figure 5</xref>) requiring more than 300–400 trials in total were broken down to multiple data–taking sessions such that each session typically took 10–20 minutes.</p></sec></sec></body><back><ack><p>We thank colleagues such as Neil Burgess, Peter Dayan, Michael Eisele, Nathalie Guyader, Michael Herzog, Alex Lewis, JingLing Li, Christoph Nothdurft, and Jeremy Wolfe for reading the draft versions of the manuscript and/or very helpful comments. We also thank the three anonymous reviewers for very helpful comments and Steward Shipp for help on references.</p></ack><glossary><title>Abbreviations</title><def-list><def-item><term>RF</term><def><p>receptive field</p></def></def-item><def-item><term>RT</term><def><p>reaction time</p></def></def-item><def-item><term>VI</term><def><p>primary visual cortex</p></def></def-item></def-list></glossary><ref-list><title>References</title><ref id="pcbi-0030062-b001"><label>1</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Jonides</surname><given-names>J</given-names></name></person-group>
					<year>1981</year>
					<article-title>Voluntary versus automatic control over the mind's eye's movement.</article-title>
					<comment>In</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Long</surname><given-names>JB</given-names></name><name name-style="western"><surname>Baddeley</surname><given-names>AD</given-names></name></person-group>
					<source>Attention and performance IX</source>
					<publisher-loc>Hillsdale (New Jersey)</publisher-loc>
					<publisher-name>Lawrence Erlbaum Associates</publisher-name>
					<fpage>187</fpage>
					<lpage>203</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b002"><label>2</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Nakayama</surname><given-names>K</given-names></name><name name-style="western"><surname>Mackeben</surname><given-names>M</given-names></name></person-group>
					<year>1989</year>
					<article-title>Sustained and transient components of focal visual attention.</article-title>
					<source>Visual Res</source>
					<volume>29</volume>
					<fpage>1631</fpage>
					<lpage>1647</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b003"><label>3</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Yantis</surname><given-names>S</given-names></name></person-group>
					<year>1998</year>
					<article-title>Control of visual attention.</article-title>
					<comment>In</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Pashler</surname><given-names>H</given-names></name></person-group>
					<source> Attention</source>
					<publisher-loc>Philadelphia (Pennsylvania)</publisher-loc>
					<publisher-name>Psychology Press</publisher-name>
					<fpage>223</fpage>
					<lpage>256</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b004"><label>4</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Treisman</surname><given-names>AM</given-names></name><name name-style="western"><surname>Gelade</surname><given-names>G</given-names></name></person-group>
					<year>1980</year>
					<article-title>A feature-integration theory of attention.</article-title>
					<source>Cognit Psychol</source>
					<volume>12</volume>
					<fpage>97</fpage>
					<lpage>136</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b005"><label>5</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Julesz</surname><given-names>B</given-names></name></person-group>
					<year>1981</year>
					<article-title>Textons, the elements of texture perception, and their interactions.</article-title>
					<source>Nature</source>
					<volume>290</volume>
					<fpage>91</fpage>
					<lpage>97</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b006"><label>6</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Allman</surname><given-names>J</given-names></name><name name-style="western"><surname>Miezin</surname><given-names>F</given-names></name><name name-style="western"><surname>McGuinness</surname><given-names>E</given-names></name></person-group>
					<year>1985</year>
					<article-title>Stimulus specific responses from beyond the classical receptive field: Neurophysiological mechanisms for local–global comparisons in visual neurons.</article-title>
					<source>Annu Rev Neurosci</source>
					<volume>8</volume>
					<fpage>407</fpage>
					<lpage>430</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b007"><label>7</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Knierim</surname><given-names>JJ</given-names></name><name name-style="western"><surname>Van Essen</surname><given-names>DC</given-names></name></person-group>
					<year>1992</year>
					<article-title>Neuronal responses to static texture patterns in area V1 of the alert macaque monkey.</article-title>
					<source>J Neurophysiol</source>
					<volume>67</volume>
					<fpage>961</fpage>
					<lpage>980</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b008"><label>8</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sillito</surname><given-names>AM</given-names></name><name name-style="western"><surname>Grieve</surname><given-names>KL</given-names></name><name name-style="western"><surname>Jones</surname><given-names>HE</given-names></name><name name-style="western"><surname>Cudeiro</surname><given-names>J</given-names></name><name name-style="western"><surname>Davis</surname><given-names>J</given-names></name></person-group>
					<year>1995</year>
					<article-title>Visual cortical mechanisms detecting focal orientation discontinuities.</article-title>
					<source>Nature</source>
					<volume>378</volume>
					<fpage>492</fpage>
					<lpage>496</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b009"><label>9</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Nothdurft</surname><given-names>HC</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name><name name-style="western"><surname>Van Essen</surname><given-names>DC</given-names></name></person-group>
					<year>1999</year>
					<article-title>Response modulation by texture surround in primate area V1: Correlates of “popout” under anesthesia.</article-title>
					<source>Vis Neurosci</source>
					<volume>16</volume>
					<fpage>15</fpage>
					<lpage>34</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b010"><label>10</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Nothdurft</surname><given-names>HC</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name><name name-style="western"><surname>Van Essen</surname><given-names>DC</given-names></name></person-group>
					<year>2000</year>
					<article-title>Response profiles to texture border patterns in area V1.</article-title>
					<source>Vis Neurosci</source>
					<volume>17</volume>
					<fpage>421</fpage>
					<lpage>436</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b011"><label>11</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Jones</surname><given-names>HE</given-names></name><name name-style="western"><surname>Grieve</surname><given-names>KL</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W</given-names></name><name name-style="western"><surname>Sillito</surname><given-names>AM</given-names></name></person-group>
					<year>2001</year>
					<article-title>Surround suppression in primate V1.</article-title>
					<source>J Neurophysiol</source>
					<volume>86</volume>
					<fpage>2011</fpage>
					<lpage>2028</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b012"><label>12</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wachtler</surname><given-names>T</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name><name name-style="western"><surname>Albright</surname><given-names>TD</given-names></name></person-group>
					<year>2003</year>
					<article-title>Representation of color stimuli in awake macaque primary visual cortex.</article-title>
					<source>Neuron</source>
					<volume>37</volume>
					<fpage>681</fpage>
					<lpage>691</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b013"><label>13</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gilbert</surname><given-names>CD</given-names></name><name name-style="western"><surname>Wiesel</surname><given-names>TN</given-names></name></person-group>
					<year>1983</year>
					<article-title>Clustered intrinsic connections in cat visual cortex.</article-title>
					<source>J Neurosci</source>
					<volume>3</volume>
					<fpage>1116</fpage>
					<lpage>1133</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b014"><label>14</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Rockland</surname><given-names>KS</given-names></name><name name-style="western"><surname>Lund</surname><given-names>JS</given-names></name></person-group>
					<year>1983</year>
					<article-title>Intrinsic laminar lattice connections in primate visual cortex.</article-title>
					<source>J Comp Neurol</source>
					<volume>216</volume>
					<fpage>303</fpage>
					<lpage>318</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b015"><label>15</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hirsch</surname><given-names>JA</given-names></name><name name-style="western"><surname>Gilbert</surname><given-names>CD</given-names></name></person-group>
					<year>1991</year>
					<article-title>Synaptic physiology of horizontal connections in the cat's visual cortex.</article-title>
					<source>J Neurosci</source>
					<volume>11</volume>
					<fpage>1800</fpage>
					<lpage>1809</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b016"><label>16</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Reynolds</surname><given-names>JH</given-names></name><name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name></person-group>
					<year>2003</year>
					<article-title>Interacting roles of attention and visual salience in V4.</article-title>
					<source>Neuron</source>
					<volume>37</volume>
					<fpage>853</fpage>
					<lpage>863</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b017"><label>17</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Beck</surname><given-names>DM</given-names></name><name name-style="western"><surname>Kastner</surname><given-names>S</given-names></name></person-group>
					<year>2005</year>
					<article-title>Stimulus context modulates competition in human extra-striate cortex.</article-title>
					<source>Nat Neurosci</source>
					<volume>8</volume>
					<fpage>1110</fpage>
					<lpage>1116</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b018"><label>18</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wolfe</surname><given-names>JM</given-names></name><name name-style="western"><surname>Cave</surname><given-names>KR</given-names></name><name name-style="western"><surname>Franzel</surname><given-names>S L</given-names></name></person-group>
					<year>1989</year>
					<article-title>Guided search: An alternative to the feature integration model for visual search.</article-title>
					<source>J Exp Psychol</source>
					<volume>15</volume>
					<fpage>419</fpage>
					<lpage>433</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b019"><label>19</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Duncan</surname><given-names>J</given-names></name><name name-style="western"><surname>Humphreys</surname><given-names>GW</given-names></name></person-group>
					<year>1989</year>
					<article-title>Visual search and stimulus similarity.</article-title>
					<source>Psychol Rev</source>
					<volume>96</volume>
					<fpage>1</fpage>
					<lpage>26</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b020"><label>20</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z</given-names></name></person-group>
					<year>1999a</year>
					<article-title>Contextual influences in V1 as a basis for pop out and asymmetry in visual search.</article-title>
					<source>Proc Natl Acad Sci U S A</source>
					<volume>96</volume>
					<fpage>10530</fpage>
					<lpage>10535</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b021"><label>21</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z</given-names></name></person-group>
					<year>1999b</year>
					<article-title>Visual segmentation by contextual influences via intracortical interactions in primary visual cortex.</article-title>
					<source>Netw Comput Neural Syst</source>
					<volume>10</volume>
					<fpage>187</fpage>
					<lpage>212</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b022"><label>22</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z</given-names></name></person-group>
					<year>2000</year>
					<article-title>Pre-attentive segmentation in the primary visual cortex.</article-title>
					<source>Spatial Vision</source>
					<volume>13</volume>
					<fpage>25</fpage>
					<lpage>50</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b023"><label>23</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z</given-names></name></person-group>
					<year>2002</year>
					<article-title>A saliency map in primary visual cortex.</article-title>
					<source>Trends Cogn Sci</source>
					<volume>6</volume>
					<fpage>9</fpage>
					<lpage>16</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b024"><label>24</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Nelson</surname><given-names>JI</given-names></name><name name-style="western"><surname>Frost</surname><given-names>BJ</given-names></name></person-group>
					<year>1985</year>
					<article-title>Intracortical facilitation among co-oriented, co-axially aligned simple cells in cat striate cortex.</article-title>
					<source>Exp Brain Res</source>
					<volume>61</volume>
					<fpage>54</fpage>
					<lpage>61</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b025"><label>25</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Kapadia</surname><given-names>MK</given-names></name><name name-style="western"><surname>Ito</surname><given-names>M</given-names></name><name name-style="western"><surname>Gilbert</surname><given-names>CD</given-names></name><name name-style="western"><surname>Westheimer</surname><given-names>G</given-names></name></person-group>
					<year>1995</year>
					<article-title>Improvement in visual sensitivity by changes in local context: Parallel studies in human observers and in V1 of alert monkeys.</article-title>
					<source>Neuron</source>
					<volume>15</volume>
					<fpage>843</fpage>
					<lpage>856</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b026"><label>26</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Tehovnik</surname><given-names>EJ</given-names></name><name name-style="western"><surname>Slocum</surname><given-names>WM</given-names></name><name name-style="western"><surname>Schiller</surname><given-names>PH</given-names></name></person-group>
					<year>2003</year>
					<article-title>Saccadic eye movements evoked by micro-stimulation of striate cortex.</article-title>
					<source>Eur J Neurosci</source>
					<volume>17</volume>
					<fpage>870</fpage>
					<lpage>878</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b027"><label>27</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Super</surname><given-names>H</given-names></name><name name-style="western"><surname>Spekreijse</surname><given-names>H</given-names></name><name name-style="western"><surname>Lamme</surname><given-names>VA</given-names></name></person-group>
					<year>2003</year>
					<article-title>Figure–ground activity in primary visual cortex (V1) of the monkey matches the speed of behavioral response.</article-title>
					<source>Neurosci Lett</source>
					<volume>344</volume>
					<fpage>75</fpage>
					<lpage>78</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b028"><label>28</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hegde</surname><given-names>J</given-names></name><name name-style="western"><surname>Felleman</surname><given-names>DJ</given-names></name></person-group>
					<year>2003</year>
					<article-title>How selective are V1 cells for pop-out stimuli?</article-title>
					<source>J Neurosci</source>
					<volume>23</volume>
					<fpage>9968</fpage>
					<lpage>9980</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b029"><label>29</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Zhaoping</surname><given-names>L</given-names></name><name name-style="western"><surname>Snowden</surname><given-names>RJ</given-names></name></person-group>
					<year>2006</year>
					<article-title>A theory of a saliency map in primary visual cortex (V1) tested by psychophysics of color-orientation interference in texture segmentation.</article-title>
					<source>Visual Cogn</source>
					<volume>14</volume>
					<fpage>911</fpage>
					<lpage>933</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b030"><label>30</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name><name name-style="western"><surname>Ullman</surname><given-names>S</given-names></name></person-group>
					<article-title>(19850 Shifts in selective visual attention: Towards the underlying neural circuitry.</article-title>
					<source>Hum Neurobiol</source>
					<volume>4</volume>
					<fpage>219</fpage>
					<lpage>227</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b031"><label>31</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Itti</surname><given-names>L</given-names></name><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name></person-group>
					<year>2000</year>
					<article-title>A saliency-based search mechanism for overt and covert shifts of visual attention.</article-title>
					<source>Vision Res</source>
					<volume>40</volume>
					<fpage>1489</fpage>
					<lpage>1506</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b032"><label>32</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hubel</surname><given-names>DH</given-names></name><name name-style="western"><surname>Wiesel</surname><given-names>TN</given-names></name></person-group>
					<year>1968</year>
					<article-title>Receptive fields and functional architecture of monkey striate cortex.</article-title>
					<source>J Physiol</source>
					<volume>195</volume>
					<fpage>215</fpage>
					<lpage>243</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b033"><label>33</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Livingstone</surname><given-names>MS</given-names></name><name name-style="western"><surname>Hubel</surname><given-names>DH</given-names></name></person-group>
					<year>1984</year>
					<article-title>Anatomy and physiology of a color system in the primate visual cortex.</article-title>
					<source>J Neurosci</source>
					<volume>4</volume>
					<fpage>309</fpage>
					<lpage>356</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b034"><label>34</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Zhaoping</surname><given-names>L</given-names></name></person-group>
					<year>2005</year>
					<article-title>The primary visual cortex creates a bottom-up saliency map.</article-title>
					<comment>In</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Itti</surname><given-names>L</given-names></name><name name-style="western"><surname>Rees</surname><given-names>G</given-names></name><name name-style="western"><surname>Tsotsos</surname><given-names>JK</given-names></name></person-group>
					<source>Neurobiology of attention</source>
					<publisher-loc>London: Elsevier</publisher-loc>
					<fpage>570</fpage>
					<lpage>575</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b035"><label>35</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Rubenstein</surname><given-names>BS</given-names></name><name name-style="western"><surname>Sagi</surname><given-names>D</given-names></name></person-group>
					<year>1990</year>
					<article-title>Spatial variability as a limiting factor in texture-discrimination tasks: Implications for performance asymmetries.</article-title>
					<source>J Opt Soc Am A Opt Image Sci Vis</source>
					<volume>7</volume>
					<fpage>1632</fpage>
					<lpage>1643</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b036"><label>36</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Foster</surname><given-names>DH</given-names></name><name name-style="western"><surname>Ward</surname><given-names>PA</given-names></name></person-group>
					<year>1991</year>
					<article-title>Asymmetries in oriented-line detection indicate two orthogonal in early vision.</article-title>
					<source>Proc R Soc Lond B Biol Sci</source>
					<volume>243</volume>
					<fpage>75</fpage>
					<lpage>81</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b037"><label>37</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wolfe</surname><given-names>JM</given-names></name></person-group>
					<year>1998</year>
					<article-title>Visual Search, a review.</article-title>
					<comment>In</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Pashler</surname><given-names>H</given-names></name></person-group>
					<source>Attention</source>
					<publisher-loc>Hove (United Kingdom)</publisher-loc>
					<publisher-name>Psychology Press</publisher-name>
					<fpage>13</fpage>
					<lpage>74</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b038"><label>38</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Jones</surname><given-names>HE</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W</given-names></name><name name-style="western"><surname>Sillito</surname><given-names>AM</given-names></name></person-group>
					<year>2002</year>
					<article-title>Spatial organization and magnitude of orientation contrast interactions in primate V1.</article-title>
					<source>J Neurophysiol</source>
					<volume>88</volume>
					<fpage>2796</fpage>
					<lpage>2808</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b039"><label>39</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Deyoe</surname><given-names>EA</given-names></name><name name-style="western"><surname>Trusk</surname><given-names>TC</given-names></name><name name-style="western"><surname>Wong-Riley</surname><given-names>MT</given-names></name></person-group>
					<year>1995</year>
					<article-title>Activity correlates of cytochrome oxidase–defined compartments in granular and supergranular layers of primary visual cortex of the macaque monkey.</article-title>
					<source>Vis Neurosci</source>
					<volume>12</volume>
					<fpage>629</fpage>
					<lpage>639</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b040"><label>40</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z</given-names></name><name name-style="western"><surname>Atick</surname><given-names>J</given-names></name></person-group>
					<year>1994</year>
					<article-title>Towards a theory of striate cortex.</article-title>
					<source>Neural Comput</source>
					<volume>6</volume>
					<fpage>127</fpage>
					<lpage>146</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b041"><label>41</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Snowden</surname><given-names>RJ</given-names></name></person-group>
					<year>1998</year>
					<article-title>Texture segregation and visual search: A comparison of the effects of random variations along irrelevant dimensions.</article-title>
					<source>J Exp Psychol Hum Percept Perform</source>
					<volume>24</volume>
					<fpage>1354</fpage>
					<lpage>1367</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b042"><label>42</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Nothdurft</surname><given-names>HC</given-names></name></person-group>
					<year>2000</year>
					<article-title>Salience from feature contrast: Additivity across dimensions.</article-title>
					<source>Vision Res</source>
					<volume>40</volume>
					<fpage>1183</fpage>
					<lpage>1201</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b043"><label>43</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Krummenacher</surname><given-names>J</given-names></name><name name-style="western"><surname>Muller</surname><given-names>HJ</given-names></name><name name-style="western"><surname>Heller</surname><given-names>D</given-names></name></person-group>
					<year>2001</year>
					<article-title>Visual search for dimensionally redundant pop-out targets: Evidence for parallel-coactive processing of dimensions.</article-title>
					<source>Percept Psychophys</source>
					<volume>63</volume>
					<fpage>901</fpage>
					<lpage>917</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b044"><label>44</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wolfson</surname><given-names>SS</given-names></name><name name-style="western"><surname>Landy</surname><given-names>MS</given-names></name></person-group>
					<year>1995</year>
					<article-title>Discrimination of orientation-defined texture edges.</article-title>
					<source>Vision Res</source>
					<volume>35</volume>
					<fpage>2863</fpage>
					<lpage>2877</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b045"><label>45</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sagi</surname><given-names>D</given-names></name><name name-style="western"><surname>Julesz</surname><given-names>B</given-names></name></person-group>
					<year>1985</year>
					<article-title>“Where” and “what” in vision.</article-title>
					<source>Science</source>
					<volume>228</volume>
					<fpage>1217</fpage>
					<lpage>1219</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b046"><label>46</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z</given-names></name></person-group>
					<year>1998</year>
					<article-title>Visual segmentation without classification: A proposed function for primary visual cortex [abstract].</article-title>
					<comment>In</comment>
					<source>Proceedings of the European Conference on Visual Perception</source>
					<conf-date>24–28 August 1998; </conf-date>
					<conf-loc>Oxford, United Kingdom.</conf-loc>
					<comment>Perception 27 (ECVP Abstract Supplement). Available: <ext-link ext-link-type="uri" xlink:href="http://www.perceptionweb.com/abstract.cgi?id=v980337" xlink:type="simple">http://www.perceptionweb.com/abstract.cgi?id=v980337</ext-link>. Accesssed 12 March 2007.</comment>
				</element-citation></ref><ref id="pcbi-0030062-b047"><label>47</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Zhaoping</surname><given-names>L</given-names></name></person-group>
					<year>2003</year>
					<article-title>V1 mechanisms and some figure–ground and border effects.</article-title>
					<source>J Physiol (Paris)</source>
					<volume>97</volume>
					<fpage>503</fpage>
					<lpage>515</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b048"><label>48</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Shipp</surname><given-names>S</given-names></name></person-group>
					<year>2004</year>
					<article-title>The brain circuitry of attention.</article-title>
					<source>Trends Cogn Sci</source>
					<volume>8</volume>
					<fpage>223</fpage>
					<lpage>230</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b049"><label>49</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>TS</given-names></name><name name-style="western"><surname>Yang</surname><given-names>CF</given-names></name><name name-style="western"><surname>Romero</surname><given-names>RD</given-names></name><name name-style="western"><surname>Mumford</surname><given-names>D</given-names></name></person-group>
					<year>2002</year>
					<article-title>Neural activity in early visual cortex reflects behavioral experience and higher-order perceptual saliency.</article-title>
					<source>Nat Neurosci</source>
					<volume>5</volume>
					<fpage>589</fpage>
					<lpage>597</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b050"><label>50</label><mixed-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Guyader</surname><given-names>N</given-names></name><name name-style="western"><surname>JingLing</surname><given-names>L</given-names></name><name name-style="western"><surname>Lewis</surname><given-names>AS</given-names></name><name name-style="western"><surname>Zhaoping</surname><given-names>L</given-names></name></person-group>
					<year>2005</year>
					<article-title>Investigation of the relative contribution of 3-D and 2-D image cues in texture segmentation.</article-title>
					<source>Perception 34 (Supplement S)</source>
					<comment>55</comment>.
				</mixed-citation></ref><ref id="pcbi-0030062-b051"><label>51</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Tsotsos</surname><given-names>JK</given-names></name></person-group>
					<year>1990</year>
					<article-title>Analyzing vision at the complexity level.</article-title>
					<source>Behav Brain Sci</source>
					<volume>13</volume>
					<fpage>423</fpage>
					<lpage>445</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b052"><label>52</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name><name name-style="western"><surname>Duncan</surname><given-names>J</given-names></name></person-group>
					<year>1995</year>
					<article-title>Neural mechanisms of selective visual attention.</article-title>
					<source>Ann Rev Neuroscience</source>
					<volume>18</volume>
					<fpage>193</fpage>
					<lpage>222</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b053"><label>53</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Yantis</surname><given-names>S</given-names></name><name name-style="western"><surname>Serences</surname><given-names>JT</given-names></name></person-group>
					<year>2003</year>
					<article-title>Cortical mechanisms of space-based and object-based attentional control.</article-title>
					<source>Curr Opin Neurobiol</source>
					<volume>13</volume>
					<fpage>187</fpage>
					<lpage>193</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b054"><label>54</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Treue</surname><given-names>S</given-names></name><name name-style="western"><surname>Martinez-Trujillo</surname><given-names>JC</given-names></name></person-group>
					<year>1999</year>
					<article-title>Feature-based attention influences motion processing gain in macaque visual cortex.</article-title>
					<source>Nature</source>
					<volume>399</volume>
					<fpage>575</fpage>
					<lpage>579</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b055"><label>55</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Chelazzi</surname><given-names>L</given-names></name><name name-style="western"><surname>Miller</surname><given-names>EK</given-names></name><name name-style="western"><surname>Duncan</surname><given-names>J</given-names></name><name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name></person-group>
					<year>1993</year>
					<article-title>A neural basis for visual search in inferior temporal cortex.</article-title>
					<source>Nature</source>
					<volume>363</volume>
					<fpage>345</fpage>
					<lpage>347</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b056"><label>56</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Thompson</surname><given-names>KG</given-names></name><name name-style="western"><surname>Hanes</surname><given-names>DP</given-names></name><name name-style="western"><surname>Bichot</surname><given-names>NP</given-names></name><name name-style="western"><surname>Schall</surname><given-names>JD</given-names></name></person-group>
					<year>1996</year>
					<article-title>Perceptual and motor processing stages identified in the activity of macaque frontal eye field neurons during visual search.</article-title>
					<source>J Neurophysiol</source>
					<volume>76</volume>
					<fpage>4040</fpage>
					<lpage>4055</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b057"><label>57</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Smithson</surname><given-names>HE</given-names></name><name name-style="western"><surname>Mollon</surname><given-names>JD</given-names></name></person-group>
					<year>2006</year>
					<article-title>Do masks terminate the icon?</article-title>
					<source>Q J Exp Psych</source>
					<volume>59</volume>
					<fpage>150</fpage>
					<lpage>160</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b058"><label>58</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gehring</surname><given-names>WJ</given-names></name><name name-style="western"><surname>Goss</surname><given-names>B</given-names></name><name name-style="western"><surname>Coles</surname><given-names>MGH</given-names></name><name name-style="western"><surname>Meyer</surname><given-names>DE</given-names></name><name name-style="western"><surname>Donchin</surname><given-names>E</given-names></name></person-group>
					<year>1993</year>
					<article-title>A neural system for error detection and compensation.</article-title>
					<source>Psychol Sci</source>
					<volume>4</volume>
					<fpage>385</fpage>
					<lpage>390</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b059"><label>59</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gottlieb</surname><given-names>JP</given-names></name><name name-style="western"><surname>Kusunoki</surname><given-names>M</given-names></name><name name-style="western"><surname>Goldberg</surname><given-names>ME</given-names></name></person-group>
					<year>1998</year>
					<article-title>The representation of visual salience in monkey parietal cortex.</article-title>
					<source>Nature</source>
					<volume>391</volume>
					<fpage>481</fpage>
					<lpage>484</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b060"><label>60</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bisley</surname><given-names>JW</given-names></name><name name-style="western"><surname>Goldberg</surname><given-names>ME</given-names></name></person-group>
					<year>2003</year>
					<article-title>Neuronal activity in the lateral intraparietal area and spatial attention.</article-title>
					<source>Science</source>
					<volume>299</volume>
					<fpage>81</fpage>
					<lpage>86</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b061"><label>61</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Lewis</surname><given-names>A</given-names></name><name name-style="western"><surname>Zhaoping</surname><given-names>L</given-names></name></person-group>
					<year>2005</year>
					<article-title>Saliency from natural scene statistics.</article-title>
					<comment>In:</comment>
					<source>Proceedings of the 35th Annual Meeting of the Society for Neuroscience</source>
					<conf-date>12–16 November 2005; </conf-date>
					<conf-loc>Washington, D. C., United States.</conf-loc>
					<comment>Program No. 821.11, 2005 Abstract Viewer/Itineary planner. </comment>
					<publisher-loc>Washington (D. C.)</publisher-loc>
					<publisher-name>Society for Neuroscience</publisher-name>
					<comment>Available: <ext-link ext-link-type="uri" xlink:href="http://sfn.scholarone.com/itin2005/index.html" xlink:type="simple">http://sfn.scholarone.com/itin2005/index.html</ext-link>. Accessed 12 March 2007.</comment>
				</element-citation></ref><ref id="pcbi-0030062-b062"><label>62</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Landy</surname><given-names>MS</given-names></name><name name-style="western"><surname>Bergen</surname><given-names>JR</given-names></name></person-group>
					<year>1991</year>
					<article-title>Texture segregation and orientation gradient.</article-title>
					<source>Vision Res</source>
					<volume>31</volume>
					<fpage>679</fpage>
					<lpage>91</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b063"><label>63</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Pomerantz</surname><given-names>JR</given-names></name></person-group>
					<year>1981</year>
					<article-title>Perceptual organization in information processing.</article-title>
					<comment>In</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Kubovy</surname><given-names>M</given-names></name><name name-style="western"><surname>Pomerantz</surname><given-names>J</given-names></name></person-group>
					<source>Perceptual organization</source>
					<publisher-loc>Hillsdale (New Jersey)</publisher-loc>
					<publisher-name>Erlbaum</publisher-name>
					<fpage>141</fpage>
					<lpage>180</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b064"><label>64</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Herzog</surname><given-names>MH</given-names></name><name name-style="western"><surname>Fahle</surname><given-names>M</given-names></name></person-group>
					<year>2002</year>
					<article-title>Effects of grouping in contextual modulation.</article-title>
					<source>Nature</source>
					<volume>415</volume>
					<fpage>433</fpage>
					<lpage>436</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b065"><label>65</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Herzog</surname><given-names>MH</given-names></name><name name-style="western"><surname>Ernst</surname><given-names>U</given-names></name><name name-style="western"><surname>Etzold</surname><given-names>A</given-names></name><name name-style="western"><surname>Eurich</surname><given-names>C</given-names></name></person-group>
					<year>2003</year>
					<article-title>Local interactions in neural networks explain global effects in the masking of visual stimuli.</article-title>
					<source>Neural Comput</source>
					<volume>15</volume>
					<fpage>2091</fpage>
					<lpage>2113</lpage>
				</element-citation></ref><ref id="pcbi-0030062-b066"><label>66</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Zhaoping</surname><given-names>L</given-names></name><name name-style="western"><surname>Guyader</surname><given-names>N</given-names></name></person-group>
					<year>2007</year>
					<article-title>Interference with bottom-up feature detection by higher-level object recognition.</article-title>
					<source>Curr Biol</source>
					<volume>17</volume>
					<fpage>26</fpage>
					<lpage>31</lpage>
				</element-citation></ref></ref-list></back></article>