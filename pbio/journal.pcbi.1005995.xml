<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005995</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-01346</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal migration</subject><subj-group><subject>Animal navigation</subject><subj-group><subject>Echolocation</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal migration</subject><subj-group><subject>Animal navigation</subject><subj-group><subject>Echolocation</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal signaling and communication</subject><subj-group><subject>Echolocation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal signaling and communication</subject><subj-group><subject>Echolocation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Echolocation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Echolocation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Echolocation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Bioacoustics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Bioacoustics</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Bats</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Audio signal processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Acoustic signals</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Bat detective—Deep learning tools for bat acoustic signal detection</article-title>
<alt-title alt-title-type="running-head">Deep learning tools for bat acoustic signal detection</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5787-5073</contrib-id>
<name name-style="western">
<surname>Mac Aodha</surname>
<given-names>Oisin</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0965-1649</contrib-id>
<name name-style="western">
<surname>Gibb</surname>
<given-names>Rory</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Barlow</surname>
<given-names>Kate E.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7959-9292</contrib-id>
<name name-style="western">
<surname>Browning</surname>
<given-names>Ella</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Firman</surname>
<given-names>Michael</given-names>
</name>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Freeman</surname>
<given-names>Robin</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Harder</surname>
<given-names>Briana</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kinsey</surname>
<given-names>Libby</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Mead</surname>
<given-names>Gary R.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Newson</surname>
<given-names>Stuart E.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff007"><sup>7</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Pandourski</surname>
<given-names>Ivan</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<xref ref-type="aff" rid="aff008"><sup>8</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1025-5616</contrib-id>
<name name-style="western">
<surname>Parsons</surname>
<given-names>Stuart</given-names>
</name>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff009"><sup>9</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Russ</surname>
<given-names>Jon</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<xref ref-type="aff" rid="aff010"><sup>10</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Szodoray-Paradi</surname>
<given-names>Abigel</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<xref ref-type="aff" rid="aff011"><sup>11</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2994-0074</contrib-id>
<name name-style="western">
<surname>Szodoray-Paradi</surname>
<given-names>Farkas</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<xref ref-type="aff" rid="aff011"><sup>11</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Tilova</surname>
<given-names>Elena</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<xref ref-type="aff" rid="aff012"><sup>12</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Girolami</surname>
<given-names>Mark</given-names>
</name>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<xref ref-type="aff" rid="aff013"><sup>13</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8472-3828</contrib-id>
<name name-style="western">
<surname>Brostow</surname>
<given-names>Gabriel</given-names>
</name>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Jones</surname>
<given-names>Kate E.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Computer Science, University College London, London, United Kingdom</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Centre for Biodiversity and Environment Research, Department of Genetics, Evolution and Environment, University College London, London, United Kingdom</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Bat Conservation Trust, Quadrant House, London, United Kingdom</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Institute of Zoology, Zoological Society of London, Regent’s Park, London, United Kingdom</addr-line></aff>
<aff id="aff005"><label>5</label> <addr-line>Bellevue, Washington, United States of America</addr-line></aff>
<aff id="aff006"><label>6</label> <addr-line>Wickford, Essex, United Kingdom</addr-line></aff>
<aff id="aff007"><label>7</label> <addr-line>British Trust for Ornithology, The Nunnery, Thetford, Norfolk, United Kingdom</addr-line></aff>
<aff id="aff008"><label>8</label> <addr-line>Institute of Biodiversity and Ecosystem Research, Bulgaria Academy of Sciences, Sofia, Bulgaria</addr-line></aff>
<aff id="aff009"><label>9</label> <addr-line>School of Earth, Environmental and Biological Sciences, Queensland University of Technology (QUT), Brisbane, QLD, Australia</addr-line></aff>
<aff id="aff010"><label>10</label> <addr-line>Ridgeway Ecology, Warwick, United Kingdom</addr-line></aff>
<aff id="aff011"><label>11</label> <addr-line>Romanian Bat Protection Association, Satu Mare, Romania</addr-line></aff>
<aff id="aff012"><label>12</label> <addr-line>Green Balkans—Stara Zagora, Stara Zagora, Bulgaria</addr-line></aff>
<aff id="aff013"><label>13</label> <addr-line>Department of Mathematics, Imperial College London, London, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Fenton</surname>
<given-names>Brock</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Western Ontario, CANADA</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">o.macaodha@cs.ucl.ac.uk</email> (OMA); <email xlink:type="simple">kate.e.jones@ucl.ac.uk</email> (KEJ)</corresp>
</author-notes>
<pub-date pub-type="epub">
<day>8</day>
<month>3</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>3</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>3</issue>
<elocation-id>e1005995</elocation-id>
<history>
<date date-type="received">
<day>9</day>
<month>8</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>21</day>
<month>1</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Mac Aodha et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005995"/>
<abstract>
<p>Passive acoustic sensing has emerged as a powerful tool for quantifying anthropogenic impacts on biodiversity, especially for echolocating bat species. To better assess bat population trends there is a critical need for accurate, reliable, and open source tools that allow the detection and classification of bat calls in large collections of audio recordings. The majority of existing tools are commercial or have focused on the species classification task, neglecting the important problem of first localizing echolocation calls in audio which is particularly problematic in noisy recordings. We developed a convolutional neural network based open-source pipeline for detecting ultrasonic, full-spectrum, search-phase calls produced by echolocating bats. Our deep learning algorithms were trained on full-spectrum ultrasonic audio collected along road-transects across Europe and labelled by citizen scientists from <ext-link ext-link-type="uri" xlink:href="http://www.batdetective.org/" xlink:type="simple">www.batdetective.org</ext-link>. When compared to other existing algorithms and commercial systems, we show significantly higher detection performance of search-phase echolocation calls with our test sets. As an example application, we ran our detection pipeline on bat monitoring data collected over five years from Jersey (UK), and compared results to a widely-used commercial system. Our detection pipeline can be used for the automatic detection and monitoring of bat populations, and further facilitates their use as indicator species on a large scale. Our proposed pipeline makes only a small number of bat specific design decisions, and with appropriate training data it could be applied to detecting other species in audio. A crucial novelty of our work is showing that with careful, non-trivial, design and implementation considerations, state-of-the-art deep learning methods can be used for accurate and efficient monitoring in audio.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>There is a critical need for robust and accurate tools to scale up biodiversity monitoring and to manage the impact of anthropogenic change. For example, the monitoring of bat species and their population dynamics can act as an important indicator of ecosystem health as they are particularly sensitive to habitat conversion and climate change. In this work we propose a fully automatic and efficient method for detecting bat echolocation calls in noisy audio recordings. We show that our approach is more accurate compared to existing algorithms and other commercial tools. Our method enables us to automatically estimate bat activity from multi-year, large-scale, audio monitoring programmes.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000266</institution-id>
<institution>Engineering and Physical Sciences Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>EP/K015664/1</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Girolami</surname>
<given-names>Mark</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000266</institution-id>
<institution>Engineering and Physical Sciences Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>EP/K503745/1</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Jones</surname>
<given-names>Kate E.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000270</institution-id>
<institution>Natural Environment Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>NE/P016677/1</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Jones</surname>
<given-names>Kate E.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution>Darwin Initiative</institution>
</funding-source>
<award-id>15003</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Jones</surname>
<given-names>Kate E.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award005">
<funding-source>
<institution>Darwin Initiative</institution>
</funding-source>
<award-id>161333</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Jones</surname>
<given-names>Kate E.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award006">
<funding-source>
<institution>Darwin Initiative</institution>
</funding-source>
<award-id>EIDPR075</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Jones</surname>
<given-names>Kate E.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award007">
<funding-source>
<institution>Philip Leverhulme Prize</institution>
</funding-source>
<principal-award-recipient>
<name name-style="western">
<surname>Jones</surname>
<given-names>Kate E.</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported financially through the Darwin Initiative (Awards 15003, 161333, EIDPR075), the Zooniverse, the People’s Trust for Endangered Species, Mammals Trust UK, the Leverhulme Trust (Philip Leverhulme Prize for KEJ), NERC (NE/P016677/1), and EPSRC (EP/K015664/1 and EP/K503745/1). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="4"/>
<table-count count="1"/>
<page-count count="19"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All training and test data, including user and expert annotations, along with the code to train and evaluate our detection algorithms are available on our GitHub page (<ext-link ext-link-type="uri" xlink:href="https://github.com/macaodha/batdetect" xlink:type="simple">https://github.com/macaodha/batdetect</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>There is a critical need for robust and accurate tools to scale up biodiversity monitoring and to manage the impact of anthropogenic change [<xref ref-type="bibr" rid="pcbi.1005995.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref002">2</xref>]. Modern hardware for passive biodiversity sensing such as camera trapping and audio recording now enables the collection of vast quantities of data relatively inexpensively. In recent years, passive acoustic sensing has emerged as a powerful tool for understanding trends in biodiversity [<xref ref-type="bibr" rid="pcbi.1005995.ref003">3</xref>–<xref ref-type="bibr" rid="pcbi.1005995.ref006">6</xref>]. Monitoring of bat species and their population dynamics can act as an important indicator of ecosystem health as they are particularly sensitive to habitat conversion and climate change [<xref ref-type="bibr" rid="pcbi.1005995.ref007">7</xref>]. Close to 80% of bat species emit ultrasonic pulses, or echolocation calls, to search for prey, avoid obstacles, and to communicate [<xref ref-type="bibr" rid="pcbi.1005995.ref008">8</xref>]. Acoustic monitoring offers a passive, non-invasive, way to collect data about echolocating bat population dynamics and the occurrence of species, and it is increasingly being used to survey and monitor bat populations [<xref ref-type="bibr" rid="pcbi.1005995.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref010">10</xref>].</p>
<p>Despite the obvious advantages of passive acoustics for monitoring echolocating bat populations, its widespread use has been hampered by the challenges of robust identification of acoustic signals, generation of meaningful statistical population trends from acoustic activity, and engaging a wide audience to take part in monitoring programmes [<xref ref-type="bibr" rid="pcbi.1005995.ref011">11</xref>]. Recent developments in statistical methodologies for estimating abundance from acoustic activity [<xref ref-type="bibr" rid="pcbi.1005995.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref013">13</xref>], and the growth of citizen science networks for bats [<xref ref-type="bibr" rid="pcbi.1005995.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref010">10</xref>] mean that efficient and robust audio signal processing tools are now a key priority. However, tool development is hampered by a lack of large scale species reference audio datasets, intraspecific variability of bat echolocation signals, and radically different recording devices being used to collect data [<xref ref-type="bibr" rid="pcbi.1005995.ref011">11</xref>].</p>
<p>To date, most full-spectrum acoustic identification tools for bats have focused on the problem of species classification from search-phase echolocation calls [<xref ref-type="bibr" rid="pcbi.1005995.ref011">11</xref>]. Existing methods typically extract a set of audio features (such as call duration, mean frequency, and mean amplitude) from high quality search-phase echolocation call reference libraries to train machine learning algorithms to classify unknown calls to species [<xref ref-type="bibr" rid="pcbi.1005995.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1005995.ref019">19</xref>]. Instead of using manually defined features, another set of approaches attempt to learn representation directly from spectrograms [<xref ref-type="bibr" rid="pcbi.1005995.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref021">21</xref>]. Localising audio events in time (defined here as ‘detection’), is an important challenge in itself, and is often a necessary pre-processing step for species classification [<xref ref-type="bibr" rid="pcbi.1005995.ref022">22</xref>]. Additionally, understanding how calls are detected is critical to quantifying any biases which may impact estimates of species abundance or occupancy [<xref ref-type="bibr" rid="pcbi.1005995.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref023">23</xref>]. For example, high levels of background noise, often found in highly disturbed anthropogenic habitats such as cities, may have a significant impact on the ability to detect signals in recordings and lead to a bias in population estimates.</p>
<p>Detecting search-phase calls by manual inspection of spectrograms tends to be subjective, highly dependent on individual experience, and its uncertainties are difficult to quantify [<xref ref-type="bibr" rid="pcbi.1005995.ref024">24</xref>]. There are a number of automatic detection tools now available which use a variety of methods, including amplitude threshold filtering, locating areas of smooth frequency change, detection of set search criteria, or based on a cross-correlation of signal spectrograms with a reference spectrogram [see review in <xref ref-type="bibr" rid="pcbi.1005995.ref011">11</xref>]. While there are some studies that analyse the biases of automated detection (and classification) tools [<xref ref-type="bibr" rid="pcbi.1005995.ref025">25</xref>–<xref ref-type="bibr" rid="pcbi.1005995.ref030">30</xref>], this is generally poorly quantified, and in particular, there is very little published data available on the accuracy of many existing closed source commercial systems. Despite this, commercial systems are commonly used in bat acoustic survey and monitoring studies, albeit often with additional manual inspection [<xref ref-type="bibr" rid="pcbi.1005995.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref010">10</xref>]. This reliance on poorly documented algorithms is scientifically undesirable, and manual detection of signals is clearly not scalable for national or regional survey and monitoring. In addition, there is the danger that manual detection and classification introduces a bias towards the less noisy and therefore more easily identifiable calls. To address these limitations, a freely available, transparent, fast, and accurate detection algorithm that can also be used alongside other classification algorithms is highly desirable.</p>
<p>Here, we develop an open source system for automatic bat search-phase echolocation call detection (i.e. localisation in time) in noisy, real world, recordings. We use the latest developments in machine learning to directly learn features from the input audio data using supervised deep convolutional neural networks (CNNs) [<xref ref-type="bibr" rid="pcbi.1005995.ref031">31</xref>]. CNNs have been shown to be very successful for classification and detection of objects in images [<xref ref-type="bibr" rid="pcbi.1005995.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref033">33</xref>]. They have also been applied to various audio classification tasks [<xref ref-type="bibr" rid="pcbi.1005995.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1005995.ref036">36</xref>], along with human speech recognition [<xref ref-type="bibr" rid="pcbi.1005995.ref037">37</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref038">38</xref>]. Although CNNs are now starting to be used for bioacoustic signal detection and classification tasks in theoretical or small-scale contexts (e.g. bird call detection) [<xref ref-type="bibr" rid="pcbi.1005995.ref039">39</xref>], to date there have been no application of CNN-based tools for bat monitoring. This is mainly due to a lack of sufficiently large labelled bat audio datasets for use as training data. To overcome this, we use data collected and annotated by thousands of citizen scientists as part of our Indicator Bats Programme [<xref ref-type="bibr" rid="pcbi.1005995.ref007">7</xref>] and Bat Detective (<ext-link ext-link-type="uri" xlink:href="http://www.batdetective.org/" xlink:type="simple">www.batdetective.org</ext-link>). We validate our system on three different challenging test datasets from Europe which represent realistic use cases for bat surveys and monitoring programmes, and we present an example real-world application of our system on five years of monitoring data collected in Jersey (UK).</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec003">
<title>Acoustic detection pipeline</title>
<p>We created a detection system to determine the temporal location of any search-phase bat echolocation calls present in ultrasonic audio recordings. Our detection pipeline consisted of four main steps (<xref ref-type="fig" rid="pcbi.1005995.g001">Fig 1</xref>) as follows: (1) <italic>Fast Fourier Transform Analysis</italic>—Raw audio (<xref ref-type="fig" rid="pcbi.1005995.g001">Fig 1A</xref>) was converted into a log magnitude spectrogram (FFT window size 2.3 milliseconds, overlap of 75%, with Hanning window), retaining the frequency bands between 5kHz and 135kHz (<xref ref-type="fig" rid="pcbi.1005995.g001">Fig 1B</xref>). Recordings with a sampling rate of 44.1kHz, time expansion factor of 10, and 2.3ms FFT window, resulted in a window size of 1,024 samples. We used spectrograms rather than raw audio for analysis, as it provides an efficient means of dealing with audio that has been recorded at different sampling rates. Provided the frequency and time bins of the spectrogram are of the same resolution, audio with different sampling rates can be input into the same network. (2) <italic>De-noising</italic>–We used the de-noising method of [<xref ref-type="bibr" rid="pcbi.1005995.ref040">40</xref>] to filter out background noise by removing the mean amplitude in each frequency band (<xref ref-type="fig" rid="pcbi.1005995.g001">Fig 1C</xref>), as this significantly improved performance. (3) <italic>Convolutional Neural Network Detection</italic>–We created a convolutional neural network (CNN) that poses search-phase bat echolocation call detection as a binary classification problem. Our CNN<sub>FULL</sub> consisted of three convolution and max pooling layers, followed by one fully connected layer (see Supplementary Information Methods for further details). We halved the size of the input spectrogram to reduce the input dimensionality to the CNN which resulted in an input array of size of 130 frequency bins by 20 time steps, corresponding to a fixed length, detection window size of 23ms. We applied the CNN in a sliding window fashion, to predict the presence of a search-phase bat call at every instance of time in the spectrogram (<xref ref-type="fig" rid="pcbi.1005995.g001">Fig 1D</xref>). As passive acoustic monitoring can generate large quantities of data, we required a detection algorithm that would run faster than real time. While CNNs produce state of the art results for many tasks, naïve application of them for detection problems at test time can be extremely computationally inefficient [<xref ref-type="bibr" rid="pcbi.1005995.ref033">33</xref>]. So, to increase the speed of our system we also created a second, smaller CNN which included fewer model weights that can be run in a fully convolutional manner (CNN<sub>FAST</sub>) (Supplementary Information Methods, Supplementary Information <xref ref-type="supplementary-material" rid="pcbi.1005995.s002">S1 Fig</xref>). (4) <italic>Call Detection Probabilities–</italic>The probabilistic predictions produced by the sliding window detector tended to be overly smooth in time (<xref ref-type="fig" rid="pcbi.1005995.g001">Fig 1D</xref>). To localise the calls precisely, we converted the probabilistic predictions into individual detections using a non-maximum suppression to return the local maximum for each peak in the output prediction (<xref ref-type="fig" rid="pcbi.1005995.g001">Fig 1E</xref>). These local maxima corresponded to the predicted locations of the start of each search-phase bat echolocation call, with associated probabilities, and were exported as text files.</p>
<fig id="pcbi.1005995.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005995.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Detection pipeline for search-phase bat echolocation calls.</title>
<p>(a) Raw audio files are converted into a spectrogram using a Fast Fourier Transform (b). Files are de-noised (c), and a sliding window Convolutional Neural Network (CNN) classifier (d, yellow box) produces a probability for each time step. Individual call detection probabilities using non-maximum suppression are produced (e, green boxes), and the time in file of each prediction along with the classifier probability are exported as text files.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005995.g001" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec004">
<title>Acoustic training datasets</title>
<p>We trained our BatDetect CNNs using a subset of full-spectrum time-expanded (TE) ultrasonic acoustic data recorded between 2005–2011 along road-transects by citizen scientists as part of the Indicator Bats Programme (iBats) [<xref ref-type="bibr" rid="pcbi.1005995.ref007">7</xref>] (see Supplementary Information Methods for detailed data collection protocols). During surveys, acoustic devices (Tranquility Transect, Courtplan Design Ltd, UK) were set to record using a TE factor of 10, a sampling time of 320ms, and sensitivity set on maximum, giving a continuous sequence of ‘snapshots’, consisting of 320ms of silence (sensor listening) and 3.2s of TE audio (sensor playing back x 10). As sensitivity was set at maximum, and no minimum amplitude trigger mechanism was used on the recording devices, our recorded audio data contained many instances of low amplitude and faint bat calls, as well as other night-time ‘background’ noises such as other biotic, abiotic, and anthropogenic sounds.</p>
<p>We generated annotations of the start time of search-phase bat echolocation calls in the acoustic recordings by uploading the acoustic data to the Zooniverse citizen science platform (<ext-link ext-link-type="uri" xlink:href="http://www.zooniverse.org/" xlink:type="simple">www.zooniverse.org</ext-link>) as part of the Bat Detective project (<ext-link ext-link-type="uri" xlink:href="http://www.batdetective.org/" xlink:type="simple">www.batdetective.org</ext-link>), to enable public users to view and annotate them. The audio data were first split up into 3.84s long sound clips to include the 3.2s of TE audio and buffered by sensor-listening silence on either side. We then uploaded each sound clip as both a wav file and a magnitude spectrogram image (represented as a 512x720 resolution image) onto the Bat Detective project website. As the original recordings were time-expanded, therefore reducing the frequency, sounds in the files were in the audible spectrum and could be easily heard by users. Users were presented with a spectrogram and its corresponding audio file, and asked to annotate the presence of bat calls in each 3.84s clip (corresponding to 320ms of real-time recordings) (Supplementary Information <xref ref-type="supplementary-material" rid="pcbi.1005995.s003">S2 Fig</xref>). After an initial tutorial (<xref ref-type="supplementary-material" rid="pcbi.1005995.s007">S1 Video</xref>), users were instructed to draw bounding boxes around the locations of bat calls within call sequences and to annotate them as being either: (1) search-phase echolocation calls; (2) terminal feeding buzzes; or (3) social calls. Users were also encouraged to annotate the presence of insect vocalisations and non-biotic mechanical noises.</p>
<p>Between Oct 2012 and Sept 2016, 2,786 users (including only the number of users which had registered with the site and performed more than five annotations) listened to 127,451 unique clips and made 605,907 annotations. 14,339 of these clips were labelled as containing a bat call, with 10,272 identified as containing search-phase echolocation calls. Due to the inherent difficulty of identifying bat calls and the inexperience of some of our users, we observed a large number of errors in the annotations provided. How to best merge different annotations for multiple users is an open research question. Instead, we visually inspected a subset of the annotations from our most active user and found that they produced high quality annotations. This top user had viewed 46,508 unique sound clips and had labelled 3,364 clips as containing bat search-phase echolocation calls (a representative sample is shown in Supplementary Information <xref ref-type="supplementary-material" rid="pcbi.1005995.s004">S3 Fig</xref>). From this we randomly selected a training set of 2,812 clips, consisting of 4,782 individual search-phase echolocation call annotations from Romania and Bulgaria, with which to train the CNNs (corresponding to data from 347 road-transect sampling events of 137 different transects collected between 2006 and 2011) (<xref ref-type="fig" rid="pcbi.1005995.g002">Fig 2A</xref>). Data were chosen from these countries as they contain the majority of the most commonly occurring bat species in Europe [<xref ref-type="bibr" rid="pcbi.1005995.ref041">41</xref>]. This training set was used for all experiments. The remaining annotated clips from the same user were used to create one of our test sets, iBats Romania and Bulgaria (<xref ref-type="fig" rid="pcbi.1005995.g002">Fig 2A</xref> and see below). Occasionally, call harmonics and the associated main call were sometimes labelled with different start times in the same audio clip. To address this problem, we automatically merged annotations that occurred within 6 milliseconds of each other, making the assumption that they belonged to the same call. We measured the top user’s annotation accuracy on the test set from Romania and Bulgaria compared to the expert curated ground truth. This resulted in an average precision of 0.845 (computed from 455 out of 500 test files this user had labelled). This is in contrast with the second most prolific annotator who had an average precision of 0.67 (based on 311 out of 500 files).</p>
<fig id="pcbi.1005995.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005995.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Spatial distribution of the BatDetect CNNs training and testing datasets.</title>
<p>(a) Location of training data for all experiments and one test dataset in Romania and Bulgaria (2006–2011) from time-expanded (TE) data recorded along road transects by the Indicator Bats Programme (iBats) [<xref ref-type="bibr" rid="pcbi.1005995.ref007">7</xref>], where red and black points represent training and test data, respectively. (b) Locations of additional test datasets from TE data recorded as part of iBats car transects in the UK (2005–2011), and from real-time recordings from static recorders from the Norfolk Bat Survey from 2015 (inset). Points represent the start location of each snapshot recording for each iBats transect or locations of static detectors for the Norfolk Bat Survey.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005995.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Acoustic testing datasets and evaluation</title>
<p>To evaluate the performance of the detection algorithms, we created three different test datasets of approximately the same size (number and length of clips) (<xref ref-type="fig" rid="pcbi.1005995.g002">Fig 2A and 2B</xref>, Supplementary Information <xref ref-type="supplementary-material" rid="pcbi.1005995.s005">S1 Table</xref>). These datasets were chosen to represent three different realistic use cases commonly used for bat surveys and monitoring programmes and included data collected both along road-transects (resulting in noisier audio), and using static ultrasonic detectors. The test sets were as follows: (1) <italic>iBats Romania and Bulgaria</italic>—audio recorded from the same region, by the same individuals, with the same equipment, and sampling protocols as the training set, corresponding to 161 sampling events of 81 different transect routes; (2) <italic>iBats UK</italic>—audio recorded from a different region (corresponding to data from 176 sampling events of 111 different transects recorded between 2005–2011 in the United Kingdom, chosen randomly), by different individuals, using the same equipment type, and identical sampling protocols as part of the iBats programme [<xref ref-type="bibr" rid="pcbi.1005995.ref007">7</xref>] as the training set; and (3) <italic>Norfolk Bat Survey</italic>—audio recorded from a different region (Norfolk, UK), by different individuals, using different equipment types (SM2BAT+ Song Meter, Wildlife Acoustics) and different protocols (static devices from random sampling locations) as part of the Norfolk Bat Survey [<xref ref-type="bibr" rid="pcbi.1005995.ref009">9</xref>] in 2015. These data corresponded to 381 sampling events from 246 static recording locations (1km<sup>2</sup> grid cells), randomly chosen. The start times of the search-phase echolocation calls in these three test sets were manually extracted. For ambiguous calls, we consulted two experts, each with over 10 years of experience with bat acoustics.</p>
<p>As these data contained a significantly greater proportion of negative (non-bat calls) as compared to positive examples (bat calls), standard error metrics used for classification such as overall accuracy were not suitable for evaluating detection. Instead, we report the interpolated average precision and recall of each method displayed as a precision-recall curve [<xref ref-type="bibr" rid="pcbi.1005995.ref042">42</xref>]. Precision was calculated as the number of true positives divided by the sum of both true and false positives. We consider a detection to be a true positive if it occurred within 10ms of the expert annotated start time of the search-phase echolocation call. Recall was measured as the overall fraction of calls that were present in the audio that were correctly detected. Curves were obtained by thresholding the detection probabilities from zero to one and recording the precision and recall at each threshold. Algorithms that did not produce a continuous output were represented as a single point on the precision-recall curves. We also report recall at 0.95 precision, a metric that measures the fraction of calls that were detected while accepting a false positive rate of 5%. Thus a detection algorithm gets a score of zero if it was not capable of retrieving any calls with a precision greater than 0.95.</p>
<p>We compared the performance of both BatDetect CNNs to three existing closed-source commercial detection systems: (1) SonoBat (version 3.1.7p) [<xref ref-type="bibr" rid="pcbi.1005995.ref043">43</xref>]; (2) SCAN’R version 1.7.7. [<xref ref-type="bibr" rid="pcbi.1005995.ref044">44</xref>]; and (3) Kaleidoscope (version 4.2.0 alpha4) [<xref ref-type="bibr" rid="pcbi.1005995.ref045">45</xref>]. For SonoBat, calls were extracted in batch mode. We set a maximum of 100 calls per file (there are never more than 20 search-phase calls in a test file), and set ‘acceptable call quality’ and ‘skip calls below this quality’ parameters both to zero, and used an auto filter of 5KHz. For SCAN’R, we used standard settings as follows: setting minimum and maximum frequency cut off at 10 kHz and 125 kHz, respectively; minimum call duration at 0.5 ms; and minimum trigger level of 10 dB. We used Kaleidoscope in batch mode, setting ‘frequency range’ to 15-120kHz, ‘duration range’ to 0-500ms, ‘maximum inter-syllable’ to 0ms, and ‘minimum number of pulses’ to 0. We also compared two other detection algorithms that we implemented ourselves, which are representative of typical approaches used for detection in audio files and in other bat acoustic classification studies: (4) Segmentation—an amplitude thresholding segmentation method [<xref ref-type="bibr" rid="pcbi.1005995.ref046">46</xref>], this is related to the approach of [<xref ref-type="bibr" rid="pcbi.1005995.ref047">47</xref>]; and (5) Random Forest–a random forest-based classifier [<xref ref-type="bibr" rid="pcbi.1005995.ref048">48</xref>]. Where relevant, the algorithms for (4) and (5) used the same processing steps as the BatDetect CNNs. For the Segmentation method, we thresholded the amplitude of the input spectrogram resulting in a binary segmentation. Regions that were greater than the threshold <italic>S</italic><sub><italic>t</italic></sub>, and bigger than size <italic>S</italic><sub><italic>r</italic></sub>, were considered as positive instances. We chose the values of <italic>S</italic><sub><italic>t</italic></sub> and <italic>S</italic><sub><italic>r</italic></sub> on the iBats (Romania and Bulgaria) test dataset that gave the best test results to quantify its best case performance. For the <italic>Random Forest</italic> algorithm, as opposed to extracting low dimensional audio features we instead we used the raw amplitude values from the gradient magnitude of the log magnitude spectrogram as a higher dimensional candidate feature set. This enabled the Random Forest to learn features that it deemed useful for detecting calls. We compared the total processing time for each of our own algorithms, and timings were calculated on a desktop with an Intel i7 processor, 32Gb of RAM, and a Nvidia GTX 1080 GPU. With the exception of the BatDetect CNN<sub>FULL</sub>, which used a GPU at test time, all the other algorithms were run on the CPU.</p>
</sec>
<sec id="sec006">
<title>Ecological monitoring application</title>
<p>To demonstrate the performance of our method in a large-scale ecological monitoring application, we compared the number of bat search-phase echolocation calls found using our BatDetect CNN<sub>FAST</sub> algorithm to those produced from a commonly used commercial package using SonoBat (version 3.1.7p) [<xref ref-type="bibr" rid="pcbi.1005995.ref043">43</xref>] as a baseline, using monitoring data collected in iBats programme in Jersey, UK from 2011–2015. Audio data was collected twice yearly (July and August) from 11 road-transect routes of approximately 40km by volunteers using the iBats protocols (Supplementary Information, Supplementary Methods), corresponding to 5.7 days of continuous TE audio over five years (or 13.75 hours of real-time data). For the BatDetect CNN<sub>FAST</sub> analysis, we ran the pipeline as described above, using a conservative probabilistic threshold of 0.90 (so as to only include high precision predictions). Computational analysis timings for the CNN<sub>FAST</sub> for this dataset were calculated as before. For the comparison to SonoBat, we used the results from an existing real-world analysis in a recent monitoring report [<xref ref-type="bibr" rid="pcbi.1005995.ref049">49</xref>], where the audio files were first split into 1 min recordings, and then SonoBat was used to detect search-phase calls and to fit a frequency-time trend line to the shape of the call [<xref ref-type="bibr" rid="pcbi.1005995.ref049">49</xref>]. All fitted lines were visually inspected and calls where the fitted line included background noise or echoes, were rejected. Typically, monitoring analyses group individual calls into sequences (a bat pass) before analysis. To replicate that here in both analyses, individual calls were assumed to be part of the same call sequence (bat pass) if they occurred within the same 3.84s sound clip and if the sequence continued into subsequent sound clips. We compared number of bat calls and passes detected per transect sampling event across the two analyses methods using generalized linear mixed models (GLMM) using lme4 [<xref ref-type="bibr" rid="pcbi.1005995.ref050">50</xref>] in R v. 3.3.3 [<xref ref-type="bibr" rid="pcbi.1005995.ref051">51</xref>] in order to control for the spatial and temporal non-independence of our survey data (Poisson GLMM including analysis method as a fixed effect and sampling event, transect route and date as random effects).</p>
</sec>
</sec>
<sec id="sec007" sec-type="results">
<title>Results</title>
<sec id="sec008">
<title>Acoustic detection performance</title>
<p>Both versions of our BatDetect CNN algorithm outperformed all other algorithms and commercial systems tested, with consistently higher average precision scores and recall rates across the three different test datasets (<xref ref-type="table" rid="pcbi.1005995.t001">Table 1</xref>, <xref ref-type="fig" rid="pcbi.1005995.g003">Fig 3A–3C</xref>). In particular, the CNNs detected a substantially higher proportion of search-phase calls at 0.95 precision (maximum 5% false positives) (<xref ref-type="table" rid="pcbi.1005995.t001">Table 1</xref>). All the other algorithms underestimated the number of search-phase echolocation calls in each dataset, except Segmentation, which produced high recall rates but with low precision (a high number of false positives). The CNNs relative improvement compared to other methods was higher on the road transect datasets (iBats Romania &amp; Bulgaria; iBats UK; <xref ref-type="table" rid="pcbi.1005995.t001">Table 1</xref>, <xref ref-type="fig" rid="pcbi.1005995.g003">Fig 3A and 3B</xref>). Overall the performance of CNN<sub>FAST</sub> was slightly worse than the larger CNN<sub>FULL</sub> across all test datasets, with the exception of improved recall at 0.95 precision in the static Norfolk Bat Survey dataset (<xref ref-type="fig" rid="pcbi.1005995.g003">Fig 3C</xref>, <xref ref-type="table" rid="pcbi.1005995.t001">Table 1</xref>). Precision scores for all commercial systems (SonoBat, SCAN’R and Kaleidoscope) were reasonably good across all test datasets (&gt;0.7) (<xref ref-type="fig" rid="pcbi.1005995.g003">Fig 3A–3C</xref>). However, this was at the expense of recall rates, which were consistently lower than for the CNNs and Random Forest, where the maximum recall rates were 44–60% of known calls detected (<xref ref-type="fig" rid="pcbi.1005995.g003">Fig 3C</xref>). The recall rates fell to a maximum of 25% of known calls for the road transect datasets (<xref ref-type="fig" rid="pcbi.1005995.g003">Fig 3A and 3B</xref>).</p>
<fig id="pcbi.1005995.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005995.g003</object-id>
<label>Fig 3</label>
<caption>
<title/>
<p><bold>Precision-recall curves for bat search-phase call detection algorithms across three testing datasets; (a) iBats Romania and Bulgaria; (b) iBats UK; and (c) Norfolk Bat Survey</bold>. Curves were obtained by sweeping the output probability for a given detector algorithm and computing the precision and recall at each threshold. The commercial systems or algorithms that did not return a continuous output or probability (SCAN’R, Segment, and Kaleidoscope) were depicted as a single point.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005995.g003" xlink:type="simple"/>
</fig>
<table-wrap id="pcbi.1005995.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005995.t001</object-id>
<label>Table 1</label> <caption><title>Average precision and recall results for bat search-phase call detection algorithms across three different test sets iBats Romania and Bulgaria; iBats UK; and Norfolk Bat Survey.</title></caption>
<alternatives>
<graphic id="pcbi.1005995.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005995.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" colspan="8">Detection Algorithms</th>
</tr>
<tr>
<th align="justify"/>
<th align="justify"/>
<th align="justify"/>
<th align="justify"/>
<th align="center" colspan="4">BatDetect</th>
</tr>
<tr>
<th align="justify">Average Precision</th>
<th align="justify">SonoBat</th>
<th align="justify">SCAN’R</th>
<th align="justify">Kaleidoscope</th>
<th align="justify">Segment</th>
<th align="justify">Random Forest</th>
<th align="justify">CNN<sub>FAST</sub></th>
<th align="justify">CNN<sub>FULL</sub></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">iBats (R&amp;B)</td>
<td align="justify">0.265</td>
<td align="justify">0.239</td>
<td align="justify">0.189</td>
<td align="justify">0.299</td>
<td align="justify">0.674</td>
<td align="justify">0.863</td>
<td align="justify"><underline>0.895</underline></td>
</tr>
<tr>
<td align="left">iBats (UK)</td>
<td align="justify">0.200</td>
<td align="justify">0.142</td>
<td align="justify">0.144</td>
<td align="justify">0.324</td>
<td align="justify">0.648</td>
<td align="justify">0.781</td>
<td align="justify"><underline>0.866</underline></td>
</tr>
<tr>
<td align="left">NBP (Norfolk)</td>
<td align="justify">0.473</td>
<td align="justify">0.456</td>
<td align="justify">0.553</td>
<td align="justify">0.506</td>
<td align="justify">0.630</td>
<td align="justify">0.861</td>
<td align="justify"><underline>0.882</underline></td>
</tr>
<tr>
<td align="justify" colspan="2"><bold>Recall at 0.95</bold></td>
<td align="justify"/>
<td align="justify"/>
<td align="justify"/>
<td align="justify"/>
<td align="justify"/>
<td align="justify"/>
</tr>
<tr>
<td align="left">iBats (R&amp;B)</td>
<td align="justify">0</td>
<td align="justify">0.251</td>
<td align="justify">0</td>
<td align="justify">0</td>
<td align="justify">0.568</td>
<td align="justify">0.777</td>
<td align="justify"><underline>0.818</underline></td>
</tr>
<tr>
<td align="left">iBats (UK)</td>
<td align="justify">0</td>
<td align="justify">0</td>
<td align="justify">0</td>
<td align="justify">0</td>
<td align="justify">0.324</td>
<td align="justify">0.570</td>
<td align="justify"><underline>0.670</underline></td>
</tr>
<tr>
<td align="left">NBP (Norfolk)</td>
<td align="justify">0.184</td>
<td align="justify">0.470</td>
<td align="justify">0</td>
<td align="justify">0</td>
<td align="justify">0.049</td>
<td align="justify"><underline>0.781</underline></td>
<td align="justify">0.754</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p>Large numbers indicate better performance. Recall results are reported at 0.95 precision, where zero indicates that the detector algorithm was unable to achieve a precision greater than 0.95 at any recall level. The results for the best performing algorithm are underlined. Details of the test datasets and detection algorithms are given in the text.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>The Random Forest baseline performed significantly better than the commercial systems on the two challenging roadside recorded datasets (<xref ref-type="fig" rid="pcbi.1005995.g003">Fig 3A and 3B</xref>). This is a result of the training data and the underlying power of the model. However, unlike our CNNs, the simple tree based model is limited in the complexity of the representations it can learn, which results in worse performance. For the static Norfolk Bat Survey its performance more closely matches that of SonoBat, but with improved recall.</p>
<p>CNN<sub>FULL</sub>, CNN<sub>FAST</sub>, Random Forest, and the Segmentation algorithms took 53, 9.5, 11, and 17 seconds respectively, to run the full detection pipeline on the 3.2 minutes of full spectrum iBats Romania and Bulgaria test dataset. Compared to CNN<sub>FULL</sub> there was therefore a significant decrease in the time required to perform detection using CNN<sub>FAST</sub>, which was also the fastest of our methods overall. Notably, close to 50% of the CNN runtime was spent generating the spectrograms for detection, making this the most computationally expensive stage in the pipeline.</p>
</sec>
<sec id="sec009">
<title>Ecological monitoring application results</title>
<p>Our BatDetect CNN<sub>FAST</sub> algorithm detected a significantly higher number of bat echolocation search-phase calls per transect sampling event, across 5 years of road transect data from iBats Jersey, compared to using SonoBat (CNN<sub>FAST</sub> mean = 107.69, sd = 48.01; SonoBat mean = 64.95, sd = 28.53, Poisson GLMM including sampling event, transect route and date as random effects p&lt;2e<sup>-16</sup>, n = 216) (<xref ref-type="fig" rid="pcbi.1005995.g004">Fig 4</xref>, Supplementary Information <xref ref-type="supplementary-material" rid="pcbi.1005995.s006">S2 Table</xref>). The differences between the two methods for bat passes was much smaller per sampling event, although CNN<sub>FAST</sub> still detected significantly more passes per transect recording (CNN<sub>FAST</sub> mean = 29.57, sd = 11.26; SonoBat mean = 27.27, sd = 10.85; Poisson GLMM including sampling event, transect route and date as random effects p = 0.00143, n = 216) (<xref ref-type="fig" rid="pcbi.1005995.g004">Fig 4</xref>, Supplementary Information <xref ref-type="supplementary-material" rid="pcbi.1005995.s006">S2 Table</xref>). Running only on the CPU, the CNN<sub>FAST</sub> algorithm took 24 seconds to process one hour of time-expanded audio.</p>
<fig id="pcbi.1005995.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005995.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Comparison of the predicted bat detections (calls and passes) for two different acoustic systems using monitoring data collected from Jersey, UK.</title>
<p>Acoustic systems used were SonoBat (version 3.1.7p) [<xref ref-type="bibr" rid="pcbi.1005995.ref043">43</xref>] using analysis in [<xref ref-type="bibr" rid="pcbi.1005995.ref049">49</xref>], and BatDetect CNN<sub>FAST</sub> using a probability threshold of 0.90. Detections are shown within each box plot, where the black line represents the mean across all transect sampling events from 2011–2015, boxes represent the middle 50% of the data, whiskers represent variability outside the upper and lower quartiles, with outliers plotted as individual points. See text for definition of a bat pass.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005995.g004" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec010" sec-type="conclusions">
<title>Discussion</title>
<p>The BatDetect deep learning algorithms show a higher detection performance (average precision and recall) for search-phase echolocation calls with the test sets, when compared to other existing algorithms and commercial systems. In particular, our algorithms were better at detecting calls in road-transect data, which tend to contain noisier recordings, suggesting that these are extremely useful tools for measuring bat abundance and occurrence in such datasets. Road-transect acoustic monitoring is a useful technique to assess bat populations over large areas and programmes have now been established by government and non-government agencies in many different countries [e.g., <xref ref-type="bibr" rid="pcbi.1005995.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref052">52</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref053">53</xref>–<xref ref-type="bibr" rid="pcbi.1005995.ref055">55</xref>]. Noisy sound environments are also likely to be a problem for other acoustic bat monitoring programmes. For example, with the falling cost and wider availability of full-spectrum audio equipment, the range of environments being acoustically monitored is increasing, including noisy urban situations [<xref ref-type="bibr" rid="pcbi.1005995.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref057">57</xref>]. Individual bats further from the microphone are less likely to be detected as their calls are fainter, and high ambient noise levels increase call masking and decrease call detectability. Additionally, a growth in open-source sensor equipment for bat acoustics using very cheap MEMs microphones [<xref ref-type="bibr" rid="pcbi.1005995.ref058">58</xref>] may also require algorithms able to detect bats in lower quality recordings, which may have a lower signal to noise ratio or a reduced call band-width due to frequency-dependent loss. Our open-source, well documented algorithms enable biases and errors to be directly incorporated into any acoustic analysis of bat populations and dynamics (e.g. occupancy models [e.g., <xref ref-type="bibr" rid="pcbi.1005995.ref023">23</xref>]. The detections with BatDetect can be directly used as input for population monitoring programmes when species identification is difficult such as the tropics, or to other CNN systems to determine bat species identity when sound libraries are available.</p>
<p>Our result that deep learning networks consistently outperformed other baselines, is consistent with the suggestion that CNNs offer substantially improved performance over other supervised learning methods for acoustic signal classification [<xref ref-type="bibr" rid="pcbi.1005995.ref039">39</xref>]. The major improvement of both CNNs over Random Forest and the three commercial systems was in terms of recall, i.e. increasing the proportion of detected bat calls in the test datasets. Although the precision of the commercial systems was often relatively high, the CNNs were able to detect much fainter and partially noise-masked bat calls that were missed by the other methods, with fewer false positives, and very quickly, particularly with CNN<sub>FAST</sub>. Previous applications of deep learning networks to bioacoustic and environmental sound recognition have used small and high-quality datasets [e.g., <xref ref-type="bibr" rid="pcbi.1005995.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref039">39</xref>]. However, our results show that, provided they are trained with suitably large and varied training data, deep learning networks have good potential for applied use in real-world heterogeneous datasets that are characteristic of acoustic wildlife monitoring (involving considerable variability in both environmental noise and distance of animal from sensor). Our comparison of CNN<sub>FULL</sub> and CNN<sub>FAST</sub> detectors was favourable, although CNN<sub>FAST</sub> had a slightly poorer performance showing a trade-off between speed and accuracy. This suggests that CNN<sub>FAST</sub> could potentially be adapted to work well with on-board low power devices (e.g. Intel’s Edison device) to deliver real-time detections. Avoiding the spectrogram generation stage entirely and using the raw audio samples as input [<xref ref-type="bibr" rid="pcbi.1005995.ref059">59</xref>], could also speed up performance of the system in the future, as currently over 50% of the CNN test time is taken up by computing spectrograms.</p>
<p>While our results have been validated on European bats, no species or region-specific knowledge, or particular acoustic sensor system is directly encoded into our system, making it possible to easily generalise to other systems (e.g. frequency division recordings), regions and species with additional training data. Despite this flexibility, this version of our deep network may be currently biased towards common species found along roads, although the algorithms did perform well on static recordings on a range of common and rare species in a range of habitats in the Norfolk Bat Survey [<xref ref-type="bibr" rid="pcbi.1005995.ref009">9</xref>]. Nevertheless, in future, extending the training dataset to include annotated bat calls from verified species-call databases to increase geographic and taxonomic coverage, will further improve the generality of our detection tool. Other improvements to the CNN detectors could also be made to lessen taxonomic bias. For example, some bat species have search phase calls longer than the fixed input time window of 23ms of both CNNs (e.g. horseshoe bats). This may limit our ability currently to detect species with these types of calls. One future approach would be to resize the input window [<xref ref-type="bibr" rid="pcbi.1005995.ref033">33</xref>], thus discarding some temporal information, or to use some form of recurrent neural network such as a Long Short-Term Memory (LSTM) [<xref ref-type="bibr" rid="pcbi.1005995.ref060">60</xref>] that can take a variable length sequence as input. There are many more unused annotations in the Bat Detective dataset that could potentially increase our training set size. However, we found some variability in the quality of the citizen science user annotations, as in other studies [<xref ref-type="bibr" rid="pcbi.1005995.ref061">61</xref>]. To make best use of these annotations, we need user models for understanding which annotations and users are reliable [<xref ref-type="bibr" rid="pcbi.1005995.ref062">62</xref>, <xref ref-type="bibr" rid="pcbi.1005995.ref063">63</xref>]. The Bat Detective dataset also includes annotations of particular acoustic behaviours (feeding buzzes and social calls), which in future can be used to train detection algorithms for different acoustic behaviours [e.g., <xref ref-type="bibr" rid="pcbi.1005995.ref064">64</xref>].</p>
<p>Our evaluation on large-scale ecological monitoring data from Jersey [<xref ref-type="bibr" rid="pcbi.1005995.ref049">49</xref>], demonstrated that our open-source BatDetect CNN<sub>FAST</sub> pipeline performs as well or better (controlling for spatial and temporal non-independence) compared with an existing widely-used commercial system (SonoBat) that had been manually filtered (false positives were removed). Here we assume that the manually filtered data represents the ground truth, although it may slightly underrepresent the true number of calls due to missing detections on the part of SonoBat. Interestingly, although the CNN<sub>FAST</sub> consistently detected more of the faint and partially-masked calls, most bat passes are likely to still contain at least one call that is clearly-recorded enough to be detected by SonoBat, meaning that the total number of detected bat passes is similar across the two methods. No manual filtering is performed for CNN<sub>FAST,</sub> but the increase in detected calls mirrors the results observed in <xref ref-type="fig" rid="pcbi.1005995.g003">Fig 3</xref> at high thresholds (i.e. the left of the curves), where both CNNs detected 10–20% more calls than SonoBat on the related driving transect based test sets. Our system results in a large reduction processing time—several minutes for our automatic approach compared to several days split between automatic processing and manual filtering as reported by the authors of [<xref ref-type="bibr" rid="pcbi.1005995.ref049">49</xref>]. Specifically, it takes CNN<sub>FAST</sub> under 10 seconds to process the 500 files in the iBats Romania and Bulgaria test set compared to 30 minutes for SonoBat in batch mode. This increase in performance both in terms of speed and accuracy is crucial for future large scale monitoring programmes.</p>
<p>The results in our monitoring application raises an interesting question—what is the value of the additional detected calls? <xref ref-type="fig" rid="pcbi.1005995.g004">Fig 4</xref> shows a large increase in the number of detected calls and a slight increase in the number of detected bat passes. It may be the case that our current heuristic for merging calls into passes is too aggressive and as a result under reports the true number of bats when there were multiple calling at the same time. Further improvements to our system may come from a better understanding of the patterns of search-phase calls within sequences [<xref ref-type="bibr" rid="pcbi.1005995.ref065">65</xref>]. Instead of the existing heuristic we would ideally also be able to learn the relationship between individual calls and passes from labelled training data.</p>
<p>The current generation of algorithms for bat species classification that are based on extracting simple audio features may perhaps not be best suited to make use of the extra calls we detect. However, when large collections of diverse species data become available only relatively minor architectural changes will be required to our detection pipeline to adapt it for species classification (e.g. changing the final layer of our CNNs). As we have already observed for detection, with enough data, representation learning based approaches can also be applied to the problem of species classification with the promise of large increases in accuracy. These extra calls will be invaluable to create more powerful models, enabling them to perform accurately in diverse and challenging real world situations. For some noisy and faint bat calls it may always be difficult to identify them to the species level, and as a result a coarser taxonomic prediction may have to suffice.</p>
<p>Our BatDetect search-phase bat call detector significantly outperforms existing methods for localising the position of bat search-phase calls, particularly in noisy audio data. It could be combined with automatic bat species classification tools to scale up the monitoring of bat populations over large geographic regions. In addition to making our system available open source, we also provide three expertly annotated test sets that can be used to benchmark future detection algorithms.</p>
</sec>
<sec id="sec011">
<title>Data reporting</title>
<p>All training and test data, including user and expert annotations, along with the code to train and evaluate our detection algorithms are available on our GitHub page (<ext-link ext-link-type="uri" xlink:href="https://github.com/macaodha/batdetect" xlink:type="simple">https://github.com/macaodha/batdetect</ext-link>).</p>
</sec>
<sec id="sec012">
<title>Supporting information</title>
<supplementary-material id="pcbi.1005995.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005995.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Supplementary methods.</title>
<p>Description of the CNN architectures, training details, and information about how the training data was collected.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005995.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005995.s002" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>CNN<sub>FAST</sub> network architecture description.</title>
<p>The CNN<sub>FAST</sub> network consists of two convolution layers (Conv1 and Conv2), with 16 filters each (shown in yellow, with the filter size shown inset). Both convolution layers are followed by a max pooling layer (Max Pool1 and Max Pool2), and the network ends with a fully connected layer with 64 units (Fully Connect). CNN<sub>FAST</sub> computes feature maps (shown as white boxes) across the entire input spectrogram, resulting in less computation and a much faster run time. The fully connected layer is also evaluated as a convolution. The output of the detector is a probability vector (shown in green) whose length is one quarter times the width of the input spectrogram. The numbers below each layer indicate the height, weight, and depth of the corresponding layer.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005995.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005995.s003" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Spectrogram annotation interface from Bat Detective.</title>
<p>Boxes represent example user annotations of sounds in a spectrogram of a 3840ms sound clip, showing annotations of two sequences of search-phase echolocation bat calls (blue boxes), and an annotation of an insect call (yellow box).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005995.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005995.s004" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Example search-phase bat echolocation calls from iBats Romania &amp; Bulgaria training dataset.</title>
<p>Each example is represented as a spectrogram of duration 23 milliseconds and frequency range from 5–115 kHz using the same FFT parameters as the main paper, and contains examples of different search-phase echolocation call type, but also a wide variety of background non-bat biotic, abiotic and anthropogenic sounds.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005995.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005995.s005" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Description of BatDetect CNNs test datasets.</title>
<p>TE represents time-expansion recordings (x10); RT real-time recordings. Note that the length of the clips is approximately comparable for both the iBats and the Norfolk Bat Survey data as the total iBats clip length of 3.84s corresponds to 320ms of ultrasonic sound slowed down ten times (3.2s) and buffered by silence on either side.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005995.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005995.s006" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Full details of the Poisson Generalised Linear Mixed Model (GLMM) used to model bat detections (calls and passes) for two acoustic analytical systems.</title>
<p>β represents slope, Std standard deviation, Z Z-value, p probability. Analytical systems compared were SonoBat (version 3.1.7p) [<xref ref-type="bibr" rid="pcbi.1005995.ref014">14</xref>] and BatDetect CNN<sub>FAST</sub>, using a 0.9 probability threshold. Data from using acoustic monitoring data collected from Jersey, UK between 2011–2015. See main text for definition of a bat pass. GLMMs were fitted using lme4 [<xref ref-type="bibr" rid="pcbi.1005995.ref015">15</xref>] with model formula: <italic>detections ~ analytical_method + (1|sampling_event) + (1|transect) + (1|date)</italic>.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005995.s007" mimetype="video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005995.s007" xlink:type="simple">
<label>S1 Video</label>
<caption>
<title>Overview of our system, Bat Detective annotation steps, and sample results.</title>
<p>(MP4)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We are enormously grateful for the efforts and enthusiasm of the amazing iBats and Bat Detective volunteers, for the many hours spent collecting data and providing valuable annotations. We would also like to thank Ian Agranat and Joe Szewczak for useful discussions and access to their systems. Finally, we would like to thank Zooniverse for setting up and hosting the Bat Detective project.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005995.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turner</surname> <given-names>W</given-names></name>. <article-title>Sensing biodiversity</article-title>. <source>Science</source>. <year>2014</year>;<volume>346</volume>(<issue>6207</issue>):<fpage>301</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1256014" xlink:type="simple">10.1126/science.1256014</ext-link></comment> <object-id pub-id-type="pmid">25324372</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cardinale</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Duffy</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Gonzalez</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hooper</surname> <given-names>DU</given-names></name>, <name name-style="western"><surname>Perrings</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Venail</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Biodiversity loss and its impact on humanity</article-title>. <source>Nature</source>. <year>2012</year>;<volume>486</volume>(<issue>7401</issue>):<fpage>59</fpage>–<lpage>67</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.nature.com/nature/journal/v486/n7401/abs/nature11148.html#supplementary-information" xlink:type="simple">http://www.nature.com/nature/journal/v486/n7401/abs/nature11148.html#supplementary-information</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature11148" xlink:type="simple">10.1038/nature11148</ext-link></comment> <object-id pub-id-type="pmid">22678280</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blumstein</surname> <given-names>DT</given-names></name>, <name name-style="western"><surname>Mennill</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Clemins</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Girod</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Yao</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Patricelli</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>Acoustic monitoring in terrestrial environments using microphone arrays: applications, technological considerations and prospectus</article-title>. <source>Journal of Applied Ecology</source>. <year>2011</year>;<volume>48</volume>(<issue>3</issue>):<fpage>758</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1365-2664.2011.01993.x" xlink:type="simple">10.1111/j.1365-2664.2011.01993.x</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005995.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marques</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Thomas</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>SW</given-names></name>, <name name-style="western"><surname>Mellinger</surname> <given-names>DK</given-names></name>, <name name-style="western"><surname>Ward</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Moretti</surname> <given-names>DJ</given-names></name>, <etal>et al</etal>. <article-title>Estimating animal population density using passive acoustics</article-title>. <source>Biological Reviews</source>. <year>2013</year>;<volume>88</volume>(<issue>2</issue>):<fpage>287</fpage>–<lpage>309</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/brv.12001" xlink:type="simple">10.1111/brv.12001</ext-link></comment> <object-id pub-id-type="pmid">23190144</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Penone</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Le Viol</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Pellissier</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Julien</surname> <given-names>J-F</given-names></name>, <name name-style="western"><surname>Bas</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kerbiriou</surname> <given-names>C</given-names></name>. <article-title>Use of Large-Scale Acoustic Monitoring to Assess Anthropogenic Pressures on Orthoptera Communities</article-title>. <source>Conservation Biology</source>. <year>2013</year>;<volume>27</volume>(<issue>5</issue>):<fpage>979</fpage>–<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/cobi.12083" xlink:type="simple">10.1111/cobi.12083</ext-link></comment> <object-id pub-id-type="pmid">23692213</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sueur</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Pavoine</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hamerlynck</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Duvail</surname> <given-names>S</given-names></name>. <article-title>Rapid Acoustic Survey for Biodiversity Appraisal</article-title>. <source>PLOS ONE</source>. <year>2009</year>;<volume>3</volume>(<issue>12</issue>):<fpage>e4065</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0004065" xlink:type="simple">10.1371/journal.pone.0004065</ext-link></comment> <object-id pub-id-type="pmid">19115006</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref007"><label>7</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Jones</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Russ</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Bashta</surname> <given-names>A-T</given-names></name>, <name name-style="western"><surname>Bilhari</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Catto</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Csősz</surname> <given-names>I</given-names></name>, <etal>et al</etal>. <chapter-title>Indicator Bats Program: A System for the Global Acoustic Monitoring of Bats</chapter-title>. <source>Biodiversity Monitoring and Conservation</source>: <publisher-name>Wiley-Blackwell</publisher-name>; <year>2013</year>. p. <fpage>211</fpage>–<lpage>47</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schnitzler</surname> <given-names>H-U</given-names></name>, <name name-style="western"><surname>Moss</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Denzinger</surname> <given-names>A</given-names></name>. <article-title>From spatial orientation to food acquisition in echolocating bats</article-title>. <source>Trends in Ecology &amp; Evolution</source>. <year>2003</year>;<volume>18</volume>(<issue>8</issue>):<fpage>386</fpage>–<lpage>94</lpage>. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0169-5347(03)00185-X" xlink:type="simple">http://dx.doi.org/10.1016/S0169-5347(03)00185-X</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Newson</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Evans</surname> <given-names>HE</given-names></name>, <name name-style="western"><surname>Gillings</surname> <given-names>S</given-names></name>. <article-title>A novel citizen science approach for large-scale standardised monitoring of bat activity and distribution, evaluated in eastern England</article-title>. <source>Biological Conservation</source>. <year>2015</year>;<volume>191</volume>:<fpage>38</fpage>–<lpage>49</lpage>. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.biocon.2015.06.009" xlink:type="simple">http://dx.doi.org/10.1016/j.biocon.2015.06.009</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barlow</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Briggs</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Haysom</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Hutson</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Lechiara</surname> <given-names>NL</given-names></name>, <name name-style="western"><surname>Racey</surname> <given-names>PA</given-names></name>, <etal>et al</etal>. <article-title>Citizen science reveals trends in bat populations: The National Bat Monitoring Programme in Great Britain</article-title>. <source>Biological Conservation</source>. <year>2015</year>;<volume>182</volume>:<fpage>14</fpage>–<lpage>26</lpage>. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.biocon.2014.11.022" xlink:type="simple">http://dx.doi.org/10.1016/j.biocon.2014.11.022</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref011"><label>11</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Walters</surname> <given-names>CL</given-names></name>, <name name-style="western"><surname>Collen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lucas</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Mroz</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Sayer</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>KE</given-names></name>. <chapter-title>Challenges of Using Bioacoustics to Globally Monitor Bats</chapter-title>. In: <name name-style="western"><surname>Adams</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Pedersen</surname> <given-names>SC</given-names></name>, editors. <source>Bat Evolution, Ecology, and Conservation</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer New York</publisher-name>; <year>2013</year>. p. <fpage>479</fpage>–<lpage>99</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lucas</surname> <given-names>TCD</given-names></name>, <name name-style="western"><surname>Moorcroft</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Freeman</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rowcliffe</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>KE</given-names></name>. <article-title>A generalised random encounter model for estimating animal density with remote sensor data</article-title>. <source>Methods in Ecology and Evolution</source>. <year>2015</year>;<volume>6</volume>(<issue>5</issue>):<fpage>500</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/2041-210X.12346" xlink:type="simple">10.1111/2041-210X.12346</ext-link></comment> <object-id pub-id-type="pmid">27547297</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stevenson</surname> <given-names>BC</given-names></name>, <name name-style="western"><surname>Borchers</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Altwegg</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Swift</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Gillespie</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Measey</surname> <given-names>GJ</given-names></name>. <article-title>A general framework for animal density estimation from acoustic detections across a fixed microphone array</article-title>. <source>Methods in Ecology and Evolution</source>. <year>2015</year>;<volume>6</volume>(<issue>1</issue>):<fpage>38</fpage>–<lpage>48</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/2041-210X.12291" xlink:type="simple">10.1111/2041-210X.12291</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005995.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Skowronski</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>JG</given-names></name>. <article-title>Acoustic detection and classification of microchiroptera using machine learning: lessons learned from automatic speech recognition</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2006</year>;<volume>119</volume>:<fpage>1817</fpage>–<lpage>33</lpage>. <object-id pub-id-type="pmid">16583922</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Armitage</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Ober</surname> <given-names>HK</given-names></name>. <article-title>A comparison of supervised learning techniques in the classification of bat echolocation calls</article-title>. <source>Ecological Informatics</source>. <year>2010</year>;<volume>5</volume>:<fpage>465</fpage>–<lpage>73</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parsons</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>G</given-names></name>. <article-title>Acoustic identification of twelve species of echolocating bat by discriminant function analysis and artificial neural networks</article-title>. <source>The Journal of Experimental Biology</source>. <year>2000</year>;<volume>203</volume>:<fpage>2641</fpage>–<lpage>56</lpage>. <object-id pub-id-type="pmid">10934005</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Russo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>G</given-names></name>. <article-title>Identification of twenty-two bat species (Mammalia: Chiroptera) from Italy by analysis of time-expanded recordings of echolocation calls</article-title>. <source>Journal of Zoology</source>. <year>2002</year>;<volume>258</volume>(<issue>01</issue>):<fpage>91</fpage>–<lpage>103</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Walters</surname> <given-names>CL</given-names></name>, <name name-style="western"><surname>Freeman</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Collen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Dietz</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Brock Fenton</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>A continental-scale tool for acoustic identification of European bats</article-title>. <source>Journal of Applied Ecology</source>. <year>2012</year>;<volume>49</volume>(<issue>5</issue>):<fpage>1064</fpage>–<lpage>74</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1365-2664.2012.02182.x" xlink:type="simple">10.1111/j.1365-2664.2012.02182.x</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005995.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zamora-Gutierrez</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Lopez-Gonzalez</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>MacSwiney Gonzalez</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Fenton</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Kalko</surname> <given-names>EKV</given-names></name>, <etal>et al</etal>. <article-title>Acoustic identification of Mexican bats based on taxonomic and ecological constraints on call design</article-title>. <source>Methods in Ecology and Evolution</source>. <year>2016</year>;<volume>7</volume>(<issue>9</issue>):<fpage>1082</fpage>–<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/2041-210X.12556" xlink:type="simple">10.1111/2041-210X.12556</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005995.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stathopoulos</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Zamora-Gutierrez</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Girolami</surname> <given-names>M</given-names></name>. <article-title>Bat echolocation call identification for biodiversity monitoring: A probabilistic approach</article-title>. <source>Journal of the Royal Statistical Society Series C: Applied Statistics</source>. <year>2017</year>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stowell</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Plumbley</surname> <given-names>MD</given-names></name>. <article-title>Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning</article-title>. <source>PeerJ</source>. <year>2014</year>;<volume>2</volume>:<fpage>e488</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7717/peerj.488" xlink:type="simple">10.7717/peerj.488</ext-link></comment> <object-id pub-id-type="pmid">25083350</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref022"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Stowell D, Wood M, Stylianou Y, Glotin H, editors. Bird detection in audio: a survey and a challenge. Machine Learning for Signal Processing (MLSP), 2016 IEEE 26th International Workshop on; 2016: IEEE.</mixed-citation></ref>
<ref id="pcbi.1005995.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clement</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Rodhouse</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Ormsbee</surname> <given-names>PC</given-names></name>, <name name-style="western"><surname>Szewczak</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Nichols</surname> <given-names>JD</given-names></name>. <article-title>Accounting for false-positive acoustic detections of bats using occupancy models</article-title>. <source>Journal of Applied Ecology</source>. <year>2014</year>;<volume>51</volume>(<issue>5</issue>):<fpage>1460</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/1365-2664.12303" xlink:type="simple">10.1111/1365-2664.12303</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005995.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Skowronski</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Fenton</surname> <given-names>MB</given-names></name>. <article-title>Model-based detection of synthetic bat echolocation calls using an energy threshold detector for initialization</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2008</year>;<volume>123</volume>:<fpage>2643</fpage>–<lpage>50</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.2896752" xlink:type="simple">10.1121/1.2896752</ext-link></comment> <object-id pub-id-type="pmid">18529184</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Adams</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Jantzen</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Hamilton</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Fenton</surname> <given-names>MB</given-names></name>. <article-title>Do you hear what I hear? Implications of detector selection for acoustic monitoring of bats</article-title>. <source>Methods in Ecology and Evolution</source>. <year>2012</year>;<volume>3</volume>(<issue>6</issue>):<fpage>992</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jennings</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Parsons</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pocock</surname> <given-names>M</given-names></name>. <article-title>Human vs. machine: identification of bat species from their echolocation calls by humans and by artificial neural networks</article-title>. <source>Canadian Journal of Zoology</source>. <year>2008</year>;<volume>86</volume>(<issue>5</issue>):<fpage>371</fpage>–<lpage>7</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clement</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>KL</given-names></name>, <name name-style="western"><surname>Solick</surname> <given-names>DI</given-names></name>, <name name-style="western"><surname>Gruver</surname> <given-names>JC</given-names></name>. <article-title>The effect of call libraries and acoustic filters on the identification of bat echolocation</article-title>. <source>Ecology and evolution</source>. <year>2014</year>;<volume>4</volume>(<issue>17</issue>):<fpage>3482</fpage>–<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/ece3.1201" xlink:type="simple">10.1002/ece3.1201</ext-link></comment> <object-id pub-id-type="pmid">25535563</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fritsch</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Bruckner</surname> <given-names>A</given-names></name>. <article-title>Operator bias in software‐aided bat call identification</article-title>. <source>Ecology and evolution</source>. <year>2014</year>;<volume>4</volume>(<issue>13</issue>):<fpage>2703</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/ece3.1122" xlink:type="simple">10.1002/ece3.1122</ext-link></comment> <object-id pub-id-type="pmid">25077021</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Russo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Voigt</surname> <given-names>CC</given-names></name>. <article-title>The use of automated identification of bat echolocation calls in acoustic monitoring: A cautionary note for a sound analysis</article-title>. <source>Ecological Indicators</source>. <year>2016</year>;<volume>66</volume>:<fpage>598</fpage>–<lpage>602</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rydell</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nyman</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Eklöf</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Russo</surname> <given-names>D</given-names></name>. <article-title>Testing the performances of automated identification of bat echolocation calls: A request for prudence</article-title>. <source>Ecological Indicators</source>. <year>2017</year>;<volume>78</volume>:<fpage>416</fpage>–<lpage>20</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Haffner</surname> <given-names>P</given-names></name>. <article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proceedings of the IEEE</source>. <year>1998</year>;<volume>86</volume>(<issue>11</issue>):<fpage>2278</fpage>–<lpage>324</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krizhevsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sutskever</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <article-title>editors. Imagenet classification with deep convolutional neural networks</article-title>. <source>Advances in neural information processing systems</source>; <year>2012</year>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref033"><label>33</label><mixed-citation publication-type="other" xlink:type="simple">Girshick R, Donahue J, Darrell T, Malik J, editors. Rich feature hierarchies for accurate object detection and semantic segmentation. Proceedings of the IEEE conference on computer vision and pattern recognition; 2014.</mixed-citation></ref>
<ref id="pcbi.1005995.ref034"><label>34</label><mixed-citation publication-type="other" xlink:type="simple">Piczak KJ, Environmental sound classification with convolutional neural networks. 2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP); 2015: IEEE.</mixed-citation></ref>
<ref id="pcbi.1005995.ref035"><label>35</label><mixed-citation publication-type="other" xlink:type="simple">Salamon J, Bello JP. Deep convolutional neural networks and data augmentation for environmental sound classification. arXiv preprint arXiv:160804363. 2016.</mixed-citation></ref>
<ref id="pcbi.1005995.ref036"><label>36</label><mixed-citation publication-type="other" xlink:type="simple">Hershey S, Chaudhuri S, Ellis DP, Gemmeke JF, Jansen A, Moore RC, et al. CNN Architectures for Large-Scale Audio Classification. arXiv preprint arXiv:160909430. 2016.</mixed-citation></ref>
<ref id="pcbi.1005995.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Deng</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Dahl</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Mohamed</surname> <given-names>A-r</given-names></name>, <name name-style="western"><surname>Jaitly</surname> <given-names>N</given-names></name>, <etal>et al</etal>. <article-title>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</article-title>. <source>IEEE Signal Processing Magazine</source>. <year>2012</year>;<volume>29</volume>(<issue>6</issue>):<fpage>82</fpage>–<lpage>97</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref038"><label>38</label><mixed-citation publication-type="other" xlink:type="simple">Hannun A, Case C, Casper J, Catanzaro B, Diamos G, Elsen E, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:14125567. 2014.</mixed-citation></ref>
<ref id="pcbi.1005995.ref039"><label>39</label><mixed-citation publication-type="other" xlink:type="simple">Goeau H, Glotin H, Vellinga W-P, Planque R, Joly A, editors. LifeCLEF Bird Identification Task 2016. The Arrival of Deep Learning. Working Notes of CLEF 2016-Conference and Labs of the Evaluation forum; 2016; Évora, Portugal.</mixed-citation></ref>
<ref id="pcbi.1005995.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aide</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Corrada-Bravo</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Campos-Cerqueira</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Milan</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Vega</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Alvarez</surname> <given-names>R</given-names></name>. <article-title>Real-time bioacoustics monitoring and automated species identification</article-title>. <source>PeerJ</source>. <year>2013</year>;<volume>1</volume>:<fpage>e103</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7717/peerj.103" xlink:type="simple">10.7717/peerj.103</ext-link></comment> <object-id pub-id-type="pmid">23882441</object-id>; PubMed Central PMCID: PMCPMC3719130.</mixed-citation></ref>
<ref id="pcbi.1005995.ref041"><label>41</label><mixed-citation publication-type="other" xlink:type="simple">The IUCN Red List of Threatened Species. Version 2017–1 [Internet]. 2017 [cited Downloaded on 12 May 2017.]. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.iucnredlist.org" xlink:type="simple">http://www.iucnredlist.org</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Everingham</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Van Gool</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Winn</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zisserman</surname> <given-names>A</given-names></name>. <article-title>The pascal visual object classes (voc) challenge</article-title>. <source>International journal of computer vision</source>. <year>2010</year>;<volume>88</volume>(<issue>2</issue>):<fpage>303</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref043"><label>43</label><mixed-citation publication-type="other" xlink:type="simple">Szewczak JM. Sonobat 2010.</mixed-citation></ref>
<ref id="pcbi.1005995.ref044"><label>44</label><mixed-citation publication-type="other" xlink:type="simple">Binary Acoustic Technology. SCAN’R. 2014.</mixed-citation></ref>
<ref id="pcbi.1005995.ref045"><label>45</label><mixed-citation publication-type="other" xlink:type="simple">Wildlife Acoustics. Kaleidoscope. 2012.</mixed-citation></ref>
<ref id="pcbi.1005995.ref046"><label>46</label><mixed-citation publication-type="other" xlink:type="simple">Lasseck M, editor Large-scale Identification of Birds in Audio Recordings. CLEF (Working Notes); 2014.</mixed-citation></ref>
<ref id="pcbi.1005995.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bas</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bas</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Julien</surname> <given-names>J-F</given-names></name>. <article-title>Tadarida: A Toolbox for Animal Detection on Acoustic Recordings</article-title>. <source>Journal of Open Research Software</source>. <year>2017</year>;<volume>5</volume>:<fpage>6</fpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.5334/jors.154" xlink:type="simple">http://doi.org/10.5334/jors.154</ext-link></mixed-citation></ref>
<ref id="pcbi.1005995.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Breiman</surname> <given-names>L</given-names></name>. <article-title>Random forests</article-title>. <source>Machine learning</source>. <year>2001</year>;<volume>45</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>32</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref049"><label>49</label><mixed-citation publication-type="other" xlink:type="simple">Walters CL, Browning E, Jones KE. iBats Jersey Review. London, UK: 2016.</mixed-citation></ref>
<ref id="pcbi.1005995.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bates</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Mächler</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bolker</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Walker</surname> <given-names>S</given-names></name>. <source>Fitting Linear Mixed-Effects Models Using lme4</source>. 2015. <year>2015</year>;<volume>67</volume>(<issue>1</issue>):<fpage>48</fpage>. Epub 2015-10-07. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18637/jss.v067.i01" xlink:type="simple">10.18637/jss.v067.i01</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005995.ref051"><label>51</label><mixed-citation publication-type="book" xlink:type="simple"><collab>R Development Core Team</collab>. <source>R: A language and environment for statistical computing</source>. <publisher-loc>Vienna, Austria</publisher-loc>: <publisher-name>R Foundation for Statistical Computing</publisher-name>; <year>2009</year>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roche</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Langton</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Aughney</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Russ</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Marnell</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Lynn</surname> <given-names>D</given-names></name>, <etal>et al</etal>. <article-title>A car-based monitoring method reveals new information on bat populations and distributions in Ireland</article-title>. <source>Animal Conservation</source>. <year>2011</year>;<volume>14</volume>:<fpage>642</fpage>–<lpage>51</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Whitby</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Carter</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Britzke</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Bergeson</surname> <given-names>SM</given-names></name>. <article-title>Evaluation of Mobile Acoustic Techniques for Bat Population Monitoring</article-title>. <source>Acta Chiropterologica</source>. <year>2014</year>;<volume>16</volume>:<fpage>223</fpage>–<lpage>30</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref054"><label>54</label><mixed-citation publication-type="other" xlink:type="simple">Loeb SC, Rodhouse TJ, Ellison LE, Lausen CL, Reichard JD, Irvine KM, et al. A plan for the North American Bat Monitoring Program (NABat). General Technical Report SRS-208. Asheville, NC: U.S.: Department of Agriculture Forest Service, Southern Research Station., 2015.</mixed-citation></ref>
<ref id="pcbi.1005995.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Azam</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Le Viol</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Julien</surname> <given-names>J-F</given-names></name>, <name name-style="western"><surname>Bas</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kerbiriou</surname> <given-names>C</given-names></name>. <article-title>Disentangling the relative effect of light pollution, impervious surfaces and intensive agriculture on bat activity with a national-scale monitoring program</article-title>. <source>Landscape Ecology</source>. <year>2016</year>;<volume>31</volume>(<issue>10</issue>):<fpage>2471</fpage>–<lpage>83</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10980-016-0417-3" xlink:type="simple">10.1007/s10980-016-0417-3</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005995.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Merchant</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Fristrup</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Tyack</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Witt</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Blondel</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Measuring acoustic habitats</article-title>. <source>Methods in Ecology and Evolution</source>. <year>2015</year>;<volume>6</volume>(<issue>3</issue>):<fpage>257</fpage>–<lpage>65</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/2041-210X.12330" xlink:type="simple">10.1111/2041-210X.12330</ext-link></comment> <object-id pub-id-type="pmid">25954500</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lintott</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Bunnefeld</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Minderman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Fuentes-Montemayor</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Mayhew</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Olley</surname> <given-names>L</given-names></name>, <etal>et al</etal>. <article-title>Differential Responses to Woodland Character and Landscape Context by Cryptic Bats in Urban Environments</article-title>. <source>PLOS ONE</source>. <year>2015</year>;<volume>10</volume>(<issue>5</issue>):<fpage>e0126850</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0126850" xlink:type="simple">10.1371/journal.pone.0126850</ext-link></comment> <object-id pub-id-type="pmid">25978034</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Whytock</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Christie</surname> <given-names>J</given-names></name>. <article-title>Solo: an open source, customizable and inexpensive audio recorder for bioacoustic research</article-title>. <source>Methods in Ecology and Evolution</source>. <year>2017</year>;<volume>8</volume>(<issue>3</issue>):<fpage>308</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/2041-210X.12678" xlink:type="simple">10.1111/2041-210X.12678</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005995.ref059"><label>59</label><mixed-citation publication-type="other" xlink:type="simple">van den Oord A, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, et al. Wavenet: A generative model for raw audio. arXiv preprint arXiv:160903499. 2016.</mixed-citation></ref>
<ref id="pcbi.1005995.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hochreiter</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name>. <article-title>Long short-term memory</article-title>. <source>Neural computation</source>. <year>1997</year>;<volume>9</volume>(<issue>8</issue>):<fpage>1735</fpage>–<lpage>80</lpage>. <object-id pub-id-type="pmid">9377276</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kosmala</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wiggins</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Swanson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Simmons</surname> <given-names>B</given-names></name>. <article-title>Assessing data quality in citizen science</article-title>. <source>Frontiers in Ecology and the Environment</source>. <year>2016</year>;<volume>14</volume>(<issue>10</issue>):<fpage>551</fpage>–<lpage>60</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/fee.1436" xlink:type="simple">10.1002/fee.1436</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005995.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Welinder</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Branson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Perona</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Belongie</surname> <given-names>SJ</given-names></name>, editors. <article-title>The multidimensional wisdom of crowds</article-title>. <source>Advances in neural information processing systems</source>; <year>2010</year>.</mixed-citation></ref>
<ref id="pcbi.1005995.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Swanson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kosmala</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lintott</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Packer</surname> <given-names>C</given-names></name>. <article-title>A generalized approach for producing, quantifying, and validating citizen science data from wildlife images</article-title>. <source>Conservation Biology</source>. <year>2016</year>;<volume>30</volume>(<issue>3</issue>):<fpage>520</fpage>–<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/cobi.12695" xlink:type="simple">10.1111/cobi.12695</ext-link></comment> <object-id pub-id-type="pmid">27111678</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Prat</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Taub</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Yovel</surname> <given-names>Y</given-names></name>. <article-title>Everyday bat vocalizations contain information about emitter, addressee, context, and behavior</article-title>. <source>Scientific Reports</source>. <year>2016</year>;<volume>6</volume>:<fpage>39419</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep39419" xlink:type="simple">10.1038/srep39419</ext-link></comment> <object-id pub-id-type="pmid">28005079</object-id></mixed-citation></ref>
<ref id="pcbi.1005995.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kershenbaum</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Blumstein</surname> <given-names>DT</given-names></name>, <name name-style="western"><surname>Roch</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Akçay</surname> <given-names>Ç</given-names></name>, <name name-style="western"><surname>Backus</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Bee</surname> <given-names>MA</given-names></name>, <etal>et al</etal>. <article-title>Acoustic sequences in non-human animals: a tutorial review and prospectus</article-title>. <source>Biological Reviews</source>. <year>2016</year>;<volume>91</volume>(<issue>1</issue>):<fpage>13</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/brv.12160" xlink:type="simple">10.1111/brv.12160</ext-link></comment> <object-id pub-id-type="pmid">25428267</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>