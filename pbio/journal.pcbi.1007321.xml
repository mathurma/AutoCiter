<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-19-00549</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1007321</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Control engineering</subject><subj-group><subject>Control systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Control systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Control systems</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Speech signal processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Somatosensory system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Somatosensory system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Somatosensory system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Jaw</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Jaw</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Digestive system</subject><subj-group><subject>Mouth</subject><subj-group><subject>Tongue</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Digestive system</subject><subj-group><subject>Mouth</subject><subj-group><subject>Tongue</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Acoustic signals</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>The FACTS model of speech motor control: Fusing state estimation and task-based control</article-title>
<alt-title alt-title-type="running-head">The FACTS model of speech motor control: Fusing state estimation and task-based control</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2610-2884</contrib-id>
<name name-style="western">
<surname>Parrell</surname> <given-names>Benjamin</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Ramanarayanan</surname> <given-names>Vikram</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Nagarajan</surname> <given-names>Srikantan</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Houde</surname> <given-names>John</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Communication Sciences and Disorders, University of Wisconsin–Madison, Madison, Wisconsin, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Otolaryngology - Head and Neck Surgery, University of California, San Francisco, San Francisco, California, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Educational Testing Service R&amp;D, San Francisco, California, United States of America</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Department of Radiology and Biomedical Imaging, University of California, San Francisco, San Francisco, California, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Haith</surname> <given-names>Adrian M</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Johns Hopkins University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">bparrell@wisc.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>9</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>3</day>
<month>9</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>9</issue>
<elocation-id>e1007321</elocation-id>
<history>
<date date-type="received">
<day>4</day>
<month>4</month>
<year>2019</year>
</date>
<date date-type="accepted">
<day>2</day>
<month>8</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Parrell et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1007321"/>
<abstract>
<p>We present a new computational model of speech motor control: the Feedback-Aware Control of Tasks in Speech or <italic>FACTS</italic> model. FACTS employs a hierarchical state feedback control architecture to control simulated vocal tract and produce intelligible speech. The model includes higher-level control of speech tasks and lower-level control of speech articulators. The task controller is modeled as a dynamical system governing the creation of desired constrictions in the vocal tract, after Task Dynamics. Both the task and articulatory controllers rely on an internal estimate of the current state of the vocal tract to generate motor commands. This estimate is derived, based on efference copy of applied controls, from a forward model that predicts both the next vocal tract state as well as expected auditory and somatosensory feedback. A comparison between predicted feedback and actual feedback is then used to update the internal state prediction. FACTS is able to qualitatively replicate many characteristics of the human speech system: the model is robust to noise in both the sensory and motor pathways, is relatively unaffected by a loss of auditory feedback but is more significantly impacted by the loss of somatosensory feedback, and responds appropriately to externally-imposed alterations of auditory and somatosensory feedback. The model also replicates previously hypothesized trade-offs between reliance on auditory and somatosensory feedback and shows for the first time how this relationship may be mediated by acuity in each sensory domain. These results have important implications for our understanding of the speech motor control system in humans.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Speaking is one of the most complex motor tasks humans perform, but it’s neural and computational bases are not well understood. We present a new computational model that generates speech movements by comparing high-level language production goals with an internal estimate of the current state of the vocal tract. This model reproduces many key human behaviors, including making appropriate responses to multiple types of external perturbations to sensory feedback, and makes a number of novel predictions about the speech motor system. These results have implications for our understanding of healthy speech as well as speech impairments caused by neurological disorders. They also suggest that the mechanisms of control are shared between speech and other motor domains.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000065</institution-id>
<institution>National Institute of Neurological Disorders and Stroke</institution>
</institution-wrap>
</funding-source>
<award-id>R01NS100440</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Nagarajan</surname> <given-names>Srikantan</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000055</institution-id>
<institution>National Institute on Deafness and Other Communication Disorders</institution>
</institution-wrap>
</funding-source>
<award-id>National Institutes of Health</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Nagarajan</surname> <given-names>Srikantan</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000055</institution-id>
<institution>National Institute on Deafness and Other Communication Disorders</institution>
</institution-wrap>
</funding-source>
<award-id>R01DC017091</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Houde</surname> <given-names>John</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000055</institution-id>
<institution>National Institute on Deafness and Other Communication Disorders</institution>
</institution-wrap>
</funding-source>
<award-id>R01DC017696</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Houde</surname> <given-names>John</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was funded by the following grants from the National Institutes of Health (<ext-link ext-link-type="uri" xlink:href="http://www.nih.gov" xlink:type="simple">www.nih.gov</ext-link>):R01DC017696 (JH,SN), R01DC013979 (SN, JH), R01NS100440 (SN, JN, MLGT), and R01DC017091 (SN,JH). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="0"/>
<page-count count="26"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-09-13</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All simulation data is available in the Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Producing speech is one of the most complex motor activities humans perform. To produce even a single word, the activity of over 100 muscles must be precisely coordinated in space and time. This precise spatiotemporal control is difficult to master, and is not fully adult-like until the late teenage years [<xref ref-type="bibr" rid="pcbi.1007321.ref001">1</xref>]. How the brain and central nervous system (CNS) controls this complex system remains an outstanding question in speech motor neuroscience.</p>
<p>Early models of speech relied on servo control [<xref ref-type="bibr" rid="pcbi.1007321.ref002">2</xref>]. In this type of <italic>feedback control</italic> schema, the current feedback from the <italic>plant</italic> (thing to be controlled–for speech, this would be the articulators of the vocal tract, as well as perhaps the phonatory and respiratory systems) is compared against desired feedback and any discrepancy between the current and desired feedback drives the generation of motor commands to move the plant towards the current production goal. A challenge for any feedback control model of speech is the short, rapid movements that characterize speech motor behavior, with durations in the range of 50-300 ms. This is potentially shorter than the delays in the sensory systems. For speech, measured latencies to respond to external perturbations of the system range from 20-50 ms for unexpected mechanical loads [<xref ref-type="bibr" rid="pcbi.1007321.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref004">4</xref>] to around 150 ms for auditory perturbations [<xref ref-type="bibr" rid="pcbi.1007321.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref006">6</xref>]. Therefore, the information about the state of the vocal tract conveyed by sensory feedback to the CNS is delayed in time. Such delays can cause serious problems for feedback control, leading to unstable movements and oscillations around goal states. Furthermore, speech production is possible even in the absence of auditory feedback, as seen in the ability of healthy speakers to produce speech when auditory feedback is masked by loud noise [<xref ref-type="bibr" rid="pcbi.1007321.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref008">8</xref>]. All of the above factors strongly indicate that speech cannot be controlled purely based on feedback control.</p>
<p>Several alternative approaches have been suggested to address these problems with feedback control in speech production and other motor domains. One approach, the equilibrium point hypothesis [<xref ref-type="bibr" rid="pcbi.1007321.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1007321.ref011">11</xref>], relegates feedback control to short-latency spinal or brainstem circuits operating on proprioceptive feedback, with high-level control based on pre-planned feedforward motor commands. Speech models of this type, such as the GEPPETO model, are able to reproduce many biomechanical aspects of speech but are not sensitive to auditory feedback [<xref ref-type="bibr" rid="pcbi.1007321.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1007321.ref016">16</xref>]. Another approach is to combine feedback and feedforward controllers operating in parallel [<xref ref-type="bibr" rid="pcbi.1007321.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref018">18</xref>]. This is the approach taken by the DIVA model [<xref ref-type="bibr" rid="pcbi.1007321.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1007321.ref022">22</xref>], which combines feedforward control based on desired articulatory positions with auditory and somatosensory feedback controllers. In this way, DIVA is sensitive to sensory feedback (via the feedback controllers) but capable of producing fast movements despite delayed or absent sensory feedback (via the feedforward controller).</p>
<p>A third approach, widely used in motor control models outside of speech, relies on the concept of state feedback control [<xref ref-type="bibr" rid="pcbi.1007321.ref023">23</xref>–<xref ref-type="bibr" rid="pcbi.1007321.ref026">26</xref>]. In this approach, the plant is assumed to have a state that is sufficiently detailed to predict the future behavior of the plant, and a controller drives the state of the plant towards a goal state, thereby accomplishing a desired behavior. A key concept in state feedback control is that the true state of the plant is not known to the controller; instead, it is only possible to estimate this state from efference copy of applied controls and sensory feedback. The internal state estimate is computed by first predicting the next plant state based on the applied controls. This state prediction is then used to generate predictions of expected feedback from the plant, and a comparison between predicted feedback and actual feedback is then used to correct the state prediction. Thus, in this process, the actual feedback from the plant only plays an indirect role in that it is only one of the inputs used to estimate the current state, making the system robust to feedback delays and noise.</p>
<p>We have earlier proposed a speech-specific instantiation of a state feedback control system [<xref ref-type="bibr" rid="pcbi.1007321.ref027">27</xref>]. The primary purpose of this earlier work was to establish the plausibility of the state feedback control architecture for speech production and suggest how such an architecture may be implemented in the human central nervous system. Computationally, our previous work built on models that have been developed in non-speech motor domains [<xref ref-type="bibr" rid="pcbi.1007321.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref025">25</xref>]. Following this work, we implemented the state estimation process as a prototypical Kalman filter [<xref ref-type="bibr" rid="pcbi.1007321.ref028">28</xref>], which provides an optimal posterior estimate of the plant state given a prior (the efference-copy-based state prediction) and a set of observations (the sensory reafference), assuming certain conditions such as a linear system. We subsequently implemented a one-dimensional model of vocal pitch control based on this framework [<xref ref-type="bibr" rid="pcbi.1007321.ref029">29</xref>].</p>
<p>However, the speech production system is substantially more complex than our one-dimensional model of pitch. First, speech production requires the multi-dimensional control of redundant and interacting articulators (e.g., lips, tongue tip, tongue body, jaw, etc.). Second, speech production relies on the control of high-level task goals rather than direct control of the articulatory configuration of the plant (e.g., for speech, positions of the vocal tract articulators). For example, speakers are able to compensate immediately for a bite block which fixes the jaw in place, producing essentially normal vowels [<xref ref-type="bibr" rid="pcbi.1007321.ref030">30</xref>]. Additionally, speakers react to displacement of a speech articulator by making compensatory movements of other articulators: speakers lower the upper lip when the jaw is pulled downward during production of a bilabial [b] [<xref ref-type="bibr" rid="pcbi.1007321.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref004">4</xref>], and raise the lower lip when the upper lip is displaced upwards during production of [p] [<xref ref-type="bibr" rid="pcbi.1007321.ref031">31</xref>]. Importantly, these actions are not reflexes, but are specific to the ongoing speech task. No upper lip movement is seen when the jaw is displaced during production of [z] (which does not require the lips to be close), nor is the lower lip movement increased if the upper lip is raised during production of [f] (where the upper lip is not involved). Together, these results strongly indicate that the goal of speech is not to achieve desired positions of each individual speech articulator, but must rather be to achieve some higher-level goal. While most models of speech motor production thus implement control at a higher speech-relevant level, the precise nature of these goals (vocal tract constrictions [<xref ref-type="bibr" rid="pcbi.1007321.ref032">32</xref>–<xref ref-type="bibr" rid="pcbi.1007321.ref034">34</xref>], auditory patterns [<xref ref-type="bibr" rid="pcbi.1007321.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref021">21</xref>], or both [<xref ref-type="bibr" rid="pcbi.1007321.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref022">22</xref>]) remains an ongoing debate.</p>
<p>One prominent model that employs control of high-level speech tasks rather than direct control of articulatory variables is the Task Dynamic model [<xref ref-type="bibr" rid="pcbi.1007321.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref035">35</xref>]. In Task Dynamics, the state of the plant (current positions and velocities of the speech articulators) is assumed to be available through proprioception. Importantly, this information is not used to directly generate an error or motor command. Rather, the current state of the plant is used to calculate values for various constrictions in the vocal tract (e.g., the distance between the upper and lower lip, the distance between the tongue tip and palate, etc.). It is these <italic>constrictions</italic>, rather than the positions of the individual articulators, that constitute the goals of speech production in Task Dynamics.</p>
<p>The model proposed here (Feedback Aware Control of Tasks in Speech, or <italic>FACTS</italic>) extends the idea of articulatory state estimation from the simple linear pitch control mechanism of our previous SFC model to the highly non-linear speech articulatory system. This presents three primary challenges: first, moving from pitch control to articulatory control requires the implementation of control at a higher level of speech-relevant tasks, rather than at the simpler level of articulator positions. To address this issue, FACTS is built upon the Task Dynamics model, as described above. However, unlike the Task Dynamics model, which assumes the state of the vocal tract is directly available through proprioception, here we model the more realistic situation in which the vocal tract state must be estimated from an efference copy of applied motor commands as well as somatosensory and auditory feedback. The second challenge is that this estimation process is highly non-linear. This required that the implementation of the observer as a Kalman filter in SFC be altered, as this estimation process is only applicable to linear systems. Here, we implement state estimation as an Unscented Kalman Filter [<xref ref-type="bibr" rid="pcbi.1007321.ref036">36</xref>], which is able to account for the nonlinearities in the speech production system and incorporates internal state prediction, auditory feedback, and somatosensory feedback. Lastly, the highly non-linear mapping between articulatory positions and speech acoustics must be approximated in order to predict the auditory feedback during speech. Here, we learn the articulatory-to-acoustic mapping using Locally Weighted Projection Regression or LWPR [<xref ref-type="bibr" rid="pcbi.1007321.ref037">37</xref>]. Thus, in the proposed FACTS model, we combine the hierarchical architecture and task-based control of Task Dynamics with a non-linear state-estimation procedure to develop a model that is capable of rapid movements with or without sensory feedback, yet is still sensitive to external perturbations of both auditory and somatosensory feedback.</p>
<p>In the following sections, we describe the architecture of the model and the computational implementation of each model process. We then describe the results of a number of model simulations designed to test the ability of the model to simulate human speech behavior. First, we probe the behavior of the model when sensory feedback is limited to a single modality (somatosensation or audition), removed entirely, or corrupted by varying amounts of noise. Second, we test the ability of the model to respond to external auditory or mechanical perturbations. In both of these sections, we show that the model performs similarly to human speech behavior and makes new, testable predictions about the effects of sensory deprivation. Lastly, we explore how sensory acuity in both the auditory and somatosensory pathways affects the model’s response to external auditory perturbations. Here, we show that the response to auditory perturbations depends not only on auditory acuity but on somatosensory acuity as well, demonstrating for the first time a potential computational principle that may underlie the demonstrated trade-off in response magnitude to auditory and somatosensory perturbations across human speakers.</p>
<sec id="sec002">
<title>Model overview</title>
<p>A schematic control diagram of the FACTS model is shown in <xref ref-type="fig" rid="pcbi.1007321.g001">Fig 1</xref>. Modules that build on Task Dynamics are shown in blue, and the articulatory state estimation process (or observer) is shown in red. Following Task Dynamics, speech tasks in the FACTS model are hypothesized to be desired constrictions in the vocal tract (e.g., close the lips for a [b]). Each of these speech tasks, or gestures, can be specified in terms of it’s constriction location (where in the vocal tract the constriction is formed) and it’s constriction degree (how narrow the constriction is). We model each gesture as a separate critically-damped second-order system [<xref ref-type="bibr" rid="pcbi.1007321.ref032">32</xref>]. Interestingly, similar dynamical behavior has been seen at a neural population level during the planning and execution of reaching movements in non-human primates [<xref ref-type="bibr" rid="pcbi.1007321.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref039">39</xref>] and recently in human speech movements [<xref ref-type="bibr" rid="pcbi.1007321.ref040">40</xref>], suggesting that a dynamical systems model of task-level control may be an appropriate first approximation to the neural activity that controls movement production. However, the architecture of the model would also allow for tasks in other control spaces, such as auditory targets (c.f. [<xref ref-type="bibr" rid="pcbi.1007321.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref019">19</xref>]), though an appropriate task feedback control law for such targets would need to be developed (consistent with engineering control theory, we refer to the term “controller” as a “control law”). To what extent, if any, incorporation of auditory targets would impact or alter the results presented here is not immediately clear. This is a promising avenue for future research, as it may provide a way to bridge existing models which posit either constriction- or sensory-based targets. We leave such explorations for future work, but note here that the results presented here may apply only to the current formulation of FACTS with constriction-based targets.</p>
<fig id="pcbi.1007321.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007321.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Architecture of the FACTS model.</title>
<p>Boxes in blue represent processes outlined in the Task Dynamics model [<xref ref-type="bibr" rid="pcbi.1007321.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref041">41</xref>]. The articulatory state estimator (shown in red) is implemented as an Unscented Kalman Filter, which estimates the current articulatory state of the plant by combining the predicted state generated by a forward model with auditory and somatosensory feedback. Additive noise is represented by <italic>ε</italic>. Time-step delays are represented by <italic>z</italic><sup>−1</sup>. Equation numbers correspond to equations found in the methods.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007321.g001" xlink:type="simple"/>
</fig>
<p>FACTS uses as the Haskins Configurable Articulatory Synthesizer (or CASY) [<xref ref-type="bibr" rid="pcbi.1007321.ref041">41</xref>–<xref ref-type="bibr" rid="pcbi.1007321.ref043">43</xref>] as the model of the vocal tract <bold>plant</bold> being controlled. The relevant parameters of the CASY model required to move the tongue body to produce a vowel (and which fully describe the articulatory space for the majority of the simulations in this paper) are the Jaw Angle (JA, angle of the jaw relative to the temporomandibular joint), Condyle Angle (CA, the angle of the center of the tongue relative to the jaw, measured at the temporomandibular joint), and the Condyle Length (CL, distance of the center of the tongue from the temporomandibular joint along the Condyle Angle). The CASY model is shown in <xref ref-type="fig" rid="pcbi.1007321.g002">Fig 2</xref>.</p>
<fig id="pcbi.1007321.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007321.g002</object-id>
<label>Fig 2</label>
<caption>
<title>The CASY plant model.</title>
<p>Articulatory variable relevant to the current paper are the Jaw Angle (JA), Condyle Angle (CA), and Condyle Length (CL). See text for a description of these variables. Diagram after [<xref ref-type="bibr" rid="pcbi.1007321.ref078">78</xref>].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007321.g002" xlink:type="simple"/>
</fig>
<p>The model begins by receiving the output from a linguistic planning module. Currently, this is implemented as a <italic>gestural score</italic> in the framework of Articulatory Phonology [<xref ref-type="bibr" rid="pcbi.1007321.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref034">34</xref>]. These gestural scores list the control parameters (e.g., target constriction degree, constriction location, damping, etc.) for each gesture in a desired utterance as well as each gesture’s onset and offset times. For example, the word “mod” ([mad]) has four gestures: simultaneous activation of a gesture driving closure at the lips for [m], a gesture driving an opening of the velum for nasalization of [m], and a gesture driving a wide opening between the tongue and pharyngeal wall for the vowel [a]. These are followed by a gesture driving closure between the tongue tip and hard palate for [d] (<xref ref-type="fig" rid="pcbi.1007321.g003">Fig 3</xref>).</p>
<fig id="pcbi.1007321.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007321.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Example of task variables and gestural score for the word “mod”.</title>
<p>A gestural score for the word “mod” [mad], which consists of a bilabial closure and velum opening gestures for [m], a wide constriction in the pharynx for [a], and a tongue tip closure gesture for [d]. Tasks are shown on the left, and a schematic of the gestural score on the right.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007321.g003" xlink:type="simple"/>
</fig>
<p>The <bold>task state feedback control law</bold> takes these gestural scores as input and generates a task-level command based on the current state of the ongoing constriction tasks. In this way, the task-level commands are dependent on the current task-level state. For example, if the lips are already closed during production of a /b/, a very different command needs to be generated than if the lips are far apart. These task-level commands are converted into motor commands that can drive changes in the positions of the speech articulators by the <bold>articulatory state feedback control law</bold>, using information about the current articulatory state of the vocal tract. The motor commands generate changes in the model vocal tract articulators (or <bold>plant</bold>), which are then used to generate an acoustic signal.</p>
<p>The <bold>articulatory state estimator</bold> (sometimes called an observer in other control models) combines a copy of the outgoing motor command (or efference copy) with auditory and somatosensory feedback to generate an internal estimate of the articulatory state of the plant. First, the efference copy of the motor command is used (in combination with the previous aritculatory state estimate) to generate a prediction of the articulatory state. This is then used by a <bold>forward model</bold> (learned here via LWPR) to generate auditory and somatosensory predictions, which are compared to incoming sensory signals to generate sensory errors. Subsequently, these sensory errors are used to correct the state prediction to generate the final state estimate.</p>
<p>The final articulatory state estimate is used by the articulatory state feedback control law to generate the next motor command, as well as being passed to the <bold>task state estimator</bold> to estimate the current task state, or values (positions) and first derivatives (velocities) of the speech tasks (note the Task State was called the Vocal Tract State in earlier presentations of the model [<xref ref-type="bibr" rid="pcbi.1007321.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref045">45</xref>]). Finally, this estimated task-level state is passed to the task state feedback control law to generate the next task-level command.</p>
<p>A more detailed mathematical description of the model can be found in the methods.</p>
</sec>
</sec>
<sec id="sec003" sec-type="results">
<title>Results</title>
<p>Here we present results showing the accuracy of the learned forward model and of various modeling experiments designed to test the ability of the model to qualitatively replicate human speech motor behavior under various conditions, including both normal speech as well as externally perturbed speech.</p>
<sec id="sec004">
<title>Forward model accuracy</title>
<p>
<xref ref-type="fig" rid="pcbi.1007321.g004">Fig 4</xref> visualizes a three dimensional subspace of the learned mapping from the 10-dimensional articulatory state space to the 3-dimensional space of formant frequencies (F1—F3). Specifically, we look at the mapping from the tongue condyle length and condyle angle to the first (see <xref ref-type="fig" rid="pcbi.1007321.g004">Fig 4A–4C</xref>) and second formants (see <xref ref-type="fig" rid="pcbi.1007321.g004">Fig 4D–4F</xref>), projected onto each two-dimensional plane. We also plot normalized histograms of the number of receptive fields that cover each region of the space (represented as a heatmap in <xref ref-type="fig" rid="pcbi.1007321.g004">Fig 4A and 4D</xref> and with a thick blue line in the other subplots). In each figure, the size of the circles is proportional to the absolute value of the error between the actual and predicted formant values. Overall, the fit of the model results in an average error magnitude of 4.2 Hz (std., 9.1 Hz) for F1 and 6.6 Hz (std., 19.1 Hz) for F2. For comparison, the range of the data was 310–681 Hz for F1 and 930–2197 Hz for F2. Fit error increases in regions of the space that are relatively sparsely covered by receptive fields. In addition, the higher frequency of smaller circles at the margins of the distribution (and therefore the edges of the articulatory space) suggest that we may need fewer receptive fields to cover these regions. Of course, this means that we do see some bigger circles in these regions where the functional mapping is not adequately represented by a small number of fields. Also note that we are only plotting the number of receptive fields that are employed to cover a given region of articulatory space, and this is <italic>not</italic> indicative of how much weight they carry in representing that region of space.</p>
<fig id="pcbi.1007321.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007321.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Learned LWPR transformations from CASY articulatory parameters to acoustics.</title>
<p>Predicted formant values are shown as white circles. The width of each circle is proportional to the absolute value of the error between the actual formant values and the formant values predicted by the LWPR model. The density distributions reflect the number of receptive fields that cover each point (represented as colors in A, D and the height of the line in B, C, D, F). (A-C) show F1 values. (D-F) show F2 values.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007321.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Model response to changes in sensory feedback availability and noise levels</title>
<p>How does the presence or absence of sensory feedback affect the speech motor control system? While there is no direct evidence to date on the effects of total loss of sensory information in human speech, some evidence comes from when sensory feedback from a single modality is attenuated or eliminated. Notably, the effects of removing auditory and somatosensory feedback differ. In terms of auditory feedback, speech production is relatively unaffected by it’s absence: speech is generally unaffected when auditory feedback is masked by loud masking noise [<xref ref-type="bibr" rid="pcbi.1007321.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref008">8</xref>]. However, alterations to somatosensory feedback have larger effects: blocking oral tactile sensation through afferent nerve injections or oral anesthesia leads to substantial imprecision in speech articulation [<xref ref-type="bibr" rid="pcbi.1007321.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref047">47</xref>].</p>
<p>
<xref ref-type="fig" rid="pcbi.1007321.g005">Fig 5</xref> presents simulations from the FACTS model testing the ability of the model to replicate the effects of removing sensory feedback seen in human speech. All simulations modeled the vowel sequence [ǝ a i]. 100 simulations were run for each of four conditions: normal feedback (5B), somatosensory feedback only (5C), auditory feedback only (5D), and no sensory feedback (5E). For clarity, only the trajectory of the tongue body in the CASY articulatory model is shown for each simulation.</p>
<fig id="pcbi.1007321.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007321.g005</object-id>
<label>Fig 5</label>
<caption>
<title>FACTS simulation of the vowel sequence [ǝ a i], varying the type of sensory feedback available to the model.</title>
<p>(A) shows a sample simulation with movements of the CASY model as well as the trajectory of the tongue center. (B-E) each show tongue center trajectories from 100 simulations (gray) and their mean (blue-green gradient) with varying types of sensory feedback available. Variability is lower when sensory feedback is available, and lower when auditory feedback is absent compared to when somatosensory feedback is absent. (F) shows the prediction error in each condition. (G-H) show the produced variability in two articulatory parameters of the CASY plant model related to vowel production and (I) shows variability of the tongue center at the final simulation sample. (J) and (K) show prediction error and articulatory variability relative to sensory noise levels when only one feedback channel is available. Decreasing sensory noise leads to increased accuracy for somatosensation but decreased accuracy for audition. Colors in (F-K) correspond to the colors in the titles of (B-E).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007321.g005" xlink:type="simple"/>
</fig>
<p>In the normal feedback condition (<xref ref-type="fig" rid="pcbi.1007321.g005">Fig 5B</xref>), the tongue lowers from [ǝ] to [a], then raises and fronts from [a] to [i]. Note that there is some variability across simulation runs due to the noise in the motor and sensory systems. This variability is also found in human behavior and the ability of the state feedback control architecture to replicate this variability is a strength of this approach [<xref ref-type="bibr" rid="pcbi.1007321.ref025">25</xref>].</p>
<p>The effect of removing auditory feedback (<xref ref-type="fig" rid="pcbi.1007321.g005">Fig 5C</xref>) leads to a significant, though small, increase in the variability of the tongue body movement as measured by the tongue location at the movement endpoint (<xref ref-type="fig" rid="pcbi.1007321.g005">Fig 5I</xref>), though this effect was not seen in measures of Condyle Angle or Condyle Length variability (<xref ref-type="fig" rid="pcbi.1007321.g005">Fig 5G and 5H</xref>). Interestingly, while variablity increased, prediction error slightly decreased in this condition (<xref ref-type="fig" rid="pcbi.1007321.g005">Fig 5F</xref>). Overall, these results are consistent with experimental results that demonstrate that speech is essentially unaffected, in the short term, by the loss of auditory information (though auditory feedback is important for pitch and amplitude regulation [<xref ref-type="bibr" rid="pcbi.1007321.ref048">48</xref>] as well as to maintain articulatory accuracy in the long term [<xref ref-type="bibr" rid="pcbi.1007321.ref048">48</xref>–<xref ref-type="bibr" rid="pcbi.1007321.ref050">50</xref>]).</p>
<p>Removing somatosensory feedback while maintaining auditory feedback (<xref ref-type="fig" rid="pcbi.1007321.g005">Fig 5B</xref>) leads to an increase in both variability across simulation runs as well as an increase in prediction error (<xref ref-type="fig" rid="pcbi.1007321.g005">Fig 5F–5I</xref>). This result is broadly consistent with the fact that reduction of tactile sensation via oral anaesthetic or nerve block leads to imprecise articulation for both consonants and vowels [<xref ref-type="bibr" rid="pcbi.1007321.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref051">51</xref>] (though the acoustic effects of this imprecision may be less perceptible for vowels [<xref ref-type="bibr" rid="pcbi.1007321.ref047">47</xref>]). However, a caveat must be made that our current model does not include tactile sensation, only proprioceptive information. Unfortunately, it is impossible to block proprioceptive information from the tongue, as that afferent information is likely carried along the same nerve (hypoglossal nerve) as the efferent motor commands [<xref ref-type="bibr" rid="pcbi.1007321.ref052">52</xref>]. It is difficult to prove, then, exactly how a complete loss of proprioception would affect speech. Nonetheless, the current model results are consistent with studies that how shown severe dyskinesia in reaching movements after elimination of proprioception in non-human primates (see [<xref ref-type="bibr" rid="pcbi.1007321.ref053">53</xref>] for a review) and in human patients with severe proprioceptive deficits [<xref ref-type="bibr" rid="pcbi.1007321.ref054">54</xref>]. In summary, although the FACTS model currently includes only proprioceptive sensory information rather than both proprioceptive and tactile signals, these simulation results are consistent with a critical role for the somatosensory system in maintaining the fine accuracy of the speech motor control system.</p>
<p>While removal of only auditory feedback lead to only small increases in variability (in both FACTS simulations and human speech), our simulations show speech in the complete absence of sensory feedback (<xref ref-type="fig" rid="pcbi.1007321.g005">Fig 5E</xref>) shows much larger variability than the absence of either auditory or somatosensory feedback alone. This is consistent with human behvaior [<xref ref-type="bibr" rid="pcbi.1007321.ref051">51</xref>], and occurs because without sensory feedback there is no way to detect and correct for the noise inherent in the motor system (shown by the large prediction errors and increased articulatory variability in <xref ref-type="fig" rid="pcbi.1007321.g005">Fig 5F</xref>).</p>
<p>The effects of changing the noise levels in the system can be see in <xref ref-type="fig" rid="pcbi.1007321.g005">Fig 5J and 5K</xref>. For these simulations, only one type of feedback was used at a time: somatosensory (cyan) or auditory (purple). Noise levels (shown on the x axis) reflect both the sensory system noise and the internal estimate of that noise, which were set to be equal. Each data point reflects 100 stable simulations. Data for the acoustic-only simulations are not shown for noise levels below 1e-5 as the model became highly unstable in these conditions to due inaccurate articulatory state estimates (the number of unstable or divergent simulations is shown in <xref ref-type="fig" rid="pcbi.1007321.g005">Fig 5J</xref>). For the somatosensory system, the prediction error and articulatory variability (shown here for the Condyle Angle) <italic>decrease</italic> as the noise decreases. However, for the auditory system, both prediction error and articulatory variability <italic>increase</italic> as the noise decreases. Because of the Kalman gain, decreased noise in a sensory or predictive signal leads not only to a more accurate signal, but also to a greater reliance on that signal compared to the internal state prediction. When the system relies more on the somatosensory signal, this results in a more accurate state estimate as the somatosensory signal directly reflects the state of the plant. When the system relies more on the auditory signal, however, this results in a less accurate state estimate as the auditory signal only indirectly reflects the state of the plant as a result of the nonlinear, many-to-one articulatory-to-acoustic mapping of the vocal tract. Thus, relying principally on the auditory signal to estimate the state of the speech articulators leads to inaccuracies in the final estimate and, subsequently, high trial-to-trial variability in movements generated from these estimates.</p>
<p>In sum, FACTS is able to replicate the variability seen in human speech, as well as qualitatively match the effects of both auditory and somatosensory masking on speech accuracy. While the variability of human speech in the absence of proprioceptive feedback remains untested, the FACTS simulation results make a strong prediction that could be empirically tested in future work if some manner of blocking or altering proprioceptive signals could be devised.</p>
</sec>
<sec id="sec006">
<title>Model response to mechanical and auditory perturbations</title>
<p>When a downward mechanical load is applied to the jaw during the production of a consonant, speakers respond by increasing the movements of the other speech articulators in a task-specific manner to achieve full closure of the vocal tract [<xref ref-type="bibr" rid="pcbi.1007321.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref031">31</xref>]. For example, when the jaw is perturbed during production of a bilabial stops /b/ or /p/, the upper lip moves downward to a greater extent than normal to compensate for the lower jaw position. This upper lip lowering is not found for jaw perturbations during /f/ or /z/, indicating it is specific to sounds produced using the upper lip. Conversely, tongue muscle activity is larger following jaw perturbation for /z/, which involves a constriction made with the tongue tip, but not for /b/, for which the tongue is not actively involved.</p>
<p>The ability to sense and compensate for mechanical perturbations relies on the somatosensory system. We tested the ability of FACTS to reproduce the task-specific compensatory responses to jaw load application seen in human speakers by applying a small downward acceleration to the jaw (Jaw Angle parameter in CASY) starting midway through a consonant closure for stops produced with the lips (/b/) and tongue tip (/d/). The perturbation continued to the end of the word. As shown in <xref ref-type="fig" rid="pcbi.1007321.g006">Fig 6A</xref>, the model produces greater lowering of the upper lip (as well as greater raising of the lower lip) when the jaw is fixed during production of /b/, but not during /d/, mirroring the observed response in human speech.</p>
<fig id="pcbi.1007321.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007321.g006</object-id>
<label>Fig 6</label>
<caption>
<title>FACTS model simulations of mechanical and auditory perturbations.</title>
<p>Times when the perturbations are applied are shown in gray. (A) shows the response of the model to fixing the jaw in place (simulating a downward force applied to the jaw) midway through the closure for second consonant in [aba] (left) and [ada] (right). Unperturbed trajectories are shown in black and perturbed trajectories in red. The upper and lower lips move more to compensate for the jaw perturbation only when the perturbation occurs on [b], mirroring the task-specific response seen in human speakers. (B) shows the response of the FACTS model to a +100 Hz auditory perturbation of F1 while producing [ǝ]. The produced F1 is shown in black and the perceived F1 is shown in blue. Note that the perceived F1 is corrupted by noise. The model responds to the perturbation by lowering F1 despite the lack of an explicit auditory target. The partial compensation to the perturbation produced by the model matches that observed in human speech.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007321.g006" xlink:type="simple"/>
</fig>
<p>In addition to the task-specific response to mechanical perturbations, speakers will also adjust their speech in response to auditory perturbations [<xref ref-type="bibr" rid="pcbi.1007321.ref055">55</xref>]. For example, when the first vowel formant (F1) is artificially shifted upwards, speakers produce within-utterance compensatory responses by lowering their produced F1. The magnitude of these responses only partially compensates for the perturbation, unlike the complete responses produced for mechanical perturbations. While the exact reason for this partial compensation is not known, it has been hypothesized to relate to small feedback gains [<xref ref-type="bibr" rid="pcbi.1007321.ref019">19</xref>] or conflict with the somatosensory feedback system [<xref ref-type="bibr" rid="pcbi.1007321.ref014">14</xref>]. We explore the cause of this partial compensation below, but focus here on the ability of the model to replicate the observed behavior.</p>
<p>To test the ability of the FACTS model to reproduce the observed partial responses to auditory feedback perturbations, we simulated production of a steady-state [ǝ] vowel. After a brief stabilization period, we abruptly introduced a +100 Hz perturbation of F1 by adding 100 Hz to the perceived F1 signal in the auditory processing stage. This introduced a discrepancy between the produced F1 (shown in black in <xref ref-type="fig" rid="pcbi.1007321.g006">Fig 6B</xref>) and the perceived F1 (shown in blue in <xref ref-type="fig" rid="pcbi.1007321.g006">Fig 6B</xref>). Upon introduction of the perturbation, the model starts to produce a compensatory lowering of F1, eventually reaching a steady value below the unperturbed production. This compensation, like the response in human speakers, is only partial (roughly 20 Hz or 15% of the total perturbation).</p>
<p>Importantly, FACTS produces compensation for auditory perturbations despite having no auditory targets in the current model. Previously, such compensation has been seen as evidence in favor of the existence of auditory targets for speech [<xref ref-type="bibr" rid="pcbi.1007321.ref014">14</xref>]. In FACTS, auditory perturbations cause a change in the estimated state of the vocal tract on which the task-level and articulatory-level feedback controllers operate. This causes a change in motor behavior compared to the unperturbed condition, resulting in what appears to be compensation for the auditory perturbation. Our model results thus show that this compensation is possible without explicit auditory goals. Of course, these results do not argue that auditory goals do not exist. Rather, we show that they are not necessary for this particular behavior.</p>
</sec>
<sec id="sec007">
<title>Model trade-offs between auditory and somatosensory acuity</title>
<p>The amount of compensation to an auditory perturbation has been found to vary substantially between individuals [<xref ref-type="bibr" rid="pcbi.1007321.ref055">55</xref>]. One explanation for the inter-individual variability is that the degree of compensation is related to the acuity of the auditory system. Indeed, some studies have found a relationship between magnitude of the compensatory response to auditory perturbation of vowel formants and auditory acuity for vowel formants [<xref ref-type="bibr" rid="pcbi.1007321.ref056">56</xref>] or other auditory parameters [<xref ref-type="bibr" rid="pcbi.1007321.ref057">57</xref>]. This point is not uncontroversial, however, as this relationship is not always present and the potential link between somatosensory acuity and response magnitude has not been established [<xref ref-type="bibr" rid="pcbi.1007321.ref058">58</xref>]. If we assume that acuity is inversely related to the amount of noise in the sensory system, this explanation fits with the UKF implementation of the state estimation procedure in FACTS, where the weight assigned to the auditory error is ultimately related to the estimate of the noise in the auditory system. In <xref ref-type="fig" rid="pcbi.1007321.g007">Fig 7B</xref>, we show that by varying the amount of noise in the auditory system (along with the internal estimate of that noise), we can drive differences in the amount of compensation the model produces to a +100 Hz perturbation of F1. When we double the auditory noise compared to baseline (top), the compensatory response is reduced. When we halve the auditory noise (bottom), the response increases.</p>
<fig id="pcbi.1007321.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007321.g007</object-id>
<label>Fig 7</label>
<caption>
<title>FACTS model simulations of the response to a +100 Hz perturbation of F1.</title>
<p>(A) and (B) show the effects of altering the amount of noise in the auditory system in tandem with the observer’s estimate of that noise. An increase in auditory noise (A) leads to a smaller perturbation response, while a decrease in auditory noise (B) leads to a larger response. (C) and (D) show the effects of altering the amount of noise in the somatosensory system in tandem with the observer’s estimate of that noise. The pattern is the opposite of that for auditory noise. Here, an increase in somatosensory noise (A) leads to a larger perturbation response, while a decrease in somatosensory noise (B) leads to a smaller response. The baseline perturbation response is shown as a dashed grey line in each plot.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007321.g007" xlink:type="simple"/>
</fig>
<p>Interestingly, the math underlying the UKF suggests that the magnitude of the response to an auditory error should be tied not only to the acuity of the auditory system, but to the acuity of the somatosensory system as well. This is because the weights assigned by the Kalman filter take the full noise covariance of all sensory systems into account. We verified this prediction empirically by running a second set of simulated responses to the same +100 Hz perturbation of F1, this time maintaining the level of auditory noise constant while varying only the level of somatosensory noise. The results can be seen in <xref ref-type="fig" rid="pcbi.1007321.g007">Fig 7C and 7D</xref>. When the level of somatosensory noise is increased, the response to the auditory perturbation increases. Conversely, when the level of somatosensory noise is reduced, the compensatory response is reduced as well. These results suggest that the compensatory response in human speakers should be related to the acuity of the somatosensory system as well as the auditory system, a hypothesis which we are currently testing experimentally. Broadly, however, these results agree with, and provide a testable hypothesis about the cause of, empirical findings that show a trading relationship across speakers in their response to auditory and somatosensory perturbations [<xref ref-type="bibr" rid="pcbi.1007321.ref059">59</xref>].</p>
</sec>
</sec>
<sec id="sec008" sec-type="conclusions">
<title>Discussion</title>
<p>The proposed FACTS model provides a novel way to understand the speech motor system. The model is an implementation of state feedback control that combines high-level control of speech tasks with a non-linear method for estimating the current articulatory state to drive speech motor behavior. We have shown that the model replicates many important characteristics of human speech motor behavior: the model produces stable articulatory behavior, but with some trial-to-trial variability. This variability increases when somatosensory information is unavailable, but is largely unaffected by the loss of auditory feedback.</p>
<p>The model is also able to reproduce task-specific responses to external perturbations. For somatosensory perturbations, when a downward force is applied to the jaw during production of an oral consonant, there is an immediate task-specific compensatory response only in those articulators needed to produce the current task. This is seen in the increased movement of the upper and lower lips to compensate for the jaw perturbation during production of a bilabial /b/ but no alterations in lip movements when the jaw was perturbed during production of a tongue-tip consonant /d/. The ability of the model to respond to perturbations in a task-specific manner replicates a critical aspect of human speech behavior and is due to the inclusion of the task state feedback control law in the model [<xref ref-type="bibr" rid="pcbi.1007321.ref035">35</xref>].</p>
<p>For auditory perturbations, we showed that FACTS is able to produce compensatory responses to external perturbations of F1, even though there is no explicit auditory goal in the model. Rather, the auditory signal is used to inform the observer about the current state of the vocal tract articulators. We additionally showed that FACTS is able to produce the inter-individual variability in the magnitude of this compensatory response as well as the previously observed relationship between the magnitude of this response and auditory acuity.</p>
<p>We have also shown that FACTS makes some predictions about the speech motor system that go beyond what has been demonstrated experimentally to date. FACTS predicts that a complete loss of sensory feedback would lead to large increases in articulatory variability beyond those seen in the absence of auditory or somatosensory feedback alone. Additionally, FACTS predicts that the magnitude of compensation for auditory perturbations should be related not only to auditory acuity, but to somatosensory acuity as well. These concrete predictions can be experimentally tested to assess the validity of the FACTS model, testing which is ongoing in our labs.</p>
<p>It is important to note here that, while the FACTS model qualitatively replicates the patterns of variability seen in human movements when feedback is selectively blocked, alternative formulations of the model could potentially lead to a different pattern of results. In the current model, somatosensory and auditory feedback are combined to estimate the state of the speech articulators for low-level articulatory control. Given that somatosensory feedback is more directly informative about this state, it is perhaps unsurprising that removing auditory feedback results in smaller changes in production variability than removing somatosensory feedback. However, auditory feedback may be more directly informative about the task-level state, including cases where the task-level goals are articulatory [<xref ref-type="bibr" rid="pcbi.1007321.ref060">60</xref>] (as in the current version of the model) or, more obviously, where the task-level goals are themselves defined in the auditory dimension [<xref ref-type="bibr" rid="pcbi.1007321.ref019">19</xref>]. Indeed, a recent model for limb control has suggested that task-level sensory feedback (vision) is incorporated into a task-level state estimator, rather than being directly integrated with somatosensory feedback in the articulatory controller [<xref ref-type="bibr" rid="pcbi.1007321.ref061">61</xref>]. A similar use of auditory feedback in the task-level state estimator in FACTS, rather than in the articulatory-level estimator in the current version, may produce different patterns of variability when sensory feedback is blocked. We are currently working on developing such an alternative model to address this issue.</p>
<p>The current version of FACTS uses constriction targets as the goals for task-level control. There are a few considerations regarding this modelling choice that warrant some discussion. First, the ultimate goal of speech production in any theory, at an abstract level, must be to communicate a linguistic message through acoustics. Additionally, all speech movements will necessarily have deterministic acoustic consequences. However, this does not imply that auditory goals must be used at the level of control, which is implemented in FACTS based only on constriction targets. Second, the current results should not be taken as arguing against the existence of auditory goals. Indeed, we believe that auditory goals may play an important role in speech production. While we have shown that auditory targets are not necessary for compensation to acoustic perturbations, they may well be necessary to explain other behaviors [<xref ref-type="bibr" rid="pcbi.1007321.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref062">62</xref>]. Future work can test the ability of FACTS to explain these behaviors. Lastly, while the architecture of FACTS is compatible with auditory goals at the task level, the results of the current model may depend on the choice of task-level targets. Again, future work is planned to explore this issue.</p>
<p>One of the major drawbacks of the current implementation of FACTS is that the model of the plant only requires kinematic control of articulatory positions. While a kinematic approach is relatively widespread in the speech motor control field–including both DIVA and Task Dynamics–there is experimental evidence that the dynamic properties of the articulators, such as gravity and tissue elasticity, need to be accounted for [<xref ref-type="bibr" rid="pcbi.1007321.ref063">63</xref>–<xref ref-type="bibr" rid="pcbi.1007321.ref066">66</xref>]. Moreover, speakers will learn to compensate for perturbations of jaw protrusion that are dependent on jaw velocity [<xref ref-type="bibr" rid="pcbi.1007321.ref059">59</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref067">67</xref>–<xref ref-type="bibr" rid="pcbi.1007321.ref069">69</xref>], indicating that speakers are able to generate motor commands that anticipate and cancel out the effects of those altered articulatory dynamics. While the FACTS model in its current implementation does not replicate this dynamical control of the speech articulators, the overall architecture of the model is compatible with control of dynamics rather than just kinematics [<xref ref-type="bibr" rid="pcbi.1007321.ref023">23</xref>]. Control of articulatory dynamics would require a dynamic model of the plant and the implementation of a new articulatory-level feedback control law that would output motor commands as forces, rather than (or potentially in addition to) articulatory accelerations. Coupled with parallel changes to the articulatory state prediction process, this would allow for FACTS to control a dynamical plant without any changes to the overall architecture of the model.</p>
<p>Another limitation of the current FACTS model is that it does not incorporate sensory delays in any way. Sensory delays are non-neglibile in speech (roughly 30-50 ms for somatosensation and 80-120 ms for audition [<xref ref-type="bibr" rid="pcbi.1007321.ref019">19</xref>]). We are currently exploring methods to incorporate these delays into the model. One potential avenue is to use an extended state representation, where the state (articulatory and/or task) is represented as a matrix where each column represents a time sample [<xref ref-type="bibr" rid="pcbi.1007321.ref070">70</xref>]. Interestingly, this approach has shown that shorter-latency signals are assigned higher weights in the Kalman gain, even when they are inherently more noisy. This suggests another potential reason for why the speech system may rely more on somatosensation for online control than audition, since its latency is much shorter.</p>
<p>While a detailed discussion of the neural basis of the computations in FACTS is beyond the scope of the current paper, in order to demonstrate the plausibility of FACTS as a neural model of speech motor control, we briefly touch on potential neural substrates that may underlie a state-feedback control architecture in speech [<xref ref-type="bibr" rid="pcbi.1007321.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref027">27</xref>]. The cerebellum is widely considered to play a critical role as an internal forward model to predict future articulatory and sensory states [<xref ref-type="bibr" rid="pcbi.1007321.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref071">71</xref>]. The process of state estimation may occur in the parietal cortex [<xref ref-type="bibr" rid="pcbi.1007321.ref024">24</xref>], and indeed inhibitory stimulation of the inferior parietal cortex with transcranial magnetic stimulation impairs sensorimotor learning in speech [<xref ref-type="bibr" rid="pcbi.1007321.ref072">72</xref>], consistent with a role in this process. However, state estimation for speech may also (or alternatively) reside in the ventral premotor cortex (vPMC) for speech, where the premotor cortices are well situated for integrating sensory information (received from sensory corteces via the arcuate fasiculus and the superior longitudinal fasiculus) with motor efference copy from primary motor cortex and cerebellum [<xref ref-type="bibr" rid="pcbi.1007321.ref027">27</xref>]. Another possible role for the vPMC might be in implementing the task state feedback control law [<xref ref-type="bibr" rid="pcbi.1007321.ref061">61</xref>]. Primary motor cortex (M1), with its descending control of the vocal tract musculature and bidirectional monosynaptic connections to primary sensory cortex, is the likely location of the articulatory feedback control law, converting task-level commands from vPMC to articulatory motor commands. Importantly, the differential contributions of vPMC and M1 observed in the movement control literature is consistent with the hierarchical division of task and articulatory control into two distinct levels as specified in FACTS. Interestingly, recent work using electrocorticography has shown that areas in M1 code activation of task-specific muscle synergies similar to those proposed in Task Dynamics and FACTS [<xref ref-type="bibr" rid="pcbi.1007321.ref073">73</xref>]. This suggests that articulatory control may rely on muscle synergies or motor primitives, rather than the control of individual articulators or muscles [<xref ref-type="bibr" rid="pcbi.1007321.ref074">74</xref>].</p>
<p>We have currently implemented the state estimation process in FACTS as an Unscented Kalman Filter. We intend this to be purely a mathematically tractable approximation of the actual neural computational process. Interestingly, recent work suggests that a related approach to nonlinear Bayesian estimation, the Neural Particle Filter, may provide a more neurobiologically plausible basis for the state estimation process [<xref ref-type="bibr" rid="pcbi.1007321.ref075">75</xref>]. Our future extensions of FACTS will involve exploring implementing this type of filter.</p>
<p>In conclusion, the FACTS model uses a widely accepted domain-general approach to motor control, is able to replicate many important speech behaviors, and makes new predictions that can be experimentally tested. This model pushes forward our knowledge of the human speech motor control system, and we plan to further develop the model to incorporate other aspects of speech motor behavior, such as pitch control and sensorimotor learning, in future work.</p>
</sec>
<sec id="sec009" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec010">
<title>Notation</title>
<p>We use the following mathematical notation to present the analyses described in this paper. Matrices are represented by bold uppercase letters (e.g., <bold>X</bold>), while vectors are represented in italics without any bold case (either upper or lower case). We use the notation <bold>X</bold><sup><italic>T</italic></sup> to denote the matrix transpose of <bold>X</bold>. Concatenations of vectors are represented using bold lowercase letters (e.g., <bold>x</bold> = [<italic>x ẋ</italic>]<sup><italic>T</italic></sup>). Scalar quantities are represented without bold and italics. Derivatives and estimates of vectors are represented with dot and tilde superscripts, respectively (i.e., <bold>ẋ</bold> and <inline-formula id="pcbi.1007321.e001"><alternatives><graphic id="pcbi.1007321.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, respectively).</p>
</sec>
<sec id="sec011">
<title>Task state feedback control law</title>
<p>In FACTS, we represent the state of the vocal tract tasks <bold>x</bold><sub><bold>t</bold></sub> = [<italic>x</italic><sub><italic>t</italic></sub> <italic>ẋ</italic><sub><italic>t</italic></sub>]<sup><italic>t</italic></sup> at time <italic>t</italic> by a set of constriction task variables <italic>x</italic><sub><italic>t</italic></sub> (given the current gestural implementation of speech tasks, this is a set of constriction degrees such as lip aperture, tongue tip constriction degree, velic aperture, etc. and constriction locations, such as tongue tip constriction location) and their velocities <italic>ẋ</italic><sub><italic>t</italic></sub>. Given a gestural score generated using a linguistic gestural model [<xref ref-type="bibr" rid="pcbi.1007321.ref076">76</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref077">77</xref>], the <italic>task state feedback control law</italic> (equivalent to the Forward Task Dynamics model in [<xref ref-type="bibr" rid="pcbi.1007321.ref032">32</xref>]) allows us to generate the dynamical evolution of <bold>x</bold><sub><bold>t</bold></sub> using the following simple second-order critically-damped differential equation:
<disp-formula id="pcbi.1007321.e002"><alternatives><graphic id="pcbi.1007321.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¨</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mi mathvariant="bold">M</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:msub><mml:mover accent="true"><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">C</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>x</italic><sub>0</sub> is the active task (or gestural) goal, <bold>M</bold>, <bold>B</bold>, and <bold>C</bold> are respectively the mass matrix, damping coefficient matrix, and stiffness coefficient matrix of the second-order dynamical system model. Essentially, the output of the task feedback controller, <italic>ẍ</italic><sub><italic>t</italic></sub>, can be seen as a desired change (or command) in task space. This is passed to the articulatory state feedback control law to generate appropriate motor commands that will move the plant to achieve the desired task-level change.</p>
<p>Although the model does include a dynamical formulation of the evolution of speech tasks (following [<xref ref-type="bibr" rid="pcbi.1007321.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref035">35</xref>]), this is not intended to model the dynamics of the vocal tract plant itself. Rather, the individual speech tasks are modelled as (abstract) dynamical systems.</p>
</sec>
<sec id="sec012">
<title>Articulatory state feedback control law</title>
<p>The desired task-level state change generated by the task feedback control law, <italic>ẍ</italic><sub><italic>t</italic></sub>, is passed to an articulatory feedback control law. In our implementation of this control law, we use <xref ref-type="disp-formula" rid="pcbi.1007321.e005">Eq 2</xref> (after [<xref ref-type="bibr" rid="pcbi.1007321.ref032">32</xref>]) to perform an inverse kinematics mapping from the task accelerations <italic>ẍ</italic><sub><italic>t</italic></sub> to the model articulator accelerations <italic>ä</italic><sub><italic>t</italic></sub>, a process which is also dependent on the current estimate of the articulator positions <italic>ã</italic><sub><italic>t</italic></sub> and velocities <inline-formula id="pcbi.1007321.e003"><alternatives><graphic id="pcbi.1007321.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:msub><mml:mover accent="true"><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>. <bold>J</bold>(<italic>ã</italic>) is the Jacobian matrix of the forward kinematics model relating changes in articulatory states to changes in task states, <inline-formula id="pcbi.1007321.e004"><alternatives><graphic id="pcbi.1007321.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">J</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the result of differentiating the elements of <bold>J</bold>(<italic>ã</italic>) with respect to time, and <bold>J</bold>(<italic>ã</italic>)* is a weighted Jacobian pseudoinverse of <bold>J</bold>(<italic>ã</italic>).
<disp-formula id="pcbi.1007321.e005"><alternatives><graphic id="pcbi.1007321.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>¨</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">J</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>*</mml:mo></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¨</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">J</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>*</mml:mo></mml:msup> <mml:mover accent="true"><mml:mi mathvariant="bold">J</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula></p>
</sec>
<sec id="sec013">
<title>Plant</title>
<p>In order to generate articulatory movements in CASY, we use Runge-Kutta integration to combine the previous articulatory state of the plant ([<italic>a</italic><sub><italic>t</italic>−1</sub> <italic>ȧ</italic><sub><italic>t</italic>−1</sub>]<sup><italic>T</italic></sup>) with the output of the inverse kinematics computation (<italic>ä</italic><sub><italic>t</italic>−1</sub>, the input to the plant, which we refer to as the <bold>motor command</bold>). This allows us to compute the model articulator positions and velocities for the next time-step ([<italic>a</italic><sub><italic>t</italic></sub> <italic>ȧ</italic><sub><italic>t</italic></sub>]<sup><italic>T</italic></sup>), which effectively “moves” the articulatory vocal tract model. Then, a tube-based <italic>synthesis model</italic> converts the model articulator and constriction task values into the output acoustics (parameterized by the vector <inline-formula id="pcbi.1007321.e006"><alternatives><graphic id="pcbi.1007321.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msubsup><mml:mi>y</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>u</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>). In order to model noise in the neural system, zero-mean Gaussian white noise <italic>ε</italic> is added to the motor command (<italic>ä</italic><sub><italic>t</italic>−1</sub>) received by the plant as well as to the somatosensory (<inline-formula id="pcbi.1007321.e007"><alternatives><graphic id="pcbi.1007321.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msubsup><mml:mi>y</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>o</mml:mi> <mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) and auditory (<inline-formula id="pcbi.1007321.e008"><alternatives><graphic id="pcbi.1007321.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msubsup><mml:mi>y</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>u</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) signals passed from the plant to the articulatory state estimator. Currently, noise levels (standard deviation of Gaussian noise) are tuned by hand for each of these signals (see below for details). Together, the CASY model and the acoustic synthesis process constitute the plant. The model vocal tract in the current implementation of the FACTS model is the Haskins Configurable Articulatory Synthesizer (or CASY) [<xref ref-type="bibr" rid="pcbi.1007321.ref041">41</xref>–<xref ref-type="bibr" rid="pcbi.1007321.ref043">43</xref>].</p>
</sec>
<sec id="sec014">
<title>Articulatory state estimator</title>
<p>The articulatory state estimator generates an estimate of the articulatory state of the plant needed to generate state-dependent motor commands. The final state estimate (<bold>â</bold><sub><bold>t</bold></sub>) generated by the observer is a combination of an articulatory state prediction (<bold>ã</bold><sub><bold>t</bold></sub>) generated from an efference copy of outgoing motor commands, combined with information about the state of the plant derived from the somatosensory and auditory systems (<bold>y</bold><sub><bold>t</bold></sub>). This combination of internal prediction and sensory information is accomplished through the use of an Unscented Kalman Filter (UKF) [<xref ref-type="bibr" rid="pcbi.1007321.ref036">36</xref>], which extends the linear Kalman Filter [<xref ref-type="bibr" rid="pcbi.1007321.ref028">28</xref>] used in most non-speech motor control models [<xref ref-type="bibr" rid="pcbi.1007321.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1007321.ref025">25</xref>] to nonlinear systems like the speech production system.</p>
<p>First, the state prediction is generated using a <bold>forward model</bold> (<inline-formula id="pcbi.1007321.e009"><alternatives><graphic id="pcbi.1007321.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mi mathvariant="script">F</mml:mi></mml:math></alternatives></inline-formula>) that predicts the evolution of the plant based on an estimate of the previous state of the plant (<bold>ã</bold><sub><bold>t</bold>−<bold>1</bold></sub>) and an efference copy of the previously issued motor command (<italic>ä</italic><sub><italic>t</italic>−1</sub>). Based on this predicted state, another forward model (<inline-formula id="pcbi.1007321.e010"><alternatives><graphic id="pcbi.1007321.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mi mathvariant="script">H</mml:mi></mml:math></alternatives></inline-formula>) generates the predicted sensory output <inline-formula id="pcbi.1007321.e011"><alternatives><graphic id="pcbi.1007321.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>o</mml:mi> <mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup> <mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>u</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> (comprising somatosensory and auditory signals <inline-formula id="pcbi.1007321.e012"><alternatives><graphic id="pcbi.1007321.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>o</mml:mi> <mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1007321.e013"><alternatives><graphic id="pcbi.1007321.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>u</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, respectively) that would be generated by the plant in the predicted state. Currently, auditory signals are modelled as the first three formant values (F1-F3; 3 dimensions), and somatosensory signals are modelled as the position and velocities of the speech articulators in the CASY model (20 dimensions).
<disp-formula id="pcbi.1007321.e014"><alternatives><graphic id="pcbi.1007321.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub><mml:mrow><mml:mo>=</mml:mo> <mml:mi mathvariant="script">F</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi mathvariant="bold">t</mml:mi> <mml:mo>-</mml:mo> <mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>¨</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
<disp-formula id="pcbi.1007321.e015"><alternatives><graphic id="pcbi.1007321.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub><mml:mrow><mml:mo>=</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi mathvariant="script">H</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>o</mml:mi> <mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>o</mml:mi> <mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi mathvariant="script">H</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>u</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>u</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula></p>
<p>These predicted sensory signals are then compared with the incoming signals from the somatosensory (<inline-formula id="pcbi.1007321.e016"><alternatives><graphic id="pcbi.1007321.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:msubsup><mml:mi>y</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>o</mml:mi> <mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) and auditory (<inline-formula id="pcbi.1007321.e017"><alternatives><graphic id="pcbi.1007321.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msubsup><mml:mi>y</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>u</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) systems, generating the sensory prediction error (comprising both somatosensory and auditory components) <inline-formula id="pcbi.1007321.e018"><alternatives><graphic id="pcbi.1007321.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>y</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>o</mml:mi> <mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup> <mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>y</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>u</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>:
<disp-formula id="pcbi.1007321.e019"><alternatives><graphic id="pcbi.1007321.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e019" xlink:type="simple"/><mml:math display="block" id="M19"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>y</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>u</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>u</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>y</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>u</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
<disp-formula id="pcbi.1007321.e020"><alternatives><graphic id="pcbi.1007321.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>y</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>o</mml:mi> <mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>o</mml:mi> <mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>y</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>o</mml:mi> <mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula></p>
<p>These sensory prediction errors are used to correct the initial articulatory state prediction, giving a final articulatory state estimate <bold>ã</bold><sub><bold>t</bold></sub>:
<disp-formula id="pcbi.1007321.e021"><alternatives><graphic id="pcbi.1007321.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="script">K</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>Δ</mml:mo> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
where <inline-formula id="pcbi.1007321.e022"><alternatives><graphic id="pcbi.1007321.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:msub><mml:mi mathvariant="script">K</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula> is the Kalman Gain, which effectively specifies the weights given to the sensory signals in informing the final state estimate. Details of how we generate <inline-formula id="pcbi.1007321.e023"><alternatives><graphic id="pcbi.1007321.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mi mathvariant="script">F</mml:mi></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1007321.e024"><alternatives><graphic id="pcbi.1007321.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mi mathvariant="script">H</mml:mi></mml:math></alternatives></inline-formula>, and <inline-formula id="pcbi.1007321.e025"><alternatives><graphic id="pcbi.1007321.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mi mathvariant="script">K</mml:mi></mml:math></alternatives></inline-formula> are given in the following sections.</p>
<sec id="sec015">
<title>Forward models for state and sensory prediction</title>
<p>One of the challenges in estimating the state of the plant is that both the process model <inline-formula id="pcbi.1007321.e026"><alternatives><graphic id="pcbi.1007321.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mi mathvariant="script">F</mml:mi></mml:math></alternatives></inline-formula> (that provides a functional mapping from <inline-formula id="pcbi.1007321.e027"><alternatives><graphic id="pcbi.1007321.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mspace width="0.222222em"/><mml:msub><mml:mover accent="true"><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mspace width="0.222222em"/><mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>¨</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:math></alternatives></inline-formula> to <bold>ã</bold><sub><bold>t</bold></sub>) as well as the observation model <inline-formula id="pcbi.1007321.e028"><alternatives><graphic id="pcbi.1007321.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mi mathvariant="script">H</mml:mi></mml:math></alternatives></inline-formula> (that maps from <bold>ã</bold><sub><bold>t</bold></sub> to <bold>ŷ</bold><sub><bold>t</bold></sub>) are unknown. Currently, we implement the process model <inline-formula id="pcbi.1007321.e029"><alternatives><graphic id="pcbi.1007321.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mi mathvariant="script">F</mml:mi></mml:math></alternatives></inline-formula> by replicating the integration of <italic>ä</italic><sub><italic>t</italic></sub> used to drive changes in the CASY model, which ignores any potential dynamical effects in the plant. However, the underlying architecture (the forward model <inline-formula id="pcbi.1007321.e030"><alternatives><graphic id="pcbi.1007321.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mi mathvariant="script">F</mml:mi></mml:math></alternatives></inline-formula>) is sufficiently general that non-zero articulatory dynamics could be accounted for in predicting <bold>â</bold> as well.</p>
<p>Implementing the observation model is more challenging due to the nonlinear relationship between articulator positions and formant values. In order to solve this problem, we <italic>learn</italic> the observation model functional mappings from articulatory positions to acoustics (<inline-formula id="pcbi.1007321.e031"><alternatives><graphic id="pcbi.1007321.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>u</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">H</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>) required for Unscented Kalman Filtering using Locally Weighted Projection Regression, or LWPR, a computationally efficient machine learning technique [<xref ref-type="bibr" rid="pcbi.1007321.ref037">37</xref>]. While we do not here explicitly relate this machine learning process to human learning, such maps could theoretically be learned during early speech acquisition, such as babbling [<xref ref-type="bibr" rid="pcbi.1007321.ref022">22</xref>]. Currently, we learn only the auditory prediction component of <inline-formula id="pcbi.1007321.e032"><alternatives><graphic id="pcbi.1007321.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mi mathvariant="script">H</mml:mi></mml:math></alternatives></inline-formula>. Since the dimensions of the somatosensory prediction are identical to those of the predicted articulatory state, the former are generated from the latter via an identity function (<inline-formula id="pcbi.1007321.e033"><alternatives><graphic id="pcbi.1007321.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>o</mml:mi> <mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec016">
<title>State correction using an Unscented Kalman Filter</title>
<p>Errors between predicted and afferent sensory signals are used to correct the initial efference-copy-based state prediction through the use of a Kalman gain <inline-formula id="pcbi.1007321.e034"><alternatives><graphic id="pcbi.1007321.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mi mathvariant="script">K</mml:mi></mml:math></alternatives></inline-formula>, which effectively dictates how each dimension of the sensory error signal Δ<bold>y</bold><sub><italic>t</italic></sub> affects each dimension of the state prediction <bold>â</bold><sub><bold>t</bold></sub>. Our previous SFC model of vocal pitch control implemented a Kalman filter to estimate the weights in the Kalman gain [<xref ref-type="bibr" rid="pcbi.1007321.ref029">29</xref>], which provides the optimal state estimate under certain strict conditions, including that the system being estimated is linear [<xref ref-type="bibr" rid="pcbi.1007321.ref028">28</xref>]. However, the traditional Kalman filter approach is only applicable to linear systems, and the speech production mechanism, even in the simplified CASY model used in FACTS, is highly non-linear.</p>
<p>Our goal in generating a state estimate is to combine the state prediction and sensory feedback to generate an optimal or near-optimal solution. To accomplish this, we use an Unscented Kalman Filter (UKF) [<xref ref-type="bibr" rid="pcbi.1007321.ref036">36</xref>], which extends the linear Kalman Filter to non-linear systems. While the UKF has not been proven to provide optimal solutions to the state estimation problem, it consistently provides more accurate estimates than other non-linear extensions of the Kalman filter, such as the Extended Kalman Filter [<xref ref-type="bibr" rid="pcbi.1007321.ref036">36</xref>].</p>
<p>In FACTS, the weights of the Kalman gain are computed as a function of the estimated covariation between the articulatory state and sensory signals, given by the posterior covariance matrices <inline-formula id="pcbi.1007321.e035"><alternatives><graphic id="pcbi.1007321.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:msub><mml:mi mathvariant="bold">P</mml:mi> <mml:mrow><mml:msub><mml:mi mathvariant="bold">a</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1007321.e036"><alternatives><graphic id="pcbi.1007321.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:msub><mml:mi mathvariant="bold">P</mml:mi> <mml:mrow><mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> as follows:
<disp-formula id="pcbi.1007321.e037"><alternatives><graphic id="pcbi.1007321.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mrow><mml:msub><mml:mi mathvariant="script">K</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:mspace width="0.222222em"/><mml:msub><mml:mi mathvariant="bold">P</mml:mi> <mml:mrow><mml:msub><mml:mi mathvariant="bold">a</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub></mml:mrow></mml:msub> <mml:msubsup><mml:mi mathvariant="bold">P</mml:mi> <mml:mrow><mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula></p>
<p>In order to generate the posterior means (the state and sensory predictions) and covariances (used to caculate <inline-formula id="pcbi.1007321.e038"><alternatives><graphic id="pcbi.1007321.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mi mathvariant="script">K</mml:mi></mml:math></alternatives></inline-formula>) in an unscented Kalman Filter (UKF) [<xref ref-type="bibr" rid="pcbi.1007321.ref036">36</xref>], multiple prior points (called sigma points, <inline-formula id="pcbi.1007321.e039"><alternatives><graphic id="pcbi.1007321.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mi mathvariant="script">X</mml:mi></mml:math></alternatives></inline-formula>) are used. These prior points are computed based on the mean and covariance of the prior state. Each of these points is then projected through the nonlinear forward model function (<inline-formula id="pcbi.1007321.e040"><alternatives><graphic id="pcbi.1007321.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mi mathvariant="script">F</mml:mi></mml:math></alternatives></inline-formula> or <inline-formula id="pcbi.1007321.e041"><alternatives><graphic id="pcbi.1007321.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mi mathvariant="script">H</mml:mi></mml:math></alternatives></inline-formula>), after which the posterior mean and covariance can be calculated from the transformed points. This process is called the unscented transform (<xref ref-type="fig" rid="pcbi.1007321.g008">Fig 8</xref>). This is used both to predict the future state of the system (process model, <inline-formula id="pcbi.1007321.e042"><alternatives><graphic id="pcbi.1007321.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mi mathvariant="script">F</mml:mi></mml:math></alternatives></inline-formula>) as well as the expected sensory feedback (observation model, <inline-formula id="pcbi.1007321.e043"><alternatives><graphic id="pcbi.1007321.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mi mathvariant="script">H</mml:mi></mml:math></alternatives></inline-formula>).</p>
<fig id="pcbi.1007321.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007321.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Cartoon showing the unscented transform (UT).</title>
<p>The final estimate of the mean and covariance provide a better fit for the true values than would be achieved with the transformation of only a single point at the pre-transformation mean.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007321.g008" xlink:type="simple"/>
</fig>
<p>First, the sigma points (<inline-formula id="pcbi.1007321.e044"><alternatives><graphic id="pcbi.1007321.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mi mathvariant="script">X</mml:mi></mml:math></alternatives></inline-formula>) are generated:
<disp-formula id="pcbi.1007321.e045"><alternatives><graphic id="pcbi.1007321.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e045" xlink:type="simple"/><mml:math display="block" id="M45"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi mathvariant="bold">t</mml:mi> <mml:mo>-</mml:mo> <mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub> <mml:mo>±</mml:mo> <mml:msqrt><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">P</mml:mi> <mml:mrow><mml:mi mathvariant="bold">t</mml:mi> <mml:mo>-</mml:mo> <mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msqrt> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
where <inline-formula id="pcbi.1007321.e046"><alternatives><graphic id="pcbi.1007321.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi mathvariant="bold">t</mml:mi> <mml:mo>-</mml:mo> <mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">a</mml:mi> <mml:mrow><mml:mi mathvariant="bold">t</mml:mi> <mml:mo>-</mml:mo> <mml:mn mathvariant="bold">1</mml:mn></mml:mrow> <mml:mi mathvariant="bold">T</mml:mi></mml:msubsup> <mml:mspace width="0.222222em"/><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi> <mml:mrow><mml:mi mathvariant="bold">t</mml:mi> <mml:mo>-</mml:mo> <mml:mn mathvariant="bold">1</mml:mn></mml:mrow> <mml:mi mathvariant="bold">T</mml:mi></mml:msubsup> <mml:mspace width="0.222222em"/><mml:msubsup><mml:mi mathvariant="bold">n</mml:mi> <mml:mrow><mml:mi mathvariant="bold">t</mml:mi> <mml:mo>-</mml:mo> <mml:mn mathvariant="bold">1</mml:mn></mml:mrow> <mml:mi mathvariant="bold">T</mml:mi></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mi mathvariant="bold">T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>), and <bold>v</bold> and <bold>n</bold> are estimates of the process noise (noise in the plant articulatory system) and observation noise (noise in the sensory systems), respectively, <italic>L</italic> is the dimension of the articulatory state <bold>a</bold>, λ is a scaling factor, and <bold>P</bold> is the noise covariance of <bold>a</bold>, <bold>v</bold>, and <bold>n</bold>. In our current implementation, the level of noise for <bold>v</bold> and <bold>n</bold> are set to be equal to the level of Gaussian noise added to the plant and sensory systems.</p>
<p>The observer then estimates how the motor command <italic>ä</italic><sub><italic>t</italic></sub> would effect the speech articulators by using the integration model (<inline-formula id="pcbi.1007321.e048"><alternatives><graphic id="pcbi.1007321.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mi mathvariant="script">F</mml:mi></mml:math></alternatives></inline-formula>) to generate the state prediction <inline-formula id="pcbi.1007321.e049"><alternatives><graphic id="pcbi.1007321.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mspace width="0.222222em"/><mml:msub><mml:mover accent="true"><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. First, all sigma points reflecting the articulatory state <inline-formula id="pcbi.1007321.e050"><alternatives><graphic id="pcbi.1007321.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:msup><mml:mi mathvariant="script">X</mml:mi> <mml:mi>a</mml:mi></mml:msup></mml:math></alternatives></inline-formula> and process noise <inline-formula id="pcbi.1007321.e051"><alternatives><graphic id="pcbi.1007321.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:msup><mml:mi mathvariant="script">X</mml:mi> <mml:mi>v</mml:mi></mml:msup></mml:math></alternatives></inline-formula> are passed through <inline-formula id="pcbi.1007321.e052"><alternatives><graphic id="pcbi.1007321.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mi mathvariant="script">F</mml:mi></mml:math></alternatives></inline-formula>:
<disp-formula id="pcbi.1007321.e053"><alternatives><graphic id="pcbi.1007321.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e053" xlink:type="simple"/><mml:math display="block" id="M53"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi mathvariant="script">X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>a</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">F</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msubsup><mml:mi mathvariant="script">X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>a</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>¨</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="script">X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>v</mml:mi></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
and the estimated articulatory state is calculated as the weighted sum of the sigma points where the weights (<italic>W</italic>) are inversely related to the distance of the sigma point from the center of the distribution.
<disp-formula id="pcbi.1007321.e054"><alternatives><graphic id="pcbi.1007321.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e054" xlink:type="simple"/><mml:math display="block" id="M54"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>L</mml:mi></mml:mrow></mml:munderover> <mml:msub><mml:mi>W</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.222222em"/><mml:msubsup><mml:mi mathvariant="script">X</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>a</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
The expected sensory state (<italic>ŷ</italic><sub><italic>t</italic></sub>) is then derived based on the predicted articulatory state in a similar manner, first by projecting the articulatory <inline-formula id="pcbi.1007321.e055"><alternatives><graphic id="pcbi.1007321.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:msup><mml:mi mathvariant="script">X</mml:mi> <mml:mi>a</mml:mi></mml:msup></mml:math></alternatives></inline-formula> and observation noise <inline-formula id="pcbi.1007321.e056"><alternatives><graphic id="pcbi.1007321.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:msup><mml:mi mathvariant="script">X</mml:mi> <mml:mi>n</mml:mi></mml:msup></mml:math></alternatives></inline-formula> sigma points through the articulatory-to-sensory transform <inline-formula id="pcbi.1007321.e057"><alternatives><graphic id="pcbi.1007321.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:mi mathvariant="script">H</mml:mi></mml:math></alternatives></inline-formula>.
<disp-formula id="pcbi.1007321.e058"><alternatives><graphic id="pcbi.1007321.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e058" xlink:type="simple"/><mml:math display="block" id="M58"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">Y</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">H</mml:mi> <mml:mspace width="0.222222em"/><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="script">X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>a</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="script">X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
<disp-formula id="pcbi.1007321.e059"><alternatives><graphic id="pcbi.1007321.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e059" xlink:type="simple"/><mml:math display="block" id="M59"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>L</mml:mi></mml:mrow></mml:munderover> <mml:msub><mml:mi>W</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.222222em"/><mml:msub><mml:mi mathvariant="script">Y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula></p>
<p>Lastly, the posterior covariance matrices <inline-formula id="pcbi.1007321.e060"><alternatives><graphic id="pcbi.1007321.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:msub><mml:mi mathvariant="bold">P</mml:mi> <mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1007321.e061"><alternatives><graphic id="pcbi.1007321.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:msub><mml:mi mathvariant="bold">P</mml:mi> <mml:mrow><mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> necessary to generate the Kalman Gain <inline-formula id="pcbi.1007321.e062"><alternatives><graphic id="pcbi.1007321.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:mi mathvariant="script">K</mml:mi></mml:math></alternatives></inline-formula> as well as the Kalman Gain itself are calculated in the following manner:
<disp-formula id="pcbi.1007321.e063"><alternatives><graphic id="pcbi.1007321.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e063" xlink:type="simple"/><mml:math display="block" id="M63"><mml:mrow><mml:msub><mml:mi mathvariant="bold">P</mml:mi> <mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub></mml:mrow></mml:msub> <mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>L</mml:mi></mml:mrow></mml:munderover> <mml:msub><mml:mi>W</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="script">X</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="script">Y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
<disp-formula id="pcbi.1007321.e064"><alternatives><graphic id="pcbi.1007321.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e064" xlink:type="simple"/><mml:math display="block" id="M64"><mml:mrow><mml:msub><mml:mi mathvariant="bold">P</mml:mi> <mml:mrow><mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub></mml:mrow></mml:msub> <mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>L</mml:mi></mml:mrow></mml:munderover> <mml:msub><mml:mi>W</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="script">Y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="script">Y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula>
<disp-formula id="pcbi.1007321.e065"><alternatives><graphic id="pcbi.1007321.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e065" xlink:type="simple"/><mml:math display="block" id="M65"><mml:mrow><mml:msub><mml:mi mathvariant="script">K</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:mspace width="0.222222em"/><mml:msub><mml:mi mathvariant="bold">P</mml:mi> <mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub></mml:mrow></mml:msub> <mml:msubsup><mml:mi mathvariant="bold">P</mml:mi> <mml:mrow><mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula></p>
</sec>
</sec>
<sec id="sec017">
<title>Task state estimator</title>
<p>Finally, we estimate the vocal tract state estimate at the next time step by passing the articulatory state estimate into a task state estimator, which in our current implementation is a forward kinematics model (see <xref ref-type="disp-formula" rid="pcbi.1007321.e005">Eq 2</xref>) [<xref ref-type="bibr" rid="pcbi.1007321.ref032">32</xref>]. <bold>J</bold>(<italic>ã</italic>), the Jacobian matrix relating changes in articulatory states to changes in task states, is the same as in <xref ref-type="disp-formula" rid="pcbi.1007321.e005">Eq 2</xref>.
<disp-formula id="pcbi.1007321.e066"><alternatives><graphic id="pcbi.1007321.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e066" xlink:type="simple"/><mml:math display="block" id="M66"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(17)</label></disp-formula>
<disp-formula id="pcbi.1007321.e047"><alternatives><graphic id="pcbi.1007321.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007321.e047" xlink:type="simple"/><mml:math display="block" id="M47"><mml:msub><mml:mover accent="true"><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>=</mml:mo> <mml:mi mathvariant="bold">J</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></alternatives> <label>(18)</label></disp-formula></p>
<p>This task state estimate is then passed to the task feedback controller to generate the next task-level command <italic>ẍ</italic><sub><italic>t</italic></sub> using <xref ref-type="disp-formula" rid="pcbi.1007321.e002">Eq 1</xref>.</p>
</sec>
<sec id="sec018">
<title>Model parameter settings</title>
<p>There are a number of tunable parameters in the FACTS model. These include: 1) the noise added to <italic>ä</italic> in the plant, <italic>y</italic><sub><italic>aud</italic></sub> in the auditory system, and <italic>y</italic><sub><italic>somat</italic></sub> in the somatosensory system; 2) the internal estimates of the process (<italic>ä</italic>) and observation <bold>y</bold> noise; and 3) initial values for the process, observation, and state covariance matrices used in the Unscented Kalman Filter. Internal estimates of the process and observation noise were set to be equal to the true noise levels. Noise levels were selected from a range from 1e-1 to 1e-8, scaled by the norm of each signal (equivalent to a SNR range of 10 to 1e8), to achieve the following goals: 1) stable system behavior in the absence of external perturbations, 2) the ability of the model to react to external auditory and somatosensory perturbations, 3) and a partial compensation for external auditory perturbations in line with observed human behavior. The final noise values used were 1e-4 for the plant/process noise, 1e-2 for the auditory noise, and 1e-6 for the somatosensory noise. The discrepancy in the values for the noise between the two sensory domains is proportional to the difference in magnitude between the two signals (300-3100 Hz for the auditory signal, 0-1.2 mm or mm/s for the articulatory position and velocity signals). Process and observation covariance matrices were initialized as identity matrices scaled by the process and observation noise, respectively. The state covariance matrix was initialized as an identity matrix scaled by 1e-2. A relatively wide range of noise values produced similar behavior: the effects of changing the auditory and somatosensory noise levels are discussed in the results section.</p>
</sec>
</sec>
<sec id="sec019">
<title>Supporting information</title>
<supplementary-material id="pcbi.1007321.s001" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007321.s001" xlink:type="simple">
<label>S1 Data</label>
<caption>
<title>The supporting information contains data from all simulations shown in this paper in MATLAB format.</title>
<p>(ZIP)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1007321.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Walsh</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>A</given-names></name>. <article-title>Articulatory movements in adolescents: evidence for protracted development of speech motor control processes</article-title>. <source>J Speech Lang Hear Res</source>. <year>2002</year>;<volume>45</volume>(<issue>6</issue>):<fpage>1119</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1044/1092-4388(2002/090)" xlink:type="simple">10.1044/1092-4388(2002/090)</ext-link></comment> <object-id pub-id-type="pmid">12546482</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fairbanks</surname> <given-names>G</given-names></name>. <article-title>Systematic Research In Experimental Phonetics:* 1. A Theory Of The Speech Mechanism As A Servosystem</article-title>. <source>Journal of Speech and Hearing Disorders</source>. <year>1954</year>;<volume>19</volume>(<issue>2</issue>):<fpage>133</fpage>–<lpage>139</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1044/jshd.1902.133" xlink:type="simple">10.1044/jshd.1902.133</ext-link></comment> <object-id pub-id-type="pmid">13212816</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Abbs</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Gracco</surname> <given-names>VL</given-names></name>. <article-title>Control of complex motor gestures: orofacial muscle responses to load perturbations of lip during speech</article-title>. <source>J Neurophysiol</source>. <year>1984</year>;<volume>51</volume>(<issue>4</issue>):<fpage>705</fpage>–<lpage>23</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1984.51.4.705" xlink:type="simple">10.1152/jn.1984.51.4.705</ext-link></comment> <object-id pub-id-type="pmid">6716120</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kelso</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Tuller</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Vatikiotis-Bateson</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Fowler</surname> <given-names>CA</given-names></name>. <article-title>Functionally specific articulatory cooperation following jaw perturbations during speech: evidence for coordinative structures</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>1984</year>;<volume>10</volume>(<issue>6</issue>):<fpage>812</fpage>–<lpage>32</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0096-1523.10.6.812" xlink:type="simple">10.1037/0096-1523.10.6.812</ext-link></comment> <object-id pub-id-type="pmid">6239907</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Parrell</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Agnew</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Nagarajan</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Houde</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Ivry</surname> <given-names>RB</given-names></name>. <article-title>Impaired Feedforward Control and Enhanced Feedback Control of Speech in Patients with Cerebellar Degeneration</article-title>. <source>J Neurosci</source>. <year>2017</year>;<volume>37</volume>(<issue>38</issue>):<fpage>9249</fpage>–<lpage>9258</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3363-16.2017" xlink:type="simple">10.1523/JNEUROSCI.3363-16.2017</ext-link></comment> <object-id pub-id-type="pmid">28842410</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cai</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Beal</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Ghosh</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Tiede</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Guenther</surname> <given-names>FH</given-names></name>, <name name-style="western"><surname>Perkell</surname> <given-names>JS</given-names></name>. <article-title>Weak responses to auditory feedback perturbation during articulation in persons who stutter: evidence for abnormal auditory-motor transformation</article-title>. <source>PLoS One</source>. <year>2012</year>;<volume>7</volume>(<issue>7</issue>):<fpage>e41830</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0041830" xlink:type="simple">10.1371/journal.pone.0041830</ext-link></comment> <object-id pub-id-type="pmid">22911857</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lane</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Tranel</surname> <given-names>B</given-names></name>. <article-title>The Lombard Sign and the Role of Hearing in Speech</article-title>. <source>Journal of Speech, Language, and Hearing Research</source>. <year>1971</year>;<volume>14</volume>(<issue>4</issue>):<fpage>677</fpage>–<lpage>709</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1044/jshr.1404.677" xlink:type="simple">10.1044/jshr.1404.677</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lombard</surname> <given-names>E</given-names></name>. <article-title>Le signe de l’elevation de la voix</article-title>. <source>Ann Maladies de L’Oreille et du Larynx</source>. <year>1911</year>;<volume>37</volume>:<fpage>2</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Feldman</surname> <given-names>A</given-names></name>. <article-title>Once more on the Equilibrium Point Hypothesis (λ) for motor control</article-title>. <source>Journal of Motor Behavior</source>. <year>1986</year>;<volume>18</volume>:<fpage>17</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/00222895.1986.10735369" xlink:type="simple">10.1080/00222895.1986.10735369</ext-link></comment> <object-id pub-id-type="pmid">15136283</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref010">
<label>10</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Feldman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Adamovich</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ostry</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Flanagan</surname> <given-names>J</given-names></name>. <source>The origin of electromyograms—Explanations based on the Equilibrium Point Hypothesis</source>. <name name-style="western"><surname>Winters</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Woo</surname> <given-names>S</given-names></name>, editors. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer Verlag</publisher-name>; <year>1990</year>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref011">
<label>11</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Feldman</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Levin</surname> <given-names>MF</given-names></name>. <chapter-title>The Equilibrium-Point Hypothesis—Past, Present and Future</chapter-title>. In: <source>Progress in Motor Control. Advances in Experimental Medicine and Biology</source>. <publisher-name>Springer</publisher-name>, <publisher-loc>Boston, MA</publisher-loc>; <year>2009</year>. p. <fpage>699</fpage>–<lpage>726</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref012">
<label>12</label>
<mixed-citation publication-type="other" xlink:type="simple">Perrier P, Ma L, Payan Y. Modeling the production of VCV sequences via the inversion of a biomechanical model of the tongue. In: Proceeding of the INTERSPEECH: Interspeech’2005 - Eurospeech, 9th European Conference on Speech Communication and Technology, Lisbon, Portugal, September 4-8, 2005; 2005. p. 1041–1044.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Perrier</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ostry</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Laboissière</surname> <given-names>R</given-names></name>. <article-title>The Equilibrium Point Hypothesis and Its Application to Speech Motor Control</article-title>. <source>Journal of Speech and Hearing Research</source>. <year>1996</year>;<volume>39</volume>:<fpage>365</fpage>–<lpage>378</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1044/jshr.3902.365" xlink:type="simple">10.1044/jshr.3902.365</ext-link></comment> <object-id pub-id-type="pmid">8729923</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref014">
<label>14</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Perrier</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Fuchs</surname> <given-names>SF</given-names></name>. <chapter-title>Motor Equivalence in Speech Production</chapter-title>. In: <name name-style="western"><surname>Redford</surname> <given-names>M</given-names></name>, editor. <source>The Handbook of Speech Production</source>. <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>Wiley-Blackwell</publisher-name>; <year>2015</year>. p. <fpage>225</fpage>–<lpage>247</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sanguineti</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Laboissière</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Ostry</surname> <given-names>DJ</given-names></name>. <article-title>A dynamic biomechanical model for neural control of speech production</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1998</year>;<volume>103</volume>(<issue>3</issue>):<fpage>1615</fpage>–<lpage>1627</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.421296" xlink:type="simple">10.1121/1.421296</ext-link></comment> <object-id pub-id-type="pmid">9514026</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Laboissière</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Ostry</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Feldman</surname> <given-names>AG</given-names></name>. <article-title>The control of multi-muscle systems: human jaw and hyoid movements</article-title>. <source>Biological Cybernetics</source>. <year>1996</year>;<volume>74</volume>(<issue>4</issue>):<fpage>373</fpage>–<lpage>384</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF00194930" xlink:type="simple">10.1007/BF00194930</ext-link></comment> <object-id pub-id-type="pmid">8936389</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kawato</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gomi</surname> <given-names>H</given-names></name>. <article-title>A computational model of four regions of the cerebellum based on feedback-error learning</article-title>. <source>Biological Cybernetics</source>. <year>1992</year>;<volume>68</volume>(<issue>2</issue>):<fpage>95</fpage>–<lpage>103</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF00201431" xlink:type="simple">10.1007/BF00201431</ext-link></comment> <object-id pub-id-type="pmid">1486143</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref018">
<label>18</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Arbib</surname> <given-names>MA</given-names></name>. <chapter-title>Perceptual Structures and Distributed Motor Control</chapter-title>. In: <name name-style="western"><surname>Brookhart</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Mountcastle</surname> <given-names>VB</given-names></name>, <name name-style="western"><surname>Brooks</surname> <given-names>V</given-names></name>, editors. <source>Handbook of Physiology, Supplement 2: Handbook of Physiology, The Nervous System, Motor Control</source>; <year>1981</year>. p. <fpage>1449</fpage>–<lpage>1480</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref019">
<label>19</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Guenther</surname> <given-names>FH</given-names></name>. <source>Neural control of speech</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>The MIT Press</publisher-name>; <year>2015</year>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Guenther</surname> <given-names>FH</given-names></name>. <article-title>Speech Sound Acquisition, Coarticulation, and Rate Effects in a Neural Network Model of Speech Production</article-title>. <source>Psychological Review</source>. <year>1995</year>;<volume>102</volume>:<fpage>594</fpage>–<lpage>621</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-295X.102.3.594" xlink:type="simple">10.1037/0033-295X.102.3.594</ext-link></comment> <object-id pub-id-type="pmid">7624456</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Guenther</surname> <given-names>FH</given-names></name>, <name name-style="western"><surname>Hampson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>D</given-names></name>. <article-title>A theoretical investigation of reference frames for the planning of speech movements</article-title>. <source>Psychological Review</source>. <year>1998</year>;<volume>105</volume>:<fpage>611</fpage>–<lpage>633</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-295X.105.4.611-633" xlink:type="simple">10.1037/0033-295X.105.4.611-633</ext-link></comment> <object-id pub-id-type="pmid">9830375</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tourville</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Guenther</surname> <given-names>FH</given-names></name>. <article-title>The DIVA model: A neural theory of speech acquisition and production</article-title>. <source>Lang Cogn Process</source>. <year>2011</year>;<volume>26</volume>(<issue>7</issue>):<fpage>952</fpage>–<lpage>981</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01690960903498424" xlink:type="simple">10.1080/01690960903498424</ext-link></comment> <object-id pub-id-type="pmid">23667281</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Scott</surname> <given-names>SH</given-names></name>. <article-title>The computational and neural basis of voluntary motor control and planning</article-title>. <source>Trends Cogn Sci</source>. <year>2012</year>;<volume>16</volume>(<issue>11</issue>):<fpage>541</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2012.09.008" xlink:type="simple">10.1016/j.tics.2012.09.008</ext-link></comment> <object-id pub-id-type="pmid">23031541</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>JW</given-names></name>. <article-title>A computational neuroanatomy for motor control</article-title>. <source>Exp Brain Res</source>. <year>2008</year>;<volume>185</volume>(<issue>3</issue>):<fpage>359</fpage>–<lpage>81</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00221-008-1280-5" xlink:type="simple">10.1007/s00221-008-1280-5</ext-link></comment> <object-id pub-id-type="pmid">18251019</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Todorov</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Jordan</surname> <given-names>MI</given-names></name>. <article-title>Optimal feedback control as a theory of motor coordination</article-title>. <source>Nat Neurosci</source>. <year>2002</year>;<volume>5</volume>(<issue>11</issue>):<fpage>1226</fpage>–<lpage>35</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn963" xlink:type="simple">10.1038/nn963</ext-link></comment> <object-id pub-id-type="pmid">12404008</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Miall</surname> <given-names>RC</given-names></name>. <article-title>Forward Models for Physiological Motor Control</article-title>. <source>Neural Netw</source>. <year>1996</year>;<volume>9</volume>(<issue>8</issue>):<fpage>1265</fpage>–<lpage>1279</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0893-6080(96)00035-4" xlink:type="simple">10.1016/S0893-6080(96)00035-4</ext-link></comment> <object-id pub-id-type="pmid">12662535</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Houde</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Nagarajan</surname> <given-names>SS</given-names></name>. <article-title>Speech production as state feedback control</article-title>. <source>Front Hum Neurosci</source>. <year>2011</year>;<volume>5</volume>:<fpage>82</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2011.00082" xlink:type="simple">10.3389/fnhum.2011.00082</ext-link></comment> <object-id pub-id-type="pmid">22046152</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kalman</surname> <given-names>RE</given-names></name>. <article-title>A New Approach to Linear Filtering and Prediction Problems</article-title>. <source>Journal of Basic Engineering</source>. <year>1960</year>;<volume>82</volume>(<issue>1</issue>):<fpage>35</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1115/1.3662552" xlink:type="simple">10.1115/1.3662552</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref029">
<label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Houde JF, Niziolek C, Kort N, Agnew Z, Nagarajan SS. Simulating a state feedback model of speaking. In: 10th International Seminar on Speech Production; 2014. p. 202–205.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fowler</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Turvey</surname> <given-names>MT</given-names></name>. <article-title>Immediate compensation in bite-block speech</article-title>. <source>Phonetica</source>. <year>1981</year>;<volume>37</volume>(<issue>5-6</issue>):<fpage>306</fpage>–<lpage>326</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1159/000260000" xlink:type="simple">10.1159/000260000</ext-link></comment> <object-id pub-id-type="pmid">7280033</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shaiman</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gracco</surname> <given-names>VL</given-names></name>. <article-title>Task-specific sensorimotor interactions in speech production</article-title>. <source>Experimental Brain Research</source>. <year>2002</year>-<month>10</month>-<day>01</day>;<volume>146</volume>(<issue>4</issue>):<fpage>411</fpage>–<lpage>418</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00221-002-1195-5" xlink:type="simple">10.1007/s00221-002-1195-5</ext-link></comment> <object-id pub-id-type="pmid">12355269</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Saltzman</surname> <given-names>EL</given-names></name>, <name name-style="western"><surname>Munhall</surname> <given-names>KG</given-names></name>. <article-title>A dynamical approach to gestural patterning in speech production</article-title>. <source>Ecological Psychology</source>. <year>1989</year>;<volume>1</volume>(<issue>4</issue>):<fpage>333</fpage>–<lpage>382</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1207/s15326969eco0104_2" xlink:type="simple">10.1207/s15326969eco0104_2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Browman</surname> <given-names>CP</given-names></name>, <name name-style="western"><surname>Goldstein</surname> <given-names>L</given-names></name>. <article-title>Articulatory phonology: An overview</article-title>. <source>Phonetica</source>. <year>1992</year>;<volume>49</volume>(<issue>3-4</issue>):<fpage>155</fpage>–<lpage>180</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1159/000261913" xlink:type="simple">10.1159/000261913</ext-link></comment> <object-id pub-id-type="pmid">1488456</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref034">
<label>34</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Browman</surname> <given-names>CP</given-names></name>, <name name-style="western"><surname>Goldstein</surname> <given-names>L</given-names></name>. <chapter-title>Dynamics and articulatory phonology</chapter-title>. In: <name name-style="western"><surname>Port</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>van Gelder</surname> <given-names>T</given-names></name>, editors. <source>Mind as motion: Explorations in the dynamics of cognition</source>. <publisher-loc>Boston</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1995</year>. p. <fpage>175</fpage>–<lpage>194</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Saltzman</surname> <given-names>E</given-names></name>. <article-title>Task dynamic coordination of the speech articulators: A preliminary model</article-title>. <source>Experimental Brain Research Series</source>. <year>1986</year>;<volume>15</volume>:<fpage>129</fpage>–<lpage>144</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref036">
<label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Wan EA, Van Der Merwe R. The unscented Kalman filter for nonlinear estimation. In: Adaptive Systems for Signal Processing, Communications, and Control Symposium 2000. AS-SPCC. The IEEE 2000. Ieee; 2000. p. 153–158.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref037">
<label>37</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Mitrovic</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Klanke</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Vijayakumar</surname> <given-names>S</given-names></name>. <chapter-title>Adaptive optimal feedback control with learned internal dynamics models</chapter-title>. In: <source>From Motor Learning to Interaction Learning in Robots</source>. <publisher-name>Springer</publisher-name>; <year>2010</year>. p. <fpage>65</fpage>–<lpage>84</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>, <name name-style="western"><surname>Sahani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Churchland</surname> <given-names>MM</given-names></name>. <article-title>Cortical control of arm movements: a dynamical systems perspective</article-title>. <source>Annu Rev Neurosci</source>. <year>2013</year>;<volume>36</volume>:<fpage>337</fpage>–<lpage>59</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-neuro-062111-150509" xlink:type="simple">10.1146/annurev-neuro-062111-150509</ext-link></comment> <object-id pub-id-type="pmid">23725001</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Churchland</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Kaufman</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Foster</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Nuyujukian</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>SI</given-names></name>, <etal>et al</etal>. <article-title>Neural population dynamics during reaching</article-title>. <source>Nature</source>. <year>2012</year>;<volume>487</volume>(<issue>7405</issue>):<fpage>51</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature11129" xlink:type="simple">10.1038/nature11129</ext-link></comment> <object-id pub-id-type="pmid">22722855</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref040">
<label>40</label>
<mixed-citation publication-type="other" xlink:type="simple">Stavisky SD, Willett FR, Murphy BA, Rezaii P, Memberg WD, Miller JP, et al. Neural ensemble dynamics in dorsal motor cortex during speech in people with paralysis. bioRxiv. 2018.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref041">
<label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Saltzman E, Nam H, Krivokapic J, Goldstein L. A task-dynamic toolkit for modeling the effects of prosodic structure on articulation. In: Proceedings of the 4th International Conference on Speech Prosody (Speech Prosody 2008), Campinas, Brazil; 2008.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nam</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Mitra</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tiede</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hasegawa-Johnson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Espy-Wilson</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Saltzman</surname> <given-names>E</given-names></name>, <etal>et al</etal>. <article-title>A procedure for estimating gestural scores from speech acoustics</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2012</year>;<volume>132</volume>(<issue>6</issue>):<fpage>3980</fpage>–<lpage>3989</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.4763545" xlink:type="simple">10.1121/1.4763545</ext-link></comment> <object-id pub-id-type="pmid">23231127</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref043">
<label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Rubin P, Saltzman E, Goldstein L, McGowan R, Tiede M, Browman C. CASY and extensions to the task-dynamic model. In: 1st ETRW on Speech Production Modeling: From Control Strategies to Acoustics; 4th Speech Production Seminar: Models and Data, Autrans, France; 1996.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref044">
<label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Ramanarayanan V, Parrell B, Goldstein L, Nagarajan S, Houde J. A New Model of Speech Motor Control Based on Task Dynamics and State Feedback. In: INTERSPEECH; 2016. p. 3564–3568.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref045">
<label>45</label>
<mixed-citation publication-type="other" xlink:type="simple">Parrell B, Ramanarayanan V, Nagarajan S, Houde JF. FACTS: A hierarchical task-based control model of speech incorporating sensory feedback. In: Interspeech 2018; 2018.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ringel</surname> <given-names>RL</given-names></name>, <name name-style="western"><surname>Steer</surname> <given-names>MD</given-names></name>. <article-title>Some Effects of Tactile and Auditory Alterations on Speech Output</article-title>. <source>Journal of Speech, Language, and Hearing Research</source>. <year>1963</year>;<volume>6</volume>(<issue>4</issue>):<fpage>369</fpage>–<lpage>378</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1044/jshr.0604.369" xlink:type="simple">10.1044/jshr.0604.369</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Scott</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Ringel</surname> <given-names>RL</given-names></name>. <article-title>Articulation without oral sensory control</article-title>. <source>Journal of Speech, Language, and Hearing Research</source>. <year>1971</year>;<volume>14</volume>(<issue>4</issue>):<fpage>804</fpage>–<lpage>818</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1044/jshr.1404.804" xlink:type="simple">10.1044/jshr.1404.804</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lane</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Webster</surname> <given-names>JW</given-names></name>. <article-title>Speech deterioration in postlingually deafened adults</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1991</year>;<volume>89</volume>(<issue>2</issue>):<fpage>859</fpage>–<lpage>866</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1894647" xlink:type="simple">10.1121/1.1894647</ext-link></comment> <object-id pub-id-type="pmid">2016436</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref049">
<label>49</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Cowie</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Douglas-Cowie</surname> <given-names>E</given-names></name>. <source>Postlingually acquired deafness: speech deterioration and the wider consequences</source>. <volume>vol. 62</volume>. <publisher-name>Walter de Gruyter</publisher-name>; <year>1992</year>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Perkell</surname> <given-names>JS</given-names></name>. <article-title>Movement goals and feedback and feedforward control mechanisms in speech production</article-title>. <source>Journal of Neurolinguistics</source>. <year>2012</year>;<volume>25</volume>(<issue>5</issue>):<fpage>382</fpage>–<lpage>407</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneuroling.2010.02.011" xlink:type="simple">10.1016/j.jneuroling.2010.02.011</ext-link></comment> <object-id pub-id-type="pmid">22661828</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Putnam</surname> <given-names>AHB</given-names></name>, <name name-style="western"><surname>Ringel</surname> <given-names>RL</given-names></name>. <article-title>A Cineradiographic Study of Articulation in Two Talkers with Temporarily Induced Oral Sensory Deprivation</article-title>. <source>Journal of Speech, Language, and Hearing Research</source>. <year>1976</year>;<volume>19</volume>(<issue>2</issue>):<fpage>247</fpage>–<lpage>266</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1044/jshr.1902.247" xlink:type="simple">10.1044/jshr.1902.247</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Borden</surname> <given-names>GJ</given-names></name>. <article-title>The Effect of Mandibular Nerve Block Upon the Speech of Four-Year-Old Boys</article-title>. <source>Language and Speech</source>. <year>1976</year>;<volume>19</volume>(<issue>2</issue>):<fpage>173</fpage>–<lpage>178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/002383097601900208" xlink:type="simple">10.1177/002383097601900208</ext-link></comment> <object-id pub-id-type="pmid">1018564</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Desmurget</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Grafton</surname> <given-names>S</given-names></name>. <article-title>Feedback or Feedforward Control: End of a dichotomy</article-title>. <source>Taking action: Cognitive neuroscience perspectives on intentional acts</source>. <year>2003</year>; p. <fpage>289</fpage>–<lpage>338</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gordon</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ghilardi</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Ghez</surname> <given-names>C</given-names></name>. <article-title>Impairments of reaching movements in patients without proprioception. I. Spatial errors</article-title>. <source>Journal of Neurophysiology</source>. <year>1995</year>;<volume>73</volume>(<issue>1</issue>):<fpage>347</fpage>–<lpage>360</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1995.73.1.347" xlink:type="simple">10.1152/jn.1995.73.1.347</ext-link></comment> <object-id pub-id-type="pmid">7714577</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Purcell</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Munhall</surname> <given-names>KG</given-names></name>. <article-title>Compensation following real-time manipulation of formants in isolated vowels</article-title>. <source>J Acoust Soc Am</source>. <year>2006</year>;<volume>119</volume>(<issue>4</issue>):<fpage>2288</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.2173514" xlink:type="simple">10.1121/1.2173514</ext-link></comment> <object-id pub-id-type="pmid">16642842</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Villacorta</surname> <given-names>VM</given-names></name>, <name name-style="western"><surname>Perkell</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Guenther</surname> <given-names>FH</given-names></name>. <article-title>Sensorimotor adaptation to feedback perturbations of vowel acoustics and its relation to perception</article-title>. <source>J Acoust Soc Am</source>. <year>2007</year>;<volume>122</volume>(<issue>4</issue>):<fpage>2306</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.2773966" xlink:type="simple">10.1121/1.2773966</ext-link></comment> <object-id pub-id-type="pmid">17902866</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Martin</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Niziolek</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Duñabeitia</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Perez</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hernandez</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Carreiras</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Online Adaptation to Altered Auditory Feedback Is Predicted by Auditory Acuity and Not by Domain-General Executive Control Resources</article-title>. <source>Frontiers in Human Neuroscience</source>. <year>2018</year>;<volume>12</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2018.00091" xlink:type="simple">10.3389/fnhum.2018.00091</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Feng</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Gracco</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>Max</surname> <given-names>L</given-names></name>. <article-title>Integration of auditory and somatosensory error signals in the neural control of speech movements</article-title>. <source>Journal of Neurophysiology</source>. <year>2011</year>;<volume>106</volume>(<issue>2</issue>):<fpage>667</fpage>–<lpage>79</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00638.2010" xlink:type="simple">10.1152/jn.00638.2010</ext-link></comment> <object-id pub-id-type="pmid">21562187</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lametti</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Nasir</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Ostry</surname> <given-names>DJ</given-names></name>. <article-title>Sensory preference in speech production revealed by simultaneous alteration of auditory and somatosensory feedback</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>(<issue>27</issue>):<fpage>9351</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0404-12.2012" xlink:type="simple">10.1523/JNEUROSCI.0404-12.2012</ext-link></comment> <object-id pub-id-type="pmid">22764242</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Iskarous</surname> <given-names>K</given-names></name>. <article-title>Vowel constrictions are recoverable from formants</article-title>. <source>Journal of Phonetics</source>. <year>2010</year>;<volume>38</volume>(<issue>3</issue>):<fpage>375</fpage>–<lpage>387</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.wocn.2010.03.002" xlink:type="simple">10.1016/j.wocn.2010.03.002</ext-link></comment> <object-id pub-id-type="pmid">20871808</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref061">
<label>61</label>
<mixed-citation publication-type="other" xlink:type="simple">Haar S, Donchin O. A revised computational neuroanatomy for motor control. bioRxiv. 2019.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nieto-Castanon</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Guenther</surname> <given-names>FH</given-names></name>, <name name-style="western"><surname>Perkell</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Curtin</surname> <given-names>H</given-names></name>. <article-title>A modeling investigation of articulatory variability and acoustic stability during American English /r/production</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>2005</year>;<volume>117</volume>(<issue>5</issue>):<fpage>3196</fpage>–<lpage>3212</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1893271" xlink:type="simple">10.1121/1.1893271</ext-link></comment> <object-id pub-id-type="pmid">15957787</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shiller</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Ostry</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Gribble</surname> <given-names>PL</given-names></name>. <article-title>Effects of Gravitational Load on Jaw Movements in Speech</article-title>. <source>Journal of Neuroscience</source>. <year>1999</year>;<volume>19</volume>(<issue>20</issue>):<fpage>9073</fpage>–<lpage>9080</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.19-20-09073.1999" xlink:type="simple">10.1523/JNEUROSCI.19-20-09073.1999</ext-link></comment> <object-id pub-id-type="pmid">10516324</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shiller</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Ostry</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Gribble</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Laboissière</surname> <given-names>R</given-names></name>. <article-title>Compensation for the Effects of Head Acceleration on Jaw Movement in Speech</article-title>. <source>Journal of Neuroscience</source>. <year>2001</year>;<volume>21</volume>(<issue>16</issue>):<fpage>6447</fpage>–<lpage>6456</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.21-16-06447.2001" xlink:type="simple">10.1523/JNEUROSCI.21-16-06447.2001</ext-link></comment> <object-id pub-id-type="pmid">11487669</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ostry</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Gribble</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Gracco</surname> <given-names>VL</given-names></name>. <article-title>Coarticulation of jaw movements in speech production: is context sensitivity in speech kinematics centrally planned?</article-title> <source>The Journal of Neuroscience</source>. <year>1996</year>;<volume>16</volume>(<issue>4</issue>):<fpage>1570</fpage>–<lpage>1579</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.16-04-01570.1996" xlink:type="simple">10.1523/JNEUROSCI.16-04-01570.1996</ext-link></comment> <object-id pub-id-type="pmid">8778306</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Perrier</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Payan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Zandipour</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Perkell</surname> <given-names>J</given-names></name>. <article-title>Influence of tongue biomechanics on speech movements during the production of velar stop consonants: A modeling study</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>2003</year>;<volume>114</volume>(<issue>3</issue>):<fpage>1582</fpage>–<lpage>1599</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1587737" xlink:type="simple">10.1121/1.1587737</ext-link></comment> <object-id pub-id-type="pmid">14514212</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref067">
<label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tremblay</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Shiller</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Ostry</surname> <given-names>DJ</given-names></name>. <article-title>Somatosensory basis of speech production</article-title>. <source>Nature</source>. <year>2003</year>;<volume>423</volume>(<issue>6942</issue>):<fpage>866</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature01710" xlink:type="simple">10.1038/nature01710</ext-link></comment> <object-id pub-id-type="pmid">12815431</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref068">
<label>68</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Tremblay</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ostry</surname> <given-names>D</given-names></name>. <chapter-title>The Achievement of Somatosensory Targets as an Independent Goal of Speech Production -Special Status of Vowel-to-Vowel Transitions</chapter-title>. In: <name name-style="western"><surname>Divenyi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Greenberg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Meyer</surname> <given-names>G</given-names></name>, editors. <source>Dynamics of Speech Production and Perception</source>. <publisher-loc>Amsterdam, The Netherlands</publisher-loc>: <publisher-name>IOS Press</publisher-name>; <year>2006</year>. p. <fpage>33</fpage>–<lpage>43</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref069">
<label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nasir</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Ostry</surname> <given-names>DJ</given-names></name>. <article-title>Somatosensory Precision in Speech Production</article-title>. <source>Current Biology</source>. <year>2006</year>;<volume>16</volume>(<issue>19</issue>):<fpage>1918</fpage>–<lpage>1923</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2006.07.069" xlink:type="simple">10.1016/j.cub.2006.07.069</ext-link></comment> <object-id pub-id-type="pmid">17027488</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Crevecoeur</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Munoz</surname> <given-names>DP</given-names></name>, <name name-style="western"><surname>Scott</surname> <given-names>SH</given-names></name>. <article-title>Dynamic Multisensory Integration: Somatosensory Speed Trumps Visual Accuracy during Feedback Control</article-title>. <source>Journal of Neuroscience</source>. <year>2016</year>;<volume>36</volume>(<issue>33</issue>):<fpage>8598</fpage>–<lpage>8611</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0184-16.2016" xlink:type="simple">10.1523/JNEUROSCI.0184-16.2016</ext-link></comment> <object-id pub-id-type="pmid">27535908</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref071">
<label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Herzfeld</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>Cerebellum estimates the sensory state of the body</article-title>. <source>Trends Cogn Sci</source>. <year>2014</year>;<volume>18</volume>(<issue>2</issue>):<fpage>66</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2013.10.015" xlink:type="simple">10.1016/j.tics.2013.10.015</ext-link></comment> <object-id pub-id-type="pmid">24263038</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref072">
<label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shum</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shiller</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Baum</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Gracco</surname> <given-names>VL</given-names></name>. <article-title>Sensorimotor integration for speech motor learning involves the inferior parietal cortex</article-title>. <source>Eur J Neurosci</source>. <year>2011</year>;<volume>34</volume>(<issue>11</issue>):<fpage>1817</fpage>–<lpage>22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1460-9568.2011.07889.x" xlink:type="simple">10.1111/j.1460-9568.2011.07889.x</ext-link></comment> <object-id pub-id-type="pmid">22098364</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref073">
<label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chartier</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Anumanchipalli</surname> <given-names>GK</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>EF</given-names></name>. <article-title>Encoding of Articulatory Kinematic Trajectories in Human Speech Sensorimotor Cortex</article-title>. <source>Neuron</source>. <year>2018</year>;<volume>98</volume>(<issue>5</issue>):<fpage>1042</fpage>–<lpage>1054.e4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2018.04.031" xlink:type="simple">10.1016/j.neuron.2018.04.031</ext-link></comment> <object-id pub-id-type="pmid">29779940</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref074">
<label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ramanarayanan</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Goldstein</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Narayanan</surname> <given-names>SS</given-names></name>. <article-title>Spatio-temporal articulatory movement primitives during speech production: Extraction, interpretation, and validation</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2013</year>;<volume>134</volume>(<issue>2</issue>):<fpage>1378</fpage>–<lpage>1394</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.4812765" xlink:type="simple">10.1121/1.4812765</ext-link></comment> <object-id pub-id-type="pmid">23927134</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref075">
<label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kutschireiter</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Surace</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Sprekeler</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Pfister</surname> <given-names>JP</given-names></name>. <article-title>Nonlinear Bayesian filtering and learning: a neuronal dynamics for perception</article-title>. <source>Scientific Reports</source>. <year>2017</year>;<volume>7</volume>(<issue>1</issue>):<fpage>8722</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-017-06519-y" xlink:type="simple">10.1038/s41598-017-06519-y</ext-link></comment> <object-id pub-id-type="pmid">28821729</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007321.ref076">
<label>76</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Nam</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Goldstein</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Saltzman</surname> <given-names>E</given-names></name>. <chapter-title>Self-organization of syllable structure: a coupled oscillator model</chapter-title>. In: <name name-style="western"><surname>Pellegrino</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Marisco</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Chitoran</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Coupé</surname> <given-names>C</given-names></name>, editors. <source>Approaches to phonological complexity</source>. <publisher-loc>Berlin/New York</publisher-loc>: <publisher-name>Mouton de Gruyter</publisher-name>; <year>2009</year>. p. <fpage>299</fpage>–<lpage>328</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref077">
<label>77</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Goldstein</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Nam</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Saltzman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Chitoran</surname> <given-names>I</given-names></name>. <chapter-title>Coupled oscillator planning model of speech timing and syllable structure</chapter-title>. In: <name name-style="western"><surname>Fant</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Fujisaki</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Shen</surname> <given-names>J</given-names></name>, editors. <source>Frontiers in phonetics and speech science</source>. <publisher-loc>Beijng</publisher-loc>: <publisher-name>The Commercial Press</publisher-name>; <year>2009</year>. p. <fpage>239</fpage>–<lpage>249</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007321.ref078">
<label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lammert</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Goldstein</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Narayanan</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Iskarous</surname> <given-names>K</given-names></name>. <article-title>Statistical Methods for Estimation of Direct and Differential Kinematics of the Vocal Tract</article-title>. <source>Speech Commun</source>. <year>2013</year>;<volume>55</volume>(<issue>1</issue>):<fpage>147</fpage>–<lpage>161</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.specom.2012.08.001" xlink:type="simple">10.1016/j.specom.2012.08.001</ext-link></comment> <object-id pub-id-type="pmid">24052685</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>