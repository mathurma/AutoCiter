<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id><journal-title-group>
<journal-title>PLoS Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PBIOLOGY-D-13-00183</article-id>
<article-id pub-id-type="doi">10.1371/journal.pbio.1001675</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories>
<title-group>
<article-title>The Assessment of Science: The Relative Merits of Post-Publication Review, the Impact Factor, and the Number of Citations</article-title>
<alt-title alt-title-type="running-head">Assessment of Scientific Merit</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Eyre-Walker</surname><given-names>Adam</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Stoletzki</surname><given-names>Nina</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>School of Life Sciences, University of Sussex, Brighton, United Kingdom</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Hannover, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Eisen</surname><given-names>Jonathan A.</given-names></name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of California Davis, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">a.c.eyre-walker@sussex.ac.uk</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>The author(s) have made the following declarations about their contributions: Conceived and designed the experiments: AEW NS. Performed the experiments: AEW NS. Analyzed the data: AEW NS. Contributed reagents/materials/analysis tools: AEW NS. Wrote the paper: AEW NS.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>10</month><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>8</day><month>10</month><year>2013</year></pub-date>
<volume>11</volume>
<issue>10</issue>
<elocation-id>e1001675</elocation-id>
<history>
<date date-type="received"><day>15</day><month>1</month><year>2013</year></date>
<date date-type="accepted"><day>26</day><month>8</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Eyre-Walker, Stoletzki</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article id="RA1" related-article-type="companion" ext-link-type="uri" vol="" page="e1001677" xlink:type="simple" xlink:href="info:doi/10.1371/journal.pbio.1001677"> <article-title>Expert Failure: Re-evaluating Research Assessment</article-title></related-article>
<abstract abstract-type="toc"><sec>
<title/>
<p>Because both subjective post-publication review and the number of citations are highly error prone and biased measures of merit of scientific papers, journal-based metrics may be a better surrogate.</p>
</sec></abstract>
<abstract>
<p>The assessment of scientific publications is an integral part of the scientific process. Here we investigate three methods of assessing the merit of a scientific paper: subjective post-publication peer review, the number of citations gained by a paper, and the impact factor of the journal in which the article was published. We investigate these methods using two datasets in which subjective post-publication assessments of scientific publications have been made by experts. We find that there are moderate, but statistically significant, correlations between assessor scores, when two assessors have rated the same paper, and between assessor score and the number of citations a paper accrues. However, we show that assessor score depends strongly on the journal in which the paper is published, and that assessors tend to over-rate papers published in journals with high impact factors. If we control for this bias, we find that the correlation between assessor scores and between assessor score and the number of citations is weak, suggesting that scientists have little ability to judge either the intrinsic merit of a paper or its likely impact. We also show that the number of citations a paper receives is an extremely error-prone measure of scientific merit. Finally, we argue that the impact factor is likely to be a poor measure of merit, since it depends on subjective assessment. We conclude that the three measures of scientific merit considered here are poor; in particular subjective assessments are an error-prone, biased, and expensive method by which to assess merit. We argue that the impact factor may be the most satisfactory of the methods we have considered, since it is a form of pre-publication review. However, we emphasise that it is likely to be a very error-prone measure of merit that is qualitative, not quantitative.</p>
</abstract>
<funding-group><funding-statement>This work was supported by the salary paid to AEW. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="8"/></counts></article-meta>
</front>
<body><boxed-text id="pbio-1001675-box001" position="float"><sec id="s1a">
<title>Author summary</title>
<p>Subjective assessments of the merit and likely impact of scientific publications are routinely made by scientists during their own research, and as part of promotion, appointment, and government committees. Using two large datasets in which scientists have made qualitative assessments of scientific merit, we show that scientists are poor at judging scientific merit and the likely impact of a paper, and that their judgment is strongly influenced by the journal in which the paper is published. We also demonstrate that the number of citations a paper accumulates is a poor measure of merit and we argue that although it is likely to be poor, the impact factor, of the journal in which a paper is published, may be the best measure of scientific merit currently available.</p>
</sec></boxed-text><sec id="s2">
<title>Introduction</title>
<p>How should we assess the merit of a scientific publication? Is the judgment of a well-informed scientist better than the impact factor (IF) of the journal the paper is published in, or the number of citations that a paper receives? These are important questions that have a bearing upon both individual careers and university departments. They are also critical to governments. Several countries, including the United Kingdom, Canada, and Australia, attempt to assess the merit of the research being produced by scientists and universities and then allocate funds according to performance. In the United Kingdom, this process was known until recently as the Research Assessment Exercise (RAE) (<ext-link ext-link-type="uri" xlink:href="http://www.rae.ac.uk" xlink:type="simple">www.rae.ac.uk</ext-link>); it has now been rebranded the Research Excellence Framework (REF) (<ext-link ext-link-type="uri" xlink:href="http://www.ref.ac.uk" xlink:type="simple">www.ref.ac.uk</ext-link>). The RAE was first performed in 1986 and has been repeated six times at roughly 5-yearly intervals. Although, the detailed structure of these exercises has varied, they have all relied, to a large extent, on the subjective assessment of scientific publications by a panel of experts.</p>
<p>In a recent attempt to investigate how good scientists are at assessing the merit and impact of a scientific paper, Allen et al. <xref ref-type="bibr" rid="pbio.1001675-Allen1">[1]</xref> asked a panel of experts to rate 716 biomedical papers, which were the outcome of research funded, at least in part, by the Wellcome Trust (WT). They found that the level of agreement between experts was low, but that rater score was moderately correlated to the number of citations the paper had obtained 3 years after publication. However, they also found that the assessor score was more strongly correlated to the IF of the journal in which the paper was published than to the number of citations; it was therefore possible that the correlation between assessor scores, and between assessor scores and the number of citations was a consequence of assessors rating papers in high profile journals more highly, rather than an ability of assessors to judge the intrinsic merit or likely impact of a paper.</p>
<p>Subsequently, Wardle <xref ref-type="bibr" rid="pbio.1001675-Wardle1">[2]</xref> has assessed the reliability of post-publication subjective assessments of scientific publications using the Faculty of 1000 (F1000) database. In the F1000 database, a panel of experts is encouraged to select and recommend the most important research papers from biology and medicine to subscribers of the database. Papers in the F1000 database are rated “recommended,” “must read,” or “exceptional.” He showed, amongst ecological papers, that selected papers were cited more often than non-selected papers, and that papers rated must read or exceptional garnered more citations than those rated recommended. However, the differences were small; the average numbers of citations for non-selected, recommended, and must read/exceptional were 21.6, 30.9, and 37.5, respectively. Furthermore, he noted that F1000 faculty had failed to recommend any of the 12 most heavily cited papers from the year 2005. Nevertheless there is a good correlation between rates of article citation and subjective assessments of research merit at an institutional level for some subjects, including most sciences <xref ref-type="bibr" rid="pbio.1001675-Mahdi1">[3]</xref>.</p>
<p>The RAE and similar procedures are time consuming and expensive. The last RAE, conducted in 2008, cost the British government £12 million to perform <xref ref-type="bibr" rid="pbio.1001675-RAE1">[4]</xref>, and universities an additional £47 million to prepare their submissions <xref ref-type="bibr" rid="pbio.1001675-PA1">[5]</xref>. This has led to the suggestion that it might be better to measure the merit of science using bibliometric methods, either by rating the merit of a paper by the IF of the journal in which it is published, or directly through the number of citations a paper receives <xref ref-type="bibr" rid="pbio.1001675-HM1">[6]</xref>.</p>
<p>Here we investigate three methods of assessing the merit of a scientific publication: subjective post-publication peer review, the number of citations a paper accrues, and the IF. We do not attempt to define merit rigorously; it is simply the qualities in a paper that lead a scientist to rate a paper highly; it is likely that this largely depends upon the perceived importance of the paper. We also largely restrict our analysis to the assessment of merit rather than impact; for example, as we show below, the number of citations, which is a measure of impact, is a very poor measure of the underlying merit of the science, because the accumulation of citations is highly stochastic. We have considered the IF, rather than other measures of journal impact, of which there are many (see <xref ref-type="bibr" rid="pbio.1001675-Bollen1">[7]</xref> for list of 39 measures), because it is simple and widely used.</p>
</sec><sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Datasets</title>
<p>To investigate methods of assessing scientific merit we used two datasets <xref ref-type="bibr" rid="pbio.1001675-EyreWalker1">[8]</xref> in which the merit of a scientific publication had been subjectively assessed by a panel of experts: (i) 716 papers from the WT dataset mentioned in the introduction, each of which had been scored by two assessors and which had been published in 2005, and (ii) 5,811 papers, also published in 2005, from the F1000 database, 1,328 of which had been assessed by more than one assessor. For each of these papers we collated citation information ∼6 years after publication. We also obtained the IF of the journal in which the paper had been published (further details in the <xref ref-type="sec" rid="s5">Materials and Methods</xref>). The datasets have strengths and weaknesses. The F1000 dataset is considerably larger than the WT dataset, but it is papers that the assessors considered good enough to be featured in F1000; the papers therefore probably represent a narrower range of merit than in the WT dataset. Furthermore, the scores of two assessors are not independent in the F1000 dataset because the second assessor might have known the score of the first assessor, and F1000 scores have the potential to affect rates of citation, whereas the WT assessments were independent and confidential. The papers in both datasets are drawn from a diverse set of journals covering a broad range of IFs (<xref ref-type="fig" rid="pbio-1001675-g001">Figure 1</xref>). Perhaps not surprisingly the F1000 data tend to be drawn from journals with higher IF, because they have been chosen by the assessors for inclusion in the F1000 database (Mean IF: WT = 6.6; F1000 = 13.9).</p>
<fig id="pbio-1001675-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001675.g001</object-id><label>Figure 1</label><caption>
<title>The distribution of the impact factor in the two datasets.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001675.g001" position="float" xlink:type="simple"/></fig></sec><sec id="s3b">
<title>Subjective Assessment of Merit</title>
<p>If scientists are good at assessing the merit of a scientific publication, and they agree on what merit is, then there should be a good level of agreement between assessors. Indeed assessors gave the same score in 47% and 50% of cases in the WT and F1000 datasets, respectively (<xref ref-type="table" rid="pbio-1001675-t001">Tables 1</xref> and <xref ref-type="table" rid="pbio-1001675-t002">2</xref>). However, we would have expected them to agree 40% of the time by chance alone in both datasets, so the excess agreement above these expectations is small. The correlations between assessor scores are correspondingly modest (WT r = 0.36, <italic>p</italic>&lt;0.001; F1000 r = 0.26, <italic>p</italic>&lt;0.001; all correlations presented in the text are summarized in <xref ref-type="supplementary-material" rid="pbio.1001675.s001">Table S1</xref>—note Spearman's rank correlations are similar to Pearson's correlations for all analyses and these are given in <xref ref-type="supplementary-material" rid="pbio.1001675.s002">Table S2</xref>). The correlation between assessor scores might be stronger in the WT dataset because the F1000 papers had been selected by the assessors as being good enough to rate; they therefore probably represent a narrower range of merit than in the WT data. Nevertheless the correlation in the F1000 dataset may have been inflated by the fact that the second assessor may have known the score of the first assessor.</p>
<table-wrap id="pbio-1001675-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001675.t001</object-id><label>Table 1</label><caption>
<title>The correspondence between assessor scores for the WT dataset.</title>
</caption><alternatives><graphic id="pbio-1001675-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001675.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="4" align="left" rowspan="1">Second Assessor</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">4</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">First assessor</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">60 (42)</td>
<td align="left" rowspan="1" colspan="1">97</td>
<td align="left" rowspan="1" colspan="1">13</td>
<td align="left" rowspan="1" colspan="1">0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">104</td>
<td align="left" rowspan="1" colspan="1">229 (222)</td>
<td align="left" rowspan="1" colspan="1">76</td>
<td align="left" rowspan="1" colspan="1">1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1">59</td>
<td align="left" rowspan="1" colspan="1">42 (23)</td>
<td align="left" rowspan="1" colspan="1">8</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">6</td>
<td align="left" rowspan="1" colspan="1">6 (0.3)</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>Table gives the number of papers rated 1 to 4 for the WT data. Figures in parentheses are the numbers expected by chance alone. Note the ordering of assessors is of no consequence in the WT data since the assessments were performed simultaneously and independently.</p></fn></table-wrap-foot></table-wrap><table-wrap id="pbio-1001675-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001675.t002</object-id><label>Table 2</label><caption>
<title>The correspondence between assessor scores for the F1000 dataset.</title>
</caption><alternatives><graphic id="pbio-1001675-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001675.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="3" align="left" rowspan="1">Second Assessor</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Recommended</td>
<td align="left" rowspan="1" colspan="1">Must Read</td>
<td align="left" rowspan="1" colspan="1">Exceptional</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">First assessor</td>
<td align="left" rowspan="1" colspan="1">Recommended</td>
<td align="left" rowspan="1" colspan="1">365 (295)</td>
<td align="left" rowspan="1" colspan="1">197</td>
<td align="left" rowspan="1" colspan="1">39</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Must Read</td>
<td align="left" rowspan="1" colspan="1">240</td>
<td align="left" rowspan="1" colspan="1">255 (223)</td>
<td align="left" rowspan="1" colspan="1">76</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Exceptional</td>
<td align="left" rowspan="1" colspan="1">46</td>
<td align="left" rowspan="1" colspan="1">66</td>
<td align="left" rowspan="1" colspan="1">44 (19)</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt102"><label/><p>Table gives the number of papers rated recommended, must read, or exceptional for F1000 papers when both assessments were made within 12 months. Figures in parentheses are the numbers expected by chance alone. Note the second assessor scored the paper after the first assessor and may have known the score the first assessor gave.</p></fn></table-wrap-foot></table-wrap>
<p>Strikingly, as Allen et al. <xref ref-type="bibr" rid="pbio.1001675-Allen1">[1]</xref> noted, there is a strong correlation between assessor score and the IF (WT r = 0.48, <italic>p</italic>&lt;0.001; F1000 r = 0.35, <italic>p</italic>&lt;0.001) (<xref ref-type="fig" rid="pbio-1001675-g002">Figure 2</xref>); in fact the correlation is stronger than that between assessor scores. The correlation between assessor score and IF might arise for two non-mutually exclusive reasons. The correlation might be due to variation in merit and the ability of both assessors and journals to judge this merit; as a result, assessors might score better quality papers more highly and journals with high IFs might publish better quality papers. Alternatively, the correlation might be due to assessor bias; assessors might tend to rate papers in high IF journals more highly irrespective of their intrinsic merit. To investigate which of these explanations is correct, let us assume that the journal of publication does not affect the number of citations a paper accumulates; then the number of citations is likely to be a measure of merit. In fact, analyses of duplicate papers clearly show, as expected, that the journal affects the number of citations a paper receives, with papers in higher IF journals accumulating more citations for a given merit <xref ref-type="bibr" rid="pbio.1001675-Opthof1">[9]</xref>–<xref ref-type="bibr" rid="pbio.1001675-Lariviere1">[11]</xref>; this makes our analysis conservative. Controlling the merit of a paper by using the number of citations as a measure of merit, we find a positive partial correlation between assessor score and IF (partial correlations: WT r = 0.35, <italic>p</italic>&lt;0.001; F1000 r = 0.28, <italic>p</italic>&lt;0.001). This suggests that assessors give higher scores to papers in high IF journals (or underrate the science in low IF journals), independent of their merit.</p>
<fig id="pbio-1001675-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001675.g002</object-id><label>Figure 2</label><caption>
<title>The correlation between assessor score and impact factor in the two datasets.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001675.g002" position="float" xlink:type="simple"/></fig>
<p>We can attempt to quantify the relative influence of IF and merit on assessor score by assuming that the number of citations is a measure of merit and then regressing assessor score against IF and the number of citations simultaneously; in essence this procedure asks how strong the relationship is between assessor score and IF when the number of citations is held constant, and between assessor score and the number of citations when IF is held constant. This analysis shows that assessor score is more strongly dependent upon the IF than the number of citations as judged by standardized regression gradients (WT, IF (b<sub>s</sub> = 0.39) and citations (b<sub>s</sub> = 0.16); F1000, IF (b<sub>s</sub> = 0.30) and citations (b<sub>s</sub> = 0.12)). The analysis underestimates the effect of the IF because the number of citations is affected by the IF of the journal in which the paper was published <xref ref-type="bibr" rid="pbio.1001675-Opthof1">[9]</xref>–<xref ref-type="bibr" rid="pbio.1001675-Lariviere1">[11]</xref>.</p>
<p>The strength of the relationship between assessor score and the IF can be further illustrated by considering papers, in the largest of our datasets, that have similar numbers of citations to each other—those distributed around the mean in the F1000 dataset with between 90 and 110 citations (<xref ref-type="fig" rid="pbio-1001675-g003">Figure 3</xref>). The proportion of papers scored in each of the three categories differs significantly across journals (chi-square test of independence <italic>p</italic>&lt;0.001); the proportion that were rated either must read or exceptional is ∼2-fold higher in journals with IF&gt;20 compared to those with IF&lt;10 (<italic>p</italic>&lt;0.001), and the proportion of papers rated exceptional is ∼10-fold higher (<italic>p</italic>&lt;0.001).</p>
<fig id="pbio-1001675-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001675.g003</object-id><label>Figure 3</label><caption>
<title>The proportion of papers, with between 90 and 110 citations in the F1000 dataset, scored in each category as a function of the IF of the journal in which the paper was published.</title>
<p>The numbers of papers in each category are 131, 194, and 128 for IF&lt;10, 10&lt;IF&lt;20, and IF&gt;20, respectively.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001675.g003" position="float" xlink:type="simple"/></fig>
<p>If we remove the influence of IF upon assessor score, the correlations between assessor scores drop below 0.2 (partial correlations between assessor scores controlling for IF: WT, r = 0.15, <italic>p</italic>&lt;0.001; F1000, r = 0.17, <italic>p</italic>&lt;0.001). Similar patterns are observed within those journals in the F1000 dataset for which we have more than 100 papers; the correlations are typically very weak (<xref ref-type="table" rid="pbio-1001675-t003">Table 3</xref>) (average correlation between assessor scores within journals = 0.11, <italic>p</italic>&lt;0.001).</p>
<table-wrap id="pbio-1001675-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001675.t003</object-id><label>Table 3</label><caption>
<title>Correlations within journals with 100 or more papers in the F1000 dataset.</title>
</caption><alternatives><graphic id="pbio-1001675-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001675.t003" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Journal</td>
<td colspan="2" align="left" rowspan="1">Correlation between Assessor Scores</td>
<td colspan="2" align="left" rowspan="1">Correlation between Assessor Score and the Number of Citations</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><italic>n</italic> Papers</td>
<td align="left" rowspan="1" colspan="1">Correlation</td>
<td align="left" rowspan="1" colspan="1"><italic>n</italic> Papers</td>
<td align="left" rowspan="1" colspan="1">Correlation</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Cell</td>
<td align="left" rowspan="1" colspan="1">114</td>
<td align="left" rowspan="1" colspan="1">0.23<xref ref-type="table-fn" rid="nt103">*</xref></td>
<td align="left" rowspan="1" colspan="1">203</td>
<td align="left" rowspan="1" colspan="1">0.11</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Current Biology</td>
<td align="left" rowspan="1" colspan="1">28</td>
<td align="left" rowspan="1" colspan="1">−0.16</td>
<td align="left" rowspan="1" colspan="1">103</td>
<td align="left" rowspan="1" colspan="1">0.23<xref ref-type="table-fn" rid="nt103">*</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Development</td>
<td align="left" rowspan="1" colspan="1">22</td>
<td align="left" rowspan="1" colspan="1">−0.18</td>
<td align="left" rowspan="1" colspan="1">100</td>
<td align="left" rowspan="1" colspan="1">−0.089</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Journal of Biological Chemistry</td>
<td align="left" rowspan="1" colspan="1">14</td>
<td align="left" rowspan="1" colspan="1">0.44</td>
<td align="left" rowspan="1" colspan="1">219</td>
<td align="left" rowspan="1" colspan="1">0.15<xref ref-type="table-fn" rid="nt103">*</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Journal of Cell Biology</td>
<td align="left" rowspan="1" colspan="1">29</td>
<td align="left" rowspan="1" colspan="1">−0.022</td>
<td align="left" rowspan="1" colspan="1">103</td>
<td align="left" rowspan="1" colspan="1">0.22<xref ref-type="table-fn" rid="nt103">*</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Journal of Neuroscience</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1">−0.063</td>
<td align="left" rowspan="1" colspan="1">133</td>
<td align="left" rowspan="1" colspan="1">−0.057</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Journal of the American Chemical Society</td>
<td align="left" rowspan="1" colspan="1">22</td>
<td align="left" rowspan="1" colspan="1">0.42</td>
<td align="left" rowspan="1" colspan="1">126</td>
<td align="left" rowspan="1" colspan="1">0.043</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Molecular Cell</td>
<td align="left" rowspan="1" colspan="1">32</td>
<td align="left" rowspan="1" colspan="1">−0.049</td>
<td align="left" rowspan="1" colspan="1">121</td>
<td align="left" rowspan="1" colspan="1">0.15</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Nature</td>
<td align="left" rowspan="1" colspan="1">217</td>
<td align="left" rowspan="1" colspan="1">0.15<xref ref-type="table-fn" rid="nt103">*</xref></td>
<td align="left" rowspan="1" colspan="1">375</td>
<td align="left" rowspan="1" colspan="1">0.20<xref ref-type="table-fn" rid="nt105">***</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Neuron</td>
<td align="left" rowspan="1" colspan="1">34</td>
<td align="left" rowspan="1" colspan="1">0.24</td>
<td align="left" rowspan="1" colspan="1">116</td>
<td align="left" rowspan="1" colspan="1">0.13</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">PNAS</td>
<td align="left" rowspan="1" colspan="1">115</td>
<td align="left" rowspan="1" colspan="1">0.32<xref ref-type="table-fn" rid="nt104">**</xref></td>
<td align="left" rowspan="1" colspan="1">531</td>
<td align="left" rowspan="1" colspan="1">0.093<xref ref-type="table-fn" rid="nt103">*</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Science</td>
<td align="left" rowspan="1" colspan="1">199</td>
<td align="left" rowspan="1" colspan="1">0.019</td>
<td align="left" rowspan="1" colspan="1">355</td>
<td align="left" rowspan="1" colspan="1">0.15<xref ref-type="table-fn" rid="nt104">**</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Average</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.11</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.11</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt103"><label>*</label><p><italic>p</italic>&lt;0.05.</p></fn><fn id="nt104"><label>**</label><p><italic>p</italic>&lt;0.01.</p></fn><fn id="nt105"><label>***</label><p>p&lt;0.001.</p></fn></table-wrap-foot></table-wrap>
<p>We can quantify the performance of assessors as follows. Let us consider an additive model in which the score given by an assessor depends upon the merit of the paper plus some error. Under this model the correlation between assessor scores is expected to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e001" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e002" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e003" xlink:type="simple"/></inline-formula> is the variance in merit and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e004" xlink:type="simple"/></inline-formula> is the error variance associated with making an assessment (see <xref ref-type="sec" rid="s5">Materials and Methods</xref> for derivation). If we assume that assessors are unaffected by the IF in making their assessment (which we have shown to be untrue) then we estimate, using the correlation between scores, that the error variance is approximately twice the variance in merit (WT <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e005" xlink:type="simple"/></inline-formula> = 1.8 [bootstrap 95% confidence intervals of 1.4 and 2.5]; F1000 <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e006" xlink:type="simple"/></inline-formula> = 2.9 [2.2–3.9]). If we assume that the correlation between assessor score and IF is entirely due to bias, then we estimate, using the partial correlation between scores, controlling for IF, that the error variance is approximately 5-fold greater than the variance in merit within journals (WT <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e007" xlink:type="simple"/></inline-formula> = 5.5 [3.3–13]; F1000 <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e008" xlink:type="simple"/></inline-formula> = 4.8 [3.4–7.7]). The true value lies somewhere between these extremes, however it is clear that an assessor's score is largely composed of error.</p>
<p>Overall it seems that subjective assessments of science are poor; they do not correlate strongly to each other and they appear to be strongly influenced by the journal in which the paper was published, with papers in high-ranking journals being afforded a higher score than their intrinsic merit warrants.</p>
</sec><sec id="s3c">
<title>Subjective Assessment of Impact</title>
<p>Scientists appear to be poor at assessing the intrinsic merit of a publication, but are they better at predicting the future impact of a scientific paper? There are many means by which impact might be assessed; here we consider the simplest of these, the number of citations a paper has received. As with the correlation between assessor scores, the correlation between the assessor score and the number of citations a paper has accumulated are modest (WT r = 0.38, <italic>p</italic>&lt;0.001; F1000 r = 0.25, <italic>p</italic>&lt;0.001; the distribution of the number of citations is skewed but correlations using the log of the number of citations are similar to those for untransformed values [<xref ref-type="supplementary-material" rid="pbio.1001675.s003">Table S3</xref>]) (<xref ref-type="fig" rid="pbio-1001675-g004">Figure 4</xref>).</p>
<fig id="pbio-1001675-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001675.g004</object-id><label>Figure 4</label><caption>
<title>The correlation between assessor score and the number of citations in the two datasets.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001675.g004" position="float" xlink:type="simple"/></fig>
<p>Part of the correlation between assessor scores and the number of citations may be due to the fact that assessors rank papers in high IF journals more highly (<xref ref-type="fig" rid="pbio-1001675-g002">Figures 2</xref> and <xref ref-type="fig" rid="pbio-1001675-g003">3</xref>) and by definition, papers in high IF journals are more highly cited. If we control for IF, we find that the correlation between assessor score and the number of citations becomes weak (partial correlations between assessor score and citations WT r = 0.15, <italic>p</italic>&lt;0.001; F1000 r = 0.11, <italic>p</italic>&lt;0.001). Similar patterns are observed within journals, for which we have enough data in the F1000 dataset (<xref ref-type="table" rid="pbio-1001675-t003">Table 3</xref>). The weak correlation between assessor score and the number of citations, controlling for IF or journal, means that assessor score explains less than 5% of the variance in the number of citations after controlling for IF; however, it should be appreciated that this is in part because the accumulation of citations is a highly stochastic process (see below). The low correlation between assessor score and the number of citations, controlling for IF, is not due to the lack of variation in the number of citations within journals; in all datasets there is more variance in the number of citations within journals than between them (the ratio of the within to the between journal variance in the number of citations is 1.6 and 3.7 in the WT and F1000 datasets, respectively) (<xref ref-type="fig" rid="pbio-1001675-g005">Figure 5</xref>). The low partial correlation does not appear to be due to differences between fields either; if we re-run the regression of assessor score against IF and the number of citations in the F1000 dataset, but control for the assessor, and hence field of study, we get similar estimates to the analysis in which assessor is not controlled for (F1000 assessor score versus IF (b<sub>s</sub> = 0.37) and citations (b<sub>s</sub> = 0.092)).</p>
<fig id="pbio-1001675-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001675.g005</object-id><label>Figure 5</label><caption>
<title>The distribution of the number of citations in journals with IF&lt;5 and IF&gt;30 in the F1000 dataset.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001675.g005" position="float" xlink:type="simple"/></fig></sec><sec id="s3d">
<title>Number of Citations</title>
<p>An alternative to the subjective assessment of scientific merit is the use of bibliometric measures such as the IF of the journal in which the paper is published or the number of citations the paper receives. The number of citations a paper accumulates is likely to be subject to random fluctuation—two papers of similar merit will not accrue the same number of citations even if they are published in similar journals. We can infer the relative error variance associated with this process as follows. Let us assume that the number of citations within a journal is due to the intrinsic merit of the paper plus some error. The correlation between assessor score and the number of citations is therefore expected to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e009" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e010" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e011" xlink:type="simple"/></inline-formula> is the error variance associated with the accumulation of citations (see <xref ref-type="sec" rid="s5">Materials and Methods</xref> for derivation). Hence we can estimate the error variance associated with the accumulation of citations relative to variance in merit by simultaneously considering the correlation between assessor scores and the correlation between assessor scores and the number of citations.</p>
<p>If we assume that assessors and the number of citations are unaffected by the IF of the journal, then we estimate the ratio of the error variance associated with citations to be approximately 1.5 times the variance in merit (WT <italic>r<sub>c</sub></italic> = 1.5 [0.83–2.7]; F1000 <italic>r<sub>c</sub></italic> = 1.6 [0.86–2.6]) and if we assume that the correlation between assessor score and IF is entirely due to bias then we estimate, using the partial correlation between score and citations, controlling for IF, that the ratio of the error variance to the variance in merit within journals to be greater than 5-fold (WT <italic>r<sub>c</sub></italic> = 5.6 [1.2–42]; F1000 <italic>r<sub>c</sub></italic> = 9.8 [4.0–31]). These estimates underestimate the error variance because they do not take into account the variance associated with which journal a paper gets published in; the stochasticity associated with this process will generate additional variance in the number of citations a paper accumulates if the journal affects the number of citations a paper receives, as analyses of duplicate papers suggest <xref ref-type="bibr" rid="pbio.1001675-Opthof1">[9]</xref>–<xref ref-type="bibr" rid="pbio.1001675-Lariviere1">[11]</xref>.</p>
</sec><sec id="s3e">
<title>Impact Factor</title>
<p>The IF might potentially be a better measure of merit than either a post-publication assessment or the number of citations, since several individuals are typically involved in a decision to publish, so the error variance associated with their combined assessment should be lower than that associated with the number of citations; although such benefits can be partially undermined by having a single individual determine whether a manuscript should be reviewed or by rejecting manuscripts if one review is unsupportive. Unfortunately, it seems likely that the IF will also be subject to considerable error. If we combine <italic>n</italic> independent assessments we expect the ratio of the error variance to the variance in merit in their combined qualitative assessment to be reduced by a factor <italic>n</italic>. Hence, if we assume that pre-publication assessments are of similar quality to post-publication assessments, and that three individuals have equal influence over the decision to publish a paper, their combined assessment is still likely to be dominated by error not merit; e.g., if we average the estimates of <italic>r<sub>s</sub></italic> from the correlation between scores and between scores controlling for IF we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e012" xlink:type="simple"/></inline-formula> = 3.7 and 3.9, for the WT and F1000 datasets, respectively, which means that the ratio of the error variance associated with the combined assessor score will be ∼1.2× the variance in merit; i.e., the error variance is still larger than the variance in merit.</p>
</sec></sec><sec id="s4">
<title>Discussion</title>
<p>Our results have some important implications for the assessment of science. We have shown that scientists are poor at estimating the merit of a scientific publication; their assessments are error prone and biased by the journal in which the paper is published. In addition, subjective assessments are expensive and time-consuming. Scientists are also poor at predicting the future impact of a paper, as measured by the number of citations a paper accumulates. This appears to be due to two factors; scientists are not good at assessing merit and the accumulation of citations is a highly stochastic process, such that two papers of similar merit can accumulate very different numbers of citations just by chance.</p>
<p>The IF and the number of citations are also likely to be poor measures of merit, though they may be better measures of impact. The number of citations is a poor measure of merit for two reasons. First, the accumulation of citations is a highly stochastic process, so the number of citations is only poorly correlated to merit. It has previously been suggested that the error variance associated with the accumulation of citations is small based on the strong correlation between the number of citations in successive years <xref ref-type="bibr" rid="pbio.1001675-Seglen1">[12]</xref>, but such an analysis does not take into account the influence that citations have on subsequent levels of citation—the citations in successive years are not independent. Second, as others have shown, the number of citations is strongly affected by the journal in which the paper is published <xref ref-type="bibr" rid="pbio.1001675-Opthof1">[9]</xref>–<xref ref-type="bibr" rid="pbio.1001675-Lariviere1">[11]</xref>. There are also additional problems associated with using the number of citations as a measure of merit since it is influenced by factors such as the geographic origin of the authors <xref ref-type="bibr" rid="pbio.1001675-Wardle2">[13]</xref>,<xref ref-type="bibr" rid="pbio.1001675-Paris1">[14]</xref>, whether they are English speaking <xref ref-type="bibr" rid="pbio.1001675-Paris1">[14]</xref>,<xref ref-type="bibr" rid="pbio.1001675-Leimu1">[15]</xref>, and the gender of the authors <xref ref-type="bibr" rid="pbio.1001675-Baldi1">[16]</xref>,<xref ref-type="bibr" rid="pbio.1001675-Davenport1">[17]</xref> (though see <xref ref-type="bibr" rid="pbio.1001675-Leimu1">[15]</xref>). The problems of using the number of citations as a measure of merit are also likely to affect other article level metrics such as downloads and social network activity.</p>
<p>The IF is likely to be poor because it is based on subjective assessment, although it does have the benefit of being a pre-publication assessment, and hence not influenced by the journal in which the paper has been published. In fact, given that the scientific community has already made an assessment of a paper's merit in deciding where it should be published, it seems odd to suggest that we could do better with post-publication assessment. Post-publication assessment cannot hope to be better than pre-publication assessment unless more individuals are involved in making the assessment, and even then it seems difficult to avoid the bias in favour of papers published in high-ranking journals that seems to pervade our assessments. However, the correlation between merit and IF is likely to be far from perfect. In fact the available evidence suggests there is little correlation between merit and IF, at least amongst low IF journals. The IF depends upon two factors, the merit of the papers being published by the journal and the effect that the journal has on the number of citations for a given level of merit. In the most extensive analysis of its kind, Lariviere and Gingras <xref ref-type="bibr" rid="pbio.1001675-Lariviere1">[11]</xref> analysed 4,532 cases in which the same paper had been published in two different journals; on average the two journals differed by 2.4-fold in their IFs and the papers differed 1.9-fold in the number of citations they had accumulated, suggesting that the higher IF journals in their analysis had gained their higher IF largely through positive feedback, not by publishing better papers. However, the mean IF of the journals in this study was less than one, and it seems unlikely that the IF is entirely a function of positive feedback amongst higher IF journals. Nevertheless the tendency for journals to affect the number of citations a paper receives means that IFs are NOT a quantitative measure of merit; a paper published in a journal with an IF of 30 is not on average six times better than one published in a journal with an IF of 5.</p>
<p>The IF has a number of additional benefits over subjective post-publication review and the number of citations as measures of merit. First, it is transparent. Second, it removes the difficult task of determining which papers should be selected for submission to an assessment exercise such as the RAE or REF; is it better to submit a paper in a high IF journal, a paper that has been highly cited, even if it appears in a low IF journal, or a paper that the submitter believes is their best work? Third, it is relatively cheap to implement. And fourth it is an instantaneous measure of merit.</p>
<p>The use of IF as a measure merit is unpopular with many scientists, a dissatisfaction that has recently found its voice in the San Francisco Declaration of Research Assessment (DORA) (<ext-link ext-link-type="uri" xlink:href="http://am.ascb.org/dora/" xlink:type="simple">http://am.ascb.org/dora/</ext-link>). The declaration urges institutions, funding bodies, and governments to avoid using journal level metrics, such as the IF, to assess the merit of scientific papers. Instead it promotes the use of subjective review and article level metrics. However, as we have shown, both subjective post-publication review and the number of citations, an example of an article level metric, are highly error prone measures of merit. Furthermore, the declaration fails to appreciate that journal level metrics are a form of pre-publication subjective review.</p>
<p>It has been argued that the IF is a poor measure of merit because the variation in the number of citations, accumulated by papers published in the same journal, is large <xref ref-type="bibr" rid="pbio.1001675-Opthof1">[9]</xref>,<xref ref-type="bibr" rid="pbio.1001675-Seglen2">[18]</xref>; the IF is therefore unrepresentative of the number of citations that individual papers accumulate. However, as we have shown the accumulation of citations is highly stochastic, so we would expect a large variance in the number of citations even if the IF were a perfect measure of merit. There are however many problems with using the IF besides the error associated with the assessment. The IF is influenced by the type of papers that are published and with the way in which the IF is calculated <xref ref-type="bibr" rid="pbio.1001675-Seglen2">[18]</xref>,<xref ref-type="bibr" rid="pbio.1001675-Editors1">[19]</xref>. Furthermore it clearly needs to be standardized across fields. A possible solution to these problems may be to get leading scientists to rank the journals in their field, and to use these ranks as a measure of merit, rather than the IF. Finally, possibly the biggest problem with the IF is simply our reaction to it; we have a tendency to overrate papers published in high IF journals. So if are to use the IF, we need to reduce this tendency; one approach might be to rank all papers by their IF and assign scores by rank.</p>
<p>The REF will be performed in the United Kingdom next year in 2014. The assessment of publications forms the largest component of this exercise. This will be done by subjective post-publication review, with citation information being provided to some panels. However, as we have shown, both subjective review and the number of citations are very error prone measures of merit, so it seems likely that these assessments will also be extremely error prone, particularly given the volume of assessments that need to be made. For example, sub-panel 14 in the 2008 version of the RAE assessed ∼9,000 research outputs, each of which was assessed by two members of a 19 person panel; therefore each panel member assessed an average of just under 1,000 papers within a few months. We have also shown that assessors tend to overrate science in high IF journals, and although the REF <xref ref-type="bibr" rid="pbio.1001675-REF1">[20]</xref>, like the RAE before it <xref ref-type="bibr" rid="pbio.1001675-RAE2">[21]</xref>, contains a stipulation that the journal of publication should not be taken into account in making an assessment, it is unclear whether this is possible.</p>
<p>In our research we have not been able to address another potential problem for a process such as the REF. It seems very likely that assessors will differ in their mean score—some assessors will tend to give higher scores than other assessors. This could potentially affect the overall score for a department, particularly if the department is small and its outputs scored by relatively few assessors.</p>
<p>The REF actually represents an unrivalled opportunity to investigate the assessment of scientific research and to assess the quality of the data produced by such an exercise. We would therefore encourage the REF to have all components of every submission assessed by two independent assessors and then investigate how strongly these are correlated and whether some assessors score more generously than others. Only then can we determine how reliable the data are.</p>
<p>In summary, we have shown that none of the measures of scientific merit that we have investigated are reliable. In particular subjective peer review is error prone, biased, and expensive; we must therefore question whether using peer review in exercises such as the RAE and the REF is worth the huge amount of resources spent on them. Ultimately the only way to obtain (a largely) unbiased estimate of merit is to have pre-publication assessment, by several independent assessors, of manuscripts devoid of author's names and addresses. Nevertheless this will be a noisy estimate of merit unless we are prepared to engage many reviewers for each paper.</p>
</sec><sec id="s5" sec-type="materials|methods">
<title>Materials and Methods</title>
<p>We compiled subjective assessments from two sources. The largest of these datasets was from the F1000 database (<ext-link ext-link-type="uri" xlink:href="http://www.F1000.com" xlink:type="simple">www.F1000.com</ext-link>). In the F1000 database a panel of experts selects and recommends papers from biology and medicine to subscribers of the database. Papers in the F1000 database are rated “recommended” (numerical score 6), “must read” (8), or “exceptional” (10). We chose to take all papers that been published in a single year, 2005; this was judged to be sufficiently recent to reflect current trends and biases in publishing, but sufficiently long ago to allow substantial numbers of citations to have accumulated. We restricted our analysis to those papers that had been assessed within 12 months of publication to minimize the influence that subsequent discussion and citation might have on the assessment. This gave us a dataset of 5,811 papers, with 1,328 papers having been assessed by two or more assessors within 12 months. We chose to consider the 5-year IFs, since it was over a similar time-scale to the period over which we collected citations. However, in our dataset the 2-year and 5-year IFs are very highly correlated (r = 0.99). Citations were obtained from Google Scholar in 2011. We also analysed the WT data collected by Allen et al. <xref ref-type="bibr" rid="pbio.1001675-Allen1">[1]</xref>. This is a dataset of 716 biomedical papers, which were published in 2005, and assessed within 6 months by two assessors. Papers were given scores of 4, landmark; 3, major addition to knowledge; 2, useful step forward; and 1, for the record. The scores were sorted such that the higher score was usually allocated to the first assessor; this will affect the correlations by reducing the variance within the first (and second) assessor scores. As a consequence the scores were randomly re-allocated to the first and second assessor. Citations were collated from Google Scholar in 2011. As with the F1000 data we used 5 year IFs from 2010. Data have been deposited with Dryad <xref ref-type="bibr" rid="pbio.1001675-EyreWalker1">[8]</xref>.</p>
<p>Because most journals are poorly represented in each dataset we estimated the within and between journal variance in the number of citations as follows. We rounded the IF to the nearest integer then grouped journals according to the integer value. We then performed ANOVA on those groups for which we had ten or more publications.</p>
<p>Estimates of the error variance in assessment relative to variance in merit can be estimated as follows. Let us assume that the score (<italic>s</italic>) given by an assessor is linearly dependent upon the merit (<italic>m</italic>) and some error (<italic>e<sub>s</sub></italic>): <italic>s</italic> = <italic>m</italic>+<italic>e<sub>s</sub></italic>. Let the variance in merit be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e013" xlink:type="simple"/></inline-formula> and that for the error be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e014" xlink:type="simple"/></inline-formula>, so the variance in the score is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e015" xlink:type="simple"/></inline-formula>. If two assessors score the same paper the covariance between their scores will simply be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e016" xlink:type="simple"/></inline-formula> and the hence the correlation between scores is<disp-formula id="pbio.1001675.e017"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pbio.1001675.e017" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e018" xlink:type="simple"/></inline-formula>.</p>
<p>If we similarly assume that the number of citations a paper accumulates depends linearly on the merit and some error (with variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e019" xlink:type="simple"/></inline-formula>) then the covariance between an assessor's score and the number of citations is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e020" xlink:type="simple"/></inline-formula> and the correlation is<disp-formula id="pbio.1001675.e021"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pbio.1001675.e021" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pbio.1001675.e022" xlink:type="simple"/></inline-formula>. It is therefore straightforward to estimate <italic>r<sub>s</sub></italic> and <italic>r<sub>c</sub></italic>, and to obtain confidence intervals by bootstrapping the data.</p>
</sec><sec id="s6">
<title>Supporting Information</title>
<supplementary-material id="pbio.1001675.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pbio.1001675.s001" position="float" xlink:type="simple"><label>Table S1</label><caption>
<p><bold>The correlations, partial correlations, and standardized regression coefficients between assessor score (AS) and IF and the number of citations (CIT).</bold> ***<italic>p</italic>&lt;0.001.</p>
<p>(DOCX)</p>
</caption></supplementary-material><supplementary-material id="pbio.1001675.s002" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pbio.1001675.s002" position="float" xlink:type="simple"><label>Table S2</label><caption>
<p><bold>Spearman correlation coefficients between assessor scores and assessor scores and the number of citations and the IF.</bold> ***<italic>p</italic>&lt;0.001.</p>
<p>(DOCX)</p>
</caption></supplementary-material><supplementary-material id="pbio.1001675.s003" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pbio.1001675.s003" position="float" xlink:type="simple"><label>Table S3</label><caption>
<p><bold>The correlations, partial correlations, and standardized regression coefficients between assessor score (AS) and the log of IF and the log of the number of citations (CIT).</bold> ***<italic>p</italic>&lt;0.001.</p>
<p>(DOCX)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We are very grateful to the Faculty of 1000 and Wellcome Trust for giving us permission to use their data. We are also grateful to Liz Allen, John Brookfield, Juan Pablo Couso, Stephen Curry, and Kevin Dolby for helpful discussion.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pbio.1001675-Allen1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Allen</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Dolby</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Lynn</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Walport</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>Looking for landmarks: the role of expert review and bibliometric analysis in evaluating scientific publication outputs</article-title>. <source>PloS ONE</source> <volume>4</volume>: <fpage>e5910</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0005910" xlink:type="simple">10.1371/journal.pone.0005910</ext-link></comment></mixed-citation>
</ref>
<ref id="pbio.1001675-Wardle1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wardle</surname><given-names>DA</given-names></name> (<year>2010</year>) <article-title>Do ‘Faculty of 1000’ (F1000) ratings of ecological publications serve as reasonable predictors of their future impact?</article-title> <source>Ideas in Ecology and Evolution</source> <volume>3</volume>: <fpage>11</fpage>–<lpage>15</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001675-Mahdi1"><label>3</label>
<mixed-citation publication-type="other" xlink:type="simple">Mahdi S, D'Este P, Neely A (2008) Citation counts: are they good predictors of RAE scores? Available: <ext-link ext-link-type="uri" xlink:href="http://dspace.lib.cranfield.ac.uk/handle/1826/2248" xlink:type="simple">http://dspace.lib.cranfield.ac.uk/handle/1826/2248</ext-link></mixed-citation>
</ref>
<ref id="pbio.1001675-RAE1"><label>4</label>
<mixed-citation publication-type="other" xlink:type="simple">RAE manager (2009) RAE Manager's report. Available: <ext-link ext-link-type="uri" xlink:href="http://www.rae.ac.uk/pubs/2009/manager/manager.pdf" xlink:type="simple">http://www.rae.ac.uk/pubs/2009/manager/manager.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pbio.1001675-PA1"><label>5</label>
<mixed-citation publication-type="other" xlink:type="simple">PA Consulting Group (2008) RAE 2008 Accountability Review. Available <ext-link ext-link-type="uri" xlink:href="http://www.hefce.ac.uk/media/hefce/content/pubs/2009/rd0809/rd08_09.pdf" xlink:type="simple">http://www.hefce.ac.uk/media/hefce/content/pubs/2009/rd0809/rd08_09.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pbio.1001675-HM1"><label>6</label>
<mixed-citation publication-type="other" xlink:type="simple">HM Treasury, Department of Trade and IndustryDepartment for Education and SkillsDepartment of Health (2006) Science and innovation investment framework 2004–2014: next steps available at <ext-link ext-link-type="uri" xlink:href="http://webarchive.nationalarchives.gov.uk//http://www.hm-treasury.gov.uk/media/7/8/bud06_science_332v1.pdf" xlink:type="simple">http://webarchive.nationalarchives.gov.uk//http://www.hm-treasury.gov.uk/media/7/8/bud06_science_332v1.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pbio.1001675-Bollen1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bollen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Van de Sompel</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hagberg</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Chute</surname><given-names>R</given-names></name> (<year>2009</year>) <article-title>A principal component analysis of 39 scientific impact measures</article-title>. <source>PloS ONE</source> <volume>4</volume>: <fpage>e6022</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0006022" xlink:type="simple">10.1371/journal.pone.0006022</ext-link></comment></mixed-citation>
</ref>
<ref id="pbio.1001675-EyreWalker1"><label>8</label>
<mixed-citation publication-type="other" xlink:type="simple">Eyre-Walker A, Stoletzki N (2013) The assessment of science: the relative merits of post-publication review, the impact factor and the number of citations. Dryad data packages ‘Wellcome Trust data’ and ‘F1000 data’. Available: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5061/dryad.2h4j5" xlink:type="simple">http://dx.doi.org/10.5061/dryad.2h4j5</ext-link>.</mixed-citation>
</ref>
<ref id="pbio.1001675-Opthof1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Opthof</surname><given-names>T</given-names></name> (<year>1997</year>) <article-title>Sense and nonsense about the impact factor</article-title>. <source>Cardiovasc Res</source> <volume>33</volume>: <fpage>1</fpage>–<lpage>7</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001675-Knothe1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knothe</surname><given-names>G</given-names></name> (<year>2006</year>) <article-title>Comparative citation analysis of duplicate or highly related publications</article-title>. <source>J Am Soc Inf Sci Tec</source> <volume>57</volume>: <fpage>1830</fpage>–<lpage>1839</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001675-Lariviere1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lariviere</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Gingras</surname><given-names>Y</given-names></name> (<year>2010</year>) <article-title>The impact factor's matthew effect: a natural experiment in bibliometrics</article-title>. <source>J Am Soc Inf Sci Tec</source> <volume>61</volume>: <fpage>424</fpage>–<lpage>427</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001675-Seglen1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seglen</surname><given-names>PO</given-names></name> (<year>1994</year>) <article-title>Causal relationship between article citedness and journal impact</article-title>. <source>J Am Soc Information Sci</source> <volume>45</volume>: <fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001675-Wardle2"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wardle</surname><given-names>DA</given-names></name> (<year>1995</year>) <article-title>Journal citation impact factors and parochial citation practices</article-title>. <source>Bull Ecol Soc Am</source> <volume>76</volume>: <fpage>102</fpage>–<lpage>104</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001675-Paris1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paris</surname><given-names>G</given-names></name>, <name name-style="western"><surname>De Leo</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Menozzi</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Gatto</surname><given-names>M</given-names></name> (<year>1998</year>) <article-title>Region-based citation bias in science</article-title>. <source>Nature</source> <volume>396</volume>: <fpage>210</fpage>.</mixed-citation>
</ref>
<ref id="pbio.1001675-Leimu1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leimu</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Koricheva</surname><given-names>J</given-names></name> (<year>2005</year>) <article-title>What determines the citation frequency of ecological papers?</article-title> <source>Trends Ecol Evol</source> <volume>20</volume>: <fpage>28</fpage>–<lpage>32</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001675-Baldi1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baldi</surname><given-names>S</given-names></name> (<year>1998</year>) <article-title>Normative versus social constructivist processes in the allocation of citations: anetwork-analytic model</article-title>. <source>Am Sociol Rev</source> <volume>63</volume>: <fpage>829</fpage>–<lpage>846</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001675-Davenport1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Davenport</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Snyder</surname><given-names>H</given-names></name> (<year>1995</year>) <article-title>Who cites women? Whom do women cite? An exploration of gender and scholarly citation in sociology</article-title>. <source>J Doc</source> <volume>51</volume>: <fpage>404</fpage>–<lpage>410</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001675-Seglen2"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seglen</surname><given-names>PO</given-names></name> (<year>1997</year>) <article-title>Why the impact factor of journals should not be used for evaluating research</article-title>. <source>BMJ</source> <volume>314</volume>: <fpage>498</fpage>–<lpage>502</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001675-Editors1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><collab xlink:type="simple">Editors</collab> (<year>2006</year>) <article-title>The impact factor game. It is time to find a better way to assess the scientific literature</article-title>. <source>PLoS Med</source> <volume>3</volume>: <fpage>e291</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pmed.0030291" xlink:type="simple">10.1371/journal.pmed.0030291</ext-link></comment></mixed-citation>
</ref>
<ref id="pbio.1001675-REF1"><label>20</label>
<mixed-citation publication-type="other" xlink:type="simple">REF manager (2012) Panel criteria and working methods. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ref.ac.uk/media/ref/content/pub/panelcriteriaandworkingmethods/01_12.pdf" xlink:type="simple">http://www.ref.ac.uk/media/ref/content/pub/panelcriteriaandworkingmethods/01_12.pdf</ext-link>. Higher Education Funding Council of England.</mixed-citation>
</ref>
<ref id="pbio.1001675-RAE2"><label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">RAE manager (2006) RAE 2008 Panel criteria and working methods: Panel D. Available: <ext-link ext-link-type="uri" xlink:href="http://rae.ac.uk/pubs/2006/01/docs/d14.pdf" xlink:type="simple">http://rae.ac.uk/pubs/2006/01/docs/d14.pdf</ext-link>. Higher Education Funding Council of England.</mixed-citation>
</ref>
</ref-list><glossary><title>Abbreviations</title><def-list><def-item>
<term>F1000</term>
<def>
<p>Faculty of 1000</p>
</def>
</def-item><def-item>
<term>IF</term>
<def>
<p>impact factor</p>
</def>
</def-item><def-item>
<term>RAE</term>
<def>
<p>Research Assessment Exercise</p>
</def>
</def-item><def-item>
<term>REF</term>
<def>
<p>Research Excellence Framework</p>
</def>
</def-item><def-item>
<term>WT</term>
<def>
<p>Wellcome Trust</p>
</def>
</def-item></def-list></glossary></back>
</article>