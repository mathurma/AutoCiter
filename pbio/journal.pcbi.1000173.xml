<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="discussion" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">08-PLCB-EN-0460R2</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000173</article-id><article-categories><subj-group subj-group-type="heading"><subject>Education</subject></subj-group>
<subj-group subj-group-type="Discipline">
<subject>Genetics and Genomics/Genomics</subject><subject>Computer Science/Information Technology</subject><subject>Computational Biology/Genomics</subject><subject>Computational Biology</subject>
</subj-group>
</article-categories><title-group><article-title>Support Vector Machines and Kernels for Computational Biology</article-title></title-group><contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Ben-Hur</surname><given-names>Asa</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Ong</surname><given-names>Cheng Soon</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="fn" rid="fn1"><sup>¤</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Sonnenburg</surname><given-names>Sören</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Schölkopf</surname><given-names>Bernhard</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Rätsch</surname><given-names>Gunnar</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Department of Computer Science, Colorado State University, Fort Collins, Colorado, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Friedrich Miescher Laboratory, Max Planck Society, Tübingen, Germany</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Max Planck Institute for Biological Cybernetics, Tübingen, Germany</addr-line>       </aff><aff id="aff4"><label>4</label><addr-line>Fraunhofer Institute FIRST, Berlin, Germany</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Lewitter</surname><given-names>Fran</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">Whitehead Institute, United States of America</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">Gunnar.Raetsch@tuebingen.mpg.de</email></corresp>
<fn fn-type="current-aff" id="fn1"><label>¤</label><p>Current address: Department of Computer Science, ETH, Zürich, Switzerland.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>10</month><year>2008</year></pub-date><pub-date pub-type="epub"><day>31</day><month>10</month><year>2008</year></pub-date><volume>4</volume><issue>10</issue><elocation-id>e1000173</elocation-id><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2008</copyright-year><copyright-holder>Ben-Hur et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><funding-group><funding-statement>AB is partially supported by National Science Foundation grant DBI-0754247.</funding-statement></funding-group><counts><page-count count="10"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>The increasing wealth of biological data coming from a large variety of platforms and the continued development of new high-throughput methods for probing biological systems require increasingly more sophisticated computational approaches. Putting all these data in simple-to-use databases is a first step; but realizing the full potential of the data requires algorithms that automatically extract regularities from the data, which can then lead to biological insight.</p>
<p>Many of the problems in computational biology are in the form of prediction: starting from prediction of a gene's structure, prediction of its function, interactions, and role in disease. Support vector machines (SVMs) and related kernel methods are extremely good at solving such problems <xref ref-type="bibr" rid="pcbi.1000173-Boser1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1000173-Vapnik1">[3]</xref>. SVMs are widely used in computational biology due to their high accuracy, their ability to deal with high-dimensional and large datasets, and their flexibility in modeling diverse sources of data <xref ref-type="bibr" rid="pcbi.1000173-Schlkopf1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1000173-Mller1">[4]</xref>–<xref ref-type="bibr" rid="pcbi.1000173-Vert1">[6]</xref>.</p>
<p>The simplest form of a prediction problem is binary classification: trying to discriminate between objects that belong to one of two categories—positive (+1) or negative (−1). SVMs use two key concepts to solve this problem: large margin separation and kernel functions. The idea of large margin separation can be motivated by classification of points in two dimensions (see <xref ref-type="fig" rid="pcbi-1000173-g001">Figure 1</xref>). A simple way to classify the points is to draw a straight line and call points lying on one side positive and on the other side negative. If the two sets are well separated, one would intuitively draw the separating line such that it is as far as possible away from the points in both sets (see <xref ref-type="fig" rid="pcbi-1000173-g002">Figures 2</xref> and <xref ref-type="fig" rid="pcbi-1000173-g003">3</xref>). This intuitive choice captures the idea of <italic>large margin separation</italic>, which is mathematically formulated in the section Classification with Large Margin.</p>
<fig id="pcbi-1000173-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000173.g001</object-id><label>Figure 1</label><caption>
<title>A linear classifier separating two classes of points (squares and circles) in two dimensions.</title>
<p>The decision boundary divides the space into two sets depending on the sign of <italic>f</italic>(<bold>x</bold>) = 〈<bold>w,x</bold>〉+<italic>b</italic>. The grayscale level represents the value of the discriminant function <italic>f</italic>(<bold>x</bold>): dark for low values and a light shade for high values.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.g001" xlink:type="simple"/></fig><fig id="pcbi-1000173-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000173.g002</object-id><label>Figure 2</label><caption>
<title>The maximum margin boundary computed by a linear SVM.</title>
<p>The region between the two thin lines defines the <italic>margin area</italic> with −1≤〈<bold>w,x</bold>〉+<italic>b</italic>≤1. The data points highlighted with black centers are the <italic>support vectors</italic>: the examples that are closest to the decision boundary. They determine the margin by which the two classes are separated. Here, there are three support vectors on the edge of the margin area (<italic>f</italic>(<bold>x</bold>) = −1 or <italic>f</italic>(<bold>x</bold>) = +1).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.g002" xlink:type="simple"/></fig><fig id="pcbi-1000173-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000173.g003</object-id><label>Figure 3</label><caption>
<title>The effect of the soft-margin constant, <italic>C</italic>, on the decision boundary.</title>
<p>We modified the toy dataset by moving the point shaded in gray to a new position indicated by an arrow, which significantly reduces the margin with which a hard-margin SVM can separate the data. (A) We show the margin and decision boundary for an SVM with a very high value of <italic>C</italic>, which mimics the behavior of the hard-margin SVM since it implies that the slack variables <italic>ξ<sub>i</sub></italic> (and hence training mistakes) have very high cost. (B) A smaller value of <italic>C</italic> allows us to ignore points close to the boundary, and increases the margin. The decision boundary between negative examples and positive examples is shown as a thick line. The thin lines are on the margin (discriminant value equal to −1 or +1).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.g003" xlink:type="simple"/></fig>
<p>Instead of the abstract idea of points in space, one can think of our data points as representing objects using a set of <italic>features</italic> derived from measurements performed on each object. For instance, in the case of <xref ref-type="fig" rid="pcbi-1000173-g001">Figures 1</xref>–<xref ref-type="fig" rid="pcbi-1000173-g002"/><xref ref-type="fig" rid="pcbi-1000173-g003"/><xref ref-type="fig" rid="pcbi-1000173-g004"/><xref ref-type="fig" rid="pcbi-1000173-g005">5</xref>, there are two measurements for each object, depicted as points in a two-dimensional space. For large margin separation, it turns out that not the exact location but only the relative position or <italic>similarity</italic> of the points to each other is important. In the simplest case of linear classification, the similarity of two objects is computed by the dot-product (a.k.a. scalar or inner product) between the corresponding feature vectors. To define different similarity measures leading to <italic>nonlinear classification</italic> boundaries (cf. <xref ref-type="fig" rid="pcbi-1000173-g006">Figures 6</xref> and <xref ref-type="fig" rid="pcbi-1000173-g007">7</xref>), one can extend the idea of dot products between points with the help of <italic>kernel functions</italic> (cf. the section <xref ref-type="sec" rid="s3">Kernels: From Linear to Nonlinear Classifiers</xref>). Kernels compute the similarity of two points and are the second important concept of SVMs and <italic>kernel methods</italic> <xref ref-type="bibr" rid="pcbi.1000173-Schlkopf1">[2]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-ShaweTaylor1">[7]</xref>.</p>
<fig id="pcbi-1000173-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000173.g004</object-id><label>Figure 4</label><caption>
<title>The major steps in protein synthesis: transcription, post-processing, and translation.</title>
<p>In the post-processing step, the pre-mRNA is transformed into mRNA. One necessary step in the process of obtaining mature mRNA is called <italic>splicing</italic>. The mRNA sequence of a eukaryotic gene is “interrupted” by noncoding regions called <italic>introns</italic>. A gene starts with an exon and may then be interrupted by an intron, followed by another exon, intron, and so on until it ends in an exon. In the splicing process, the introns are removed. There are two different splice sites: the exon–intron boundary, referred to as the donor site or 5′ site (of the intron), and the intron–exon boundary, that is, the acceptor or 3′ site. Splice sites have quite strong consensus sequences, i.e., almost every position in a small window around the splice site is representative of the most frequently occurring nucleotide when many existing sequences are compared in an alignment (cf. <xref ref-type="fig" rid="pcbi-1000173-g005">Figure 5</xref>). (The caption text appeared similarly in <xref ref-type="bibr" rid="pcbi.1000173-Rtsch1">[30]</xref>, the idea for this figure is from <xref ref-type="bibr" rid="pcbi.1000173-Lewin1">[11]</xref>.)</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.g004" xlink:type="simple"/></fig><fig id="pcbi-1000173-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000173.g005</object-id><label>Figure 5</label><caption>
<title>Sequence logo for acceptor splice sites: splice sites have quite strong consensus sequences, i.e., almost every position in a small window around the splice site is representative of the most frequently occurring nucleotide when many existing sequences are compared in an alignment.</title>
<p>The sequence logo <xref ref-type="bibr" rid="pcbi.1000173-Schneider1">[72]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-Crooks1">[73]</xref> shows the region around the intron/exon boundary—the acceptor splice site. In the running example, we use the region up to 40 nt upstream and downstream of the consensus site AG.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.g005" xlink:type="simple"/></fig><fig id="pcbi-1000173-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000173.g006</object-id><label>Figure 6</label><caption>
<title>The effect of the degree of a polynomial kernel.</title>
<p>The polynomial kernel of degree 1 leads to a linear separation (A). Higher-degree polynomial kernels allow a more flexible decision boundary (B,C). The style follows that of <xref ref-type="fig" rid="pcbi-1000173-g003">Figure 3</xref>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.g006" xlink:type="simple"/></fig><fig id="pcbi-1000173-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000173.g007</object-id><label>Figure 7</label><caption>
<title>The effect of the width parameter of the Gaussian kernel (<italic>σ</italic>) for a fixed value of the soft-margin constant.</title>
<p>For large values of <italic>σ</italic> (A), the decision boundary is nearly linear. As <italic>σ</italic> decreases, the flexibility of the decision boundary increases (B). Small values of <italic>σ</italic> lead to overfitting (C). The figure style follows that of <xref ref-type="fig" rid="pcbi-1000173-g003">Figure 3</xref>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.g007" xlink:type="simple"/></fig>
<p>The domain knowledge inherent in any classification task is captured by defining a suitable kernel (i.e., similarity) between objects. As we shall see later, this has two advantages: the ability to generate nonlinear decision boundaries using methods designed for linear classifiers; and the possibility of applying a classifier to data that have no obvious vector space representation; for example, DNA/RNA, or protein sequences, or protein structures.</p>
<sec id="s1a">
<title/>
<sec id="s1a1">
<title/>
<sec id="s1a1a">
<title>Running example: Splice site recognition</title>
<p>Throughout this tutorial we are going to use an example problem for illustration. It is a problem arising in computational gene finding and concerns the recognition of splice sites that mark the boundaries between exons and introns in eukaryotes. Introns are excised from premature mRNAs in a processing step after transcription (see <xref ref-type="fig" rid="pcbi-1000173-g004">Figure 4</xref> and, for instance, <xref ref-type="bibr" rid="pcbi.1000173-Black1">[8]</xref>–<xref ref-type="bibr" rid="pcbi.1000173-Holste1">[12]</xref> for more details).</p>
<p>The vast majority of all splice sites are characterized by the presence of specific dimers on the intronic side of the splice site: GT for donor and AG for acceptor sites (see <xref ref-type="fig" rid="pcbi-1000173-g005">Figure 5</xref>). However, only about 0.1%–1% of all GT and AG occurrences in the genome represent true splice sites. In this tutorial, we consider the problem of recognizing acceptor splice sites as a running example, which allows us to illustrate different properties of SVMs using different kernels (similar results can be obtained for donor splice sites as well <xref ref-type="bibr" rid="pcbi.1000173-Sonnenburg1">[13]</xref>).</p>
<p>In the first part of the tutorial we are going to use real-valued features describing the sequence surrounding the splice site. For illustration purposes, we use only two features: the GC content in the exon and intron flanking potential <italic>acceptor</italic> sites. These features are motivated by the fact that the GC-content of exons is typically higher than that of introns (see, e.g., <xref ref-type="fig" rid="pcbi-1000173-g001">Figure 1</xref>). In the second part, we show how to take advantage of the flanking pre-mRNA sequence itself, leading to considerable performance improvements. (The data used in the numerical examples was generated by taking a random subset of 200 true splice sites and 2,000 decoy sites from the first 100,000 entries in the <italic>C. elegans</italic> acceptor splice site dataset from <xref ref-type="bibr" rid="pcbi.1000173-Sonnenburg1">[13]</xref> (cf. <ext-link ext-link-type="uri" xlink:href="http://www.fml.tuebingen.mpg.de/raetsch/projects/splice" xlink:type="simple">http://www.fml.tuebingen.mpg.de/raetsch/projects/splice</ext-link>). Note that this dataset is much smaller than the original dataset, and is also less unbalanced. In the graphical examples in this tutorial, we show only a small and selected subset of the data suitable for illustration purposes. In practice, there is a considerably stronger overlap in the space of GC content between positive examples (true acceptor sites) and negative examples (other occurrences of AG) than appears on the figures.</p>
<p>To evaluate the classifier performance, we will use so-called <italic>receiver operating characteristic</italic> (ROC) curves <xref ref-type="bibr" rid="pcbi.1000173-Metz1">[14]</xref>, which show the true positive rates (<italic>y</italic>-axis) over the full range of false positive rates (<italic>x</italic>-axis). Different values are obtained by using different thresholds on the value of the discriminant function for assigning the class membership. The area under the curve quantifies the quality of the classifier, and a larger value indicates better performance. Research has shown that it is a better measure of classifier performance than the success or error rate of the classifier <xref ref-type="bibr" rid="pcbi.1000173-Provost1">[15]</xref>, in particular when the fraction of examples in one class is much smaller than the other. (Please note that the auROC is independent of the class ratios. Hence, its value is not necessarily connected with the success of identifying rare events. The area under the so-called precision recall curve is better suited to evaluate how well one can find rare events <xref ref-type="bibr" rid="pcbi.1000173-Davis1">[16]</xref>.)</p>
</sec><sec id="s1a1b">
<title>SVM toolbox</title>
<p>All computational results in this tutorial were generated using the <italic>Shogun</italic>-based <italic>Easysvm</italic> tool <xref ref-type="bibr" rid="pcbi.1000173-1">[17]</xref> written in python <xref ref-type="bibr" rid="pcbi.1000173-Python1">[18]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-Bassi1">[19]</xref>. The source code to generate the figures and results is provided under the GNU General Public License <xref ref-type="bibr" rid="pcbi.1000173-2">[20]</xref> at <ext-link ext-link-type="uri" xlink:href="http://svmcompbio.tuebingen.mpg.de" xlink:type="simple">http://svmcompbio.tuebingen.mpg.de</ext-link>. That site also provides a Web service that allows one to train and evaluate SVMs. An alternative implementation using PyML is also available <xref ref-type="bibr" rid="pcbi.1000173-5">[70]</xref>.</p>
</sec></sec></sec></sec><sec id="s2">
<title>Large Margin Separation</title>
<sec id="s2a">
<title/>
<sec id="s2a1">
<title>Linear separation with hyperplanes</title>
<p>In this section, we introduce the idea of linear classifiers. Support vector machines are an example of a linear two-class classifier. The data for a two-class learning problem consists of objects labeled with one of two labels; for convenience we assume the labels are +1 (positive examples) and −1 (negative examples). Let <bold>x</bold> denote a vector with <italic>M</italic> components <italic>x<sub>j</sub></italic>, <italic>j</italic> = 1,…,<italic>M</italic>, i.e., a point in an <italic>M</italic>-dimensional vector space. The notation <bold>x</bold><italic><sub>i</sub></italic> will denote the <italic>i</italic><sup>th</sup> vector in a dataset <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e001" xlink:type="simple"/></inline-formula>, where <italic>y<sub>i</sub></italic> is the label associated with <bold>x</bold><italic><sub>i</sub></italic>, and n is the number of examples. The objects <bold>x</bold><italic><sub>i</sub></italic> are called <italic>patterns</italic>, <italic>inputs</italic>, and also <italic>examples</italic>.</p>
<p>A key concept required for defining a linear classifier is the <italic>dot product</italic> between two vectors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e002" xlink:type="simple"/></inline-formula>, also referred to as the <italic>inner product</italic> or <italic>scalar product</italic>. A linear classifier is based on a linear <italic>discriminant function</italic> of the form<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e003" xlink:type="simple"/><label>(1)</label></disp-formula></p>
<p>The discriminant function <italic>f</italic>(<bold>x</bold>) assigns a “score” for the input <bold>x</bold>, and is used to decide how to classify it. The vector <bold>w</bold> is known as the <italic>weight vector</italic>, and the scalar <italic>b</italic> is called the <italic>bias</italic>. In two dimensions, the points satisfying the equation 〈<bold>w</bold>,<bold>x</bold>〉 = 0 correspond to a line through the origin, in three dimensions a plane, and more generally a <italic>hyperplane</italic>. The bias translates the hyperplane with respect to the origin (see <xref ref-type="fig" rid="pcbi-1000173-g001">Figure 1</xref>). (Unlike many schematic representations that the reader may have seen, the figures in this paper are generated by actually applying the SVM on the data points as shown. More details, including code and data, are available at <ext-link ext-link-type="uri" xlink:href="http://svmcompbio.tuebingen.mpg.de" xlink:type="simple">http://svmcompbio.tuebingen.mpg.de</ext-link>.)</p>
<p>The hyperplane divides the space into two half spaces according to the sign of <italic>f</italic>(<bold>x</bold>), that indicates on which side of the hyperplane a point is located (see <xref ref-type="fig" rid="pcbi-1000173-g001">Figure 1</xref>): if <italic>f</italic>(<bold>x</bold>)&gt;0, then one decides for the positive class, otherwise for the negative. The boundary between regions classified as positive and negative is called the <italic>decision boundary</italic> of the classifier. The decision boundary defined by a hyperplane (cf. Equation 1) is said to be <italic>linear</italic> because it is linear in the input. (Note that strictly speaking, for <italic>b</italic>≠0, this is <italic>affine</italic> rather than linear, but we will ignore this distinction.) A classifier with a linear decision boundary is called a linear classifier. In the next section, we introduce one particular linear classifier, the (linear) Support Vector Machine, which turns out to be particularly well suited to high-dimensional data.</p>
</sec><sec id="s2a2">
<title>Classification with large margin</title>
<p>Whenever a dataset such as is shown in <xref ref-type="fig" rid="pcbi-1000173-g001">Figure 1</xref> is linearly separable, i.e., there exists a hyperplane that correctly classifies all data points, there exist many such separating hyperplanes. We are thus faced with the question of which hyperplane to choose, ensuring that not only the training data, but also <italic>future examples</italic>, unseen by the classifier at training time, are classified correctly. Our intuition as well as statistical learning theory <xref ref-type="bibr" rid="pcbi.1000173-Vapnik1">[3]</xref> suggest that hyperplane classifiers will work better if the hyperplane not only separates the examples correctly, but does so with a large margin. Here, the margin of a linear classifier is defined as the distance of the closest example to the decision boundary, as shown in <xref ref-type="fig" rid="pcbi-1000173-g002">Figure 2</xref>. Let us adjust <italic>b</italic> such that the hyperplane is half way in between the closest positive and negative examples. If, moreover, we scale the discriminant function, Equation 1, to take the values ±1 for these examples, we find that the margin is 1/∥<bold>w</bold>∥, where ∥<bold>w</bold>∥ is the length of <bold>w</bold>, also known as its <italic>norm</italic>, given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e004" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1000173-Schlkopf1">[2]</xref>.</p>
<p>The so-called <italic>hard margin SVM</italic>, applicable to linearly separable data, is the classifier with maximum margin among all classifiers that correctly classify all the input examples (see <xref ref-type="fig" rid="pcbi-1000173-g002">Figure 2</xref>). To compute <bold>w</bold> and <italic>b</italic> corresponding to the maximum margin hyperplane, one has to solve the following optimization problem:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e005" xlink:type="simple"/><label>(2)</label></disp-formula>where the constraints ensure that each example is correctly classified, and minimizing ∥<bold>w</bold>∥<sup>2</sup> is equivalent to maximizing the margin. (The set of formulas above describes a <italic>quadratic optimization problem</italic>, in which the optimal solution (<bold>w</bold>,<italic>b</italic>) is described to satisfy the constraints <italic>y<sub>i</sub></italic>(〈<bold>w</bold>,<bold>x</bold><italic><sub>i</sub></italic>〉+<italic>b</italic>)≥1, while the length of <bold>w</bold> is as small as possible. Such optimization problems can be solved using standard tools from convex optimization (see, e.g., <xref ref-type="bibr" rid="pcbi.1000173-Boyd1">[21]</xref>). For specific optimization problems like the one above, there exist specialized techniques to efficiently solve such optimization problems for millions of examples or dimensions.)</p>
<sec id="s2a2a">
<title>Soft margin</title>
<p>In practice, data are often not linearly separable; and even if they are, a greater margin can be achieved by allowing the classifier to misclassify some points—see <xref ref-type="fig" rid="pcbi-1000173-g003">Figure 3</xref>. Theory and experimental results show that the resulting larger margin will generally provide better performance than the hard margin SVM. To allow errors, we replace the inequality constraints in Equation 2 with<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e006" xlink:type="simple"/></disp-formula>where <italic>ξ<sub>i</sub></italic>≥0 are <italic>slack variables</italic> that allow an example to be in the margin or misclassified. To discourage excess use of the slack variables, a term <italic>C</italic>Σ<sub>i</sub><italic>ξ<sub>i</sub></italic> is added to the function to be optimized:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e007" xlink:type="simple"/><label>(3)</label></disp-formula></p>
<p>The constant <italic>C</italic>&gt;0 sets the relative importance of maximizing the margin and minimizing the amount of slack. This formulation is called the <italic>soft-margin SVM</italic> <xref ref-type="bibr" rid="pcbi.1000173-Cortes1">[22]</xref>.</p>
<p>The effect of the choice of <italic>C</italic> is illustrated in <xref ref-type="fig" rid="pcbi-1000173-g003">Figure 3</xref>. For a large value of <italic>C</italic>, a large penalty is assigned to errors. This is seen in <xref ref-type="fig" rid="pcbi-1000173-g003">Figure 3A</xref>, where the two points closest to the hyperplane strongly affect its orientation, leading to a hyperplane that comes close to several other data points. When <italic>C</italic> is decreased (<xref ref-type="fig" rid="pcbi-1000173-g003">Figure 3B</xref>), those points move inside the margin, and the hyperplane's orientation is changed, leading to a much larger margin for the rest of the data. Note that the scale of <italic>C</italic> has no direct meaning, and there is a formulation of SVMs that uses a more intuitive parameter 0&lt;<italic>ν</italic>≤1 instead. The parameter <italic>ν</italic> controls the fraction of support vectors, and of margin errors (<italic>ν</italic>-SVM, see <xref ref-type="bibr" rid="pcbi.1000173-Schlkopf1">[2]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-ShaweTaylor1">[7]</xref>).</p>
</sec><sec id="s2a2b">
<title>Dual formulation</title>
<p>Using the method of Lagrange multipliers (see, e.g., <xref ref-type="bibr" rid="pcbi.1000173-Boyd1">[21]</xref>), we can obtain the <italic>dual</italic> formulation. (The dual optimization problem is a reformulation of the original, primal optimization problem. It typically has as many variables as the primal problem has constraints. Its objective value at optimality is equal to the optimal objective value of the primal problem, under certain conditions; see, e.g., <xref ref-type="bibr" rid="pcbi.1000173-Boyd1">[21]</xref> for more details.) It is expressed in terms of variables <italic>α<sub>i</sub></italic> <xref ref-type="bibr" rid="pcbi.1000173-Schlkopf1">[2]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-Cortes1">[22]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e008" xlink:type="simple"/><label>(4)</label></disp-formula></p>
<p>One can prove that the weight vector <bold>w</bold> in Equation 3 can be expressed in terms of the examples <bold>x</bold><italic><sub>i</sub></italic> and the solution <italic>α<sub>i</sub></italic> of the above optimization problem as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e009" xlink:type="simple"/><label>(5)</label></disp-formula></p>
<p>The <bold>x</bold><italic><sub>i</sub></italic> for which <italic>α<sub>i</sub></italic>&gt;0 are called <italic>support vectors</italic>; they can be shown to lie on or within the margin (points with black circles in <xref ref-type="fig" rid="pcbi-1000173-g002">Figures 2</xref>–<xref ref-type="fig" rid="pcbi-1000173-g003"/><xref ref-type="fig" rid="pcbi-1000173-g004"/><xref ref-type="fig" rid="pcbi-1000173-g005"/><xref ref-type="fig" rid="pcbi-1000173-g006"/><xref ref-type="fig" rid="pcbi-1000173-g007">7</xref>). Intuitively, all other training examples do not contribute to the geometric location of the large margin hyperplane—the solution would have been the same even if they had not been in the training set to begin with. It is thus not surprising that they drop out of the expansion in Equation 5.</p>
<p>Note that the dual formulation of the SVM optimization problem depends on the inputs <bold>x</bold><italic><sub>i</sub></italic> only through dot products. In the next section, we will show that the same holds true for the discriminant function given by Equation 1. This will allow us to “kernelize” the algorithm.</p>
</sec></sec></sec></sec><sec id="s3">
<title>Kernels: From Linear to Nonlinear Classifiers</title>
<p>In many applications, a nonlinear classifier provides better accuracy. And yet linear classifiers have advantages, one of them being that they often have simple training algorithms that scale well with the number of examples <xref ref-type="bibr" rid="pcbi.1000173-Hastie1">[23]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-Bishop1">[24]</xref>. This begs the question whether the machinery of linear classifiers can be extended to generate nonlinear decision boundaries. Furthermore, can we handle domains such as biological sequences where a vector space representation is not necessarily available?</p>
<p>There is a straightforward way of turning a linear classifier nonlinear, or making it applicable to nonvectorial data. It consists of mapping our data to some vector space, which we will refer to as the feature space, using a function <italic>φ</italic>. The discriminant function then is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e010" xlink:type="simple"/><label>(6)</label></disp-formula></p>
<p>Note that <italic>f</italic>(<bold>x</bold>) is linear in the feature space defined by the mapping <italic>φ</italic>; but when viewed in the original input space, it is a nonlinear function of <bold>x</bold> if <italic>φ</italic>(<bold>x</bold>) is a nonlinear function. The simplest example of such a mapping is one that considers all products of pairs of features (related to the polynomial kernel; see below). The resulting classifier has a quadratic discriminant function (see example in <xref ref-type="fig" rid="pcbi-1000173-g006">Figure 6B</xref>). This approach of explicitly computing nonlinear features does not scale well with the number of input features. The dimensionality of the feature space associated with the above example is quadratic in the number of dimensions of the input space. If we were to use monomials of degree <italic>d</italic> rather than degree 2 monomials as above, the dimensionality would be exponential in <italic>d</italic>, resulting in a substantial increase in memory usage and the time required to compute the discriminant function. If our data are high-dimensional to begin with, such as in the case of gene expression data, this is not acceptable. Kernel methods avoid this complexity by avoiding the step of explicitly mapping the data to a high-dimensional feature space.</p>
<p>We have seen above (Equation 5) that the weight vector of a large margin separating hyperplane can be expressed as a linear combination of the training points, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e011" xlink:type="simple"/></inline-formula>. The same holds true for a large class of linear algorithms, as shown by the <italic>representer theorem</italic> (see <xref ref-type="bibr" rid="pcbi.1000173-Schlkopf1">[2]</xref>). Our discriminant function then becomes<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e012" xlink:type="simple"/><label>(7)</label></disp-formula></p>
<p>The representation in terms of the variables <italic>α<sub>i</sub></italic> is known as the <italic>dual</italic> representation (cf. the section Classification with Large Margin). We observe that the dual representation of the discriminant function depends on the data only through dot products in feature space. The same observation holds for the dual optimization problem (Equation 4) when we replace <bold>x</bold><italic><sub>i</sub></italic> with <italic>φ</italic>(<bold>x</bold><italic><sub>i</sub></italic>) (analogously for <bold>x</bold><italic><sub>j</sub></italic>).</p>
<p>If the <italic>kernel function k</italic>(<bold>x</bold>,<bold>x</bold>′) defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e013" xlink:type="simple"/><label>(8)</label></disp-formula>can be computed efficiently, then the dual formulation becomes useful, as it allows us to solve the problem without ever carrying out the mapping <italic>φ</italic> into a potentially very high-dimensional space. The recurring theme in what follows is to define meaningful similarity measures (kernel functions) that can be computed efficiently.</p>
<sec id="s3a">
<title/>
<sec id="s3a1">
<title>Kernels for real-valued data</title>
<p>Real-valued data, i.e., data where the examples are vectors of a given dimensionality, are common in bioinformatics and other areas. A few examples of applying SVM to real-valued data include prediction of disease state from microarray data (see, e.g., <xref ref-type="bibr" rid="pcbi.1000173-Guyon1">[25]</xref>), and prediction of protein function from a set of features that include amino acid composition and various properties of the amino acids in the protein (see, e.g., <xref ref-type="bibr" rid="pcbi.1000173-Cai1">[26]</xref>).</p>
<p>The two most commonly used kernel functions for real-valued data are the polynomial and the Gaussian kernel. The <italic>polynomial kernel</italic> of degree <italic>d</italic> is defined as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e014" xlink:type="simple"/><label>(9)</label></disp-formula>where <italic>κ</italic> is often chosen to be 0 (homogeneous) or 1 (inhomogeneous). The feature space for the inhomogeneous kernel consists of all monomials with degree up to <italic>d</italic> <xref ref-type="bibr" rid="pcbi.1000173-Schlkopf1">[2]</xref>. And yet, its computation time is linear in the dimensionality of the input space. The kernel with <italic>d</italic> = 1 and <italic>κ</italic> = 0, denoted by <italic>k</italic><sup>linear</sup>, is the <italic>linear kernel</italic> leading to a linear discriminant function.</p>
<p>The degree of the polynomial kernel controls the flexibility of the resulting classifier (<xref ref-type="fig" rid="pcbi-1000173-g006">Figure 6</xref>). The lowest degree polynomial is the linear kernel, which is not sufficient when a nonlinear relationship between features exists. For the data in <xref ref-type="fig" rid="pcbi-1000173-g006">Figure 6</xref>, a degree 2 polynomial is already flexible enough to discriminate between the two classes with a good margin. The degree 5 polynomial yields a similar decision boundary, with greater curvature. Normalization (cf. the section Normalization) can help to improve performance and numerical stability for large <italic>d</italic>.</p>
<p>The second very widely used kernel is the <italic>Gaussian kernel</italic> defined by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e015" xlink:type="simple"/><label>(10)</label></disp-formula>where <italic>σ</italic>&gt;0 is a parameter that controls the width of the Gaussian. It plays a similar role as the degree of the polynomial kernel in controlling the flexibility of the resulting classifier (see <xref ref-type="fig" rid="pcbi-1000173-g006">Figures 6</xref> and <xref ref-type="fig" rid="pcbi-1000173-g007">7</xref>). The Gaussian kernel is essentially zero if the squared distance ∥<bold>x</bold>−<bold>x′</bold>∥<sup>2</sup> is much larger than <italic>σ</italic>; i.e., for a fixed <bold>x′</bold> there is a region around <bold>x′</bold> with high kernel values. The discriminant function (Equation 7) is thus a sum of Gaussian “bumps” centered around each support vector (SV). When <italic>σ</italic> is large (<xref ref-type="fig" rid="pcbi-1000173-g007">Figure 7A</xref>), a given data point <bold>x</bold> has a nonzero kernel value relative to any example in the set of examples. Therefore, the whole set of SVs affects the value of the discriminant function at <bold>x</bold>, leading to a smooth decision boundary. As we decrease <italic>σ</italic>, the kernel becomes more local, leading to greater curvature of the decision surface. When <italic>σ</italic> is small, the value of the discriminant function is nonzero only in the close vicinity of each SV, leading to a discriminant that is essentially constant outside the close proximity of the region where the data are concentrated (<xref ref-type="fig" rid="pcbi-1000173-g007">Figure 7C</xref>).</p>
<p>As seen from the examples in <xref ref-type="fig" rid="pcbi-1000173-g006">Figures 6</xref> and <xref ref-type="fig" rid="pcbi-1000173-g007">7</xref>, the width parameter of the Gaussian kernel and the degree of polynomial kernel determine the flexibility of the resulting SVM in fitting the data. Large degree or small width values can lead to overfitting and suboptimal performance (<xref ref-type="fig" rid="pcbi-1000173-g007">Figure 7C</xref>).</p>
<p>Results on a much larger sample of the two dimensional splice site recognition dataset are shown in <xref ref-type="table" rid="pcbi-1000173-t001">Table 1</xref>. We observe that the use of a nonlinear kernel, either Gaussian or polynomial, leads to a small improvement in classifier performance when compared to the linear kernel. For the large degree polynomial and small width Gaussian kernel, we obtained reduced accuracy, which is the result of a kernel that is too flexible, as described above.</p>
<table-wrap id="pcbi-1000173-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000173.t001</object-id><label>Table 1</label><caption>
<title>SVM accuracy on the task of acceptor site recognition using polynomial and Gaussian kernels with different degrees <italic>d</italic> and widths <italic>σ</italic>.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000173-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Kernel</td>
<td align="left" colspan="1" rowspan="1">auROC</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Linear</td>
<td align="left" colspan="1" rowspan="1">88.2%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Polynomial <italic>d</italic> = 3</td>
<td align="left" colspan="1" rowspan="1">91.4%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Polynomial <italic>d</italic> = 7</td>
<td align="left" colspan="1" rowspan="1">90.4%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Gaussian <italic>σ</italic> = 100</td>
<td align="left" colspan="1" rowspan="1">87.9%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Gaussian <italic>σ</italic> = 1</td>
<td align="left" colspan="1" rowspan="1">88.6%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Gaussian <italic>σ</italic> = 0.01</td>
<td align="left" colspan="1" rowspan="1">77.3%</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt101"><p>Accuracy is measured using the area under the ROC curve (auROC) and is computed using 5-fold cross-validation (cf. the section Running Example: Splice Site Recognition for details).</p></fn></table-wrap-foot></table-wrap></sec><sec id="s3a2">
<title>Kernels for sequences</title>
<p>So far we have shown how SVMs perform on our splice site example if we use kernels based only on the two GC content features derived from the exonic and intronic parts of the sequence. The small subset of the dataset shown in <xref ref-type="fig" rid="pcbi-1000173-g001">Figures 1</xref>–<xref ref-type="fig" rid="pcbi-1000173-g002"/><xref ref-type="fig" rid="pcbi-1000173-g003"/><xref ref-type="fig" rid="pcbi-1000173-g004"/><xref ref-type="fig" rid="pcbi-1000173-g005"/><xref ref-type="fig" rid="pcbi-1000173-g006"/><xref ref-type="fig" rid="pcbi-1000173-g007">7</xref> seems to suggest that these features are sufficient to distinguish between the true splice sites and the decoys. This is not the case for a larger dataset, where examples from the two classes highly overlap. Therefore, to be able to separate true splice sites from decoys, one needs additional features derived from the same sequences. For instance, one may use the count of all four letters on the intronic and exonic part of the sequence (leading to eight features), or even all dimers (32 features), trimers (128 features), or longer ℓ-mers (2 · 4<sup>ℓ</sup> features).</p>
<sec id="s3a2a">
<title>Kernels describing ℓ-mer content</title>
<p>The above idea is realized in the so-called <italic>spectrum kernel</italic> that was first proposed for classifying protein sequences <xref ref-type="bibr" rid="pcbi.1000173-Leslie1">[27]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-Leslie2">[28]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e016" xlink:type="simple"/><label>(11)</label></disp-formula>where <bold>x̅</bold>,<bold>x̅</bold>′ are two sequences over an alphabet Σ, e.g., protein or DNA sequences. By |Σ|, we denote the number of letters in the alphabet. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e017" xlink:type="simple"/></inline-formula> is a mapping of the sequence <bold>x̅</bold> into a |Σ|<sup>ℓ</sup> dimensional feature space. Each dimension corresponds to one of the |Σ|<sup>ℓ</sup> possible strings <italic>s</italic> of length ℓ and is the count of the number of occurrences of <italic>s</italic> in <bold>x̅</bold>. Please note that computing the spectrum kernel using the explicit computation of Φ will be inefficient for large ℓ: since it requires computation of the |Σ|<sup>ℓ</sup> entries of the mapping Φ, which would be unfeasible for nucleotide sequences with ℓ≥10 or protein sequences with ℓ≥5. Faster computation is possible by exploiting the fact that the only ℓ-mers that contribute to the dot product (in Equation 11) are those that actually appear in the sequences. This leads to algorithms that are linear in the length of the sequences instead of the exponential |Σ|<sup>ℓ</sup> computation time (see, e.g., <xref ref-type="bibr" rid="pcbi.1000173-Sonnenburg2">[29]</xref> for more details and references).</p>
<p>If we use the spectrum kernel for the splice site recognition task, we obtain considerable improvement over the simple GC content features (see <xref ref-type="table" rid="pcbi-1000173-t002">Table 2</xref>). The co-occurrence of long substrings is more informative than those of short ones. This explains the increase in performance of the spectrum kernel as the length of substrings ℓ is increased. Since the spectrum kernel allows no mismatches, when ℓ is sufficiently long the chance of observing common occurrences becomes small and the kernel will no longer perform well. This explains the decrease in the performance observed in <xref ref-type="table" rid="pcbi-1000173-t002">Table 2</xref> for ℓ = 5. This problem is alleviated if we use the <italic>mixed spectrum kernel</italic>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e018" xlink:type="simple"/><label>(12)</label></disp-formula>where <italic>β<sub>d</sub></italic> is a weighting for the different substring lengths (details below).</p>
<table-wrap id="pcbi-1000173-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000173.t002</object-id><label>Table 2</label><caption>
<title>The area under the ROC curve (auROC) of SVMs with the spectrum, mixed spectrum, and weighted degree kernels on the acceptor splice site recognition task for different substring lengths ℓ.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000173-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.t002" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Kernel</td>
<td align="left" colspan="1" rowspan="1">auROC</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Spectrum ℓ = 1</td>
<td align="left" colspan="1" rowspan="1">94.0%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Spectrum ℓ = 3</td>
<td align="left" colspan="1" rowspan="1">96.4%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Spectrum ℓ = 5</td>
<td align="left" colspan="1" rowspan="1">94.5%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Mixed spectrum ℓ = 1</td>
<td align="left" colspan="1" rowspan="1">94.0%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Mixed spectrum ℓ = 3</td>
<td align="left" colspan="1" rowspan="1">96.9%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Mixed spectrum ℓ = 5</td>
<td align="left" colspan="1" rowspan="1">97.2%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">WD ℓ = 1</td>
<td align="left" colspan="1" rowspan="1">98.2%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">WD ℓ = 3</td>
<td align="left" colspan="1" rowspan="1">98.7%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">WD ℓ = 5</td>
<td align="left" colspan="1" rowspan="1">98.9%</td>
</tr>
</tbody>
</table></alternatives></table-wrap></sec><sec id="s3a2b">
<title>Kernels using positional information</title>
<p>The kernels mentioned above ignore the position of substrings within the input sequence. However, in our example of splice site prediction, it is known that there exist sequence motifs near the splice site that allow the spliceosome to accurately recognize the splice sites. While the spectrum kernel is in principle able to recognize such motifs, it cannot distinguish where exactly the motif appears in the sequence. However, this is crucial in deciding where exactly the splice site is located. And indeed, Position Weight Matrices (PWMs) are able to predict splice sites with high accuracy. The kernel introduced next is analogous to PWMs in the way it uses positional information, and its use in conjunction with a large margin classifier leads to improved performance <xref ref-type="bibr" rid="pcbi.1000173-Rtsch1">[30]</xref>. The idea is to analyze sequences of fixed length <italic>L</italic> and consider substrings starting at each position <italic>l</italic> = 1,…,<italic>L separately</italic>, as implemented by the so-called <italic>weighted degree (WD) kernel</italic>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e019" xlink:type="simple"/><label>(13)</label></disp-formula>where <bold>x̅</bold><sub>[<italic>l</italic>:<italic>l</italic>+<italic>d</italic>]</sub> is the substring of length <italic>d</italic> of <bold>x̅</bold> at position <italic>l</italic>. A suggested setting for <italic>β<sub>d</sub></italic> is the weighting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e020" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1000173-Sonnenburg2">[29]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-Rtsch1">[30]</xref>. Note that using the WD kernel is equivalent to using a mixed spectrum kernel for each position of the sequence separately (ignoring boundary effects). Observe in <xref ref-type="table" rid="pcbi-1000173-t002">Table 2</xref> that, as expected, the positional information considerably improves the SVM performance.</p>
<p>The <italic>WD kernel with shifts</italic> <xref ref-type="bibr" rid="pcbi.1000173-Rtsch2">[31]</xref> is an extension of the WD kernel, allowing some positional flexibility of matching substrings. The locality improved kernel <xref ref-type="bibr" rid="pcbi.1000173-Zien1">[32]</xref> and the <italic>oligo kernel</italic> <xref ref-type="bibr" rid="pcbi.1000173-Meinicke1">[33]</xref> achieve a similar goal in a slightly different way.</p>
<p>Note that since the polynomial and Gaussian kernels are functions of the linear kernel, the above-described sequence kernels can be used in conjunction with the polynomial or Gaussian kernel to model more complex decision boundaries. For instance, the polynomial kernel of degree <italic>d</italic> combined with the ℓ-spectrum kernel, i.e.,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e021" xlink:type="simple"/></disp-formula>can model up to <italic>d</italic> co-occurrences of ℓ-mers (similarly proposed in <xref ref-type="bibr" rid="pcbi.1000173-Zien1">[32]</xref>).</p>
</sec><sec id="s3a2c">
<title>Other sequence kernels</title>
<p>Because of the importance of sequence data and the many ways of modeling it, there are many alternatives to the spectrum and weighted degree kernels. Most closely related to the spectrum kernel are extensions allowing for gaps or mismatches <xref ref-type="bibr" rid="pcbi.1000173-Leslie2">[28]</xref>. The feature space of the spectrum kernel and these related kernels is the set of <italic>all</italic> ℓ-mers of a given length. An alternative is to restrict attention to a predefined set of motifs <xref ref-type="bibr" rid="pcbi.1000173-Logan1">[34]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-BenHur1">[35]</xref>.</p>
<p>Sequence similarity has been studied extensively in the bioinformatics community, and local alignment algorithms like BLAST and Smith-Waterman are good at revealing regions of similarity between proteins and DNA sequences. The statistics produced by these algorithms do not satisfy the mathematical condition required of a kernel function. But they can still be used as a basis for highly effective kernels. The simplest way is to represent a sequence in terms of its BLAST/Smith-Waterman scores against a database of sequences <xref ref-type="bibr" rid="pcbi.1000173-Liao1">[36]</xref>. This is a general method for using a similarity measure as a kernel. An alternative approach taken was to modify the Smith-Waterman algorithm to consider the space of <italic>all</italic> local alignments, leading to the <italic>local alignment kernel</italic> <xref ref-type="bibr" rid="pcbi.1000173-Vert2">[37]</xref>.</p>
<p>Probabilistic models, and Hidden Markov Models in particular, are in wide use for sequence analysis. The dependence of the log-likelihood of a sequence on the parameters of the model can be used to represent a variable-length sequence in a fixed dimensional vector space. The so-called Fisher-kernel uses the sensitivity of the log-likelihood of a sequence with respect to the model parameters as the feature space <xref ref-type="bibr" rid="pcbi.1000173-Jaakkola1">[38]</xref> (see also <xref ref-type="bibr" rid="pcbi.1000173-Tsuda1">[39]</xref>). The intuition is that if we were to update the model to increase the likelihood of the data, this is the direction a gradient-based method would take. Thus, we are characterizing a sequence by its effect on the model. Other kernels based on probabilistic models include the Covariance kernel <xref ref-type="bibr" rid="pcbi.1000173-Seeger1">[40]</xref> and Marginalized kernels <xref ref-type="bibr" rid="pcbi.1000173-Tsuda2">[41]</xref>.</p>
</sec></sec></sec></sec><sec id="s4">
<title>Summary and Further Reading</title>
<p>This tutorial introduced the concepts of large margin classification as implemented by SVMs, an idea that is both intuitive and also supported by theoretical results in statistical learning theory. The SVM algorithm allows the use of kernels, which are efficient ways of computing scalar products in nonlinear feature spaces. The “kernel trick” is also applicable to other types of data, e.g., sequence data, which we illustrated in the problem of predicting splice sites in <italic>C. elegans</italic>.</p>
<p>In the rest of this section, we outline issues that we have not covered in this tutorial and provide pointers for further reading. For a comprehensive discussion of SVMs and kernel methods, we refer the reader to recent books on the subject <xref ref-type="bibr" rid="pcbi.1000173-Schlkopf1">[2]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-Schlkopf2">[5]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-ShaweTaylor1">[7]</xref>.</p>
<sec id="s4a">
<title/>
<sec id="s4a1">
<title>Normalization</title>
<p>Large margin classifiers are known to be sensitive to the way features are scaled (see, for example <xref ref-type="bibr" rid="pcbi.1000173-Chang1">[42]</xref>, in the context of SVMs). It can therefore be essential to normalize the data. This observation carries over to kernel-based classifiers that use nonlinear kernel functions. Normalization can be performed at the level of the input features or at the level of the kernel (normalization in feature space), or both. When features are measured in different scales and have different ranges of possible values, it is often beneficial to scale them to a common range, e.g., by <italic>standardizing</italic> the data (for each feature, subtracting its mean and dividing by its standard deviation). An alternative to normalizing each feature separately is to normalize each example to be a unit vector. This can be done at the level of the input features by dividing each example by its norm, i.e., <bold>x̃</bold>: = <bold>x</bold>/∥<bold>x</bold>∥, or at the level of the kernel which normalizes in the feature-space of the kernel, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000173.e022" xlink:type="simple"/></inline-formula>. For the discussed splice site data, the results differed considerably when using different normalizations for the linear, polynomial, and Gaussian kernels. Generally, our experience shows that normalization often leads to improved performance for both linear and nonlinear kernels, and can also lead to faster convergence.</p>
</sec><sec id="s4a2">
<title>Handling unbalanced data</title>
<p>Many datasets encountered in bioinformatics and other areas of application are unbalanced, i.e., one class contains a lot more examples than the other. For instance, in the case of splice site detection, there are 100 times fewer positive examples than negative ones. Unbalanced datasets can present a challenge when training a classifier, and SVMs are no exception. The standard approach to addressing this issue is to assign a different misclassification cost to each class. For SVMs, this is achieved by associating a different soft-margin constant to each class according to the number of examples in the class (see, e.g., <xref ref-type="bibr" rid="pcbi.1000173-Provost2">[43]</xref> for a general overview of the issue). For instance, for the splice site recognition example, one may use a value of <italic>C</italic> (in Equation 3) that is 100 times larger for the positive class than for the negative class. Often when data is unbalanced, the cost of misclassification is also unbalanced; for example, having a false negative is more costly than a false positive. In some cases, considering the SVM score directly rather than just the sign of the score is more useful.</p>
</sec><sec id="s4a3">
<title>Kernel choice and model selection</title>
<p>A question frequently posed by practitioners is “which kernel with which parameters should I use for my data?” There are several answers to this question. The first is that it is, like most practical questions in machine learning, data-dependent, so several kernels should be tried. That being said, one typically follows the following procedure: try a linear kernel first, and then see if we can improve on its performance using a nonlinear kernel. The linear kernel provides a useful baseline, and in many bioinformatics applications it is hard to beat, in particular if the dimensionality of the inputs is large and the number of examples small. The flexibility of the Gaussian and polynomial kernels can lead to overfitting in high-dimensional datasets with a small number of examples, such as in micro-array datasets. If the examples are (biological) sequences, then the spectrum or the WD kernel of relatively low order (say ℓ = 3) are good starting points if the sequences have varying or fixed length. Depending on the problem, one may then try the spectrum kernel with mismatches, the oligo kernel, the WD kernel with shifts, or the local alignment kernel.</p>
<p>In problems such as prediction of protein function or protein interactions, there are several sources of genomic data that are relevant, each of which may require a different kernel to model. Rather than choosing a single kernel, several papers have established that using a combination of multiple kernels can significantly boost classifier performance <xref ref-type="bibr" rid="pcbi.1000173-Pavlidis1">[44]</xref>–<xref ref-type="bibr" rid="pcbi.1000173-BenHur2">[46]</xref>.</p>
<p>When selecting the kernel, its parameters, and the soft-margin parameter <italic>C</italic>, one has to take care that this choice is made completely independently of the examples used for performance evaluation of the method. Otherwise, one will overestimate the accuracy of the classifier on unseen data points. This can be done by suitably splitting the data into several parts, where one part, say 50%, is used for training, another part (20%) for tuning of SVM and kernel parameters, and a third part (30%) for final evaluation. Techniques such as <italic>N</italic>-fold cross-validation can help if the parts become too small to reliably measure prediction performance (see, for example, <xref ref-type="bibr" rid="pcbi.1000173-Tarca1">[47]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-Duda1">[48]</xref>).</p>
</sec><sec id="s4a4">
<title>Kernels for other data types</title>
<p>We have focused on kernels for real-valued and sequence data; and while this covers many bioinformatics applications, often data is better modeled by more complex data types. Many types of bioinformatics data can be modeled as graphs, and the inputs can be either nodes in the graph, e.g., proteins in an interaction network, or the inputs can be represented by graphs, e.g., proteins modeled by phylogenetic trees. Kernels have been developed for both scenarios. Researchers have developed kernels to compare phylogenetic profiles modeled as trees <xref ref-type="bibr" rid="pcbi.1000173-Vert3">[49]</xref>, protein structures modeled as graphs of secondary-structural elements <xref ref-type="bibr" rid="pcbi.1000173-Borgwardt1">[50]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-Borgwardt2">[51]</xref>, and graphs representing small molecules <xref ref-type="bibr" rid="pcbi.1000173-Kashima1">[52]</xref>. The diffusion kernel is a general method for propagating kernel values on a graph <xref ref-type="bibr" rid="pcbi.1000173-Kondor1">[53]</xref>. Several of the kernels described above are based on the framework of <italic>convolution kernels</italic> <xref ref-type="bibr" rid="pcbi.1000173-Haussler1">[54]</xref>, which is a method for developing kernels for an object based on kernels defined on its sub-parts, such as a protein structure composed of secondary structural elements <xref ref-type="bibr" rid="pcbi.1000173-Borgwardt1">[50]</xref>. Kernels (and hence the similarity) on structured data can also be understood as how much one object has to be transformed before it is identical to the other, which leads to the idea of transducers <xref ref-type="bibr" rid="pcbi.1000173-Cortes2">[55]</xref>. More details on kernels can be found in books such as <xref ref-type="bibr" rid="pcbi.1000173-Schlkopf1">[2]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-Schlkopf2">[5]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-ShaweTaylor1">[7]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-Grtner1">[56]</xref>.</p>
</sec><sec id="s4a5">
<title>SVM training algorithms and software</title>
<p>The popularity of SVMs has led to the development of a large number of special-purpose solvers for the SVM optimization problem <xref ref-type="bibr" rid="pcbi.1000173-Bottou1">[57]</xref>. <italic>LIBSVM</italic> <xref ref-type="bibr" rid="pcbi.1000173-Chang1">[42]</xref> and <italic>SVM<sup>light</sup></italic> <xref ref-type="bibr" rid="pcbi.1000173-Joachims1">[58]</xref> are two popular examples of this class of software. The complexity of training of nonlinear SVMs with solvers such as <italic>LIBSVM</italic> has been estimated to be quadratic in the number of training examples <xref ref-type="bibr" rid="pcbi.1000173-Bottou1">[57]</xref>, which can be prohibitive for datasets with hundreds of thousands of examples. Researchers have therefore explored ways to achieve faster training times. For linear SVMs, very efficient solvers are available that converge in a time that is linear in the number of examples <xref ref-type="bibr" rid="pcbi.1000173-Bottou1">[57]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-Joachims2">[59]</xref>,<xref ref-type="bibr" rid="pcbi.1000173-Sindhwani1">[60]</xref>. Approximate solvers that can be trained in linear time without a significant loss of accuracy were also developed <xref ref-type="bibr" rid="pcbi.1000173-Bordes1">[61]</xref>.</p>
<p>Another class of software includes machine learning libraries that provide a variety of classification methods and other facilities such as methods for feature selection, preprocessing, etc. The user has a large number of choices, and the following is an incomplete list of environments that provide an SVM classifier: <italic>Orange</italic> <xref ref-type="bibr" rid="pcbi.1000173-Demsar1">[62]</xref>, <italic>The Spider</italic> <xref ref-type="bibr" rid="pcbi.1000173-3">[63]</xref>, <italic>Elefant</italic> <xref ref-type="bibr" rid="pcbi.1000173-Gawande1">[64]</xref>, <italic>Plearn</italic> <xref ref-type="bibr" rid="pcbi.1000173-4">[65]</xref>, <italic>Weka</italic> <xref ref-type="bibr" rid="pcbi.1000173-Witten1">[66]</xref>, Lush <xref ref-type="bibr" rid="pcbi.1000173-Bottou2">[67]</xref>, <italic>Shogun</italic> <xref ref-type="bibr" rid="pcbi.1000173-Sonnenburg3">[68]</xref>, <italic>RapidMiner</italic> <xref ref-type="bibr" rid="pcbi.1000173-Mierswa1">[69]</xref>, <italic>PyML</italic> <xref ref-type="bibr" rid="pcbi.1000173-5">[70]</xref>, and <italic>Easysvm</italic> <xref ref-type="bibr" rid="pcbi.1000173-1">[17]</xref>. The SVM implementations in several of these packages are wrappers for the <italic>LIBSVM</italic> <xref ref-type="bibr" rid="pcbi.1000173-Chang1">[42]</xref> or <italic>SVM<sup>light</sup></italic> <xref ref-type="bibr" rid="pcbi.1000173-Joachims1">[58]</xref> library. The <italic>Shogun</italic> toolbox contains eight different SVM implementations together with a large collection of different kernels for real-valued and sequence data.</p>
<p>A repository of machine learning open source software is available at <ext-link ext-link-type="uri" xlink:href="http://mloss.org" xlink:type="simple">http://mloss.org</ext-link> as part of an initiative advocating distribution of machine learning algorithms as open source software <xref ref-type="bibr" rid="pcbi.1000173-Sonnenburg4">[71]</xref>.</p>
</sec></sec></sec></body>
<back>
<ack>
<p>We would like to thank Alexander Zien for discussions, and Nora Toussaint, Sebastian Henschel, and Petra Philips for comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1000173-Boser1"><label>1</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Boser</surname><given-names>BE</given-names></name>
<name name-style="western"><surname>Guyon</surname><given-names>IM</given-names></name>
<name name-style="western"><surname>Vapnik</surname><given-names>VN</given-names></name>
</person-group>             <year>1992</year>             <article-title>A training algorithm for optimal margin classifiers.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Haussler</surname><given-names>D</given-names></name>
</person-group>             <source>5th Annual ACM Workshop on COLT</source>             <publisher-loc>Pittsburgh (Pennsylvania)</publisher-loc>             <publisher-name>ACM Press</publisher-name>             <fpage>144</fpage>             <lpage>152</lpage>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://www.clopinet.com/isabelle/Papers" xlink:type="simple">http://www.clopinet.com/isabelle/Papers</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Schlkopf1"><label>2</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Smola</surname><given-names>A</given-names></name>
</person-group>             <year>2002</year>             <source>Learning with kernels</source>             <publisher-loc>Cambridge (Massachusetts)</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000173-Vapnik1"><label>3</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Vapnik</surname><given-names>V</given-names></name>
</person-group>             <year>1999</year>             <source>The nature of statistical learning theory. 2nd edition</source>             <publisher-name>Springer</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000173-Mller1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Müller</surname><given-names>KR</given-names></name>
<name name-style="western"><surname>Mika</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Rätsch</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Tsuda</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name>
</person-group>             <year>2001</year>             <article-title>An introduction to kernel-based learning algorithms.</article-title>             <source>IEEE Trans Neural Netw</source>             <volume>12</volume>             <fpage>181</fpage>             <lpage>201</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Schlkopf2"><label>5</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Tsuda</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Vert</surname><given-names>JP</given-names></name>
</person-group>             <year>2004</year>             <source>Kernel methods in computational biology</source>             <publisher-loc>Cambridge (Massachusetts)</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000173-Vert1"><label>6</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Vert</surname><given-names>JP</given-names></name>
</person-group>             <year>2007</year>             <article-title>Kernel methods in genomics and computational biology.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Camps-Valls</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Rojo-Alvarez</surname><given-names>JL</given-names></name>
<name name-style="western"><surname>Martinez-Ramon</surname><given-names>M</given-names></name>
</person-group>             <source>Kernel methods in bioengineering, signal and image processing</source>             <publisher-name>Idea Group</publisher-name>             <fpage>42</fpage>             <lpage>63</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-ShaweTaylor1"><label>7</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Shawe-Taylor</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Cristianini</surname><given-names>N</given-names></name>
</person-group>             <year>2004</year>             <source>Kernel methods for pattern analysis</source>             <publisher-loc>Cambridge (United Kingdom)</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000173-Black1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Black</surname><given-names>DL</given-names></name>
</person-group>             <year>2003</year>             <article-title>Mechanisms of alternative pre-messenger RNA splicing.</article-title>             <source>Annu Rev Biochem</source>             <volume>72</volume>             <fpage>291</fpage>             <lpage>336</lpage>             <comment>doi:10.1146/annurev.biochem.72.121801.161720. Available: <ext-link ext-link-type="uri" xlink:href="http://arjournals.annualreviews.org/doi/abs/10.1146/annurev.biochem.72.121801.161720?cookieSet=1∓journalCode=biochem" xlink:type="simple">http://arjournals.annualreviews.org/doi/abs/10.1146/annurev.biochem.72.121801.161720?cookieSet=1&amp;journalCode=biochem</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Burge1"><label>9</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Burge</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Tuschl</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Sharp</surname><given-names>P</given-names></name>
</person-group>             <year>1999</year>             <article-title>Splicing of precursors to mRNAs by the spliceosomes.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Gesteland</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Cech</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Atkins</surname><given-names>JF</given-names></name>
</person-group>             <source>The RNA world. 2nd edition</source>             <publisher-name>Cold Spring Harbor Laboratory Press</publisher-name>             <fpage>525</fpage>             <lpage>560</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Nilsen1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nilsen</surname><given-names>T</given-names></name>
</person-group>             <year>2003</year>             <article-title>The spliceosome: The most complex macromolecular machine in the cell?</article-title>             <source>Bioessays</source>             <volume>25</volume>          </element-citation></ref>
<ref id="pcbi.1000173-Lewin1"><label>11</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lewin</surname><given-names>B</given-names></name>
</person-group>             <year>2007</year>             <source>Genes IX</source>             <publisher-name>Jones &amp; Bartlett Publishers</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000173-Holste1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Holste</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Ohler</surname><given-names>U</given-names></name>
</person-group>             <year>2008</year>             <article-title>Strategies for identifying RNA splicing regulatory motifs and predicting alternative splicing events.</article-title>             <source>PLoS Computational Biology</source>             <volume>4</volume>             <fpage>e21</fpage>             <comment>doi/10.1371/journal.pcbi.0040021</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Sonnenburg1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sonnenburg</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Schweikert</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Philips</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Behr</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Rätsch</surname><given-names>G</given-names></name>
</person-group>             <year>2007</year>             <article-title>Accurate splice site prediction using support vector machines.</article-title>             <source>BMC Bioinformatics</source>             <volume>8</volume>             <fpage>S7</fpage>          </element-citation></ref>
<ref id="pcbi.1000173-Metz1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Metz</surname><given-names>CE</given-names></name>
</person-group>             <year>1978</year>             <article-title>Basic principles of ROC analysis.</article-title>             <source>Seminars in Nuclear Medicine</source>             <volume>VIII</volume>          </element-citation></ref>
<ref id="pcbi.1000173-Provost1"><label>15</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Provost</surname><given-names>FJ</given-names></name>
<name name-style="western"><surname>Fawcett</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Kohavi</surname><given-names>R</given-names></name>
</person-group>             <year>1998</year>             <article-title>The case against accuracy estimation for comparing induction algorithms.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Shavlik</surname><given-names>J</given-names></name>
</person-group>             <comment>ICML '98: Proceedings of the Fifteenth International Conference on Machine Learning. San Francisco: Morgan Kaufmann Publishers Inc</comment>          <fpage>445</fpage>             <lpage>453</lpage>             </element-citation></ref>
<ref id="pcbi.1000173-Davis1"><label>16</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Davis</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Goadrich</surname><given-names>M</given-names></name>
</person-group>             <year>2006</year>             <source>The relationship between precision-recall and ROC curves. ICML</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>ACM Press</publisher-name>             <fpage>233</fpage>             <lpage>240</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-1"><label>17</label><element-citation publication-type="other" xlink:type="simple">             <article-title><italic>Easysvm</italic> toolbox.</article-title>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://www.easysvm.org" xlink:type="simple">http://www.easysvm.org</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Python1"><label>18</label><element-citation publication-type="other" xlink:type="simple">             <collab xlink:type="simple">Python Software Foundation</collab>             <year>2007</year>             <article-title>Python.</article-title>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://python.org" xlink:type="simple">http://python.org</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Bassi1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bassi</surname><given-names>S</given-names></name>
</person-group>             <year>2007</year>             <article-title>A primer on python for life science researchers.</article-title>             <source>PLoS Comput Biol</source>             <volume>3</volume>             <fpage>e199</fpage>             <comment>doi:10.1371/journal.pcbi.0030199</comment>          </element-citation></ref>
<ref id="pcbi.1000173-2"><label>20</label><element-citation publication-type="other" xlink:type="simple">             <article-title>GNU general public license.</article-title>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://www.gnu.org/copyleft/gpl.html" xlink:type="simple">http://www.gnu.org/copyleft/gpl.html</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Boyd1"><label>21</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Boyd</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Vandenberghe</surname><given-names>L</given-names></name>
</person-group>             <year>2004</year>             <source>Convex optimization</source>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000173-Cortes1"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cortes</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Vapnik</surname><given-names>V</given-names></name>
</person-group>             <year>1995</year>             <article-title>Support vector networks.</article-title>             <source>Mach Learn</source>             <volume>20</volume>             <fpage>273</fpage>             <lpage>297</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Hastie1"><label>23</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Friedman</surname><given-names>J</given-names></name>
</person-group>             <year>2001</year>             <source>The elements of statistical learning</source>             <publisher-name>Springer</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000173-Bishop1"><label>24</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bishop</surname><given-names>C</given-names></name>
</person-group>             <year>2007</year>             <source>Pattern recognition and machine learning</source>             <publisher-name>Springer</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000173-Guyon1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Guyon</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Weston</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Barnhill</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Vapnik</surname><given-names>V</given-names></name>
</person-group>             <year>2002</year>             <article-title>Gene selection for cancer classification using support vector machines.</article-title>             <source>Mach Learn</source>             <volume>46</volume>             <fpage>489</fpage>             <lpage>422</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Cai1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cai</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Han</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Ji</surname><given-names>Z</given-names></name>
<name name-style="western"><surname>Chen</surname><given-names>X</given-names></name>
<name name-style="western"><surname>Chen</surname><given-names>Y</given-names></name>
</person-group>             <year>2003</year>             <article-title>SVM-Prot: Web-based support vector machine software for functional classification of a protein from its primary sequence.</article-title>             <source>Nucleic Acids Res</source>             <volume>31</volume>             <fpage>3692</fpage>             <lpage>3697</lpage>             <comment>doi:10.1093/nar/gkg600. Available: <ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/abstract/31/13/3692" xlink:type="simple">http://nar.oxfordjournals.org/cgi/content/abstract/31/13/3692</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Leslie1"><label>27</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Leslie</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Eskin</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Noble</surname><given-names>WS</given-names></name>
</person-group>             <year>2002</year>             <article-title>The spectrum kernel: A string kernel for SVM protein classification.</article-title>             <fpage>564</fpage>             <lpage>575</lpage>             <comment>In: Proceedings of the Pacific Symposium on Biocomputing</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Leslie2"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Leslie</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Eskin</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Weston</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Noble</surname><given-names>W</given-names></name>
</person-group>             <year>2003</year>             <article-title>Mismatch string kernels for discriminative protein classification.</article-title>             <source>Bioinformatics</source>             <volume>20</volume>          </element-citation></ref>
<ref id="pcbi.1000173-Sonnenburg2"><label>29</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sonnenburg</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Rätsch</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Rieck</surname><given-names>K</given-names></name>
</person-group>             <year>2007</year>             <article-title>Large scale learning with string kernels.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Bottou</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Chapelle</surname><given-names>O</given-names></name>
<name name-style="western"><surname>DeCoste</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Weston</surname><given-names>J</given-names></name>
</person-group>             <source>Large scale kernel machines</source>             <publisher-name>MIT Press</publisher-name>             <fpage>73</fpage>             <lpage>104</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Rtsch1"><label>30</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rätsch</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Sonnenburg</surname><given-names>S</given-names></name>
</person-group>             <year>2004</year>             <article-title>Accurate splice site detection for <italic>Caenorhabditis elegans</italic>.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>B Schölkopf</surname><given-names>KT</given-names></name>
<name name-style="western"><surname>Vert</surname><given-names>JP</given-names></name>
</person-group>             <source>Kernel methods in computational biology</source>             <publisher-loc>Cambridge (Massachusetts)</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>277</fpage>             <lpage>298</lpage>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://www.fml.tuebingen.mpg.de/raetsch/projects/MITBookSplice/files/RaeSon04.pdf" xlink:type="simple">http://www.fml.tuebingen.mpg.de/raetsch/projects/MITBookSplice/files/RaeSon04.pdf</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Rtsch2"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rätsch</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Sonnenburg</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name>
</person-group>             <year>2005</year>             <article-title>RASE: Recognition of alternatively spliced exons in <italic>C. elegans</italic>.</article-title>             <source>Bioinformatics</source>             <volume>21</volume>             <fpage>i369</fpage>             <lpage>i377</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Zien1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zien</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Rätsch</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Mika</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Lengauer</surname><given-names>T</given-names></name>
<etal/></person-group>             <year>2000</year>             <article-title>Engineering support vector machine kernels that recognize translation initiation sites.</article-title>             <source>Bioinformatics</source>             <volume>16</volume>             <fpage>799</fpage>             <lpage>807</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Meinicke1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Meinicke</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Tech</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Morgenstern</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Merkl</surname><given-names>R</given-names></name>
</person-group>             <year>2004</year>             <article-title>Oligo kernels for datamining on biological sequences: A case study on prokaryotic translation initiation sites.</article-title>             <source>BMC Bioinformatics</source>             <volume>5</volume>             <fpage>169</fpage>          </element-citation></ref>
<ref id="pcbi.1000173-Logan1"><label>34</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Logan</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Moreno</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Suzek</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Weng</surname><given-names>Z</given-names></name>
<name name-style="western"><surname>Kasif</surname><given-names>S</given-names></name>
</person-group>             <year>2001</year>             <article-title>A study of remote homology detection.</article-title>             <comment>Technical report CRL 2001/05. Compaq Cambridge Research Laboratory</comment>          </element-citation></ref>
<ref id="pcbi.1000173-BenHur1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ben-Hur</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Brutlag</surname><given-names>D</given-names></name>
</person-group>             <year>2003</year>             <article-title>Remote homology detection: A motif based approach.</article-title>             <source>Bioinformatics</source>             <volume>19</volume>             <fpage>i26</fpage>             <lpage>i33</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Liao1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Liao</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Noble</surname><given-names>W</given-names></name>
</person-group>             <year>2003</year>             <article-title>Combining pairwise similarity and support vector machines for detecting remote protein evolutionary and structural relationships.</article-title>             <source>J Comput Biol</source>             <volume>10</volume>             <fpage>2429</fpage>             <lpage>2437</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Vert2"><label>37</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Vert</surname><given-names>JP</given-names></name>
<name name-style="western"><surname>Saigo</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Akutsu</surname><given-names>T</given-names></name>
</person-group>             <year>2004</year>             <article-title>Local alignment kernels for biological sequences.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>B Schölkopf</surname><given-names>KT</given-names></name>
<name name-style="western"><surname>Vert</surname><given-names>JP</given-names></name>
</person-group>             <source>Kernel methods in computational biology</source>             <publisher-loc>Cambridge (Massachusetts)</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>131</fpage>             <lpage>154</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Jaakkola1"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Jaakkola</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Diekhans</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Haussler</surname><given-names>D</given-names></name>
</person-group>             <year>2000</year>             <article-title>A discriminative framework for detecting remote protein homologies.</article-title>             <source>J Comp Biol</source>             <volume>7</volume>             <fpage>95</fpage>             <lpage>114</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Tsuda1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tsuda</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Kawanabe</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Rätsch</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Sonnenburg</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Müller</surname><given-names>K</given-names></name>
</person-group>             <year>2002</year>             <article-title>A new discriminative kernel from probabilistic models.</article-title>             <source>Neural Computation</source>             <volume>14</volume>             <fpage>2397</fpage>             <lpage>2414</lpage>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://neco.mitpress.org/cgi/content/abstract/14/10/2397" xlink:type="simple">http://neco.mitpress.org/cgi/content/abstract/14/10/2397</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Seeger1"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seeger</surname><given-names>M</given-names></name>
</person-group>             <year>2002</year>             <article-title>Covariance kernels from Bayesian generative models.</article-title>             <source>Adv Neural Information Proc Sys</source>             <volume>14</volume>             <fpage>905</fpage>             <lpage>912</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Tsuda2"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tsuda</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Kin</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Asai</surname><given-names>K</given-names></name>
</person-group>             <year>2002</year>             <article-title>Marginalized kernels for biological sequences.</article-title>             <source>Bioinformatics</source>             <volume>18</volume>             <fpage>268S</fpage>             <lpage>275S</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Chang1"><label>42</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chang</surname><given-names>CC</given-names></name>
<name name-style="western"><surname>Lin</surname><given-names>CJ</given-names></name>
</person-group>             <year>2001</year>             <article-title>LIBSVM: A library for support vector machines.</article-title>             <comment>Software available at <ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~]cjlin/libsvm" xlink:type="simple">http://www.csie.ntu.edu.tw/̃cjlin/libsvm</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Provost2"><label>43</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Provost</surname><given-names>F</given-names></name>
</person-group>             <year>2000</year>             <article-title>Learning with imbalanced data sets 101.</article-title>             <comment>In: AAAI 2000 workshop on imbalanced data sets. Available: <ext-link ext-link-type="uri" xlink:href="http://pages.stern.nyu.edu/~fprovost/Papers/skew.PDF" xlink:type="simple">http://pages.stern.nyu.edu/̃fprovost/Papers/skew.PDF</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Pavlidis1"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pavlidis</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Weston</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Cai</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Noble</surname><given-names>W</given-names></name>
</person-group>             <year>2002</year>             <article-title>Learning gene functional classifications from multiple data types.</article-title>             <source>J Comput Biol</source>             <volume>9</volume>             <fpage>401</fpage>             <lpage>411</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Lanckriet1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lanckriet</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Bie</surname><given-names>TD</given-names></name>
<name name-style="western"><surname>Cristianini</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Jordan</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Noble</surname><given-names>W</given-names></name>
</person-group>             <year>2004</year>             <article-title>A statistical framework for genomic data fusion.</article-title>             <source>Bioinformatics</source>             <volume>20</volume>             <fpage>2626</fpage>             <lpage>2635</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-BenHur2"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ben-Hur</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Noble</surname><given-names>WS</given-names></name>
</person-group>             <year>2005</year>             <article-title>Kernel methods for predicting protein–protein interactions.</article-title>             <source>Bioinformatics</source>             <volume>21</volume><supplement>(Supplement 1)</supplement>             <fpage>i38</fpage>             <lpage>i46</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Tarca1"><label>47</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tarca</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Carey</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Chen</surname><given-names>XW</given-names></name>
<name name-style="western"><surname>Romero</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Draghici</surname><given-names>S</given-names></name>
</person-group>             <year>2007</year>             <article-title>Machine learning and its applications to biology.</article-title>             <source>PLoS Comput Biol</source>             <volume>3</volume>             <fpage>e116</fpage>             <comment>doi/10.1371/journal.pcbi.0030116</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Duda1"><label>48</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Duda</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Hart</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Stork</surname><given-names>D</given-names></name>
</person-group>             <year>2001</year>             <source>Pattern classification</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Wiley-Interscience</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000173-Vert3"><label>49</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Vert</surname><given-names>JP</given-names></name>
</person-group>             <year>2002</year>             <article-title>A tree kernel to analyze phylogenetic profiles.</article-title>             <source>Bioinformatics</source>             <volume>18</volume>             <fpage>S276</fpage>             <lpage>S284</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Borgwardt1"><label>50</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Borgwardt</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Ong</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Schnauer</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Vishwanathan</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Smola</surname><given-names>A</given-names></name>
<etal/></person-group>             <year>2005</year>             <article-title>Protein function prediction via graph kernels.</article-title>             <source>Bioinformatics</source>             <volume>21</volume>             <fpage>i47</fpage>             <lpage>i56</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Borgwardt2"><label>51</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Borgwardt</surname><given-names>KM</given-names></name>
</person-group>             <year>2007</year>             <source>Graph dernels [Ph.D. thesis]</source>             <publisher-loc>Munich</publisher-loc>             <publisher-name>Ludwig-Maximilians-University Munich</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000173-Kashima1"><label>52</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kashima</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Tsuda</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Inokuchi</surname><given-names>A</given-names></name>
</person-group>             <year>2004</year>             <article-title>Kernels for graphs.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Tsuda</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Vert</surname><given-names>JP</given-names></name>
</person-group>             <source>Kernel methods in computational biology</source>             <publisher-loc>Cambridge (Massachusetts)</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>155</fpage>             <lpage>170</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Kondor1"><label>53</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kondor</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Vert</surname><given-names>JP</given-names></name>
</person-group>             <year>2004</year>             <article-title>Diffusion kernels.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Tsuda</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Vert</surname><given-names>JP</given-names></name>
</person-group>             <source>Kernel methods in computational biology</source>             <publisher-loc>Cambridge (Massachusetts)</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>171</fpage>             <lpage>192</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Haussler1"><label>54</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Haussler</surname><given-names>D</given-names></name>
</person-group>             <year>1999</year>             <article-title>Convolutional kernels on discrete structures.</article-title>             <comment>Technical Report UCSC-CRL-99-10. Santa Cruz (California): UC Santa Cruz Computer Science Department</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Cortes2"><label>55</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cortes</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Haffner</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Mohri</surname><given-names>M</given-names></name>
</person-group>             <year>2004</year>             <article-title>Rational kernels: Theory and algorithms.</article-title>             <source>J Mach Learn Res</source>             <volume>5</volume>             <fpage>1035</fpage>             <lpage>1062</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Grtner1"><label>56</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gärtner</surname><given-names>T</given-names></name>
</person-group>             <year>2008</year>             <source>Kernels for structured data.</source>             <publisher-name>World Scientific Publishing</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000173-Bottou1"><label>57</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="editor">
<name name-style="western"><surname>Bottou</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Chapelle</surname><given-names>O</given-names></name>
<name name-style="western"><surname>DeCoste</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Weston</surname><given-names>J</given-names></name>
</person-group>             <year>2007</year>             <source>Large scale kernel machines</source>             <publisher-loc>Cambridge (Massachusetts)</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://leon.bottou.org/papers/lskm-2007" xlink:type="simple">http://leon.bottou.org/papers/lskm-2007</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Joachims1"><label>58</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Joachims</surname><given-names>T</given-names></name>
</person-group>             <year>1998</year>             <article-title>Making large-scale support vector machine learning practical.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Burges</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Smola</surname><given-names>A</given-names></name>
</person-group>             <source>Advances in kernel methods: Support vector machines</source>             <publisher-loc>Cambridge (Massachusetts)</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <comment>Chapter 11</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Joachims2"><label>59</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Joachims</surname><given-names>T</given-names></name>
</person-group>             <year>2006</year>             <article-title>Training linear SVMs in linear time.</article-title>             <fpage>217</fpage>             <lpage>226</lpage>             <comment>In: Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Sindhwani1"><label>60</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sindhwani</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Keerthi</surname><given-names>SS</given-names></name>
</person-group>             <year>2006</year>             <article-title>Large scale semi-supervised linear SVMs.</article-title>             <fpage>477</fpage>             <lpage>484</lpage>             <comment>In: Proceedings of the 29th Annual International ACM SI-GIR Conference on Research and Development in Information Retrieval. New York: ACM Press,</comment>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://doi.acm.org/10.1145/1148170.1148253" xlink:type="simple">http://doi.acm.org/10.1145/1148170.1148253</ext-link>. Available: <ext-link ext-link-type="uri" xlink:href="http://portal.acm.org/citation.cfm?id=1148170.1148253" xlink:type="simple">http://portal.acm.org/citation.cfm?id=1148170.1148253</ext-link>. Accessed 16 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Bordes1"><label>61</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bordes</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Ertekin</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Weston</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Bottou</surname><given-names>L</given-names></name>
</person-group>             <year>2005</year>             <article-title>Fast kernel classifiers with online and active learning.</article-title>             <source>J Mach Learn Res</source>             <volume>6</volume>             <fpage>1579</fpage>             <lpage>1619</lpage>          </element-citation></ref>
<ref id="pcbi.1000173-Demsar1"><label>62</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Demsar</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Zupan</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Leban</surname><given-names>G</given-names></name>
</person-group>             <year>2004</year>             <article-title>Orange: From experimental machine learning to interactive data mining.</article-title>             <comment>Faculty of Computer and Information Science, University of Ljubljana. <ext-link ext-link-type="uri" xlink:href="http://www.ailab.si/orange" xlink:type="simple">http://www.ailab.si/orange</ext-link></comment>          </element-citation></ref>
<ref id="pcbi.1000173-3"><label>63</label><element-citation publication-type="other" xlink:type="simple">             <article-title><italic>The Spider</italic> toolbox.</article-title>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://www.kyb.tuebingen.mpg.de/bs/people/spider" xlink:type="simple">http://www.kyb.tuebingen.mpg.de/bs/people/spider</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Gawande1"><label>64</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gawande</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Webers</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Smola</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Vishwanathan</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Gunter</surname><given-names>S</given-names></name>
<etal/></person-group>             <year>2007</year>             <article-title>ELEFANT user manual (revision 0.1).</article-title>             <comment>Technical report. NICTA. Available: <ext-link ext-link-type="uri" xlink:href="http://elefant.developer.nicta.com.au" xlink:type="simple">http://elefant.developer.nicta.com.au</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-4"><label>65</label><element-citation publication-type="other" xlink:type="simple">             <article-title><italic>Plearn</italic> toolbox.</article-title>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://plearn.berlios.de/" xlink:type="simple">http://plearn.berlios.de/</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Witten1"><label>66</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Witten</surname><given-names>IH</given-names></name>
<name name-style="western"><surname>Frank</surname><given-names>E</given-names></name>
</person-group>             <year>2005</year>             <source>Data mining: Practical machine learning tools and techniques. 2nd edition.</source>             <publisher-name>Morgan Kaufmann</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000173-Bottou2"><label>67</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bottou</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Cun</surname><given-names>YL</given-names></name>
</person-group>             <year>2002</year>             <article-title>Lush reference manual.</article-title>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://lush.sourceforge.net" xlink:type="simple">http://lush.sourceforge.net</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Sonnenburg3"><label>68</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sonnenburg</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Rätsch</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Schäfer</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name>
</person-group>             <year>2006</year>             <article-title>Large scale multiple kernel learning.</article-title>             <source>J Mach Learn Res</source>             <volume>7</volume>             <fpage>1531</fpage>             <lpage>1565</lpage>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://jmlr.csail.mit.edu/papers/v7/sonnenburg06a.html" xlink:type="simple">http://jmlr.csail.mit.edu/papers/v7/sonnenburg06a.html</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Mierswa1"><label>69</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mierswa</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Wurst</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Klinkenberg</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Scholz</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Euler</surname><given-names>T</given-names></name>
</person-group>             <year>2006</year>             <article-title>YALE: Rapid prototyping for complex data mining tasks.</article-title>             <comment>In: Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Available: <ext-link ext-link-type="uri" xlink:href="http://portal.acm.org/citation.cfm?id=1276958.1277323" xlink:type="simple">http://portal.acm.org/citation.cfm?id=1276958.1277323</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-5"><label>70</label><element-citation publication-type="other" xlink:type="simple">             <article-title><italic>PyML</italic> toolbox.</article-title>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://pyml.sourceforge.net" xlink:type="simple">http://pyml.sourceforge.net</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Sonnenburg4"><label>71</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sonnenburg</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Braun</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Ong</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Bengio S Bottou</surname><given-names>L</given-names></name>
<etal/></person-group>             <year>2007</year>             <article-title>The need for open source software in machine learning.</article-title>             <source>J Mach Learn Res</source>             <volume>8</volume>             <fpage>2443</fpage>             <lpage>2466</lpage>             <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://jmlr.csail.mit.edu/papers/v8/sonnenburg07a.html" xlink:type="simple">http://jmlr.csail.mit.edu/papers/v8/sonnenburg07a.html</ext-link>. Accessed 11 August 2008</comment>          </element-citation></ref>
<ref id="pcbi.1000173-Schneider1"><label>72</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schneider</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Stephens</surname><given-names>R</given-names></name>
</person-group>             <year>1990</year>             <article-title>Sequence logos: A new way to display consensus sequences.</article-title>             <source>Nucleic Acids Res</source>             <volume>18</volume>          </element-citation></ref>
<ref id="pcbi.1000173-Crooks1"><label>73</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Crooks</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Hon</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Chandonia</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Brenner</surname><given-names>S</given-names></name>
</person-group>             <year>2004</year>             <article-title>Weblogo: A sequence logo generator.</article-title>             <source>Genome Res</source>             <volume>14</volume>             <fpage>1188</fpage>             <lpage>1190</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>