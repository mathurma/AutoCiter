<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005338</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-00445</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Hearing</subject><subj-group><subject>Pitch perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Hearing</subject><subj-group><subject>Pitch perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Hearing</subject><subj-group><subject>Pitch perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Ears</subject><subj-group><subject>Inner ear</subject><subj-group><subject>Cochlea</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Ears</subject><subj-group><subject>Inner ear</subject><subj-group><subject>Cochlea</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Psychophysics</subject><subj-group><subject>Psychoacoustics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Psychophysics</subject><subj-group><subject>Psychoacoustics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject><subj-group><subject>Psychoacoustics</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject><subj-group><subject>Psychoacoustics</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject><subj-group><subject>Psychoacoustics</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Auditory system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A New Approach to Model Pitch Perception Using Sparse Coding</article-title>
<alt-title alt-title-type="running-head">A New Approach to Model Pitch Perception Using Sparse Coding</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4855-9519</contrib-id>
<name name-style="western">
<surname>Barzelay</surname>
<given-names>Oded</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Furst</surname>
<given-names>Miriam</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Barak</surname>
<given-names>Omri</given-names>
</name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>School of Electrical Engineering, Faculty of Engineering, Tel-Aviv University, Tel Aviv, Israel</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Rappaport Faculty of Medicine, Network Biology Research Laboratories, Technion, Haifa, Israel</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bethge</surname>
<given-names>Matthias</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Tübingen and Max Planck Institute for Biologial Cybernetics, GERMANY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"> <list-item><p><bold>Conceived and designed the experiments:</bold> OdB OmB MF.</p></list-item> <list-item><p><bold>Performed the experiments:</bold> OdB.</p></list-item> <list-item><p><bold>Analyzed the data:</bold> OdB.</p></list-item> <list-item><p><bold>Wrote the paper:</bold> OdB MF OmB.</p></list-item></list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">odedbarz@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>18</day>
<month>1</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<month>1</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>1</issue>
<elocation-id>e1005338</elocation-id>
<history>
<date date-type="received">
<day>20</day>
<month>3</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>31</day>
<month>12</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Barzelay et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005338"/>
<abstract>
<p>Our acoustical environment abounds with repetitive sounds, some of which are related to pitch perception. It is still unknown how the auditory system, in processing these sounds, relates a physical stimulus and its percept. Since, in mammals, all auditory stimuli are conveyed into the nervous system through the auditory nerve (AN) fibers, a model should explain the perception of pitch as a function of this particular input. However, pitch perception is invariant to certain features of the physical stimulus. For example, a missing fundamental stimulus with resolved or unresolved harmonics, or a low and high-level amplitude stimulus with the same spectral content–these all give rise to the same percept of pitch. In contrast, the AN representations for these different stimuli are not invariant to these effects. In fact, due to saturation and non-linearity of both cochlear and inner hair cells responses, these differences are enhanced by the AN fibers. Thus there is a difficulty in explaining how pitch percept arises from the activity of the AN fibers. We introduce a novel approach for extracting pitch cues from the AN population activity for a given arbitrary stimulus. The method is based on a technique known as sparse coding (SC). It is the representation of pitch cues by a few spatiotemporal atoms (templates) from among a large set of possible ones (a dictionary). The amount of activity of each atom is represented by a non-zero coefficient, analogous to an active neuron. Such a technique has been successfully applied to other modalities, particularly vision. The model is composed of a cochlear model, an SC processing unit, and a harmonic sieve. We show that the model copes with different pitch phenomena: extracting resolved and non-resolved harmonics, missing fundamental pitches, stimuli with both high and low amplitudes, iterated rippled noises, and recorded musical instruments.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>By means of a sound's pitch, we can easily discern between low and high musical notes, regardless of whether they originate from a guitar, piano or a vocalist. The relation between different sounds that yield the same percept is what makes pitch an interesting subject of research. Today, despite extensive research, the mechanism behind this physical to perceptual transformation is still unclear. The large dynamic range of the cochlea combined with its nonlinear nature makes the modeling and understanding of this process a challenging task. Given a large amount of physiological and psychological data, a general explanation consistent with many of these phenomena would be a major step in elucidating the nature of pitch perception. In this paper, we recast the problem in the general framework of sparse coding of sensory stimuli. This framework, initially developed for the visual modality, posits that the goal of the neural representation is to represent the flow of sensory information in a concise and parsimonious way. We show that applying this principle to the problem of pitch perception can explain many perceptual phenomena.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100003977</institution-id>
<institution>Israel Science Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>Grant no. 563/12 (M.F.), ERC FP7 CIG 2013-618543</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4855-9519</contrib-id>
<name name-style="western">
<surname>Barzelay</surname>
<given-names>Oded</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>Fondation Adelis</institution>
</funding-source>
<principal-award-recipient>
<name name-style="western">
<surname>Barak</surname>
<given-names>Omri</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This study was partially supported by the Israel Science Foundation Grant no. 563/12 (MF), <ext-link ext-link-type="uri" xlink:href="https://www.isf.org.il/" xlink:type="simple">https://www.isf.org.il/</ext-link> ERC FP7 CIG 2013-618543 and by Fondation Adelis (OmB). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="15"/>
<table-count count="0"/>
<page-count count="36"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2017-02-14</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The model is available at <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/odedbarz/sc_pitch" xlink:type="simple">https://bitbucket.org/odedbarz/sc_pitch</ext-link></meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The perception of pitch is an important feature of speech recognition and perception of musical melodies. It conveys information of prosody and speaker identity; it helps in grouping different tones into one auditory object; and it conveys information about melody and harmony. The sensation of a pitch, usually associated with the periodicity of a given physical stimulus, is usually perceived as having two dimensions: pitch class and pitch height. The pitch class, or the pitch chroma, is the set of all pitches that are related by whole octave numbers and is known in musical theory as "octave equivalence"; the pitch height is the continuum perception of sound from low to high. The percept of pitch is so inherent in us that usually even a slight repetition in time is needed to create it. When dealing with harmonic signals, pitch is usually related to the first harmonic, the fundamental frequency, of that signal. Even though most natural sounds are not strictly periodic, pitch is still clearly perceived and used by the brain in various hearing-related tasks. A unique property of pitch perception is that it is a many-to-many mapping: a similar pitch can be perceived by different acoustic stimuli, and a given acoustic stimulus can yield different percepts of pitch. This property is the reason that makes pitch an interesting property of the mind, but it is also the reason that makes it hard to explain. The question arises: How does a brain manage to perform this task?</p>
<p>For almost a century, pitch properties have been extensively researched both experimentally and theoretically. Generally, most of the existing models that have emerged from this research activity can be divided into two main categories: (1) temporal models and (2) spectral models [<xref ref-type="bibr" rid="pcbi.1005338.ref001">1</xref>]. Modern temporal models, which are currently regarded as prominent, are usually based on autocorrelation principles [<xref ref-type="bibr" rid="pcbi.1005338.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref004">4</xref>]. These models rely on the fact that periodic stimuli, with the same perceived pitch but with possibly different spectral harmonic content, have the same temporal periodic response. For example, a signal consisting of the first six consecutive harmonics will have the same temporal period as a signal that contains just three successive harmonics. Thus, both signals are likely to reveal the same perception of pitch. The predictions of the temporal models are consistent with a large number of psychoacoustic properties of pitch perception, including: (a) the missing fundamental case, also known as virtual pitch, which is the pitch of a harmonic series that does not include its fundamental frequency; (b) the pitch shift effect, which is the perception of a signal with shifted, equally spaced, harmonic components that yield ambiguous pitches [<xref ref-type="bibr" rid="pcbi.1005338.ref005">5</xref>]; and (c) the invariance to the stimuli amplitude levels, which is an inherent property of the autocorrelation process, in accordance with psychophysical measurements [<xref ref-type="bibr" rid="pcbi.1005338.ref006">6</xref>]. There is also neurophysiological evidence for reliably predicting pitches for different stimuli [<xref ref-type="bibr" rid="pcbi.1005338.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref008">8</xref>] based on calculations of the autocorrelation of a cat’s AN population response⁠. On the other hand, it seems that temporal models perform too well compared to human psychophysics. Consider for example the case of resolved and unresolved stimuli. Low harmonics are known to be resolved, meaning they are transformed into distinct rate activity within AN fibers and with distinct peaks at certain CFs. On the other hand, higher harmonics, approximately the 5<sup>th</sup> to 10<sup>th</sup> and above [<xref ref-type="bibr" rid="pcbi.1005338.ref009">9</xref>], are unresolved in the sense that having these harmonics in the same stimulus they share the same spatial area along the cochlea. Temporal models cope well with both types of stimuli [<xref ref-type="bibr" rid="pcbi.1005338.ref010">10</xref>] because autocorrelation accounts for the interaction between the different harmonic components of the signal. However, previous measurements have shown that stimuli composed of resolved (low) harmonics are usually more salient than stimuli that are composed of unresolved (high) harmonics [<xref ref-type="bibr" rid="pcbi.1005338.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref012">12</xref>]. Another example of the excessive performance of these temporal models over human performance is the case of the transposed tones [<xref ref-type="bibr" rid="pcbi.1005338.ref013">13</xref>]. Transposed tones of low harmonic stimuli are designed to have, using modulation, the same auditory peripheral representation as their low harmonic counterparts. These experiments suggest that temporal information alone is not sufficient and that tonotopic organization must be considered [<xref ref-type="bibr" rid="pcbi.1005338.ref014">14</xref>]⁠. Finally, temporal models require certain physiological structures in the auditory neural pathway to work. In particular, the autocorrelation functionality requires the existence of (at least) 40ms long tapped delay lines [<xref ref-type="bibr" rid="pcbi.1005338.ref015">15</xref>]. But at present there is currently no physiological evidence to support such mechanism.</p>
<p>A second major class of models is the spectral theory for pitch. These models are based on the tonotopic organization, or mapping, from stimulus frequencies to stimulated spatial locations along the cochlea; high frequencies resonate the basal parts of the cochlea while low frequencies resonate its apical parts. These vibrations are transduced into the auditory system through the innervation of the auditory nerve (AN) fibers. The spatial arrangement of these ANs along the cochlea means that each of these neurons is most responsive to a specific frequency, which is denoted as its characteristic frequency (CF). Spectral models exploit this mapping to extract the frequency components of the incoming stimuli. A prominent implementation of these models is the class of pattern-matching models [<xref ref-type="bibr" rid="pcbi.1005338.ref016">16</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref018">18</xref>]. The overall structure shared by these types of models is composed of two main phases: the first extracts the spectral components of the stimulus from the AN population activity, and the second matches the resulting spectral pattern with the model's existing templates. Each of these templates is indexed to match different pitches, and a percept of a particular pitch is the best probable match between a given stimulus and a certain template.</p>
<p>Similarly to the temporal models, the predictions of these models are also consistent with a large number of psychoacoustic properties of pitch [<xref ref-type="bibr" rid="pcbi.1005338.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref022">22</xref>]. However, there are also psychoacoustic phenomena that are difficult to explain within this framework. One main disadvantage of these models is their inability to infer pitches that are composed of high harmonic components. As mentioned above, the cochlea decomposes the stimulus’ frequencies into spatial locations, which are represented by the CFs, and not all harmonics transduce into auditory activities in the same way. As a result, spectral models cannot easily account for pitches of unresolved stimuli.</p>
<p>Common to these models is the use of the AN population response to extract features from a given stimulus. These features are then translated into a scalar that represents the pitch percept. The problem of feature extraction from input stimuli has been studied in more general settings [<xref ref-type="bibr" rid="pcbi.1005338.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref024">24</xref>], and it is instructive to consider this approach in a wider sense. The general task of all modalities is the need to process streams of incoming sensed data abundant with information, and to extract desired low dimensional properties from it. For example, in the visual system a low dimensional percept of an object’s orientation is extracted from the activities of photoreceptors in the retina at the time of the stimulus (high dimensional input). Likewise, in the case of the auditory system, the input signal is a continuous auditory stream. It is composed of the spiking activity of approximately 30,000 ANFs in a healthy human adult and lasts for the duration of the whole stimulus. Yet, the auditory system usually extracts relatively low-dimensional and slow changing features such as the pitch of that signal. Namely, the perception of the pitch height and the pitch class is represented by just two dimensions as opposed to about 30,000 dimensions for each ANF that changes over time.</p>
<p>Today there are well developed and closely related mathematical frameworks that specialize in feature extraction. Specifically, we refer to a family of algorithms known as <italic>sparse coding</italic> (SC). This mathematical technique has many applications displaying a wide variety of variants and flavors [<xref ref-type="bibr" rid="pcbi.1005338.ref025">25</xref>]. Additionally, it seems that the SC approach is in accordance with the known physiology of the central nervous system, that of the auditory system and of other physiological modalities [<xref ref-type="bibr" rid="pcbi.1005338.ref023">23</xref>].</p>
<p>In this paper we apply a SC algorithm to predict pitches for different signals. We apply it to the AN population responses taken from simulations of known cochlear models [<xref ref-type="bibr" rid="pcbi.1005338.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref029">29</xref>]. The SC algorithm concurrently uses both the spatial and temporal domain. In this sense, the proposed model is a hybrid of the “classic” temporal and the spectral pitch models mentioned above. We show that this type of model can predict a variety of psychoacoustic properties of pitch. Specifically, the model can infer the pitches of missing fundamental complex tones; it exhibits the psychoacoustic phenomena of pitch shifts; and it is invariant to stimuli levels. Our results suggest that the principle of sparse coding can explain relatively high perception functionality such as pitch.</p>
<sec id="sec002">
<title>Model Overview</title>
<p>The proposed model consists of three main parts: (i) a cochlear model that translates auditory stimulus, s<sub>in</sub>(t), into the AN population activity, <bold>S</bold><sub>AN</sub>(t,f<sub>CF</sub>) (<xref ref-type="fig" rid="pcbi.1005338.g001">Fig 1A</xref>); (ii) a sparse coding (SC) unit that represents the AN population response as a sparse (few non-zeros) set of coefficients, <bold>h</bold>. (<xref ref-type="fig" rid="pcbi.1005338.g001">Fig 1B</xref>); (iii) a readout unit that translates the active coefficients in <bold>h</bold> into a probability density function <italic>pdf</italic>(<italic>f</italic><sub><italic>p</italic></sub>) of pitches. To compare the model with known psychoacoustic phenomena, we set the estimated stimulus’ pitch to be the maximum value of that pdf function, s<sub>p</sub> ∈ <italic>R</italic><sup>1</sup> (<xref ref-type="fig" rid="pcbi.1005338.g001">Fig 1C</xref>). This scalar represents the most plausible pitch for the particular given input signal, s<sub>in</sub>(t). Each part is briefly described in the following sections.</p>
<fig id="pcbi.1005338.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The model.</title>
<p>The model is composed of three main sections: <bold>(A)</bold> The cochlear model [<xref ref-type="bibr" rid="pcbi.1005338.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref029">29</xref>] transduces a one-dimensional input stimulus, s<sub>in</sub>(t), into a two-dimensional matrix that represents the AN population response, <bold>S</bold><sub>AN</sub>(t,f<sub>CF</sub>). <bold>(B)</bold> The AN’s spatiotemporal response is introduced into the sparse coding (SC) block to produce the sparse coefficient vector, <bold>h</bold>. The vector <bold>h</bold> carries invariant information of the input stimulus that we refer to as pitch cues. The (sparse) information in <bold>h</bold> represents harmonics in s<sub>in</sub>(t). <bold>(C)</bold> Finally, the likelihood probability of the pitch given the vector <bold>h</bold> is extracted and denoted as <italic>pdf</italic>(<italic>f</italic><sub><italic>p</italic></sub>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g001" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>The Cochlear Model</title>
<p>For low and medium sound levels, each location along the BM represents a specific frequency, the CF of this location. We denote these frequencies by f<sub>CF</sub>. The cochlear models that we use in this paper [<xref ref-type="bibr" rid="pcbi.1005338.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref029">29</xref>] transduce a stimulus, <inline-formula id="pcbi.1005338.e001"><alternatives><graphic id="pcbi.1005338.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:msub><mml:mtext>s</mml:mtext><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mtext>t</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mtext>T</mml:mtext><mml:mtext>a</mml:mtext></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, into instantaneous rates of the ANs, <bold>S</bold><sub>AN</sub>(t,f<sub>CF</sub>) ∈ <italic>R</italic><sup>T×N</sup>. Accordingly, T<sub>a</sub> is the total length of the stimulus in samples; T is a segment out of T<sub>a</sub> for the cochlear response; and N is the number of the CF channels in the cochlear model. Throughout this paper, regardless of the cochlear model that we use, we always set T = 5ms; this 5ms interval is taken from the end of the total T<sub>a</sub> = 15ms cochlear simulation.</p>
<p><xref ref-type="fig" rid="pcbi.1005338.g002">Fig 2B</xref> shows the cochlear model [<xref ref-type="bibr" rid="pcbi.1005338.ref026">26</xref>] responses <bold>S</bold><sub>AN</sub>(t,f<sub>CF</sub>) for the following three stimuli</p>
<fig id="pcbi.1005338.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Different complex harmonic stimuli with the same pitch.</title>
<p>(A) The Fourier transform (FT) of three complex harmonics stimuli with a fundamental frequency of f<sub>0</sub> = 240 Hz. The three signals have different spectral components: <bold>S</bold><sub>in,1</sub>(f) is composed of the first harmonic component of f<sub>0</sub>; <bold>S</bold><sub>in,2</sub>(f) consists of the first four successive harmonics of f<sub>0</sub>; and <bold>S</bold><sub>in,3</sub>(f) is formed from of the 10–13 harmonics. (B) The corresponding output of the cochlear model [<xref ref-type="bibr" rid="pcbi.1005338.ref026">26</xref>], i.e., the AN population responses for the three stimuli. The y-axis represents the normalized characteristic frequencies (CFs), which is CF divided by f<sub>0</sub>, on a linear scale, and the x-axis shows the post-stimulus time in milliseconds. The cochlear input is a 15ms long stimulus, and the resulting output is taken from the last 5ms. Note the different patterns of the AN activities that correspond to the three different cases: a stimulus with low frequencies excites the apical parts of the cochlea (lower part in the images), while a stimulus with higher frequencies excites the basal parts. Note also that the AN population responses define unique spatiotemporal patterns of activities for each of the stimuli. All the three stimuli have relatively low sound levels (30 dB SPL), which means that the cochlea response is linear.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g002" xlink:type="simple"/>
</fig>
<disp-formula id="pcbi.1005338.e002">
<alternatives>
<graphic id="pcbi.1005338.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>30</mml:mn><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">π</mml:mi><mml:mtext>kf</mml:mtext></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mtext>t</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>30</mml:mn><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:munderover><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">π</mml:mi><mml:mtext>kf</mml:mtext></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mtext>t</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>30</mml:mn><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>13</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">π</mml:mi><mml:mtext>kf</mml:mtext></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mtext>t</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
<p>The parameter f<sub>0</sub> = 240 Hz is the fundamental frequency of the given harmonic series. The amplitude of each of the stimuli, g<sub>30dB</sub>, is given in Pascals and is equivalent to 30 dB SPL in this case. <xref ref-type="fig" rid="pcbi.1005338.g002">Fig 2A</xref> represents the Fourier transform (FT) of these three stimuli, <bold>S</bold><sub>in,r</sub>(f) = FT{s<sub>in,r</sub>(t)}, which denote the three different cases for r ∈ {1, 2, 3}, respectively. The first stimulus, <bold>S</bold><sub>in,1</sub>(f), consists of the first fundamental component, f<sub>0</sub>. The second stimulus, <bold>S</bold><sub>in,2</sub>(f), includes the first four successive harmonics of f<sub>0</sub>, and <bold>S</bold><sub>in,3</sub>(f) contains the 10<sup>th</sup> to 13<sup>th</sup> harmonics of f<sub>0.</sub> Note that although these stimuli sound differently, it is known [<xref ref-type="bibr" rid="pcbi.1005338.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref030">30</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref031">31</xref>] that these stimuli, especially the first and second ones, yield the same percept of pitch, namely f<sub>0</sub>.</p>
<p>The normalized AN population responses, <bold>S</bold><sub>AN,r</sub>(t,f<sub>CF</sub>), of all the three stimuli, r ∈ {1, 2, 3}, are depicted in <xref ref-type="fig" rid="pcbi.1005338.g002">Fig 2B</xref>. The matrix <bold>S</bold><sub>AN,r</sub>(t,f<sub>CF</sub>) is presented in the figure as a color-coded image, where the x-axis represents the post-stimulus time (10ms to 15ms), and the y-axis represents the normalized CFs relative to the stimuli’s fundamental frequency, i.e., f<sub>CF</sub> /f<sub>0</sub>. It is clear from <xref ref-type="fig" rid="pcbi.1005338.g002">Fig 2B</xref> that the AN population responses depend on the spectrum of the input signal. For instance, the <bold>S</bold><sub>AN,1</sub>(t,f<sub>CF</sub>), that corresponds to the 240 Hz sine wave, shows local AN activities only in lower CFs (at the apical part of the cochlea). On the other hand, the <bold>S</bold><sub>AN,3</sub>(t,f<sub>CF</sub>), which is composed of the 10–13 harmonics of f<sub>0</sub>, yields activity at higher CFs (towards the basal part of the cochlea).</p>
<p>It is noteworthy that each frequency component in the auditory stimulus reflects the ANs activity of specific location along the cochlea. Hence, each of these AN population responses has its own spatiotemporal typical pattern.</p>
</sec>
<sec id="sec004">
<title>The Sparse Coding Phase</title>
<p>Next, we wish to exploit the unique aforementioned spatiotemporal structures of the AN population responses. Given the AN response to a specific stimulus, we wish to represent it as a weighted sum of a small number of response primitives using the following optimization:
<disp-formula id="pcbi.1005338.e003">
<alternatives>
<graphic id="pcbi.1005338.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mrow><mml:mi mathvariant="normal">arg</mml:mi><mml:mspace width="0.25em"/><mml:munder><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:munder><mml:mspace width="0.25em"/><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mrow><mml:mtext>AN</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msubsup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>1</mml:mn><mml:mrow/></mml:msubsup></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
Where the operator ∥∙∥<sub>2</sub> specifies the Euclidian norm, ∥⋅∥<sub>1</sub> is the ℓ<sub>1</sub>-norm. The vector <bold>v</bold><sub>AN</sub> ∈ <italic>R</italic><sup><italic>T</italic>∙<italic>N</italic></sup> is the vectorized AN response; the matrix <bold>D</bold> ∈ <italic>R</italic><sup><italic>T</italic>∙<italic>N</italic>×<italic>M</italic></sup> is a collection of M primitives known as the dictionary; the vector <bold>h</bold> ∈ <italic>R</italic><sup><italic>M</italic></sup> is the (sparse) coefficient vector; and <italic>λ</italic> is a scalar that controls the sparseness of the solution <bold>h</bold>. Note that the entries in <bold>h</bold> assign weights to different atoms in <bold>D</bold>, and thus we have <italic>h</italic>[<italic>k</italic>] ≥ 0 for all <italic>k</italic> ∈ [1,<italic>M</italic>]. There are various numerical techniques [<xref ref-type="bibr" rid="pcbi.1005338.ref025">25</xref>] to solve <xref ref-type="disp-formula" rid="pcbi.1005338.e003">Eq 2</xref> for <bold>h</bold>; here we chose to use LASSO, a linear regression with ℓ<sub>1</sub>-norm regularization [<xref ref-type="bibr" rid="pcbi.1005338.ref032">32</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref033">33</xref>].</p>
<p>The dictionary <bold>D</bold> can either be learned from examples [<xref ref-type="bibr" rid="pcbi.1005338.ref024">24</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref037">37</xref>], or chosen according to some prior knowledge (see for example [<xref ref-type="bibr" rid="pcbi.1005338.ref025">25</xref>], Ch. 12). In this paper, we opt for the latter option but will also explore the effect of a different dictionary later on. Thus, we chose those primitives, which within the SC paradigm are known as atoms, to be the AN population response to pure sine waves,
<disp-formula id="pcbi.1005338.e004">
<alternatives>
<graphic id="pcbi.1005338.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>30</mml:mn><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">π</mml:mi><mml:mtext>f</mml:mtext></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mtext>t</mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula></p>
<p>In this equation, <italic>d</italic> ∈ [1,<italic>gM</italic>] is the index of the atom, each one is created by its own <italic>s</italic><sub><italic>d</italic></sub>(<italic>t</italic>) stimulus; the parameter f<sub><italic>d</italic></sub> ∈ [100 <italic>Hz</italic>, 20<italic>k Hz</italic>] is the frequency that this particular atom represents. When solving for <xref ref-type="disp-formula" rid="pcbi.1005338.e003">Eq 2</xref>, we may need the dictionary to account for different phases of an incoming stimulus. One simple way to achieve that is to use groups <italic>g</italic> ≥ 1 of atoms with the same frequencies f<sub><italic>d</italic></sub> but with different phases <italic>ϕ</italic><sub><italic>g</italic></sub> = 2<italic>π</italic>/<italic>g</italic> ⋅ <italic>k</italic>, <italic>k</italic> ∈ [0,<italic>g</italic>−1]. This technique can be seen as a simplified variant of the group-lasso algorithms [<xref ref-type="bibr" rid="pcbi.1005338.ref038">38</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref039">39</xref>]. Solving for <xref ref-type="disp-formula" rid="pcbi.1005338.e003">Eq 2</xref>, the solution of the sparse coefficient vector <inline-formula id="pcbi.1005338.e005"><alternatives><graphic id="pcbi.1005338.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> contains entries that belong to the same group (same frequency, different phases). These entries represent the same frequency and should not “compete”. Thus, the entries of each group are summed together, forming the final sparse vector <bold>h</bold> ∈ <italic>R</italic><sup><italic>M</italic></sup>. In the following, we would assume that <italic>g</italic> = 1 for simplicity (i.e., each atom is a group that is created by a sine with zero phase). We would use the extended scheme (<italic>g</italic> &gt; 1) when dealing with stimuli of random or unknown phase (see Iterated Rippled Noise and Musical Notes).</p>
<p><xref ref-type="fig" rid="pcbi.1005338.g003">Fig 3A</xref> shows an example of an atom <bold>d</bold> ∈ <italic>R</italic><sup><italic>T</italic>⋅<italic>N</italic></sup>, <italic>g</italic> = 1, that corresponds to a sinusoid stimulus of about <italic>f</italic><sub><italic>d</italic></sub> = 1.5<italic>k Hz</italic>. <xref ref-type="fig" rid="pcbi.1005338.g003">Fig 3B</xref> shows a typical dictionary that is a concatenation of M such atoms, i.e., <bold>D</bold> = [<bold>d</bold><sub>1</sub>,…,<bold>d</bold><sub><italic>M</italic></sub>]. As mentioned, the scalar <italic>λ</italic> = 0.01 in <xref ref-type="disp-formula" rid="pcbi.1005338.e003">Eq 2</xref> determines the sparseness of the solution <bold>h</bold>. On the one hand, increasing <italic>λ</italic> assigns more weight to the ℓ<sub>1</sub>-norm (the second term in <xref ref-type="disp-formula" rid="pcbi.1005338.e003">Eq 2</xref>) and so leads to a sparser solution for <bold>h</bold> (i.e., more components in <bold>h</bold> are set to zero). But this sparseness comes at the expense of the matching between <bold>v</bold><sub>AN</sub> and <bold>Dh</bold> (the first term in <xref ref-type="disp-formula" rid="pcbi.1005338.e003">Eq 2</xref>) and for a large enough <italic>λ</italic> the solution becomes trivial, <bold>h</bold> = 0. On the other hand, setting <italic>λ</italic> = 0.0 usually leads to a non-sparse solution, that is, most of the entries in <bold>h</bold> are nonzero. With regard to this observation, the solution of <xref ref-type="disp-formula" rid="pcbi.1005338.e003">Eq 2</xref> decreases to an ordinary least-square (LS) solution, without any sparseness considerations.</p>
<fig id="pcbi.1005338.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Composing the dictionary D.</title>
<p><bold>(A)</bold> An example of one atom in <bold>D</bold>. It is the AN population response to a sine wave of 1.5k Hz tone generated by the cochlear model [<xref ref-type="bibr" rid="pcbi.1005338.ref026">26</xref>]. The atom was normalized to a peak value of 1, as for all other population responses. The y-axis of the two-dimensional matrix represents the CF along the BM, and the x-axis is the post-stimulus time in milliseconds. <bold>(B)</bold> Each of the atoms, <bold>d</bold><sub>j</sub>, j ∈ [1, M], is vectorized into a column in <bold>D</bold>. These M columns are concatenated to form the dictionary matrix <bold>D</bold>. All the input signals used for the creation of the dictionary have the same level of 30 dB SPL (i.e., at the cochlear linear region). In this example, we used only one atom per group (<italic>g</italic> = 1).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g003" xlink:type="simple"/>
</fig>
<p>As a simplified example, consider the AN population response of the second stimulus case (<xref ref-type="fig" rid="pcbi.1005338.g002">Fig 2B</xref>). Solving for <bold>h</bold>, the AN population response is equivalent to a linear combination of just four atoms in <bold>D</bold>, that is <bold>v</bold><sub><italic>AN</italic></sub> ≈ 0.05 ⋅ <bold>d</bold><sub>5</sub> + 0.31 ⋅ <bold>d</bold><sub><bold>1</bold>5</sub> + 0.46 ⋅ <bold>d</bold><sub><bold>2</bold>5</sub> + 0.82 ⋅ <bold>d</bold><sub><bold>3</bold>5</sub> (<xref ref-type="fig" rid="pcbi.1005338.g004">Fig 4B</xref>, green circles in <bold>h</bold><sub>2</sub>). Solving for <xref ref-type="disp-formula" rid="pcbi.1005338.e003">Eq 2</xref> for the other three signals, we get the solutions of <bold>h</bold><sub>1</sub>, <bold>h</bold><sub>2</sub>, and <bold>h</bold><sub>3</sub>, respectively (<xref ref-type="fig" rid="pcbi.1005338.g004">Fig 4B</xref>). Note that there is a clear similarity between the FT of the input signals (<xref ref-type="fig" rid="pcbi.1005338.g002">Fig 2A</xref>) and the derived coefficients of <bold>h</bold><sub>1</sub>, <bold>h</bold><sub>2</sub>, and <bold>h</bold><sub>3</sub>. Explicitly, the spectral structures of the stimuli are reconstructed by the SC algorithm, yet one can see that the three expressions are not alike. For example, there is a difference between the magnitudes of the FT coefficients of s<sub>in,2</sub>(t) and the nonzero entries in <bold>h</bold><sub>2</sub>, meaning that the SC decomposition is not in general an FT decomposition.</p>
<fig id="pcbi.1005338.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g004</object-id>
<label>Fig 4</label>
<caption>
<title>The sparse coefficient vector h and the final pitch probability vector.</title>
<p><bold>(A)</bold> A simplified view of the SC methodology. The algorithm decomposed the two-dimensional signal <bold>S</bold><sub>AN,2</sub>(t,f<sub>CF</sub>) into a linear combination of four atoms (columns) within <bold>D</bold>. This is a simplified view that shows the primary values in <bold>h</bold><sub>2</sub> (green indices) multiplied by the atoms. <bold>(B)</bold> The sparse coefficient solution vectors, <bold>h</bold><sub>k</sub>, for the three cases (k ∈ [<xref ref-type="bibr" rid="pcbi.1005338.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref003">3</xref>]). The green circles in the figure of <bold>h</bold><sub>2</sub> correspond to the four terms in the simplified example of (A). All x-axes are normalized by the fundamental frequency f<sub>0</sub> = 240 Hz for convenience. Observe that the solutions for <bold>h</bold><sub>k</sub> resemble that of the FT for the respective stimuli (<xref ref-type="fig" rid="pcbi.1005338.g002"><bold>Fig 2</bold>A</xref>). <bold>(C)</bold> Using the pitch estimation unit (harmonic sieve), we can easily map the information in <bold>h</bold><sub>k</sub>, for k ∈ [<xref ref-type="bibr" rid="pcbi.1005338.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref003">3</xref>], into a pitch probability vector, <italic>pdf</italic>(f<sub>p</sub>). Each of the y-axes of the pdfs functions is multiplied by a constant (x100) for visual clearance. The red arrows indicate the locations of the maximum peaks, all of which are shown to occur at the fundamental harmonic. In other words, it is most probable that all three stimuli represent the same pitch. Still, note that other options are also plausible, especially in rational ratios of f<sub>0</sub>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Pitch Estimation</title>
<p>The output of the SC stage is a sparse vector <bold>h</bold> (<xref ref-type="fig" rid="pcbi.1005338.g004">Fig 4B</xref>) that represents the weights of each atom in the dictionary <bold>D</bold>. We wish to relate a single pitch percept for each such vector to facilitate a comparison with human psychophysics' tests. We do this by computing the likelihood for each possible pitch [<xref ref-type="bibr" rid="pcbi.1005338.ref016">16</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref040">40</xref>], by assuming a generative model of a harmonic series for each pitch. The resulting likelihood function is a normalized product of the vector <bold>h</bold> with a template for the specific pitch in question:
<disp-formula id="pcbi.1005338.e006">
<alternatives>
<graphic id="pcbi.1005338.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mrow><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">G</mml:mi><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mi mathvariant="bold">G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
where <inline-formula id="pcbi.1005338.e007"><alternatives><graphic id="pcbi.1005338.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mover accent="true"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is an interpolated version of the vector <bold>h</bold> (see <xref ref-type="sec" rid="sec021">Methods</xref>); each row of the matrix <bold>G</bold> (<italic>f</italic><sub><italic>p</italic></sub>,<italic>σ</italic><sub><italic>p</italic></sub>) ∈ <italic>R</italic><sup><italic>P</italic>×<italic>P</italic></sup> contains Gaussian functions centered around the harmonics of the pitch <italic>nf</italic><sub><italic>p</italic></sub> as weights; and <inline-formula id="pcbi.1005338.e008"><alternatives><graphic id="pcbi.1005338.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">G</mml:mi><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the normalization factor of the pdf. The derived probability density functions <italic>pdf</italic><sub><italic>r</italic></sub>(<italic>f</italic><sub><italic>p</italic></sub>), for r ∈ {1, 2, 3}, are depicted in <xref ref-type="fig" rid="pcbi.1005338.g004">Fig 4C</xref>. The three pdfs are aligned, and the x-axis is normalized by the fundamental frequency <italic>f</italic><sub><italic>o</italic></sub> for convenience. The red arrows show that the maximum peaks in each of the three cases are pointing at <italic>f</italic><sub><italic>o</italic></sub>. Note that the maximum peak in the probability distribution is just one option among many. For example, in the first case, in which s<sub>in,1</sub>(t) is composed of just the fundamental spectral component, the <italic>pdf</italic><sub>1</sub>(<italic>f</italic><sub><italic>p</italic></sub>) indicates that other pitches are also possible; specifically, these other options occur at subharmonics of <italic>f</italic><sub><italic>o</italic></sub>. Adding spectral components into the stimulus, i.e., in the second case of <italic>pdf</italic><sub>2</sub>(<italic>f</italic><sub><italic>p</italic></sub>), narrows the width of the peaks. Additionally, more peaks appear at harmonics that are not complete ratios of <italic>f</italic><sub><italic>o</italic></sub>. In the third case of s<sub>in,3</sub>(t), which contains the 10<sup>th</sup>-13<sup>th</sup> harmonics, the probability function is denser. This thickening of the pdf indicates that the third stimulus is perceived as having less salience than the other two (for a detailed treatment of salience, see Resolved and Unresolved Harmonics). Additional peaks are formed (not shown, but see the same effect in Resolved and Unresolved Harmonics) in its probability function, <italic>pdf</italic><sub>3</sub>(<italic>f</italic><sub><italic>p</italic></sub>), around the ‘center-of-mass’ of the spectral components (i.e., the f<sub>locus</sub> around the 10<sup>th</sup>-13<sup>th</sup> harmonics[<xref ref-type="bibr" rid="pcbi.1005338.ref001">1</xref>]).</p>
<p>Finally, a standard paradigm in psychoacoustic experiments is to yield a scalar, i.e., specific pitch or a pitch difference, per a given stimulus. Moreover, it is a known practice in psychoacoustics to restrict the participants (or to modulate the results) to a one octave interval (see for example [<xref ref-type="bibr" rid="pcbi.1005338.ref041">41</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref043">43</xref>]). Thus, the collapsing of the resulting probability function, the <italic>pdf</italic> (<italic>f</italic><sub><italic>p</italic></sub>), into a single scalar, the inferred pitch, is a straightforward and convenient procedure that enables us to compare the model at hand with known psychophysical results. In this paper we defined the estimated pitch for a given stimulus as
<disp-formula id="pcbi.1005338.e009">
<alternatives>
<graphic id="pcbi.1005338.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>o</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula></p>
<p>Additionally, in some of the displayed cases, we follow the convention by limiting the inferred pitch to an octave around the fundamental frequency.</p>
</sec>
</sec>
</sec>
<sec id="sec006" sec-type="results">
<title>Results</title>
<p>Below, we demonstrate the ability of the proposed model to match known psychoacoustic phenomena qualitatively. Using these phenomena, we illustrate how the various components of the model contribute to its performance.</p>
<sec id="sec007">
<title>Why Do We Need Sparse Representation?</title>
<p>To demonstrate the advantage of using sparse coding algorithms, we compared the performance of the algorithm (<xref ref-type="disp-formula" rid="pcbi.1005338.e003">Eq 2</xref>) for sparse (λ = 0.01) and non-sparse solutions (λ = 0, Least squares). The resulting vectors, <bold>h</bold><sub><italic>LS</italic></sub> and <bold>h</bold><sub><italic>SC</italic></sub>, for the two cases are shown in <xref ref-type="fig" rid="pcbi.1005338.g005">Fig 5</xref> for the two aforementioned 30 dB SPL stimuli of <italic>s</italic><sub><italic>in</italic>,2</sub>(<italic>t</italic>) and <italic>s</italic><sub><italic>in</italic>,3</sub>(<italic>t</italic>). In the case of <italic>s</italic><sub><italic>in</italic>,2</sub>(<italic>t</italic>), there is little difference between the two solutions (<xref ref-type="fig" rid="pcbi.1005338.g005">Fig 5A</xref>). For the case of <italic>s</italic><sub><italic>in</italic>,3</sub>(<italic>t</italic>), which includes relatively higher (non-resolved) frequency components, the difference is more substantial (<xref ref-type="fig" rid="pcbi.1005338.g005">Fig 5B</xref>). Still, although the LS solutions (<bold>h</bold><sub><italic>LS</italic></sub>) yield more nonzero coefficients than those of the sparse ones (<bold>h</bold><sub><italic>SC</italic></sub>), the solutions can visibly be related to the harmonic structures of the two input stimuli (compare the resulting <bold>h</bold> vectors with the FTs in <xref ref-type="fig" rid="pcbi.1005338.g002">Fig 2B</xref>).</p>
<fig id="pcbi.1005338.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Comparing LS with SC.</title>
<p><bold>(A)</bold> From left to right: the AN population response for a harmonic complex with the 1<sup>st</sup> – 4<sup>th</sup> harmonics. The y-axis is the CFs normalized by the fundamental frequency, in a linear scale (f<sub>0</sub> = 240 Hz). The x-axis indicates the post-stimulus time (between 10ms to 15ms). Next, the <bold>h</bold> coefficient vectors for the LS case (λ = 0.0) and for the sparse case (λ = 0.01). <bold>(B)</bold> Same as in (A) but for a complex tone stimulus that contains the harmonics 10<sup>th</sup>–13<sup>th</sup>. Note that for the lower harmonic stimulus (A), the results between the two cases, i.e., <bold>h</bold><sub>LS</sub> vs. <bold>h</bold><sub>SC</sub>, are almost identical. On the other hand, for the stimuli with the higher harmonics (B), the difference is more substantial. Specifically, there are much more nonzero coefficients in <bold>h</bold><sub>LS</sub> than in <bold>h</bold><sub>SC</sub> that are unrelated to the original spectrum structure of the signal (compare with the FTs in <xref ref-type="fig" rid="pcbi.1005338.g002"><bold>Fig 2</bold>A</xref>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g005" xlink:type="simple"/>
</fig>
<p>The benefits of the sparse representation are evident when we introduce stimuli with high volume levels. <xref ref-type="fig" rid="pcbi.1005338.g006">Fig 6</xref> compares the processing of a missing fundamental harmonic series with six harmonic components (k ∈ [<xref ref-type="bibr" rid="pcbi.1005338.ref003">3</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref008">8</xref>]),
<disp-formula id="pcbi.1005338.e010">
<alternatives>
<graphic id="pcbi.1005338.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e010" xlink:type="simple"/>
<mml:math display="block" id="M10">
<mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>L</mml:mi><mml:mrow/></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">π</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mtext>f</mml:mtext><mml:mn>0</mml:mn></mml:msub><mml:mtext>k</mml:mtext><mml:mo>⋅</mml:mo><mml:mtext>t</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula></p>
<fig id="pcbi.1005338.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Stimulus level invariance.</title>
<p><bold>(A)</bold> The AN population response for the missing-fundamental harmonic complex tone of <xref ref-type="disp-formula" rid="pcbi.1005338.e010">Eq 6</xref>, with f<sub>0</sub> = 225 Hz. The stimulus has an amplitude level of 30 dB SPL, and the AN population response is normalized to one, as usual. The x-axis shows post-stimulus time, and the y-axis denotes the (linear) mapping between locations along the cochlea and CFs. <bold>(B)</bold> The AN population response for the same spectral structure as in <bold>A</bold> (3–8 harmonics), but for a stimulus level of 90 dB SPL. For this relatively high stimulus level, the nonlinearity effects of the cochlea over the AN population response are apparent. <bold>(C–D)</bold> The solutions of the LS case (<bold>h</bold><sub>LS</sub>) and the SC case (<bold>h</bold><sub>SC</sub>) for the 30 dB (C) and 90 dB SPL (D) stimulus levels, respectively. <bold>(E–F)</bold> Probability functions of the LS (<bold>S</bold><sub>p,LS</sub>) and the SC (<bold>S</bold><sub>p,SC</sub>) cases, for the two amplitude levels, respectively. In the 30 dB SPL case (E), the same pitch is succesfully estimated for both the LS and the SC simulations (blue and red arrows indicate maximum peaks). However, for the 90 dB SPL case (F) only the SC solution proved to be robust and invariant to the stimulus level, as desired (red arrow indicates maximum peaks). In order to account for the cochlear nonlinearities due to the changing in the stimuli levels, all simulations of the AN fibers in this section were made using Carney's cochlea model (Zilany’s et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref029">29</xref>]).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g006" xlink:type="simple"/>
</fig>
<p>The fundamental frequency is set to f<sub>0</sub> = 225 Hz, and the amplitude <italic>g</italic><sub><italic>L</italic></sub> corresponds to either 30 dB (<xref ref-type="fig" rid="pcbi.1005338.g006">Fig 6A</xref>) or 90 dB SPL. In these simulations, to account for the nonlinearities of the cochlea in the case of the higher sound level, we used the more updated model of Zilany et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref029">29</xref>]. <xref ref-type="fig" rid="pcbi.1005338.g006">Fig 6A</xref> shows that for the 30 dB SPL signal, the AN population response is limited to a small region around the CFs of the stimulus’ spectral components, as expected. On the other hand, for the 90 dB SPL case (<xref ref-type="fig" rid="pcbi.1005338.g006">Fig 6B</xref>), the AN population response is dramatically different as it spreads out along the whole cochlea. Moreover, since the response is heavily saturated, the spatiotemporal patterns of peaks and troughs for each of the AN fibers are biased relative to the case of the moderate sound levels. Despite this significant difference in the AN responses between different sound levels, psychoacoustic measurements indicate that the sensation of pitch is robust to that effect [<xref ref-type="bibr" rid="pcbi.1005338.ref006">6</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref044">44</xref>].</p>
<p>The output of the SC model, <bold>h</bold>, is shown in <xref ref-type="fig" rid="pcbi.1005338.g006">Fig 6C and 6D</xref> for the two stimuli levels, 30 and 90 dB-SPL, respectively. Each panel shows both the sparse (SC, λ = 0.01, red) and non-sparse (LS, λ = 0.0, blue) solutions. As can be expected, the number of nonzero components in the vector <bold>h</bold> is much smaller for the SC solution when compared with that obtained by the LS algorithm. This sparseness applies to both sound levels, but the difference between the two solutions is much more noticeable for higher sound level (compare the blue line in <xref ref-type="fig" rid="pcbi.1005338.g006">Fig 6C</xref> with that of <xref ref-type="fig" rid="pcbi.1005338.g006">Fig 6D</xref>). Specifically, the LS solutions for the two stimuli levels are fundamentally different, and there is no apparent preservation of the spectral components of the input stimulus. Thus, the LS solution is variant to sound level, as opposed to what we would have expected from a representation of a pitch in the auditory system [<xref ref-type="bibr" rid="pcbi.1005338.ref045">45</xref>]. In comparison, the SC solutions do manage to preserve their overall structure. While the two SC solutions are not identical, both have only a few non-zero terms that directly relate to the frequency components of the input stimulus. Consequently, it seems that the sparse requirement in <xref ref-type="disp-formula" rid="pcbi.1005338.e003">Eq 2</xref>, at the expense of the accuracy of the ordinary LS solution, contributes to the invariant representation of the stimulus in the vector <bold>h</bold>, regardless of its sound level.</p>
<p>In order to understand the effect of these different representations on pitch perception, we compare the resulting probability density functions (the <italic>pdf</italic>(<italic>f</italic><sub><italic>p</italic></sub>)) for the two stimulus levels and both LS and SC solutions (<xref ref-type="fig" rid="pcbi.1005338.g006">Fig 6E and 6F</xref>). For the low sound level (<xref ref-type="fig" rid="pcbi.1005338.g006">Fig 6E and 6F</xref>), the two solutions are alike, and there is no apparent benefit to using one over the other: both curves (blue and red) have peaks at the same frequencies, and the maximum probability point equals that of the stimulus' fundamental frequency, i.e., <inline-formula id="pcbi.1005338.e011"><alternatives><graphic id="pcbi.1005338.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>225</mml:mn><mml:mspace width="0.25em"/><mml:mtext>Hz</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula>. The result is substantially different for the 90 dB SPL amplitude. In this case, the pdf that corresponds to the solution of the LS algorithm has lost all resemblance with the stimulus' frequency components—it is just a flat, noisy curve. In comparison, the pdf that corresponds to the SC solution still has a clear indication of the original stimulus properties. In particular, the pdf curve (red line in <xref ref-type="fig" rid="pcbi.1005338.g006">Fig 6F</xref>) peaks at the harmonics of the fundamental f<sub>0</sub>, with a maximum peak at <inline-formula id="pcbi.1005338.e012"><alternatives><graphic id="pcbi.1005338.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>225</mml:mn><mml:mspace width="0.25em"/><mml:mtext>Hz</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec008">
<title>Effect of Different Dictionaries</title>
<p>Unlike, for example, the Fourier transform, the SC transform enables the use of various dictionaries that can be set according to some desired specifications. A standard option is to train a dictionary according to desired optimal constraints [<xref ref-type="bibr" rid="pcbi.1005338.ref035">35</xref>], but this is certainly not a prerequisite. For instance, when mapping patches of images into their parsimonious representations, one can choose to set the atoms of the dictionary as the basis of the discrete cosine transform, which is a straightforward and efficient choice (see for example Ch.12 in [<xref ref-type="bibr" rid="pcbi.1005338.ref025">25</xref>]).</p>
<p>In the current study, we checked two families of dictionaries: the first (<italic>D</italic><sub>sine</sub>) contains atoms created by sine stimuli, and the second (<italic>D</italic><sub><italic>stack</italic></sub>) contains atoms created from harmonics tones stimuli (harmonic stack). Specifically, each of the atoms in <italic>D</italic><sub>sine</sub> was produced by stimuli of one tone with random (uniformly distributed) amplitudes, and the atoms of <italic>D</italic><sub><italic>stack</italic></sub> were created by complex tone stimuli (harmonics 1<sup>st</sup>-6<sup>th</sup>) of the same moderate amplitude level (see <xref ref-type="sec" rid="sec021">Methods</xref>)</p>
<p>The two dictionaries were checked with both stimuli of 45dB SPL (<xref ref-type="fig" rid="pcbi.1005338.g007">Fig 7A–7C</xref>) and high amplitude levels of 90dB SPL (<xref ref-type="fig" rid="pcbi.1005338.g007">Fig 7B–7D</xref>). All simulations had the same spectral structure as given by <xref ref-type="disp-formula" rid="pcbi.1005338.e010">Eq 6</xref>, i.e., all signals were complex tones with the 3–8 harmonics (the missing fundamental case). The results show the maximum peaks (blue dots) of the resulting pdfs that are taken from an interval of an octave around the fundamental frequency, as is the practice in psychoacoustic measurements (see for example [<xref ref-type="bibr" rid="pcbi.1005338.ref041">41</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref042">42</xref>]).</p>
<fig id="pcbi.1005338.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Comparing the performance of different dictionaries over moderate and high amplitude stimulus levels.</title>
<p>All simulations have the same spectral structure (<xref ref-type="disp-formula" rid="pcbi.1005338.e010">Eq 6</xref>). This spectral structure is simulated for various fundamental frequencies, <italic>f</italic><sub>0</sub>, and the figures show the estimated pitches for each such case (i.e., the maximum peak in each pdf). The estimations are taken from an interval of ± 0 .5 octaves around <italic>f</italic><sub>0</sub>. Each row, i.e., figures A-B and figures C-D, show the estimation results of the SC model for the two dictionaries <italic>D</italic><sub>sine</sub>, and <italic>D</italic><sub><italic>stack</italic></sub>, respectively (see text). The column subplots refer to different stimuli levels: moderate (45dB SPL), and high (90dB SPL) amplitudes. The x-axis denotes the location of the first harmonic within the stimuli (i.e., the 3<sup>rd</sup> harmonic); the thick black dashed lines define the main octave (<italic>f</italic><sub>0</sub>), and the thin black dashed lines define the lower and upper octaves, i.e., 0.5 <italic>f</italic><sub>0</sub> and 2<italic>f</italic><sub>0</sub>, respectively. (A-B) At low frequencies, up to about 4k Hz of the lower harmonic in the complex stimulus, the estimations of the <italic>D</italic><sub>sine</sub> dictionary converge to the expected frequencies for both moderate and high stimuli. However, from 4k Hz and above, the pitch estimations for the high stimuli levels diverge from the main octave to other ratios of <italic>f</italic><sub>0</sub>. (C-D) The pitch estimations of the <italic>D</italic><sub><italic>stack</italic></sub> dictionary converge to the main octave better for the low and high frequencies and for both amplitudes.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g007" xlink:type="simple"/>
</fig>
<p>Comparing the four subplots of <xref ref-type="fig" rid="pcbi.1005338.g007">Fig 7</xref>, the dissimilarities between the results of the two dictionaries demonstrate quantitatively different results but qualitatively similar performances, at least in the lower frequency regions. For example, the two dictionaries yield relatively good estimations of the pitches up to about 4k Hz of the first stimulus’ harmonic (the third harmonic). For frequencies higher than 4k Hz, there is more variance around the expected fundamental frequency (thick black dashed middle line). Specifically, the 90dB SPL estimations for both <italic>D</italic><sub>sine</sub> and <italic>D</italic><sub><italic>stack</italic></sub> seem to be horizontally spread (<xref ref-type="fig" rid="pcbi.1005338.g007">Fig 7B–7D</xref>). From our experience, these deviations can be reduced by using dictionaries of higher resolution (i.e., with more cochlear channels and more atoms). However, due to the naïve structure of this model, there is a computational limit to the dictionary size that we can use. This restriction will hopefully be alleviated by a future model (see <xref ref-type="sec" rid="sec014">Discussion</xref>).</p>
<p><xref ref-type="fig" rid="pcbi.1005338.g008">Fig <bold>8</bold></xref> shows in more detail four selected examples of <italic>f</italic><sub>0</sub> = 606.4 <italic>Hz</italic>. The two <bold>h</bold> vectors in <xref ref-type="fig" rid="pcbi.1005338.g008">Fig <bold>8</bold>A</xref> are taken from the moderate and high amplitudes levels of the <italic>D</italic><sub>sine</sub> dictionary case. The performance of the model for the 90dB level is a bit degraded compared to the 45dB level, as expected (see also <xref ref-type="fig" rid="pcbi.1005338.g006">Fig 6</xref>). Specifically, in the 90 dB SPL, the model estimates lower coefficients in <bold>h</bold> due to the nonlinearity of the cochlea. The resulting pdf, i.e., the probability for a particular pitch given these SC coefficients, is shown in <xref ref-type="fig" rid="pcbi.1005338.g008">Fig <bold>8</bold>B</xref>. <xref ref-type="fig" rid="pcbi.1005338.g008">Fig <bold>8</bold>C and <bold>8</bold>D</xref> show the <bold>h</bold> vectors and pdfs for the second dictionary, the <italic>D</italic><sub><italic>stack</italic></sub>. Comparing <xref ref-type="fig" rid="pcbi.1005338.g008">Fig <bold>8</bold>A</xref> with <xref ref-type="fig" rid="pcbi.1005338.g008">Fig <bold>8</bold>C</xref> for the respected amplitude levels shows that the two SC vectors have different coefficients. This difference is due to the particular structure of the atoms in each of the two dictionaries above. Still, in spite of this structural differences, the <bold>h</bold> vectors have nonzero coefficients over indices that represent harmonics of f<sub>0</sub>. Consequently, the resulting pdfs have qualitatively similar results and the maximum peaks appear at the same location. Thus, the result is that the SC model predicts the same pitches for these two cases. Interestingly, the harmonic sieve is designed, in principle, to be optimal in the case of <italic>D</italic><sub>sine</sub> (and under certain assumptions, see [<xref ref-type="bibr" rid="pcbi.1005338.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref040">40</xref>]). Even so, it can still be used for the <italic>D</italic><sub><italic>stack</italic></sub> dictionary and yield good results. Note that it might be that for the <italic>D</italic><sub><italic>stack</italic></sub> dictionary there is a better (in the aforementioned optimal sense) representation for the harmonic sieve, but we did not pursue this path any further.</p>
<fig id="pcbi.1005338.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Detailed results for <italic>f</italic><sub>0</sub> = 606.4<italic>Hz</italic>.</title>
<p>The selected examples are taken from <xref ref-type="fig" rid="pcbi.1005338.g007">Fig 7</xref> and show the SC coefficient vectors <bold>h</bold> and the pdfs for the two dictionaries and for the two amplitudes. <bold>(A, C)</bold> The SC coefficient vectors <bold>h</bold> for the <italic>D</italic><sub>sine</sub> and <italic>D</italic><sub><italic>stack</italic></sub> dictionaries, respectively. <bold>(B, D)</bold> The resulting pdfs, over one octave around <italic>f</italic><sub>0</sub> = 606.4<italic>Hz</italic>, for the <italic>D</italic><sub>sine</sub> and <italic>D</italic><sub><italic>stack</italic></sub> dictionaries, respectively. Note the difference between the SC coefficients of the two dictionaries, but the qualitative resemblance between the two pdfs.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g008" xlink:type="simple"/>
</fig>
<p>In summary, this section emphasizes the notion that choosing a dictionary can improve or reduce the performance of the model in different aspects. Thus, one emerging interesting question from the above discussion is which dictionary can be acquired in a biologically compelling manner to match psychoacoustic and physiological measurements best? In this paper, however, we do not address this issue, but we have chosen to focus on demonstrating that parsimonious representation of an auditory information can explain relatively high cognitive tasks, i.e., the percept of a pitch. In what follows we assume a dictionary that is built of a single tone. Hopefully, the choosing of such a relatively simple dictionary, instead of a more intricate one, would prove to be clear and emphasize the qualitative abilities of such an approach.</p>
</sec>
<sec id="sec009">
<title>Resolved and Unresolved Harmonics</title>
<p>The harmonics of a periodic signal are spatially distributed along the BM. Because of BM properties, low harmonics create separate peaks that are translated into distinct excitation patterns in the activities of the ANs. Since higher harmonics, on the other hand, do not yield such distinct peaks, these harmonics do not have distinct excitation patterns. Consequently, low harmonics are referred to as resolved and higher harmonics, approximately at the 5<sup>th</sup>–10<sup>th</sup> harmonics [<xref ref-type="bibr" rid="pcbi.1005338.ref014">14</xref>], as unresolved. For unresolved stimuli, the temporal aspects of AN response convey more information about the pitch than the spatial aspects of that response. Thus, using stimuli with resolved and unresolved harmonics is a controlled way to inspect the temporal processing of a pitch in the auditory system.</p>
<p>Broadly speaking, temporal models, such as the summary autocorrelation function (SACF) [<xref ref-type="bibr" rid="pcbi.1005338.ref004">4</xref>], disregard the resolvability of the stimulus. This is because such models account for the interactions between the harmonic components of the periodic signal whether they are resolved or not [<xref ref-type="bibr" rid="pcbi.1005338.ref044">44</xref>]. This indifference, however, stands in contrast to psychoacoustic observations [<xref ref-type="bibr" rid="pcbi.1005338.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref014">14</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref046">46</xref>].</p>
<p>The current model combines temporal and spatial aspects of the AN response within its atoms. Thus, it is interesting to examine the model’s response to this class of stimuli. We compare five stimuli of complex tones (<xref ref-type="fig" rid="pcbi.1005338.g009">Fig 9</xref>),
<disp-formula id="pcbi.1005338.e013">
<alternatives>
<graphic id="pcbi.1005338.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e013" xlink:type="simple"/>
<mml:math display="block" id="M13">
<mml:mrow><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>45</mml:mn><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">π</mml:mi><mml:mtext>f</mml:mtext></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mtext>k</mml:mtext><mml:mo>⋅</mml:mo><mml:mtext>t</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula></p>
<fig id="pcbi.1005338.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Resolved vs. unresolved representation of harmonic cues.</title>
<p><bold>(A)</bold> The solutions <bold>h</bold><sub>k</sub>, k ∈ [1, 5], for the stimuli of <xref ref-type="disp-formula" rid="pcbi.1005338.e013">Eq 7</xref>. We compare the SCs of the two dictionaries, <italic>D</italic><sub>sine</sub> (lines) and <italic>D</italic><sub><italic>stack</italic></sub> (dashed lines). <italic>D</italic><sub>sine</sub> consists of tone-atoms and <italic>D</italic><sub><italic>stack</italic></sub> consists of complex tones that contain six harmonics with decreasing amplitudes (1 to 1/6). All stimuli contain four harmonics of the same fundamental frequency, f<sub>0</sub> = 433 Hz, but at different spectral locations (r ∈ {1, 6, 10, 17, 22}). The x-axis is normalized by f<sub>0</sub> for convenience. The correlation between the SC solutions and the stimuli' spectral components (<xref ref-type="disp-formula" rid="pcbi.1005338.e013">Eq 7</xref>) are apparent. Note that signals with low-frequency components (such as <bold>h</bold><sub>1</sub>) have more prominent nonzero coefficients than those of the higher harmonics (e.g., <bold>h</bold><sub>5</sub>). A closer look at <bold>h</bold><sub>5</sub> (the inset) shows that only two of the four harmonics are successfully reconstructed (the 23 and 24 tones of the 22–25 harmonics). <bold>(B)</bold> Pitch probabilities (pdfs) for the five complex tones for the <italic>D</italic><sub>sine</sub> (see text). The right figure shows all f<sub>p</sub> frequencies and the left one views fewer octaves around f<sub>0</sub>. The numbers above the curves state the four prominent peaks of the pdfs, from the highest (1) to the fourth lower peak. Observe that all five solutions peak at the first harmonic, that is, the model predicts the same 433 Hz pitch for all stimuli. Additionally, most of the other plausible pitches, i.e., other peaks, are usually located at harmonic ratios of f<sub>0</sub>, that is, they represent octave equivalence options. It is also instructive to note the f<sub>LOCUS</sub> frequencies in the right figure of (B). These peaks indicate the additional possibility of perceiving the pitches at the locus of the stimuli spectral energy and not of f<sub>0</sub> [<xref ref-type="bibr" rid="pcbi.1005338.ref001">1</xref>]. All simulations were performed with Slaney's model and with a sound level of 45 dB SPL.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g009" xlink:type="simple"/>
</fig>
<p>In <xref ref-type="disp-formula" rid="pcbi.1005338.e013">Eq 7</xref>, each stimulus has the same spectral structure, that is, four consecutive harmonics, but the spectral locations of harmonics vary. Specifically, the spectral location of the first harmonic in each signal is set by r ∈ {1, 6, 10, 17, 22}. Additionally, the fundamental frequency is configured to f<sub>0</sub> = 443 Hz, and the gains (<italic>g</italic><sub>45 <italic>dB</italic></sub>) of all stimuli are equivalent to 45 dB SPL.</p>
<p><xref ref-type="fig" rid="pcbi.1005338.g009">Fig 9A</xref> shows the sparse coefficient vectors <bold>h</bold> for all five stimuli. Additionally, we compare the SCs of the two dictionaries, <italic>D</italic><sub>sine</sub> (lines) and <italic>D</italic><sub><italic>stack</italic></sub> (dashed lines). <italic>D</italic><sub>sine</sub> consists of tone-atoms and <italic>D</italic><sub><italic>stack</italic></sub> consists of complex tones that contain six harmonics with decreasing amplitudes (1 to 1/6). All the atoms are created with 60 dB SPL stimuli. As can be seen, there is a slight difference between the results, but overall both dictionaries yield the same SCs, pdfs, and pitch estimations. For that reason, we focused the rest of the analysis on the <italic>D</italic><sub>sine</sub> results.</p>
<p>In <xref ref-type="disp-formula" rid="pcbi.1005338.e013">Eq 7</xref>, Low values of r represent stimuli with resolved harmonics while larger values (r &gt; 5) represents stimuli with unresolved harmonics. Observe (<xref ref-type="fig" rid="pcbi.1005338.g009">Fig 9A</xref>) that for most of the stimuli, the four prominent spectral components of <bold>h</bold> are successfully reconstructed. Still, there is an apparent degradation (from resolved to unresolved stimuli) in the amplitude of the coefficient terms. For resolved stimuli, e.g., the 1<sup>st</sup>–4<sup>th</sup> harmonics stimulus (r = 1), the <bold>h</bold> vector holds prominent terms equivalent to the frequency components of the input signal. As r increases, the estimated terms in <bold>h</bold> decrease; for example, in the 22<sup>nd</sup>–25<sup>th</sup> harmonics stimulus (r = 22), the vector <bold>h</bold> only contains two out of the four equivalent frequency components of the input signal (<xref ref-type="fig" rid="pcbi.1005338.g009">Fig 9A</xref>, inset). This degradation is due to the reduced ability of the AN population to phase-lock with high frequency stimuli. Thus, the match between the atoms and the stimuli is less accurate, which results in smaller SC coefficients.</p>
<p><xref ref-type="fig" rid="pcbi.1005338.g009">Fig 9B</xref> shows the corresponding pdfs of each of the five stimuli. The right panel shows the entire pdf while the left panel focuses on the vicinity of the fundamental f<sub>0</sub> = 443 Hz. The numbers in the figure indicate the arrangement in a descending order of the peak heights, starting from the highest peak (peak number 1). For all stimuli, the maximum of each density function was obtained at the fundamental frequency. It is instructive to note that for each of the stimuli, other local maxima, indicating other pitch possibilities, are allocated at harmonic ratios of f<sub>0</sub>. For example, in the case of the lowest harmonic complex (r = 1), maximum peaks in the pdf are also available at the f<sub>0</sub>/4, f<sub>0</sub>/2, and 2f<sub>0</sub>. These other options represent the octave equivalence of the perceived pitch. Usually, humans perceive these options to be the same pitch, or to have the same pitch chroma. Hence, in psychoacoustic measurements, this mixing between octaves is generally not considered as an error [<xref ref-type="bibr" rid="pcbi.1005338.ref041">41</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref043">43</xref>] (specifically, see SI in [<xref ref-type="bibr" rid="pcbi.1005338.ref041">41</xref>]).</p>
<p>In <xref ref-type="fig" rid="pcbi.1005338.g010">Fig 10</xref> we compare the pdfs of the two complex tones of <xref ref-type="fig" rid="pcbi.1005338.g009">Fig 9</xref>. The stimulus that contains the 1–4 harmonics is shown in blue while the stimulus that contains the 22–25 harmonics is in green. This comparison is done over one octave to avoid the other pitch equivalence solutions (which are approximately at the same height of the 1<sup>st</sup> peak). By inspection (<xref ref-type="fig" rid="pcbi.1005338.g010">Fig 10A</xref>), the ratio between the 1<sup>st</sup> and the 2<sup>nd</sup> peaks is higher for the resolved stimulus (blue line) compared to that of the unresolved stimulus (green line). We chose to denote this difference in the ratios as the salience of the stimuli. <xref ref-type="fig" rid="pcbi.1005338.g010">Fig 10B</xref> shows a comparison of additional stimuli with different fundamental frequencies; each one is a complex tone that contains four consecutive tones. The colors of the circles match the stimuli colors of <xref ref-type="fig" rid="pcbi.1005338.g009">Fig 9</xref>. Note that the salience of all stimuli decline with the increase of the location of the first harmonic, i.e., as the harmonics transcend from resolved to unresolved. Finally, stimuli with high harmonics also have additional peaks around the locus of the harmonic components (see the f<sub>LOCUS</sub> peaks in <xref ref-type="fig" rid="pcbi.1005338.g009">Fig 9B</xref>). These phenomena are consistent with known physiological data [<xref ref-type="bibr" rid="pcbi.1005338.ref009">9</xref>].</p>
<fig id="pcbi.1005338.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Salience of complex tones.</title>
<p><bold>(A)</bold> A Comparison between the two probability functions of the complex tones from <xref ref-type="fig" rid="pcbi.1005338.g009">Fig 9</xref>: the blue line is the pdf of the complex harmonic tone with the 1–4 harmonics, and the green line is the pdf of the fifth stimulus, which comprises 22–25 harmonics. The x-axis is limited to one octave in order to compare the pitch's relative heights and without considering the octave equivalence of consecutive harmonics. The blue and green arrows show the 1<sup>st</sup> and the 2<sup>nd</sup> largest peaks of the two curves, respectively. Computing the ratio for each curve between the 1<sup>st</sup> and the 2<sup>nd</sup> peaks yields a measure of the pitch's salience; a larger ratio indicates a more prominent percept of tha pitch. <bold>(B)</bold> Calculating the ratio between the 1<sup>st</sup> and 2<sup>nd</sup> peaks for harmonic tones with four consecutive tones at different harmonic numbers. The x-axis indicates the location of the first harmonic in each stimulus, and the y-axis shows the ratio between the 1<sup>st</sup> and the 2<sup>nd</sup> peaks (as demonstrated in (A)). Colored circles indicate the relevant stimuli that are shown in <xref ref-type="fig" rid="pcbi.1005338.g009"><bold>Fig 9</bold></xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g010" xlink:type="simple"/>
</fig>
<p>To conclude, despite the model’s seemingly spatially-based nature, it can derive the pitch of unresolved harmonics to some extent. Unlike purely temporal models, however, it penalizes these stimuli in relation to resolved ones. Note that this penalty is a consequence of the cochlear properties and not of the SC module. Specifically, this penalty was not introduced artificially into the model—it is an implicit property of the atoms and stems directly from the properties of the cochlear model.</p>
</sec>
<sec id="sec010">
<title>Pitch Shift of Inharmonic Equally Spaced Tones</title>
<p>Perceived pitches are usually considered within the context of periodic signals. For example, the perceived pitch of a complex tone is its fundamental frequency, f<sub>0</sub>, whether it exists in the complex or not. Consider the following harmonic series
<disp-formula id="pcbi.1005338.e014">
<alternatives>
<graphic id="pcbi.1005338.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e014" xlink:type="simple"/>
<mml:math display="block" id="M14">
<mml:mrow><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext>f</mml:mtext><mml:mn>0</mml:mn></mml:msub><mml:mtext>k</mml:mtext><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mtext>f</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula></p>
<p>For Δf = 0, human subjects usually perceive the pitch of s<sub>in</sub>(t) as f<sub>0</sub>, the fundamental frequency of the harmonic signal [<xref ref-type="bibr" rid="pcbi.1005338.ref047">47</xref>]. This is true even for cases when f<sub>0</sub> is not present in the signal, i.e., k<sub>0</sub> &gt; 1. For increased Δf &gt; 0, the expression in <xref ref-type="disp-formula" rid="pcbi.1005338.e014">Eq 8</xref> is no longer harmonic. Nonetheless, previous psychoacoustic experiments have revealed that human subjects do manage to perceive pitches with these shifted stimuli, and the detected pitches are approximately shifted on a linear scale relative to the fundamental frequency [<xref ref-type="bibr" rid="pcbi.1005338.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref040">40</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref047">47</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref049">49</xref>]. These kind of stimuli are important because they demonstrate that pitch detection does not follow the stimulus' envelope, which does not change in this case (this is not true for stimuli with unresolved harmonics); nor does it follow the spacing between frequency components of a stimulus [<xref ref-type="bibr" rid="pcbi.1005338.ref001">1</xref>]. Thus, the phenomenon of pitch shift was used as a counter example for models that exploit the temporal envelope of a stimulus or other of its temporal features, such as zero crossing, peaks, etc. [<xref ref-type="bibr" rid="pcbi.1005338.ref018">18</xref>].</p>
<p><xref ref-type="fig" rid="pcbi.1005338.g011">Fig 11A–11D</xref> shows the model's solutions of the sparse coefficient vectors, <bold>h</bold>, for different shifted signals (<xref ref-type="disp-formula" rid="pcbi.1005338.e014">Eq 8</xref>). In these simulations, the input signal, s<sub>in</sub>(t), has four frequency components that are set to k ∈ [4,7], and the (missing) fundamental frequency is set to f<sub>0</sub> = 200 Hz. In this example, setting Δf = 0 creates a stimulus s<sub>in</sub>(t) that is a complex harmonic series of f<sub>0</sub> = 200 Hz. For a shift of Δf = 40 Hz, the stimulus is no longer a complex tone of f<sub>0</sub> = 200 Hz, and a shift of Δf = 100 Hz changes the stimulus to be a complex tone of f<sub>0</sub> = 100 Hz (with the [9, 11, 13, 15] harmonics). Finally, for a frequency shift of Δf = 200 Hz, the input signal is once again a complex tone of the fundamental f<sub>0</sub> = 200 Hz, but this time with the [5, 6, 7, 8] harmonics. Following these observations, we would expect the model to exhibit this ambiguity by the frequency shift Δf, and to alternate its predictions between the frequencies of f<sub>0</sub>, 0.5f<sub>0</sub>, 2f<sub>0</sub>, etc.</p>
<fig id="pcbi.1005338.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Pitch shift of equally spaced harmonics.</title>
<p><bold>(A-D)</bold> The vectors <bold>h</bold> for complex harmonic stimuli that contain the four harmonics of 4–7 (<xref ref-type="disp-formula" rid="pcbi.1005338.e014">Eq 8</xref>). The x-axis denotes f<sub>d</sub> normalized by the fundamental frequency, f<sub>0</sub> = 200Hz. The four figures show the stimulus in <xref ref-type="disp-formula" rid="pcbi.1005338.e014">Eq 8</xref> for the cases of Δf = 0 Hz, 40 Hz, 100 Hz, and 200 Hz, respectively. The zero shift case represents a regular complex harmonic signal. The 40 Hz shift is no longer a complex tone of 200 Hz. The third option (C) is a harmonic complex of 100 Hz (with the harmonics 9, 11, 13, and 15). Finally, the Δf = 200 Hz shift results again in a complex harmonics of f<sub>0</sub> = 200 Hz but this time with the 5–8 harmonics. <bold>(E)</bold> The peaks of the probability functions, <italic>pdf</italic>(<italic>f</italic><sub><italic>p</italic></sub>), for 500 uniformly shifted stimuli. Each stimulus is given by <xref ref-type="disp-formula" rid="pcbi.1005338.e014">Eq 8</xref>, i.e., each signal includes the first four terms (1–4) of the fundamental f<sub>0</sub> = 200Hz, plus an incremental frequency shift of Δf. The x-axis denotes the frequency of the lowest harmonic component of the input stimulus (f<sub>0</sub> + Δf) normalized by f<sub>0</sub> for visual clarity. The y-axis denotes the estimated pitch. To demonstrate the ambiguity of this process, we included the first four largest peaks of each of the resulted pdfs. We focused the view along the 100 Hz, 200 Hz, and 400 Hz in the y-axis; all other regions are mostly empty. Note the linear shifts in the pitch estimations and the changing of these slopes as a function of Δf [<xref ref-type="bibr" rid="pcbi.1005338.ref047">47</xref>].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g011" xlink:type="simple"/>
</fig>
<p>We performed 500 simulations of s<sub>in</sub>(t) (<xref ref-type="disp-formula" rid="pcbi.1005338.e014">Eq 8</xref>) with f<sub>0</sub> = 200 Hz. Each signal contains the first six harmonics (i.e., k ∈ [1, 6]), and each is simulated with a different Δf. <xref ref-type="fig" rid="pcbi.1005338.g011">Fig 11E</xref> shows the estimated frequency <inline-formula id="pcbi.1005338.e015"><alternatives><graphic id="pcbi.1005338.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> as a function of the lower harmonic component <italic>f</italic><sub><italic>L</italic></sub> in the input signal. For each of the stimuli, the corresponding pdfs are calculated, and the four highest peaks are indicated. We chose to include the four prominent peaks of each pdf to show the ambiguities of these signals. The estimated pitches are clustered in lines as a function of the lower frequency <italic>f</italic><sub><italic>L</italic></sub>, consistent with the known “first effect of pitch shift” [<xref ref-type="bibr" rid="pcbi.1005338.ref047">47</xref>]. One can also see that as the shift Δf increases, the slopes of the estimated lines slightly decrease, in accordance with the known psychoacoustic phenomenon [<xref ref-type="bibr" rid="pcbi.1005338.ref047">47</xref>].</p>
<p>To conclude, the current model qualitatively reproduces the known psychoacoustic phenomenon of pitch shift, even though these aperiodic signals are not part of the model's dictionary. This implies that the current model: generalizes to complex new stimuli; it does not depend on the stimulus temporal envelope for cues; and it does not use the spacing between the stimulus harmonics to estimate pitch.</p>
</sec>
<sec id="sec011">
<title>Transposed Tones</title>
<p>Transposed tones (TTs) were first introduced to explore the relative sensitivity of the auditory system for binaural timing stimuli [<xref ref-type="bibr" rid="pcbi.1005338.ref050">50</xref>]. Oxenham et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref013">13</xref>] used these signals to check the relation between the tonotopic organization in the cochlea and the perception of pitch. The motivation of using these signals is to introduce low-frequency temporal structures into the basal part of the cochlea that usually processes high frequencies. TTs are produced by the modulation of half-wave rectified sine waves with carrier waves. Due to the limited synchronization and low-pass properties of the basilar membrane, the outer hair cells, and the ANs, the fine details of the carrier waves would be negligible. Thus, the results are half-wave rectified sine waves of low frequency (f<sub>0</sub>) in high CFs regions (&gt; 4 kHz). In this manner, there is a separation between spatial locations and fine temporal structures along the cochlea.</p>
<p>Measurements carried out by Oxenham et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref013">13</xref>] showed that subjects could not estimate the fundamental frequency of the TT stimuli. This inability means that the spatial arrangement of CFs along the cochlea is essential for the perception of pitch. Oxenham et al. have also shown that the summary autocorrelation function (SACF), a well-known temporal model, is indifferent to TTs and thus to the tonotopic organization in the cochlea. When applying TTs to the analysis of the SACF, the model does manage to extract the correct fundamental frequencies, in contrast to the aforementioned psychoacoustic evidence.</p>
<p>In the case of the proposed model, each region along the cochlea is characterized by its local spatiotemporal activities. These localized patterns are embedded in the atoms for each CF. Hence, we predicted that the SC model would not be able to separate between spatial and temporal processing. <xref ref-type="fig" rid="pcbi.1005338.g012">Fig 12</xref> shows the processing of TT stimuli by the SC model. We first considered three TTs with the fundamentals f<sub>0</sub> = 230 Hz, 250 Hz, and 270 Hz. The expression for each of these TTs is given by
<disp-formula id="pcbi.1005338.e016">
<alternatives>
<graphic id="pcbi.1005338.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e016" xlink:type="simple"/>
<mml:math display="block" id="M16">
<mml:mrow><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>30</mml:mn><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="normal">s</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">π</mml:mi><mml:mtext>f</mml:mtext></mml:mrow><mml:mrow><mml:mtext>c</mml:mtext><mml:mo>,</mml:mo><mml:mtext>k</mml:mtext></mml:mrow><mml:mrow/></mml:msubsup><mml:mo>⋅</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula></p>
<fig id="pcbi.1005338.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Transposing low-frequency tones into high-frequency regions of the cochlea.</title>
<p><bold>(A)</bold> An example of three sparse coefficient vectors, <bold>h</bold>, for the three frequencies f<sub>0</sub> = 229 Hz, 249 Hz, and 269.7 Hz. The resulting <bold>h</bold> vectors have the same nonzero indices, i.e., these stimuli cannot be differentiated based on their sparse representations. <bold>(B)</bold> The pdfs of the three TTs are noisy and inconclusive, as expected. <bold>(C)</bold> Predictions of 100 epochs; only the 1<sup>st</sup> peak in the pdf is considered. There are two distinct types of stimuli: (i) pure tones (blue), and (ii) TTs (red). Both stimuli are simulated with incremental fundamental frequencies of f<sub>0</sub> ∈ [100 Hz, 500 Hz]. Each stimulus is normalized relative to the fundamental f<sub>0</sub>. The model could estimate the f<sub>0</sub> of the pure tones with a high degree of accuracy but could not predict those of the TTs at all (compare with [<xref ref-type="bibr" rid="pcbi.1005338.ref013">13</xref>]).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g012" xlink:type="simple"/>
</fig>
<p>In this equation, <inline-formula id="pcbi.1005338.e017"><alternatives><graphic id="pcbi.1005338.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="normal">s</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is a low-pass filtered version of a rectified sine wave (see <xref ref-type="sec" rid="sec021">Methods</xref>), and each of these tones is modulated by the three carriers: f<sub>c,1</sub> = 4 kHz, f<sub>c,2</sub> = 6.35 kHz, and f<sub>c,3</sub> = 10.08 kHz. <xref ref-type="fig" rid="pcbi.1005338.g012">Fig 12A</xref> shows the results of the sparse coefficient vectors (<bold>h</bold>) for the above three TT stimuli. It is apparent that the three sparse representations occupy the same indices, i.e., the same frequencies f<sub>d</sub> in <bold>h</bold>. Therefore, the SC model cannot distinguish between these stimuli based on their sparse representations. And indeed, the pdfs of these sparse vectors are inconclusive (<xref ref-type="fig" rid="pcbi.1005338.g012">Fig 12B</xref>).</p>
<p>Next, we simulated a batch of 100 epochs (<xref ref-type="fig" rid="pcbi.1005338.g012">Fig 12C</xref>). Each epoch contained the TT of <xref ref-type="disp-formula" rid="pcbi.1005338.e016">Eq 9</xref>, and each had an incremented fundamental frequency taken from the interval f<sub>0</sub> ∈ [100 Hz, 500 Hz] (red bars in <xref ref-type="fig" rid="pcbi.1005338.g012">Fig 12C</xref>). We repeated the simulation also for pure tones (blue bars in <xref ref-type="fig" rid="pcbi.1005338.g012">Fig 12C</xref>) and compared the two by normalizing the measurements with the respective f<sub>0</sub>. The results are consistent with the findings of Oxenham et al., that is, the SC model could not estimate the f<sub>0</sub> for the TT stimuli successfully.</p>
</sec>
<sec id="sec012">
<title>Iterated Rippled Noise</title>
<p>Delaying a signal of broadband white noise and adding it back to the original one creates a signal known as rippled noise. When this process of delaying and adding is repeated, a signal known as iterated rippled noise (IRN) is created. These signals contain temporal regularities in the time domain and spectral peaks at the reciprocal of the delayed time in the spectrum domain. Due to the nature of these signals, human listeners report perceiving two sensations: a tonal part that amounts to the pitch of the reciprocal of the delay (<italic>d</italic> ms) and an additional noisy sensation [<xref ref-type="bibr" rid="pcbi.1005338.ref051">51</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref052">52</xref>]. Repeating the iteration process results in a more prominent sensation of the tonal pitch [<xref ref-type="bibr" rid="pcbi.1005338.ref053">53</xref>].</p>
<p>Adding the delayed noisy signal back to the original one with a gain of one (delay-add) yields a signal with spectral peaks that are located at the reciprocal of the delay time <italic>d</italic>. But adding the delayed signal with a gain of minus one (delay-subtract) yields a signal with peaks in the power spectrum that are shifted by 1/2<italic>d</italic>, as if the delay-subtract signal is an odd-harmonic complex of half the frequency of the delay-add version. Delay-add stimulus raises a sensation of pitches of 1/<italic>d</italic> Hz, whereas delay-subtract is usually perceived to be more ambiguous and yields pitches that are slightly higher or lower than 1/<italic>d</italic> Hz [<xref ref-type="bibr" rid="pcbi.1005338.ref053">53</xref>].</p>
<p>Simulations of delay-add and delay-subtract stimuli with the delays of d = 2, 4, and 5 ms, are shown in <xref ref-type="fig" rid="pcbi.1005338.g013">Fig 13</xref>. These signals are created as follows: for a white noise, <italic>x</italic>(<italic>t</italic>), the iterated signal <italic>s</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is created by
<disp-formula id="pcbi.1005338.e018">
<alternatives>
<graphic id="pcbi.1005338.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e018" xlink:type="simple"/>
<mml:math display="block" id="M18">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
for <italic>i</italic> = 1,…,n<sub><italic>itr</italic></sub>, and n<sub><italic>itr</italic></sub> is the number of iterations (e.g., 1, 2, or 10). All simulations were done using Carney's model (Zilany et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref029">29</xref>]) with stimuli levels of 70dB SPL; the dictionary contained 1000 sine-atom groups and each group has 10 time-shifts (phases) in it (g = 10, <xref ref-type="disp-formula" rid="pcbi.1005338.e004">Eq 3</xref>).</p>
<fig id="pcbi.1005338.g013" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g013</object-id>
<label>Fig 13</label>
<caption>
<title>Iterated rippled noise for different time delays and repetitions.</title>
<p>The figures show the results of 500 simulations for each case of IRN stimulus. Each subplot along the columns show the delays of d = 5, 4, and 2 ms that correspond to the fundamental frequencies of 200, 250, and 500 Hz, respectively. The subplots in the first row show the delay-add simulations, and the lower row shows the delay-subtract simulations. The results are derived from the first peaks of the resulting pdfs, and all estimations are taken from an interval of one octave around the appropriate fundamental frequency [<xref ref-type="bibr" rid="pcbi.1005338.ref042">42</xref>]. Simulations are done using Carney's model (Zilany et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref029">29</xref>]) with stimuli of 70 dB SPL. The dictionary contained 1000 groups of sine-atoms with distinct CFs and 10 phases in each group (g = 10, <xref ref-type="disp-formula" rid="pcbi.1005338.e004">Eq 3</xref>).The blue dots indicate rippled noise (one repetition), red points correspond to IRN with 2 repetitions, and yellow dots are for the 10 repetitions. <bold>(A-C)</bold> The delay-add simulations show distinct peaks around the 1/<italic>d</italic> frequencies. <bold>(D-F)</bold> The delay-subtract simulations show accumulation of the inferred pitches at frequencies equal to or greater than 1/<italic>d</italic>±10%, but the results for this case are noisy and inaccurate relative to psychoacoustic measurements.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g013" xlink:type="simple"/>
</fig>
<p>In the first row (<xref ref-type="fig" rid="pcbi.1005338.g013">Fig 13A–13C</xref>), which contains the delay-add cases, a clear peak appears at the reciprocal of the delay, 1/<italic>d</italic>, as expected. As the number of repetitions increases, so does the prediction quality of the model, i.e., more estimations are concentrated around 1/<italic>d</italic> Hz. In <xref ref-type="fig" rid="pcbi.1005338.g013">Fig 13D–13F</xref> we show the delay-subtract cases for the same delays. In these simulations (<xref ref-type="fig" rid="pcbi.1005338.g013">Fig 13E and 13F</xref>), the inferred pitches are located around the reciprocal of the delays, as expected. But we would also expect the measurements to peak at approximately 1/<italic>d</italic>±10%, which does not happen.</p>
</sec>
<sec id="sec013">
<title>Musical Notes</title>
<p>Music in the Western culture is based on a musical scale that relates periodic (or quasiperiodic) sounds to their fundamental frequencies. Thus, musical instruments that are based on this musical scale produce harmonic sounds based on these fundamental frequencies. As such, different instruments have different spectral coloring (i.e., timbre), but human listeners can perceive and compare the fundamental frequencies between the instruments [<xref ref-type="bibr" rid="pcbi.1005338.ref054">54</xref>]. This ability is due to the pitch perception property of clustering periodic (or quasiperiodic) sounds into classes, i.e., musical notes.</p>
<p>In this section, we checked the SC model with recorded musical notes [<xref ref-type="bibr" rid="pcbi.1005338.ref055">55</xref>]. For this, we used a dictionary with 1000 atoms, each of which had ten different phases (g = 10, <xref ref-type="disp-formula" rid="pcbi.1005338.e004">Eq 3</xref>). Each recorded stimulus was divided into <italic>T</italic><sub><italic>steps</italic></sub> = 100 time steps that were analyzed separately. <xref ref-type="fig" rid="pcbi.1005338.g014">Fig 14A</xref> shows the FT of a recorded violin note of A5 (880 Hz) played with a bow (arco). At each time step <italic>T</italic><sub><italic>steps</italic></sub>, <xref ref-type="disp-formula" rid="pcbi.1005338.e004">Eq 3</xref> is solved separately to obtain <inline-formula id="pcbi.1005338.e019"><alternatives><graphic id="pcbi.1005338.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. Next, the coefficients of each group are summed together to get the SC vector <bold>h</bold> ∈ <italic>R</italic><sup><italic>M</italic></sup>. The matrix <inline-formula id="pcbi.1005338.e020"><alternatives><graphic id="pcbi.1005338.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is the aggregation of all <bold>h</bold> ∈ <italic>R</italic><sup><italic>M</italic></sup> over <italic>T</italic><sub><italic>steps</italic></sub> = 100 steps (<xref ref-type="fig" rid="pcbi.1005338.g014">Fig 14B</xref>). Each of the columns of <italic>H</italic><sub><italic>g</italic></sub> (the SC vectors) are then processed by the harmonic sieve unit to produce the probability of that time step. The collection of all these pdf is given by the matrix <inline-formula id="pcbi.1005338.e021"><alternatives><graphic id="pcbi.1005338.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> (<xref ref-type="fig" rid="pcbi.1005338.g014">Fig 14C</xref>). Finally, to have one single probability for each stimulus, the matrix <italic>P</italic><sub><italic>g</italic></sub> is averaged over the time domain and normalized appropriately (<xref ref-type="fig" rid="pcbi.1005338.g014">Fig 14D</xref>). As in previous cases, the pitch of the signal is defined as the maximum point in this pdf.</p>
<fig id="pcbi.1005338.g014" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g014</object-id>
<label>Fig 14</label>
<caption>
<title>Analyzing a recorded stimulus of a violin.</title>
<p><bold>(A)</bold> The Fourier transform of the recorded signal. This is a note of A5 (880 Hz) played by a bow (arco). The 880 Hz and its harmonics are clearly seen. <bold>(B)</bold> Each time step <italic>T</italic><sub><italic>steps</italic></sub> of the stimulus is processed separately. The results are collected to form the columns of the matricx <italic>H</italic><sub><italic>g</italic></sub>. <bold>(C)</bold> Each of the SC vectors (columns) of <italic>H</italic><sub><italic>g</italic></sub> are processed by the harmonic sieve separately to produce the pitch probability of that time step (<italic>P</italic><sub><italic>g</italic></sub>). <bold>(D)</bold> To compare between simulations, we average over the time steps to extract the most prominent pitch of the signal. The result is the usual pdf vector, and the estimated pitch is set to the maximum of this pdf.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g014" xlink:type="simple"/>
</fig>
<p>We repeated this procedure with recorded notes of a flute, a violin, and a piano (<xref ref-type="fig" rid="pcbi.1005338.g015">Fig 15</xref>). All results are shown on a chromatic scale. Each dot in <xref ref-type="fig" rid="pcbi.1005338.g015">Fig 15</xref> is the estimated pitch of a recorded instrument; the colored text indicates the played note.</p>
<fig id="pcbi.1005338.g015" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005338.g015</object-id>
<label>Fig 15</label>
<caption>
<title>Results for musical notes on a chromatic scale.</title>
<p>We analyzed three musical instruments: a flute, a violin, and a piano for different notes. The results are shown on a chromatic musical scale (equal-tempered). The colored labels along the colored dots specify the notes played in specific recordings. All of the recordings were downloaded from [<xref ref-type="bibr" rid="pcbi.1005338.ref055">55</xref>]. Although not exact, the model does manage to assign most of the measurements to the right note (pitch).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005338.g015" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec014" sec-type="conclusions">
<title>Discussion</title>
<p>We showed that a model based on the sparse coding of the spatiotemporal pattern of auditory nerve responses is consistent with many pitch perception phenomena. The model represents input stimuli as sparse linear combinations of atoms, where each atom is derived from the AN population response to a pure tone.</p>
<p>Since the perception of pitch can be elicited by a variety of different stimuli [<xref ref-type="bibr" rid="pcbi.1005338.ref031">31</xref>], we tested the model on various such categories. We demonstrated that the sparse representation arising from a given stimulus at different sound levels could be linked to the spectral components of that stimulus, giving rise to a level-invariant representation of a pitch. Resolved and unresolved stimuli lead to a different pitch estimate in the model, with the difference stemming directly from cochlear properties. Inharmonic stimuli were used to show that the model can generalize to new stimuli while relying neither on the spacing between harmonics nor the temporal envelope of the stimulus. Next, we demonstrate that the use of the ANs spatiotemporal patterns as atoms force a tonotopic structure into the model. Consequently, it cannot estimate transposed tones (TTs), in accordance with known psychoacoustic measurements [<xref ref-type="bibr" rid="pcbi.1005338.ref013">13</xref>]. We showed that the model complies with IRN stimuli, and it is also able to process the recorded sounds of musical notes.</p>
<p>The focus of this paper was the application of sparse coding to the problem of pitch perception. The particular choice of the supporting elements used here (i.e., cochlear model, pitch estimation unit, and LASSO) are somewhat arbitrary. First, we chose standard, biologically inspired, cochlear models [<xref ref-type="bibr" rid="pcbi.1005338.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref029">29</xref>]. Second, we implemented the sparse coding (SC) algorithm by a known algorithm with available implementations, the LASSO [<xref ref-type="bibr" rid="pcbi.1005338.ref032">32</xref>]. Other plausible choices are presented in the literature, such as matching pursuit algorithms [<xref ref-type="bibr" rid="pcbi.1005338.ref025">25</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref056">56</xref>] (see [<xref ref-type="bibr" rid="pcbi.1005338.ref057">57</xref>] for different implementation in the auditory system). Third, the final pitch-estimation phase was implemented as an instance of the commonly used [<xref ref-type="bibr" rid="pcbi.1005338.ref058">58</xref>] harmonic sieve (via pattern-matching models). This construction enables an algorithm-level view [<xref ref-type="bibr" rid="pcbi.1005338.ref059">59</xref>] of the topic at hand.</p>
<sec id="sec015">
<title>The SC Model and Related Models for Pitch</title>
<p>The SC model presented here combines both spatial and temporal aspects of the AN population response. On the one hand, the SC model is based on atoms that have limited spatial support, namely, the nonzero section along the BM that is given by the equivalent rectangular bandwidth of the cochlea [<xref ref-type="bibr" rid="pcbi.1005338.ref060">60</xref>]. On the other hand, each atom also includes temporal information about the activities of the AN fibers at that spatial location.</p>
<p>The current model is structured in a similar manner to pattern-matching models [<xref ref-type="bibr" rid="pcbi.1005338.ref005">5</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref016">16</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref018">18</xref>]. The Fourier-like spectrum analyzer that extracts the resolved harmonic components of a given stimulus [<xref ref-type="bibr" rid="pcbi.1005338.ref018">18</xref>] is modified to include both the cochlea and the SC modules. Both models use templates to associate an estimated pitch with presented stimuli [<xref ref-type="bibr" rid="pcbi.1005338.ref058">58</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref061">61</xref>]. Despite these similarities, there is a fundamental difference between pattern-matching models and the current one. The atoms (templates) used in this study contain both spatial and temporal activities (<xref ref-type="fig" rid="pcbi.1005338.g003">Fig 3</xref>). Consequently, although the patterns of the AN population activities may be spectrally unresolved, there is still enough spatiotemporal information for identifying the different harmonics (<xref ref-type="fig" rid="pcbi.1005338.g009">Fig 9</xref>). Additionally, the SC does not rely on synthetic bases such as the sine and cosine of the Fourier transform (FT) but actual AN activities. Of course, these bases are also unresolved for high tones which means that the model exhibits less salience with unresolved stimuli, again, in line with psychoacoustic experiments [<xref ref-type="bibr" rid="pcbi.1005338.ref013">13</xref>]. Note that since the SC model implicitly inherits this property for high tones from the known attributes of the cochlear model, there is no need to add this feature into the model explicitly.</p>
<p>Historically, models of pitches were linked to harmonic analysis theories. Thus, it is instructive to note the mathematical connection between the SC model and the FT. Indeed, for the particular case of deprecating the dictionary matrix <bold>D</bold> into a square matrix, and setting λ = 0, the optimal solution for any periodic signal <bold>x</bold> of <xref ref-type="disp-formula" rid="pcbi.1005338.e003">Eq 2</xref> is given by the FT. In this case, the optimal solution is for <bold>h</bold> to be the coefficients of the FT and <bold>D</bold> its matrix [<xref ref-type="bibr" rid="pcbi.1005338.ref062">62</xref>]. In the SC case, applying λ ≠ 0 enables the use of biologically oriented, non-orthogonal, and redundant dictionaries [<xref ref-type="bibr" rid="pcbi.1005338.ref063">63</xref>]. In this sense, the current proposed model can be seen as an extension of classic pattern-matching models.</p>
<p>How does the SC model compare with temporal ones? Traditional temporal models, the most prominent of which are based on the summary autocorrelation function (SACF), also exploit temporal features from the AN population responses [<xref ref-type="bibr" rid="pcbi.1005338.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref003">3</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref064">64</xref>]. However, there are several open issues with these type of models. First, SACF models need to have long tapped delay lines for the correlation module, i.e., about 40ms and maybe more if noise is accounted for [<xref ref-type="bibr" rid="pcbi.1005338.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref065">65</xref>]. Currently, however, there is no physiological evidence to support such structures [<xref ref-type="bibr" rid="pcbi.1005338.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref031">31</xref>]. In contrast, the SC model exploits local spatiotemporal features without the need for long tapped delay lines. For that reason and to keep the model biologically plausible, we chose to use only short time segments of 5ms (see [<xref ref-type="bibr" rid="pcbi.1005338.ref066">66</xref>]). Different temporal interval durations were also tested. For shorter time intervals, e.g., 2ms, the SC model acts as a place-rate model; i.e., it managed to estimate only low resolved signals by their spatial activities along the BM. Longer time intervals also improved the model predictions for unresolved stimuli, up to a maximal estimation of about 10ms. Another issue associated with temporal models is that they treat resolved and unresolved frequencies in a similar manner in contrast to known psychoacoustic measurements [<xref ref-type="bibr" rid="pcbi.1005338.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref014">14</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref046">46</xref>]. This may imply that tonotopic organization is not necessary for auditory processing, but, again, physiological evidence suggests otherwise [<xref ref-type="bibr" rid="pcbi.1005338.ref013">13</xref>]. Tonotopic organization is preserved in the auditory system up to the auditory cortex for all mammals [<xref ref-type="bibr" rid="pcbi.1005338.ref067">67</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref070">70</xref>], and current evidence suggests that pitch processing is also sensitive to it [<xref ref-type="bibr" rid="pcbi.1005338.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref014">14</xref>].</p>
<p>Recently, Laudanski et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref071">71</xref>] proposed a structural theory of pitch that considers both the spatial and the temporal aspects of the AN population response. Within this framework, the perception of pitch is derived from correlated activity in pairs of points in the spatiotemporal representation of AN activities. These two points are not necessarily located along the temporal activity of the same AN fiber (pure temporal processing), nor between different AN fibers at a particular time (pure spatial processing).</p>
<p>Both the structural theory and the proposed SC model are strongly related. In the SC model we incorporate the so-called cross-channel delays of the structural theory in the spatiotemporal patterns of the atoms. Specifically, cross-channel delays of a stimulus are compared to other cross-channel delays of the model that are embedded in the atoms. We think, however, that approaching the problem of pitch estimation from the SC aspect offers considerable benefits. First, the SC approach provides a mathematical framework that generalizes to other modalities whereas the structural theory approach offers a more specific pointwise approach [<xref ref-type="bibr" rid="pcbi.1005338.ref023">23</xref>]. Second, cross-channel delays of the structural theory can be simply acquired under the SC method by using predesigned atoms (as shown in this paper) or unsupervised training of atoms (see [<xref ref-type="bibr" rid="pcbi.1005338.ref036">36</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref072">72</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref074">74</xref>] to name just a few). These techniques were already tested, including within the auditory system [<xref ref-type="bibr" rid="pcbi.1005338.ref057">57</xref>], with great success. Thus, the SC framework can explain different possible options for such cross-channel correlations.</p>
<p>Other theories and models that exploit the spatial, the temporal, or both, include: the spatial cross-correlation theory of Loeb et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref075">75</xref>]; de Cheveigne’s solution for the problem of tapped delay lines in temporal theories [<xref ref-type="bibr" rid="pcbi.1005338.ref065">65</xref>]; Carney’s model of phase-opponency [<xref ref-type="bibr" rid="pcbi.1005338.ref076">76</xref>], Shamma et al.’s lateral-inhibition and cross-correlation-matrix model [<xref ref-type="bibr" rid="pcbi.1005338.ref077">77</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref079">79</xref>], and the MASD of Cedolin et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref080">80</xref>]. Note, however, that, these models consider only a small subset of the whole two-dimensional spatiotemporal structure created by the AN fibers. For example, Loeb et al. proposed comparing two locations along the BM that vibrate with the same phase, that is, a spatial comparison without the time domain. De Cheveigne proposed to compensate phase shifts between adjacent cochlear filters, i.e., extracting temporal lags and discarding the spatial information; and Cedolin et al. proposed a model that is based on spatial derivation between cochlear filters with a temporal summation, namely, accounting for the differences between two adjacent cochlear filters and averaging over the time domain. Additionally, these models account well for resolved stimuli but not for unresolved ones [<xref ref-type="bibr" rid="pcbi.1005338.ref071">71</xref>].</p>
</sec>
<sec id="sec016">
<title>The Pitch Estimation Module</title>
<p>The use of the harmonic sieve can be considered from different perspectives. First, from the probabilistic point of view it can be seen as an implementation of a likelihood function: the probability of a particular pitch given a set of (parsimonious) coefficients <bold>h</bold>. This approach originates from the pattern-matching theory, and since the proposed model can be seen as an extension to the pattern-matching models, the same theoretical and experimental motivations also apply here. For example, following Goldstein et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref016">16</xref>], we chose the templates of the harmonic sieve to be Gaussian functions [<xref ref-type="bibr" rid="pcbi.1005338.ref016">16</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref040">40</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref058">58</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref081">81</xref>]. It might be that for different dictionaries, e.g., dictionaries that contain harmonic stacks, there are better options, but we were not concern with optimizing this feature in this paper.</p>
<p>Second, from the physiological perspective, the harmonic sieve can be thought of as a simple feedforward neural network. In such instance, a set of Gaussian templates of one tone (one row in the matrix G) can be seen as a neuron with a modulated selectivity curve, i.e., a neuron that responds to a particular tone and its successive octaves. For examples of such implementations, see [<xref ref-type="bibr" rid="pcbi.1005338.ref061">61</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref082">82</xref>]. Finally, the harmonic sieve can be considered as a simple (i.e., linear) readout function that extracts the perceived pitch from the activities of the spatiotemporal receptive fields and introduces it in a manner that enables an easy comparison with psychoacoustic data.</p>
<p>Third, from the biological perspective, it had been shown [<xref ref-type="bibr" rid="pcbi.1005338.ref083">83</xref>] that harmonic templates of this sort can emerge naturally from basic processing in the auditory periphery. Specifically, Shamma et al. demonstrated that the fundamental features include: frequency analysis, fast changing delays at the CFs, phase-locking, and half-wave rectification. All of these properties part of the cochlear models that we used.</p>
</sec>
<sec id="sec017">
<title>The Dictionary</title>
<p>In this paper, we checked two types of dictionary families: the first is created by sine stimuli while the second is created by stimuli of harmonic tones (i.e., harmonic stacks). These dictionary types were constructed and tested for various amplitude levels. Choosing a dictionary can influence the congruity of the SC model’s results with psychoacoustic measurements: the sensitivity of the model to resolved and unresolved stimuli, the response to low and high stimuli levels, etc. All these features emerge from the cochlear properties that are encapsulated within the atoms. Thus, an important question is which dictionary can be acquired in a biologically compelling manner and will best match psychoacoustic measurements? We intend to investigate this interesting question further in a future paper. However, in this paper we focused on the main premise—that parsimonious representation of auditory information can explain relatively high cognitive tasks, such as that of the percept of a pitch.</p>
<p>Accordingly, we chose to work, for the most part, with a simple dictionary of pure tones. Indeed, pure sinusoidal stimuli are rare in our natural acoustic surroundings [<xref ref-type="bibr" rid="pcbi.1005338.ref084">84</xref>] and there is no guarantee that the auditory system has access to such components at all. However, note that there is a subtle difference between plausible stimuli for a (hypothetical) training process and the outcome of this process, the atoms themselves. This is an important distinction because it implies that it is reasonable to assume that a learning process over natural-like stimuli, for example, vowels and consonants, can yield local spatiotemporal atoms and not necessarily stack-like atoms. Moreover, there is circumstantial physiological evidence to support similar spatiotemporal structures along different areas in the auditory system. For example, Norman-Haignere et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref085">85</xref>] researched specific regions in the anterior auditory cortex that strongly react to resolved harmonic tones, and, to a lesser extent, to unresolved ones. Additionally, Carney et al. found, in the anteroventral cochlear nucleus [<xref ref-type="bibr" rid="pcbi.1005338.ref066">66</xref>], cells that have distinct spatiotemporal tuning patterns in response to pure tones.</p>
<p>Acquiring a dictionary directly from the AN population response is not the necessarily the only implementation nor is it the optimal one. A different approach that has been successfully applied and has many variants [<xref ref-type="bibr" rid="pcbi.1005338.ref024">24</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref037">37</xref>] is to perform an unsupervised training from a randomly selected stimulus set that represents sampled statistics of the input domain. These unsupervised learning algorithms create dictionaries that are oriented to specific kind of inputs, for example, natural sounds [<xref ref-type="bibr" rid="pcbi.1005338.ref073">73</xref>]. Subsequently, they usually generate dictionaries that lead to sparser and more accurate results. It is thus important to realize that the proposed SC technique is not limited to a particular set of atoms (see for example Ch. 12 in [<xref ref-type="bibr" rid="pcbi.1005338.ref025">25</xref>]).</p>
</sec>
<sec id="sec018">
<title>Plausible Implementations by Artificial Neural Networks</title>
<p>Throughout this paper, we tried to keep the discussion at the representational level[<xref ref-type="bibr" rid="pcbi.1005338.ref059">59</xref>]. Accordingly, we did not introduce a plausible neural network mechanism to concentrate on what we saw as the central theme of the current paper—the generalized principle of using sparse representation also for high perception tasks—such as the estimation of pitches. We felt sufficiently confident to follow such a path because the current literature already includes several plausible neural network implementations of sparse coding [<xref ref-type="bibr" rid="pcbi.1005338.ref086">86</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref090">90</xref>] Another important point is that we did not introduce a state-of-the-art solution but a qualitative one. For this reason, the current model operates on stimuli with fixed time intervals. One relatively simple and standard technique to alleviate this restriction is by running the same model in consecutive times (for example, [<xref ref-type="bibr" rid="pcbi.1005338.ref073">73</xref>]).</p>
</sec>
<sec id="sec019">
<title>About the Normalization of the AN Response</title>
<p>In the current paper, we normalized all AN population activities. By normalization we mean that we divided each AN simulated result by its maximum response. The normalization that we propose stems from the assumption that relevant information about the pitch is related to the overall spatiotemporal structures of the AN population responses and not their absolute instantaneous rate level. The problem of keeping the estimation of the pitch invariant to the stimulus’ level is due to other deformations and nonlinearities in the AN responses: saturation in the activity of the AN fibers; change in the locations of peaks of ANs population activities; or the relative phases between the different AN fibers [<xref ref-type="bibr" rid="pcbi.1005338.ref031">31</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref045">45</xref>]. It is important to stress that this normalization neither changes nor corrects these effects.</p>
<p>It is worthwhile noting that normalization has been observed across the central neural system in general and in the auditory system in particular [<xref ref-type="bibr" rid="pcbi.1005338.ref091">91</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref092">92</xref>]. This stems from the fact the different modalities need to process a large dynamic range of stimuli, whether it is brightness in the visual sensors or a change of few orders of magnitude in the level of the sound stimulus.</p>
</sec>
</sec>
<sec id="sec020" sec-type="conclusions">
<title>Conclusion</title>
<p>We showed that sparse coding principles that were successfully applied to other modalities can explain pitch perception. This general approach of a parsimonious representation of the sensory information is the main premise of this paper and this finding resonates with ideas of a canonical computation by the nervous system [<xref ref-type="bibr" rid="pcbi.1005338.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref074">74</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref093">93</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref098">98</xref>]. Specifically, sparse representation of information can explain neural activity in the visual cortex [<xref ref-type="bibr" rid="pcbi.1005338.ref024">24</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref034">34</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref095">95</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref099">99</xref>]; the olfactory system of insects [<xref ref-type="bibr" rid="pcbi.1005338.ref100">100</xref>,<xref ref-type="bibr" rid="pcbi.1005338.ref101">101</xref>]; and findings in the mammalian auditory cortex [<xref ref-type="bibr" rid="pcbi.1005338.ref102">102</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref105">105</xref>] (however, see [<xref ref-type="bibr" rid="pcbi.1005338.ref106">106</xref>]). Hopefully, this paper is one small step in searching for such a generalized theory.</p>
</sec>
<sec id="sec021" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec022">
<title>The Cochlear Model</title>
<p>We used two types of cochlear models: (i) Slaney's MATLAB auditory toolbox [<xref ref-type="bibr" rid="pcbi.1005338.ref026">26</xref>] and (ii) the more moderate model of Zilany et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref029">29</xref>]. We used Slaney’s model for most of the simulations in this paper because it is relatively accurate and computationally fast for moderate sound levels. This model, however, does not account for the high amplitude level nonlinearities of the cochlea; for this phenomenon we used Zilany’s et al. model (Figs <xref ref-type="fig" rid="pcbi.1005338.g005">5</xref> and <xref ref-type="fig" rid="pcbi.1005338.g006">6</xref>). In Zilany’s model, the parameters were set as: (1) the outer and inner hair cells were taken to be in healthy condition; (2) the fractional Gaussian noise that is related to the spike rate generated by each AN was approximated in order to save computation time; (3) the model was originally built to match the AN population response of the cat, but we used the built-in option to tune it to the human cochlea [<xref ref-type="bibr" rid="pcbi.1005338.ref060">60</xref>]; and (4), only high spontaneous rate AN fibers were used.</p>
<p>For both cochlear models, the input stimulus and the simulation of the AN fibers were performed with a sampling rate of F<sub>S</sub> = 100k Hz; both had the same number of AN fibers (N = 200). The simulations of the AN population responses were performed as follows: a 15ms stimulus, <inline-formula id="pcbi.1005338.e022"><alternatives><graphic id="pcbi.1005338.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, is constructed and the AN population response is calculated using the chosen model. Finally, the number of samples is T<sub>a</sub> = 15ms∙F<sub>S</sub> (rounded to an integer if necessary).</p>
<p>The output of the cochlear model is a matrix of time-samples over the number of AN fibers, i.e., T<sub>a</sub> × N. This matrix is then truncated to contain only the last 5ms samples, i.e., <bold>S</bold><sub><italic>AN</italic></sub>(<italic>t</italic>,<italic>f</italic><sub><italic>CF</italic></sub>) ∈ <italic>R</italic><sup><italic>T</italic>×<italic>N</italic></sup> and T = 5ms∙F<sub>S</sub>. Lastly, <bold>S</bold><sub><italic>AN</italic></sub>(<italic>t</italic>,<italic>f</italic><sub><italic>CF</italic></sub>) is normalized by its maximum value (0 &lt; |<bold>S</bold><sub><italic>AN</italic></sub>(<italic>t</italic>,<italic>f</italic><sub><italic>CF</italic></sub>)|&lt;1). Since the model of Zilany et al. has upper and lower bounds on possible simulated CFs, between 125Hz to 20k Hz, all simulations were performed within this frequency interval. Finally, following the usual SPL format, the stimulus levels are introduced into the cochlea in pascal units normalized by the threshold of hearing, i.e., <inline-formula id="pcbi.1005338.e023"><alternatives><graphic id="pcbi.1005338.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:msub><mml:mtext>g</mml:mtext><mml:mrow><mml:mtext>Pa</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mi mathvariant="normal">μ</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mtext>g</mml:mtext><mml:mrow><mml:mtext>dB</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec023">
<title>The Sparse Coding Model</title>
<p>Each of the atoms <bold>d</bold><sub><italic>j</italic></sub> ∈ <italic>R</italic><sup><italic>T</italic>⋅<italic>N</italic>×1</sup>, j ∈ [1, M], in <bold>D</bold> are taken as the AN population response to a pure tone. Each <bold>d</bold><sub><italic>j</italic></sub> is the vector form of the respective AN population response matrix, i.e., the vectorized form. The vectorization is performed using MATLAB’s convention of stacking the matrix’s columns one after the other.</p>
<p>The dictionary matrix <bold>D</bold> = [<bold>d</bold><sub>1</sub>,…,<bold>d</bold><sub><italic>M</italic></sub>] ∈ <italic>R</italic><sup><italic>T</italic>⋅<italic>N</italic>×<italic>gM</italic></sup> contains M = 1000 atoms for each group g. The groups are collection of atoms with the same CFs but with different phases (<xref ref-type="disp-formula" rid="pcbi.1005338.e004">Eq 3</xref>). Note that <bold>D</bold> is a rectangular (N∙T ≫ M), highly redundant, matrix. The number of atoms was set arbitrarily by trial-and-error. From our experience, fewer atoms (e.g., M = 250), also yielded reasonable results.</p>
<p>In this paper we chose to implement the SC (<xref ref-type="disp-formula" rid="pcbi.1005338.e003">Eq 2</xref>) by means of the least absolute shrinkage and selection (LASSO) algorithm [<xref ref-type="bibr" rid="pcbi.1005338.ref032">32</xref>]. We did so because it has a simple implementation, has a relatively acceptable running time, and usually yields good results.</p>
<p>The implementation of the LASSO involves an iterative solution derived by gradient descent. Specifically, the vector <bold>h</bold> is the solution of the following iterated equation:
<disp-formula id="pcbi.1005338.e024">
<alternatives>
<graphic id="pcbi.1005338.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e024" xlink:type="simple"/>
<mml:math display="block" id="M24">
<mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>soft</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>α</mml:mi></mml:mfrac><mml:msup><mml:mi mathvariant="bold">D</mml:mi><mml:mtext>T</mml:mtext></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mrow><mml:mtext>AN</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">D</mml:mi><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula></p>
<p>In this equation, <bold>v</bold><sub>AN</sub> is the vectorized form of the AN population response <bold>S</bold><sub><italic>AN</italic></sub>(<italic>t</italic>,<italic>f</italic><sub><italic>CF</italic></sub>) (after vectorization and normalization), and the operator soft(<italic>x</italic>,<italic>T</italic>) = <italic>sign</italic>(<italic>x</italic>)max{0,|<italic>x</italic>| − <italic>T</italic>} is defined for each entry in the vector <bold>x</bold>. The algorithm runs until a convergent criterion is met or until a pre-set number of iterations is exceeded. For the algorithm to converge, the parameter α should maintain a certain condition (<italic>α</italic> ≥ max eig(<bold>D</bold><sup>T</sup><bold>D</bold>)). For the simulations in this paper we used the LASSO implementation within MATLAB Inc. [<xref ref-type="bibr" rid="pcbi.1005338.ref107">107</xref>]. We also created a tweaked version of this algorithm, but there was no substantial difference between the two implementations.</p>
</sec>
<sec id="sec024">
<title>The Pitch Estimation Unit</title>
<p>The pitch estimation unit is a variant of the known harmonic sieve [<xref ref-type="bibr" rid="pcbi.1005338.ref058">58</xref>] implementation. It denotes the likelihood of a particular pitch given the sparse coefficient vector <bold>h</bold> ∈ <italic>R</italic><sup><italic>M</italic></sup>. We implemented it as a multiplication between <inline-formula id="pcbi.1005338.e025"><alternatives><graphic id="pcbi.1005338.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and the matrix <bold>G</bold> ∈ <italic>R</italic><sup><italic>P</italic>×<italic>P</italic></sup> (<xref ref-type="disp-formula" rid="pcbi.1005338.e006">Eq 4</xref>) in which P = 15M, interpolating from the M values of <bold>h</bold>.</p>
<p>Each row in <bold>G</bold> corresponds to a candidate pitch f<sub><italic>p</italic></sub> ∈ [125<italic>Hz</italic>,20<italic>kHz</italic>] and is composed of a set of Gaussian weights at successive harmonics for this particular pitch, i.e.,
<disp-formula id="pcbi.1005338.e026">
<alternatives>
<graphic id="pcbi.1005338.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e026" xlink:type="simple"/>
<mml:math display="block" id="M26">
<mml:mrow><mml:msub><mml:mi mathvariant="bold">G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mi>k</mml:mi><mml:mrow/></mml:munderover><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula></p>
<p>The index <italic>i</italic> is taken from 1 to ⌊(20<italic>k</italic>−125)/<italic>σ</italic><sub><italic>p</italic></sub>⌋. Thus, if the sparse vector <bold>h</bold> has a coefficient that relates to the harmonics of the <italic>f</italic><sub><italic>i</italic></sub> pitch, multiplying by the matrix <bold>G</bold> would emphasize (give high score) to that entry in the <italic>pdf</italic>. Otherwise, if <bold>h</bold> does not contain harmonics that relate to the <italic>f</italic><sub><italic>i</italic></sub> pitch, the result would be a low score in the <italic>pdf</italic>. The standard deviations of each of the Gaussian curves in <bold>G</bold> is a function of <italic>f</italic><sub><italic>i</italic></sub>,
<disp-formula id="pcbi.1005338.e027">
<alternatives>
<graphic id="pcbi.1005338.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e027" xlink:type="simple"/>
<mml:math display="block" id="M27">
<mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mrow/></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn><mml:mo>⋅</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>B</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn><mml:mo>⋅</mml:mo><mml:mn>24.7</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>4.37</mml:mn><mml:mspace width="0.25em"/><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula></p>
<p>In this equation, <italic>ERB</italic> stands for the equivalent rectangular bandwidth of Glasberg and Moore [<xref ref-type="bibr" rid="pcbi.1005338.ref060">60</xref>]. We tried different variations of <italic>σ</italic><sub><italic>i</italic></sub>, including piecewise curves, all with relatively similar qualitative results. The main constraint was to avoid overlap between the Gaussian distributions and to keep adjacent Gaussian curves wide enough to account for noise in the pitch cues in <inline-formula id="pcbi.1005338.e028"><alternatives><graphic id="pcbi.1005338.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mover accent="true"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. Finally, using this scheme we only had to calculate the matrix <bold>G</bold> once, making the algorithm relatively efficient and fast.</p>
</sec>
<sec id="sec025">
<title>Effect of Different Dictionaries</title>
<p>The dictionary <italic>D</italic><sub>sine</sub> ∈ <italic>R</italic><sup><italic>T</italic>⋅<italic>N</italic>×<italic>M</italic></sup> is constructed with stimuli of one tone. It has N = 300 CF channels and M = 1500 atoms, each formed by a tone stimuli of uniformly selected amplitudes over the interval of 30dB to 70dB SPL (<italic>g</italic><sub>sine</sub>∼unif(30<sub><italic>dB</italic></sub>,70<sub><italic>dB</italic></sub>)),
<disp-formula id="pcbi.1005338.e029">
<alternatives>
<graphic id="pcbi.1005338.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e029" xlink:type="simple"/>
<mml:math display="block" id="M29">
<mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">sine</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi mathvariant="normal">sine</mml:mi></mml:mrow><mml:mrow/></mml:msubsup><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">π</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mtext>f</mml:mtext><mml:mn>0</mml:mn></mml:msub><mml:mtext>k</mml:mtext><mml:mo>⋅</mml:mo><mml:mtext>t</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula></p>
<p>The dictionary <italic>D</italic><sub><italic>stack</italic></sub> has 200 CF channels and 1000 atoms. Each of these atoms contains a stimulus of six consecutive harmonics (1<sup>st</sup>-6<sup>th</sup>); these harmonics have linearly decreasing amplitudes (from 1 to 1/6),
<disp-formula id="pcbi.1005338.e030">
<alternatives>
<graphic id="pcbi.1005338.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e030" xlink:type="simple"/>
<mml:math display="block" id="M30">
<mml:mrow><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>6</mml:mn></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>k</mml:mi></mml:mfrac><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mtext>f</mml:mtext><mml:mn>0</mml:mn></mml:msub><mml:mtext>k</mml:mtext><mml:mo>⋅</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>All of the <italic>D</italic><sub><italic>stack</italic></sub> atoms are formed by <italic>g</italic><sub><italic>stack</italic></sub> = 40<sub><italic>dB</italic></sub> <italic>SPL</italic> level stimuli. All simulations were made using Carney's model (Zilany's et al.[<xref ref-type="bibr" rid="pcbi.1005338.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref029">29</xref>]) to account for these high amplitude levels. In order to avoid aliasing in the spatiotemporal domain, the maximum frequency in <italic>D</italic><sub><italic>stack</italic></sub> is the maximum frequency (20k Hz) divided by the number of harmonics (6).</p>
</sec>
<sec id="sec026">
<title>Transposed Tones</title>
<p>The transposed tone stimuli in this paper are modulated rectified tones with three carrier waves: f<sub>c,1</sub> = 4k Hz, f<sub>c,2</sub> = 6.35k Hz, and f<sub>c,3</sub> = 10.08k Hz (<xref ref-type="disp-formula" rid="pcbi.1005338.e016">Eq 9</xref>). The modulated tone is given by
<disp-formula id="pcbi.1005338.e031">
<alternatives>
<graphic id="pcbi.1005338.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e031" xlink:type="simple"/>
<mml:math display="block" id="M31">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="normal">s</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">π</mml:mi><mml:mtext>f</mml:mtext></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>*</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula></p>
<p>In this equation, the rectification operator is given as [<italic>x</italic>]<sup>+</sup> = max(0,<italic>x</italic>). The operation * is a convolution, and <italic>h</italic><sub><italic>LP</italic>,<italic>k</italic></sub>(<italic>t</italic>) is a four-order Butterworth low-pass filter (see Oxenham et al. paper [<xref ref-type="bibr" rid="pcbi.1005338.ref013">13</xref>] for more details). The cutoff frequencies of the low-pass filter is taken as 0.2f<sub>c,k</sub> for each of the three modulated frequencies (k = {1,2,3}).</p>
</sec>
<sec id="sec027">
<title>Iterated Rippled Noise</title>
<p>Following Yost et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref042">42</xref>], all the stimuli have amplitude levels of 70dB SPL. To account for this amplitude level, all simulations were performed using Carney's model (Zilany et al. [<xref ref-type="bibr" rid="pcbi.1005338.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1005338.ref029">29</xref>]). To account for the random phases of the stimuli we used <italic>g</italic> = 10 groups (<xref ref-type="disp-formula" rid="pcbi.1005338.e004">Eq 3</xref>). The same simulations with a dictionary that contains no groups, <italic>g</italic> = 1 and <italic>ϕ</italic><sub><italic>g</italic></sub> = 0, have been slightly noisier but qualitatively the same. Each of the six cases shown in <xref ref-type="fig" rid="pcbi.1005338.g013">Fig 13</xref> is a histogram of 500 simulations. All the repetition cases are normalized respectively. The maximum peaks of the pdfs are selected within an interval of one octave around the pitch frequency, 1/<italic>d</italic>. All simulations are performed using the same dictionary; this dictionary contains atoms that are 5ms long and have one tone at a level of 45dB SPL. Finally, all stimuli are filtered by a low-pass with a frequency band of 4k Hz [<xref ref-type="bibr" rid="pcbi.1005338.ref042">42</xref>].</p>
</sec>
<sec id="sec028">
<title>Musical Notes</title>
<p>We used a dictionary with 1000 sine-atoms of length 5ms. Each such sine-atom composed a group of 10 different phases (g = 10, <xref ref-type="disp-formula" rid="pcbi.1005338.e004">Eq 3</xref>). For these simulations, we used Slaney's MATLAB toolbox [<xref ref-type="bibr" rid="pcbi.1005338.ref108">108</xref>] as the cochlear model (much faster); the cochlea had 300 CF channels. We used <xref ref-type="disp-formula" rid="pcbi.1005338.e003">Eq 2</xref> for each of the <italic>T</italic><sub><italic>steps</italic></sub> = 100 timesteps separately. Each of the SC coefficient vectors <inline-formula id="pcbi.1005338.e032"><alternatives><graphic id="pcbi.1005338.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> was averaged and normalized appropriately into <bold>h</bold> ∈ <italic>R</italic><sup><italic>M</italic></sup> (each group separately). The collection of all these vectors, for all <italic>T</italic><sub><italic>steps</italic></sub>, formed the matrix <inline-formula id="pcbi.1005338.e033"><alternatives><graphic id="pcbi.1005338.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>Each of these SC vectors (the columns of <italic>H</italic><sub><italic>g</italic></sub>) are then processed by the harmonic sieve to produce the probability of pitch <inline-formula id="pcbi.1005338.e034"><alternatives><graphic id="pcbi.1005338.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005338.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> at each time step (<xref ref-type="fig" rid="pcbi.1005338.g014">Fig 14C</xref>). Finally, to have one single probability for each stimulus, the matrix <italic>P</italic><sub><italic>g</italic></sub> is averaged over the time domain and normalized appropriately (<xref ref-type="fig" rid="pcbi.1005338.g014">Fig 14D</xref>). As in previous cases, the pitch of the signal is defined as the maximum point in this pdf.</p>
<p>All measurements were downloaded from the University of Iowa, electronic music studio, from the musical instrument samples page [<xref ref-type="bibr" rid="pcbi.1005338.ref055">55</xref>].</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1005338.ref001"><label>1</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>De Cheveigne</surname> <given-names>A</given-names></name>. <source>Pitch perception models. in Pitch: Neural Coding and Perception</source>. <publisher-name>Springer</publisher-name>; <year>2005</year>. pp. <fpage>169</fpage>–<lpage>233</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meddis</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hewitt</surname> <given-names>M</given-names></name>. <article-title>Virtual pitch and phase sensitivity of a computer model of the auditory periphery. II: Phase sensitivity</article-title>. <source>J Acoust Soc Am</source>. <year>1991</year>;<volume>89</volume>: <fpage>2883</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meddis</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hewitt</surname> <given-names>MJ</given-names></name>. <article-title>Virtual pitch and phase sensitivity of a computer model of the auditory periphery. I: Pitch identification</article-title> [Internet]. <source>The Journal of the Acoustical Society of America</source>. <year>1991</year>. p. <fpage>2866</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meddis</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>O’Mard</surname> <given-names>L</given-names></name>. <article-title>A unitary model of pitch perception</article-title>. <source>J Acoust Soc Am</source>. <year>1997</year>;<volume>102</volume>: <fpage>1811</fpage>–<lpage>1820</lpage>. <object-id pub-id-type="pmid">9301058</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Terhardt</surname> <given-names>E</given-names></name>. <article-title>Pitch, consonance, and harmony</article-title>. <source>J Acoust Soc Am</source>. <year>1974</year>;<volume>55</volume>: <fpage>1061</fpage>–<lpage>1069</lpage>. <object-id pub-id-type="pmid">4833699</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Cheveignי</surname> <given-names>A</given-names></name>. <article-title>Pitch perception</article-title>. <source>Oxford Handb Audit Sci Hear</source>. <year>2005</year>; <fpage>71</fpage>–<lpage>104</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/oxfordhb/9780199233557.013.0004" xlink:type="simple">10.1093/oxfordhb/9780199233557.013.0004</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005338.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cariani</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Delgutte</surname> <given-names>B</given-names></name>. 1996 <article-title>Cariani—Neural correlates of the pitch of complex tones. I. Pitch and pitch salience.pdf</article-title>. <source>J Neurophysiol</source>. <year>1996</year>;<volume>76</volume>: <fpage>1698</fpage>–<lpage>1716</lpage>. <object-id pub-id-type="pmid">8890286</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cariani</surname> <given-names>P a</given-names></name>, <name name-style="western"><surname>Delgutte</surname> <given-names>B</given-names></name>. <article-title>Neural correlates of the pitch of complex tones. II. Pitch shift, pitch ambiguity, phase invariance, pitch circularity, rate pitch, and the dominance region for pitch</article-title>. <source>J Neurophysiol</source>. <year>1996</year>;<volume>76</volume>: <fpage>1717</fpage>–<lpage>34</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/8890287" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/8890287</ext-link> <object-id pub-id-type="pmid">8890287</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Plack</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>. <article-title>The Psychophysics of Pitch</article-title>. <source>Pitch Neural Coding Percept</source>. <year>2005</year>; <fpage>7</fpage>–<lpage>55</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meddis</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>O’Mard</surname> <given-names>LP</given-names></name>. <article-title>Virtual pitch in a computational physiological model</article-title>. <source>J Acoust Soc Am</source>. <year>2006</year>;<volume>120</volume>: <fpage>3861</fpage>–<lpage>3869</lpage>. <object-id pub-id-type="pmid">17225413</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carlyon</surname> <given-names>RP</given-names></name>. <article-title>Comparing the fundamental frequencies of resolved and unresolved harmonics: Evidence for two pitch mechanisms?</article-title> <source>J Acoust Soc Am</source>. <year>1994</year>;<volume>95</volume>: <fpage>3541</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carlyon</surname> <given-names>RP</given-names></name>. <article-title>Comments on “A unitary model of pitch perception” [J. Acoust. Soc. Am. 102, 1811–1820 (1997)]</article-title>. <source>J Acoust Soc Am</source>. <year>1998</year>;<volume>104</volume>: <fpage>1118</fpage>–<lpage>1121</lpage>. Available: d:%5CMyResearch%5CBib%5CReferences%5CCarlyon_JASA1998.pdf <object-id pub-id-type="pmid">9714929</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Bernstein</surname> <given-names>JGW</given-names></name>, <name name-style="western"><surname>Penagos</surname> <given-names>H</given-names></name>. <article-title>Correct tonotopic representation is necessary for complex pitch perception</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2004</year>;<volume>101</volume>: <fpage>1421</fpage>–<lpage>1425</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0306958101" xlink:type="simple">10.1073/pnas.0306958101</ext-link></comment> <object-id pub-id-type="pmid">14718671</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>. <article-title>Revisiting place and temporal theories of pitch</article-title>. <source>Acoust Sci Technol</source>. <year>2013</year>;<volume>34</volume>: <fpage>388</fpage>–<lpage>396</lpage>. <object-id pub-id-type="pmid">25364292</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pressnitzer</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Patterson</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Krumbholz</surname> <given-names>K</given-names></name>. <article-title>The lower limit of melodic pitch</article-title>. <source>J Acoust Soc Am</source>. <year>2001</year>;<volume>109</volume>: <fpage>2074</fpage>–<lpage>2084</lpage>. <object-id pub-id-type="pmid">11386559</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goldstein</surname> <given-names>JL</given-names></name>. <article-title>An optimum processor theory for the central formation of the pitch of complex tones</article-title>. <source>J Acoust Soc Am</source>. <year>1973</year>;<volume>54</volume>: <fpage>1496</fpage>–<lpage>1516</lpage>. <object-id pub-id-type="pmid">4780803</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Terhardt</surname> <given-names>E</given-names></name>. <article-title>Calculating virtual pitch</article-title>. <source>Hear Res</source>. <year>1979</year>;<volume>1</volume>: <fpage>155</fpage>–<lpage>182</lpage>. <object-id pub-id-type="pmid">521399</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wightman</surname> <given-names>FL</given-names></name>. <article-title>The pattern-transformation model of pitch</article-title>. <source>J Acoust Soc Am</source>. <year>1973</year>;<volume>54</volume>: <fpage>407</fpage>–<lpage>416</lpage>. <object-id pub-id-type="pmid">4759014</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goldstein</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Gerson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Srulovicz</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Furst</surname> <given-names>M</given-names></name>. <article-title>Verification of the optimal probabilistic basis of aural processing in pitch of complex tones</article-title>. <source>J Acoust Soc Am</source>. <year>1978</year>;<volume>63</volume>: <fpage>486</fpage>–<lpage>497</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/670546" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/670546</ext-link> <object-id pub-id-type="pmid">670546</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Micheyl</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Keebler M</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>. <article-title>Pitch perception for mixtures of spectrally overlapping harmonic complex tones</article-title>. <source>J Acoust Soc Am</source>. <year>2010</year>;<volume>128</volume>: <fpage>257</fpage>–<lpage>69</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1121/1.3372751" xlink:type="simple">10.1121/1.3372751</ext-link></comment> <object-id pub-id-type="pmid">20649221</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moore</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Psychological</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>BCJ</given-names></name>. <article-title>Frequency difference limens for short-duration tones</article-title>. <source>J Acoust Soc Am</source>. <year>1973</year>;<volume>54</volume>: <fpage>610</fpage>–<lpage>619</lpage>.: 10.1121/1.1913640 <object-id pub-id-type="pmid">4754385</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Houtsma</surname> <given-names>AJM</given-names></name>, <name name-style="western"><surname>Goldstein</surname> <given-names>JL</given-names></name>. <article-title>The Central Origin of the Pitch of Complex Tones: Evidence from Musical Interval Recognition</article-title>. <source>J Acoust Soc Am</source>. <year>1972</year>;<volume>51</volume>: <fpage>520</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1121/1.1912873" xlink:type="simple">10.1121/1.1912873</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005338.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname> <given-names>B a.</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Sparse coding of sensory inputs</article-title>. <source>Curr Opin Neurobiol</source>. <year>2004</year>;<volume>14</volume>: <fpage>481</fpage>–<lpage>487</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2004.07.007" xlink:type="simple">10.1016/j.conb.2004.07.007</ext-link></comment> <object-id pub-id-type="pmid">15321069</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname> <given-names>B a</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>: <fpage>607</fpage>–<lpage>609</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/381607a0" xlink:type="simple">10.1038/381607a0</ext-link></comment> <object-id pub-id-type="pmid">8637596</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elad</surname> <given-names>M</given-names></name>. <article-title>Sparse and Redundant Representations</article-title>. <source>Sparse Redundant Represent From Theory to Appl Signal Image Process</source>. <year>2010</year>; <fpage>359</fpage>–<lpage>361</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref026"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Slaney M. Auditory toolbox: A Matlab Toolbox for Auditory Modeling Work [Internet]. Interval Research Corporation, Tech. Rep. 1998. Available: <ext-link ext-link-type="uri" xlink:href="http://www.tka4.org/materials/lib/Articles-Books/SpeechRecognition/AuditoryToolboxTechReport.pdf" xlink:type="simple">http://www.tka4.org/materials/lib/Articles-Books/SpeechRecognition/AuditoryToolboxTechReport.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1005338.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zilany</surname> <given-names>MS a</given-names></name>., <name name-style="western"><surname>Bruce</surname> <given-names>IC</given-names></name>, <name name-style="western"><surname>Carney</surname> <given-names>LH</given-names></name>. <article-title>Updated parameters and expanded simulation options for a model of the auditory periphery</article-title>. <source>J Acoust Soc Am</source>. <year>2014</year>;<volume>135</volume>: <fpage>283</fpage>–<lpage>286</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1121/1.4837815" xlink:type="simple">10.1121/1.4837815</ext-link></comment> <object-id pub-id-type="pmid">24437768</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zilany</surname> <given-names>MS a</given-names></name>, <name name-style="western"><surname>Bruce</surname> <given-names>IC</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>PC</given-names></name>, <name name-style="western"><surname>Carney</surname> <given-names>LH</given-names></name>. <article-title>A phenomenological model of the synapse between the inner hair cell and auditory nerve: long-term adaptation with power-law dynamics</article-title>. <source>J Acoust Soc Am</source>. <year>2009</year>;<volume>126</volume>: <fpage>2390</fpage>–<lpage>412</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1121/1.3238250" xlink:type="simple">10.1121/1.3238250</ext-link></comment> <object-id pub-id-type="pmid">19894822</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref029"><label>29</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Ibrahim</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Bruce</surname> <given-names>IC</given-names></name>. <chapter-title>Effects of peripheral tuning on the auditory nerve’s representation of speech envelope and temporal fine structure cues</chapter-title>. <source>The neurophysiological bases of auditory perception</source>. <publisher-name>Springer</publisher-name>; <year>2010</year>. pp. <fpage>429</fpage>–<lpage>438</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hartmann</surname> <given-names>WM</given-names></name>. <article-title>Pitch, periodicity, and auditory organization</article-title>. <source>J Acoust Soc Am</source>. <year>1996</year>;<volume>100</volume>: <fpage>3491</fpage>–<lpage>502</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://link.aip.org/link/?JASMAN/100/3491/1" xlink:type="simple">http://link.aip.org/link/?JASMAN/100/3491/1</ext-link> <object-id pub-id-type="pmid">8969472</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schnupp</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>A</given-names></name>. <article-title>Audtiory Neuroscience: Making Sense of Sound</article-title>. <source>Audit Neurosci</source>. MIT Press; <year>2011</year>; <volume>347</volume>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>. <article-title>Regression Selection and Shrinkage via the Lasso [Internet]</article-title>. <source>Journal of the Royal Statistical Society B</source>. <year>1996</year>. pp. <fpage>267</fpage>–<lpage>288</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Donoho</surname> <given-names>DL</given-names></name>, <article-title>Saunders M a. Atomic Decomposition by Basis Pursuit</article-title>. <source>SIAM J Sci Comput</source>. <year>1998</year>;<volume>20</volume>: <fpage>33</fpage>–<lpage>61</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname> <given-names>B a.</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Sparse coding with an overcomplete basis set: A strategy employed by V1?</article-title> <source>Vision Research</source>. <year>1997</year>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aharon</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Elad</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bruckstein</surname> <given-names>A</given-names></name>. <article-title>K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation</article-title>. <source>IEEE Trans Signal Process</source>. <year>2006</year>;<volume>54</volume>: <fpage>4311</fpage>–<lpage>4322</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref036"><label>36</label><mixed-citation publication-type="other" xlink:type="simple">Mairal J, Bach F, Edu GUMN. Online Dictionary Learning for Sparse Coding. Int Conf Mach Learn. 2009;</mixed-citation></ref>
<ref id="pcbi.1005338.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Learning overcomplete representations</article-title>. <source>Neural Comput</source>. <year>2000</year>;<volume>12</volume>: <fpage>337</fpage>–<lpage>365</lpage>. <object-id pub-id-type="pmid">10636946</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sprechmann</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ramírez</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Sapiro</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Eldar</surname> <given-names>YC</given-names></name>, <name name-style="western"><surname>Ramirez</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Sapiro</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>C-HiLasso: A Collaborative Hierarchical Sparse Modeling Framework</article-title>. <source>IEEE Trans Signal Process</source>. <year>2010</year>;<volume>59</volume>: <fpage>4183</fpage>–<lpage>4198</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref039"><label>39</label><mixed-citation publication-type="other" xlink:type="simple">Chi YT, Ali M, Rajwade A, Ho J. Block and group regularized sparse modeling for dictionary learning. Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. 2013; 377–382.</mixed-citation></ref>
<ref id="pcbi.1005338.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gerson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Goldstein</surname> <given-names>JL</given-names></name>. <article-title>Evidence for a general template in central optimal processing for pitch of complex tones</article-title>. <source>J Acoust Soc Am</source>. <year>1978</year>;<volume>63</volume>: <fpage>498</fpage>–<lpage>510</lpage>. <object-id pub-id-type="pmid">670547</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Micheyl</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Keebler</surname> <given-names>M V</given-names></name>, <name name-style="western"><surname>Loper</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Santurette</surname> <given-names>S</given-names></name>. <article-title>Pitch perception beyond the traditional existence region of pitch</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2011</year>;<volume>108</volume>: <fpage>7629</fpage>–<lpage>7634</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1015291108" xlink:type="simple">10.1073/pnas.1015291108</ext-link></comment> <object-id pub-id-type="pmid">21502495</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yost</surname> <given-names>WA</given-names></name>. <article-title>Pitch of iterated rippled noise</article-title>. <source>J Acoust Soc Am</source>. <year>1996</year>;<volume>100</volume>: <fpage>511</fpage>–<lpage>518</lpage>. <object-id pub-id-type="pmid">8675844</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miyazaki</surname> <given-names>K</given-names></name>. <article-title>Absolute pitch and its implications for music</article-title>. <source>Arch Acoust</source>. <year>2007</year>;<volume>32</volume>: <fpage>529</fpage>–<lpage>540</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref044"><label>44</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Moore</surname> <given-names>BCJ</given-names></name>. <source>An Introduction to the Psychology of Hearing</source>. <publisher-name>Bost Acad Press. Brill</publisher-name>; <year>2003</year>;<volume>3</volume>: <fpage>413</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carlyon</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Long</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Micheyl</surname> <given-names>C</given-names></name>. <article-title>Across-channel timing differences as a potential code for the frequency of pure tones</article-title>. <source>JARO—J Assoc Res Otolaryngol</source>. <year>2012</year>;<volume>13</volume>: <fpage>159</fpage>–<lpage>171</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shackleton</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Carlyon</surname> <given-names>RP</given-names></name>. <article-title>The role of resolved and unresolved harmonics in pitch perception and frequency modulation discrimination</article-title>. <source>J Acoust Soc Am</source>. <year>1994</year>;<volume>95</volume>: <fpage>3529</fpage>–<lpage>3540</lpage>. <object-id pub-id-type="pmid">8046144</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schouten</surname> <given-names>JF</given-names></name>. <article-title>Pitch of the Residue</article-title>. <source>J Acoust Soc Am</source>. <year>1962</year>;<volume>34</volume>: <fpage>1418</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>R a</given-names></name>., <name name-style="western"><surname>Williams</surname> <given-names>RP</given-names></name>. <article-title>Residue pitches from two-tone complexes</article-title>. <source>J Sound Vib</source>. Academic Press Inc. (London) Limited; <year>1970</year>;<volume>13</volume>: <fpage>195</fpage>–<lpage>199</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lin</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Hartmann</surname> <given-names>WM</given-names></name>. <article-title>The pitch of a mistuned harmonic: evidence for a template model</article-title>. <source>J Acoust Soc Am</source>. <year>1998</year>;<volume>103</volume>: <fpage>2608</fpage>–<lpage>2617</lpage>. <object-id pub-id-type="pmid">9604355</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bernstein</surname> <given-names>LR</given-names></name>. <article-title>Auditory processing of interaural timing information: New insights</article-title>. <source>J Neurosci Res</source>. <year>2001</year>;<volume>66</volume>: <fpage>1035</fpage>–<lpage>1046</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/jnr.10103" xlink:type="simple">10.1002/jnr.10103</ext-link></comment> <object-id pub-id-type="pmid">11746435</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yost</surname> <given-names>WA</given-names></name>. <article-title>Strength of the pitches associated with ripple noise</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1978</year>. pp. <fpage>485</fpage>–<lpage>492</lpage>. <object-id pub-id-type="pmid">712010</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bilsen</surname> <given-names>FA</given-names></name>, <name name-style="western"><surname>Ritsma</surname> <given-names>RJ</given-names></name>. <article-title>Repetition pitch and its implications for hearing theory</article-title>. <source>Acta Acust united with Acust 222</source>. S. Hirzel Verlag; <year>1969</year>;<volume>22</volume>: <fpage>63</fpage>–<lpage>73</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Patterson</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Handel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Yost</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Datta</surname> <given-names>AJ</given-names></name>. <article-title>The relative strength of the tone and noise components in iterated rippled noise</article-title>. <source>J Acoust Soc Am</source>. <year>1996</year>;<volume>100</volume>: <fpage>3286</fpage>–<lpage>3294</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref054"><label>54</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Riess</surname> <given-names>Jones M</given-names></name>, <name name-style="western"><surname>Fay</surname> <given-names>RR</given-names></name>, <name name-style="western"><surname>Popper</surname> <given-names>AN</given-names></name>, editors. <source>Music Perception</source> [Internet]. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer New York</publisher-name>; <year>2010</year>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref055"><label>55</label><mixed-citation publication-type="other" xlink:type="simple">University of Iowa, musical instrument samples [Internet]. Available: <ext-link ext-link-type="uri" xlink:href="http://theremin.music.uiowa.edu/MIS.html" xlink:type="simple">http://theremin.music.uiowa.edu/MIS.html</ext-link></mixed-citation></ref>
<ref id="pcbi.1005338.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Friedman</surname> <given-names>J</given-names></name>. <article-title>The Elements of Statistical Learning</article-title>. <source>Elements</source>. <year>2009</year>;<volume>1</volume>: <fpage>337</fpage>–<lpage>387</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>. <article-title>Efficient auditory coding</article-title>. <source>Nature</source>. <year>2006</year>;<volume>439</volume>: <fpage>978</fpage>–<lpage>982</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature04485" xlink:type="simple">10.1038/nature04485</ext-link></comment> <object-id pub-id-type="pmid">16495999</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Duifhuis</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Willems</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Sluyter</surname> <given-names>RJ</given-names></name>. <article-title>Measurement of pitch in speech: an implementation of Goldstein’s theory of pitch perception</article-title>. <source>J Acoust Soc Am</source>. <year>1982</year>;<volume>71</volume>: <fpage>1568</fpage>–<lpage>1580</lpage>. <object-id pub-id-type="pmid">7108032</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marr</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>From understanding computation to understanding neural circuitry</article-title> [Internet]. <source>AI Memo</source>. <year>1976</year>. pp. <fpage>1</fpage>–<lpage>22</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://mit.dspace.org/handle/1721.1/5782" xlink:type="simple">http://mit.dspace.org/handle/1721.1/5782</ext-link></mixed-citation></ref>
<ref id="pcbi.1005338.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Glasberg</surname> <given-names>BR</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>BC</given-names></name>. <article-title>Derivation of auditory filter shapes from notched-noise data</article-title>. <source>Hear Res</source>. <year>1990</year>;<volume>47</volume>: <fpage>103</fpage>–<lpage>138</lpage>. <object-id pub-id-type="pmid">2228789</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohen</surname> <given-names>M a</given-names></name>, <name name-style="western"><surname>Grossberg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wyse</surname> <given-names>LL</given-names></name>. <article-title>A spectral network model of pitch perception</article-title>. <source>J Acoust Soc Am</source>. <year>1995</year>;<volume>98</volume>: <fpage>862</fpage>–<lpage>79</lpage>. <object-id pub-id-type="pmid">7642825</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref062"><label>62</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sundararajan</surname> <given-names>D</given-names></name>. <source>The discrete Fourier transform: theory, algorithms and applications</source>. <publisher-name>World Scientific</publisher-name>; <year>2001</year>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>O’Connor</surname> <given-names>KN</given-names></name>. <article-title>A new window on sound</article-title>. <source>Nat Neurosci</source>. <year>2002</year>;<volume>5</volume>: <fpage>292</fpage>–<lpage>4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn0402-292" xlink:type="simple">10.1038/nn0402-292</ext-link></comment> <object-id pub-id-type="pmid">11914717</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Licklider</surname> <given-names>JCR</given-names></name>. <article-title>A duplex theory of pitch perception</article-title>. <source>Experientia</source>. <year>1951</year>;<volume>7</volume>: <fpage>128</fpage>–<lpage>134</lpage>. <object-id pub-id-type="pmid">14831572</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Cheveigné</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pressnitzer</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>de Cheveigné</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pressnitzer</surname> <given-names>D</given-names></name>. <article-title>The case of the missing delay lines: synthetic delays obtained by cross-channel phase interaction</article-title>. <source>J Acoust Soc Am</source>. <year>2006</year>;<volume>119</volume>: <fpage>3908</fpage>–<lpage>3918</lpage>. <object-id pub-id-type="pmid">16838534</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carney</surname> <given-names>LH</given-names></name>, <name name-style="western"><surname>Friedman</surname> <given-names>M</given-names></name>. <article-title>Spatiotemporal tuning of low-frequency cells in the anteroventral cochlear nucleus</article-title>. <source>J Neurosci</source>. <year>1998</year>;<volume>18</volume>: <fpage>1096</fpage>–<lpage>104</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/9437029" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/9437029</ext-link> <object-id pub-id-type="pmid">9437029</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref067"><label>67</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Butler</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Hodos</surname> <given-names>W</given-names></name>. <source>Comparative vertebrate neuroanatomy: evolution and adaptation</source>. <publisher-name>John Wiley &amp; Sons</publisher-name>; <year>2005</year>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref068"><label>68</label><mixed-citation publication-type="other" xlink:type="simple">Zatorre RJ. Auditory Cortex. 2002;1: 289–301.</mixed-citation></ref>
<ref id="pcbi.1005338.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Merzenich</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Knight</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Roth</surname> <given-names>GL</given-names></name>, <name name-style="western"><surname>MERZENPCH</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Knight</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Roth</surname> <given-names>GL</given-names></name>. <article-title>Representation of cochlea within primary auditory cortex in the cat</article-title>. <source>J Neurophysiol</source>. <year>1975</year>;<volume>38</volume>: <fpage>231</fpage>–<lpage>249</lpage>. <object-id pub-id-type="pmid">1092814</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bendor</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X</given-names></name>. <article-title>The neuronal representation of pitch in primate auditory cortex</article-title>. <source>Nature</source>. <year>2005</year>;<volume>436</volume>: <fpage>1161</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature03867" xlink:type="simple">10.1038/nature03867</ext-link></comment> <object-id pub-id-type="pmid">16121182</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laudanski</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zheng</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Brette</surname> <given-names>R</given-names></name>. <article-title>A Structural Theory of Pitch</article-title>. <source>eNeuro</source>. <year>2014</year>;<volume>1</volume>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mairal</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Elad</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sapiro</surname> <given-names>G</given-names></name>. <article-title>Sparse representation for color image restoration</article-title>. <source>IEEE Trans Image Process</source>. <year>2008</year>;<volume>17</volume>: <fpage>53</fpage>–<lpage>69</lpage>. <object-id pub-id-type="pmid">18229804</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grosse</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Raina</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Kwong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Ng</surname> <given-names>a Y</given-names></name>. <article-title>Shift-invariant sparse coding for audio classification</article-title>. <source>Cortex</source>. <year>2007</year>;<volume>9</volume>: <fpage>8</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saxe</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bhand</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Mudur</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Suresh</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Ng</surname> <given-names>AY</given-names></name>. <article-title>Unsupervised learning models of primary cortical receptive fields and receptive field plasticity</article-title>. <source>Adv Neural Inf Process Syst</source>. <year>2011</year>; <fpage>1</fpage>–<lpage>9</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://books.nips.cc/papers/files/nips24/NIPS2011_1115.pdf" xlink:type="simple">http://books.nips.cc/papers/files/nips24/NIPS2011_1115.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1005338.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loeb</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>White</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Merzenich</surname> <given-names>M</given-names></name>. <article-title>Spatial cross-correlation</article-title>. <source>Biol Cybern</source>. <year>1983</year>;<volume>163</volume>: <fpage>149</fpage>–<lpage>163</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.springerlink.com/index/H7555317M8271573.pdf" xlink:type="simple">http://www.springerlink.com/index/H7555317M8271573.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1005338.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carney</surname> <given-names>LH</given-names></name>, <name name-style="western"><surname>Heinz</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Evilsizer</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Gilkey</surname> <given-names>RH</given-names></name>, <name name-style="western"><surname>Colburn</surname> <given-names>HS</given-names></name>. <article-title>Auditory Phase Opponency: A Temporal Model for Masked Detection at Low Frequencies</article-title>. <source>Acta Acust</source>. <year>2002</year>;<volume>88</volume>: <fpage>334</fpage>–<lpage>347</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref077"><label>77</label><mixed-citation publication-type="other" xlink:type="simple">Shamma S, Klein D, Depireux D. Coincidence Detection in Pitch Perception. <ext-link ext-link-type="uri" xlink:href="http://theearlab.org" xlink:type="simple">theearlab.org</ext-link>. 2000; 1–7. Available: <ext-link ext-link-type="uri" xlink:href="http://theearlab.org/pubs/ISH00PitchTemplates.pdf" xlink:type="simple">http://theearlab.org/pubs/ISH00PitchTemplates.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1005338.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamma</surname> <given-names>S a</given-names></name>. <article-title>Speech processing in the auditory system. I: The representation of speech sounds in the responses of the auditory nerve</article-title>. <source>J Acoust Soc Am</source>. <year>1985</year>;<volume>78</volume>: <fpage>1612</fpage>–<lpage>1621</lpage>. <object-id pub-id-type="pmid">4067077</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamma</surname> <given-names>SA</given-names></name>. <article-title>Speech processing in the auditory system. II: Lateral inhibition and the central processing of speech invoked activity in the auditory nerve</article-title>. <source>J Acoust Soc Am</source>. <year>1985</year>;<volume>78</volume>: <fpage>1622</fpage>–<lpage>1632</lpage>. <object-id pub-id-type="pmid">3840813</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cedolin</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Delgutte</surname> <given-names>B</given-names></name>. <article-title>Spatiotemporal Representation of the Pitch of Harmonic Complex Tones in the Auditory Nerve</article-title>. <source>J Neurosci</source>. <year>2010</year>;<volume>30</volume>: <fpage>12712</fpage>–<lpage>12724</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.6365-09.2010" xlink:type="simple">10.1523/JNEUROSCI.6365-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20861376</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Srulovicz</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Goldstein</surname> <given-names>J</given-names></name>. <article-title>A central spectrum model: A synthesis of auditory nerve timing and place cues in monoaural communication offrequency spectrum</article-title>. <source>J Acoust Soc Am</source>. <year>1983</year>;<volume>73</volume>: <fpage>1266</fpage>–<lpage>1276</lpage>. <object-id pub-id-type="pmid">6853838</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grossberg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Govindarajan</surname> <given-names>KK</given-names></name>, <name name-style="western"><surname>Wyse</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>M a</given-names></name>. <article-title>ARTSTREAM: a neural network model of auditory scene analysis and source segregation</article-title>. <source>Neural Netw</source>. <year>2004</year>;<volume>17</volume>: <fpage>511</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2003.10.002" xlink:type="simple">10.1016/j.neunet.2003.10.002</ext-link></comment> <object-id pub-id-type="pmid">15109681</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamma</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Klein</surname> <given-names>D</given-names></name>. <article-title>The case of the missing pitch templates: how harmonic templates emerge in the early auditory system</article-title>. <source>J Acoust Soc Am</source>. <year>2000</year>;<volume>107</volume>: <fpage>2631</fpage>–<lpage>44</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/10830385" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/10830385</ext-link> <object-id pub-id-type="pmid">10830385</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamma</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Klein</surname> <given-names>D</given-names></name>. <article-title>The case of the missing pitch templates: How harmonic templates emerge in the early auditory system</article-title>. <source>J Acoust Soc Am</source>. <year>2000</year>;<volume>107</volume>: <fpage>2631</fpage>. <object-id pub-id-type="pmid">10830385</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Norman-Haignere</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>McDermott</surname> <given-names>JH</given-names></name>. <article-title>Cortical Pitch Regions in Humans Respond Primarily to Resolved Harmonics and Are Located in Specific Tonotopic Regions of Anterior Auditory Cortex</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>: <fpage>19451</fpage>–<lpage>19469</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2880-13.2013" xlink:type="simple">10.1523/JNEUROSCI.2880-13.2013</ext-link></comment> <object-id pub-id-type="pmid">24336712</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Foldiak</surname> <given-names>P</given-names></name>. <article-title>Forming sparse representations by local anti-Hebbian learning</article-title>. <source>Biol Cybern</source>. <year>1990</year>;<volume>64</volume>: <fpage>165</fpage>–<lpage>170</lpage>. <object-id pub-id-type="pmid">2291903</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rozell</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Baraniuk</surname> <given-names>RG</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>. <article-title>Sparse coding via thresholding and local competition in neural circuits</article-title>. <source>Neural Comput</source>. <year>2008</year>;<volume>20</volume>: <fpage>2526</fpage>–<lpage>2563</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2008.03-07-486" xlink:type="simple">10.1162/neco.2008.03-07-486</ext-link></comment> <object-id pub-id-type="pmid">18439138</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Spratling</surname> <given-names>MW</given-names></name>. <article-title>Classification using sparse representations: a biologically plausible approach</article-title>. <source>Biol Cybern</source>. <year>2013</year>;<volume>108</volume>: <fpage>61</fpage>–<lpage>73</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00422-013-0579-x" xlink:type="simple">10.1007/s00422-013-0579-x</ext-link></comment> <object-id pub-id-type="pmid">24306061</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knag</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Member</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Member</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Member</surname> <given-names>S</given-names></name>. <article-title>A Sparse Coding Neural Network ASIC With On-Chip Learning for Feature Extraction and Encoding</article-title>. <year>2015</year>;<volume>50</volume>: <fpage>1070</fpage>–<lpage>1079</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Murphy</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>DeWeese</surname> <given-names>MR</given-names></name>. <article-title>A sparse coding model with synaptically local plasticity and spiking neurons can account for the diverse shapes of V1 simple cell receptive fields</article-title>. <source>PLoS Comput Biol</source>. <year>2011</year>;<volume>7</volume>: <fpage>1</fpage>–<lpage>33</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>D</given-names></name>. <article-title>Normalization as a canonical neural computation</article-title>. <source>Nat Rev Neurosci</source>. <year>2012</year>; <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Natural signal statistics and sensory gain control</article-title>. <source>Nat Neurosci</source>. <year>2001</year>;<volume>4</volume>: <fpage>819</fpage>–<lpage>825</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/90526" xlink:type="simple">10.1038/90526</ext-link></comment> <object-id pub-id-type="pmid">11477428</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamma</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fritz</surname> <given-names>J</given-names></name>. <article-title>Adaptive auditory computations</article-title>. <source>Curr Opin Neurobiol</source>. Elsevier Ltd; <year>2014</year>;<volume>25</volume>: <fpage>164</fpage>–<lpage>168</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2014.01.011" xlink:type="simple">10.1016/j.conb.2014.01.011</ext-link></comment> <object-id pub-id-type="pmid">24525107</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graham</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Sparse coding in the neocortex</article-title>. <source>Evol Nerv Syst</source>. <year>2010</year>;<volume>3</volume>: <fpage>181</fpage>–<lpage>187</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref095"><label>95</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ganguli</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Compressed Sensing, Sparsity, and Dimensionality in Neuronal Information Processing and Data Analysis</article-title>. <source>Annual Review of Neuroscience</source>. <year>2012</year>. pp. <fpage>485</fpage>–<lpage>508</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-neuro-062111-150410" xlink:type="simple">10.1146/annurev-neuro-062111-150410</ext-link></comment> <object-id pub-id-type="pmid">22483042</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref096"><label>96</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>Honglak</given-names></name>, <name name-style="western"><surname>Battle</surname> <given-names>Alexis</given-names></name>, <name name-style="western"><surname>Rajat</surname> <given-names>Raina AYN</given-names></name>. <article-title>Efficient Sparse coding algorithms</article-title>. <source>Adv nerual infromation Process Syst</source>. <year>2006</year>; <fpage>801</fpage>–<lpage>808</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref097"><label>97</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Edelman</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Mountcastle</surname> <given-names>VB</given-names></name>. <source>The mindful brain: Cortical organization and the group-selective theory of higher brain function</source>. <publisher-name>Massachusetts Inst of Technology Pr</publisher-name>; <year>1978</year>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref098"><label>98</label><mixed-citation publication-type="other" xlink:type="simple">Dean T, Corrado G, Shlens J. Three Controversial Hypotheses Concerning Computation in the Primate Cortex. Twenty-Sixth AAAI Conf Artif Intell. 2012; 1543–1549. Available: <ext-link ext-link-type="uri" xlink:href="http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewPDFInterstitial/5093/5299" xlink:type="simple">http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewPDFInterstitial/5093/5299</ext-link></mixed-citation></ref>
<ref id="pcbi.1005338.ref099"><label>99</label><mixed-citation publication-type="other" xlink:type="simple">Olshausen B a. Probabilistic Models of the Brain: Perception and Neural Function. Image Rochester NY. 2002; 257–272. Available: <ext-link ext-link-type="uri" xlink:href="http://scholar.google.com/scholar?hl=en&amp;btnG=Search&amp;q=intitle:Sparse+Codes+and+Spikes#0" xlink:type="simple">http://scholar.google.com/scholar?hl=en&amp;btnG=Search&amp;q=intitle:Sparse+Codes+and+Spikes#0</ext-link></mixed-citation></ref>
<ref id="pcbi.1005338.ref100"><label>100</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stopfer</surname> <given-names>M</given-names></name>. <article-title>Olfactory processing: massive convergence onto sparse codes</article-title>. <source>Curr Biol</source>. <year>2007</year>;<volume>17</volume>: <fpage>R363</fpage>–<lpage>4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2007.03.019" xlink:type="simple">10.1016/j.cub.2007.03.019</ext-link></comment> <object-id pub-id-type="pmid">17502089</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref101"><label>101</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>. <article-title>From synchrony to sparseness</article-title>. <source>Trends Neurosci</source>. <year>2003</year>;<volume>26</volume>: <fpage>61</fpage>–<lpage>64</lpage>. <object-id pub-id-type="pmid">12536128</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref102"><label>102</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hromádka</surname> <given-names>T</given-names></name>. <article-title>Sparse representation of sounds in the unanesthetized auditory cortex</article-title>. <source>PLoS Biol</source>. <year>2008</year>; Available: <ext-link ext-link-type="uri" xlink:href="http://dx.plos.org/10.1371/journal.pbio.0060016" xlink:type="simple">http://dx.plos.org/10.1371/journal.pbio.0060016</ext-link></mixed-citation></ref>
<ref id="pcbi.1005338.ref103"><label>103</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chechik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Bar-Yosef</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>ED</given-names></name>, <name name-style="western"><surname>Tishby</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Reduction of Information Redundancy in the Ascending Auditory Pathway</article-title>. <source>Neuron</source>. <year>2006</year>;<volume>51</volume>: <fpage>359</fpage>–<lpage>368</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2006.06.030" xlink:type="simple">10.1016/j.neuron.2006.06.030</ext-link></comment> <object-id pub-id-type="pmid">16880130</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref104"><label>104</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Terashima</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hosoya</surname> <given-names>H</given-names></name>. <article-title>Sparse codes of harmonic natural sounds and their modulatory interactions</article-title>. <source>Network</source>. <year>2009</year>;<volume>20</volume>: <fpage>253</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3109/09548980903447751" xlink:type="simple">10.3109/09548980903447751</ext-link></comment> <object-id pub-id-type="pmid">19919283</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref105"><label>105</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Terashima</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hosoya</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Tani</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ichinohe</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Okada</surname> <given-names>M</given-names></name>. <article-title>Sparse coding of harmonic vocalization in monkey auditory cortex</article-title>. <source>Neurocomputing</source>. <year>2013</year>;<volume>103</volume>: <fpage>14</fpage>–<lpage>21</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref106"><label>106</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Spanne</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Jörntell</surname> <given-names>H</given-names></name>. <article-title>Questioning the role of sparse coding in the brain</article-title>. <source>Trends Neurosci</source>. <year>2015</year>;<volume>38</volume>: <fpage>417</fpage>–<lpage>427</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2015.05.005" xlink:type="simple">10.1016/j.tins.2015.05.005</ext-link></comment> <object-id pub-id-type="pmid">26093844</object-id></mixed-citation></ref>
<ref id="pcbi.1005338.ref107"><label>107</label><mixed-citation publication-type="book" xlink:type="simple"><source>MATLAB. version 8.3 (R2014a)</source>. <publisher-loc>Natick, Massachusetts</publisher-loc>: <publisher-name>The MathWorks Inc.</publisher-name>; <year>2010</year>.</mixed-citation></ref>
<ref id="pcbi.1005338.ref108"><label>108</label><mixed-citation publication-type="other" xlink:type="simple">Slaney M, Slaney M, Corproation IR, Corproation IR, Toolbox A, Toolbox A. Auditory Toolbox. 1998</mixed-citation></ref>
</ref-list>
</back>
</article>