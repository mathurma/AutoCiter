<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">09-PLCB-RA-0179R2</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000464</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology/Computational Neuroscience</subject><subject>Neuroscience/Sensory Systems</subject><subject>Neuroscience/Theoretical Neuroscience</subject></subj-group></article-categories><title-group><article-title>Recognizing Sequences of Sequences</article-title><alt-title alt-title-type="running-head">Recognition of Sequences</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Kiebel</surname><given-names>Stefan J.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>von Kriegstein</surname><given-names>Katharina</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Daunizeau</surname><given-names>Jean</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Wellcome Trust Centre for Neuroimaging, London, United Kingdom</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Kötter</surname><given-names>Rolf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">Radboud University Nijmegen Medical Centre, Netherlands</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">skiebel@fil.ion.ucl.ac.uk</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: SJK. Performed the experiments: SJK. Analyzed the data: SJK. Contributed reagents/materials/analysis tools: SJK JD KJF. Wrote the paper: SJK KvK JD KJF.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>8</month><year>2009</year></pub-date><pub-date pub-type="epub"><day>14</day><month>8</month><year>2009</year></pub-date><volume>5</volume><issue>8</issue><elocation-id>e1000464</elocation-id><history>
<date date-type="received"><day>18</day><month>2</month><year>2009</year></date>
<date date-type="accepted"><day>10</day><month>7</month><year>2009</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2009</copyright-year><copyright-holder>Kiebel et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>The brain's decoding of fast sensory streams is currently impossible to emulate, even approximately, with artificial agents. For example, robust speech recognition is relatively easy for humans but exceptionally difficult for artificial speech recognition systems. In this paper, we propose that recognition can be simplified with an internal model of how sensory input is generated, when formulated in a Bayesian framework. We show that a plausible candidate for an internal or generative model is a hierarchy of ‘stable heteroclinic channels’. This model describes continuous dynamics in the environment as a hierarchy of sequences, where slower sequences cause faster sequences. Under this model, online recognition corresponds to the dynamic decoding of causal sequences, giving a representation of the environment with predictive power on several timescales. We illustrate the ensuing decoding or recognition scheme using synthetic sequences of syllables, where syllables are sequences of phonemes and phonemes are sequences of sound-wave modulations. By presenting anomalous stimuli, we find that the resulting recognition dynamics disclose inference at multiple time scales and are reminiscent of neuronal dynamics seen in the real brain.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>Despite tremendous advances in neuroscience, we cannot yet build machines that recognize the world as effortlessly as we do. One reason might be that there are computational approaches to recognition that have not yet been exploited. Here, we demonstrate that the ability to recognize temporal sequences might play an important part. We show that an artificial decoding device can extract natural speech sounds from sound waves if speech is generated as dynamic and transient sequences of sequences. In principle, this means that artificial recognition can be implemented robustly and online using dynamic systems theory and Bayesian inference.</p>
</abstract><funding-group><funding-statement>SJK is funded by the Max Planck Society. KvK is funded by a independent junior research group grant of the Max Planck Society. JD is funded by a European Marie-Curie fellowship. KJF is funded by the Wellcome Trust. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="13"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Many aspects of our sensory environment can be described as dynamic sequences. For example, in the auditory domain, speech and music are sequences of sound-waves <xref ref-type="bibr" rid="pcbi.1000464-Poeppel1">[1]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Zatorre1">[2]</xref>, where speech can be described as a sequence of phonemes. Similarly, in the visual domain, speaking generates sequences of facial cues with biological motion <xref ref-type="bibr" rid="pcbi.1000464-Simon1">[3]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Thompson1">[4]</xref>. These auditory and visual sequences have an important characteristic: the transitions between the elements are continuous; i.e., it is often impossible to identify a temporal boundary between two consecutive elements. For example, phonemes (speech sounds) in a syllable are not discrete entities that follow each other like beads on a string but rather show graded transitions to the next phoneme. These transitions make artificial speech recognition notoriously difficult <xref ref-type="bibr" rid="pcbi.1000464-Deng1">[5]</xref>. Similarly, in the visual domain, when we observe someone speaking, it is extremely difficult to determine exactly where the movements related to a phoneme start or finish. These dynamic sequences, with brief transitions periods between elements, are an inherent part of our environment, because sensory input is often generated by the fluent and continuous movements of other people, or indeed oneself.</p>
<p>Dynamic sequences are generated on various time-scales. For example, in speech, formants form phonemes and phonemes form syllables. Sequences, which exist at different time-scales, are often structured hierarchically, where sequence elements on one time-scale constrain the expression of sequences on a finer time-scale; e.g. a syllable comprises a specific sequence of phonemes. This functional hierarchy of time-scales may be reflected in the hierarchical, anatomical organisation of the brain <xref ref-type="bibr" rid="pcbi.1000464-Kiebel1">[6]</xref>. For example, in avian brains, there is anatomical and functional evidence that birdsong is generated and perceived by a hierarchical system, where low levels represent transient acoustic details and high levels encode song structure at slower time-scales <xref ref-type="bibr" rid="pcbi.1000464-Long1">[7]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Sen1">[8]</xref>. An equivalent temporal hierarchy might also exist in the human brain for representing auditory information, such as speech <xref ref-type="bibr" rid="pcbi.1000464-Poeppel1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1000464-Davis1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1000464-vonKriegstein1">[12]</xref>.</p>
<p>Here we ask the following question: How does the brain recognize the dynamic and ambiguous causes of noisy sensory input? Based on experimental and theoretical evidence <xref ref-type="bibr" rid="pcbi.1000464-Wolpert1">[13]</xref>–<xref ref-type="bibr" rid="pcbi.1000464-Friston1">[18]</xref> we assume the brain is a recognition system that uses an internal model of its environment. The structure of this model is critical: On one hand, the form of the model must capture the essential architecture of the process generating sensory data. On the other hand, it must also support robust inference. We propose that a candidate that fulfils both criteria is a model based on a hierarchy of stable heteroclinic channels (SHCs). SHCs have been introduced recently as a model of neuronal dynamics <italic>per se</italic> <xref ref-type="bibr" rid="pcbi.1000464-Rabinovich1">[19]</xref>. Here, we use SHCs as the basis of neuronal recognition, using an established Bayesian scheme for modelling perception <xref ref-type="bibr" rid="pcbi.1000464-Friston2">[20]</xref>. This brings together two recent developments in computational approaches to perception: Namely, winnerless competition in stable heteroclinic channels and the hypothesis that the brain performs Bayesian inference. This is important because it connects a dynamic systems perspective on neuronal dynamics <xref ref-type="bibr" rid="pcbi.1000464-Rabinovich1">[19]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Fukai1">[21]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Rabinovich2">[22]</xref> with the large body of work on the brain as an inference machine <xref ref-type="bibr" rid="pcbi.1000464-Wolpert1">[13]</xref>–<xref ref-type="bibr" rid="pcbi.1000464-Friston1">[18]</xref>.</p>
<p>To demonstrate this we generate artificial speech input (sequences of syllables) and describe a system that can recognize these syllables, online from incoming sound waves. We show that the resulting recognition dynamics display functional characteristics that are reminiscent of psychophysical and neuronal responses.</p>
</sec><sec id="s2">
<title>Model</title>
<p>In this section, we describe an online recognition scheme for continuous sequences with hierarchical structure. This scheme rests on the concept of stable heteroclinic channels (SHCs) <xref ref-type="bibr" rid="pcbi.1000464-Rabinovich3">[23]</xref>, which are combined with an online Bayesian inversion scheme <xref ref-type="bibr" rid="pcbi.1000464-Friston2">[20]</xref>. We now describe these elements and how they are brought together. Note that all variables and their meaning are also listed in <xref ref-type="table" rid="pcbi-1000464-t001">Table 1</xref> and <xref ref-type="table" rid="pcbi-1000464-t002">2</xref>.</p>
<table-wrap id="pcbi-1000464-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000464.t001</object-id><label>Table 1</label><caption>
<title>Variables used for hierarchies of stable heteroclinic channels (SHCs).</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000464-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/></colgroup>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e001" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Nonlinear evolution and observation function</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e002" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Scalar rate constant</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e003" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Hidden and causal state vectors</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e004" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e005" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e006" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Scalar control parameters:</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e007" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e008" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Inhibitory connectivity matrix</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><italic>S</italic></td>
<td align="left" colspan="1" rowspan="1">Sigmoid function</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><italic>w,z</italic></td>
<td align="left" colspan="1" rowspan="1">state and observation noise vectors</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e009" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><italic>k</italic>th template connectivity matrix</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt101"><p>This table lists all variables and their meaning for Eqs. 1 to 3. The additional superscript <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e010" xlink:type="simple"/></inline-formula> in Eqs. 2 and 3 denotes the level of the SHC, where level <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e011" xlink:type="simple"/></inline-formula> is the lowest.</p></fn></table-wrap-foot></table-wrap><table-wrap id="pcbi-1000464-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000464.t002</object-id><label>Table 2</label><caption>
<title>Variables used in Bayesian recognition scheme.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000464-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.t002" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/></colgroup>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e012" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Sensory input vector</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e013" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Concatenated hidden and causal state vectors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e014" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">m</td>
<td align="left" colspan="1" rowspan="1">A model, which specifies the structure of likelihood and priors</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e015" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Recognition density used by recognition system to approximate the true but unknown generative density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e016" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><italic>F,U,S</italic></td>
<td align="left" colspan="1" rowspan="1">Free energy, energy, and entropy (scalars)</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e017" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Sufficient statistics vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e018" xlink:type="simple"/></inline-formula> of normal recognition density <italic>q</italic></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e019" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Prediction error vector (causal states)</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e020" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Prediction error vector (hidden states)</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt102"><p>This table lists all variables used in Eqs. 4 to 8. Note that all variables except for <italic>m</italic> are functions of time.</p></fn></table-wrap-foot></table-wrap><sec id="s2a">
<title>Stable heteroclinic channels (SHCs)</title>
<p>SHCs are attractors formed by artificial neuronal networks, which prescribe sequences of transient dynamics <xref ref-type="bibr" rid="pcbi.1000464-Rabinovich2">[22]</xref>–<xref ref-type="bibr" rid="pcbi.1000464-Rabinovich4">[25]</xref>. The key aspect of these dynamical systems is that their equations of motion describe a manifold with a series of saddle points. At each saddle point, trajectories are attracted from nearly all directions but are expelled in the direction of another saddle point. If the saddle points are linked up to form a chain, the neuronal state follows a trajectory that passes through all these points, thereby forming a sequence. These sequences are exhibited robustly, even in the presence of high levels of noise. In addition, the dynamics of the SHCs are itinerant due to dynamical instability in the equations of motion and noise on the states. This noise also induces a variation in the exact times that sequence elements are visited. This can be exploited during recognition, where the SHC places prior constraints on the sequence that elements (repelling fixed-points) are visited but does not constrain the exact timing of these visits.</p>
<p>The combination of these two features, robustness of sequence order but flexibility in sequence timing, makes the SHC a good candidate for the neuronal encoding of trajectories <xref ref-type="bibr" rid="pcbi.1000464-Rabinovich1">[19]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Friston3">[26]</xref>. Rabinovich et al. have used SHCs to explain how spatiotemporal neuronal dynamics observed in odour perception, or motor control of a marine mollusc, can be expressed in terms of a dynamic system <xref ref-type="bibr" rid="pcbi.1000464-Rabinovich2">[22]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Varona1">[27]</xref>.</p>
<p>Varona et al. used Lotka-Volterra-type dynamics to model a network of six neurons in a marine mollusc <xref ref-type="bibr" rid="pcbi.1000464-Varona1">[27]</xref>: With particular lateral inhibition between pairs of neurons and input to each neuron, the network displayed sequences of activity. Following a specific order, each neuron became active for a short time and became inactive again, while the next neuron became active, and so on. Stable heteroclinic channels rest on a particular form of attractor manifold that supports itinerant dynamics. This itinerancy can result from deterministic chaos in the absence of noise, which implies the presence of heteroclinic cycles. When noise is added, itinerancy can be assured, even if the original system has stable fixed-points. However, our motivation for considering stochastic differential equations is to construct a probabilistic model, where assumptions about the distribution of noise provide a formal generative model of sensory dynamics.</p>
<p>As reviewed in <xref ref-type="bibr" rid="pcbi.1000464-Rabinovich2">[22]</xref>, Lotka-Volterra dynamics can be derived from simple neural mass models of mean membrane potential and mean firing rate <xref ref-type="bibr" rid="pcbi.1000464-Fukai1">[21]</xref>. Here, we use a different neural mass model, where the state-vector <italic>x</italic> can take positive or negative values:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e021" xlink:type="simple"/><label>(1)</label></disp-formula>where the motion of a hidden-state vector (e.g., mean membrane potentials) <italic>x</italic> is a nonlinear function of itself with scalar parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e022" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e023" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e024" xlink:type="simple"/></inline-formula> and a connectivity matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e025" xlink:type="simple"/></inline-formula>. The hidden state-vector enters a nonlinear function <italic>S</italic> to generate outcomes (e.g., neuronal firing rates) <italic>y</italic>. Each element <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e026" xlink:type="simple"/></inline-formula> determines the strength of lateral inhibition from state <italic>j</italic> to <italic>i</italic>. Both the state and observation equations above include additive normally distributed noise vectors <italic>w</italic> and <italic>z</italic>. When choosing specific parameter values (see below), the states display stereotyped sequences of activity <xref ref-type="bibr" rid="pcbi.1000464-Afraimovich2">[28]</xref>. Rabinovich et al. <xref ref-type="bibr" rid="pcbi.1000464-Rabinovich1">[19]</xref> termed these dynamics ‘stable heteroclinic channels’ (SHCs). If the channel forms a ring, once a state is attracted to a saddle point, it will remain in the SHC.</p>
<p>SHCs represent a form of itinerant dynamics <xref ref-type="bibr" rid="pcbi.1000464-Friston3">[26]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Breakspear1">[29]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Durstewitz1">[30]</xref> and may represent a substrate for neuronal computations <xref ref-type="bibr" rid="pcbi.1000464-Buonomano1">[31]</xref>. Remarkably, the formation of SHCs seems to depend largely on the lateral inhibition matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e027" xlink:type="simple"/></inline-formula> and not on the type of neuronal model; see Ivanchenko et al. <xref ref-type="bibr" rid="pcbi.1000464-Ivanchenko1">[32]</xref> for an example using a complex two-compartment spiking neuron model.</p>
<p>In this paper, we propose to use SHCs not as a model for neuronal dynamics <italic>per se</italic> but as a generative model of how sensory input is generated. This means that we interpret <italic>x</italic> as hidden states in the environment, which generate sensory input <italic>y</italic>. The neuronal response to sampling sensory input <italic>y</italic> are described by recognition dynamics, which decode or deconvolve the causes <italic>x</italic> from that input. These recognition dynamics are described below. This re-interpretation of Eq. 1 is easy to motivate: sensory input is usually generated by our own body and other organisms. This means input is often generated by neuronal dynamics of the sort described in Eq. 1.</p>
</sec><sec id="s2b">
<title>Hierarchies of stable heteroclinic channels</title>
<p>A SHC can generate repetitive, stereotyped sequences. For example, in a system with four saddle points, an SHC forces trajectories through the saddle points in a sequence, e.g. ‘1-2-3-4-1-2-3-4-1…’. In contrast, a SHC cannot generate ‘1-2-3-4-3-4-2-1…’, because the sequence is not repetitive. However, to model sensory input, for example speech, one must be able to recombine basic sequence-elements like phonemes in ever-changing sequences. One solution would be to represent each possible sequence of phonemes (e.g. each syllable) with a specific SHC. A more plausible and parsimonious solution is to construct a hierarchy of SHCs, which can encode sequences generated by SHCs whose attractor topology (e.g. the channels linking the saddle points) is changed by a supraordinate SHC. This can be achieved by making the connectivity matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e028" xlink:type="simple"/></inline-formula> at a subordinate level a function of the output states of the supra-ordinate level. This enables the hierarchy to generate sequences of sequences to any hierarchical depth required.</p>
<p>Following a recent account of how macroscopic cortical anatomy might relate to time-scales in our environment <xref ref-type="bibr" rid="pcbi.1000464-Kiebel1">[6]</xref>, we can construct a hierarchy by setting the rate constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e029" xlink:type="simple"/></inline-formula> of the <italic>j</italic>-th level to a rate that is slower than its subordinate level, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e030" xlink:type="simple"/></inline-formula>. As a result, the states of subordinate levels change faster than the states of the level above. This means the control parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e031" xlink:type="simple"/></inline-formula> at any level change more slowly than its states, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e032" xlink:type="simple"/></inline-formula>; because the slow change in the attractor manifold is controlled by the supraordinate states:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e033" xlink:type="simple"/><label>(2)</label></disp-formula>where the superscript indexes level <italic>j</italic> (level 1 being the lowest level), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e034" xlink:type="simple"/></inline-formula> are ‘hidden states’, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e035" xlink:type="simple"/></inline-formula> are outputs to the subordinate level, which we will call ‘causal states’. As before, at the first level, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e036" xlink:type="simple"/></inline-formula> is the sensory stream. In this paper, we consider hierarchies with relative time-scales <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e037" xlink:type="simple"/></inline-formula> of around four. This means that the time spent in the vicinity of a saddle point at a supraordinate level is long enough for the subordinate level to go through several saddle points. As before, all levels are subject to noise on the motion of the hidden states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e038" xlink:type="simple"/></inline-formula> and the causal states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e039" xlink:type="simple"/></inline-formula>. At the highest level, the control parameters, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e040" xlink:type="simple"/></inline-formula> are constant over time. At all other levels, the causal states of the supraordinate level, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e041" xlink:type="simple"/></inline-formula>, enter the subordinate level by changing the control parameters, the connectivity matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e042" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e043" xlink:type="simple"/><label>(3)</label></disp-formula>Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e044" xlink:type="simple"/></inline-formula> is a linear mixture of ‘template’ control matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e045" xlink:type="simple"/></inline-formula>, weighted by the causal states at level <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e046" xlink:type="simple"/></inline-formula>. Each of these templates is chosen to generate a SHC. Below, we will show examples of how these templates can be constructed to generate various sequential phenomena. The key point about this construction is that states from the supraordinate level select which template controls the dynamics of the lower level. By induction, the states at each level follow a SHC because the states at the supraordinate level follow a SHC. This means only one state is active at any time and only one template is selected for the lower level. An exception to this is the transition from one state to another, which leads to a transient superposition of two SHC-inducing templates (see below). Effectively, the transition transient at a specific level gives rise to brief spells of non-SHC dynamics at the subordinate levels (see <xref ref-type="sec" rid="s3">results</xref>). These transition periods are characterized by dissipative dynamics, due to the largely inhibitory connectivity matrices, inhibition controlled by parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e047" xlink:type="simple"/></inline-formula> (Eq. 2) and the saturating nonlinearity <italic>S</italic>.</p>
<p>In summary, a hierarchy of SHCs generates the sensory stream <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e048" xlink:type="simple"/></inline-formula> at the lowest (fastest) level, which forms a sequence of sequences expressed in terms of first-level states. In these models, the lower level follows a SHC, i.e. the states follow an itinerant trajectory through a sequence of saddle points. This SHC will change whenever the supraordinate level, which follows itself a SHC, moves from one saddle point to another. Effectively, we have constructed a system that can generate a stable pattern of transients like an oscillator; however, as shown below, the pattern can have deep or hierarchical structure. Next, we describe how the causes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e049" xlink:type="simple"/></inline-formula> can be recognized or deconvolved from sensory input <italic>y</italic>.</p>
</sec><sec id="s2c">
<title>Bayesian recognition using SHC hierarchies and the free-energy principle</title>
<p>We have described how SHCs can, in principle, generate sequences of sequences that, we assume, are observed by an agent as its input <italic>y</italic>. To recognise the causes of the sensory stream the agent must infer the hidden states online, i.e. the system does not look into the future but recognizes the current states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e050" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e051" xlink:type="simple"/></inline-formula> of the environment, at all levels of the hierarchy, by the fusion of current sensory input and internal dynamics elicited by past input. An online recognition scheme can be derived from the ‘free-energy principle’, which states that an agent will minimize its surprise about its sensory input, under a model it entertains about the environment; or, equivalently maximise the evidence for that model <xref ref-type="bibr" rid="pcbi.1000464-Friston1">[18]</xref>. This requires the agent to have a dynamic model, which relates environmental states to sensory input. In this context, recognition is the Bayesian inversion of a generative model. This inversion corresponds to mapping sensory input to the posterior or conditional distribution of hidden states. In general, Bayesian accounts of perception rest on a generative model. Given such a model, one can use the ensuing recognition schemes in artificial perception and furthermore compare simulated recognition dynamics (in response to sensory input), with evoked responses in the brain. The generative model in this paper is dynamical and based on the nonlinear equations 1 and 2. More precisely, these stochastic differential equations play the role of empirical priors on the dynamics of hidden states causing sensory data.</p>
<p>In the following, we review briefly, the Bayesian model inversion described in <xref ref-type="bibr" rid="pcbi.1000464-Friston2">[20]</xref> for stochastic, hierarchical systems and apply it, in the next section, to hierarchical SHCs.</p>
<p>Given some sensory data vector <italic>y</italic>, the general inference problem is to compute the model evidence or marginal likelihood of y, given a model <italic>m</italic>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e052" xlink:type="simple"/><label>(4)</label></disp-formula>where the generative model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e053" xlink:type="simple"/></inline-formula> is defined in terms of a likelihood <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e054" xlink:type="simple"/></inline-formula> and prior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e055" xlink:type="simple"/></inline-formula> on hidden states. In Equation 4, the state vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e056" xlink:type="simple"/></inline-formula> subsumes the hidden and causal states at all levels of a hierarchy (Eq. 2). The model evidence can be estimated by converting this difficult integration problem (Eq. 4) into an easier optimization problem by optimising a free-energy bound on the log-evidence <xref ref-type="bibr" rid="pcbi.1000464-Beal1">[33]</xref>. This bound is constructed using Jensen's inequality and is a function of an arbitrary <italic>recognition</italic> density, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e057" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e058" xlink:type="simple"/><label>(5)</label></disp-formula>The free-energy comprises an energy term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e059" xlink:type="simple"/></inline-formula> and an entropy term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e060" xlink:type="simple"/></inline-formula> and is defined uniquely, given a generative model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e061" xlink:type="simple"/></inline-formula>. The free-energy is an upper bound on the surprise or negative log-evidence, because the Kullback-Leibler divergence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e062" xlink:type="simple"/></inline-formula>, between the recognition and conditional density, is always positive. Minimising the free-energy minimises the divergence, rendering the recognition density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e063" xlink:type="simple"/></inline-formula> an approximate conditional density. When using this approach, one usually employs a parameterized fixed-form recognition density, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e064" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1000464-Friston2">[20]</xref>. Inference corresponds to optimising the free-energy with respect to the sufficient statistics, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e065" xlink:type="simple"/></inline-formula> of the recognition density:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e066" xlink:type="simple"/><label>(6)</label></disp-formula>The optimal statistics <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e067" xlink:type="simple"/></inline-formula> are sufficient to describe the approximate posterior density; i.e. the agent's belief about (or representation of) the trajectory of the hidden and causal states. We refer the interested reader to Friston et al. <xref ref-type="bibr" rid="pcbi.1000464-Friston4">[34]</xref> for technical details about this variational Bayesian treatment of dynamical systems. Intuitively, this scheme can be thought of as augmented gradient descent on a free-energy bound on the model's log-evidence. Critically, it outperforms conventional Bayesian filtering (e.g., Extended Kalman filtering) and eschews the computation of probability transition matrices. This means it can be implemented in a simple and neuronally plausible fashion <xref ref-type="bibr" rid="pcbi.1000464-Friston2">[20]</xref>.</p>
<p>In short, this recognition scheme operates online and recognizes current states of the environment by combining current sensory input with internal recognition dynamics, elicited by past input.</p>
<p>A recognition system that minimizes its free-energy efficiently will come to represent the environmental dynamics in terms of the sufficient statistics of recognition density; e.g. the conditional expectations and variances of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e068" xlink:type="simple"/></inline-formula>. We assume that the conditional moments are encoded by neuronal activity; i.e., Equation 6 prescribes neuronal recognition dynamics. These dynamics implement Bayesian inversion of the generative model, under the approximations entailed by the form of the recognition density. Neuronally, Equation 6 can be implemented using a message passing scheme, which, in the context of hierarchical models, involves passing prediction errors up and passing predictions down, from one level to the next. These prediction errors are the difference between the causal states (Equation 2);<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e069" xlink:type="simple"/><label>(7)</label></disp-formula>at any level <italic>j</italic>, and their prediction from the level above, evaluated at the conditional expectations <xref ref-type="bibr" rid="pcbi.1000464-Friston1">[18]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Friston5">[35]</xref>. In addition, there are prediction errors that mediate dynamical priors on the motion of hidden states within each level (Equation 2);<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e070" xlink:type="simple"/><label>(8)</label></disp-formula>This means that neuronal populations encode two types of dynamics: the conditional expectations of states of the world and the prediction errors. The dynamics of the first are given by Equation 6, which can be formulated as a function of prediction error. These dynamics effectively suppress or explain away prediction error; see <xref ref-type="bibr" rid="pcbi.1000464-Friston4">[34]</xref> for details.</p>
<p>This inversion scheme is a generic recognition process that receives dynamic sensory input and can, given an appropriate generative model, rapidly identify and track environmental states that are generating current input. More precisely, the recognition dynamics resemble the environmental (hidden) states they track (to which they are indirectly coupled), but differ from the latter because they are driven by a gradient descent on free-energy; Eq. 6 (i.e. minimize prediction errors: Eqs. 7 and 8). This is important, because we want to use SHCs as a generative model, not as a model of neuronal encoding <italic>per se</italic>. This means that the neuronal dynamics will only recapitulate the dynamics entitled by SHCs in the environment, if the recognition scheme can suppress prediction errors efficiently in the face of sensory noise and potential beliefs about the world.</p>
<p>We are now in a position to formulate hierarchies of SHCs as generative models, use them to generate sensory input and simulate recognition of the causal states generating that input. In terms of low-level speech processing, this means that any given phoneme will predict the next phoneme. At the same time, as phonemes are recognized, there is also a prediction about which syllable is the most likely context for generating these phonemes. This prediction arises due to the learnt regularities in speech. In turn, the most likely syllable predicts the next phoneme. This means that speech recognition can be described as a dynamic process, on multiple time-scales, with recurrently evolving representations and predictions, all driven by the sensory input.</p>
</sec><sec id="s2d">
<title>A model of speech recognition</title>
<p>In the auditory system, higher cortical levels appear to represent features that are expressed at slower temporal scales <xref ref-type="bibr" rid="pcbi.1000464-Creutzfeldt1">[36]</xref>. Wang et al. <xref ref-type="bibr" rid="pcbi.1000464-Wang1">[37]</xref> present evidence from single-neuron recordings that there is a ‘slowing down’ of representational trajectories from human auditory sensory thalamus (a ‘relay’ to the primary auditory cortex), the medial geniculate body (MGB) to primary auditory cortex (AI). In humans, it has been found that the sensory thalamus responds preferentially to faster temporal modulations of sensory signals, whereas primary cortex prefers slower modulations <xref ref-type="bibr" rid="pcbi.1000464-Giraud1">[10]</xref>. These findings indicate that neuronal populations, at lower levels of the auditory system (e.g. MGB), represent faster environmental trajectories than higher levels (e.g., A1). Specifically, the,MGB responds preferentially to temporal modulations of ∼20 Hz (∼50 ms), whereas AI prefers modulations at ∼6 Hz (∼150 ms) <xref ref-type="bibr" rid="pcbi.1000464-Giraud1">[10]</xref>. Such a temporal hierarchy would be optimal for speech recognition, in which information over longer time-scales provides predictions for processing at shorter time scales. In accord with this conjecture, optimal encoding of fast (rapidly modulated) dynamics by top-down predictions has been found to be critical for communication <xref ref-type="bibr" rid="pcbi.1000464-Poeppel1">[1]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-vonKriegstein1">[12]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Nahum1">[38]</xref>.</p>
<p>We model this ‘slowing down’ with a hierarchical generative model based on SHCs. This model generates sequences of syllables, where each syllable is a sequence of phonemes. Phonemes are the smallest speech sounds that distinguishes meaning and a syllable is a unit of organization for a sequence of phonemes. Each phoneme prescribes a sequence of sound-wave modulations which correspond to sensory data. We generated data in this fashion and simulated online recognition (see <xref ref-type="fig" rid="pcbi-1000464-g001">Figure 1</xref>). By recognizing speech-like phoneme-sequences, we provide a proof-of-principle that a hierarchical system can use sensory streams to infer sequences. This not only models the slowing down of representations in the auditory system <xref ref-type="bibr" rid="pcbi.1000464-Giraud1">[10]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-vonKriegstein1">[12]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Wang1">[37]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Nahum1">[38]</xref>, but may point to computational approaches to speech recognition. In summary, the recognition dynamics following Equation 6 are coupled to a generative model based on SHCs via sensory input. The systems generating and recognising states in <xref ref-type="fig" rid="pcbi-1000464-g001">Fig. 1</xref> are both dynamic systems, where a non-autonomous recognition system is coupled to an autonomous system generating speech.</p>
<fig id="pcbi-1000464-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000464.g001</object-id><label>Figure 1</label><caption>
<title>Schematic of the generative model and recognition system.</title>
<p>This schematic shows the equations which define both the generation of stimuli (left, see Equation 2) and the recognition scheme based on a generative model. There are three levels; the phonemic and syllabic levels employ stable heteroclinic channels, while the acoustic level is implemented by a linear transform. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e071" xlink:type="simple"/></inline-formula> corresponds to sound file extracts and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e072" xlink:type="simple"/></inline-formula> is the resulting sound wave. This sound wave is input to the recognition system, with a linear (forward) projection using the pseudo-inverse <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e073" xlink:type="simple"/></inline-formula>. The recognition of the phonemic and syllabic level uses bottom-up and top-down message passing between the phonemic and syllabic level, following Equation 6.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.g001" xlink:type="simple"/></fig>
<p>All our simulations used hierarchies with two levels (<xref ref-type="fig" rid="pcbi-1000464-g002">Figure 2</xref>). The first (phonemic) level produces a sequence of phonemes, and the second (syllabic) level encodes sequences of syllables. We used Equation 2 to produce phoneme sequences, where the generating parameters are listed in <xref ref-type="table" rid="pcbi-1000464-t003">Table 3</xref>. The template matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e074" xlink:type="simple"/></inline-formula> (Equation 3) were produced in the following way: We first specified the sequence each template should induce; e.g., sequence 1-2-3 for three neuronal populations. We then set elements on the main diagonal to 1, the elements (2,1), (3,2), (1,3) to value 0.5, and all other elements to 5 <xref ref-type="bibr" rid="pcbi.1000464-Afraimovich2">[28]</xref>. More generally for sequence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e075" xlink:type="simple"/></inline-formula> <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e076" xlink:type="simple"/><label>(9)</label></disp-formula>Note that SHC hierarchies can be used to create a variety of different behaviours, using different connectivity matrices. Here we explore only a subset of possible sequential dynamics.</p>
<fig id="pcbi-1000464-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000464.g002</object-id><label>Figure 2</label><caption>
<title>Two-level model to generate phoneme sequences.</title>
<p>Schematic illustration of the phoneme sequence generation process. At the syllabic level, one of three syllables is active and induces a specific lateral connectivity structure at the phonemic level. The transition speed at the phonemic level is four times faster than at the syllabic level. The resulting phoneme and syllable dynamics of the model are shown in <xref ref-type="fig" rid="pcbi-1000464-g003">Fig. 3a</xref>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.g002" xlink:type="simple"/></fig><table-wrap id="pcbi-1000464-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000464.t003</object-id><label>Table 3</label><caption>
<title>Default parameters used for simulations with Equations 2 and 3.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000464-t003-3" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.t003" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/></colgroup>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e077" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.3</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e078" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">50</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e079" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.5</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e080" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">1/8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e081" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">1/32</td>
</tr>
</tbody>
</table></alternatives></table-wrap>
<p>When generating sensory data <italic>y</italic>, we added noise <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e082" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e083" xlink:type="simple"/></inline-formula> to both the hidden and causal states. At the first and second levels, this was normally distributed zero-mean noise with log-precisions of ten and sixteen, respectively. These noise levels were chosen to introduce noisy dynamics but not to the extent that the recognition became difficult to visualise. We repeated all the simulations reported below with higher noise levels and found that the findings remained qualitatively the same (results not shown). Synthetic stimuli were generated by taking a linear mixture of sound waves extracted from sound files, in which a single speaker pronounced each of four vowel-phonemes: [a], [e], [i], [o]. These extracts <italic>W</italic> were sampled at 22050 Hz and about 14 ms long. The mixture was weighted by the causal states of the phonemic level; <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e084" xlink:type="simple"/></inline-formula>. This resulted in a concatenated sound wave file <italic>w</italic>. When this sound file is played, one perceives a sequence of vowels with smooth, overlapping transitions (<xref ref-type="supplementary-material" rid="pcbi.1000464.s001">audio file S1</xref>). These transitions are driven by the SHCs guiding the expression of the phonemes and syllables at both levels of the generative hierarchy.</p>
<p>For computational simplicity, we circumvented a detailed generative model of the acoustic level. For simulated recognition, the acoustic input (the sound wave) was transformed to phonemic input by inverting the linear mixing described above every seven ms of simulated time (one time bin). This means that our recognition scheme at the acoustic level assumes forward processing only (<xref ref-type="fig" rid="pcbi-1000464-g001">Fig. 1</xref>). However, in principle, given an appropriate generative model <xref ref-type="bibr" rid="pcbi.1000464-Holmberg1">[39]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Sumner1">[40]</xref>, one could invert a full acoustic model, using forward and backward message passing between the acoustic and phonemic levels.</p>
</sec></sec><sec id="s3">
<title>Results</title>
<p>In this section, we illustrate that the recognition scheme described above can reliably decode syllabic and phonemic structure from sensory input online, if it has the correct generative model. We will also describe how recognition fails when the generative model does not have a form that provides veridical predictions of the sensorium, e.g., when agents are not conspecific or we hear a foreign language. These simulations relate to empirical studies of brain responses evoked by unpredicted linguistic stimuli. We conclude with a more subtle violation that we deal with in everyday audition; namely the recognition of speech presented at different speeds.</p>
<sec id="s3a">
<title>Recognising a sequence of sequences</title>
<p>To create synthetic stimuli we generated syllable sequences consisting of four phonemes or states; [a], [e], [i], and [o], over 11.25 seconds (800 time points), using a two-level SHC model (<xref ref-type="fig" rid="pcbi-1000464-g002">Fig. 2</xref>). To simulate word-like stimuli, we imposed silence at the beginning and the end by windowing the phoneme sequence (<xref ref-type="fig" rid="pcbi-1000464-g003">Fig. 3A</xref>, top left). At the syllabic level, we used three syllables or states to form the second-level sequence (1–2–3)<sup>(2)</sup>; where the numbers denote the sequence and the superscript indicates the sequence level. The three causal states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e085" xlink:type="simple"/></inline-formula> of the syllabic level entered the phonemic level as control parameters to induce their template matrices as in Equation 3. This means that each of the three syllable states at the second level causes a phoneme sequence at the first: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e086" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e087" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e088" xlink:type="simple"/></inline-formula>, see <xref ref-type="fig" rid="pcbi-1000464-g002">Fig. 2</xref> and listen to the <xref ref-type="supplementary-material" rid="pcbi.1000464.s001">audio file S1</xref>. In <xref ref-type="fig" rid="pcbi-1000464-g003">Fig. 3A</xref> we show the causal and hidden states, at both levels, generated by this model. The remaining parameters, for both levels, are listed in <xref ref-type="table" rid="pcbi-1000464-t003">Table 3</xref>. Note that the rate constant of the syllabic level is four times slower than at the phonemic level. As expected, the phoneme sequence at the first level changes as a function of the active syllable at the second level. The transients caused by transitions between syllables manifest at the first level as temporary changes in the amplitude or duration of the active phoneme.</p>
<fig id="pcbi-1000464-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000464.g003</object-id><label>Figure 3</label><caption>
<title>Recognition of a sequence of sequences.</title>
<p>(A): Dynamics of generated causal and hidden states at the phonemic and syllabic level, using Equation 2. At the syllabic level, there are three different syllables (1: blue, 2: green, 3: red), following the syllable sequence 1→2→3. The slowly changing state Syllable 1 causes the faster-moving phoneme sequence a→e→i→o (blue→green→red→cyan), syllable 2: o→i→e→a (cyan→red→green→blue), and syllable 3: a→i→e→o (blue→red→green→cyan). See <xref ref-type="fig" rid="pcbi-1000464-g002">Fig 2</xref> for a schematic description of these sequences. At the beginning and end of the time-series <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e089" xlink:type="simple"/></inline-formula> (top-left plot), we introduced silence by applying a windowing function to zero time points 0 to 50 and 750 to 800. The red arrow indicates the end of the initial silent period. The phonemic states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e090" xlink:type="simple"/></inline-formula> cause sound waves, resolved at 22050 Hz (see <xref ref-type="fig" rid="pcbi-1000464-g001">Fig. 1</xref>). These sound waves are the input to the recognition system. (B): The recognition dynamics after inverting the sound wave. At the phonemic level, the states follow the true states closely. At the syllabic level, the recognized causal state dynamics <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e091" xlink:type="simple"/></inline-formula> are rougher than the true states but track the true syllable sequence veridically. The high-amplitude transients of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e092" xlink:type="simple"/></inline-formula> at the beginning and end of the time-series are due to the silent periods, where the syllabic recognition states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e093" xlink:type="simple"/></inline-formula> experience high uncertainty (plotted in grey: confidence intervals of 95% around the mean). Note that the hidden states, at both levels, experience high uncertainty whenever a phoneme or syllable is inactive. The red arrow indicates an initial but rapidly corrected mis-recognition of the causing syllable.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.g003" xlink:type="simple"/></fig>
<p>We then simulated recognition of these sequences. <xref ref-type="fig" rid="pcbi-1000464-g003">Fig. 3B</xref> shows that our recognition model successfully tracks the true states at both levels. Note the recognition dynamics rapidly ‘lock onto’ the causal states from the onset of the first phoneme of the first syllable (time point 50). Interestingly, the system did not recognize the first syllable (true: syllable 3 (red line), recognized: syllable 2 (green line) between time points 50 to 80 (see red arrow in <xref ref-type="fig" rid="pcbi-1000464-g003">Fig. 3B</xref>), but corrected itself fairly quickly, when the sensory stream indicated a new phoneme that could only be explained by the third syllable. This initial transient at the syllabic level shows that recognition dynamics can show small but revealing deviations from the true state dynamics. In principle, these deviations could be used to test whether the real auditory system uses a recognition algorithm similar to the one proposed; in particular, the simulated recognition dynamics could be used to explain empirical neurophysiological responses.</p>
</sec><sec id="s3b">
<title>Sensitivity to sequence violations</title>
<p>What happens if the stimuli deviate from learned expectations (e.g. violation of phonotactic rules)? In other words, what happens if we presented known phonemes that form unknown syllables? This question is interesting for two reasons. First, our artificial recognition scheme should do what we expect real brains to do when listening to a foreign language: they should be able to recognize the phonemes but should not derive high-order ‘meaning’ from them; i.e. should not recognize any syllable. Secondly, there are well-characterised brain responses to phonotactic violations, e.g. <xref ref-type="bibr" rid="pcbi.1000464-DehaeneLambertz1">[41]</xref>–<xref ref-type="bibr" rid="pcbi.1000464-Friedrich1">[43]</xref>. These are usually event-related responses that contain specific waveform components late in peristimulus time, such as the N400. The N400 is an event-related potential (ERP) component typically elicited by unexpected linguistic stimuli. It is characterized as a negative deflection (topologically distributed over central-parietal sites on the scalp), peaking approximately 400 ms after the presentation of an unexpected stimulus.</p>
<p>To model phonotactic violations, we generated data with the two-level model presented above. However, we used syllables, i.e. sequences of phonemes, that the recognition scheme was not informed about and consequently could not recognise (it has three syllables in its repertoire: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e094" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e095" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e096" xlink:type="simple"/></inline-formula>). Thus the recognition scheme knows all four phonemes but is unable to predict the sequences heard. <xref ref-type="fig" rid="pcbi-1000464-g004">Fig. 4A</xref> shows that the recognition system cannot track the syllables; the recognized syllables are very different from the true syllable dynamics. At the phonemic level, the prediction error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e097" xlink:type="simple"/></inline-formula> deviates from zero whenever a new (unexpected) phoneme is encountered (<xref ref-type="fig" rid="pcbi-1000464-g004">Fig. 4B</xref>). The prediction error at the syllabic level is sometimes spike-like and can reach high amplitudes, relative to the typical amplitudes of the true states (see <xref ref-type="fig" rid="pcbi-1000464-g004">Fig. 4A and B</xref>). This means that the prediction error signals violation of phonotactic rules. In <xref ref-type="fig" rid="pcbi-1000464-g004">Fig. 4C</xref>, we zoom in onto time points 440 to 470 to show how the prediction error evolves when evidence of a phonotactic violation emerges: At the phoneme level, prediction error builds up because an unexpected phoneme appears. After time point 450, the prediction error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e098" xlink:type="simple"/></inline-formula> grows quickly, up to the point that the system resolves the prediction error. This is done by ‘switching’ to a new syllable, which can explain the transition to the emerging phoneme. The switching creates a large amplitude prediction error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e099" xlink:type="simple"/></inline-formula> at time point 460. In other words, in face of emerging evidence that its current representation of syllables and phonemes cannot explain sensory input, the system switches rapidly to a new syllable representation, giving rise to a new prediction error. It may be that these prediction errors are related to electrophysiological responses to violations of phonotactic rules, <xref ref-type="bibr" rid="pcbi.1000464-Friederici1">[44]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Lau1">[45]</xref>. This is because the largest contributors to non-invasive electromagnetic signals are thought to be superficial pyramidal cells. In biological implementations of the recognition scheme used here <xref ref-type="bibr" rid="pcbi.1000464-Friston2">[20]</xref>, these cells encode prediction error.</p>
<fig id="pcbi-1000464-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000464.g004</object-id><label>Figure 4</label><caption>
<title>Recognition of sequences with phonotactic violation.</title>
<p>(A): True and recognized syllable dynamics of a two-level model when the syllables are unknown to the recognition system. Left: True dynamics of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e100" xlink:type="simple"/></inline-formula>, Right: Recognition dynamics for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e101" xlink:type="simple"/></inline-formula>. (B): Left: Prediction error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e102" xlink:type="simple"/></inline-formula> at phonemic level. Right: Prediction error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e103" xlink:type="simple"/></inline-formula> at syllabic level. (C): Zoom of dynamics shown in A and B from time points 440 to 470. See text for description of these dynamics.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.g004" xlink:type="simple"/></fig>
<p>In summary, these simulations show that a recognition system cannot represent trajectories or sequences that are not part of its generative model. In these circumstances, recognition experiences intermittent high-amplitude prediction errors because the internal predictions do not match the sensory input. There is a clear formal analogy between the expression of prediction error in these simulations and mismatch or prediction violation responses observed empirically. The literature that examines event-related brain potentials (ERPs) and novelty processing “reveals that the orienting response engendered by deviant or unexpected events consists of a characteristic ERP pattern, comprised sequentially of the mismatch negativity (MMN) and the novelty P3 or P3a” <xref ref-type="bibr" rid="pcbi.1000464-Friedman1">[46]</xref>.</p>
</sec><sec id="s3c">
<title>Robustness to speed of speech</title>
<p>Human speech recognition is robust to the speed of speech <xref ref-type="bibr" rid="pcbi.1000464-Foulke1">[47]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Versfeld1">[48]</xref>. How do our brains recognize speech at different rates? There are two possible mechanisms in our model that can deal with ‘speaker speed’ parameters online. First, one could make the rate constants <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e104" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e105" xlink:type="simple"/></inline-formula> free parameters and optimise them during inversion. Adjusting to different speaker parameters is probably an essential faculty, because people speak at different speeds <xref ref-type="bibr" rid="pcbi.1000464-Pisoni1">[49]</xref>. The second mechanism is that the recognition itself might be robust to deviations from the expected rate of phonemic transitions; i.e., even though the recognition uses the rate parameters appropriate for much slower speech, it still can recognize fast speech. This might explain why human listeners can understand speech at rates that they have never experienced previously <xref ref-type="bibr" rid="pcbi.1000464-Foulke1">[47]</xref>. In the following, we show that our scheme has this robustness.</p>
<p>To simulate speed differences we used the same two-level model as in the simulations above with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e106" xlink:type="simple"/></inline-formula> for the generation of phonemes, but with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e107" xlink:type="simple"/></inline-formula> for recognition so that the stimulus stream was 50% faster than expected. As can be seen in <xref ref-type="fig" rid="pcbi-1000464-g005">Fig. 5A</xref>, the recognition can successfully track the syllables. This was because the second level supported the adaption to the fast sensory input by changing its recognition dynamics in responses to prediction error (see <xref ref-type="fig" rid="pcbi-1000464-g005">Fig. 5B</xref>: note the amplitude difference in <xref ref-type="fig" rid="pcbi-1000464-g005">Fig. 5A</xref> between the true and recognized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e108" xlink:type="simple"/></inline-formula>). The prediction errors at both levels, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e109" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e110" xlink:type="simple"/></inline-formula>, are shown in <xref ref-type="fig" rid="pcbi-1000464-g005">Fig. 5C</xref>. In particular, the second-level error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e111" xlink:type="simple"/></inline-formula> displayed spike-like corrections around second-level transitions. These are small in amplitude compared to both the amplitude of the hidden states and the prediction errors of the previous simulation (<xref ref-type="fig" rid="pcbi-1000464-g004">Fig. 4B</xref>). These results show that the system can track the true syllables veridically, where the prediction error accommodates the effects caused by speed differences. This robustness to variations in the speed of phoneme transitions might be a feature shared with the auditory system <xref ref-type="bibr" rid="pcbi.1000464-Vaughan1">[50]</xref>.</p>
<fig id="pcbi-1000464-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000464.g005</object-id><label>Figure 5</label><caption>
<title>Recognition of unexpectedly fast phoneme sequences.</title>
<p>(A): True and recognized syllable dynamics of a two-level model when the phoneme sequence is generated with a rate constant of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e112" xlink:type="simple"/></inline-formula> but recognized with a rate constant of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e113" xlink:type="simple"/></inline-formula>, i.e. speech was 50% faster than expected. Left: True dynamics of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e114" xlink:type="simple"/></inline-formula>, Right: Recognition dynamics for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e115" xlink:type="simple"/></inline-formula>. (B): Prediction error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e116" xlink:type="simple"/></inline-formula> at syllabic level. (C) Top: Prediction error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e117" xlink:type="simple"/></inline-formula> at phonemic level. Bottom: Prediction error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e118" xlink:type="simple"/></inline-formula> at syllabic level.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.g005" xlink:type="simple"/></fig></sec></sec><sec id="s4">
<title>Discussion</title>
<p>We have shown that stable heteroclinic channels (SHCs) can be used as generative models for online recognition. In particular, we have provided proof-of-concept that sensory input generated by these hierarchies can be deconvolved to disclose the hidden states causing that input. This is a non-trivial observation because nonlinear, hierarchical and stochastic dynamical systems are difficult to invert online <xref ref-type="bibr" rid="pcbi.1000464-Budhiraja1">[51]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Judd1">[52]</xref>. However, we found that the inversion of models based on SHCs is relatively simple. Furthermore, the implicit recognition scheme appears robust to noise and deviations from true parameters. This suggests that SHCs may be a candidate for neuronal models that contend with the same problem of de-convolving causes from sensory consequences. Moreover, hierarchical SHCs seem, in principle, an appropriate description of natural sequential input, which is usually generated by our own body or other organisms, and can be described as a mixture of transients and discrete events.</p>
<p>The general picture of recognition that emerges is as follows: Sensory input is generated by a hierarchy of dynamic systems in the environment. We couple this dynamic system, via sensory sampling, to our recognition system implementing the inversion dynamics (<xref ref-type="fig" rid="pcbi-1000464-g001">Fig. 1</xref>). The recognition system minimizes a proxy for surprise or model evidence; the negative free-energy (Eq. 6). To do this, the states of the recognition system move on manifolds, defined through the free-energy by the generative model. Here, we use a hierarchy of SHCs as generative model so that the manifold changes continuously at various time-scales. The inferred SHC states never reach a fixed point, but are perpetually following a trajectory through state-space, in the attempt to mirror the generative dynamics of the environment. When sensory input is unexpected (see second simulation, <xref ref-type="fig" rid="pcbi-1000464-g004">Fig. 4</xref>), the system uses the prediction error to change its representation quickly, at all levels, such that it best explains the sensory stream.</p>
<p>In a previous paper <xref ref-type="bibr" rid="pcbi.1000464-Kiebel1">[6]</xref>, we have shown that one can use chaotic attractors (i.e., a hierarchy of Lorenz attractors) to model auditory perception. However, SHCs may provide a more plausible model of sensory dynamics: First, they show structure over extended temporal scales, much like real sensory streams. This may reflect the fact that the processes generating sensory data are themselves (usually) neuronal dynamics showing winnerless competition. Secondly, many chaotic systems like the Lorenz attractor have only few states and cannot be extended to high dimensions in a straightforward fashion. This was no problem in our previous model, where we modelled a series of simple chirps, with varying amplitude and frequency <xref ref-type="bibr" rid="pcbi.1000464-Kiebel1">[6]</xref>. However, it would be difficult to generate sequences of distinct states that populate a high dimensional state-space; e.g. phonemes in speech. In contrast, stable heteroclinic channels can be formulated easily in high dimensional state spaces.</p>
<p>In this paper, we used a generative model which was formally identical to the process actually generating sensory input. We did this for simplicity; however, any generative model that could predict sensory input would be sufficient. In one sense, there is no true model because it is impossible to disambiguate between models that have different forms but make the same predictions. This is a common issue in ill-posed inverse problems, where there are an infinite number of models that could explain the same data. In this context the best model is usually identified as the most parsimonious. Furthermore, we are not suggesting that all aspects of perception can be framed in terms of the inversion of SHCs; we only consider recognition of those sensory data that are generated by mechanisms that are formally similar to the itinerant but structured dynamics of SHCs.</p>
<p>The proof-of-concept presented above makes the SHC hierarchy a potential candidate for speech recognition models. The recognition dynamics we simulated can outpace the dynamics they are trying to recognise. In all our simulations, after some initial transient, the recognition started tracking the veridical states early in the sequence. For example, the scheme can identify the correct syllable before all of its phonemes have been heard. We only simulated two levels, but this feature of fast recognition on exposure to brief parts of the sequence may hold for many more levels. Such rapid recognition of potentially long sequences is seen in real systems; e.g., we can infer that someone is making a cup of tea from observing a particular movement, like getting a teabag out of a kitchen cupboard. The reason why recognition can be fast is that the generative model is nonlinear (through the top-down control of attractor manifolds). With nonlinearities, slow time-scales in hierarchical sequences can be recognized rapidly because they disclose themselves in short unique sequences in the sensory input. Furthermore, we demonstrated another requirement for efficient communication: recognition signals, via prediction error, when unrecognised syllables cannot be decoded with its phonotactic model. This is important, because, an agent can decide online whether its decoding of the message is successful or not. Following the free-energy principle, this would oblige the agent to act on its environment, so that future prediction error is minimized <xref ref-type="bibr" rid="pcbi.1000464-Friston1">[18]</xref>. For example, the prediction error could prompt an action (‘repeat, please’) and initiate learning of new phonotactic rules.</p>
<p>Another aspect of SHC-based models is that they can recombine sensory primitives like phonemes in a large number of ways. This means that neuronal networks implementing SHC dynamics, based on a few primitives at the first level, can encode a large number of sequences. This feature is critical for encoding words in a language; e.g., every language contains many more words than phonemes <xref ref-type="bibr" rid="pcbi.1000464-Nowak1">[53]</xref>. The number of sequences that a SHC system can encode is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.e119" xlink:type="simple"/><label>(10)</label></disp-formula>where <italic>N</italic> is the number of elements <xref ref-type="bibr" rid="pcbi.1000464-Rabinovich2">[22]</xref>. This would mean, in theory, that the number of states that can be encoded with a sequence, given a few dozens primitives, is nearly endless. It is unlikely that this full capacity is exploited in communication. Rather, for efficient communication, it might be useful to restrict the number of admissible sequences to make them identifiable early in the sequence.</p>
<p>We did not equip the recognition model with a model of the silent periods at the beginning and end of a word (<xref ref-type="fig" rid="pcbi-1000464-g003">Fig. 3A</xref>). It is interesting to see how recognition resolves this: to approximate silence, the system held hidden phoneme states very negative by driving the states away from the SHC attractor and tolerating the violation of top-down predictions. However, the tolerance is limited as can be seen by the slightly positive inferred hidden states (<xref ref-type="fig" rid="pcbi-1000464-g003">Fig. 3B</xref>). Such behaviour is beneficial for recognition because the agent, within bounds, can deviate from internal predictions. A built-in error tolerance which is sensitive to the kind of errors it should endure to make recognition robust is important in an uncertain world. Robustness to errors would be impossible with an inversion scheme based on a deterministic model, which assumes that the sensory input follows a deterministic trajectory without any noise on the environmental causes. With such a recognition system, the agent could not deal with (unexpected) silence, because the SHC-based inversion dynamics would attract the state-trajectory without any means of resolving the resulting prediction error between the zero (silent) sensory input and the internal predictions. Recognition schemes based on stochastic systems can deviate adaptively from prior predictions, with a tolerance related to the variance of the stochastic innovations. Optimising this second-order parameter then becomes critical for recognition (see <xref ref-type="bibr" rid="pcbi.1000464-Friston2">[20]</xref>).</p>
<sec id="s4a">
<title>Links to neuroscience</title>
<p>There is emerging evidence in several areas of neuroscience that temporal hierarchies play a critical role in brain function <xref ref-type="bibr" rid="pcbi.1000464-Kiebel1">[6]</xref>. The three areas where this is most evident are auditory processing <xref ref-type="bibr" rid="pcbi.1000464-vonKriegstein1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1000464-Wang1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1000464-Kumar1">[54]</xref>–<xref ref-type="bibr" rid="pcbi.1000464-Denham1">[56]</xref>, cognitive control <xref ref-type="bibr" rid="pcbi.1000464-Badre1">[57]</xref>–<xref ref-type="bibr" rid="pcbi.1000464-Koechlin1">[59]</xref>, and motor control <xref ref-type="bibr" rid="pcbi.1000464-Todorov1">[60]</xref>. Our conclusions are based on a generic recognition scheme <xref ref-type="bibr" rid="pcbi.1000464-Friston2">[20]</xref> and are therefore a consequence of our specific generative model, a temporal hierarchy of SHCs. This hierarchy of time-scales agrees well with the temporal anatomy of the hierarchical auditory system, where populations close to the periphery encode the fast acoustics, while higher areas form slower representations <xref ref-type="bibr" rid="pcbi.1000464-Davis1">[9]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Giraud1">[10]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Wang1">[37]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Nahum1">[38]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Chechik1">[61]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Nelken1">[62]</xref>. In particular, our model is consistent with findings that phonological (high) levels have strong expectations about the relevance of acoustic (low) dynamics <xref ref-type="bibr" rid="pcbi.1000464-Nahum1">[38]</xref>.</p>
<p>Neurobiological treatments of the present framework suppose that superficial pyramidal cell populations encode prediction error; it is these cells that contribute most to evoked responses as observed in magneto/electroencephalography (M/EEG) <xref ref-type="bibr" rid="pcbi.1000464-Nunez1">[63]</xref>. There is an analogy between the expression of prediction error in our simulations and mismatch or prediction violation responses observed empirically. In our simulations, prediction error due to a deviation from expectations is resolved by all levels (<xref ref-type="fig" rid="pcbi-1000464-g004">Fig. 4B</xref>). This might be an explanation for prominent responses to prediction violations to be spatially distributed, e.g., the mismatch negativity, the P300, and the N400 all seem to involve various brain sources in temporal and frontal regions <xref ref-type="bibr" rid="pcbi.1000464-Lau1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1000464-Friedman1">[46]</xref>, <xref ref-type="bibr" rid="pcbi.1000464-Garrido1">[64]</xref>–<xref ref-type="bibr" rid="pcbi.1000464-VanPetten1">[66]</xref>. Inference on predictable auditory streams has been studied and modelled in several ways, in an attempt to explain the rapid recognition of words in the context of sentences, e.g., <xref ref-type="bibr" rid="pcbi.1000464-Nahum1">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1000464-MarslenWilson1">[67]</xref>–<xref ref-type="bibr" rid="pcbi.1000464-Norris2">[70]</xref>. Our simulations show how, in principle, these accounts might be implemented in terms of neuronal population dynamics.</p>
</sec><sec id="s4b">
<title>Links to computational models</title>
<p>Learning, storing, inferring and executing sequences is a key topic in experimental <xref ref-type="bibr" rid="pcbi.1000464-Botvinick2">[71]</xref>–<xref ref-type="bibr" rid="pcbi.1000464-Redcay1">[79]</xref>, and theoretical neurosciences <xref ref-type="bibr" rid="pcbi.1000464-Berns1">[80]</xref>–<xref ref-type="bibr" rid="pcbi.1000464-Jensen1">[82]</xref>; and robotics <xref ref-type="bibr" rid="pcbi.1000464-Kulvicius1">[83]</xref>–<xref ref-type="bibr" rid="pcbi.1000464-Wyss1">[86]</xref>. An early approach to modelling sequence processing focussed on feed-forward architectures. However, it was realised quickly that these networks could not store long sequences, because new input overwrote the internal representation of past states. The solution was to introduce explicit memory into recurrent networks, in various forms; e.g. as contextual nodes or ‘short-term memory’ <xref ref-type="bibr" rid="pcbi.1000464-Cleeremans1">[87]</xref>,<xref ref-type="bibr" rid="pcbi.1000464-Hochreiter1">[88]</xref>. Although framed in different terms, these approaches can be seen as an approximation to temporal hierarchies, where different units encode representations at different time-scales.</p>
<p>A central issue in modelling perception is how sequences are not just recalled but used as predictions for incoming sensory input. This requires the ‘dynamic fusion’ of bottom-up sensory input and top-down predictions, Several authors e.g., <xref ref-type="bibr" rid="pcbi.1000464-Kulvicius1">[83]</xref>, <xref ref-type="bibr" rid="pcbi.1000464-Berniker1">[89]</xref>–<xref ref-type="bibr" rid="pcbi.1000464-Yamashita1">[92]</xref> use recurrent networks to implement this fusion. Exact Bayesian schemes based on discrete hierarchical hidden Markov models, specified as a temporal hierarchy, have been used to implement memory and recognition <xref ref-type="bibr" rid="pcbi.1000464-George1">[93]</xref>. Here, we have used the free-energy principle (i.e. variational Bayesian inference on continuous hierarchical dynamical systems) to show how the ensuing recognition process leads naturally to a scheme which can deal with fast sequential inputs.</p>
<p>In conclusion, we have described a scheme for inferring the causes of sensory sequences with hierarchical structure. The key features of this scheme are: (i) the ability to describe natural sensory input as hierarchical and dynamic sequences, (ii) modeling this input using generative models, (iii) using dynamic systems theory to create plausible models, and (iv) online Bayesian inversion of the resulting models. This scheme is theoretically principled but is also accountable to the empirical evidence available from the auditory system; furthermore, the ensuing recognition dynamics are reminiscent of real brain responses.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1000464.s001" mimetype="video/mpeg" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000464.s001" xlink:type="simple"><label>Audio S1</label><caption>
<p>Phoneme sequence generated in first simulation - mpg-file containing phoneme sequence sampled at 22050 Hz. The time courses of the four vowels can be seen in <xref ref-type="fig" rid="pcbi-1000464-g003">Fig. 3A</xref> (top left).</p>
<p>(0.18 MB MPG)</p>
</caption></supplementary-material></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1000464-Poeppel1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Idsardi</surname><given-names>WJ</given-names></name>
<name name-style="western"><surname>van</surname><given-names>WV</given-names></name>
</person-group>             <year>2008</year>             <article-title>Speech perception at the interface of neurobiology and linguistics.</article-title>             <source>PhilosTransRSocLond B BiolSci</source>             <volume>363</volume>             <fpage>1071</fpage>             <lpage>1086</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Zatorre1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zatorre</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>Belin</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Penhune</surname><given-names>VB</given-names></name>
</person-group>             <year>2002</year>             <article-title>Structure and function of auditory cortex: music and speech.</article-title>             <source>Trends Cogn Sci</source>             <volume>6</volume>             <fpage>37</fpage>             <lpage>46</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Simon1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Simon</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Craig</surname><given-names>KD</given-names></name>
<name name-style="western"><surname>Miltner</surname><given-names>WH</given-names></name>
<name name-style="western"><surname>Rainville</surname><given-names>P</given-names></name>
</person-group>             <year>2006</year>             <article-title>Brain responses to dynamic facial expressions of pain.</article-title>             <source>Pain</source>             <volume>126</volume>             <fpage>309</fpage>             <lpage>318</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Thompson1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Thompson</surname><given-names>JC</given-names></name>
<name name-style="western"><surname>Hardee</surname><given-names>JE</given-names></name>
<name name-style="western"><surname>Panayiotou</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Crewther</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Puce</surname><given-names>A</given-names></name>
</person-group>             <year>2007</year>             <article-title>Common and distinct brain activation to viewing dynamic sequences of face and hand movements.</article-title>             <source>Neuroimage</source>             <volume>37</volume>             <fpage>966</fpage>             <lpage>973</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Deng1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Deng</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Yu</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Acero</surname><given-names>A</given-names></name>
</person-group>             <year>2006</year>             <article-title>Structured speech modeling.</article-title>             <source>Ieee Transactions on Audio Speech and Language Processing</source>             <volume>14</volume>             <fpage>1492</fpage>             <lpage>1504</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Kiebel1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name>
<name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>
</person-group>             <year>2008</year>             <article-title>A hierarchy of time-scales and the brain.</article-title>             <source>PLoS ComputBiol</source>             <volume>4</volume>             <fpage>e1000209</fpage>          </element-citation></ref>
<ref id="pcbi.1000464-Long1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Long</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name>
</person-group>             <year>2008</year>             <article-title>Using temperature to analyse temporal dynamics in the songbird motor pathway.</article-title>             <source>Nature</source>             <volume>456</volume>             <fpage>189</fpage>             <lpage>194</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Sen1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sen</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name>
<name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name>
</person-group>             <year>2001</year>             <article-title>Feature analysis of natural sounds in the songbird auditory forebrain.</article-title>             <source>JNeurophysiol</source>             <volume>86</volume>             <fpage>1445</fpage>             <lpage>1458</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Davis1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Davis</surname><given-names>MH</given-names></name>
<name name-style="western"><surname>Johnsrude</surname><given-names>IS</given-names></name>
</person-group>             <year>2003</year>             <article-title>Hierarchical processing in spoken language comprehension.</article-title>             <source>JNeurosci</source>             <volume>23</volume>             <fpage>3423</fpage>             <lpage>3431</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Giraud1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Giraud</surname><given-names>AL</given-names></name>
<name name-style="western"><surname>Lorenzi</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Ashburner</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Wable</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Johnsrude</surname><given-names>I</given-names></name>
<etal/></person-group>             <year>2000</year>             <article-title>Representation of the temporal envelope of sounds in the human brain.</article-title>             <source>JNeurophysiol</source>             <volume>84</volume>             <fpage>1588</fpage>             <lpage>1598</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Overath1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Overath</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Kumar</surname><given-names>S</given-names></name>
<name name-style="western"><surname>von Kriegstein</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Griffiths</surname><given-names>TD</given-names></name>
</person-group>             <year>2008</year>             <article-title>Encoding of spectral correlation over time in auditory cortex.</article-title>             <source>JNeurosci</source>             <volume>28</volume>             <fpage>13268</fpage>             <lpage>13273</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-vonKriegstein1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>von Kriegstein</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Patterson</surname><given-names>RD</given-names></name>
<name name-style="western"><surname>Griffiths</surname><given-names>TD</given-names></name>
</person-group>             <year>2008</year>             <article-title>Task-dependent modulation of medial geniculate body is behaviorally relevant for speech recognition.</article-title>             <source>CurrBiol</source>             <volume>18</volume>             <fpage>1855</fpage>             <lpage>1859</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Wolpert1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name>
<name name-style="western"><surname>Ghahramani</surname><given-names>Z</given-names></name>
<name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name>
</person-group>             <year>1995</year>             <article-title>An internal model for sensorimotor integration.</article-title>             <source>Science</source>             <volume>269</volume>             <fpage>1880</fpage>             <lpage>1882</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Kersten1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kersten</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Mamassian</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Yuille</surname><given-names>A</given-names></name>
</person-group>             <year>2004</year>             <article-title>Object perception as Bayesian inference.</article-title>             <source>Annual Review of Psychology</source>             <volume>55</volume>             <fpage>271</fpage>             <lpage>304</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Yuille1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Yuille</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Kersten</surname><given-names>D</given-names></name>
</person-group>             <year>2006</year>             <article-title>Vision as Bayesian inference: analysis by synthesis?</article-title>             <source>Trends in Cognitive Sciences</source>             <volume>10</volume>             <fpage>301</fpage>             <lpage>308</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Lee1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lee</surname><given-names>TS</given-names></name>
<name name-style="western"><surname>Mumford</surname><given-names>D</given-names></name>
</person-group>             <year>2003</year>             <article-title>Hierarchical Bayesian inference in the visual cortex.</article-title>             <source>Journal of the Optical Society of America a-Optics Image Science and Vision</source>             <volume>20</volume>             <fpage>1434</fpage>             <lpage>1448</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Kording1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kording</surname><given-names>KP</given-names></name>
<name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name>
</person-group>             <year>2004</year>             <article-title>Bayesian integration in sensorimotor learning.</article-title>             <source>Nature</source>             <volume>427</volume>             <fpage>244</fpage>             <lpage>247</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Friston1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Kilner</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Harrison</surname><given-names>L</given-names></name>
</person-group>             <year>2006</year>             <article-title>A free energy principle for the brain.</article-title>             <source>JPhysiol Paris</source>             <volume>100</volume>             <fpage>70</fpage>             <lpage>87</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Rabinovich1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rabinovich</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Huerta</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Laurent</surname><given-names>G</given-names></name>
</person-group>             <year>2008</year>             <article-title>Neuroscience - Transient dynamics for neural processing.</article-title>             <source>Science</source>             <volume>321</volume>             <fpage>48</fpage>             <lpage>50</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Friston2"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
</person-group>             <year>2008</year>             <article-title>Hierarchical models in the brain.</article-title>             <source>PLoS ComputBiol</source>             <volume>4</volume>             <fpage>e1000211</fpage>          </element-citation></ref>
<ref id="pcbi.1000464-Fukai1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fukai</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Tanaka</surname><given-names>S</given-names></name>
</person-group>             <year>1997</year>             <article-title>A simple neural network exhibiting selective activation of neuronal ensembles: from winner-take-all to winners-share-all.</article-title>             <source>Neural Comput</source>             <volume>9</volume>             <fpage>77</fpage>             <lpage>97</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Rabinovich2"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name>
<name name-style="western"><surname>Varona</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Selverston</surname><given-names>AI</given-names></name>
<name name-style="western"><surname>Abarbanel</surname><given-names>HDI</given-names></name>
</person-group>             <year>2006</year>             <article-title>Dynamical principles in neuroscience.</article-title>             <source>Reviews of Modern Physics</source>             <volume>78</volume>             <fpage>1213</fpage>             <lpage>1265</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Rabinovich3"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name>
<name name-style="western"><surname>Huerta</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Varona</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Afraimovich</surname><given-names>VS</given-names></name>
</person-group>             <year>2008</year>             <article-title>Transient cognitive dynamics, metastability, and decision making.</article-title>             <source>Plos Computational Biology</source>             <volume>4</volume>          </element-citation></ref>
<ref id="pcbi.1000464-Afraimovich1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Afraimovich</surname><given-names>VS</given-names></name>
<name name-style="western"><surname>Zhigulin</surname><given-names>VP</given-names></name>
<name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name>
</person-group>             <year>2004</year>             <article-title>On the origin of reproducible sequential activity in neural circuits.</article-title>             <source>Chaos</source>             <volume>14</volume>             <fpage>1123</fpage>             <lpage>1129</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Rabinovich4"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rabinovich</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Volkovskii</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Lecanda</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Huerta</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Abarbanel</surname><given-names>HDI</given-names></name>
<etal/></person-group>             <year>2001</year>             <article-title>Dynamical encoding by networks of competing neuron groups: Winnerless competition.</article-title>             <source>Physical Review Letters</source>             <volume>8706</volume>          </element-citation></ref>
<ref id="pcbi.1000464-Friston3"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>
</person-group>             <year>1997</year>             <article-title>Transients, metastability, and neuronal dynamics.</article-title>             <source>Neuroimage</source>             <volume>5</volume>             <fpage>164</fpage>             <lpage>171</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Varona1"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Varona</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name>
<name name-style="western"><surname>Selverston</surname><given-names>AI</given-names></name>
<name name-style="western"><surname>Arshavsky</surname><given-names>YI</given-names></name>
</person-group>             <year>2002</year>             <article-title>Winnerless competition between sensory neurons generates chaos: A possible mechanism for molluscan hunting behavior.</article-title>             <source>Chaos</source>             <volume>12</volume>             <fpage>672</fpage>             <lpage>677</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Afraimovich2"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Afraimovich</surname><given-names>VS</given-names></name>
<name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name>
<name name-style="western"><surname>Varona</surname><given-names>P</given-names></name>
</person-group>             <year>2004</year>             <article-title>Heteroclinic contours in neural ensembles and the winnerless competition principle.</article-title>             <source>International Journal of Bifurcation and Chaos</source>             <volume>14</volume>             <fpage>1195</fpage>             <lpage>1208</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Breakspear1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Breakspear</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Terry</surname><given-names>JR</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>
</person-group>             <year>2003</year>             <article-title>Modulation of excitatory synaptic coupling facilitates synchronization and complex dynamics in a nonlinear model of neuronal dynamics.</article-title>             <source>Neurocomputing</source>             <volume>52-4</volume>             <fpage>151</fpage>             <lpage>158</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Durstewitz1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Durstewitz</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Deco</surname><given-names>G</given-names></name>
</person-group>             <year>2008</year>             <article-title>Computational significance of transient dynamics in cortical networks.</article-title>             <source>European Journal of Neuroscience</source>             <volume>27</volume>             <fpage>217</fpage>             <lpage>227</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Buonomano1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Buonomano</surname><given-names>DV</given-names></name>
<name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>
</person-group>             <year>2009</year>             <article-title>State-dependent computations: spatiotemporal processing in cortical networks.</article-title>             <source>NatRevNeurosci</source>             <volume>10</volume>             <fpage>113</fpage>             <lpage>125</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Ivanchenko1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ivanchenko</surname><given-names>MV</given-names></name>
<name name-style="western"><surname>Nowotny</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Selverston</surname><given-names>AI</given-names></name>
<name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name>
</person-group>             <year>2008</year>             <article-title>Pacemaker and network mechanisms of rhythm generation: Cooperation and competition.</article-title>             <source>Journal of Theoretical Biology</source>             <volume>253</volume>             <fpage>452</fpage>             <lpage>461</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Beal1"><label>33</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Beal</surname><given-names>MJ</given-names></name>
</person-group>             <year>2003</year>             <comment>Variational algorithms for approximate Bayesian inference [PhD]: University of London</comment>          </element-citation></ref>
<ref id="pcbi.1000464-Friston4"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>
<name name-style="western"><surname>Trujillo-Barreto</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>DEM: a variational treatment of dynamic systems.</article-title>             <source>Neuroimage</source>             <volume>41</volume>             <fpage>849</fpage>             <lpage>885</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Friston5"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
</person-group>             <year>2005</year>             <article-title>A theory of cortical responses.</article-title>             <source>Philosophical Transactions of the Royal Society B-Biological Sciences</source>             <volume>360</volume>             <fpage>815</fpage>             <lpage>836</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Creutzfeldt1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Creutzfeldt</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Hellweg</surname><given-names>FC</given-names></name>
<name name-style="western"><surname>Schreiner</surname><given-names>C</given-names></name>
</person-group>             <year>1980</year>             <article-title>Thalamocortical Transformation of Responses to Complex Auditory-Stimuli.</article-title>             <source>Experimental Brain Research</source>             <volume>39</volume>             <fpage>87</fpage>             <lpage>104</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Wang1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wang</surname><given-names>X</given-names></name>
<name name-style="western"><surname>Lu</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Bendor</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Bartlett</surname><given-names>E</given-names></name>
</person-group>             <year>2008</year>             <article-title>Neural coding of temporal information in auditory thalamus and cortex.</article-title>             <source>Neuroscience</source>             <volume>154</volume>             <fpage>294</fpage>             <lpage>303</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Nahum1"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nahum</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Nelken</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Ahissar</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Low-level information and high-level perception: The case of speech in noise.</article-title>             <source>Plos Biology</source>             <volume>6</volume>             <fpage>978</fpage>             <lpage>991</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Holmberg1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Holmberg</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Gelbart</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Hemmert</surname><given-names>W</given-names></name>
</person-group>             <year>2007</year>             <article-title>Speech encoding in a model of peripheral auditory processing: Quantitative assessment by means of automatic speech recognition.</article-title>             <source>Speech Communication</source>             <volume>49</volume>             <fpage>917</fpage>             <lpage>932</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Sumner1"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sumner</surname><given-names>CJ</given-names></name>
<name name-style="western"><surname>Lopez-Poveda</surname><given-names>EA</given-names></name>
<name name-style="western"><surname>O'Mard</surname><given-names>LP</given-names></name>
<name name-style="western"><surname>Meddis</surname><given-names>R</given-names></name>
</person-group>             <year>2002</year>             <article-title>A revised model of the inner-hair cell and auditory-nerve complex.</article-title>             <source>Journal of the Acoustical Society of America</source>             <volume>111</volume>             <fpage>2178</fpage>             <lpage>2188</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-DehaeneLambertz1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Dupoux</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Gout</surname><given-names>A</given-names></name>
</person-group>             <year>2000</year>             <article-title>Electrophysiological correlates of phonological processing: A cross-linguistic study.</article-title>             <source>Journal of Cognitive Neuroscience</source>             <volume>12</volume>             <fpage>635</fpage>             <lpage>647</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Eulitz1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Eulitz</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Lahiri</surname><given-names>A</given-names></name>
</person-group>             <year>2004</year>             <article-title>Neurobiological evidence for abstract phonological representations in the mental lexicon during speech recognition.</article-title>             <source>Journal of Cognitive Neuroscience</source>             <volume>16</volume>             <fpage>577</fpage>             <lpage>583</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Friedrich1"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friedrich</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name>
</person-group>             <year>2005</year>             <article-title>Phonotactic knowledge and lexical-semantic processing in one-year-olds: Brain responses to words and nonsense words in picture contexts.</article-title>             <source>Journal of Cognitive Neuroscience</source>             <volume>17</volume>             <fpage>1785</fpage>             <lpage>1802</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Friederici1"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name>
</person-group>             <year>2002</year>             <article-title>Towards a neural basis of auditory sentence processing.</article-title>             <source>Trends in Cognitive Sciences</source>             <volume>6</volume>             <fpage>78</fpage>             <lpage>84</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Lau1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lau</surname><given-names>EF</given-names></name>
<name name-style="western"><surname>Phillips</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>
</person-group>             <year>2008</year>             <article-title>A cortical network for semantics: (de)constructing the N400.</article-title>             <source>Nature Reviews Neuroscience</source>             <volume>9</volume>             <fpage>920</fpage>             <lpage>933</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Friedman1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friedman</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Cycowicz</surname><given-names>YM</given-names></name>
<name name-style="western"><surname>Gaeta</surname><given-names>H</given-names></name>
</person-group>             <year>2001</year>             <article-title>The novelty P3: an event-related brain potential (ERP) sign of the brain's evaluation of novelty.</article-title>             <source>Neuroscience and Biobehavioral Reviews</source>             <volume>25</volume>             <fpage>355</fpage>             <lpage>373</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Foulke1"><label>47</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Foulke</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Sticht</surname><given-names>TG</given-names></name>
</person-group>             <year>1969</year>             <article-title>Review of Research on Intelligibility and Comprehension of Accelerated Speech.</article-title>             <source>Psychological Bulletin</source>             <volume>72</volume>             <fpage>50</fpage>             <lpage>&amp;</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Versfeld1"><label>48</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Versfeld</surname><given-names>NJ</given-names></name>
<name name-style="western"><surname>Dreschler</surname><given-names>WA</given-names></name>
</person-group>             <year>2002</year>             <article-title>The relationship between the intelligibility of time-compressed speech and speech in noise in young and elderly listeners.</article-title>             <source>Journal of the Acoustical Society of America</source>             <volume>111</volume>             <fpage>401</fpage>             <lpage>408</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Pisoni1"><label>49</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pisoni</surname><given-names>DB</given-names></name>
</person-group>             <year>1993</year>             <article-title>Long-Term-Memory in Speech-Perception - Some New Findings on Talker Variability, Speaking Rate and Perceptual-Learning.</article-title>             <source>Speech Communication</source>             <volume>13</volume>             <fpage>109</fpage>             <lpage>125</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Vaughan1"><label>50</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Vaughan</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Storzbach</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Furukawa</surname><given-names>I</given-names></name>
</person-group>             <year>2006</year>             <article-title>Sequencing versus nonsequencing working memory in understanding of rapid speech by older listeners.</article-title>             <source>Journal of the American Academy of Audiology</source>             <volume>17</volume>             <fpage>506</fpage>             <lpage>518</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Budhiraja1"><label>51</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Budhiraja</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Chen</surname><given-names>LJ</given-names></name>
<name name-style="western"><surname>Lee</surname><given-names>C</given-names></name>
</person-group>             <year>2007</year>             <article-title>A survey of numerical methods for nonlinear filtering problems.</article-title>             <source>Physica D-Nonlinear Phenomena</source>             <volume>230</volume>             <fpage>27</fpage>             <lpage>36</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Judd1"><label>52</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Judd</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Smith</surname><given-names>LA</given-names></name>
</person-group>             <year>2004</year>             <article-title>Indistinguishable states II - The imperfect model scenario.</article-title>             <source>Physica D-Nonlinear Phenomena</source>             <volume>196</volume>             <fpage>224</fpage>             <lpage>242</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Nowak1"><label>53</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nowak</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Krakauer</surname><given-names>DC</given-names></name>
<name name-style="western"><surname>Dress</surname><given-names>A</given-names></name>
</person-group>             <year>1999</year>             <article-title>An error limit for the evolution of language.</article-title>             <source>ProcBiolSci</source>             <volume>266</volume>             <fpage>2131</fpage>             <lpage>2136</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Kumar1"><label>54</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kumar</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>
<name name-style="western"><surname>Warren</surname><given-names>JD</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>
<name name-style="western"><surname>Griffiths</surname><given-names>TD</given-names></name>
</person-group>             <year>2007</year>             <article-title>Hierarchical processing of auditory objects in humans.</article-title>             <source>PLoSComputBiol</source>             <volume>3</volume>             <fpage>e100</fpage>          </element-citation></ref>
<ref id="pcbi.1000464-Boemio1"><label>55</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Boemio</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Fromm</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Braun</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>
</person-group>             <year>2005</year>             <article-title>Hierarchical and asymmetric temporal sensitivity in human auditory cortices.</article-title>             <source>NatNeurosci</source>             <volume>8</volume>             <fpage>389</fpage>             <lpage>395</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Denham1"><label>56</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Denham</surname><given-names>SL</given-names></name>
<name name-style="western"><surname>Winkler</surname><given-names>I</given-names></name>
</person-group>             <year>2006</year>             <article-title>The role of predictive models in the formation of auditory streams.</article-title>             <source>Journal of Physiology-Paris</source>             <volume>100</volume>             <fpage>154</fpage>             <lpage>170</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Badre1"><label>57</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Badre</surname><given-names>D</given-names></name>
</person-group>             <year>2008</year>             <article-title>Cognitive control, hierarchy, and the rostro-caudal organization of the frontal lobes.</article-title>             <source>Trends Cogn Sci</source>          </element-citation></ref>
<ref id="pcbi.1000464-Botvinick1"><label>58</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Botvinick</surname><given-names>MM</given-names></name>
</person-group>             <year>2008</year>             <article-title>Hierarchical models of behavior and prefrontal function.</article-title>             <source>Trends Cogn Sci</source>          </element-citation></ref>
<ref id="pcbi.1000464-Koechlin1"><label>59</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Koechlin</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Summerfield</surname><given-names>C</given-names></name>
</person-group>             <year>2007</year>             <article-title>An information theoretical approach to prefrontal executive function.</article-title>             <source>Trends Cogn Sci</source>             <volume>11</volume>             <fpage>229</fpage>             <lpage>235</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Todorov1"><label>60</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Todorov</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Li</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Pan</surname><given-names>X</given-names></name>
</person-group>             <year>2005</year>             <article-title>From task parameters to motor synergies: A hierarchical framework for approximately-optimal control of redundant manipulators.</article-title>             <source>JRobotSyst</source>             <volume>22</volume>             <fpage>691</fpage>             <lpage>710</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Chechik1"><label>61</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chechik</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Anderson</surname><given-names>MJ</given-names></name>
<name name-style="western"><surname>Bar-Yosef</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Young</surname><given-names>ED</given-names></name>
<name name-style="western"><surname>Tishby</surname><given-names>N</given-names></name>
<etal/></person-group>             <year>2006</year>             <article-title>Reduction of information redundancy in the ascending auditory pathway.</article-title>             <source>Neuron</source>             <volume>51</volume>             <fpage>359</fpage>             <lpage>368</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Nelken1"><label>62</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nelken</surname><given-names>I</given-names></name>
</person-group>             <year>2008</year>             <article-title>Processing of complex sounds in the auditory system.</article-title>             <source>Current Opinion in Neurobiology</source>             <volume>18</volume>             <fpage>413</fpage>             <lpage>417</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Nunez1"><label>63</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nunez</surname><given-names>PL</given-names></name>
<name name-style="western"><surname>Silberstein</surname><given-names>RB</given-names></name>
</person-group>             <year>2000</year>             <article-title>On the relationship of synaptic activity to macroscopic measurements: Does co-registration of EEG with fMRI make sense?</article-title>             <source>Brain Topography</source>             <volume>13</volume>             <fpage>79</fpage>             <lpage>96</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Garrido1"><label>64</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Garrido</surname><given-names>MI</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>
<name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name>
<name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>
<name name-style="western"><surname>Baldeweg</surname><given-names>T</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>The functional anatomy of the MMN: A DCM study of the roving paradigm.</article-title>             <source>Neuroimage</source>             <volume>42</volume>             <fpage>936</fpage>             <lpage>944</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Maess1"><label>65</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Maess</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Herrmann</surname><given-names>CS</given-names></name>
<name name-style="western"><surname>Hahne</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Nakamura</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name>
</person-group>             <year>2006</year>             <article-title>Localizing the distributed language network responsible for the N400 measured by MEG during auditory sentence processing.</article-title>             <source>Brain Research</source>             <volume>1096</volume>             <fpage>163</fpage>             <lpage>172</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-VanPetten1"><label>66</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Van Petten</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Luka</surname><given-names>BJ</given-names></name>
</person-group>             <year>2006</year>             <article-title>Neural localization of semantic context effects in electromagnetic and hemodynamic studies.</article-title>             <source>Brain and Language</source>             <volume>97</volume>             <fpage>279</fpage>             <lpage>293</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-MarslenWilson1"><label>67</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Marslen-Wilson</surname><given-names>WD</given-names></name>
<name name-style="western"><surname>Bouma</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Bouwhuis</surname><given-names>D</given-names></name>
</person-group>             <year>1984</year>             <source>Function and process in spoken word recognition. Attention and Performance X: Control of Language Processes</source>             <publisher-loc>Hillsdale, , N.J.</publisher-loc>             <publisher-name>Erlbaum</publisher-name>             <fpage>125</fpage>             <lpage>150</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-McClelland1"><label>68</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>McClelland</surname><given-names>JL</given-names></name>
<name name-style="western"><surname>Elman</surname><given-names>JL</given-names></name>
</person-group>             <year>1986</year>             <article-title>The Trace Model of Speech-Perception.</article-title>             <source>Cognitive Psychology</source>             <volume>18</volume>             <fpage>1</fpage>             <lpage>86</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Norris1"><label>69</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Norris</surname><given-names>D</given-names></name>
</person-group>             <year>1994</year>             <article-title>Shortlist - A Connectionist Model of Continuous Speech Recognition.</article-title>             <source>Cognition</source>             <volume>52</volume>             <fpage>189</fpage>             <lpage>234</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Norris2"><label>70</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Norris</surname><given-names>D</given-names></name>
<name name-style="western"><surname>McQueen</surname><given-names>JM</given-names></name>
</person-group>             <year>2008</year>             <article-title>Shortlist B: A Bayesian model of continuous speech recognition.</article-title>             <source>Psychological Review</source>             <volume>115</volume>             <fpage>357</fpage>             <lpage>395</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Botvinick2"><label>71</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Botvinick</surname><given-names>MM</given-names></name>
<name name-style="western"><surname>Plaut</surname><given-names>DC</given-names></name>
</person-group>             <year>2006</year>             <article-title>Short-term memory for serial order: a recurrent neural network model.</article-title>             <source>PsycholRev</source>             <volume>113</volume>             <fpage>201</fpage>             <lpage>233</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Broome1"><label>72</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Broome</surname><given-names>BM</given-names></name>
<name name-style="western"><surname>Jayaraman</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Laurent</surname><given-names>G</given-names></name>
</person-group>             <year>2006</year>             <article-title>Encoding and decoding of overlapping odor sequences.</article-title>             <source>Neuron</source>             <volume>51</volume>             <fpage>467</fpage>             <lpage>482</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Davis2"><label>73</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Davis</surname><given-names>MH</given-names></name>
<name name-style="western"><surname>Johnsrude</surname><given-names>IS</given-names></name>
</person-group>             <year>2007</year>             <article-title>Hearing speech sounds: top-down influences on the interface between audition and speech perception.</article-title>             <source>HearRes</source>             <volume>229</volume>             <fpage>132</fpage>             <lpage>147</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Fee1"><label>74</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name>
<name name-style="western"><surname>Kozhevnikov</surname><given-names>AA</given-names></name>
<name name-style="western"><surname>Hahnloser</surname><given-names>RH</given-names></name>
</person-group>             <year>2004</year>             <article-title>Neural mechanisms of vocal sequence generation in the songbird.</article-title>             <source>AnnNYAcadSci</source>             <volume>1016</volume>             <fpage>153</fpage>             <lpage>170</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Ji1"><label>75</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ji</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Wilson</surname><given-names>MA</given-names></name>
</person-group>             <year>2008</year>             <article-title>Firing rate dynamics in the hippocampus induced by trajectory learning.</article-title>             <source>JNeurosci</source>             <volume>28</volume>             <fpage>4679</fpage>             <lpage>4689</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Koechlin2"><label>76</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Koechlin</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Jubault</surname><given-names>T</given-names></name>
</person-group>             <year>2006</year>             <article-title>Broca's area and the hierarchical organization of human behavior.</article-title>             <source>Neuron</source>             <volume>50</volume>             <fpage>963</fpage>             <lpage>974</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Kumaran1"><label>77</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kumaran</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Maguire</surname><given-names>EA</given-names></name>
</person-group>             <year>2007</year>             <article-title>Match mismatch processes underlie human hippocampal responses to associative novelty.</article-title>             <source>JNeurosci</source>             <volume>27</volume>             <fpage>8517</fpage>             <lpage>8524</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Nadasdy1"><label>78</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nadasdy</surname><given-names>Z</given-names></name>
<name name-style="western"><surname>Hirase</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Czurko</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Csicsvari</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Buzsaki</surname><given-names>G</given-names></name>
</person-group>             <year>1999</year>             <article-title>Replay and time compression of recurring spike sequences in the hippocampus.</article-title>             <source>JNeurosci</source>             <volume>19</volume>             <fpage>9497</fpage>             <lpage>9507</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Redcay1"><label>79</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Redcay</surname><given-names>E</given-names></name>
</person-group>             <year>2008</year>             <article-title>The superior temporal sulcus performs a common function for social and speech perception: implications for the emergence of autism.</article-title>             <source>NeurosciBiobehavRev</source>             <volume>32</volume>             <fpage>123</fpage>             <lpage>142</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Berns1"><label>80</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Berns</surname><given-names>GS</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name>
</person-group>             <year>1998</year>             <article-title>A computational model of how the basal ganglia produce sequences.</article-title>             <source>Journal of Cognitive Neuroscience</source>             <volume>10</volume>             <fpage>108</fpage>             <lpage>121</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Elman1"><label>81</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Elman</surname><given-names>JL</given-names></name>
</person-group>             <year>1990</year>             <article-title>Finding Structure in Time.</article-title>             <source>Cognitive Science</source>             <volume>14</volume>             <fpage>179</fpage>             <lpage>211</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Jensen1"><label>82</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Jensen</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Lisman</surname><given-names>JE</given-names></name>
</person-group>             <year>1996</year>             <article-title>Theta/gamma networks with slow NMDA channels learn sequences and encode episodic memory: Role of NMDA channels in recall.</article-title>             <source>Learning &amp; Memory</source>             <volume>3</volume>             <fpage>264</fpage>             <lpage>278</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Kulvicius1"><label>83</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kulvicius</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Porr</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Worgotter</surname><given-names>F</given-names></name>
</person-group>             <year>2007</year>             <article-title>Chained learning architectures in a simple closed-loop behavioural context.</article-title>             <source>Biological Cybernetics</source>             <volume>97</volume>             <fpage>363</fpage>             <lpage>378</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Namikawa1"><label>84</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Namikawa</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Tani</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>A model for learning to segment temporal sequences, utilizing a mixture of RNN experts together with adaptive variance.</article-title>             <source>Neural Netw</source>             <volume>21</volume>             <fpage>1466</fpage>             <lpage>1475</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Tani1"><label>85</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tani</surname><given-names>J</given-names></name>
</person-group>             <year>2003</year>             <article-title>Learning to generate articulated behavior through the bottom-up and the top-down interaction processes.</article-title>             <source>Neural Netw</source>             <volume>16</volume>             <fpage>11</fpage>             <lpage>23</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Wyss1"><label>86</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wyss</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Konig</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Verschure</surname><given-names>PFMJ</given-names></name>
</person-group>             <year>2006</year>             <article-title>A model of the ventral visual system based on temporal stability and local memory.</article-title>             <source>Plos Biology</source>             <volume>4</volume>             <fpage>836</fpage>             <lpage>843</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Cleeremans1"><label>87</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cleeremans</surname><given-names>A</given-names></name>
<name name-style="western"><surname>McClelland</surname><given-names>JL</given-names></name>
</person-group>             <year>1991</year>             <article-title>Learning the Structure of Event Sequences.</article-title>             <source>Journal of Experimental Psychology-General</source>             <volume>120</volume>             <fpage>235</fpage>             <lpage>253</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Hochreiter1"><label>88</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hochreiter</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Schmidhuber</surname><given-names>J</given-names></name>
</person-group>             <year>1997</year>             <article-title>Long short-term memory.</article-title>             <source>Neural Computation</source>             <volume>9</volume>             <fpage>1735</fpage>             <lpage>1780</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Berniker1"><label>89</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Berniker</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Kording</surname><given-names>K</given-names></name>
</person-group>             <year>2008</year>             <article-title>Estimating the sources of motor errors for adaptation and generalization.</article-title>             <source>Nature Neuroscience</source>             <volume>11</volume>             <fpage>1454</fpage>             <lpage>1461</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Sakata1"><label>90</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sakata</surname><given-names>JT</given-names></name>
<name name-style="western"><surname>Brainard</surname><given-names>MS</given-names></name>
</person-group>             <year>2008</year>             <article-title>Online Contributions of Auditory Feedback to Neural Activity in Avian Song Control Circuitry.</article-title>             <source>Journal of Neuroscience</source>             <volume>28</volume>             <fpage>11378</fpage>             <lpage>11390</lpage>          </element-citation></ref>
<ref id="pcbi.1000464-Tani2"><label>91</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tani</surname><given-names>J</given-names></name>
</person-group>             <year>2007</year>             <article-title>On the interactions between top-down anticipation and bottom-up regression.</article-title>             <source>Front Neurorobotics</source>             <volume>1</volume>             <fpage>2</fpage>          </element-citation></ref>
<ref id="pcbi.1000464-Yamashita1"><label>92</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Yamashita</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Tani</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment.</article-title>             <source>Plos Computational Biology</source>             <volume>4</volume>          </element-citation></ref>
<ref id="pcbi.1000464-George1"><label>93</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>George</surname><given-names>D</given-names></name>
</person-group>             <year>2008</year>             <comment>How the brain might work: A hierarchical and temporal model for learning and recognition [Ph.D.]: Stanford University</comment>          </element-citation></ref>
</ref-list>

</back>
</article>