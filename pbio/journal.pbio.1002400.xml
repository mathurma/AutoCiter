<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article article-type="other" dtd-version="3.0" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id>
<journal-title-group>
<journal-title>PLOS Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pbio.1002400</article-id>
<article-id pub-id-type="publisher-id">PBIOLOGY-D-15-02155</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Essay</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Thermodynamics</subject><subj-group><subject>Free energy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Topology</subject><subj-group><subject>Manifolds</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Geometry</subject><subj-group><subject>Symmetry</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Geometry</subject><subj-group><subject>Differential geometry</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Towards a Neuronal Gauge Theory</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Sengupta</surname>
<given-names>Biswa</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Tozzi</surname>
<given-names>Arturo</given-names>
</name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Cooray</surname>
<given-names>Gerald K.</given-names>
</name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Douglas</surname>
<given-names>Pamela K.</given-names>
</name>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Friston</surname>
<given-names>Karl J.</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Wellcome Trust Centre for Neuroimaging, University College London, London, United Kingdom</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Center for Nonlinear Science, University of North Texas, Denton, Texas, United States of America</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Clinical Neurophysiology, Karolinska University Hospital, Stockholm, Sweden</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>LINT Laboratory, University of California, Los Angeles, Los Angeles California, United States of America</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">b.sengupta@ucl.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>8</day>
<month>3</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="collection">
<month>3</month>
<year>2016</year>
</pub-date>
<volume>14</volume>
<issue>3</issue>
<elocation-id>e1002400</elocation-id>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Sengupta et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pbio.1002400"/>
<abstract>
<p>Given the amount of knowledge and data accruing in the neurosciences, is it time to formulate a general principle for neuronal dynamics that holds at evolutionary, developmental, and perceptual timescales? In this paper, we propose that the brain (and other self-organised biological systems) can be characterised via the mathematical apparatus of a gauge theory. The picture that emerges from this approach suggests that any biological system (from a neuron to an organism) can be cast as resolving uncertainty about its external milieu, either by changing its internal states or its relationship to the environment. Using formal arguments, we show that a gauge theory for neuronal dynamics—based on approximate Bayesian inference—has the potential to shed new light on phenomena that have thus far eluded a formal description, such as attention and the link between action and perception.</p>
</abstract>
<abstract abstract-type="toc">
<p>This Essay presents a formalism that not only provides a quantitative framework for modelling neural activity but also shows that neuronal dynamics across scales are described by the same principle.</p>
</abstract>
<funding-group>
<funding-statement>BS and KJF are supported by the Wellcome Trust (088130/Z/09/Z). BS is thankful to the Mathematical Biosciences Institute for hosting him, and facilitating communication with experts working on the symmetry perspective of dynamical systems. We thank John Ashburner for his comments on an initial draft of this paper. GKC is supported by a postdoctoral fellowship from the Swedish Brain Foundation (Hjärnfonden). PD is funded by the Klingenstein Foundation, and the Keck Foundation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="1"/>
<table-count count="0"/>
<page-count count="12"/>
</counts>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Recent innovations such as optogenetics and neuroimaging enable us to characterise the relationship between the activity of neurons and their system-level behaviour. These technical innovations call for theories that describe neuronal interactions and reveal the underlying principles. In physics, electromagnetism, the general theory of relativity, and the quantum field theory have been consolidated within the rigorous and holistic framework of a gauge theory [<xref ref-type="bibr" rid="pbio.1002400.ref001">1</xref>]. Here, we propose that a gauge-theoretic formalism in neurosciences might not only provide a quantitative framework for modelling neural activity but also show that neuronal dynamics across scales—from single neurons to population activity—are described by the same principle. This paper suggests that if we could formulate a gauge theory for the brain—or cast an existing theory as a gauge theory—then many aspects of neurobiology can be seen as consequences of fundamental invariance properties. This approach could clarify the intimate relationship between apparently distinct phenomena (e.g., action and perception) and, potentially, offer new tools for computational neuroscience and modelling.</p>
<p>This paper is comprised of three sections. The first provides an informal description of gauge theories in physics. The second focuses on the biological features required for a gauge theory for the brain. In the third section, we discuss the three ingredients of a gauge theory: a system equipped with symmetry, some local forces, and one or more gauge fields. To illustrate the approach, we use the principle of variational free energy minimisation as a candidate for a neuronal gauge theory. This principle says that every aspect of the brain, from neuronal activity to synaptic connection strengths, can be described as a process that minimises variational free energy, where variational free energy measures the probability of sensory inputs, under the brain’s model of how those inputs were caused. The variational aspect of variational free energy comes from its derivation from variational calculus, which is at the heart of gauge theories and information geometry, which are the focus of this article.</p>
<p>At its simplest, variational free energy reduces to prediction error or surprise in exactly the same way that estimating the parameters of a statistical model can be reduced to minimising the sum of squared error. We will see later that when physical laws, such as Hamilton's principle of least action, are applied to variational free energy, the resulting behaviour looks very much like statistical inference, lending the brain—and indeed any self-organising system—the look and feel of a system that is trying to estimate or infer the causes of its (sensory) exchanges with the world. In fact, advanced statistical analyses of data minimise variational free energy, where negative free energy approximates Bayesian model evidence. Effectively, minimising variational free energy corresponds to maximising model evidence. This means that if the brain minimises variational free energy it is inherently Bayesian; hence the Bayesian brain hypothesis [<xref ref-type="bibr" rid="pbio.1002400.ref002">2</xref>–<xref ref-type="bibr" rid="pbio.1002400.ref005">5</xref>].</p>
<p>We provide a glossary of technical terms (<xref ref-type="supplementary-material" rid="pbio.1002400.s004">S1 Glossary</xref>) and series of appendices (<xref ref-type="supplementary-material" rid="pbio.1002400.s001">S1 Appendix</xref>; <xref ref-type="supplementary-material" rid="pbio.1002400.s005">S1</xref>–<xref ref-type="supplementary-material" rid="pbio.1002400.s009">S5 Text</xref>) that offer a technical account of the free energy formalism, gauge theory, and their relationship. These appendices unpack the key conceptual points for technically-minded readers. Although formulating the free energy principle as a gauge theory may have important implications for new inference schemes that can be used to analyse data (i.e., infer the structure and parameters of models of empirical data), we focus on the implications for neurobiology.</p>
</sec>
<sec id="sec002">
<title>I. Gauge Theories in Physics</title>
<p>A gauge theory is a physical theory that predicts how one or more physical fields interact with matter. Every gauge theory has an associated Lagrangian (i.e., a function that summarises the dynamics of the system in question), which is invariant under a group of local transformations [<xref ref-type="bibr" rid="pbio.1002400.ref006">6</xref>]. Consider Newton’s laws of motion in an inertial frame of reference (e.g., a ball in an empty room). These laws are valid at every point in the room. This means that the dynamics and the Lagrangian do not depend on the position of the ball. In this system, the dynamics will be invariant under translations in space. These transformations—that preserve the Lagrangian—are said to be equipped with gauge symmetry. In short, a symmetry is simply an invariance or immunity to changes in the frame of reference [<xref ref-type="bibr" rid="pbio.1002400.ref007">7</xref>].</p>
<p>Take a rotating frame of reference (e.g., the Earth) and call it the system. During the rigid rotation of the Earth, the distances between every town remain invariant. As Earth rotates, one can say that we have a gauge transformation that does not change the distance between the cities (<xref ref-type="supplementary-material" rid="pbio.1002400.s001">S1 Appendix</xref>). This means that the system preserves its symmetry (c.f., the laws of Nature), despite the action (or gauge transformations) of global forces (acting on every point of the system). This action corresponds to a global transformation of the system—a transformation that preserves invariances; in this instance, distances. There is, however, another possibility: for example, compare the Earth and moving objects on its surface (e.g., the water in the ocean). When the motion of water is described relative to the Earth, its inertia (that is, its tendency to move in a straight line) distorts its motion. This phenomenon is called the Coriolis effect: rather than flowing directly from areas of high pressure to low pressure—as they would in a nonrotating world—currents tend to flow in a clockwise sense (with respect to the direction of travel) in the Northern hemisphere and counterclockwise in the Southern hemisphere. The Coriolis effect is thus a curvilinear motion in a rotating (noninertial) frame of reference. A gauge transformation that is not constant, i.e., varies with spatial location, is referred to as a local symmetry. One or more local forces acting on just a few parts of the system produce local transformations.</p>
<p>Why is the symmetry preserved, despite the perturbing action of local forces? This is because the local forces are balanced by another force—called the gauge field. Note that when the global symmetry of the system is preserved despite local forces, there is a local deformation, and the symmetry is said to be locally broken, or at best, hidden. In a system equipped with global symmetry, it is thus possible to find a local breaking of symmetry that can be “repaired” by the gauge field. Let us again take the example of the Coriolis forces. For someone who only uses inertial frames of reference, the Coriolis forces do not exist; they are just forces we invent to pretend we are in an inertial system when in fact we are not: they are practical, because they allow us to continue using Newton’s laws. Albert Einstein discovered something similar; the laws of special relativity are not valid when gravitational forces (which are due to energy and momentum) are present. However, by allowing energy and momentum to bend space–time, he could retain special relativity, at least locally, making the gravitational forces disappear. For Einstein, gravitational forces are as unreal as the Coriolis forces; they are forces we have to invent when we choose an inappropriate frame of reference. See <xref ref-type="supplementary-material" rid="pbio.1002400.s001">S1 Appendix</xref> that illustrates these ideas using a simple pendulum example.</p>
<p>In summary, gauge theory is relatively straightforward: one has a Lagrangian that prescribes equations of motion. These equations of motion are generally invariant under changes in a spatial frame of reference (e.g., a straight line is a straight line everywhere on a sheet of paper); however, if we stand back and see that motion (a line in space–time) changes with position (e.g., a straight line on a sheet of paper that has been bent), we can extend the global symmetry to a local symmetry by introducing compensatory fields or forces—the gauge fields. The notion of bending or curvature here is important, because manifolds that shape trajectories in the space of probability distributions are necessarily curved. This is important for systems that minimise variational free energy (<xref ref-type="supplementary-material" rid="pbio.1002400.s005">S1 Text</xref>) and related Bayesian brain theories [<xref ref-type="bibr" rid="pbio.1002400.ref008">8</xref>]. In fact, we shall see below that quintessentially cognitive faculties like “attention” can, in principle, be derived using gauge theory.</p>
</sec>
<sec id="sec003">
<title>II. Gauge Theories for the Nervous System</title>
<p>Gauge theories originate from physics; however, they could be applied to countless fields of biology: cell structure, morphogenesis, and so on. Examples that lend themselves to a gauge-theoretic treatment include recent simulations of embryogenesis [<xref ref-type="bibr" rid="pbio.1002400.ref009">9</xref>] and the self-organisation of dendritic spines [<xref ref-type="bibr" rid="pbio.1002400.ref010">10</xref>]. Although these examples appear unrelated, both can be formulated in terms of a gradient ascent on variational free energy. In other words, we may be looking at the same fundamental behaviour in different contexts [<xref ref-type="bibr" rid="pbio.1002400.ref011">11</xref>]. Here, we focus on the central nervous system (CNS). Can we sketch a gauge theory of brain function? The answer rests on identifying—in biological systems like the brain—the tenets of a gauge theory; namely,</p>
<list list-type="order">
<list-item><p>A system equipped with a continuous symmetry that can be conserved.</p></list-item>
<list-item><p>The presence of local forces acting on the system (a group of local transformations), where a gauge field renders its Lagrangian invariant under such transformations.</p></list-item>
</list>
<p>When attempting to establish what aspect of CNS function might be understood in terms of a Lagrangian, the variational free energy looks highly plausible [<xref ref-type="bibr" rid="pbio.1002400.ref012">12</xref>]. The basic idea is that any self-organising system, at nonequilibrium steady-state with its environment, will appear to minimise its (variational) free energy, thus resisting a natural tendency to disorder. This formulation reduces the physiology of biological systems to their homeostasis (and allostasis); namely, the maintenance of their states and form, in the face of a constantly changing environment.</p>
<p>If the minimisation of variational free energy is a ubiquitous aspect of biological systems, could it be the Lagrangian of a gauge theory? This (free energy) Lagrangian has proved useful in understanding many aspects of functional brain architectures; for example, its hierarchical organisation and the asymmetries between forward and backward connections in cortical hierarchies [<xref ref-type="bibr" rid="pbio.1002400.ref013">13</xref>]. In this setting, the system stands for the brain (with neuronal or internal states), while the environment (with external states) is equipped with continuous forces and produces local sensory perturbations that are countered through action and perception (that are functionals of the gauge field).</p>
<p>In summary, the free energy formalism rests on a statistical separation between the agent (the internal states) and the environment (the external states). Agents suppress free energy (or surprise) by changing sensory input, by acting on external states, or by modifying their internal states through perception [<xref ref-type="bibr" rid="pbio.1002400.ref012">12</xref>,<xref ref-type="bibr" rid="pbio.1002400.ref014">14</xref>]. In what follows, we try to show that the need to minimise variational free energy (and hence achieve homeostasis) acquires a useful logical mathematical formalism, when framed as a gauge theory.</p>
<sec id="sec004">
<title>The Utility of a Gauge Theory for the CNS</title>
<p>Whenever we posit a normative theory in neuroscience, we appeal to some objective function or Lagrangian—and the opportunity to develop a gauge theory. So what would this gauge theory tell us? Put simply, it would allow us to understand complex and apparently chaotic neuronal dynamics in much simpler yet formal terms. In the same way that we model and understand the gauge fields (e.g., Coriolis forces) driving our weather patterns, it may be possible to understand the driving effects of neuronal afferents on postsynaptic responses as a necessary consequence of much simpler invariances or symmetries. This prospect is particularly prescient if neuronal activity encodes representations or (Bayesian) beliefs, where the Lagrangian (i.e., variational free energy) becomes a function of probability distributions.</p>
<p>Minimising variational free energy describes how a system changes its (internal) states so that they are the most probable under the (external) states of the environment. The system does so by implicitly learning a generative model of its environment. A generative model is simply a model that generates sensory inputs. If the model can reproduce or predict sensory samples from the environment, then it implicitly infers how sensations were generated or caused. Clearly, a good model will make predictions with a very low prediction error (or variational free energy). Put another way, minimising variational free energy ensures the brain becomes a good model of its environment. If we add into this mix the fact that sensory input is actively sampled, then acting to minimise free energy will inevitably make the model’s predictions come true. This is how action and perception are understood in terms of variational free energy minimisation.</p>
<p>Recall that a key ingredient of a gauge theory is a continuous symmetry that can be conserved. So what does gauge symmetry bring to the table? Here, we develop the idea that distinct classes of inference or behaviour are equivalent to pattern formation in the nervous system. For example, the perception of a visual object involves highly organised patterns of neuronal responses throughout the visual cortical hierarchy, reflecting a functional specialisation for colour and motion [<xref ref-type="bibr" rid="pbio.1002400.ref015">15</xref>]—right down to the architecture of classical receptive fields [<xref ref-type="bibr" rid="pbio.1002400.ref016">16</xref>,<xref ref-type="bibr" rid="pbio.1002400.ref017">17</xref>]. Since patterns can be classified into a variety of groups, so can a system’s behaviour. Specifically, the gauge perspective enables us to specify an ontology of patterns or responses that is independent of the generative model. This does not mean that the generative model is irrelevant, but rather a great deal could be learnt by knowing about the symmetries (invariances) of that model. All models with a given symmetry explore the same range of pattern-forming behaviour—without reference to the underlying model [<xref ref-type="bibr" rid="pbio.1002400.ref018">18</xref>]. For example, could experience-dependent or Hebbian plasticity [<xref ref-type="bibr" rid="pbio.1002400.ref019">19</xref>] be an example of a symmetry that minimises variational free energy [<xref ref-type="bibr" rid="pbio.1002400.ref020">20</xref>], and yet the products of experience lead to very different—but equally good—neuronal patterns and architectures. In other words, could we understand phenomena like bistable perception [<xref ref-type="bibr" rid="pbio.1002400.ref021">21</xref>] and intersubject variability [<xref ref-type="bibr" rid="pbio.1002400.ref022">22</xref>] in terms of a gauge theory.</p>
<p>In the animal kingdom, many aspects of behaviour are similar, yet the neural mechanisms that mediate such behaviours can be very different. For example, bees may forage using different neuronal mechanisms [<xref ref-type="bibr" rid="pbio.1002400.ref019">19</xref>] from those used by a rat to explore a maze [<xref ref-type="bibr" rid="pbio.1002400.ref023">23</xref>], or those we employ while foraging for information with visual saccades [<xref ref-type="bibr" rid="pbio.1002400.ref024">24</xref>]. The difference emerges from the heterogeneity of evolutionary pressures from the ecological niche. A gauge theory that describes behaviour and establishes a mathematical framework to understand the gauge transformations that render behaviour invariant, within and between species, may be invaluable. Particularly because such an approach paves the way for a computational neuroethology, enabling us to study what properties of neuronal dynamics are conserved over species, and what constraints have caused certain neural network properties to diverge over evolution. For example, the minimisation of variational free energy may provide an explanation for foraging that transcends the ecological context: see [<xref ref-type="bibr" rid="pbio.1002400.ref025">25</xref>] for an example of epistemic foraging that is formally equivalent to saccadic sampling of the visual field [<xref ref-type="bibr" rid="pbio.1002400.ref026">26</xref>].</p>
<p>In the study of dynamical attractors and pattern formation, it generally makes sense to study model-independent generalities as a first step, adding the details later. For example, the Lorenz attractor has found a useful role in modelling a variety of systems; from catalytic reactions [<xref ref-type="bibr" rid="pbio.1002400.ref027">27</xref>] to meteorology [<xref ref-type="bibr" rid="pbio.1002400.ref028">28</xref>] and immunology [<xref ref-type="bibr" rid="pbio.1002400.ref029">29</xref>]. A technical issue here is the distinction between phase and physical space [<xref ref-type="bibr" rid="pbio.1002400.ref018">18</xref>]. Any system evolves in a physical space, wherein transformations to a phase-space involve an unspecified change of coordinates. Such coordinate transformations can disconnect variables of the dynamical system and the variables observed in physical space (e.g., the phase reduction techniques used to characterise rhythmic brain activity in terms of fluctuations in the phase of a neuronal oscillation at any given frequency [<xref ref-type="bibr" rid="pbio.1002400.ref030">30</xref>] and other dynamical systems ranging from cell biology to cancer research [<xref ref-type="bibr" rid="pbio.1002400.ref031">31</xref>]. See also <xref ref-type="supplementary-material" rid="pbio.1002400.s001">S1 Appendix</xref>). The gauge-symmetry perspective alleviates the potentially problematic disconnect between physical and phase spaces: a symmetry property in phase space translates into a symmetry property in the physical space, and vice-versa, because this is the defining property of properly formulated gauge symmetries. A related benefit of understanding the symmetries of a dynamical system is that it enables one to map from one dynamical system to another, allowing us to identify a range of solutions with identical dynamics [<xref ref-type="bibr" rid="pbio.1002400.ref032">32</xref>,<xref ref-type="bibr" rid="pbio.1002400.ref033">33</xref>].</p>
<p>The second important question posed by framing a gauge theory is "how are symmetries related to gauge fields?" The answer to this question is that gauge fields are objects (compensatory fields) introduced to maintain symmetry and thus also the original equation of motion (<xref ref-type="supplementary-material" rid="pbio.1002400.s006">S2 Text</xref>). In other words, one can imagine them as fields that ensure the Lagrangian is invariant to transformations. In the next section, we use these notions to formulate the variational free energy formalism as a gauge theory for the nervous system.</p>
</sec>
</sec>
<sec id="sec005">
<title>III. Sensory Entropy as a Lagrangian</title>
<p>The variational free energy formalism uses the fact that biological systems must resist the second law of thermodynamics (i.e., a tendency to disorder), so that they do not decay to equilibrium. In a similar vein to Maxwell's demon, an organism reduces its entropy through sampling the environment—to actively minimise the self information or surprise of each successive sensory sample (this surprise is upper bounded by free energy). By doing so, it places a bound on the entropy of attributes of the environment in which it is immersed. Variational free energy operationalises this bound by ensuring internal states of the system become a replica (i.e., a generative model) of its immediate environment. This can be regarded as a formulation of the good regulator hypothesis [<xref ref-type="bibr" rid="pbio.1002400.ref034">34</xref>], which states that every good regulator of a system must be a model of that system.</p>
<p>We know that a gauge theory would leave the Lagrangian invariant under continuous symmetry transformations. Therefore, a gauge theory of the brain requires the interaction among three ingredients: a system equipped with symmetry, some local forces applied to the system, and one or more gauge fields to compensate for the local perturbations that are introduced. The first ingredient is a system equipped with symmetry: for the purposes of our argument, the system is the nervous system and the Lagrangian is the entropy of sensory samples (which is upper-bounded by variational free energy, averaged over time). The local forces are mediated by the external states of the world (i.e., through sensory stimuli). The gauge fields can then be identified by considering the fact that variational free energy is a scalar quantity based on probability measures. Let us see how.</p>
<p>From differential geometry treatments of probability measures (<xref ref-type="supplementary-material" rid="pbio.1002400.s007">S3 Text</xref>), it is known that the manifold traced out by sufficient statistics of a probability measure is curved in nature [<xref ref-type="bibr" rid="pbio.1002400.ref035">35</xref>,<xref ref-type="bibr" rid="pbio.1002400.ref036">36</xref>]; more specifically, it has negative curvature and is therefore endowed with a hyperbolic geometry. Barring technical details, it suffices to understand that a functional (e.g., variational free energy) of such probability measures is not flat but induces a curved (Riemannian) landscape. Moving along such a curved manifold requires a measure of distance that corresponds to the distance between two distributions. Again, differential geometry tells us that such a measure of distance is provided by the Fisher information metric (Cencov’s characterisation theorem) that quantifies the amount of information that a random variable carries about an unknown parameter. Technically, the Fisher information represents the curvature of the relative entropy (Kullback-Leibler divergence). Put simply, distances in the curved geometry of sufficient statistics—that define variational free energy—correspond to the relative entropy in going from one point on the free energy manifold to another.</p>
<p>Instead of using classical results from differential geometry [<xref ref-type="bibr" rid="pbio.1002400.ref035">35</xref>–<xref ref-type="bibr" rid="pbio.1002400.ref039">39</xref>], we will motivate the gauge formulation of variational free energy by asking an empirical question: how does neuronal activity follow the steepest descent direction to attain its free energy minimum? In other words, how does it find the shortest path to the nearest minimum? As the free energy manifold is curved, there are no orthonormal linear coordinates to describe it. This means the distance between two points on the manifold can only be determined with the help of the Fisher information metric that accounts for the curvature. Algebraic derivations (<xref ref-type="supplementary-material" rid="pbio.1002400.s007">S3 Text</xref>) tell us that, in such free energy landscapes, a Euclidean gradient descent is replaced by a Riemann gradient, which simply weights the Euclidean gradient by its asymptotic variance.</p>
<p>In the free energy framework, when the posterior probability is approximated with a Gaussian distribution (the Laplace approximation; <xref ref-type="supplementary-material" rid="pbio.1002400.s008">S4 Text</xref>), perception and action simply become gradient flows driven by precision-weighted prediction errors. Here, prediction errors are simply the difference between sensory input (local perturbations) and predictions of those inputs based upon the systems internal states (that encode probability distributions or Bayesian beliefs about external states that cause sensory input). Mathematically, precision-weighted prediction errors emerge when one computes the Euclidean gradient of the free energy with respect to the sufficient statistics. In a curvilinear space, the precision-weighted prediction errors are replaced by dispersion- and precision-weighted prediction errors. This says something quite fundamental—perception cannot be any more optimal than the asymptotic dispersion (inverse Fisher information) regardless of the generative model. In statistics, this result is known as the Cramér-Rao bound of an estimator. In other words, the well-known bound (upper limit) on the precision of any unbiased estimate of a model parameter in statistics emerges here as a natural consequence of applying information geometry. In the context of the Bayesian brain, this means there is a necessary limit to the certainty with which we can estimate things. We will see next, that attaining this limit translates into attention. See <xref ref-type="boxed-text" rid="pbio.1002400.box001">Box 1</xref> for an overview.</p>
<boxed-text id="pbio.1002400.box001" position="float">
<sec id="sec006">
<title>Box 1. How Does Local Gauge Invariance Explain Neuronal Inference?</title>
<p>This essay considers the principle of free energy minimization as a candidate gauge theory that prescribes neuronal dynamics in terms of a Lagrangian. Here, the Lagrangian is the variational free energy, which is a functional (a function of a function) of a probability distribution encoded by neuronal activity. This probabilistic encoding means that neuronal activity can be described by a path or trajectory on a manifold in the space of sufficient statistics (variables that are sufficient to describe a probability distribution). In other words, if one interprets the brain as making inferences, the underlying beliefs must be induced by biophysical representations that play the role of sufficient statistics. This is important because it takes us into the realm of differential geometry (<xref ref-type="supplementary-material" rid="pbio.1002400.s006">S2 Text</xref>), where the metric space—on which the geometry is defined—is constituted by sufficient statistics (like the mean and variance of a Gaussian distribution). Crucially, the gauge theoretic perspective provides a rigorous way of measuring distance on a manifold, such that the neuronal dynamics transporting one distribution of neuronal activity to another is given by the shortest path. Such a free energy manifold is curvilinear, and finding the shortest path is a nontrivial problem—a problem that living organisms appear to have solved. It is at this point that the utility of a gauge theoretic approach appears; suggesting particular solutions to the problem of finding the shortest path on curved manifolds. The nature of the solution prescribes a normative theory for self-organised neuronal dynamics. In other words, solving the fundamental problem of minimising free energy—in terms of its path integrals—may illuminate not only how the brain works but may provide efficient schemes in statistics and machine learning.</p>
<p>Variational or Monte Carlo formulations of the Bayesian brain require the brain to invert a generative model of the latent (unknown or hidden) causes of sensations (<xref ref-type="supplementary-material" rid="pbio.1002400.s007">S3 Text</xref>). The implicit normative theory means that neuronal activity (and connectivity) maximises Bayesian model evidence or minimises variational free energy (the Lagrangian)—effectively fitting a generative model to sensory samples. This entails an encoding of beliefs (probability distributions) about the latent causes, in terms of biophysical variables whose trajectories trace out a manifold. In (deterministic) variational schemes, the coordinates on this manifold are the sufficient statistics (like the mean and covariance) of the distribution or belief, while for a (stochastic) Monte Carlo formulation, the coordinates are the latent causes themselves (<xref ref-type="supplementary-material" rid="pbio.1002400.s007">S3 Text</xref>). The inevitable habitat of these sufficient statistics (e.g., neuronal activity) is a curved manifold (see <xref ref-type="supplementary-material" rid="pbio.1002400.s002">S1</xref> and <xref ref-type="supplementary-material" rid="pbio.1002400.s003">S2</xref> Figs).</p>
<p>This curvature (and associated information geometry) may have profound implications for neuronal dynamics and plasticity. It may be the case that neuronal dynamics—or motion in some vast neuronal frame of reference—is as simple as the pendulum (<xref ref-type="supplementary-material" rid="pbio.1002400.s001">S1 Appendix</xref>). However, because the Lagrangian is a function of beliefs (probabilities), the manifold that contains this motion is necessarily curved. This means neuronal dynamics, in a local frame of reference, will (appear to) be subject to forces and drives (i.e., Levi-Civita connections). For example, the motion of synaptic connection strengths (sufficient statistics of the parameters of generative models) depends upon the motion of neural activity (sufficient statistics of beliefs about latent causes), leading to experience-dependent plasticity. A more interesting manifestation (highlighted in the main text) may be attention that couples the motion of different neuronal states in a way that depends explicitly on the curvature of the manifold (as measured by things like Fisher information). In brief, a properly formulated gauge theory should, in principle, provide the exact form of neuronal dynamics and plasticity. These forms may reveal the underlying simplicity of many phenomena that we are already familiar with, such as event-related brain responses, associative plasticity, attentional gating, adaptive learning rates, and so on.</p>
</sec>
</boxed-text>
<p>Notice that the definition of a system immersed in its environment can be extended hierarchically, wherein the gauge theory can be applied at a variety of nested levels. At every step, as the Lagrangian is disturbed (e.g., through changes in forward or bottom-up sensory input), the precision-weighted compensatory forces change to keep the Lagrangian invariant via (backward or top-down) messages. In the setting of predictive coding formulations of variational free energy minimisation, the bottom-up or forward messages are assumed to convey prediction error from a lower hierarchical level to a higher level, while the backward messages comprise predictions of sufficient statistics in the level below. These predictions are produced to explain away prediction errors in the lower level. From the perspective of a gauge theory, one can think of the local forces as prediction errors that increase variational free energy, thereby activating the gauge fields to explain away local forces [<xref ref-type="bibr" rid="pbio.1002400.ref040">40</xref>]. In this geometrical interpretation, perception and action are educed to form cogent predictions, whereby minimization of prediction errors is an inevitable consequence of the nervous system minimising its Lagrangian. Crucially, the cognitive homologue of precision-weighting is attention, which suggests gauge fields are intimately related to (exogenous) attention. In other words, attention is a force that manifests from the curvature of information geometry, in exactly the same way that gravity is manifest when the space–time continuum is curved by massive bodies. In summary, gauge theoretic arguments suggest that attention (and its neurophysiological underpinnings) constitutes a necessary weighting of prediction errors (or sensory evidence) that arises because the manifolds traced out by the path of least free energy (or least surprise) are inherently curved.</p>
</sec>
<sec id="sec007" sec-type="conclusions">
<title>Conclusions</title>
<p>The importance of the reciprocal interactions among different scales and between the nervous system and the world has been repeatedly emphasised here and elsewhere. A quantitative formulation of this holistic aspect of information processing in the brain is clearly needed and has been framed in terms of variational free energy minimisation or, more simply, as a suppression of prediction errors encountered by an agent that is actively sampling its environment [<xref ref-type="bibr" rid="pbio.1002400.ref012">12</xref>,<xref ref-type="bibr" rid="pbio.1002400.ref014">14</xref>]. In this essay, we have reformulated the resolution of prediction error (or surprise) as a gauge theory for the nervous system. Such geometrical formulations endow variational inference with symmetry breaking, enabling invariance with respect to the generative model; irrespective of the model’s parameterization [<xref ref-type="bibr" rid="pbio.1002400.ref041">41</xref>,<xref ref-type="bibr" rid="pbio.1002400.ref042">42</xref>].</p>
<p>The free energy formulation enables us (or our brains) to compare a variety of hypotheses about the environment given our sensory samples. So what does a gauge formulation add? After scoring a variety of models according to their variational free energy, one selects the best generative model that explains the data at hand. If symmetry transformations can be evaluated for the best performing model, afforded by the symmetry group, we can possibly go a step further and ask what other models could explain the data with the same model evidence? Such symmetry transformations therefore furnish us with a family of models that explain the data with equal probability [<xref ref-type="bibr" rid="pbio.1002400.ref043">43</xref>–<xref ref-type="bibr" rid="pbio.1002400.ref045">45</xref>]: see <xref ref-type="fig" rid="pbio.1002400.g001">Fig 1</xref> for a schematic illustration of this idea. The challenge of such an approach would be to deduce the symmetry groups of the probability distributions encoded by neuronal activity (<xref ref-type="supplementary-material" rid="pbio.1002400.s009">S5 Text</xref>). If this was possible, one could identify the class of models explaining data using a single inference methodology. From the perspective of biological self-organisation, this means that there are potentially many different models that do the same job of predicting sensory data. It is tempting to associate the equivalence classes afforded by symmetry groups above with the biological species—all doing an equally good job of modelling their (local) environments.</p>
<fig id="pbio.1002400.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002400.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Invariance of inference—a proposal.</title>
<p>A univariate normal model space (of the equivalent data space) whose coordinates are sufficient statistics. The orange circles represent two model spaces, where minimization of variational free energy leads to the appropriate model (blue circles in each case). It might be that one parameterisation (blue outer circle for model space 1) is more suitable than another. Notice that using the Riemann gradient instead of the Euclidean gradient automatically guarantees gauge-invariance by breaking the symmetry within a model. But what about between-model symmetries? After the optimal model has been selected, symmetry transformation could enable one to derive a range of models (black circles) that describe the data equally well (in terms of log model evidence or negative free energy). These can be obtained by using the intrinsic geometry of the gauge field (the Levi-Civita connection) and the symmetries afforded by the variational free energy.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002400.g001" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec008">
<title>Supporting Information</title>
<supplementary-material id="pbio.1002400.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002400.s001" xlink:type="simple">
<label>S1 Appendix</label>
<caption>
<title>Global and local gauge invariance.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002400.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002400.s002" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Conjugate gradient-descent algorithm on manifolds.</title>
<p>(A) New parameters (<italic>θ</italic>) are selected by performing gradient descent on orthogonal subspaces with gradient <italic>G</italic> and the descent direction <italic>H</italic>. (B) On a Riemannian manifold, minimization along lines (as in a Euclidean subspace described in A) is replaced by minimization along geodesics. This creates a problem, in that <italic>H</italic><sub><italic>i</italic></sub> and <italic>H</italic><sub><italic>i−1</italic></sub> are in two different tangent spaces and thus cannot be added together. (C) Vector addition as in Eqn. 12 (in S3.2) is undefined on a Riemannian manifold. Addition is replaced by exponential mapping followed with parallel transport described using a covariant gauge field (Levi-Civita connection; see text).</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002400.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002400.s003" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Parallel transport on Riemannian manifolds.</title>
<p>(A) The Riemann exponential map is used to map a vector field <italic>H</italic> from <inline-formula id="pbio.1002400.e001"><alternatives><graphic id="pbio.1002400.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.1002400.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mi>T</mml:mi><mml:mi mathvariant="script">M</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> whilst a logarithmic map is used to map the vector field from <inline-formula id="pbio.1002400.e002"><alternatives><graphic id="pbio.1002400.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.1002400.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:mi mathvariant="script">M</mml:mi><mml:mo>→</mml:mo><mml:mi>T</mml:mi><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. (B) Graphical illustration of parallel transporting a vector field <italic>H</italic> using a Schild’s ladder (see text for details).</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002400.s004" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002400.s004" xlink:type="simple">
<label>S1 Glossary</label>
<caption>
<title/>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002400.s005" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002400.s005" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Variational free energy.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002400.s006" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002400.s006" xlink:type="simple">
<label>S2 Text</label>
<caption>
<title>A tutorial on differential geometry.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002400.s007" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002400.s007" xlink:type="simple">
<label>S3 Text</label>
<caption>
<title>Dynamics on manifolds.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002400.s008" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002400.s008" xlink:type="simple">
<label>S4 Text</label>
<caption>
<title>The geometry of a univariate normal distribution.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002400.s009" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002400.s009" xlink:type="simple">
<label>S5 Text</label>
<caption>
<title>Symmetry-aware algorithms.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pbio.1002400.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O'Raifeartaigh</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Straumann</surname> <given-names>N</given-names></name>. <article-title>Gauge theory: Historical origins and some modern developments</article-title>. <source>Rev Mod Phys</source>. <year>2000</year>;<volume>72</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="pbio.1002400.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knill</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>The Bayesian brain: the role of uncertainty in neural coding and computation</article-title>. <source>Trends Neurosci</source>. <year>2004</year>;<volume>27</volume>(<issue>12</issue>):<fpage>712</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">15541511</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>BJ</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Bayesian inference with probabilistic population codes</article-title>. <source>Nat Neurosci</source>. <year>2006</year>;<volume>9</volume>(<issue>11</issue>):<fpage>1432</fpage>–<lpage>8</lpage>. <object-id pub-id-type="pmid">17057707</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Neal</surname> <given-names>R</given-names></name>. <article-title>The Helmholtz machine</article-title>. <source>Neural Computation</source>. <year>1995</year>;<volume>7</volume>:<fpage>889</fpage>–<lpage>904</lpage>. <object-id pub-id-type="pmid">7584891</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kersten</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Mamassian</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Yuille</surname> <given-names>A</given-names></name>. <article-title>Object perception as Bayesian inference</article-title>. <source>Annu Rev Psychol</source>. <year>2004</year>;<volume>55</volume>:<fpage>271</fpage>–<lpage>304</lpage>. <object-id pub-id-type="pmid">14744217</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref006"><label>6</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Zeidler</surname> <given-names>E</given-names></name>. <source>Quantum Field Theory III: Gauge Theory</source>: <publisher-name>Springer</publisher-name>; <year>2011</year>.</mixed-citation></ref>
<ref id="pbio.1002400.ref007"><label>7</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Rosen</surname> <given-names>J</given-names></name>. <source>Symmetry in Science: An Introduction to the General Theory</source>: <publisher-name>Springer-Verlag</publisher-name>; <year>1995</year>.</mixed-citation></ref>
<ref id="pbio.1002400.ref008"><label>8</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ishii</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rao</surname> <given-names>RPN</given-names></name>, editors. <source>Bayesian Brain: Probabilistic Approaches to Neural Coding</source>: <publisher-name>The MIT Press</publisher-name>; <year>2011</year>.</mixed-citation></ref>
<ref id="pbio.1002400.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Levin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sengupta</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>. <article-title>Knowing one's place: a free-energy approach to pattern regulation</article-title>. <source>Journal of the Royal Society, Interface / the Royal Society</source>. <year>2015</year>;<volume>12</volume>(<issue>105</issue>).</mixed-citation></ref>
<ref id="pbio.1002400.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kiebel</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Free energy and dendritic self-organization</article-title>. <source>Front Syst Neurosci</source>. <year>2011</year>;<volume>5</volume>:<fpage>80</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnsys.2011.00080" xlink:type="simple">10.3389/fnsys.2011.00080</ext-link></comment> <object-id pub-id-type="pmid">22013413</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>Life as we know it</article-title>. <source>Journal of the Royal Society, Interface / the Royal Society</source>. <year>2013</year>;<volume>10</volume>(<issue>86</issue>):<fpage>20130475</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rsif.2013.0475" xlink:type="simple">10.1098/rsif.2013.0475</ext-link></comment> <object-id pub-id-type="pmid">23825119</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>The free-energy principle: a unified brain theory?</article-title> <source>Nature reviews Neuroscience</source>. <year>2010</year>;<volume>11</volume>(<issue>2</issue>):<fpage>127</fpage>–<lpage>38</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2787" xlink:type="simple">10.1038/nrn2787</ext-link></comment> <object-id pub-id-type="pmid">20068583</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>Hierarchical models in the brain</article-title>. <source>PLoS Comput Biol</source>. <year>2008</year>;<volume>4</volume>(<issue>11</issue>):<fpage>e1000211</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000211" xlink:type="simple">10.1371/journal.pcbi.1000211</ext-link></comment> <object-id pub-id-type="pmid">18989391</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sengupta</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Stemmler</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Information and efficiency in the nervous system—a synthesis</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>(<issue>7</issue>):<fpage>e1003157</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003157" xlink:type="simple">10.1371/journal.pcbi.1003157</ext-link></comment> <object-id pub-id-type="pmid">23935475</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zeki</surname> <given-names>S</given-names></name>. <article-title>The Ferrier Lecture 1995 behind the seen: The functional specialization of the brain in space and time</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2005</year>;<volume>360</volume>(<issue>1458</issue>):<fpage>1145</fpage>–<lpage>83</lpage>. <object-id pub-id-type="pmid">16147515</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>:<fpage>607</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">8637596</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Angelucci</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bullier</surname> <given-names>J</given-names></name>. <article-title>Reaching beyond the classical receptive field of V1 neurons: horizontal or feedback axons?</article-title> <source>J Physiol Paris</source>. <year>2003</year>;<volume>97</volume>:<fpage>141</fpage>–<lpage>54</lpage>. <object-id pub-id-type="pmid">14766139</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref018"><label>18</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Golubitsky</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Stewart</surname> <given-names>I</given-names></name>. <source>The Symmetry Perspective: From Equilibrium to Chaos in Phase Space and Physical Space</source>: <publisher-loc>Birkhäuser</publisher-loc>; <year>2003</year>.</mixed-citation></ref>
<ref id="pbio.1002400.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Person</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Bee foraging in uncertain environments using predictive Hebbian learning</article-title>. <source>Nature</source>. <year>1995</year>;<volume>377</volume>(<issue>6551</issue>):<fpage>725</fpage>–<lpage>8</lpage>. <object-id pub-id-type="pmid">7477260</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>Hierarchical models in the brain</article-title>. <source>PLoS Comput Biol</source>. <year>2008</year>;<volume>4</volume>(<issue>11</issue>):<fpage>e1000211</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000211" xlink:type="simple">10.1371/journal.pcbi.1000211</ext-link></comment> <object-id pub-id-type="pmid">18989391</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brascamp</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Klink</surname> <given-names>PC</given-names></name>, <name name-style="western"><surname>Levelt</surname> <given-names>WJ</given-names></name>. <article-title>The 'laws' of binocular rivalry: 50 years of Levelt's propositions</article-title>. <source>Vision research</source>. <year>2015</year>;<volume>109</volume>(<issue>Pt A</issue>):<fpage>20</fpage>–<lpage>37</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2015.02.019" xlink:type="simple">10.1016/j.visres.2015.02.019</ext-link></comment> <object-id pub-id-type="pmid">25749677</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Vladar</surname> <given-names>HP</given-names></name>, <name name-style="western"><surname>Szathmary</surname> <given-names>E</given-names></name>. <article-title>Neuronal boost to evolutionary dynamics</article-title>. <source>Interface focus</source>. <year>2015</year>;<volume>5</volume>(<issue>6</issue>):<fpage>20150074</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rsfs.2015.0074" xlink:type="simple">10.1098/rsfs.2015.0074</ext-link></comment> <object-id pub-id-type="pmid">26640653</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lisman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>Prediction, sequences and the hippocampus</article-title>. <source>Philosophical transactions of the Royal Society of London Series B, Biological sciences</source>. <year>2009</year>;<volume>364</volume>(<issue>1521</issue>):<fpage>1193</fpage>–<lpage>201</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.2008.0316" xlink:type="simple">10.1098/rstb.2008.0316</ext-link></comment> <object-id pub-id-type="pmid">19528000</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wurtz</surname> <given-names>RH</given-names></name>, <name name-style="western"><surname>McAlonan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Cavanaugh</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Berman</surname> <given-names>RA</given-names></name>. <article-title>Thalamic pathways for active vision</article-title>. <source>Trends Cogn Sci</source>. <year>2011</year>;<volume>5</volume>(<issue>4</issue>):<fpage>177</fpage>–<lpage>84</lpage>.</mixed-citation></ref>
<ref id="pbio.1002400.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Rigoli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ognibene</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Mathys</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fitzgerald</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>. <article-title>Active inference and epistemic value</article-title>. <source>Cognitive neuroscience</source>. <year>2015</year>:<fpage>1</fpage>–<lpage>28</lpage>.</mixed-citation></ref>
<ref id="pbio.1002400.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Adams</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Perrinet</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Breakspear</surname> <given-names>M</given-names></name>. <article-title>Perceptions as hypotheses: saccades as experiments</article-title>. <source>Front Psychol</source>. <year>2012</year>;<volume>3</volume>:<fpage>151</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fpsyg.2012.00151" xlink:type="simple">10.3389/fpsyg.2012.00151</ext-link></comment> <object-id pub-id-type="pmid">22654776</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poland</surname> <given-names>D</given-names></name>. <article-title>Cooperative catalysis and chemical chaos: a chemical model for the Lorenz equations</article-title>. <source>Physica D</source>. <year>1993</year>;<volume>65</volume> (<issue>1</issue>):<fpage>86</fpage>–<lpage>99</lpage>.</mixed-citation></ref>
<ref id="pbio.1002400.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lorenz</surname> <given-names>EN</given-names></name>. <article-title>Deterministic nonperiodic flow</article-title>. <source>J Atmos Sci</source>. <year>1963</year>;<volume>20</volume>(<issue>2</issue>):<fpage>130</fpage>–<lpage>41</lpage>.</mixed-citation></ref>
<ref id="pbio.1002400.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Boer</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Perelson</surname> <given-names>AS</given-names></name>. <article-title>Size and connectivity as emergent properties of a developing immune network</article-title>. <source>Journal of theoretical biology</source>. <year>1991</year>;<volume>149</volume>(<issue>3</issue>):<fpage>381</fpage>–<lpage>424</lpage>. Epub 1991/04/07. 2062103. <object-id pub-id-type="pmid">2062103</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Moehlis</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Holmes</surname> <given-names>P</given-names></name>. <article-title>On the phase reduction and response dynamics of neural oscillator populations</article-title>. <source>Neural Comput</source>. <year>2004</year>;<volume>16</volume>:<fpage>673</fpage>–<lpage>715</lpage>. <object-id pub-id-type="pmid">15025826</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dinicola</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>D'Anselmi</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Pasqualato</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Proietti</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lisi</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Cucina</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>A Systems Biology Approach to Cancer: Fractals, Attractors, and Nonlinear Dynamics</article-title>. <source>Omics-a Journal of Integrative Biology</source>. <year>2011</year>;<volume>15</volume>(<issue>3</issue>):<fpage>93</fpage>–<lpage>104</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1089/omi.2010.0091" xlink:type="simple">10.1089/omi.2010.0091</ext-link></comment> <object-id pub-id-type="pmid">21319994</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref032"><label>32</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bluman</surname> <given-names>GW</given-names></name>, <name name-style="western"><surname>Kumei</surname> <given-names>S</given-names></name>. <source>Symmetries and Differential Equations</source>: <publisher-name>Springer</publisher-name> <publisher-loc>New York</publisher-loc>; <year>1989</year>.</mixed-citation></ref>
<ref id="pbio.1002400.ref033"><label>33</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Olver</surname> <given-names>P</given-names></name>. <source>Applications of Lie groups to differential equations</source>: <publisher-name>Springer-Verlag</publisher-name>; <year>1993</year>.</mixed-citation></ref>
<ref id="pbio.1002400.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Conant</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Ashby</surname> <given-names>WR</given-names></name>. <article-title>Every good regulator of a system must be a model of that system</article-title>. <source>Int J Systems Sci</source>. <year>1970</year>;<volume>1</volume>(<issue>2</issue>):<fpage>89</fpage>–<lpage>97</lpage>.</mixed-citation></ref>
<ref id="pbio.1002400.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amari</surname> <given-names>S</given-names></name>. <article-title>Information geometry of the EM and EM algorithms for neural networks</article-title>. <source>Neural Networks</source>. <year>1995</year>;<volume>8</volume>(<issue>9</issue>):<fpage>1379</fpage>–<lpage>408</lpage>.</mixed-citation></ref>
<ref id="pbio.1002400.ref036"><label>36</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname> <given-names>T</given-names></name>. <chapter-title>Information geometry of mean-field approximation</chapter-title>. In: <name name-style="western"><surname>Opper</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Saad</surname> <given-names>D</given-names></name>, editors. <source>Advanced Mean Field Methods: Theory and Practice</source>: <publisher-name>The MIT Press</publisher-name>; <year>2001</year>. p. <fpage>259</fpage>–<lpage>73</lpage>.</mixed-citation></ref>
<ref id="pbio.1002400.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ikeda</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tanaka</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Amari</surname> <given-names>S</given-names></name>. <article-title>Stochastic reasoning, free energy, and information geometry</article-title>. <source>Neural Computation</source>. <year>2004</year>;<volume>16</volume>(<issue>9</issue>):<fpage>1779</fpage>–<lpage>810</lpage>. <object-id pub-id-type="pmid">15265322</object-id></mixed-citation></ref>
<ref id="pbio.1002400.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Skovgaard</surname> <given-names>LT</given-names></name>. <article-title>A Riemannian geometry of the multivariate normal model</article-title>. <source>Scand J Statist</source>. <year>1984</year>;<volume>11</volume>:<fpage>211</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="pbio.1002400.ref039"><label>39</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Amari</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Nagaoka</surname> <given-names>H</given-names></name>. <source>Methods of Information Geometry</source>: <publisher-name>Oxford University Press</publisher-name>; <year>2000</year>.</mixed-citation></ref>
<ref id="pbio.1002400.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tschacher</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Haken</surname> <given-names>H</given-names></name>. <article-title>Intentionality in non-equilibrium systems? The functional aspects of self-organized pattern formation</article-title>. <source>New Ideas Psychol</source>. <year>2007</year>;<volume>25</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation></ref>
<ref id="pbio.1002400.ref041"><label>41</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Diaconis</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Gupta</surname> <given-names>SS</given-names></name>. <source>Group representations in probability and statistics</source>; <year>1988</year>.</mixed-citation></ref>
<ref id="pbio.1002400.ref042"><label>42</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Eaton</surname> <given-names>ML</given-names></name>. <source>Group Invariance Applications in Statistics</source>: <publisher-name>Institute of Mathematical Statistics</publisher-name>; <year>1989</year>.</mixed-citation></ref>
<ref id="pbio.1002400.ref043"><label>43</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Lauritzen</surname> <given-names>SL</given-names></name>. <chapter-title>Statistical Manifolds</chapter-title>. In: <name name-style="western"><surname>Amari</surname> <given-names>S-I</given-names></name>, <name name-style="western"><surname>Barndorff-Nielsen</surname> <given-names>OE</given-names></name>, <name name-style="western"><surname>Kass</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Lauritzen</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Rao</surname> <given-names>CR</given-names></name>, editors. <source>Differential Geometry in Statistical Inference: IMS Monographs</source>; <year>1987</year>. p. <fpage>165</fpage>–<lpage>216</lpage>.</mixed-citation></ref>
<ref id="pbio.1002400.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Helland</surname> <given-names>IS</given-names></name>. <article-title>Statistical inference under symmetry</article-title>. <source>International Statistical Review</source>. <year>2004</year>;<volume>72</volume>(<issue>3</issue>):<fpage>409</fpage>–<lpage>22</lpage>.</mixed-citation></ref>
<ref id="pbio.1002400.ref045"><label>45</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Barndorff-Nielsen</surname> <given-names>O</given-names></name>. <source>Information and Exponential Families in Statistical Theory</source>; <year>2014</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>