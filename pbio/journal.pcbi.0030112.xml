<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="publisher">pcbi</journal-id><journal-id journal-id-type="allenpress-id">plcb</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="doi">10.1371/journal.pcbi.0030112</article-id><article-id pub-id-type="sici">plcb-03-06-21</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience/Theoretical Neuroscience</subject>
          <subject>Neuroscience/Sensory Systems</subject>
        </subj-group>
      </article-categories><title-group><article-title>Slowness: An Objective for Spike-Timing–Dependent Plasticity?</article-title><alt-title alt-title-type="running-head">Slowness Principle and STDP</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Sprekeler</surname>
            <given-names>Henning</given-names>
          </name>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Michaelis</surname>
            <given-names>Christian</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Wiskott</surname>
            <given-names>Laurenz</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
      </contrib-group><aff id="aff1">
        <label>1</label>
        <addr-line>Institute for Theoretical Biology, Humboldt-Universität zu Berlin, Berlin, Germany</addr-line>
      </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Graham</surname>
            <given-names>Lyle</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">UFR Biomédicale de l'Université René Descart, France</aff><author-notes>
        <fn fn-type="con" id="ack1">
          <p>LW formulated the problem. All authors contributed to the general line of arguments. HS and CM worked out the details of the analysis. All authors contributed to the writing of the paper.</p>
        </fn>
        <corresp id="cor1">* To whom correspondence should be addressed. E-mail: <email xlink:type="simple">h.sprekeler@biologie.hu-berlin.de</email></corresp>
      <fn fn-type="conflict" id="ack3">
        <p> The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="ppub">
        <month>6</month>
        <year>2007</year>
      </pub-date><pub-date pub-type="epub">
        <day>29</day>
        <month>6</month>
        <year>2007</year>
      </pub-date><volume>3</volume><issue>6</issue><elocation-id>e112</elocation-id><history>
        <date date-type="received">
          <day>27</day>
          <month>12</month>
          <year>2006</year>
        </date>
        <date date-type="accepted">
          <day>4</day>
          <month>5</month>
          <year>2007</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2007</copyright-year><copyright-holder>Sprekeler et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>Our nervous system can efficiently recognize objects in spite of changes in contextual variables such as perspective or lighting conditions. Several lines of research have proposed that this ability for invariant recognition is learned by exploiting the fact that object identities typically vary more slowly in time than contextual variables or noise. Here, we study the question of how this “temporal stability” or “slowness” approach can be implemented within the limits of biologically realistic spike-based learning rules. We first show that slow feature analysis, an algorithm that is based on slowness, can be implemented in linear continuous model neurons by means of a modified Hebbian learning rule. This approach provides a link to the trace rule, which is another implementation of slowness learning. Then, we show analytically that for linear Poisson neurons, slowness learning can be implemented by spike-timing–dependent plasticity (STDP) with a specific learning window. By studying the learning dynamics of STDP, we show that for functional interpretations of STDP, it is not the learning window alone that is relevant but rather the convolution of the learning window with the postsynaptic potential. We then derive STDP learning windows that implement slow feature analysis and the “trace rule.” The resulting learning windows are compatible with physiological data both in shape and timescale. Moreover, our analysis shows that the learning window can be split into two functionally different components that are sensitive to reversible and irreversible aspects of the input statistics, respectively. The theory indicates that irreversible input statistics are not in favor of stable weight distributions but may generate oscillatory weight dynamics. Our analysis offers a novel interpretation for the functional role of STDP in physiological neurons.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <sec id="st1">
          <title/>
          <p>Neurons interact by exchanging information via small connection sites, so-called synapses. Interestingly, the efficiency of synapses in transmitting neuronal signals is not static, but changes dynamically depending on the signals that the associated neurons emit. As neurons receive thousands of synaptic input signals, they can thus “choose” the input signals they are interested in by adjusting their synapses accordingly. This adaptation mechanism, known as synaptic plasticity, has long been hypothesized to form the neuronal correlate of learning. It raises a difficult question: what aspects of the input signals are the neurons interested in, given that the adaptation of the synapses follows a certain mechanistic rule? We address this question for spike-timing–dependent plasticity, a type of synaptic plasticity that has raised a lot of interest in the last decade. We show that under certain assumptions regarding neuronal information transmission, spike-timing–dependent plasticity focuses on aspects of the input signals that vary slowly in time. This relates spike-timing–dependent plasticity to a class of abstract learning rules that were previously proposed as a means of learning to recognize objects in spite of contextual changes such as size or position. Based on this link, we propose a novel functional interpretation of spike-timing–dependent plasticity.</p>
        </sec>
      </abstract><funding-group><funding-statement>This work was generously supported by the Volkswagen Foundation.</funding-statement></funding-group><counts>
        <page-count count="13"/>
      </counts><!--===== Restructure custom-meta-wrap to custom-meta-group =====--><custom-meta-group>
        <custom-meta>
          <meta-name>citation</meta-name>
          <meta-value>Sprekeler H, Michaelis C, Wiskott L (2007) Slowness: An objective for spike-timing–dependent plasticity? PLoS Comput Biol 3(6): e112. doi:<ext-link ext-link-type="doi" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.0030112" xlink:type="simple">10.1371/journal.pcbi.0030112</ext-link></meta-value>
        </custom-meta>
      </custom-meta-group></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>The ability to recognize objects in spite of possible changes in position, lighting conditions, or perspective is doubtlessly an advantage in everyday life. However, our brain usually performs this task with such astonishing ease that we are seldom aware of the complexity this recognition problem comprises. On the level of primary sensory signals (e.g., light that stimulates a single retinal receptor), even small changes in the position of the object to be recognized may lead to vastly different stimuli. Our brain thus has to somehow identify rather different stimuli as representations of the same underlying cause, i.e., it has to develop an internal representation that is invariant to irrelevant changes of the stimulus. The work presented here is motivated by the question of how such invariant representations could be established.</p>
      <p>Because of the limited amount of information in the genome as well as the apparent flexibility of the neural development in different environments, it seems unlikely that the information needed to form invariant representations is already there at the beginning of individual development. Some information must be gathered from the sensory input experienced during interaction with the environment; it has to be learned. As this learning process is likely to be at least partially unsupervised, the brain requires a heuristics as to what stimuli should be classified as being the same.</p>
      <p>One possible indicator for stimuli to represent the same object is temporal proximity. A scene that the eye views is very unlikely to change completely from one moment to the next. Rather, there is a good chance that an object that can be seen now will also be present at the next instant of time. This implies that invariant representations should remain stable over time, that is, they should vary slowly. Inverting this reasoning, a sensory system that adapts to its sensory input in order to extract slowly varying aspects may succeed in learning invariant representations. This “slowness” or “temporal stability” principle is the basis of a whole class of learning algorithms [<xref ref-type="bibr" rid="pcbi-0030112-b001">1</xref>–<xref ref-type="bibr" rid="pcbi-0030112-b007">7</xref>]. Most applications of this approach have focused on models of the visual system, in particular on the self-organized formation of complex cell receptive fields in the primary visual cortex [<xref ref-type="bibr" rid="pcbi-0030112-b008">8</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b009">9</xref>].</p>
      <p>For clarity, we will focus on one of these algorithms, slow feature analysis (SFA; [<xref ref-type="bibr" rid="pcbi-0030112-b010">10</xref>]); a close link to the so-called “trace rule” will arise naturally. The goal of SFA is the following: given a multidimensional input signal <bold>x</bold>(<italic>t</italic>) and a finite-dimensional function space <named-content content-type="script" xlink:type="simple">F</named-content>, find the input–output function <italic>g</italic><sub>1</sub>(<bold>x</bold>) in <named-content content-type="script" xlink:type="simple">F</named-content> that generates the most slowly varying output signal <italic>y</italic><sub>1</sub>(<italic>t</italic>) = <italic>g</italic><sub>1</sub>(<bold>x</bold>(<italic>t</italic>)). It is important to note that the function <italic>g</italic><sub>1</sub>(<bold>x</bold>) is required to be an instantaneous function of the input signal. Otherwise, slow output signals could be generated by low-pass filtering the input signal. As the goal of the slowness principle is to detect slowly varying features of the <italic>input</italic> signals, a mere low-pass filter would certainly generate slow output signals, but it would not serve the purpose.</p>
      <p>As a measure of slowness, or rather “fastness,” SFA uses the variance of the time derivative,<inline-formula id="pcbi-0030112-ex001"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex001" xlink:type="simple"/></inline-formula>
				, which is the objective function to be minimized. Here, 〈·〉<italic><sub>t</sub></italic> denotes temporal averaging. For mathematical convenience and to avoid the trivial constant response, <italic>y</italic><sub>1</sub>(<italic>t</italic>) = const, a zero-mean, and unit variance constraint are imposed. Furthermore, it is possible to find a second function <italic>g</italic><sub>2</sub>(<bold>x</bold>) extracting <italic>y</italic><sub>2</sub>(<italic>t</italic>) = <italic>g</italic><sub>2</sub>(<bold>x</bold>(<italic>t</italic>)) that again minimizes the given objective under the constraint of being uncorrelated with <italic>y</italic><sub>1</sub>(<italic>t</italic>), a third one uncorrelated with both <italic>y</italic><sub>1</sub>(<italic>t</italic>) and <italic>y</italic><sub>2</sub>(<italic>t</italic>), and so on, thereby generating a set of slow features of the input ordered by the degree of slowness. However, in this paper, we will consider just one single output unit.
			</p>
      <p>SFA has been applied to the learning of translation, rotation, and other invariances in a model of the visual system [<xref ref-type="bibr" rid="pcbi-0030112-b010">10</xref>], and it has been shown that when applied to image sequences generated from static natural images, SFA learns functions that reproduce a wide range of features of complex cells in primary visual cortex [<xref ref-type="bibr" rid="pcbi-0030112-b008">8</xref>]. Iteration of the same principle in a hierarchical model in combination with a sparseness objective has been used to model the self-organized formation of spatial representations resembling place cells as found in the hippocampal formation of rodents [<xref ref-type="bibr" rid="pcbi-0030112-b011">11</xref>] (see [<xref ref-type="bibr" rid="pcbi-0030112-b012">12</xref>] for related work).</p>
      <p>These findings suggest that on an abstract level SFA reflects certain aspects of cortical information processing. However, SFA as a technical algorithm is biologically rather implausible. There is in particular one step in its canonical formulation that seems especially odd compared with what neurons are normally thought to do. In this step the eigenvector that corresponds to the smallest eigenvalue of the covariance matrix of the time derivative of some multidimensional signal is extracted. The aim of this paper is to show how this kind of computation can be realized in a spiking model neuron.</p>
      <p>In the following, we will first consider a continuous model neuron and demonstrate that a modified Hebbian learning rule enables the neuron to learn the slowest (in the sense of SFA) linear combination of its inputs. Apart from providing the basis for the analysis of the spiking model, this section reveals a mathematical link between SFA and the trace learning rule, another implementation of the slowness principle. We then examine if these findings also hold for a spiking model neuron, and find that for a linear Poisson neuron, spike-timing–dependent plasticity (STDP) can be interpreted as an implementation of the slowness principle.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>Continuous Model Neuron</title>
        <sec id="s2a1">
          <title>Linear model neuron and basic assumptions.</title>
          <p>First, consider a linear continuous model neuron with an input–output function given by
						<disp-formula id="pcbi-0030112-e001"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e001" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>with <inline-formula id="pcbi-0030112-ex002"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex002" xlink:type="simple"/></inline-formula>
						 indicating the input signals, <italic>w<sub>i</sub></italic> the weights, and <italic>a</italic><sup>out</sup> the output signal. For mathematical convenience, let <inline-formula id="pcbi-0030112-ex003"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex003" xlink:type="simple"/></inline-formula>
						and <italic>a</italic><sup>out</sup> (<italic>t</italic>) be defined on the interval <italic>t</italic> ∈ [−∞, ∞] but differ from zero only on [0,<italic>T</italic>], which could be the lifetime of the system. We assume that the input is approximately whitened on any sufficiently large interval [<italic>t<sub>a</sub></italic>,<italic>t<sub>b</sub></italic>] ⊂ [0,<italic>T</italic>] (i.e., each input signal has approximately zero mean and unit variance and is uncorrelated with other input signals):
						<disp-formula id="pcbi-0030112-e002"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e002" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mo>&ap;</mml:mo><mml:mn>0</mml:mn><mml:mtext>&emsp;</mml:mtext></mml:mrow></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>zero mean</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e003"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e003" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mo>&ap;</mml:mo><mml:mn>1</mml:mn><mml:mtext>&emsp;</mml:mtext></mml:mrow></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>unit variance</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e004"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e004" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>&ne;</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mo>&ap;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mtext>&emsp;</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mtext>decorrelation</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>This can be achieved by a normalization and decorrelation step of the units projecting to the considered unit. Furthermore, we assume that the output is normalized to unit variance, which for whitened input means that the weight vector is normalized to length 1. In an online learning rule, this could be implemented by either an activity-dependent or a weight-dependent normalization term. Thus, for the output signal we have:
						<disp-formula id="pcbi-0030112-e005"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e005" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mover><mml:mo>&ap;</mml:mo><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mover><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext>&emsp;</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>zero mean</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e006"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e006" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mover><mml:mo>&ap;</mml:mo><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mover><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext></mml:mrow></mml:mstyle><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mtext>&emsp;</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>unit variance</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math> --></disp-formula>In the following, we will often consider filtered signals. Therefore, we introduce abbreviations for the convolution <italic>f</italic><sub>∘</sub><italic>g</italic> and the cross-correlation <italic>f</italic> * <italic>g</italic> of two functions <italic>f</italic>(<italic>t</italic>) and <italic>g</italic>(<italic>t</italic>):
						<disp-formula id="pcbi-0030112-e007"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e007" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mtext>Convolution</mml:mtext><mml:mo>:</mml:mo><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mi>f</mml:mi><mml:mo>&compfn;</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>&tau;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mi>&tau;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>d</mml:mtext><mml:mi>&tau;</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e008"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e008" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mtext>Cross</mml:mtext><mml:mo>&hyphen;</mml:mo><mml:mtext>correlation</mml:mtext><mml:mo>:</mml:mo><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mi>f</mml:mi><mml:mo>&lowast;</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>&tau;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&plus;</mml:mo><mml:mi>&tau;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>d</mml:mtext><mml:mi>&tau;</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>For convenience, we will often use windowed signals, indicated by a hat
						<disp-formula id="pcbi-0030112-e009"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e009" xlink:type="simple"/><!-- <mml:math display='block'><mml:mover accent='true'><mml:mi>s</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mo stretchy="true">&lcub;</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mtd><mml:mtd><mml:mtext>for</mml:mtext></mml:mtd><mml:mtd><mml:mi>t</mml:mi><mml:mo>&isin;</mml:mo><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy='false'>&rsqb;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd></mml:mtd><mml:mtd><mml:mtext>otherwise</mml:mtext><mml:mtext></mml:mtext><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math> --></disp-formula>which allows us to replace the integration of a signal <italic>s</italic>(<italic>t</italic>) over [<italic>t<sub>a</sub></italic>,<italic>t<sub>b</sub></italic>] by an integration of ŝ<italic>(t)</italic> over [−∞, ∞]. We assume that the interval [<italic>t<sub>a</sub></italic>,<italic>t<sub>b</sub></italic>] is long compared to the width of the filters. In this case, effects from the integration boundaries are negligible, and we have
						<disp-formula id="pcbi-0030112-e010"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e010" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mi>f</mml:mi><mml:mo>&compfn;</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mspace width='3pt'/><mml:mi>h</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mo>&ap;</mml:mo><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mi>f</mml:mi><mml:mo>&compfn;</mml:mo><mml:mover accent='true'><mml:mi>s</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>Similar considerations hold for the cross-correlation (<xref ref-type="disp-formula" rid="pcbi-0030112-e008">Equation 8</xref>).
					</p>
          <p>Since convolution and cross-correlation are conveniently treated in Fourier space, we repeat the definition of the Fourier transform <inline-formula id="pcbi-0030112-ex101"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex101" xlink:type="simple"/></inline-formula> and the power spectrum <italic>P<sub>s</sub></italic>(<italic>ν</italic>) of a signal <italic>s</italic>(<italic>t</italic>).
						<disp-formula id="pcbi-0030112-e011"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e011" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mtext>Fourier transform</mml:mtext><mml:mo>:</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mo>:</mml:mo><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mtext>&scriptF;</mml:mtext><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>&pi;</mml:mi><mml:mtext>i</mml:mtext><mml:mi>&nu;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mtext>&hairsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>&nu;</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e012"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e012" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mtext>Power spectrum</mml:mtext><mml:mo>:</mml:mo><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mtext>s</mml:mtext></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:mtext>&hairsp;</mml:mtext><mml:mtext>&hairsp;</mml:mtext><mml:msub><mml:mtext>&scriptF;</mml:mtext><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:msub><mml:mover accent='true'><mml:mtext>&scriptF;</mml:mtext><mml:mo>&macr;</mml:mo></mml:mover><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
          <p>Throughout the paper, we make the assumption that input signals (and hence also the output signals) do not have significant power above some reasonable frequency <italic>ν<sub>max</sub></italic>.</p>
        </sec>
        <sec id="s2a2">
          <title>Reformulation of the slowness objective.</title>
          <p>SFA is based on the minimization of the second moment of the time derivative, <inline-formula id="pcbi-0030112-ex004"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex004" xlink:type="simple"/></inline-formula>
						. Even though there are neurons with transient responses to changes in the input, we believe it would be more plausible if we could derive an SFA-learning rule that does not depend on the time derivative, because it might be difficult to extract, especially for spiking neurons. It is indeed possible to replace the time derivative by a low-pass filtering as follows:
						<disp-formula id="pcbi-0030112-e013"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e013" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mtext>minimize </mml:mtext><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&dot;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e014"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e014" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:msup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&dot;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>because of Parseval's theorem</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e015"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e015" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo>&equals;</mml:mo><mml:mn>4</mml:mn><mml:msup><mml:mi>&pi;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>&nu;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>&nu;</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mtext>(since</mml:mtext><mml:msub><mml:mtext>&scriptF;</mml:mtext><mml:mrow><mml:mover accent='true'><mml:mi>s</mml:mi><mml:mo>&dot;</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mn>2</mml:mn><mml:mi>&pi;</mml:mi><mml:mtext>i</mml:mtext><mml:mi>&nu;</mml:mi><mml:msub><mml:mtext>&scriptF;</mml:mtext><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e016"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e016" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo>&iff;</mml:mo><mml:mtext>maximize</mml:mtext><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mtext>-</mml:mtext><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mtext>-</mml:mtext><mml:msup><mml:mi>&nu;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>&nu;</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e017"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e017" xlink:type="simple"/><!-- <mml:math display='block'><mml:mtable columnalign='left'><mml:mtr><mml:mtd><mml:mo>&iff;</mml:mo><mml:mtext>maximize</mml:mtext><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mtext>-</mml:mtext><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>&nu;</mml:mi><mml:mrow><mml:mi>max</mml:mi><mml:mo></mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&minus;</mml:mo><mml:msup><mml:mi>&nu;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy='false'>)</mml:mo><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>&nu;</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>since</mml:mtext><mml:mstyle displaystyle='true'><mml:mrow><mml:msubsup><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mtext>&thinsp;</mml:mtext></mml:mrow></mml:mrow></mml:mstyle><mml:mtext>d</mml:mtext><mml:mi>&nu;</mml:mi><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:msubsup><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy='true'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='true'>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext>&thinsp;</mml:mtext></mml:mrow></mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mover><mml:mo>&ap;</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>6</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mover><mml:mtext>const</mml:mtext></mml:mrow><mml:mo stretchy="true"></mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e018"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e018" xlink:type="simple"/><!-- <mml:math display='block'><mml:mtable columnalign='left'><mml:mtr><mml:mtd><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mtext>max(0,(</mml:mtext><mml:msubsup><mml:mi>&nu;</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mtext>2</mml:mtext></mml:msubsup><mml:mo>&minus;</mml:mo><mml:msup><mml:mi>&nu;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>&nu;</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>since</mml:mtext><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mn>0</mml:mn><mml:mtext>for</mml:mtext><mml:mo>|</mml:mo><mml:mi>&nu;</mml:mi><mml:mo>|</mml:mo><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>&nu;</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub><mml:mtext>by assumption</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e019"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e019" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>SFA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>&nu;</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e020"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e020" xlink:type="simple"/><!-- <mml:math display='block'><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>with</mml:mtext><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>SFA</mml:mtext></mml:mrow></mml:msub><mml:mtext>(t) defined such that</mml:mtext><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>SFA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mtext>(</mml:mtext><mml:mi>&nu;</mml:mi><mml:mtext>)&equals;max(0,(</mml:mtext><mml:msubsup><mml:mi>&nu;</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mtext>2</mml:mtext></mml:msubsup><mml:mo>&minus;</mml:mo><mml:msup><mml:mi>&nu;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e021"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e021" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>SFA</mml:mtext></mml:mrow></mml:msub><mml:mo>&compfn;</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>Thus, SFA can be achieved either by minimizing the variance of the time derivative of the output signal or by maximizing the variance of the appropriately filtered output signal. <xref ref-type="fig" rid="pcbi-0030112-g001">Figure 1</xref> provides an intuition for this alternative. The filter <italic>f<sub>SFA</sub></italic> is obviously a low-pass filter, as one would expect, with a <inline-formula id="pcbi-0030112-ex005"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex005" xlink:type="simple"/></inline-formula>
						 power spectrum below the limiting frequency <italic>ν</italic><sub>max</sub>. Because the phases are not determined, further assumptions are required to fully determine an SFA filter. However, we will proceed without defining a concrete filter, since it is not required for the considerations below.
					</p>
          <fig id="pcbi-0030112-g001" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.0030112.g001</object-id>
            <label>Figure 1</label>
            <caption>
              <title>Choosing Slow Directions of the Input</title>
              <p>Finding the direction of least variance in the time derivative of the input (which is part of the SFA algorithm) can be replaced by finding the direction of maximum variance in an appropriately low-pass filtered version of the input signal.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030112.g001" xlink:type="simple"/>
          </fig>
        </sec>
        <sec id="s2a3">
          <title>Hebbian learning on filtered signals.</title>
          <p>It is known that standard Hebbian learning under the constraint of a unit weight vector applied to a linear unit maximizes the variance of the output signal. We have seen in the previous section that SFA can be reformulated as a maximization problem for the variance of the low-pass filtered output signal. To achieve this, we simply apply Hebbian learning to the filtered input and output signals, instead of to the original signals.</p>
          <p>Consider a hypothetical unit that receives low-pass filtered inputs and, therefore, because of the linearity of the unit and the filtering, generates a low-pass filtered output
						<disp-formula id="pcbi-0030112-e022"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e022" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>SFA</mml:mtext></mml:mrow></mml:msub><mml:mo>&compfn;</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mover><mml:mo>&equals;</mml:mo><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo stretchy="true">&lsqb;</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>SFA</mml:mtext></mml:mrow></mml:msub><mml:mo>&compfn;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">&rsqb;</mml:mo></mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>SFA</mml:mtext></mml:mrow></mml:msub><mml:mo>&compfn;</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>where <italic>f<sub>SFA</sub></italic> is the kernel of the linear filter applied. It is obvious that a <italic>filtered Hebbian learning rule</italic>
						<disp-formula id="pcbi-0030112-e023"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e023" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mover accent='true'><mml:mi>w</mml:mi><mml:mo>&dot;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mi>&gamma;</mml:mi><mml:mrow><mml:mo stretchy="true">&lsqb;</mml:mo><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup><mml:mo>&compfn;</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">&rsqb;</mml:mo></mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo>&compfn;</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:math> --></disp-formula>with <italic>f</italic><sup>in</sup>: = <italic>f</italic><sup>out</sup>: = <italic>f<sub>SFA</sub></italic> maximizes the objective in <xref ref-type="disp-formula" rid="pcbi-0030112-e021">Equation 21</xref>.
					</p>
          <p>Remember that the input is white (i.e., the <inline-formula id="pcbi-0030112-ex006"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex006" xlink:type="simple"/></inline-formula>
						 are uncorrelated and have unit variance), and the weight vector is normalized to norm one by some additional normalization rule, so that we know that the output signal <italic>a</italic><sup>out</sup> has the same variance no matter what the direction of the weight vector is. Thus, the filtered Hebbian plasticity rule (together with the normalization rule not specified here) optimizes slowness (<xref ref-type="disp-formula" rid="pcbi-0030112-e013">Equation 13</xref>) under the constraint of unit variance (<xref ref-type="disp-formula" rid="pcbi-0030112-e006">Equation 6</xref>). <xref ref-type="fig" rid="pcbi-0030112-g002">Figure 2</xref> illustrates this learning scheme. It also underlines the necessity for a clear distinction between processing and learning. Although the slowness principle does not allow low-pass filtering as a means of generating slow signals during processing, the learning rule may well make use of low-pass filtered signals to detect slowly varying features in the input signal. This distinction will become particularly important for the Poisson model neuron below, as it incorporates an excitatory postsynaptic potential (EPSP) that acts as a low-pass filter during processing. An implementation of the slowness principle in such a system must avoid the system exploiting the EPSP as a means of generating slow signals.
					</p>
          <fig id="pcbi-0030112-g002" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.0030112.g002</object-id>
            <label>Figure 2</label>
            <caption>
              <title>Filtered Hebbian Learning Rule</title>
              <p>Input and output signals are filtered (downward arrows). The weight change is the result of applying the Hebbian learning rule on the filtered signals (square box and upward arrow). Thereby, the variance of the filtered version of the output is maximized without actually filtering the output during processing.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030112.g002" xlink:type="simple"/>
          </fig>
        </sec>
        <sec id="s2a4">
          <title>Alternative filtering procedures.</title>
          <p>If learning is slow, the total weight change over a time interval [<italic>t<sub>a</sub></italic>,<italic>t<sub>b</sub></italic>] in a synapse can be written as
						<disp-formula id="pcbi-0030112-e024"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e024" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>&Delta;</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mover accent='true'><mml:mi>w</mml:mi><mml:mo>&dot;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e025"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e025" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mover><mml:mo>&equals;</mml:mo><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>23</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mover><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true">&lsqb;</mml:mo><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup><mml:mo>&compfn;</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="true">&rsqb;</mml:mo></mml:mrow><mml:mtext>(t)</mml:mtext><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo>&compfn;</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow><mml:mtext>(t)</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e026"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e026" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mover><mml:mo>&ap;</mml:mo><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mover><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true">&lsqb;</mml:mo><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup><mml:mo>&compfn;</mml:mo><mml:msubsup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="true">&rsqb;</mml:mo></mml:mrow><mml:mtext>(t)</mml:mtext><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo>&compfn;</mml:mo><mml:msup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow><mml:mtext>(t)</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e027"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e027" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo>&equals;</mml:mo><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo>&lowast;</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow><mml:mo>&compfn;</mml:mo><mml:msubsup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow><mml:mtext>(t)</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mtext>(t)</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e028"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e028" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo>&equals;</mml:mo><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup><mml:mo>&lowast;</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow><mml:mo>&compfn;</mml:mo><mml:msup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow><mml:mtext>(t)d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e029"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e029" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo>&equals;</mml:mo><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup><mml:mo>&lowast;</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow><mml:mtext>(t)</mml:mtext><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:msup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo>&lowast;</mml:mo><mml:msubsup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow><mml:mtext>(t)</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
          <p>Thus, one can either convolve input and output signal with filters <italic>f</italic><sup>in</sup> and <italic>f</italic><sup>out</sup>, respectively, the input signal with <italic>f</italic><sup>out</sup> * <italic>f</italic><sup>in</sup>, or the output signal with <italic>f</italic><sup>in</sup> * <italic>f</italic><sup>out</sup>. Note that [<italic>f</italic><sup>in</sup> * <italic>f</italic><sup>out</sup>](t) = [<italic>f</italic><sup>out</sup> * <italic>f</italic><sup>in</sup>](−<italic>t</italic>). One can actually use any pair of filters <italic>f</italic><sup>in</sup> and <italic>f</italic><sup>out</sup> as long as <italic>f</italic><sup>in</sup> * <italic>f</italic><sup>out</sup> fulfills the condition
						<disp-formula id="pcbi-0030112-e030"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e030" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mtext>&scriptF;</mml:mtext><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup><mml:mo>&lowast;</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>SFA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
        </sec>
        <sec id="s2a5">
          <title>Relation to other learning rules.</title>
          <p>Hebbian learning on low-pass filtered signals is the basis of several other models for unsupervised learning of invariances [<xref ref-type="bibr" rid="pcbi-0030112-b001">1</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b006">6</xref>]. These models essentially subject the output signal to an exponential temporal filter <italic>f</italic>(<italic>t</italic>) = <italic>θ</italic>(<italic>t)</italic>exp(−<italic>γt</italic>) and then use Hebbian learning to associate it with the input signal. Here, <italic>θ</italic>(<italic>t</italic>) denotes the Heaviside step function, which is 0 for <italic>t</italic> &lt; 0 and 1 for <italic>t</italic> ≥ 0. This learning rule has been named the “trace rule.” The considerations in the last section provide a link between this approach and ours. We simply have to replace <italic>f</italic><sup>in</sup> with a <italic>δ-</italic>function and <italic>f</italic><sup>out</sup> with <italic>f</italic>(<italic>t</italic>). <xref ref-type="disp-formula" rid="pcbi-0030112-e029">Equation 29</xref> then takes the form
						<disp-formula id="pcbi-0030112-e031"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e031" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>&Delta;</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:munder><mml:mo>&sum;</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:mrow><mml:mo stretchy="true">&lsqb;</mml:mo><mml:mrow><mml:mstyle displaystyle='true'><mml:mrow><mml:msubsup><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:msubsup><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:msubsup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo>&lowast;</mml:mo><mml:msubsup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="true">&rsqb;</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>since the output signal <inline-formula id="pcbi-0030112-ex007"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex007" xlink:type="simple"/></inline-formula>
						 is a linear function of the input (see  <xref ref-type="disp-formula" rid="pcbi-0030112-e001">Equation 1</xref>). In the previously mentioned applications of the trace rule, the statistics of the input signals were always reversible, so we will assume that all correlation functions <inline-formula id="pcbi-0030112-ex008"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex008" xlink:type="simple"/></inline-formula>
						 are symmetric in time. This implies that only the symmetric component of <italic>f</italic>(<italic>t</italic>) is relevant for learning:
						<disp-formula id="pcbi-0030112-e032"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e032" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>sym</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mo>&minus;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mi>&gamma;</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>exp(-</mml:mtext><mml:mi>&gamma;</mml:mi><mml:mo>|</mml:mo><mml:mi>t</mml:mi><mml:mo>|</mml:mo><mml:mtext>)</mml:mtext></mml:mrow><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>It is easy to show that the learning rule in <xref ref-type="disp-formula" rid="pcbi-0030112-e031">Equation 31</xref> can be interpreted as a gradient ascent on the following objective function:
						<disp-formula id="pcbi-0030112-e033"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e033" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>&Psi;</mml:mi><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>sym</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:msup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo>&lowast;</mml:mo><mml:msup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e034"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e034" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mtext>&scriptF;</mml:mtext><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>sym</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:msup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>&nu;</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>By comparison with <xref ref-type="disp-formula" rid="pcbi-0030112-e019">Equation 19</xref>, it becomes clear that the trace rule implements a very similar objective as our model. The only difference is that the power spectrum in <xref ref-type="disp-formula" rid="pcbi-0030112-e020">Equation 20</xref> is replaced by the Fourier transform of the filter <italic>f</italic><sup>sym</sup>. Note that in order to be able to interpret Ψ as an objective function, it should be real-valued. The replacement of <italic>f</italic> with <italic>f</italic><sup>sym</sup> ensures that <inline-formula id="pcbi-0030112-ex009"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex009" xlink:type="simple"/></inline-formula>
						 is real-valued and symmetric, so Ψ is real-valued as well. The Fourier transform of <italic>f</italic><sup>sym</sup> is given by
						<disp-formula id="pcbi-0030112-e035"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e035" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mtext>&scriptF;</mml:mtext><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>sym</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mi>&gamma;</mml:mi><mml:mrow><mml:msup><mml:mi>&gamma;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&plus;</mml:mo><mml:msup><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>2</mml:mn><mml:mi>&pi;</mml:mi><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>This shows that the only difference between the trace rule and our model lies in the choice of the power spectrum for the low-pass filter. While we are using a parabolic power spectrum with a cutoff (<xref ref-type="disp-formula" rid="pcbi-0030112-e020">Equation 20</xref>), the trace rule uses a power spectrum with the shape of a Cauchy function (<xref ref-type="disp-formula" rid="pcbi-0030112-e035">Equation 35</xref>).
					</p>
          <p>From this perspective, one can interpret SFA as a quadratic approximation of the trace rule. To what extent this approximation is valid depends on the power spectra of the input signals. If most of the input power is concentrated at low frequencies, where the power spectrum resembles a parabola, the learning rules can be expected to learn very similar weight vectors. In fact, any Hebbian learning rule that leads to an objective function of the shape of <xref ref-type="disp-formula" rid="pcbi-0030112-e019">Equation 19</xref> with a low-pass filtering spectrum in the place of <inline-formula id="pcbi-0030112-ex010"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex010" xlink:type="simple"/></inline-formula>
						 essentially implements the slowness principle, as among signals with the same variance, it will favor slower ones.
					</p>
        </sec>
      </sec>
      <sec id="s2b">
        <title>Spiking Model Neuron</title>
        <p>Real neurons do not transmit information via a continuous stream of analog values like the model neuron considered in the previous section, but rather emit action potentials that carry information by means of their rate and probably also by their exact timing, a fact we will not consider here. How can the model developed so far be mapped onto this scenario?</p>
        <sec id="s2b1">
          <title>The linear Poisson neuron.</title>
          <p>Again, we restrict our analysis to a simple case by modeling the spike-train signals by inhomogeneous Poisson processes. Note that at this point, we restrict our analysis to a rate code, thus neglecting possible coding paradigms that rely on precise timing of spikes.</p>
          <p>To generate the input spike trains, we first add sufficiently large constants <inline-formula id="pcbi-0030112-ex011"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex011" xlink:type="simple"/></inline-formula>
						 to the continuous and zero-mean signals <inline-formula id="pcbi-0030112-ex012"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex012" xlink:type="simple"/></inline-formula>
						 to turn them into strictly positive signals that can be interpreted as rates
						<disp-formula id="pcbi-0030112-e036"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e036" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo>&plus;</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
          <p>The constants <inline-formula id="pcbi-0030112-ex013"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex013" xlink:type="simple"/></inline-formula>
						 represent mean firing rates, which are modulated by the input signals <inline-formula id="pcbi-0030112-ex014"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex014" xlink:type="simple"/></inline-formula>
						. From the input rates <inline-formula id="pcbi-0030112-ex015"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex015" xlink:type="simple"/></inline-formula>
						, we then derive inhomogeneous Poisson spike trains <inline-formula id="pcbi-0030112-ex016"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex016" xlink:type="simple"/></inline-formula>
						 drawn from ensembles <inline-formula id="pcbi-0030112-ex017"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex017" xlink:type="simple"/></inline-formula>
						 such that
						<disp-formula id="pcbi-0030112-e037"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e037" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mrow><mml:mo>&lang;</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&rang;</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:mo>&equals;</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>where <inline-formula id="pcbi-0030112-ex018"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex018" xlink:type="simple"/></inline-formula>
						 denotes the average over the ensemble <inline-formula id="pcbi-0030112-ex019"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex019" xlink:type="simple"/></inline-formula>
						.
					</p>
          <p>The output rate is modeled as a weighted sum over the input spike trains convolved with an EPSP <italic>ε</italic>(<italic>t</italic>) plus a baseline firing rate <italic>r</italic><sub>0</sub>, which ensures that the output firing rate remains positive. This is necessary as we allow inhibitory synapses (i.e., negative weights).
						<disp-formula id="pcbi-0030112-e038"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e038" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">&lsqb;</mml:mo><mml:mrow><mml:mi>&epsiv;</mml:mi><mml:mo>&compfn;</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">&rsqb;</mml:mo></mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
					</p>
          <p>Note that in this scheme, the EPSP reflects the change in the postsynaptic firing probability due to a presynaptic spike rather than a change in the membrane potential. Ideally, it includes all delay effects in neuronal transmission.</p>
          <p>The output of this spiking neuron is yet another inhomogeneous Poisson spike train <italic>S</italic><sup>out</sup>(<italic>t</italic>) drawn from an ensemble <italic>E</italic><sup>out</sup>, given a realization of the input spike trains <inline-formula id="pcbi-0030112-ex020"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex020" xlink:type="simple"/></inline-formula>
						 such that
						<disp-formula id="pcbi-0030112-e039"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e039" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mrow><mml:mo>&lang;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&rang;</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mo>&lcub;</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo>&rcub;</mml:mo></mml:mrow></mml:msub><mml:mo>&equals;</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
          <p>It should be noted that not only is the output spike train <italic>S</italic><sup>out</sup>(<italic>t</italic>) stochastic in this model, but also the underlying output rate <italic>m</italic>(<italic>t</italic>), which is a function of the stochastic variables <inline-formula id="pcbi-0030112-ex021"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex021" xlink:type="simple"/></inline-formula>
						 and generally differs for each realization of the input. This is the reason why the input and output spike trains are not statistically independent. However, due to the linearity of the model neuron, the output rate is still simply
						<disp-formula id="pcbi-0030112-e040"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e040" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mrow><mml:mo>&lang;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&rang;</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e041"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e041" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mover><mml:mo>&equals;</mml:mo><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>39</mml:mn><mml:mo>,</mml:mo><mml:mn>38</mml:mn><mml:mo>,</mml:mo><mml:mn>37</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mover><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">&lsqb;</mml:mo><mml:mrow><mml:mi>&epsiv;</mml:mi><mml:mo>&compfn;</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">&rsqb;</mml:mo></mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e042"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e042" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mover><mml:mo>&equals;</mml:mo><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>36</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mover><mml:munder><mml:munder><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mi>&epsiv;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy='true'>&underbrace;</mml:mo></mml:munder><mml:mrow><mml:mo>&equals;</mml:mo><mml:mo>:</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mo>&plus;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">&lsqb;</mml:mo><mml:mrow><mml:mi>&epsiv;</mml:mi><mml:mo>&compfn;</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">&rsqb;</mml:mo></mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e043"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e043" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo>&equals;</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo>&plus;</mml:mo><mml:mrow><mml:mo stretchy="true">&lsqb;</mml:mo><mml:mrow><mml:mi>&epsiv;</mml:mi><mml:mo>&compfn;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">&rsqb;</mml:mo></mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e044"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e044" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mover><mml:mo>&equals;</mml:mo><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mover><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo>&plus;</mml:mo><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:mi>&epsiv;</mml:mi><mml:mo>&compfn;</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext> ,</mml:mtext></mml:mrow></mml:math> --></disp-formula>and the joint firing rate is
						<disp-formula id="pcbi-0030112-e045"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e045" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in,out</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mrow><mml:mo>&lang;</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mo>&rang;</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e046"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e046" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo>&equals;</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>&epsiv;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo>&minus;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&emsp;</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>see</mml:mtext><mml:mrow><mml:mo>&lsqb;</mml:mo><mml:mrow><mml:mn>13</mml:mn></mml:mrow><mml:mo>&rsqb;</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:math> --></disp-formula>
					</p>
          <p>The first term would result also from a rate model, while the second term captures the statistical dependencies between input and output spike trains mediated by the synaptic weights <italic>w<sub>i</sub></italic> and the EPSP <italic>ε</italic>(<italic>t</italic>).</p>
        </sec>
        <sec id="s2b2">
          <title>STDP can perform SFA.</title>
          <p>In this section, we will demonstrate that in an ensemble-averaged sense it is possible to generate the same weight distribution as in the continuous model by means of an STDP rule with a specific learning window.</p>
          <p>Synaptic plasticity that depends on the temporal order of pre- and postsynaptic spikes has been found in a number of neuronal systems [<xref ref-type="bibr" rid="pcbi-0030112-b014">14</xref>–<xref ref-type="bibr" rid="pcbi-0030112-b018">18</xref>], and has raised a lot of interest among modelers [<xref ref-type="bibr" rid="pcbi-0030112-b019">19</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b020">20</xref>] (for a review, see [<xref ref-type="bibr" rid="pcbi-0030112-b021">21</xref>]). Typically, synapses undergo long-term potentiation (LTP) if a presynaptic spike precedes a postsynaptic spike within a timescale of tens of milliseconds and long-term depression (LTD) for the opposite temporal order. Assuming that the change in synaptic efficacy occurs on a slower timescale than the typical interspike interval, the STDP weight dynamics can be modeled as
						<disp-formula id="pcbi-0030112-e047"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e047" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>&Delta;</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mi>&alpha;</mml:mi><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:munderover><mml:mrow><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mi>&beta;</mml:mi><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:munderover><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>&alpha;</mml:mi></mml:mrow><mml:mtext>in</mml:mtext></mml:msubsup><mml:mo>&minus;</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mo>&beta;</mml:mo><mml:mtext>out</mml:mtext></mml:msubsup><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
          <p>Here, <inline-formula id="pcbi-0030112-ex022"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex022" xlink:type="simple"/></inline-formula>
						 denotes the spike times of the presynaptic spikes at synapse <italic>i</italic> and <inline-formula id="pcbi-0030112-ex023"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex023" xlink:type="simple"/></inline-formula>
						 denotes the postsynaptic spike times. <italic>W</italic>(<italic>t</italic>) is the learning window that determines if and to what extent the synapse is potentiated or depressed by a single spike pair. The convention is such that negative arguments <italic>t</italic> in <italic>W</italic>(<italic>t</italic>) correspond to the situation where the presynaptic spike precedes the postsynaptic spike. <inline-formula id="pcbi-0030112-ex024"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex024" xlink:type="simple"/></inline-formula>
						 and <italic>m</italic><sup>out</sup> are the numbers of pre- and postsynaptic spikes occurring in the time interval [<italic>t<sub>a</sub></italic>, <italic>t<sub>b</sub></italic>] under consideration. <italic>γ</italic> is a small positive learning rate. Note that due to the presence of this learning rate, the absolute scale of the learning window <italic>W</italic> is not important for our analysis.
					</p>
          <p>We circumvent the well-known stability problem of STDP by applying an explicit weight normalization (<inline-formula id="pcbi-0030112-ex025"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex025" xlink:type="simple"/></inline-formula>
						) instead of weight-dependent learning rates as used elsewhere [<xref ref-type="bibr" rid="pcbi-0030112-b022">22</xref>–<xref ref-type="bibr" rid="pcbi-0030112-b024">24</xref>]. Such a normalization procedure could be implemented by means of a homeostatic mechanism targeting the output firing rate (e.g., by synaptic scaling; for reviews, see [<xref ref-type="bibr" rid="pcbi-0030112-b025">25</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b026">26</xref>]).
					</p>
          <p>Modeling the spike trains as sums of delta pulses (i.e., <inline-formula id="pcbi-0030112-ex026"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex026" xlink:type="simple"/></inline-formula>
						), the learning rule in <xref ref-type="disp-formula" rid="pcbi-0030112-e047">Equation 47</xref> can be rewritten as
						<disp-formula id="pcbi-0030112-e048"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e048" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>&Delta;</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e049"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e049" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo>&ap;</mml:mo><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:msubsup><mml:mover accent='true'><mml:mi>S</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:msup><mml:mover accent='true'><mml:mi>S</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
          <p>Taking the ensemble average allows us to retrieve the rates that underlie the spike trains and thus the signals <inline-formula id="pcbi-0030112-ex027"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex027" xlink:type="simple"/></inline-formula>
						 and <inline-formula id="pcbi-0030112-ex028"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex028" xlink:type="simple"/></inline-formula>
						 of the continuous model:
						<disp-formula id="pcbi-0030112-e050"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e050" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mrow><mml:mo>&lang;</mml:mo><mml:mi>&Delta;</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&rang;</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mover><mml:mo>&ap;</mml:mo><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>49</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mover><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mrow><mml:mo>&lang;</mml:mo><mml:msubsup><mml:mover accent='true'><mml:mi>S</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:msup><mml:mover accent='true'><mml:mi>S</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mo>&rang;</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e051"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e051" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mover><mml:mo>&equals;</mml:mo><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>46</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mover><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent='true'><mml:mi>r</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&hairsp;</mml:mtext><mml:msup><mml:mover accent='true'><mml:mi>r</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>&epsiv;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo>&minus;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:msubsup><mml:mover accent='true'><mml:mi>r</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e052"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e052" xlink:type="simple"/><!-- <mml:math display='block'><mml:mover><mml:mo>&equals;</mml:mo><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>36</mml:mn><mml:mo>,</mml:mo><mml:mn>44</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mover><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mtext>&thinsp;</mml:mtext><mml:mi>W</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo>&plus;</mml:mo><mml:msubsup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo>&plus;</mml:mo><mml:mi>&epsiv;</mml:mi><mml:mo>&compfn;</mml:mo><mml:msup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mtext>&hairsp;</mml:mtext><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:mtext>&plus;</mml:mtext><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover></mml:mrow></mml:mstyle><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>&epsiv;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo>&minus;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo>&plus;</mml:mo><mml:msubsup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mtext>&hairsp;</mml:mtext><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo></mml:mrow></mml:mstyle><mml:mtext></mml:mtext><mml:mtext>.</mml:mtext></mml:math> --></disp-formula>
					</p>
          <p>Expanding the products in <xref ref-type="disp-formula" rid="pcbi-0030112-e052">Equation 52</xref> gives rise to a number of terms, among which only one depends on both the input and the output signal <inline-formula id="pcbi-0030112-ex029"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex029" xlink:type="simple"/></inline-formula>
						 and <inline-formula id="pcbi-0030112-ex030"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex030" xlink:type="simple"/></inline-formula>
						. Because each input signal has a vanishing mean, terms containing just one input signal lead to negligible contributions. The remaining terms depend only on the mean firing rates <inline-formula id="pcbi-0030112-ex031"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex031" xlink:type="simple"/></inline-formula>
						 and <inline-formula id="pcbi-0030112-ex032"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex032" xlink:type="simple"/></inline-formula>
						:
						<disp-formula id="pcbi-0030112-e053"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e053" xlink:type="simple"/><!-- <mml:math display='block'><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mo>&lang;</mml:mo><mml:mi>&Delta;</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&rang;</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mover><mml:mo>&ap;</mml:mo><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>52</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mover><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:msubsup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mi>&epsiv;</mml:mi><mml:mo>&compfn;</mml:mo><mml:msup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi><mml:mo>&prime;</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&plus;</mml:mtext><mml:mi>&gamma;</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mi>&epsiv;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mo>&minus;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&plus;</mml:mtext><mml:mi>&gamma;</mml:mi><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msubsup><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math> --></disp-formula>
					</p>
          <p>A generalized version of <xref ref-type="disp-formula" rid="pcbi-0030112-e053">Equation 53</xref> that incorporates non-Hebbian plasticity (i.e., terms that depend on the pre/postsynaptic signals only) has been derived and discussed by Kempter et al. [<xref ref-type="bibr" rid="pcbi-0030112-b027">27</xref>]. Regarding the effects of the input signals on learning, the decisive term is the first one. The other two are rather unspecific in that they do not depend on the properties of the input and output signals <inline-formula id="pcbi-0030112-ex033"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex033" xlink:type="simple"/></inline-formula>
						 and <inline-formula id="pcbi-0030112-ex034"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex034" xlink:type="simple"/></inline-formula>
						.
					</p>
          <p>The second term alone would generate a competition between the weights: synapses that experience a higher mean input firing rate <inline-formula id="pcbi-0030112-ex035"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex035" xlink:type="simple"/></inline-formula>
						 grow more rapidly than those with smaller input firing rates. If we assume that the input neurons fire with the same mean firing rate, all weights grow with the same rate, so the direction of the weight vector remains unchanged. Thus, due to the explicit weight normalization, this term has no effect on the weight dynamics and can be neglected.
					</p>
          <p>If the integral over the learning window is positive, the third term in <xref ref-type="disp-formula" rid="pcbi-0030112-e053">Equation 53</xref> favors a weight vector that is proportional to the vector of the mean firing rates of the input neurons. It thus stabilizes the homogeneous weight distribution and opposes the effect of the first term, which captures correlations in the input signals. Note that this is only true if the integral over the learning window is positive; otherwise, this term introduces a competition between the weights [<xref ref-type="bibr" rid="pcbi-0030112-b024">24</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b027">27</xref>]. One possible interpretation is that the neuron has a “default state” in which all synapses are equally strong and that correlations in the input need to surpass a certain threshold in order to be imprinted in the synaptic connections. Interestingly, this threshold is determined by the integral over the learning window, which implies that neurons that balance LTP and LTD should be more sensitive to input correlations.</p>
          <p>An alternative possibility is that the neuron possesses a mechanism of canceling the effects of this term. From a computational perspective this would be sensible, as the mean firing rates <inline-formula id="pcbi-0030112-ex036"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex036" xlink:type="simple"/></inline-formula>
						 and <inline-formula id="pcbi-0030112-ex037"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex037" xlink:type="simple"/></inline-formula>
						 do not carry information about the input, neither in rate nor in a timing code. If we conceive neurons as information encoders aiming at adapting to the structure of their input, this term is thus more hindrance than help. Assuming that the neuron compensates for this term, the dynamics of the synaptic weights are governed exclusively by the correlations in the input signals as reflected by the first term. In the following, we will restrict our considerations to this term and omit the others.
					</p>
          <p>Rearranging the temporal integrations, we can rewrite <xref ref-type="disp-formula" rid="pcbi-0030112-e053">Equation 53</xref> for the weight updates as
						<disp-formula id="pcbi-0030112-e054"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e054" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mrow><mml:mo>&lang;</mml:mo><mml:mi>&Delta;</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&rang;</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mover><mml:mo>&ap;</mml:mo><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>53</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mover><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:munderover><mml:mo>&int;</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&infin;</mml:mi></mml:mrow><mml:mi>&infin;</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mi>W</mml:mi><mml:mo>&compfn;</mml:mo><mml:mi>&epsiv;</mml:mi><mml:mo stretchy='false'>&rsqb;</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:msup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo>&lowast;</mml:mo><mml:msubsup><mml:mover accent='true'><mml:mi>a</mml:mi><mml:mo>&circ;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mtext>in</mml:mtext></mml:msubsup><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
          <p>The first conclusion we can draw from this reformulation is that for the dynamics of the learning process the convolution of the learning window with the EPSP and not the learning window alone is relevant. As discussed below, this might have important consequences for functional interpretations of the shape of the learning window.</p>
          <p>Second, by comparison with <xref ref-type="disp-formula" rid="pcbi-0030112-e029">Equation 29</xref>, it is obvious that in order to learn the same weight distribution as in the continuous model, the learning window has to fulfill the condition that
						<disp-formula id="pcbi-0030112-e055"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e055" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mi>W</mml:mi><mml:mo>&compfn;</mml:mo><mml:mi>&epsiv;</mml:mi><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup><mml:mo>&lowast;</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mo>:</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0030112-e056"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e056" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo>&iff;</mml:mo><mml:msub><mml:mtext>&scriptF;</mml:mtext><mml:mrow><mml:mi>W</mml:mi><mml:mo>&compfn;</mml:mo><mml:mi>&epsiv;</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>&scriptF;</mml:mi><mml:mi>W</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mtext>&scriptF;</mml:mtext><mml:mi>&epsiv;</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mtext>&scriptF;</mml:mtext><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup><mml:mo>&lowast;</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>SFA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mtext>&scriptF;</mml:mtext><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math> --></disp-formula>
					</p>
          <p>Here, <italic>W</italic><sub>0</sub> is the convolution of <italic>W</italic> with <italic>ε</italic> and is equal to the learning window in the limit of an infinitely short, <italic>δ</italic>-shaped EPSP. As the power spectrum <inline-formula id="pcbi-0030112-ex038"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex038" xlink:type="simple"/></inline-formula>
						 is of course real, <italic>W</italic><sub>0</sub> is symmetric in time. Note that the width of <italic>W</italic><sub>0</sub> scales inversely with the width of the power spectrum <inline-formula id="pcbi-0030112-ex039"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex039" xlink:type="simple"/></inline-formula>
						, which in turn is proportional to ν<sub>max</sub>. Once the power spectrum <inline-formula id="pcbi-0030112-ex040"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex040" xlink:type="simple"/></inline-formula>
						 and the EPSP is given, <xref ref-type="disp-formula" rid="pcbi-0030112-e056">Equation 56</xref> uniquely determines the learning window <italic>W</italic>. Because it is <italic>W</italic><sub>0</sub> rather than <italic>W</italic> that determines the learning dynamics, we will refer to <italic>W</italic><sub>0</sub> as the “effective learning window.”
					</p>
        </sec>
        <sec id="s2b3">
          <title>Learning windows.</title>
          <p>According to the last section, we require special learning windows to learn the slow directions in the input. This of course raises the question of which window shapes are favorable, and in particular if these are in agreement with physiological findings.</p>
          <p>Given the shape of the EPSP and the power spectrum <inline-formula id="pcbi-0030112-ex041"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex041" xlink:type="simple"/></inline-formula>
						, the learning window is uniquely determined by <xref ref-type="disp-formula" rid="pcbi-0030112-e056">Equation 56</xref>. Remember that the only parameter in the power spectrum <inline-formula id="pcbi-0030112-ex042"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex042" xlink:type="simple"/></inline-formula>
						 is the frequency <italic>ν</italic><sub>max</sub>, above which the power spectrum of the input data was assumed to vanish. For simplicity, we model the EPSP as a single exponential with a time constant <italic>τ</italic>:
						<disp-formula id="pcbi-0030112-e057"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e057" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>&epsiv;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>&theta;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mtext>e</mml:mtext><mml:mrow><mml:mo>&minus;</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:mi>&tau;</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
          <p>For this particular EPSP shape, the learning window can be calculated analytically by inverting the Fourier transform in <xref ref-type="disp-formula" rid="pcbi-0030112-e056">Equation 56</xref>. The result can be written as
						<disp-formula id="pcbi-0030112-e058"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e058" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mrow><mml:mo stretchy="true">&lsqb;</mml:mo><mml:mrow><mml:mfrac><mml:mtext>d</mml:mtext><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>&plus;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>&tau;</mml:mi></mml:mfrac></mml:mrow><mml:mo stretchy="true">&rsqb;</mml:mo></mml:mrow><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math> --></disp-formula><italic>W</italic><sub>0</sub> is symmetric, so its derivative is antisymmetric. Thus, the learning window is a linear combination of a symmetric and an antisymmetric component. As the width of <italic>W</italic><sub>0</sub> scales with the inverse of <italic>ν</italic><sub>max</sub>, its temporal derivative scales with <italic>ν</italic><sub>max</sub>. Accordingly, the symmetry of the learning window is governed by an interplay of the duration <italic>τ</italic> of the EPSP and the maximal input frequency <italic>ν</italic><sub>max</sub>. For <italic>τ</italic> ≪ 1 / <italic>ν</italic><sub>max</sub> the learning window is dominated by <italic>W</italic><sub>0</sub> and thus symmetric, whereas for <italic>τ</italic> ≫ 1 / <italic>ν</italic><sub>max</sub>, the temporal derivative of <italic>W</italic><sub>0</sub> is dominant, so the learning window is antisymmetric.
					</p>
          <p>We have assumed that the input signals have negligible power above the maximal input frequency <italic>ν</italic><sub>max</sub>. Thus, the temporal structure of the input signals can only provide a lower bound for <italic>ν</italic><sub>max</sub>. On the other hand, exceedingly high values for <italic>ν</italic><sub>max</sub> lead to very narrow learning windows, thereby sharpening the coincidence detection and reducing the speed of learning. Moreover, it may be metabolically costly to implement physiological processes that are faster than necessary. Thus, it appears sensible to choose <italic>ν</italic><sub>max</sub> such that 1 / <italic>ν</italic><sub>max</sub> reflects the fastest timescale in the input signals. Accordingly, the symmetry of the learning window is governed by the relation between the length of the EPSP and the fastest timescale in the input data. If the EPSP is short enough to resolve the fastest input components, the learning window is symmetric. If the EPSP is too long to fully resolve the temporal structure of the input (i.e., it acts as a low-pass filter), the learning window will tend to be antisymmetric.</p>
          <p>We choose a value of <italic>ν</italic><sub>max</sub> = 1 / (40 ms). The argument for this choice is that within a rate code, the cells that project to the neuron under consideration can hardly convey signals that vary on a faster timescale than the duration of their EPSP. It is thus reasonable to choose the time constant of the EPSP and the inverse of the cutoff frequency to have the same order of magnitude. Typical durations of cortical EPSPs are of the order of tens of milliseconds (see [<xref ref-type="bibr" rid="pcbi-0030112-b028">28</xref>] for further references and a critical discussion), so 40 ms seems a reasonable value.</p>
          <p><xref ref-type="fig" rid="pcbi-0030112-g003">Figure 3</xref> illustrates the connection between <inline-formula id="pcbi-0030112-ex043"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex043" xlink:type="simple"/></inline-formula>
						, <italic>W</italic><sub>0</sub>, the learning window, and the EPSP. It also shows the learning windows for three different durations of the EPSP, while keeping <italic>ν</italic><sub>max</sub> = 1 / (40 ms). The oscillatory and slowly decaying tails of <italic>W</italic>(<italic>t</italic>) are due to the sharp cutoff of the power spectrum <inline-formula id="pcbi-0030112-ex044"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex044" xlink:type="simple"/></inline-formula>
						 at |<italic>ν</italic>| = <italic>ν</italic><sub>max</sub> and become less pronounced if <inline-formula id="pcbi-0030112-ex045"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex045" xlink:type="simple"/></inline-formula>
						 is smoothened out.
					</p>
          <fig id="pcbi-0030112-g003" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.0030112.g003</object-id>
            <label>Figure 3</label>
            <caption>
              <title>Relation between the EPSP and the Learning Window</title>
              <p>The power spectrum<inline-formula id="pcbi-0030112-ex049"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex049" xlink:type="simple"/></inline-formula>
								 is the Fourier transform of the effective learning window <italic>W</italic><sub>0</sub>, which in turn is the convolution of the learning window <italic>W</italic> and the EPSP <italic>ε</italic>. The figure shows the learning windows required for SFA for three different EPSP durations (<italic>τ</italic> = 4, 40, 400 ms). The maximal input frequency <italic>ν</italic><sub>max</sub> was 1 / (40 ms) in all plots.
							</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030112.g003" xlink:type="simple"/>
          </fig>
          <p>As negative time arguments in <italic>W</italic>(<italic>t</italic>) correspond to the case in which the presynaptic spike (and thus the onset of the resulting EPSP) precedes the postsynaptic spike, the shape of the theoretically derived learning window for physiologically plausible values of <italic>τ</italic> and <italic>ν</italic><sub>max</sub> (<italic>τ</italic> = 1 / <italic>ν</italic><sub>max</sub> = 40 ms; middle row in <xref ref-type="fig" rid="pcbi-0030112-g003">Figure 3</xref>) predicts potentiation of the synapse when a postsynaptic spike is preceded by the onset of an EPSP and depression of the synapse when this temporal order is reversed. This behavior is in agreement with experimental data from neocortex and hippocampus in rats as well as from the optic tectum in <italic>Xenopus</italic> [<xref ref-type="bibr" rid="pcbi-0030112-b014">14</xref>–<xref ref-type="bibr" rid="pcbi-0030112-b018">18</xref>]. To further illustrate this agreement, <xref ref-type="fig" rid="pcbi-0030112-g004">Figure 4</xref> compares the data as published by Bi and Poo [<xref ref-type="bibr" rid="pcbi-0030112-b016">16</xref>] with the learning window resulting from a smoothened power spectrum with the shape of a Cauchy function (<xref ref-type="disp-formula" rid="pcbi-0030112-e035">Equation 35</xref>) instead of <inline-formula id="pcbi-0030112-ex046"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex046" xlink:type="simple"/></inline-formula>
						. As demonstrated above, this corresponds to implementing the slowness principle in form of the trace rule. Interestingly, the resulting learning window has the double-exponential shape that is regularly used in models of STDP (e.g., [<xref ref-type="bibr" rid="pcbi-0030112-b024">24</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b029">29</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b030">30</xref>]). As the absolute scale of the learning window is not determined in our analysis, it was adjusted to facilitate the comparison with the experimental data.
					</p>
          <fig id="pcbi-0030112-g004" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.0030112.g004</object-id>
            <label>Figure 4</label>
            <caption>
              <title>Comparison of the Learning Window with Experimental Data</title>
              <p>The plot compares the theoretically predicted learning window with experimental data from hippocampal pyramidal cells as published by Bi and Poo [<xref ref-type="bibr" rid="pcbi-0030112-b016">16</xref>] (larger plot in the middle). Instead of the ideal power spectrum<inline-formula id="pcbi-0030112-ex050"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex050" xlink:type="simple"/></inline-formula>
								 with the abrupt cutoff at <italic>ν</italic><sub>max</sub> as stated in <xref ref-type="disp-formula" rid="pcbi-0030112-e020">Equation 20</xref>, a Cauchy function with <italic>γ</italic> = 1 / 15 ms was used (top left, the dashed line is<inline-formula id="pcbi-0030112-ex051"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex051" xlink:type="simple"/></inline-formula>
								 for <italic>ν</italic><sub>max</sub> = 1 / (40 ms)). Again, the EPSP decay time was <italic>τ</italic> = 40 ms. This learning window corresponds to an implementation of the “trace rule” [<xref ref-type="bibr" rid="pcbi-0030112-b001">1</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b006">6</xref>] for a decay time of the exponential filter of 15 ms.
							</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0030112.g004" xlink:type="simple"/>
          </fig>
        </sec>
        <sec id="s2b4">
          <title>Interpretation of the learning windows.</title>
          <p>The last section leaves a central question open: why are these learning windows optimal for slowness learning and why does the EPSP play such an important role for the shape of the learning window?</p>
          <p>Let us first discuss the case of the symmetric learning window, that is, the situation in which the EPSP is shorter than the fastest timescale in the input signal. Then, the convolution with the EPSP has practically no effect on the temporal structure of the signal and the output firing rate can be regarded as an instantaneous function of the input rates. We can thus neglect the EPSP altogether. The learning mechanism can then be understood as follows: assume at a given time <italic>t</italic> the postsynaptic firing rate <italic>r</italic><sup>out</sup> is high and causes a postsynaptic spike. Then, the finite width of the learning window leads to potentiation not only of those synapses that participated in initiating the spike but also of those that transmit a spike within a certain time window around the time of the postsynaptic spike. As this leads to an increase of the firing rate within this time window, the learning mechanism tends to equilibrate the firing rates for neighboring times and thus favors temporally slow output signals.</p>
          <p>If the duration of the EPSP is longer than the fastest timescale in the input signal, the output firing rate is no longer an instantaneous function of the input signals but generated by low-pass filtering the signal <italic>a</italic><sup>out</sup> with the EPSP. This affects learning, because the objective of the continuous model is to optimize the slowness of <italic>a</italic><sup>out</sup>, whose temporal structure is now “obscured” by the EPSP. In order to optimize the objective, the system thus has to develop a deconvolution mechanism to reconstruct <italic>a</italic><sup>out</sup>. From this point of view, the learning window has to perform two tasks simultaneously. It has to first perform the deconvolution and then enforce slowness on the resulting signal. This is most easily illustrated by means of the condition in <xref ref-type="disp-formula" rid="pcbi-0030112-e055">Equation 55</xref>. The convolution of the learning window with the EPSP generates the effective learning window <italic>W</italic><sub>0</sub> that is independent of the EPSP and which coincides with the learning window for infinitely short EPSPs. Intuitively, we could solve <xref ref-type="disp-formula" rid="pcbi-0030112-e055">Equation 55</xref> by choosing a learning window that consists of the “inverse” of the EPSP and the EPSP-free learning window <italic>W</italic><sub>0</sub>. An intuitive example is the limiting case of an infinitely long EPSP. The EPSP then corresponds to a Heaviside function and performs an integration, which can be inverted by taking the derivative. Thus, the learning window for long EPSPs is the temporal derivative of the learning window for short EPSPs. The dependence of the required learning window on the shape of the EPSP is thus caused by the need of the learning window to “invert” the EPSP.</p>
          <p>These considerations shed a different light on the shape of physiologically measured learning windows. The antisymmetry of the learning window may not act as a physiological implementation of a causality detector after all, but rather as a mechanism for compensating intrinsic low-pass filters in neuronal processing such as the EPSP. For functional interpretations of STDP, it may be more sensible to consider the convolution of the learning window with the EPSP than the learning window alone.</p>
          <p>It should be noted that, according to our learning rule, the weights adapt in order to make a hypothetical instantaneous output signal <italic>a</italic><sup>out</sup> optimally slow. This does not necessarily imply that the output firing rate <italic>r</italic><sup>out</sup>, which is generated by low-pass filtering <italic>a</italic><sup>out</sup> with the EPSP, is optimally slow. In principle, the system could generate more slowly varying signals by exploiting the temporal structure of the EPSP. However, the motivation for the slowness principle is the idea that the system learns to detect invariances in the <italic>input</italic> signal, and that from this perspective the goal of creating a slowly varying output signal is not an end in itself but a means to learn invariances. Thus, the low-pass filtering effect of the EPSP should not be exploited but ignored or compensated.</p>
        </sec>
        <sec id="s2b5">
          <title>General learning windows and EPSPs.</title>
          <p>Although the asymmetry in LTP/LTD induction observed by Bi and Poo [<xref ref-type="bibr" rid="pcbi-0030112-b016">16</xref>] has also been observed in other studies, the decay times for the LTP and the LTD branches of the learning window appear to be different in other preparations [<xref ref-type="bibr" rid="pcbi-0030112-b018">18</xref>]. One may thus ask how robust our interpretation is with respect to the detailed shape of the learning window. To address this question, we start with some general learning window <italic>W</italic> and EPSP <italic>ε</italic> and ask under which conditions the effective learning window <italic>W</italic><sub>0</sub> = <italic>W</italic> <sub>°</sub> <italic>ε</italic> prefers slowly varying features in the input.</p>
          <p>As a starting point, we use the dynamics of the weights in <xref ref-type="disp-formula" rid="pcbi-0030112-e054">Equation 54</xref> as generated by the input statistics. Using <inline-formula id="pcbi-0030112-ex047"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex047" xlink:type="simple"/></inline-formula>
						 and defining the correlation functions <inline-formula id="pcbi-0030112-ex048"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0030112.ex048" xlink:type="simple"/></inline-formula>
						 yields
						<disp-formula id="pcbi-0030112-e059"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e059" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mrow><mml:mo>&lang;</mml:mo><mml:mi>&Delta;</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&rang;</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='false'><mml:msub><mml:mo>&sum;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:munder><mml:munder><mml:mrow><mml:mrow><mml:mo stretchy="true">&lsqb;</mml:mo><mml:mrow><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:mo>&int;</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">&rsqb;</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy='true'>&underbrace;</mml:mo></mml:munder><mml:mrow><mml:mo>&equals;</mml:mo><mml:mo>:</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>The dynamics thus follows a linear difference equation with a dynamic matrix <italic>A<sub>ij</sub></italic> whose properties are determined by the correlation function <italic>C<sub>ij</sub></italic>(<italic>t</italic>) and the effective learning window <italic>W</italic><sub>0</sub>(<italic>t</italic>). One important question is whether the weights approach a stable fixed-point state or oscillate. In this context, the symmetry properties of <italic>A<sub>ij</sub></italic> and thus those of <italic>C<sub>ij</sub></italic> are crucial. The correlation functions obey the relation
						<disp-formula id="pcbi-0030112-e060"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e060" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>which couples their spatial symmetry (i.e., the symmetry with respect to the indices <italic>i</italic> and <italic>j</italic>) to their temporal symmetry. For instance, if the input statistics are reversible, i.e., for <italic>C<sub>ij</sub></italic>(<italic>t</italic>) = <italic>C<sub>ij</sub></italic>(−<italic>t</italic>), <italic>C<sub>ij</sub></italic> is symmetric in the indices and so is <italic>A<sub>ij</sub></italic>. If the input statistics were “perfectly irreversible,” i.e., <italic>C<sub>ij</sub></italic>(<italic>t</italic>) = − <italic>C<sub>ij</sub></italic>(−<italic>t</italic>), <italic>C<sub>ij</sub></italic> and <italic>A<sub>ij</sub></italic> would be antisymmetric. This motivates the splitting of the correlation functions <italic>C<sub>ij</sub></italic> into a temporally symmetric and an antisymmetric component: <italic>C<sub>ij</sub></italic> = <italic>C<sub>ij</sub><sup>+</sup> + <italic>C<sub>ij</sub><sup>−</sup></italic></italic> with <italic>C<sub>ij</sub><sup>±</sup>(t)</italic> = ±<italic>C<sub>ij</sub><sup>±</sup>(−t)</italic>. In a similar fashion, we split the effective learning window <italic>W</italic><sub>0</sub> = <italic>W</italic><sub>0</sub><sup>+</sup> + <italic>W</italic><sub>0</sub><sup>−</sup>. For symmetry reasons, the dynamical matrix <italic>A<sub>ij</sub></italic> can then be separated into two components
						<disp-formula id="pcbi-0030112-e061"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e061" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&equals;</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:mo>&int;</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>0</mml:mn><mml:mo>&plus;</mml:mo></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>&plus;</mml:mo></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy='true'>&underbrace;</mml:mo></mml:munder><mml:mrow><mml:mo>&equals;</mml:mo><mml:mo>:</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>&plus;</mml:mo></mml:msubsup></mml:mrow></mml:munder><mml:mo>&plus;</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mi>&gamma;</mml:mi><mml:mstyle displaystyle='true'><mml:mrow><mml:mo>&int;</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>0</mml:mn><mml:mo>&minus;</mml:mo></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>&minus;</mml:mo></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy='true'>&underbrace;</mml:mo></mml:munder><mml:mrow><mml:mo>&equals;</mml:mo><mml:mo>:</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>&minus;</mml:mo></mml:msubsup></mml:mrow></mml:munder><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
          <p>Because of the symmetry relation in <xref ref-type="disp-formula" rid="pcbi-0030112-e060">Equation 60</xref>, <italic>A<sub>ij</sub></italic><sup>+</sup> is symmetric in <italic>i</italic> and <italic>j</italic>, while <italic>A<sub>ij</sub></italic><sup>−</sup> is antisymmetric. This shows that the effective learning window <italic>W</italic><sub>0</sub> can be split into two functionally different components. The symmetric component picks up the reversible aspects of the input statistics while the antisymmetric component detects irreversibilities, e.g., possible causal relations within the input data. It is this antisymmetric component of the learning window that has previously been interpreted as a means for sequence learning and predictive coding [<xref ref-type="bibr" rid="pcbi-0030112-b019">19</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b031">31</xref>]. Note that the associated weight update ∑<italic><sub>j</sub>A<sub>ij</sub><sup>−</sup>w<sub>j</sub></italic> is always orthogonal to the weight itself. Thus, irreversibilities in the input data in combination with an antisymmetric learning window work against the development of a stable weight distribution, even if the input statistics are stationary. In particular, weight oscillations on the timescale of learning may occur. For instance, in networks with recurrent connections that learn according to STDP, previous studies have shown that the network tends to develop a state of distributed synchrony [<xref ref-type="bibr" rid="pcbi-0030112-b032">32</xref>] that resembles synfire chains. These activity patterns display a pronounced causal structure, so it would be interesting to check if the synaptic weights that emerge in such a network are stable or show oscillations. It is likely that in this context the model constraints on the weights play an important role. If the weights are limited by hard boundaries as in [<xref ref-type="bibr" rid="pcbi-0030112-b032">32</xref>], they tend to saturate, thereby avoiding oscillatory solutions. In the case of softer weight constraints, e.g., in models of STDP with multiplicative weight-dependence, oscillations may occur.</p>
          <p>If <italic>W</italic><sub>0</sub> is symmetric or if the input statistics are reversible, <italic>C<sub>ij</sub><sup>−</sup></italic> = 0, the dynamical matrix <italic>A<sub>ij</sub></italic> = <italic>A<sub>ij</sub></italic><sup>+</sup> is symmetric. As already seen for the case of the continuous model neuron, the learning dynamics can then be interpreted as a gradient ascent on the objective function
						<disp-formula id="pcbi-0030112-e062"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0030112.e062" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>&Psi;</mml:mi><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle displaystyle='true'><mml:munder><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>&plus;</mml:mo></mml:msubsup><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle displaystyle='true'><mml:mrow><mml:mo>&int;</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>0</mml:mn><mml:mo>&plus;</mml:mo></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:msub><mml:mtext>&scriptP;</mml:mtext><mml:mrow><mml:mtext></mml:mtext><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>&nu;</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>d</mml:mtext><mml:mi>&nu;</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
          <p>As discussed earlier, this objective function can be interpreted as an implementation of the slowness principle if <italic>W</italic><sub>0</sub><sup>+</sup>(ν) is a low-pass filter, i.e., it has a global maximum at zero frequency. This indicates that at least for reversible input statistics the preference of STDP for slow signals may be rather insensitive to details of the learning window.</p>
        </sec>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>Neurons in the central nervous system display a wide range of invariances in their response behavior, examples of which are phase invariance in complex cells in the early visual system [<xref ref-type="bibr" rid="pcbi-0030112-b033">33</xref>], head direction invariance in hippocampal place cells [<xref ref-type="bibr" rid="pcbi-0030112-b034">34</xref>], or more complex invariances in neurons associated with face recognition [<xref ref-type="bibr" rid="pcbi-0030112-b035">35</xref>]. If these invariances are learned, the associated learning rule must somehow reflect a heuristics as to which sensory stimuli are supposed to be categorized as being the same. Objects in our environment are unlikely to change completely from one moment to the next but rather undergo typical transformations. Intuitively, responses of neurons with invariances to these transformations should thus vary more slowly than others. The slowness principle uses this intuition and conjectures that neurons learn these invariances by favoring slowly varying output signals without exploiting low-pass filtering.</p>
      <p>SFA [<xref ref-type="bibr" rid="pcbi-0030112-b010">10</xref>] is one implementation of the slowness principle in that it minimizes the mean square of the temporal derivative of the output signal for a given set of training data. SFA has been used to model a wide range of physiologically observed properties of complex cells in primary visual cortex [<xref ref-type="bibr" rid="pcbi-0030112-b008">8</xref>] as well as translation, rotation, and other invariances in the visual system [<xref ref-type="bibr" rid="pcbi-0030112-b010">10</xref>]. In combination with a sparse coding objective, SFA has also been used to describe the self-organized formation of place cells in the hippocampal formation [<xref ref-type="bibr" rid="pcbi-0030112-b011">11</xref>].</p>
      <p>The algorithm that underlies SFA is rather technical, and it has not yet been examined whether it is feasible to implement SFA within the limitations of neuronal circuitry. In this paper we approach this question analytically and demonstrate that such an implementation is possible in both continuous and spiking model neurons.</p>
      <p>In the first part of the paper, we show that for linear continuous model neurons, the slowest direction in the input signal can be learned by means of Hebbian learning on low-pass filtered versions of the input and the output signal. The power spectrum of the low-pass filter required for implementing SFA can be derived from the learning objective and has the shape of an upside-down parabola.</p>
      <p>The idea of using low-pass filtered signals for invariance learning is a feature that our model has in common with several others [<xref ref-type="bibr" rid="pcbi-0030112-b001">1</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b006">6</xref>]. By means of the continuous model neuron, we have discussed the relation of our model to these “trace rules” and have shown that they bear strong similarities.</p>
      <p>The second part of the paper discusses the modifications that have to be made to adjust the learning rule for a Poisson neuron. We find that in an ensemble-averaged sense it is possible to reproduce the behavior of the continuous model neuron by means of spike-timing–dependent plasticity (STDP). Our study suggests that the outcome of STDP learning is not governed by the learning window alone but rather by the convolution of the learning window with the EPSP, which is of relevance for functional interpretations of STDP.</p>
      <p>The learning window that realizes SFA can be calculated analytically. Its shape is determined by the interplay of the duration of the EPSP and the maximal input frequency <italic>ν</italic><sub>max</sub>, above which the input signals are assumed to have negligible power. If <italic>ν</italic><sub>max</sub> is small, i.e., if the EPSP is sufficiently short to temporally resolve the most quickly varying components of the input data, the learning window is symmetric, whereas for large <italic>ν</italic><sub>max</sub> or long EPSPs, it is antisymmetric. Interestingly, physiologically plausible parameters lead to a learning window whose shape and width is in agreement with experimental findings. Based on this result, we propose a new functional interpretation of the STDP learning window as an implementation of the slowness principle that compensates for neuronal low-pass filters such as the EPSP.</p>
      <p>An important question in this context is on which timescales is this interpretation valid. It is conceivable that for signals that vary on a timescale of less than a hundred milliseconds, a learning window with a width of tens of milliseconds can distinguish slower from faster signals. STDP could thus be sufficient to establish invariant representations in early sensory processing, e.g., visual receptive fields that become invariant to microsaccades inducing small translations. Although it is unlikely that STDP alone can distinguish between signals that vary on behavioral timescales of hundreds of milliseconds or even seconds, this may not be problematic, because it is probably not sensible to order <italic>all</italic> aspects of the stimuli according to how quickly they vary. Rather, one should distinguish input components that vary so quickly that they are unlikely to be behaviorally relevant from those that vary on behavioral timescales. From this perspective, the intrinsic timescale of the learning rule should be such that its discriminative power is best on a timescale where this transition occurs. It is conceivable that this transition timescale lies on the order of several tens of milliseconds. The learning of high level invariances that correspond to behavioral timescales will probably require additional mechanisms with corresponding intrinsic timescales, e.g., sustained firing in response to a stimulus [<xref ref-type="bibr" rid="pcbi-0030112-b036">36</xref>].</p>
      <p>For general learning windows and EPSPs, the convolution of the learning window with the EPSP can be split into a symmetric component and an antisymmetric component. The symmetric component picks up reversible aspects of the input statistics while the antisymmetric component detects irreversible aspects. Previous functional interpretations of STDP have mostly concentrated on the antisymmetric component, which has been interpreted, e.g., as a mechanism for sequence learning or predictive coding [<xref ref-type="bibr" rid="pcbi-0030112-b019">19</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b031">31</xref>] or for reducing recurrent connectivity in favor of feed-forward structures [<xref ref-type="bibr" rid="pcbi-0030112-b030">30</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b032">32</xref>]. Other studies have neglected the phase structure of the learning window altogether and concentrated on its power spectrum, proposing that timing-dependent plasticity performs Hebbian learning on an optimal estimate of the input signals in the presence of noise [<xref ref-type="bibr" rid="pcbi-0030112-b037">37</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b038">38</xref>]. Note that these interpretations are not necessarily contradictory to ours, because the slowness interpretation relies on the symmetric component of the learning window only and thus on the reversible aspect of the input statistics. These considerations indicate that depending on the temporal structure of the input, STDP may have different functional roles.</p>
      <p>A different approach to unsupervised learning of invariances with a biologically realistic model neuron has been taken by Körding and König [<xref ref-type="bibr" rid="pcbi-0030112-b039">39</xref>]. In their model, bursts of backpropagating spikes gate synaptic plasticity by providing sufficient amounts of dendritic depolarization. These bursts are assumed to be triggered by lateral connections that evoke calcium spikes in the apical dendrites of cortical pyramidal cells.</p>
      <p>Of course the model presented here is not a complete implementation of SFA. We have only considered the central step of SFA, the extraction of the most slowly varying direction from a set of whitened input signals. To implement the full algorithm, additional steps are necessary: a nonlinear expansion of the input space, the whitening of the expanded input signals, and a means of normalizing the weights. When traversing the dendritic arborizations of a postsynaptic neuron, axons often make more than one synaptic contact. As different input channels may be subjected to different nonlinearities in the dendritic tree (cf. [<xref ref-type="bibr" rid="pcbi-0030112-b040">40</xref>]), the postsynaptic neuron may have access to several nonlinearly transformed versions of the same presynaptic signals. Conceptually, this resembles a nonlinear expansion of the input signals. However, it is not obvious how these signals could be whitened within the dendrite. On the network level, however, whitening could be achieved by adaptive recurrent inhibition between the neurons [<xref ref-type="bibr" rid="pcbi-0030112-b041">41</xref>]. This mechanism may also be suitable for extracting several slow uncorrelated signals as required in the original formulation of SFA [<xref ref-type="bibr" rid="pcbi-0030112-b010">10</xref>] instead of just one. We assumed an explicit weight normalization in the description of our model. However, one could also use a modified learning rule that implicitly normalizes the weight vector as long as it extracts the signal with the largest variance. A possible biological mechanism is synaptic scaling [<xref ref-type="bibr" rid="pcbi-0030112-b025">25</xref>], which is believed to multiplicatively rescale all synaptic weights according to postsynaptic activity, similar to Oja's rule [<xref ref-type="bibr" rid="pcbi-0030112-b026">26</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b042">42</xref>]. Thus, it appears that most of the mechanisms necessary for an implementation of the full SFA algorithm are available, but that it is not yet clear how to combine them in a biologically plausible way.</p>
      <p>Another critical point in the analytical derivation for the spiking model is the replacement of the temporal by the ensemble average, as this allows recovery of the rates that underlie the Poisson processes. The validity of the analytical results thus requires some kind of ergodicity in the training data, a condition which of course needs to be justified for the specific input data at hand.</p>
      <p>It is still open whether the results presented here can be reproduced with more realistic model neurons. The spiking model neuron used here was simplified in that it had a linear relationship between input and output firing rate. In many real neurons, highly nonlinear behavior was observed. Interestingly, Hebbian learning for nonlinear rate-based neurons has previously been associated with the detection of higher-order moments of the input statistics [<xref ref-type="bibr" rid="pcbi-0030112-b043">43</xref>], thereby providing a mechanism for extracting statistically independent components of the input signal. Because for sparse input statistics independent component analysis is closely related to sparse coding [<xref ref-type="bibr" rid="pcbi-0030112-b044">44</xref>], it is tempting to speculate that within a rate picture, temporally nonlocal plasticity with a nonlinear input–output relation implements a combination of sparseness and slowness. Learning paradigms that combine these two objectives are thus an interesting field for further studies [<xref ref-type="bibr" rid="pcbi-0030112-b011">11</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b045">45</xref>].</p>
      <p>Another nonlinearity that we have neglected is the frequency- and weight-dependence of STDP [<xref ref-type="bibr" rid="pcbi-0030112-b016">16</xref>,<xref ref-type="bibr" rid="pcbi-0030112-b046">46</xref>]. Additional work will be needed to examine how these interfere with the proposed functional role of STDP. Furthermore, modeling the spiking mechanism of a neuron by an inhomogeneous Poisson process is also a severe simplification that ignores basic phenomena of spike generation in biological neurons such as refractoriness and thresholding. It is not clear how these characteristics would change the learning rule that leads to an implementation of the slowness principle. It seems to be a very difficult task to answer these questions analytically. Simulations will be necessary to verify the results derived here and to analyze which changes appear and which adaptations must be made in a more realistic model of neural information processing.</p>
      <p>In summary, the analytical considerations presented here show that (i) slowness can be equivalently achieved by minimizing the variance of the time derivative signal or by maximizing the variance of the low-pass filtered signal, the latter of which can be achieved by standard Hebbian learning on the low-pass filtered input and output signals; (ii) the difference between SFA and the trace learning rule lies in the exact shape of the effective low-pass filter—for most practical purposes the results are probably equivalent; (iii) for a spiking Poisson model neuron with an STDP learning rule, it is not the learning window that governs the weight dynamics but the convolution of the learning window with the EPSP; (iv) the STDP learning window that implements the slowness objective is in good agreement with learning windows found experimentally. With these results, we have reduced the gap between slowness as an abstract learning principle and biologically plausible STDP learning rules, and we offer a completely new interpretation of the standard STDP learning window.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <p>The methods employed in this paper rely on standard mathematical techniques as commonly used in the theory of synaptic plasticity (see, e.g., [<xref ref-type="bibr" rid="pcbi-0030112-b047">47</xref>]).</p>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank Christian Leibold and Richard Kempter for helpful discussion. We also thank the reviewers for helping to improve the manuscript.</p>
    </ack>
    
    <glossary>
      <title>Abbreviations</title>
      <def-list>
        <def-item>
          <term>EPSP</term>
          <def>
            <p>excitatory postsynaptic potential</p>
          </def>
        </def-item>
        <def-item>
          <term>LTD</term>
          <def>
            <p>long-term depression</p>
          </def>
        </def-item>
        <def-item>
          <term>LTP</term>
          <def>
            <p>long-term potentiation</p>
          </def>
        </def-item>
        <def-item>
          <term>SFA</term>
          <def>
            <p>slow feature analysis</p>
          </def>
        </def-item>
        <def-item>
          <term>STDP</term>
          <def>
            <p>spike-timing–dependent plasticity</p>
          </def>
        </def-item>
      </def-list>
    </glossary>
    <ref-list>
      <title>References</title>
      <ref id="pcbi-0030112-b001">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Földiak</surname><given-names>P</given-names></name></person-group>
					<year>1991</year>
					<article-title>Learning invariance from transformation sequences.</article-title>
					<source>Neural Comput</source>
					<volume>3</volume>
					<fpage>194</fpage>
					<lpage>200</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b002">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Mitchison</surname><given-names>G</given-names></name></person-group>
					<year>1991</year>
					<article-title>Removing time variation with the anti-Hebbian differential synapse.</article-title>
					<source>Neural Comput</source>
					<volume>3</volume>
					<fpage>312</fpage>
					<lpage>320</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b003">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Becker</surname><given-names>S</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name></person-group>
					<year>1992</year>
					<article-title>A self-organizing neural network that discovers surfaces in random-dot stereograms.</article-title>
					<source>Nature</source>
					<volume>355</volume>
					<fpage>161</fpage>
					<lpage>163</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b004">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>O'Reilly</surname><given-names>RC</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>MH</given-names></name></person-group>
					<year>1994</year>
					<article-title>Object recognition and sensitive periods: A computational analysis of visual imprinting.</article-title>
					<source>Neural Comput</source>
					<volume>6</volume>
					<fpage>357</fpage>
					<lpage>389</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b005">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Stone</surname><given-names>JV</given-names></name><name name-style="western"><surname>Bray</surname><given-names>A</given-names></name></person-group>
					<year>1995</year>
					<article-title>A learning rule for extracting spatio–temporal invariances.</article-title>
					<source>Network: Comput Neural Sys</source>
					<volume>6</volume>
					<fpage>429</fpage>
					<lpage>436</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b006">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wallis</surname><given-names>G</given-names></name><name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name></person-group>
					<year>1997</year>
					<article-title>Invariant face and object recognition in the visual system.</article-title>
					<source>Prog Neurobiol</source>
					<volume>51</volume>
					<fpage>167</fpage>
					<lpage>194</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b007">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>HC</given-names></name><name name-style="western"><surname>Sha</surname><given-names>LF</given-names></name><name name-style="western"><surname>Gan</surname><given-names>Q</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y</given-names></name></person-group>
					<year>1998</year>
					<article-title>Energy function for learning invariance in multilayer perceptron.</article-title>
					<source>Electronics Lett</source>
					<volume>34</volume>
					<fpage>292</fpage>
					<lpage>294</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b008">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name><name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name></person-group>
					<year>2005</year>
					<article-title>Slow feature analysis yields a rich repertoire of complex cells.</article-title>
					<source>J Vis</source>
					<volume>5</volume>
					<fpage>579</fpage>
					<lpage>602</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b009">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Körding</surname><given-names>KP</given-names></name><name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name><name name-style="western"><surname>Einhäuser</surname><given-names>W</given-names></name><name name-style="western"><surname>König</surname><given-names>P</given-names></name></person-group>
					<year>2004</year>
					<article-title>How are complex cell properties adapted to the statistics of natural stimuli?</article-title>
					<source>J Neurophysiol</source>
					<volume>91</volume>
					<fpage>206</fpage>
					<lpage>212</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b010">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name></person-group>
					<year>2002</year>
					<article-title>Slow feature analysis: Unsupervised learning of invariances.</article-title>
					<source>Neural Comput</source>
					<volume>14</volume>
					<fpage>715</fpage>
					<lpage>770</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b011">
        <label>11</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Franzius</surname><given-names>M</given-names></name><name name-style="western"><surname>Sprekeler</surname><given-names>H</given-names></name><name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name></person-group>
					<year>2007</year>
					<article-title>Unsupervised learning of place cells, head direction cells, and spatial-view cells with slow feature analysis on quasi-natural videos.</article-title>
					<source>Cognitive Sciences EPrint Archive (CogPrints) 5492</source>
        	<comment>Available: <ext-link ext-link-type="uri" xlink:href="http://cogprints.org/5492/" xlink:type="simple">http://cogprints.org/5492/</ext-link>. Accessed 4 June 2007.</comment>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b012">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wyss</surname><given-names>R</given-names></name><name name-style="western"><surname>König</surname><given-names>P</given-names></name><name name-style="western"><surname>Verschure</surname><given-names>PFMJ</given-names></name></person-group>
					<year>2006</year>
					<article-title>A model of the ventral visual system based on temporal stability and local memory.</article-title>
					<source>PLoS Biol</source>
					<volume>4</volume>
					<fpage>e120</fpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b013">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Kempter</surname><given-names>R</given-names></name><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name><name name-style="western"><surname>van Hemmen</surname><given-names>JL</given-names></name></person-group>
					<year>1999</year>
					<article-title>Hebbian learning and spiking neurons.</article-title>
					<source>Phys Rev E</source>
					<volume>59</volume>
					<fpage>4498</fpage>
					<lpage>4514</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b014">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Debanne</surname><given-names>D</given-names></name><name name-style="western"><surname>Gähwiler</surname><given-names>BH</given-names></name><name name-style="western"><surname>Thomson</surname><given-names>SM</given-names></name></person-group>
					<year>1994</year>
					<article-title>Asynchronous pre- and postsynaptic activity induces associative long-term depression in area CA1 of the rat hippocampus.</article-title>
					<source>Proc Natl Acad Sci U S A</source>
					<volume>91</volume>
					<fpage>1148</fpage>
					<lpage>1152</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b015">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name><name name-style="western"><surname>Lübke</surname><given-names>J</given-names></name><name name-style="western"><surname>Frotscher</surname><given-names>M</given-names></name><name name-style="western"><surname>Sakmann</surname><given-names>B</given-names></name></person-group>
					<year>1997</year>
					<article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs.</article-title>
					<source>Science</source>
					<volume>275</volume>
					<fpage>213</fpage>
					<lpage>215</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b016">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bi</surname><given-names>G-q</given-names></name><name name-style="western"><surname>Poo</surname><given-names>M-m</given-names></name></person-group>
					<year>1998</year>
					<article-title>Synaptic modifications in cultured hippocampal neurons: Dependence on spike timing, synaptic strength, and postsynaptic cell type.</article-title>
					<source>J Neurosci</source>
					<volume>18</volume>
					<fpage>10464</fpage>
					<lpage>10472</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b017">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>LI</given-names></name><name name-style="western"><surname>Tao</surname><given-names>HW</given-names></name><name name-style="western"><surname>Holt</surname><given-names>CE</given-names></name><name name-style="western"><surname>Harris</surname><given-names>WA</given-names></name><name name-style="western"><surname>Poo</surname><given-names>M-m</given-names></name></person-group>
					<year>1998</year>
					<article-title>A critical window for cooperation and competition among developing retinotectal synapses.</article-title>
					<source>Nature</source>
					<volume>395</volume>
					<fpage>37</fpage>
					<lpage>44</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b018">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Feldman</surname><given-names>DE</given-names></name></person-group>
					<year>2000</year>
					<article-title>Timing-based LTP and LTD at vertical input to layer II/III pyramidal cells in rat barrel cortex.</article-title>
					<source>Neuron</source>
					<volume>27</volume>
					<fpage>45</fpage>
					<lpage>56</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b019">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name><name name-style="western"><surname>Blum</surname><given-names>KI</given-names></name></person-group>
					<year>1996</year>
					<article-title>Functional significance of long-term potentiation for sequence learning and prediction.</article-title>
					<source>Cereb Cortex</source>
					<volume>6</volume>
					<fpage>406</fpage>
					<lpage>416</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b020">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name><name name-style="western"><surname>Kempter</surname><given-names>R</given-names></name><name name-style="western"><surname>van Hemmen</surname><given-names>JL</given-names></name><name name-style="western"><surname>Wagner</surname><given-names>H</given-names></name></person-group>
					<year>1996</year>
					<article-title>A neuronal learning rule for sub-millisecond temporal coding.</article-title>
					<source>Nature</source>
					<volume>383</volume>
					<fpage>76</fpage>
					<lpage>78</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b021">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Kepecs</surname><given-names>A</given-names></name><name name-style="western"><surname>van Rossum</surname><given-names>MCW</given-names></name><name name-style="western"><surname>Song</surname><given-names>S</given-names></name><name name-style="western"><surname>Tegner</surname><given-names>J</given-names></name></person-group>
					<year>2002</year>
					<article-title>Spike-timing–dependent plasticity: Common themes and divergent vistas.</article-title>
					<source>Biol Cybern</source>
					<volume>87</volume>
					<fpage>446</fpage>
					<lpage>458</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b022">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Kistler</surname><given-names>WM</given-names></name><name name-style="western"><surname>van Hemmen</surname><given-names>JL</given-names></name></person-group>
					<year>2000</year>
					<article-title>Modeling synaptic plasticity in conjunction with the timing of pre- and postsynaptic action potentials.</article-title>
					<source>Neural Comput</source>
					<volume>12</volume>
					<fpage>385</fpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b023">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Rubin</surname><given-names>J</given-names></name><name name-style="western"><surname>Lee</surname><given-names>DD</given-names></name><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group>
					<year>2001</year>
					<article-title>Equilibrium properties of temporally asymmetric Hebbian learning.</article-title>
					<source>Phys Rev Lett</source>
					<volume>86</volume>
					<fpage>364</fpage>
					<lpage>367</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b024">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gütig</surname><given-names>R</given-names></name><name name-style="western"><surname>Aharonov</surname><given-names>S</given-names></name><name name-style="western"><surname>Rotter</surname><given-names>S</given-names></name><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group>
					<year>2003</year>
					<article-title>Learning input correlations through nonlinear temporally asymmetric Hebbian plasticity.</article-title>
					<source>J Neurosci</source>
					<volume>23</volume>
					<fpage>3697</fpage>
					<lpage>3714</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b025">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name><name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name></person-group>
					<year>2000</year>
					<article-title>Hebb and homeostasis in neuronal plasticity.</article-title>
					<source>Curr Opin Neurobiol</source>
					<volume>10</volume>
					<fpage>358</fpage>
					<lpage>364</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b026">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name><name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name></person-group>
					<year>2000</year>
					<article-title>Synaptic plasticity: Taming the beast.</article-title>
					<source>Nat Neurosci</source>
					<volume>3</volume>
					<fpage>1178</fpage>
					<lpage>1183</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b027">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Kempter</surname><given-names>R</given-names></name><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name><name name-style="western"><surname>van Hemmen</surname><given-names>JL</given-names></name></person-group>
					<year>2001</year>
					<article-title>Intrinsic stabilization of output rates by spike-based Hebbian learning.</article-title>
					<source>Neural Comput</source>
					<volume>13</volume>
					<fpage>2709</fpage>
					<lpage>2741</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b028">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name><name name-style="western"><surname>Rapp</surname><given-names>M</given-names></name><name name-style="western"><surname>Segev</surname><given-names>I</given-names></name></person-group>
					<year>1996</year>
					<article-title>A brief history of time (constants).</article-title>
					<source>Cereb Cortex</source>
					<volume>6</volume>
					<fpage>92</fpage>
					<lpage>101</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b029">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>van Rossum</surname><given-names>MCW</given-names></name><name name-style="western"><surname>Bi</surname><given-names>G-q</given-names></name><name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name></person-group>
					<year>2000</year>
					<article-title>Stable Hebbian learning from spike-timing–dependent plasticity.</article-title>
					<source>J Neurosci</source>
					<volume>20</volume>
					<fpage>8812</fpage>
					<lpage>8821</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b030">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>S</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>
					<year>2001</year>
					<article-title>Cortical mapping and development through spike-timing–dependent plasticity.</article-title>
					<source>Neuron</source>
					<volume>32</volume>
					<fpage>339</fpage>
					<lpage>350</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b031">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>
					<year>2001</year>
					<article-title>Spike-timing–dependent Hebbian plasticity as temporal difference learning.</article-title>
					<source>Neural Comput</source>
					<volume>13</volume>
					<fpage>2221</fpage>
					<lpage>2238</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b032">
        <label>32</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Horn</surname><given-names>D</given-names></name><name name-style="western"><surname>Levy</surname><given-names>N</given-names></name><name name-style="western"><surname>Meilijson</surname><given-names>I</given-names></name><name name-style="western"><surname>Ruppin</surname><given-names>E</given-names></name></person-group>
					<year>2000</year>
					<article-title>Distributed synchrony of spiking neurons in a Hebbian cell assembly.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Müller</surname><given-names>K-R</given-names></name></person-group>
					<source>Adv Neural Info Process Syst (NIPS) 12</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b033">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hubel</surname><given-names>D</given-names></name><name name-style="western"><surname>Wiesel</surname><given-names>T</given-names></name></person-group>
					<year>1968</year>
					<article-title>Receptive fields and functional architecture of monkey striate cortex.</article-title>
					<source>J Physiol (Lond)</source>
					<volume>195</volume>
					<fpage>215</fpage>
					<lpage>243</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b034">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Muller</surname><given-names>R</given-names></name><name name-style="western"><surname>Bostock</surname><given-names>E</given-names></name><name name-style="western"><surname>Taube</surname><given-names>JS</given-names></name><name name-style="western"><surname>Kubie</surname><given-names>JL</given-names></name></person-group>
					<year>1994</year>
					<article-title>On the directional firing properties of hippocampal place cells.</article-title>
					<source>J Neurosci</source>
					<volume>14</volume>
					<fpage>7235</fpage>
					<lpage>7251</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b035">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Quiroga</surname><given-names>RQ</given-names></name><name name-style="western"><surname>Reddy</surname><given-names>L</given-names></name><name name-style="western"><surname>Kreiman</surname><given-names>G</given-names></name><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name><name name-style="western"><surname>Fried</surname><given-names>I</given-names></name></person-group>
					<year>2005</year>
					<article-title>Invariant visual representation by single neurons in the human brain.</article-title>
					<source>Nature</source>
					<volume>435</volume>
					<fpage>1102</fpage>
					<lpage>1107</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b036">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Drew</surname><given-names>PJ</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>
					<year>2006</year>
					<article-title>Extending the effects of spike-timing–dependent plasticity to behavioral timescales.</article-title>
					<source>Proc Natl Acad Sci U S A</source>
					<volume>103</volume>
					<fpage>8876</fpage>
					<lpage>8881</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b037">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wallis</surname><given-names>G</given-names></name><name name-style="western"><surname>Baddeley</surname><given-names>R</given-names></name></person-group>
					<year>1997</year>
					<article-title>Optimal, unsupervised learning in invariant object recognition.</article-title>
					<source>Neural Comput</source>
					<volume>9</volume>
					<fpage>883</fpage>
					<lpage>894</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b038">
        <label>38</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name><name name-style="western"><surname>Häusser</surname><given-names>M</given-names></name><name name-style="western"><surname>London</surname><given-names>M</given-names></name></person-group>
					<year>2004</year>
					<article-title>Plasticity kernels and temporal statistics.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name></person-group>
					<source>Adv Neural Info Process Syst (NIPS) 16</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b039">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Körding</surname><given-names>KP</given-names></name><name name-style="western"><surname>König</surname><given-names>P</given-names></name></person-group>
					<year>2001</year>
					<article-title>Neurons with two sites of synaptic integration learn invariant representations.</article-title>
					<source>Neural Comput</source>
					<volume>13</volume>
					<fpage>2823</fpage>
					<lpage>2849</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b040">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>London</surname><given-names>M</given-names></name><name name-style="western"><surname>Häusser</surname><given-names>M</given-names></name></person-group>
					<year>2005</year>
					<article-title>Dendritic computation.</article-title>
					<source>Ann Rev Neurosci</source>
					<volume>28</volume>
					<fpage>503</fpage>
					<lpage>532</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b041">
        <label>41</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Barlow</surname><given-names>H</given-names></name><name name-style="western"><surname>Földiak</surname><given-names>P</given-names></name></person-group>
					<year>1989</year>
					<article-title>Adaptation and decorrelation in the cortex.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Durbin</surname><given-names>R</given-names></name><name name-style="western"><surname>Miall</surname><given-names>C</given-names></name><name name-style="western"><surname>Mitchison</surname><given-names>G</given-names></name></person-group>
					<source>Computing neuron</source>
					<publisher-loc>New York</publisher-loc>
					<publisher-name>Addison-Wesley</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">43</size>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b042">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Oja</surname><given-names>E</given-names></name></person-group>
					<year>1982</year>
					<article-title>A simplified neuron as a principal component analyzer.</article-title>
					<source>J Math Biol</source>
					<volume>15</volume>
					<fpage>267</fpage>
					<lpage>273</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b043">
        <label>43</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Oja</surname><given-names>E</given-names></name><name name-style="western"><surname>Karhunen</surname><given-names>J</given-names></name></person-group>
					<year>1995</year>
					<article-title>Signal separation by nonlinear Hebbian learning.</article-title>
					<source>Computational intelligence: A dynamic system perspective</source>
					<fpage>83</fpage>
					<lpage>97</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b044">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name><name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name></person-group>
					<year>1997</year>
					<article-title>Sparse coding with an overcomplete basis set: A strategy employed by V1?</article-title>
					<source>Vision Res</source>
					<volume>37</volume>
					<fpage>3311</fpage>
					<lpage>3325</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b045">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Blaschke</surname><given-names>T</given-names></name><name name-style="western"><surname>Zito</surname><given-names>T</given-names></name><name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name></person-group>
					<year>2007</year>
					<article-title>Independent slow feature analysis and nonlinear blind source separation.</article-title>
					<source>Neural Comput</source>
					<volume>19</volume>
					<fpage>994</fpage>
					<lpage>1021</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b046">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sjöström</surname><given-names>PJ</given-names></name><name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name><name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name></person-group>
					<year>2001</year>
					<article-title>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity.</article-title>
					<source>Neuron</source>
					<volume>32</volume>
					<fpage>1149</fpage>
					<lpage>1164</lpage>
				</element-citation>
      </ref>
      <ref id="pcbi-0030112-b047">
        <label>47</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name><name name-style="western"><surname>Kistler</surname><given-names>WM</given-names></name></person-group>
					<year>2002</year>
					<source>Spiking model neurons</source>
					<publisher-name>Cambridge University Press</publisher-name>
				</element-citation>
      </ref>
    </ref-list>
  </back>
</article>