<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-00939</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002280</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Computational biology</subject>
          </subj-group>
          <subj-group>
            <subject>Neuroscience</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Probability theory</subject>
          </subj-group>
          <subj-group>
            <subject>Statistics</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Social and behavioral sciences</subject>
          <subj-group>
            <subject>Psychology</subject>
            <subj-group>
              <subject>Experimental psychology</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Computational Biology</subject>
          <subject>Neuroscience</subject>
          <subject>Mathematics</subject>
        </subj-group>
      </article-categories><title-group><article-title>Optimizing Experimental Design for Comparing Models of Brain Function</article-title><alt-title alt-title-type="running-head">Optimal Design for Model Comparison</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Daunizeau</surname>
            <given-names>Jean</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Preuschoff</surname>
            <given-names>Kerstin</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Friston</surname>
            <given-names>Karl</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Stephan</surname>
            <given-names>Klaas</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Wellcome Trust Centre for Neuroimaging, University College of London, London, United Kingdom</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Laboratory for Social and Neural Systems Research, Department of Economics, University of Zurich, Zurich, Switzerland</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">jean.daunizeau@gmail.com</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: JD KP KS. Performed the experiments: JD KP. Analyzed the data: JD. Contributed reagents/materials/analysis tools: JD. Wrote the paper: JD KF KS. Revised the Manuscript: KP KF KS.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>11</month>
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>17</day>
        <month>11</month>
        <year>2011</year>
      </pub-date><volume>7</volume><issue>11</issue><elocation-id>e1002280</elocation-id><history>
        <date date-type="received">
          <day>25</day>
          <month>6</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>5</day>
          <month>10</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Daunizeau et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>This article presents the first attempt to formalize the optimization of experimental design with the aim of comparing models of brain function based on neuroimaging data. We demonstrate our approach in the context of Dynamic Causal Modelling (DCM), which relates experimental manipulations to observed network dynamics (via hidden neuronal states) and provides an inference framework for selecting among candidate models. Here, we show how to optimize the sensitivity of model selection by choosing among experimental designs according to their respective model selection accuracy. Using Bayesian decision theory, we (i) derive the <italic>Laplace-Chernoff risk</italic> for model selection, (ii) disclose its relationship with classical design optimality criteria and (iii) assess its sensitivity to basic modelling assumptions. We then evaluate the approach when identifying brain networks using DCM. Monte-Carlo simulations and empirical analyses of fMRI data from a simple bimanual motor task in humans serve to demonstrate the relationship between network identification and the optimal experimental design. For example, we show that deciding whether there is a feedback connection requires shorter epoch durations, relative to asking whether there is experimentally induced change in a connection that is known to be present. Finally, we discuss limitations and potential extensions of this work.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>During the past two decades, brain mapping research has undergone a paradigm switch. In addition to localizing brain regions that encode specific sensory, motor or cognitive processes, neuroimaging data is nowadays further exploited to ask questions about how information is transmitted through brain networks. The ambition here is to ask questions such as: “what is the nature of the information that region A passes on to region B”. This can be experimentally addressed by, e.g., showing that the influence that A exerts onto B depends upon specific sensory, motor or cognitive manipulations. This means one has to compare (in a statistical sense) candidate network models of the brain (with different modulations of effective connectivity, say), based on experimental data. The question we address here is how one should design the experiment in order to best discriminate such candidate models. We approach the problem from a statistical decision theoretical perspective, whereby the optimal design is the one that minimizes the model selection error rate. We demonstrate the approach using simulated and empirical data and show how it can be applied to any experimental question that can be framed as a model comparison problem.</p>
      </abstract><funding-group><funding-statement>This work was funded by the Wellcome Trust (KJF), SystemsX.ch (JD, KES) and NCCR “Neural Plasticity” (KES). The authors also gratefully acknowledge support by the University Research Priority Program “Foundations of Human Social Behaviour” at the University of Zurich (JD, KES). Relevant URLs are given below: SystemsX.ch: <ext-link ext-link-type="uri" xlink:href="http://www.systemsx.ch/projects/systemsxch-projects/research-technology-and-development-projects-rtd/neurochoice/" xlink:type="simple">http://www.systemsx.ch/projects/systemsxch-projects/research-technology-and-development-projects-rtd/neurochoice/</ext-link>; NCCR: “Neural Plasticity”: <ext-link ext-link-type="uri" xlink:href="http://www.nccr-neuro.ethz.ch/" xlink:type="simple">http://www.nccr-neuro.ethz.ch/</ext-link>; University Research Priority Program “Foundations of Human Social Behaviour” at the University of Zurich: <ext-link ext-link-type="uri" xlink:href="http://www.socialbehavior.uzh.ch/index.html" xlink:type="simple">http://www.socialbehavior.uzh.ch/index.html</ext-link>. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="18"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>The history of causal modeling of fMRI data in terms of effective connectivity began in the mid-1990's and has unfolded in two major phases (for reviews, see <xref ref-type="bibr" rid="pcbi.1002280-Friston1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1002280-Stephan1">[2]</xref>). The first phase addressed the <italic>optimization of connectivity estimates</italic>. This involved optimising methods that exploited the information contained in fMRI time series and dealt with confounds such as inter-regional variability in hemodynamic responses. In this development, the community progressed from using methods originally developed for other types of data (such as structural equation modeling; <xref ref-type="bibr" rid="pcbi.1002280-McIntosh1">[3]</xref>) to dynamic causal models, specifically tailored to fMRI <xref ref-type="bibr" rid="pcbi.1002280-Friston2">[4]</xref>. The second phase concerned <italic>optimization of model structure</italic>, introducing Bayesian model selection methods to neuroimaging that are increasingly frequently used for selecting among competing models <xref ref-type="bibr" rid="pcbi.1002280-Penny1">[5]</xref>. This paper goes beyond this and hopes to contribute to the initiation of a third phase. It describes a method for selecting experimental design parameters to minimize the model selection error rate, when comparing candidate models of fMRI data. This is the first attempt to formalize the <italic>optimization of experimental design</italic> for studying brain connectivity with functional neuroimaging data.</p>
      <p>This paper describes a general framework for design optimization. Although we examine design optimization in the specific context of inferring effective connectivity and network structure from fMRI data, it should be noted that the approach is very general and not limited to any data acquisition technique, nor to any particular generative model. In brief, it can be used whenever one wishes to optimize experimental design for studying empirical responses by means of generative models.</p>
      <p>To date, statistical approaches to experimental design for fMRI studies have focused on the problem of detecting regionally specific effects of experimental (e.g., cognitive, sensory or motor) manipulations <xref ref-type="bibr" rid="pcbi.1002280-Josephs1">[6]</xref>–<xref ref-type="bibr" rid="pcbi.1002280-Henson1">[10]</xref>. This addresses the traditional question of <italic>functional specialization</italic> of individual areas for processing components of interest <xref ref-type="bibr" rid="pcbi.1002280-Friston3">[11]</xref>. The associated statistical procedure involves testing for the significance of contrasts of effects of interest, encoded by regressors in the design matrix of a general linear model (GLM). The established approach to fMRI experimental design thus proceeds by extremising the experimental variance in summary statistics (e.g., GLM parameters estimates) at the subject level. This is typically done under (non statistical) constraints, such as psychological validity or experimental feasibility (see, e.g., <xref ref-type="bibr" rid="pcbi.1002280-Wager1">[12]</xref>).</p>
      <p>However, no attempt has been made so far to optimise experimental designs in relation to <italic>functional integration</italic>, i.e. the information transfer among activated brain regions. Here, the challenge is to identify context-dependent interactions among spatially segregated areas <xref ref-type="bibr" rid="pcbi.1002280-McIntosh2">[13]</xref>. The key notion in this context is that optimizing the experimental design requires both a quantitative model that relates the experimental manipulation to observed network dynamics and a formal statistical framework for deciding, for example, whether or not a specific manipulation modulated some connection within the network (see <xref ref-type="fig" rid="pcbi-1002280-g001">Figure 1</xref>).</p>
      <fig id="pcbi-1002280-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>The DCM cycle.</title>
          <p>The DCM cycle summarizes the interaction between modelling, experimental work and statistical data analysis. One starts with new competing hypotheses about a neural system of interest. These are then embodied into a set of candidate DCMs that are to be compared with each other given empirical data. One then designs an experiment that is maximally discriminative with respect to the candidate DCMs. This is the critical step addressed in this article. Data acquisition and analysis then proceed, the conclusion of which serves to generate a new set of competing hypotheses, etc…</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g001" xlink:type="simple"/>
      </fig>
      <p>Dynamic Causal Modelling (DCM) was developed to exploit biophysical quantitative knowledge in order to assess the context-specific effects of an experimental manipulation on brain dynamics and connectivity <xref ref-type="bibr" rid="pcbi.1002280-Friston2">[4]</xref>. Typically, DCM relies upon Bayesian model comparison to identify the most likely network structure subtending observed fMRI time series within regions of interest. We refer the interested reader to <xref ref-type="bibr" rid="pcbi.1002280-Daunizeau1">[14]</xref> for a critical review on the biophysical and statistical foundations of the DCM framework. At present, DCM is the most suitable framework within which to address the problem of optimizing the experimental design to infer on brain network structure. This is because it is based upon a generative model that describes how experimental manipulations induce changes in hidden neuronal states that cause the observed measurements. This is in contrast to other network models based on functional connectivity that simply characterise the surface structure or statistical dependencies among observed responses <xref ref-type="bibr" rid="pcbi.1002280-Smith1">[15]</xref>.</p>
      <p>In this paper, we argue that one should choose among experimental designs according to their induced model selection error rate and demonstrate that this can be done by deriving an information theoretic measure of discriminability between models. We first derive and evaluate the <italic>Laplace-Chernoff risk</italic>, both in terms of how it relates to known optimality measures and in terms of its sensitivity to basic modelling choices. The ensuing framework is very general and can be used for any experimental application that rests upon Bayesian model comparison. We then use both numerical simulations and empirical fMRI data to assess standard design parameters (e.g., epoch duration or site of transcranial magnetic stimulation). In brief, we formalize the intuitive notion that the best design depends on the specific question of interest. <italic>En passant</italic>, we also identify the data features that inform inference about network structure. Finally, we discuss the limitations and potential extensions of the method.</p>
    </sec>
    <sec id="s2" sec-type="methods">
      <title>Methods</title>
      <p>Bayesian model selection is a powerful method for determining the most likely among a set of competing hypotheses about (models of) the mechanisms that generated observed data. It has recently found widespread application in neuroimaging, particularly in the context of dynamic causal modelling (DCM). However, so far, optimizing experimental design has relied upon classical (frequentist) results that apply to parameter estimation in the context of the general linear model. This section presents the derivation of the <italic>Laplace-Chernoff risk</italic>, which serves as a proxy to the model selection error rate. The emphasis here is on model selection, rather than parameter estimation. This is important, because the former problem cannot, in general, be reduced to the latter, for which most formal optimality criteria have been designed <xref ref-type="bibr" rid="pcbi.1002280-Myung1">[16]</xref>. We thus outline the theory, which involves: (i) deriving a Bayesian decision theoretic design optimality score: this can be understood, in information theoretic terms, as expected model discriminability; (ii) disclosing its relationship to classical (frequentist) design optimality and (iii) inspecting its sensitivity to basic modelling assumptions.</p>
      <sec id="s2a">
        <title>Bayesian model comparison</title>
        <p>To interpret any observed data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e001" xlink:type="simple"/></inline-formula> with a view to making predictions based upon it, we need to select the best model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e002" xlink:type="simple"/></inline-formula> that provides formal constraints on the way those data were generated; (and will be generated in the future). This selection can be based on (Bayesian) probability theory to identify the best model in the light of data. This necessarily involves evaluating the model evidence or marginal likelihood <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e003" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e004" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e005" xlink:type="simple"/></inline-formula> is the (known) experimental manipulation (or design) and the generative model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e006" xlink:type="simple"/></inline-formula> is defined in terms of a likelihood <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e007" xlink:type="simple"/></inline-formula> and prior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e008" xlink:type="simple"/></inline-formula> on the unknown model parameters, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e009" xlink:type="simple"/></inline-formula>, whose product yields the joint density by Bayes rule:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e010" xlink:type="simple"/><label>(2)</label></disp-formula>Generally speaking, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e011" xlink:type="simple"/></inline-formula> is a density over the set of all possible datasets <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e012" xlink:type="simple"/></inline-formula> that can be generated under model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e013" xlink:type="simple"/></inline-formula> and experimental design <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e014" xlink:type="simple"/></inline-formula>. Having measured data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e015" xlink:type="simple"/></inline-formula>, Bayesian model comparison relies on evaluating the posterior probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e016" xlink:type="simple"/></inline-formula> of models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e017" xlink:type="simple"/></inline-formula> belonging to a predefined set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e018" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e019" xlink:type="simple"/><label>(3)</label></disp-formula>The reason why <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e020" xlink:type="simple"/></inline-formula> is a good proxy for the plausibility of any model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e021" xlink:type="simple"/></inline-formula> is that the data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e022" xlink:type="simple"/></inline-formula> sampled by the experiment are likely to lie within a subset of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e023" xlink:type="simple"/></inline-formula> that is highly plausible under the model whose predictions are the most similar to the true generative process. However, there is a possibility that the particular experimental sample <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e024" xlink:type="simple"/></inline-formula> could end up being more probable under a less reasonable model. This ‘model selection error’ could simply be due to chance, since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e025" xlink:type="simple"/></inline-formula> is sampled from a (hidden) probability distribution. In what follows, we focus on inferential procedures based on Bayesian model selection (e.g., DCM studies, see below). The experimental design should then minimize the expected model selection error. We now turn to a formal Bayesian decision theoretical approach for design optimization (we refer the interested reader to <xref ref-type="bibr" rid="pcbi.1002280-Chaloner1">[17]</xref> for an exhaustive review).</p>
      </sec>
      <sec id="s2b">
        <title>The Chernoff bound to the model selection error rate</title>
        <p>Following <xref ref-type="bibr" rid="pcbi.1002280-Lindley1">[18]</xref>, we consider the following decision theoretic problem. A design <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e026" xlink:type="simple"/></inline-formula> must be chosen from some set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e027" xlink:type="simple"/></inline-formula> and data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e028" xlink:type="simple"/></inline-formula> from a sample space <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e029" xlink:type="simple"/></inline-formula> is observed. Based on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e030" xlink:type="simple"/></inline-formula>, a model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e031" xlink:type="simple"/></inline-formula> will be chosen from the comparison set or model space <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e032" xlink:type="simple"/></inline-formula>. Note that the decision is in two parts: first the selection of the design <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e033" xlink:type="simple"/></inline-formula>, and then the model selection <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e034" xlink:type="simple"/></inline-formula>. Before the experiment is actually performed, the unknown variables are the models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e035" xlink:type="simple"/></inline-formula> and the data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e036" xlink:type="simple"/></inline-formula>. Within a Bayesian decision theoretic framework (see e.g., <xref ref-type="bibr" rid="pcbi.1002280-Robert1">[19]</xref>), the goal of the experiment is quantified by a loss function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e037" xlink:type="simple"/></inline-formula>, which measures the cost incurred in making decision <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e038" xlink:type="simple"/></inline-formula> (the selected model) when the hidden model is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e039" xlink:type="simple"/></inline-formula>. Note that obviously, no model is ‘true’ (or ‘false’): it is an imperfect approximation to reality, whose imperfections can, in certain circumstances, become salient; by ‘hidden model’, we mean ‘the model that is the least imperfect’. Following the Neyman-Pearson argument for hypothesis testing <xref ref-type="bibr" rid="pcbi.1002280-Neyman1">[20]</xref>, we define the model selection error or loss <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e040" xlink:type="simple"/></inline-formula> as follows:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e041" xlink:type="simple"/><label>(4)</label></disp-formula>According to Bayesian decision theory, the optimal decision <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e042" xlink:type="simple"/></inline-formula> is the one that minimizes the so-called <italic>posterior risk</italic>, i.e. the expected model selection error, given the observed data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e043" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e044" xlink:type="simple"/><label>(5)</label></disp-formula>where the expectation is taken over the model posterior distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e045" xlink:type="simple"/></inline-formula>. The optimal decision rule <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e046" xlink:type="simple"/></inline-formula> depends on the observed data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e047" xlink:type="simple"/></inline-formula>, whose marginal density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e048" xlink:type="simple"/></inline-formula> depends on the experimental design <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e049" xlink:type="simple"/></inline-formula>. A model selection error might still arise, even when applying the optimal model selection in equation 5. Note that the probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e050" xlink:type="simple"/></inline-formula> of selecting an erroneous model, given the data and having applied the optimal model selection rule is simply given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e051" xlink:type="simple"/><label>(6)</label></disp-formula>where we have used <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e052" xlink:type="simple"/></inline-formula>, for the potential error we make when selecting the optimal model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e053" xlink:type="simple"/></inline-formula>. Equation 6 means that the probability of making a model selection error is determined by the experimental evidence in favour of the selected model. Thus, repetitions of the same experiment might not lead to the same model being selected because of the variability of the posterior probability distribution over models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e054" xlink:type="simple"/></inline-formula>, induced by the sampling process.</p>
        <p>In this context, the task of design optimization is to reduce the effect of the data sampling process upon the overall probability of selecting the wrong model. This means we have to marginalize the probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e055" xlink:type="simple"/></inline-formula> of making an error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e056" xlink:type="simple"/></inline-formula> over the data sample space <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e057" xlink:type="simple"/></inline-formula>. Note that design optimization is the only Bayesian problem where it is meaningful to average over the sample space <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e058" xlink:type="simple"/></inline-formula>. This is because the experimental sample <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e059" xlink:type="simple"/></inline-formula> has not yet been observed, which makes the decision theoretic principle of averaging over what is unknown valid for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e060" xlink:type="simple"/></inline-formula>. More formally, the potential error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e061" xlink:type="simple"/></inline-formula> is the loss in our design decision theoretical problem, and the model selection error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e062" xlink:type="simple"/></inline-formula> is the <italic>design risk</italic> for Bayesian model selection. We define the optimal design (for Bayesian model selection) as the design <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e063" xlink:type="simple"/></inline-formula> that minimizes the design risk; i.e. the expectation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e064" xlink:type="simple"/></inline-formula> under the marginal prior predictive density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e065" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e066" xlink:type="simple"/><label>(7)</label></disp-formula>where we have used the expression for the error probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e067" xlink:type="simple"/></inline-formula> in equation 6. The integrand in equation 7 switches from one model to another one as one spans the data sample space <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e068" xlink:type="simple"/></inline-formula>. Unfortunately, this means that the error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e069" xlink:type="simple"/></inline-formula> has no analytical close form, and might therefore be difficult to evaluate. Instead, we propose to minimize an information theoretic criterion <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e070" xlink:type="simple"/></inline-formula> that yields both upper and lower bounds to the above error rate <xref ref-type="bibr" rid="pcbi.1002280-Lin1">[21]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e071" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e072" xlink:type="simple"/></inline-formula> is the cardinality of the model comparison set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e073" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e074" xlink:type="simple"/></inline-formula> is the Shannon entropy and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e075" xlink:type="simple"/></inline-formula> is the so-called <italic>Jensen-Shannon divergence</italic> (see, e.g., <xref ref-type="bibr" rid="pcbi.1002280-Topsoe1">[22]</xref>), which is an entropic measure of dissimilarity between probability density functions:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e076" xlink:type="simple"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e077" xlink:type="simple"/></inline-formula> is the Kullback-Leibler divergence between the densities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e078" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e079" xlink:type="simple"/></inline-formula>. Note that the Jensen-Shannon divergence is symmetric, nonnegative, bounded by 1 (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e080" xlink:type="simple"/></inline-formula>) and equal to zero if and only if all densities are equal. It is also the square of a metric (that of convergence in total variation).</p>
        <p>In the context of classification or clustering, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e081" xlink:type="simple"/></inline-formula> is known as the <italic>Chernoff bound</italic> to the classification error rate <xref ref-type="bibr" rid="pcbi.1002280-Lin1">[21]</xref>. Note that, since the prior distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e082" xlink:type="simple"/></inline-formula> over model space <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e083" xlink:type="simple"/></inline-formula> is independent of design <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e084" xlink:type="simple"/></inline-formula>, minimizing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e085" xlink:type="simple"/></inline-formula> with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e086" xlink:type="simple"/></inline-formula> corresponds to maximizing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e087" xlink:type="simple"/></inline-formula> with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e088" xlink:type="simple"/></inline-formula>. From equation 9, one can see that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e089" xlink:type="simple"/></inline-formula> is the difference between the entropy of the average prior predictive density over models minus the average entropy. In this setting, entropy can be thought of as average self information over models. Maximising <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e090" xlink:type="simple"/></inline-formula> minimises the dependencies among the prior predictive densities. Informally, one could think of this as orthogonalising the design, in the same way that one would orthogonalise a covariance matrix, namely minimise the covariances (the first term in equation 9 – first line) under the constraint that the variances are fixed (second term in equation 9 – first line). The second line in equation 9 gives yet another interpretation to the Jensen-Shannon divergence: it is the average Kullback-Leibler divergence between each prior predictive density and the average prior predictive density. It is a global measure of dissimilarity of the prior predictive densities; maximizing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e091" xlink:type="simple"/></inline-formula> thus separates each model prediction from the others. In turn, this means that the optimal design <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e092" xlink:type="simple"/></inline-formula> is the one that is the most discriminative, with respect to the prior predictive density of models included in the comparison set.</p>
        <p>In summary, we have derived the Bayesian decision theoretic design optimization rule that minimizes the model selection error rate. We have then proposed an information theoretic bound, which relies upon maximizing the discriminability of model predictions with respect to experimental design. We now turn to a specific class of generative models, that of nonlinear Gaussian likelihood functions, which is a class of generative models that encompasses most models used in neuroimaging data analyses.</p>
      </sec>
      <sec id="s2c">
        <title>Nonlinear Gaussian models and the approximate Laplace-Chernoff risk</title>
        <p>In the following, we will focus on the class of nonlinear Gaussian generative models. Without loss of generality (under appropriate nonlinear transformations), this class of models has the following form:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e093" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e094" xlink:type="simple"/></inline-formula> is the covariance matrix of the residual error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e095" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e096" xlink:type="simple"/></inline-formula> is the (deterministic) observation mapping of model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e097" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e098" xlink:type="simple"/></inline-formula> are the prior mean and covariance of the unknown parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e099" xlink:type="simple"/></inline-formula> (under model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e100" xlink:type="simple"/></inline-formula>).</p>
        <p>For this class of models, and using an appropriate Taylor expansion of the observation mapping, one can derive (see <xref ref-type="supplementary-material" rid="pcbi.1002280.s001">Text S1</xref>) an analytical approximation to the lower Chernoff bound to the model selection error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e101" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e102" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e103" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e104" xlink:type="simple"/></inline-formula> are defined as follows:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e105" xlink:type="simple"/><label>(12)</label></disp-formula>In the following, we will refer to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e106" xlink:type="simple"/></inline-formula> as the <italic>Laplace-Chernoff risk</italic>. In the following, we will show that, under mild conditions, the Laplace-Chernoff risk is monotonically related to the model selection error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e107" xlink:type="simple"/></inline-formula>, and is therefore a valid proxy.</p>
        <p>So far, we have considered the problem of selecting a single model from a set of alternatives. However, we may want to compare families of models, irrespective of detailed aspects of model structure <xref ref-type="bibr" rid="pcbi.1002280-Penny2">[23]</xref>. This optimization of experimental design for comparing <italic>model families</italic> is described in <xref ref-type="supplementary-material" rid="pcbi.1002280.s003">Text S3</xref>.</p>
      </sec>
      <sec id="s2d">
        <title>Relationship to classical design efficiency</title>
        <p>The Laplace-Chernoff risk is simple to compute and interpret. For example, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e108" xlink:type="simple"/></inline-formula> models and assuming that (i) both models are a priori equally likely, and (ii) both prior predictive densities have similar variances, i.e.: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e109" xlink:type="simple"/></inline-formula>, the Laplace-Chernoff risk is given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e110" xlink:type="simple"/><label>(13)</label></disp-formula>Equation 13 shows that the Laplace-Chernoff bound <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e111" xlink:type="simple"/></inline-formula> is a simple contrast resolution measure, in a signal detection theory sense (see <xref ref-type="fig" rid="pcbi-1002280-g002">Figure 2</xref>). Another perspective would be to think of it as a (log-transformed) t-test of the mean difference under two designs. From equation 13, one can see that the Laplace-Chernoff bound tends to one (i.e. the upper bound on the error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e112" xlink:type="simple"/></inline-formula> tends to 0.5) whenever either the difference <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e113" xlink:type="simple"/></inline-formula> between the first-order moments of the prior predictive densities goes to zero or their second-order moment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e114" xlink:type="simple"/></inline-formula> goes to infinity.</p>
        <fig id="pcbi-1002280-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Selection error rate and the Laplace-Chernoff risk.</title>
            <p>The (univariate) prior predictive density of two generative models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e115" xlink:type="simple"/></inline-formula> (blue) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e116" xlink:type="simple"/></inline-formula> (green) are plotted as a function of data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e117" xlink:type="simple"/></inline-formula>, given an arbitrary design <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e118" xlink:type="simple"/></inline-formula>. The dashed grey line shows the marginal predictive density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e119" xlink:type="simple"/></inline-formula> that captures the probabilistic prediction of the whole comparison set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e120" xlink:type="simple"/></inline-formula>. The area under the curve (red) measures the model selection error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e121" xlink:type="simple"/></inline-formula>, which depends upon the discriminability between the two prior predictive density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e122" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e123" xlink:type="simple"/></inline-formula>. This is precisely what the Laplace-Chernoff risk <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e124" xlink:type="simple"/></inline-formula> is a measure of.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g002" xlink:type="simple"/>
        </fig>
        <p>Optimizing the design <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e125" xlink:type="simple"/></inline-formula> with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e126" xlink:type="simple"/></inline-formula> thus reduces to discriminating the prior predictive densities, either by increasing the distance between their first-order moments, and/or by decreasing their second-order moments. Although this is not directly apparent from the general mathematical form of the Laplace-Chernoff bound (c.f. Equation 11), this intuition generalizes well to an arbitrary number of models and data dimensions.</p>
        <p>To demonstrate the properties of the Laplace-Chernoff bound, we will compare it with the classical design efficiency measure, under the general linear model (GLM), which is a special case of equation 10:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e127" xlink:type="simple"/><label>(14)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e128" xlink:type="simple"/></inline-formula> is the design matrix. The classical efficiency of a given contrast of parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e129" xlink:type="simple"/></inline-formula> is simply a function of the expected variance of the estimator of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e130" xlink:type="simple"/></inline-formula>. For example, when a contrast is used to test the null assumption <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e131" xlink:type="simple"/></inline-formula>, the classical efficiency <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e132" xlink:type="simple"/></inline-formula> is <xref ref-type="bibr" rid="pcbi.1002280-Henson1">[10]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e133" xlink:type="simple"/><label>(15)</label></disp-formula>where the contrast vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e134" xlink:type="simple"/></inline-formula> has zero entries everywhere except on its <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e135" xlink:type="simple"/></inline-formula><sup>th</sup> element, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e136" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e137" xlink:type="simple"/></inline-formula><sup>th</sup> column of the design matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e138" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e139" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e140" xlink:type="simple"/></inline-formula> without <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e141" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e142" xlink:type="simple"/></inline-formula> is the noise variance. Since decreasing the variance of the parameter estimates increases the significance for a given effect size, optimizing the classical efficiency <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e143" xlink:type="simple"/></inline-formula> simply improves statistical power; i.e., the chance of correctly rejecting the null. Although there are other design efficiency metrics (see, e.g., <xref ref-type="bibr" rid="pcbi.1002280-Friston2">[4]</xref>), this design efficiency measure, so-called C-optimality, is the one that is established in the context of standard fMRI studies <xref ref-type="bibr" rid="pcbi.1002280-Henson1">[10]</xref>.</p>
        <p>The equivalent Bayesian test relies on comparing two models, one with the full design matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e144" xlink:type="simple"/></inline-formula> and one with the reduced design matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e145" xlink:type="simple"/></inline-formula>. Under i.i.d. Gaussian priors for the unknown parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e146" xlink:type="simple"/></inline-formula> and flat priors on models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e147" xlink:type="simple"/></inline-formula>, one can show (see <xref ref-type="supplementary-material" rid="pcbi.1002280.s002">Text S2</xref>) that the Laplace-Chernoff risk <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e148" xlink:type="simple"/></inline-formula> simplifies to the following expression:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e149" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e150" xlink:type="simple"/></inline-formula> is the prior variance of the unknown parameters. <xref ref-type="supplementary-material" rid="pcbi.1002280.s002">Text S2</xref> demonstrates that the optimal design at the frequentist limit (non-informative priors, i.e.: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e151" xlink:type="simple"/></inline-formula>) is the design that maximizes the classical design efficiency measure:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e152" xlink:type="simple"/><label>(17)</label></disp-formula>In brief, under flat priors, optimizing the classical efficiency of the design minimizes the model selection error rate for the equivalent Bayesian model comparison. This is important, since it allows one to generalise established experimental design rules to a Bayesian analysis under the GLM.</p>
        <p>This result generalizes to any classical null hypothesis testing, which can be cast as a comparison of nested models (as above), under appropriate rotations of the design matrix. However, there are model comparisons that cannot be performed within a classical framework, such as non-nested models. This means that even at the frequentist limit and for linear models, equation 16 is more general than equation 15.</p>
        <p>Note that this equivalence is only valid at the limit of uninformative priors. For linear generative models, such as the GLM, this may not be a crucial condition. However, priors can be crucial when it comes to comparing nonlinear models. This is because <italic>a priori</italic> implausible regions of parameter space will have a negligible influence on the prior predictive density, even though their (conditional) likelihood may be comparatively quite high (e.g., a multimodal likelihood).</p>
      </sec>
      <sec id="s2e">
        <title>Tightness of the Laplace-Chernoff bounds</title>
        <p>We now examine the tightness of the Laplace-Chernoff bounds on the selection error rate. More precisely, we look at the influence of the moments <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e153" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e154" xlink:type="simple"/></inline-formula> of the prior predictive densities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e155" xlink:type="simple"/></inline-formula>, the dimension of the data (i.e. the sample size <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e156" xlink:type="simple"/></inline-formula>) and the number of models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e157" xlink:type="simple"/></inline-formula> in the comparison set (see <xref ref-type="fig" rid="pcbi-1002280-g003">Figure 3</xref>).</p>
        <fig id="pcbi-1002280-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Tightness of the Laplace-Chernoff bounds.</title>
            <p>The figure depicts the influence of a moment contrast between two prior predictive densities (left column), the number of models (middle column) and the data dimension (right column) onto the exact error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e158" xlink:type="simple"/></inline-formula> (green) and the Laplace-Chernoff risk <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e159" xlink:type="simple"/></inline-formula> (upper bound: solid red, lower bound: dashed red). This is assessed in terms of a mean shift (left inset) and a variance scaling (right inset). The blue lines depict the approximate Jensen-Shannon density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e160" xlink:type="simple"/></inline-formula> (see equations 8, 9 and 11 in the main text and equation A1.5 in <xref ref-type="supplementary-material" rid="pcbi.1002280.s001">Text S1</xref>).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g003" xlink:type="simple"/>
        </fig>
        <p>We will first focus on the comparison of two models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e161" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e162" xlink:type="simple"/></inline-formula>, whose respective prior predictive densities were assumed to be univariate Gaussian (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e163" xlink:type="simple"/></inline-formula>), with mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e164" xlink:type="simple"/></inline-formula> and variance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e165" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e166" xlink:type="simple"/></inline-formula> and varying moments for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e167" xlink:type="simple"/></inline-formula> (see below). For this low-dimensional case, solving Equation 7 with numerical integration is possible and yields the exact selection error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e168" xlink:type="simple"/></inline-formula> for each model comparison. The left column in <xref ref-type="fig" rid="pcbi-1002280-g003">Figure 3</xref> depicts the Laplace-Chernoff bounds as a function of the first order moment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e169" xlink:type="simple"/></inline-formula> (bottom inset) and as a function of the second order moment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e170" xlink:type="simple"/></inline-formula> (upper inset) of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e171" xlink:type="simple"/></inline-formula>, when comparing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e172" xlink:type="simple"/></inline-formula> versus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e173" xlink:type="simple"/></inline-formula>. One can see that the error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e174" xlink:type="simple"/></inline-formula> decreases as the moment contrast (either a mean shift or a variance scaling) increases. In addition, the Laplace-Chernoff risk <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e175" xlink:type="simple"/></inline-formula> is related monotonically to the error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e176" xlink:type="simple"/></inline-formula>. However, there is a moment contrast above which the upper bound breaks down, in the sense that the condition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e177" xlink:type="simple"/></inline-formula> is not satisfied.</p>
        <p>Second, we varied the number of models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e178" xlink:type="simple"/></inline-formula> in the comparison set, where each model was characterized by a univariate Gaussian prior predictive density (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e179" xlink:type="simple"/></inline-formula>). The middle column in <xref ref-type="fig" rid="pcbi-1002280-g003">Figure 3</xref> depicts the Laplace-Chernoff bounds as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e180" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e181" xlink:type="simple"/></inline-formula> had mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e182" xlink:type="simple"/></inline-formula> and variance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e183" xlink:type="simple"/></inline-formula>, and any new model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e184" xlink:type="simple"/></inline-formula> had a mean shift of 1 (bottom inset) or a variance scaling of 4 (upper inset), with respect to the preceding one. This ensured that the discriminability between two neighbouring models was comparable. One can see that the error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e185" xlink:type="simple"/></inline-formula> increases as the number of models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e186" xlink:type="simple"/></inline-formula> increases and that the Laplace-Chernoff risk <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e187" xlink:type="simple"/></inline-formula> follows monotonically. However, there may be a number of models above which the upper bound becomes vacuous, in the sense that the condition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e188" xlink:type="simple"/></inline-formula> is not satisfied (although the bounding condition seems to be preserved).</p>
        <p>Finally, we varied the sample size <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e189" xlink:type="simple"/></inline-formula>, when comparing models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e190" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e191" xlink:type="simple"/></inline-formula>. The right column in <xref ref-type="fig" rid="pcbi-1002280-g003">Figure 3</xref> depicts the Laplace-Chernoff bounds as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e192" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e193" xlink:type="simple"/></inline-formula> had mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e194" xlink:type="simple"/></inline-formula> and variance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e195" xlink:type="simple"/></inline-formula> and model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e196" xlink:type="simple"/></inline-formula> had a mean shift of 1 in each dimension; i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e197" xlink:type="simple"/></inline-formula>−(bottom inset) or a variance scaling of 4 – i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e198" xlink:type="simple"/></inline-formula>−(upper inset). This ensured that the discriminability increased monotonically with the sample size. One can see that the error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e199" xlink:type="simple"/></inline-formula> decreases as the sample size <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e200" xlink:type="simple"/></inline-formula> increases and that the Laplace-Chernoff risk <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e201" xlink:type="simple"/></inline-formula> again changes monotonically. However, again, there is a sample size above which the upper bound breaks down; in the sense that the condition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e202" xlink:type="simple"/></inline-formula> is not satisfied. This situation is very similar to increasing the mean or variance contrast; i.e., increasing the sample size can be thought of as increasing the discriminability of models in the comparison set.</p>
        <p>Taken together, these results suggest that the Laplace-Chernoff risk <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e203" xlink:type="simple"/></inline-formula> is a good proxy for the model selection error rate; in that there is a monotonic mapping between the two quantities. Furthermore, the upper bound becomes tightest for the worst (least decisive) model comparisons. This is important, because this means that the approximation by the Laplace-Chernoff risk is best when we most need it most. However, the Laplace-Chernoff risk can become more liberal than the true error probability. The subtle point here is that the model number and their discriminability have an opposite effect on the tightness of the bound. We will further examine the quality of the Laplace-Chernoff bounds in the context of effective connectivity analysis with DCM in the next section.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Results</title>
      <sec id="s3a">
        <title>Design risk for DCM: preliminary considerations</title>
        <p>In Dynamic Causal Modelling (DCM), hemodynamic (fMRI) signals arise from a network of functionally segregated sources; i.e., brain regions or neuronal sources. More precisely, DCMs rely on two processes:</p>
        <list list-type="bullet">
          <list-item>
            <p>DCMs describe how experimental manipulations (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e204" xlink:type="simple"/></inline-formula>) influence the dynamics of hidden (neuronal and hemodynamic) states of the system (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e205" xlink:type="simple"/></inline-formula>). This is typically written in terms of the following ordinary differential equation (the <italic>evolution equation</italic>):<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e206" xlink:type="simple"/><label>(18)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e207" xlink:type="simple"/></inline-formula> is the rate of change of the system's states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e208" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e209" xlink:type="simple"/></inline-formula> summarizes the biophysical mechanisms underlying the system's temporal evolution and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e210" xlink:type="simple"/></inline-formula> is a set of unknown evolution parameters. In particular, the system states include ‘neural’ states, which are driven by the experimental stimuli and cause variations in the fMRI signal. Their evolution function is given by <xref ref-type="bibr" rid="pcbi.1002280-Friston2">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002280-Stephan2">[24]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e211" xlink:type="simple"/><label>(19)</label></disp-formula>The parameters of this neural evolution function include a between-region coupling (matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e212" xlink:type="simple"/></inline-formula>), input-dependent coupling modulation (matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e213" xlink:type="simple"/></inline-formula>), input driving gains (matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e214" xlink:type="simple"/></inline-formula>) and gating effects (matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e215" xlink:type="simple"/></inline-formula>).</p>
          </list-item>
          <list-item>
            <p>DCMs map the system's hidden states (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e216" xlink:type="simple"/></inline-formula>) to experimental measures (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e217" xlink:type="simple"/></inline-formula>). This is typically written as the following static <italic>observation equation</italic>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e218" xlink:type="simple"/><label>(20)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e219" xlink:type="simple"/></inline-formula> is the instantaneous non-linear mapping from system's states to observations, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e220" xlink:type="simple"/></inline-formula> is a set of unknown observation parameters and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e221" xlink:type="simple"/></inline-formula> are model residuals.</p>
          </list-item>
        </list>
        <p>Note that the ensuing dynamic causal model includes the effect of the hemodynamic response function that can change over brain regions. Equations 18 and 20 can be compiled into a nonlinear Gaussian generative model (similar in form to equation 10), which, given experimental data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e222" xlink:type="simple"/></inline-formula>, can then be inverted using a variational Bayesian approach. This scheme provides an approximate posterior density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e223" xlink:type="simple"/></inline-formula> over the unknown model parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e224" xlink:type="simple"/></inline-formula> and a lower bound <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e225" xlink:type="simple"/></inline-formula> (free energy) to the models log-evidence or marginal likelihood <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e226" xlink:type="simple"/></inline-formula>. The free energy is used for comparing DCMs that represent competing hypotheses about network mechanisms, specified in terms of network structure and the modulation of specific connections. See <xref ref-type="bibr" rid="pcbi.1002280-Daunizeau1">[14]</xref> for a critical review of the biophysical and statistical foundations of DCM.</p>
        <p>In brief, DCMs belong to the class of generative models for which we have derived the Laplace-Chernoff design risk (Equation 11). In what follows, we will evaluate the proposed method in the context of network discovery with DCM. First, we will evaluate the quality of the Laplace-Chernoff bound. Having established the conditions for this bound to hold, we will then focus on optimal designs for some canonical questions. These two steps will be performed using Monte-Carlo simulations. Finally, we will turn to an empirical validation of the simulation results, using data acquired from two subjects performing a simple finger-tapping experiment in the fMRI scanner.</p>
      </sec>
      <sec id="s3b">
        <title>Evaluation of the model selection error bounds</title>
        <p>In this section, we ask whether the Laplace-Chernoff bounds on the error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e227" xlink:type="simple"/></inline-formula> are consistent. This can be addressed by comparing the predicted bounds to the observed model selection error rate across repetitions of the same experiment. We have conducted a series of Monte-Carlo simulations, which reproduced the main characteristics of the finger-tapping task used in the section on empirical validation. Specifically, we considered two candidate DCMs (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e228" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e229" xlink:type="simple"/></inline-formula>) that consist of two (reciprocally connected) regions, each driven by a different experimentally controlled input (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e230" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e231" xlink:type="simple"/></inline-formula>, respectively). The two models differed in which of the two inputs drove which region. We then examined Bayesian model comparison (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e232" xlink:type="simple"/></inline-formula> versus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e233" xlink:type="simple"/></inline-formula>) under three designs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e234" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e235" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e236" xlink:type="simple"/></inline-formula>, which differed in the temporal dynamics of the two inputs they affect. More precisely, we increase the correlations between the two stimuli: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e237" xlink:type="simple"/></inline-formula>. This makes it increasingly difficult to disambiguate the respective impact of each input on network dynamics. In turn, we expect these three designs to be increasingly risky when discriminating among the two candidate DCMs. <xref ref-type="fig" rid="pcbi-1002280-g004">Figure 4</xref> summarizes the structure of the two DCMs and shows the time course of the three designs' stimulation paradigms (experimental inputs).</p>
        <fig id="pcbi-1002280-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Evaluation of the Laplace-Chernoff bounds: DCM comparison set and candidate designs.</title>
            <p>This figure summarizes the Monte-Carlo simulation environment of section “Evaluation of the model selection error bounds” we used for evaluating the Laplace-Chernoff bounds in the context of network identification. The comparison set is shown on the left. It consists of two models that differ in terms of where the two inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e238" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e239" xlink:type="simple"/></inline-formula> enter the network. The three candidate designs are shown on the right. They consist of three different stimulation sequences, with different degrees of temporal correlation between the two inputs.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g004" xlink:type="simple"/>
        </fig>
        <p>To explore a range of plausible scenarios, we varied the following four factors to simulate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e240" xlink:type="simple"/></inline-formula> datasets <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e241" xlink:type="simple"/></inline-formula>:</p>
        <list list-type="bullet">
          <list-item>
            <p>Sixteen random realisations of the residuals <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e242" xlink:type="simple"/></inline-formula>, which were sampled according to their prior density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e243" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e244" xlink:type="simple"/></inline-formula> is the residuals' precision (see below).</p>
          </list-item>
          <list-item>
            <p>Two levels of effective connectivity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e245" xlink:type="simple"/></inline-formula>. This factor was used to manipulate the discriminability of the two models. This is because it is more difficult to determine the respective contribution of the two inputs to the responses in each region as the effective connectivity increases.</p>
          </list-item>
          <list-item>
            <p>Two generative models (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e246" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e247" xlink:type="simple"/></inline-formula>). This factor is required because the selection error probability is symmetric with respect to the model that generated the data.</p>
          </list-item>
          <list-item>
            <p>Two levels of noise, i.e.: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e248" xlink:type="simple"/></inline-formula>, which correspond to realistic signal-to-noise ratios. This factor controls the overall discriminability of the two models, by scaling non-specific processes contributing to the data. Note that the approximate error probability bounds are conditional on the expected noise precision.</p>
          </list-item>
        </list>
        <p>Each dataset was inverted (fitted) under both models (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e249" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e250" xlink:type="simple"/></inline-formula>), using a variational Laplace scheme <xref ref-type="bibr" rid="pcbi.1002280-Friston4">[25]</xref>, and Bayesian model selection was performed using the free energy approximation to the log evidence. We used shrinkage i.i.d. Gaussian priors for evolution and observation parameters (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e251" xlink:type="simple"/></inline-formula>), and weakly informative Gamma priors for the precision (scale parameter equal to the simulated noise precision and unit shape parameter). The same priors were used to derive the Laplace-Chernoff bounds. <xref ref-type="fig" rid="pcbi-1002280-g005">Figure 5</xref> depicts a typical simulation and model inversion.</p>
        <fig id="pcbi-1002280-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Evaluation of the Laplace-Chernoff bounds: simulated data and VB inversion.</title>
            <p>Upper-left: simulated (neural and hemodynamic) states dynamics <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e252" xlink:type="simple"/></inline-formula> as a function of time under model 1 and design 1 (two regions, five states per region). Lower-left: simulated fMRI data (blue: region 1, green: region 2). Solid lines show the observable BOLD changes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e253" xlink:type="simple"/></inline-formula> (without noise) and dashed lines show the actual noisy time series <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e254" xlink:type="simple"/></inline-formula> that are sent to the VB inversion scheme. Upper-middle: the iterative increase in the lower bound to the model evidence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e255" xlink:type="simple"/></inline-formula> (free energy) as the VB inversion scheme proceeds (from the prior to the final posterior approximation), under model 1. Lower-middle: Posterior correlation matrix between the model parameters. Red or blue entries indicate a potential non-identifiability issue and grey entries are associated with fixed model parameters. Upper-right: approximate posterior density over (neural and hemodynamic) states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e256" xlink:type="simple"/></inline-formula>. The first two moments of the density are shown (solid line: mean, shaded area: standard deviation). Lower-right: approximate posterior predictive density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e257" xlink:type="simple"/></inline-formula> and data time series.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g005" xlink:type="simple"/>
        </fig>
        <p>We counted the number of times the selected model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e258" xlink:type="simple"/></inline-formula> was different from the simulated ground truth. Averaging over the first three factors, this yielded a Monte-Carlo estimate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e259" xlink:type="simple"/></inline-formula> of the selection error rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e260" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e261" xlink:type="simple"/></inline-formula> is the standard deviation of the Monte-Carlo estimate, for each of the three designs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e262" xlink:type="simple"/></inline-formula> and each of the two noise levels <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e263" xlink:type="simple"/></inline-formula>.</p>
        <p><xref ref-type="fig" rid="pcbi-1002280-g006">Figure 6</xref> presents a graphical comparison between the Monte-Carlo confidence interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e264" xlink:type="simple"/></inline-formula> on the error rate with the Laplace-Chernoff bounds. First, one can see that the average selection error probability (both predicted and estimated) decreases with the residual precision <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e265" xlink:type="simple"/></inline-formula>. This is expected: as signal-to-noise ratio increases, the more discriminative evidence favouring one model or another exists in the data. Second, one can see that both estimated and predicted intervals on the selection error probability agree quantitatively: more precisely, the Monte-Carlo confidence intervals <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e266" xlink:type="simple"/></inline-formula> always intersect with the Laplace-Chernoff bounds; and for both residual precision levels, both the Monte-Carlo estimate of the error rate and the Laplace-Chernoff risk equally rank the three designs: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e267" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e268" xlink:type="simple"/></inline-formula>. This means that for these levels of noise and sample sizes, the Laplace-Chernoff bound is in good agreement with the design risk. However, this quantitative agreement might break down for higher sample sizes or noise precision (cf. section “Tightness of the Laplace-Chernoff bounds” and <xref ref-type="fig" rid="pcbi-1002280-g003">Figure 3</xref>).</p>
        <fig id="pcbi-1002280-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Evaluation of the Laplace-Chernoff bounds: Monte-Carlo results.</title>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g006" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s3c">
        <title>Laplace-Chernoff risk for canonical network identification questions</title>
        <p>The aim of this section is twofold: to investigate the sensitivity of the Laplace-Chernoff risk to the prior densities, and to demonstrate the importance of the model comparison set. We thus chose three “canonical network identification questions”, i.e. three simple model comparison sets that represent typical questions addressed by DCM. <xref ref-type="fig" rid="pcbi-1002280-g007">Figure 7</xref> shows these model sets, each of which is composed of two variants of a two-region network:</p>
        <list list-type="bullet">
          <list-item>
            <p><italic>Driving input</italic>: the two DCMs differ in terms of where the input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e271" xlink:type="simple"/></inline-formula> enters the network.</p>
          </list-item>
          <list-item>
            <p><italic>Modulatory input</italic>: the two DCMs differ in terms of whether or not the experimental manipulation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e272" xlink:type="simple"/></inline-formula> modulates the feedforward connection from node 1 to node 2.</p>
          </list-item>
          <list-item>
            <p><italic>Feedback connection</italic>: the two DCMs differ in terms of whether or not there is a feedback connection from node 2 to node 1.</p>
          </list-item>
        </list>
        <fig id="pcbi-1002280-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Canonical network identification questions: DCM comparison sets.</title>
            <p>This figure depicts the three canonical DCM comparison sets, each of which consists of two variants of a simple two-region network. Upper-row: driving input; middle-row: modulatory input; Lower-row: feedback connection.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g007" xlink:type="simple"/>
        </fig>
        <p>We then compared different experimental designs, considering blocked on/off (square wave) designs, and varying the epoch duration within the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e273" xlink:type="simple"/></inline-formula>. Comparing the Laplace-Chernoff risk of such designs allows one to identify the optimal epoch duration for each network identification question. In addition, we varied the first-order moment of the prior densities over neural evolution parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e274" xlink:type="simple"/></inline-formula> within the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e275" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e276" xlink:type="simple"/></inline-formula>. As above, we used i.i.d. shrinkage priors for the hemodynamic evolution and observation parameters (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e277" xlink:type="simple"/></inline-formula>) and non-informative Gamma priors for the noise precision (with scale parameter equal to 10<sup>−1</sup> and unit shape parameter). This allowed us to evaluate the influence of the expected coupling strength on design optimisation. The average time interval between two blocks was held at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e278" xlink:type="simple"/></inline-formula>, but a random jitter was added to this average inter-block time interval. For each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e279" xlink:type="simple"/></inline-formula> pair, we randomly drew sixteen stimulation sequences <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e280" xlink:type="simple"/></inline-formula>. <xref ref-type="fig" rid="pcbi-1002280-g008">Figure 8</xref> depicts the average (across random jitters) Laplace-Chernoff risk as a function of both epoch duration and prior mean of the evolution parameters, for the three canonical network identification questions.</p>
        <fig id="pcbi-1002280-g008" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Canonical network identification questions: optimal epoch duration.</title>
            <p>This figure shows plots of the average (across jitters) Laplace-Chernoff risk as a function of epoch duration (in seconds) and prior expectation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e281" xlink:type="simple"/></inline-formula> of neural evolution parameters, for the three canonical comparison sets (left: driving input, middle: modulatory input, right: feedback connection). Blue: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e282" xlink:type="simple"/></inline-formula>, green: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e283" xlink:type="simple"/></inline-formula>, red: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e284" xlink:type="simple"/></inline-formula> and magenta: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e285" xlink:type="simple"/></inline-formula>. Error bars depict the variability (one standard deviation) induced by varying jitters in the stimulation sequence.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g008" xlink:type="simple"/>
        </fig>
        <p>First, one can see that the main effect of the prior mean is to increase the discriminability among the models in the comparison set, except in the ‘driving input’ case. This means that, in general, the discriminative power of the design increases with the expected effect size. This does not work for the ‘driving input’ case, however, because of the feedback connections, which tend to synchronize the two regions of the network and thus blur the distinction between the predictions of the two models.</p>
        <p>Second, the optimal epoch duration depends on the question of interest. For example, the optimal epoch duration is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e286" xlink:type="simple"/></inline-formula> seconds, when asking whether there is a modulatory input or where the driving input enters the network, which is close to the optimal epoch duration for classical (SPM) activation studies <xref ref-type="bibr" rid="pcbi.1002280-Robert1">[19]</xref>. Strictly speaking, note that in the “driving input” case, the optimal epoch duration additionally depends upon the expected coupling strength: about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e287" xlink:type="simple"/></inline-formula> seconds for low coupling and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e288" xlink:type="simple"/></inline-formula> seconds for high coupling. On average however, the optimal epoch duration is much shorter when trying to disclose the feedback connection (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e289" xlink:type="simple"/></inline-formula> seconds). This might be due to the fact that a feedback connection mostly expresses itself during the transient dynamics of the network's response to stimulation (moving from or returning to steady-state). Decreasing the epoch duration increases the number of repetitions of such transitions, thus increasing the discriminative power of the design. To test this, we looked at the difference between the covariance matrices of the prior predictive densities of a model with and without feedback, respectively. This difference is depicted on <xref ref-type="fig" rid="pcbi-1002280-g009">Figure 9</xref>, for the highest prior mean of evolution parameters: i.e., highest coupling strength.</p>
        <fig id="pcbi-1002280-g009" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g009</object-id>
          <label>Figure 9</label>
          <caption>
            <title>The signature of feedback connections.</title>
            <p>The figure depicts the difference in the data correlation matrices induced by two network structures (model fbk-: without feedback, model fbk+: with feedback). Red (respectively, blue) entries indicate an increase (respectively, a decrease) in the correlation induced by adding a feedback connection from node 2 to node 1. Each block within the matrix corresponds to a node-to-node temporal correlation structure (upper-left: node 1 to node 1, lower-right: node 2 to node 2, upper-right/lower-left: node 1 to node 2). For example, the dashed back box reads as follows: adding the feedback connection increases between activity in node 2 at the end of the block and node 1 during the whole block. The solid black box indicates the time interval, during which input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e290" xlink:type="simple"/></inline-formula> to node 1 was ‘on’. Note that its effect onto the two-region network dynamics is delayed, due to the hemodynamic response function.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g009" xlink:type="simple"/>
        </fig>
        <p>One can see that a feedback connection expresses itself when the system goes back to steady-state and increases the correlations between the nodes. This specific contribution to the statistical structure of the fMRI data is what DCM uses to infer the presence of a feedback connection.</p>
        <p>Finally, one can see that there is a clear difference in the average Laplace-Chernoff risk between the three canonical network identification questions. This speaks to the overall discriminability of the models, within each comparison set. For example, it is easier to decide where the driving input enters the network (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e291" xlink:type="simple"/></inline-formula>) than to detect a modulatory effect (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e292" xlink:type="simple"/></inline-formula>) or a feedback connection (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e293" xlink:type="simple"/></inline-formula>). However, when optimizing other design parameters unrelated to epoch duration (e.g., sampling rate), this ranking could change.</p>
      </sec>
      <sec id="s3d">
        <title>Investigating psycho-physiological interactions with DCM</title>
        <p>In the context of DCM for fMRI, there are many design parameters one may want to control. These include, but are not limited to: (i) the physics of MRI acquisition (e.g., sampling rate versus signal-to-noise ratio), (ii) sample size, (iii) stimulus design and timing (e.g., categorical versus parametric, epoch duration, inter-stimulus time interval), and (iv) the use of biophysical interventions (e.g., transcranial magnetic stimulation, TMS). Assessing all these design parameters is well beyond the scope of the present article, and will be the focus of forthcoming publications. In this section, we demonstrate the use of the Laplace-Chernoff risk in the context of (iii) or (iv). This is addressed by two simulations that recapitulate common experimental questions of interest: characterizing psycho-physiological interactions (PPI) and using TMS for network analysis, respectively.</p>
        <p>In the first simulation, we examined how different interpretations of a PPI could be disambiguated by comparing DCMs. One demonstrates a PPI by showing that the activity in region 2 can be explained by the interaction between the activity of region 1 and a psychological factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e294" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002280-Friston5">[26]</xref>–<xref ref-type="bibr" rid="pcbi.1002280-Gitelman1">[27]</xref>. There are two qualitatively different interpretations of such effects: either region 1 modulates the response of region 2 to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e295" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e296" xlink:type="simple"/></inline-formula> modulates the influence region 1 exerts on region 2. A standard activation analysis of PPI cannot disambiguate these interpretations. However, they correspond to different DCMs. <xref ref-type="fig" rid="pcbi-1002280-g010">Figure 10</xref> depicts six DCMs that are compatible with the same PPI. This is a 3×2 factorial model comparison set, with the following factors (see <xref ref-type="table" rid="pcbi-1002280-t001">Table 1</xref>):</p>
        <list list-type="bullet">
          <list-item>
            <p><italic>Class of PPI</italic>. A DCM compatible with the notion that region 1 modulates the region 2 response to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e297" xlink:type="simple"/></inline-formula> would be such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e298" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e299" xlink:type="simple"/></inline-formula> (model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e300" xlink:type="simple"/></inline-formula>). In contradistinction, one could think of at least two DCMs compatible with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e301" xlink:type="simple"/></inline-formula> modulating the influence of region 1 onto region 2: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e302" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e303" xlink:type="simple"/></inline-formula> (models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e304" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e305" xlink:type="simple"/></inline-formula>, respectively).</p>
          </list-item>
          <list-item>
            <p><italic>Presence of a feedback connection</italic>. In addition, one could include or omit a feedback connection from region 2 to region 1. We will denote <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e306" xlink:type="simple"/></inline-formula> models with such a feedback (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e307" xlink:type="simple"/></inline-formula>) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e308" xlink:type="simple"/></inline-formula> without (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e309" xlink:type="simple"/></inline-formula>).</p>
          </list-item>
        </list>
        <fig id="pcbi-1002280-g010" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g010</object-id>
          <label>Figure 10</label>
          <caption>
            <title>PPI: the 3×2 factorial DCM comparison set.</title>
            <p>The figure depicts the set of DCMs that are compatible with a PPI (correlation between region 2 and the interaction of region 1 and manipulation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e310" xlink:type="simple"/></inline-formula>). This comparison set is constructed in a factorial way: (i) three PPI classes and (ii) with/without a feedback connection from node 2 to node 1. It can be partitioned into two partitions of two families each. Partition 1 corresponds to the two qualitatively different interpretations of a PPI (“region 1 modulates the response of region 2 to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e311" xlink:type="simple"/></inline-formula>” versus “<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e312" xlink:type="simple"/></inline-formula> modulates the influence of region 1 onto region 2”). Partition 2 relates to the presence versus absence of the feedback connection.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g010" xlink:type="simple"/>
        </fig>
        <table-wrap id="pcbi-1002280-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.t001</object-id><label>Table 1</label><caption>
            <title>3×2 factorial comparison set for PPI.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002280-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.t001" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="2" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e313" xlink:type="simple"/></inline-formula> modulates 1→2</td>
                <td align="left" colspan="1" rowspan="1">1 modulates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e314" xlink:type="simple"/></inline-formula>→2</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e315" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e316" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e317" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e318" xlink:type="simple"/></inline-formula> (no feedback)</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e319" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e320" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e321" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e322" xlink:type="simple"/></inline-formula> (feedback)</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e323" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e324" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e325" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
            </tbody>
          </table></alternatives></table-wrap>
        <p>We first ask whether we can find the optimal epoch duration that discriminates among the PPI comparison set, either at the model level or at the family level <xref ref-type="bibr" rid="pcbi.1002280-Penny2">[23]</xref>. We considered two partitions of the comparison set (see <xref ref-type="fig" rid="pcbi-1002280-g010">Figure 10</xref>): (i) partition 1 separates the two qualitatively different interpretations of PPIs and (ii) partition 2 separates models with and without feedback connections. We then adapted the analysis of section “Laplace-Chernoff risk for canonical network identification questions”, as follows:</p>
        <p>We considered blocked on/off (square wave) designs, and varied the epoch duration within the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e326" xlink:type="simple"/></inline-formula>. In addition, we varied the first-order moment of the prior densities over evolution parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e327" xlink:type="simple"/></inline-formula> within the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e328" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e329" xlink:type="simple"/></inline-formula>. In other respects, the simulation parameters were as above. For all stimulation paradigms, the fMRI session was assumed to last for five minutes. Note that the experimental designs that were balanced in terms of the number of repetitions of factorial conditions (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e330" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e331" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e332" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e333" xlink:type="simple"/></inline-formula>). <xref ref-type="fig" rid="pcbi-1002280-g011">Figure 11</xref> depicts the average (across random jitters) Laplace-Chernoff risk as a function of both epoch duration and the prior mean of the evolution parameters, for the three comparisons, i.e. at the model level and for the two above partitions.</p>
        <fig id="pcbi-1002280-g011" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g011</object-id>
          <label>Figure 11</label>
          <caption>
            <title>PPI: optimal epoch duration.</title>
            <p>This figure shows plots of the average (across jitters) Laplace-Chernoff risk as a function of epoch duration (in seconds) and prior expectation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e334" xlink:type="simple"/></inline-formula> of neural evolution parameters, for the three inference levels defined in relation to the PPI comparison set of <xref ref-type="fig" rid="pcbi-1002280-g010">Fig. 10</xref>. It uses the same format as <xref ref-type="fig" rid="pcbi-1002280-g008">Fig. 8</xref>. Left: model comparison, middle: family comparison (partition 1), right: family comparison (partition 2).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g011" xlink:type="simple"/>
        </fig>
        <p>One can see that for strong coupling strengths, the optimal block length seems to be about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e335" xlink:type="simple"/></inline-formula> seconds, irrespective of the level of inference. Note that this is slightly smaller than the optimal block length in activation studies <xref ref-type="bibr" rid="pcbi.1002280-Henson1">[10]</xref>. In addition, one can see that the level of inference impacts upon the absolute Laplace-Chernoff risk. For example, it is easier to discriminate between the two qualitative interpretations of the PPI (i.e., family level inference, between the two subsets of partition 1), than to perform an inference at the model level. Interestingly, the most risky inference is about the presence of feedback connections, which reproduces the results in section “Laplace-Chernoff risk for canonical network identification questions”.</p>
        <p>In a second simulation, we demonstrate how the Laplace-Chernoff risk could be optimized with respect to the use of TMS. More precisely, we addressed the question of choosing the intervention site, i.e. either on region 1 or on region 2. This defines three possible designs: TMS1 (intervenes on region 1), TMS2 (intervenes on region 2) and no TMS.</p>
        <p>We assumed TMS was used ‘on-line’, using brief stimulation pulses grouped in epochs of 8 seconds duration. We used balanced on/off designs and 5 minutes scanning sessions. To distinguish the physiological effect of TMS from other experimental stimuli, we chose prior densities on evolution parameters that emulated comparatively weak effects; i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e336" xlink:type="simple"/></inline-formula>. Priors on the observation parameters and the precision hyperparameter were set as above. We draw 16 samples with different random jitters (standard deviation: 2 seconds). <xref ref-type="fig" rid="pcbi-1002280-g012">Figure 12</xref> depicts the average Laplace-Chernoff risk for the three TMS designs, for two comparison sets: (i) the first subset of partition 2 (only the models without feedback) and (ii) the full comparison set (with and without feedback connections).</p>
        <fig id="pcbi-1002280-g012" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g012</object-id>
          <label>Figure 12</label>
          <caption>
            <title>PPI: optimal TMS intervention site.</title>
            <p>This figure shows plots of the average (across jitters) Laplace-Chernoff risk as a function of the TMS design (TMS1, TMS 2 or no TMS), for two different PPI comparison sets. Left: the two TMS ‘on’ designs (TMS1: target region 1, TMS2: target region 2). Upper-right: average Laplace-Chernoff risk for the first family of partition 2 (three models, no feedback connection from node 2 to node 1). Lower-right: average Laplace-Chernoff risk for the whole PPI comparison set (six models, with and without a feedback connection from node 2 to node 1).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g012" xlink:type="simple"/>
        </fig>
        <p>One can see that using on-line TMS generally improves the discriminability over models, irrespective of the comparison set (the Laplace-Chernoff risk of the ‘no TMS’ design is systematically higher than those of ‘TMS1’ and ‘TMS2’). However, the optimal intervention site (region 1 or region 2) does depend upon the comparison set: one should stimulate region 1 if one is only interested into discriminating between the ‘no-feedback’ models, and region 2 if one wants to select the best among all models. This makes intuitive sense, since stimulating region 2 (orthogonally to the other experimental manipulations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e337" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e338" xlink:type="simple"/></inline-formula>) will disclose the presence of the feedback connection more readily.</p>
      </sec>
      <sec id="s3e">
        <title>Empirical validation</title>
        <p>In this section, we apply the above approach to empirical fMRI data acquired during a simple finger-tapping (motor) task. <xref ref-type="fig" rid="pcbi-1002280-g013">Figure 13</xref> reports the structure of the task.</p>
        <fig id="pcbi-1002280-g013" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g013</object-id>
          <label>Figure 13</label>
          <caption>
            <title>Finger-tapping task: paradigm and classical SPM.</title>
            <p>Left: inner stimulation sequence of one trial of the finger-tapping task (fixation cross, then motor pacing – left or right or both- and the final recording of the subject's response – button press-). Right: SPM t-contrast (right&gt;left) thresholded at p = 0.05 (FWE corrected) for subject KER under the blocked design.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g013" xlink:type="simple"/>
        </fig>
        <p>Each trial consisted of a fixation period and a pacing stimulus (‘right’, ‘left’, ‘right and left’ or null) that ended with the subject's motor response (button press). The whole fMRI session comprised 400 events (100 left, 100 right, 100 left &amp; right, 100 null events). The average inter-trial interval was two seconds. Each subject participated in two sessions, corresponding to two variants of the experimental design, i.e., blocked (ten consecutive identical trials per block) and event-related (randomized trials). There were two subjects in total (but see above).</p>
        <p>About 700 T2*-weighted single-shot gradient echo echo-planar images (TE = 40 ms, TR = 1.3 s, 24 interleaved axial slices of 4.4 mm thickness, FOV = 24×24 cm<sup>2</sup>, 80×80 matrix) were acquired over a 35-min session on a 3 Tesla MRI scanner. FMRI data were pre-processed using SPM8 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/" xlink:type="simple">http://www.fil.ion.ucl.ac.uk/spm/</ext-link>). EPI time series were realigned, spatially smoothed with an 8 mm FWHM isotropic Gaussian kernel and normalized. A GLM was constructed to assess the presence of regional BOLD changes related to the motor responses. The design matrix contained two pacing regressors (‘left’ and ‘right’), as well as realignment parameters to correct for motion-related changes.</p>
        <p>Left and right motor cortices (MC) were identified by means of subject-specific <italic>t</italic>-contrasts testing for the difference between the ‘left’ and ‘right’ pacing conditions (p&lt;0.05, whole-brain FWE corrected, see <xref ref-type="fig" rid="pcbi-1002280-g013">Figure 13</xref>). A summary time series was derived for each ROI by computing the first eigenvariate of all suprathreshold voxel time series within 10 mm of the ROI centres.</p>
        <p>Four models were included in the comparison set, which is depicted in <xref ref-type="fig" rid="pcbi-1002280-g014">Figure 14</xref>:</p>
        <list list-type="bullet">
          <list-item>
            <p>Full model (F): the left (respectively, right) MC is driven by the ‘right’ (respectively, ‘left’) pace. Feedback connections between both MC are included.</p>
          </list-item>
          <list-item>
            <p>Inverted full (F): the driving effects of the pacing stimuli are inverted, when compared to model F.</p>
          </list-item>
          <list-item>
            <p>No feedback (NF): similar to F, but without the feedback connections.</p>
          </list-item>
          <list-item>
            <p>No feedback 2 (NF2): each pacing stimulus is allowed to drive both motor cortices.</p>
          </list-item>
        </list>
        <fig id="pcbi-1002280-g014" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g014</object-id>
          <label>Figure 14</label>
          <caption>
            <title>Finger-tapping task: DCM comparison set.</title>
            <p>The figure depicts the DCM comparison set we used to analyze the finger-tapping task fMRI data. This set can be partitioned into two families of models. Family 1 gathers two plausible network structures for the finger-tapping task (left pace drives right motor cortex and right pace drives left motor cortex, with and without feedback connections). Family 2 pools over two implausible motor networks subtending the finger-tapping task (allowing the left pace to drive the left motor cortex, and reciprocally).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g014" xlink:type="simple"/>
        </fig>
        <p>We know that motor action is associated with activity in the contralateral motor cortex. This establishes a point of reference for our model comparisons (akin to the “ground truth” scenario used for validating models by simulated data). We therefore assume that models F or NF best capture the motor preparation processes during the finger-tapping task. We will thus place the inference at the family level, with two families: (i) family 1: models F and NF and (ii) family 2: models IF and NF2. A selection error thus arises whenever the posterior family comparison selects family 2.</p>
        <p>We can now derive the Laplace-Chernoff risk for the two designs (blocked versus event-related). This is summarized in <xref ref-type="table" rid="pcbi-1002280-t002">Table 2</xref> above, as a function of the first-order moment of the prior densities over neural evolution parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e339" xlink:type="simple"/></inline-formula> within the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e340" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e341" xlink:type="simple"/></inline-formula>. As in the simulations, we used i.i.d. shrinkage priors for the hemodynamic evolution and observation parameters (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e342" xlink:type="simple"/></inline-formula>) and the expected noise precision was 0.05.</p>
        <table-wrap id="pcbi-1002280-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.t002</object-id><label>Table 2</label><caption>
            <title>Laplace-Chernoff risks for the event-related versus blocked design (when comparing family 1 versus family 2).</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002280-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.t002" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">event-related design</td>
                <td align="left" colspan="1" rowspan="1">blocked design</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e343" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">−1.26</td>
                <td align="left" colspan="1" rowspan="1">−1.63</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e344" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">−1.21</td>
                <td align="left" colspan="1" rowspan="1">−1.61</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e345" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">−0.92</td>
                <td align="left" colspan="1" rowspan="1">−1.70</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e346" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">−0.96</td>
                <td align="left" colspan="1" rowspan="1">−3.74</td>
              </tr>
            </tbody>
          </table></alternatives></table-wrap>
        <p>One can see that the Laplace-Chernoff risk is smaller for the blocked-design than for the event-related design, irrespective of the first-order moment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e347" xlink:type="simple"/></inline-formula> of the neural evolution parameters prior density. In addition, it seems that the event-related design is much less sensitive to a change in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e348" xlink:type="simple"/></inline-formula> than the blocked design.</p>
        <p>We then inverted the four models using the variational Bayesian approach under standard shrinkage priors (see section “Laplace-Chernoff risk for canonical network identification questions” above), for both subjects and both designs. <xref ref-type="fig" rid="pcbi-1002280-g015">Figure 15</xref> summarizes the inversion of model F for subject KER, under the blocked design.</p>
        <fig id="pcbi-1002280-g015" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g015</object-id>
          <label>Figure 15</label>
          <caption>
            <title>Finger-tapping task: VB inversion of model F under the blocked design (subject KER).</title>
            <p>Upper-left: estimated coupling strengths of model F, under the blocked design (subject KER). These are taken from the first-order moment of the approximate posterior density over evolution parameters. Lower-left: parameter posterior correlation matrix. Upper-right: observed versus fitted data in the right motor cortex. Lower-right: linearised impulse responses (first-order Volterra kernels) to the ‘right’ pace in both motor cortices as a function of time.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g015" xlink:type="simple"/>
        </fig>
        <p>One can see that the observed BOLD responses are well fitted by the model. Not surprisingly, inspection of the first-order Volterra kernels <xref ref-type="bibr" rid="pcbi.1002280-Friston6">[28]</xref> shows that the average response of the left MC to the ‘right’ pacing stimuli is positive and bigger in amplitude than that of the right MC (and reciprocally). Also, there are very small posterior correlations between the hemodynamic and the neuronal parameters, which reflect their identifiability. However, further inspection of the posterior correlation matrix shows that, for this particular dataset and model, the feedback connections and the driving effects of the pacing stimuli are not perfectly separable. This means that the design is not optimal for a precise estimation of these parameters. However, one can still compare the two designs in terms of how well they can discriminate the four DCMs included in the comparison set. This is summarized in <xref ref-type="fig" rid="pcbi-1002280-g016">Figure 16</xref>, which plots the free energies of the four models, for both subjects and both designs.</p>
        <fig id="pcbi-1002280-g016" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g016</object-id>
          <label>Figure 16</label>
          <caption>
            <title>Finger-tapping task: DCM comparison results.</title>
            <p>This figure plots the log-model evidences of the four DCMs included in the comparison set for both subjects (orange bars: subject KER, green bars: subject JUS) and both designs (left: event-related, right: blocked design). Green (respectively, rose) shaded areas indicate the models belonging to family 1 (respectively, family 2). Black dots show the four winning models (one per subject and per design). Note that the free energies are relative to the minimal free energy within the comparison set, for each subject and design.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g016" xlink:type="simple"/>
        </fig>
        <p>One can see that no model selection error was made under the blocked design, whereas there was one model selection error for subject JUS under the event-related design. Deriving the posterior probabilities of model families shows exactly the same result. Thus, as predicted by the Laplace-Chernoff risk (c.f. <xref ref-type="table" rid="pcbi-1002280-t002">Table 2</xref>), the observed error selection rate is higher for the event-related design than for the blocked design.</p>
        <p>One may wonder how reliable this result is, given that only two subjects were used to derive the selection error rate. This is because a solid validation of the Laplace-Chernoff risk necessitates an estimate of the model selection error rate in terms of the frequency of incorrect model selections (as in section “Evaluation of the model selection error bounds”). We thus performed the following analysis:</p>
        <p>For each subject and each design, we first split the data (and the stimulation sequence) into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e349" xlink:type="simple"/></inline-formula> consecutive segments (see <xref ref-type="fig" rid="pcbi-1002280-g017">Figure 17</xref>). This allows us to artificially inflate the number of subjects (by five and ten, respectively), at the cost of reducing the effective sample size for each ‘subject’. We can then derive the Laplace-Chernoff risks for the splitting procedure, i.e.: (i) no split (as above), (ii) split into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e350" xlink:type="simple"/></inline-formula> segments and (iii) split into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e351" xlink:type="simple"/></inline-formula> segments. In addition, we can conduct a complete analysis for each segment independently of each other; i.e., invert the four DCMs included in the comparison set, derive the posterior probabilities over model families, and perform the comparison. The cost of this procedure is a loss of total degrees of freedom (and thus model discriminability power), since we allow the model parameters to vary between each data segment. However, this allows us to artificially increase the number of model selections, by considering each segment as a dummy subject. Note that the posterior probability of family 2 <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e352" xlink:type="simple"/></inline-formula> measures the objective probability of making a model selection error (see Equation 6). Averaging <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e353" xlink:type="simple"/></inline-formula> across segments and subjects thus provides an approximation to the true selection error rate under both designs (see Equation 7). This serves as sampled reference for the Laplace-Chernoff risk. <xref ref-type="fig" rid="pcbi-1002280-g017">Figure 17</xref> summarizes the results of this analysis.</p>
        <fig id="pcbi-1002280-g017" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002280.g017</object-id>
          <label>Figure 17</label>
          <caption>
            <title>Finger-tapping task: splitting analysis.</title>
            <p>This figures summarizes the results of the splitting analysis (see main text), in terms of the relationship between the Laplace-Chernoff risk and the observed model selection error rate. Left: splitting procedure. The complete data and input sequence (one per subject and per design) is split into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e354" xlink:type="simple"/></inline-formula> segments, each of which is analyzed independently. Right: the average (across segments and subjects) probability of making a model selection mistake (i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e355" xlink:type="simple"/></inline-formula>) is plotted as a function of the Laplace-Chernoff risk, for both designs (blue: event-related, red: blocked). Each point corresponds to a different splitting procedure (no split, split into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e356" xlink:type="simple"/></inline-formula> segments, split into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e357" xlink:type="simple"/></inline-formula> segments).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.g017" xlink:type="simple"/>
        </fig>
        <p>First, one can see that the Laplace-Chernoff risk of the blocked design is always smaller than that of the event-related design, irrespective of the number of splits. Second, this difference decreases as the number of splits increases. The average selection error rate exactly reproduces this pattern. First, the observed error rate is higher for the event-related design than for the blocked design, irrespective of the number of splits. Second, this difference decreases as the number of splits increases. However, in this example, the Laplace-Chernoff risks increases as the number of splits increases, irrespective of the particular design used. This is in contradiction with the observed selection error rate, which seems to increase as the number of splits increases, only for the blocked design (as opposed to the event-related design). This might be due to a different optimal balance between number of subjects and sample size per subject for the two designs. We will comment on these issues in the <xref ref-type="sec" rid="s4">discussion</xref>. Nevertheless, this splitting procedure provides further evidence that the Laplace-Chernoff risk is a reliable predictor of the average selection error rate, and hence a useful metric for comparing experimental designs.</p>
      </sec>
    </sec>
    <sec id="s4">
      <title>Discussion</title>
      <p>In this article, we have proposed a general method for optimizing the experimental design to maximise the sensitivity of subsequent Bayesian model selection. We have examined design optimization in the specific context of effective connectivity methods for fMRI and have focused on how to best decide among hypotheses about network structure and the contextual modulation of specific connections therein. We reiterate, however, that our method is very general and is applicable to any generative model of observed data (e.g., brain activity or behavioural responses, c.f., e.g., <xref ref-type="bibr" rid="pcbi.1002280-Daunizeau2">[29]</xref>).</p>
      <p>Our method relies upon the definition of a statistical risk, in terms of an approximate information theoretic bound on the model selection error rate. Theoretical and numerical evaluations of the proposed Laplace-Chernoff risk demonstrate its reliability. This optimality criterion was then applied to the problem of optimising design when identifying the structure of brain networks using DCM for fMRI data. Using both numerical evaluations and empirical fMRI data, we examined the impact of the priors (on model parameters), the level of inference (model versus family) and the specific question about network structure (the model comparison set) on the optimal experimental design. For example, we have shown that asking whether a feedback connection exists requires shorter epoch durations than when asking whether there is a contextual modulation of a feedforward connection. In addition, our empirical results suggest that the method has good predictive validity (as established with the splitting analysis). In the following, we discuss the strengths and limitations of the approach as well as potential extensions.</p>
      <p>First, one may wonder how general the proposed design optimality criterion for (Bayesian) model comparison is. In other words, one could start from a completely different perspective and ask whether it would be possible to derive another design optimality criterion that would eventually yield another optimal design for the same model comparison set. A first response to this question draws on the equivalence with the classical design efficiency (c.f. section “Tightness of the Laplace-Chernoff bounds”), which shows that in specific circumstances (flat priors, nested linear models); the Laplace-Chernoff risk is monotonically related to frequentist statistical power. We conjecture this to be a very general statement that applies whenever Bayesian model comparison can be reduced to classical hypothesis testing (in the frequentist limit). This is important, since it means that the Laplace-Chernoff optimal design would be no different from established classical designs. Interestingly, it seems that the use of the Jensen-Shannon divergence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e358" xlink:type="simple"/></inline-formula> for design optimality can be justified from purely information theoretic considerations, without reference to the model selection error rate <xref ref-type="bibr" rid="pcbi.1002280-Busetto1">[30]</xref>–<xref ref-type="bibr" rid="pcbi.1002280-Busetto2">[31]</xref>. The degree to which the two approaches are similar (and/or generalize other schemes such as classical design efficiency) will be the focus of subsequent publications, in collaboration with these authors (evidence in favour of the equivalence between the two frameworks arose from a very recent informal meeting with Dr. A. G. Busetto, who independently derived his own approach). In our opinion, the most relevant line of work, in this context, is to finesse the necessary approximations to the Jensen-Shannon divergence. This is because different approximations to the Chernoff bound could lead to different approximate optimal designs. We will discuss this particular issue below.</p>
      <p>The numerical simulations we have conducted identified general factors that have an unambiguous influence on design efficiency, namely: the number of models and the data dimension (see section “Tightness of the Laplace-Chernoff bounds”), as well as the signal-to-noise ratio (SNR, see section “Evaluation of the model selection error bounds”). Note that increasing the data dimension enables two (or more) models to make distinct predictions, provided that their respective predictive densities differ sufficiently (c.f. <xref ref-type="fig" rid="pcbi-1002280-g003">Figure 3</xref>). This is because uncontrolled variability in the data can be averaged out. In other terms, increasing the data dimension simply increases the effective SNR. In summary, the overall discriminative power of any design increases with the effective SNR, and decreases with the number of models. Both the effective SNR and the typical number of models will usually depend upon the modelling context. We have presented numerical simulations (and empirical data analyses) that span the realistic range of the effective SNR, when analyzing fMRI data with DCM. Typically, one would focus on a set of two to five regions of interest, with fifteen minutes session duration (i.e., for typical fMRI sampling rates, the data dimension is of order 10<sup>3</sup>). The SNR may depend upon the anatomical location of the network (e.g., lower SNR for subcortical compared to cortical structures), but should be of the order of 1 dB. In terms of the size of the comparison set, we have deliberately chosen to keep this small; although it can vary from one study to the next, depending upon network dimensionality and prior knowledge. However, we anticipate that hypothesis-driven experiments that would benefit from design optimization will focus on the comparison of a handful of models or families of models (see <xref ref-type="supplementary-material" rid="pcbi.1002280.s003">Text S3</xref>). In other words it may be difficult to design a study that can discriminate efficiently among a few thousands of models (or more). This is because of the inevitable dilution of experimental evidence across models (see, e.g., <xref ref-type="bibr" rid="pcbi.1002280-Penny2">[23]</xref>). Recall that the exact probability of making a model selection error can be evaluated a posteriori, following Equation 6. Typically, the winning model among a few thousand alternatives will never attain a posterior probability of about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e359" xlink:type="simple"/></inline-formula>, which leads to an unacceptable model selection error probability of at least 0.9!</p>
      <p>Second, one may ask whether the Laplace-Chernoff risk is a suitable criterion for choosing among potential designs within the context of a group analysis. This is because we did not consider a (more general) hierarchical scenario, which would account explicitly for the variability of the hidden model within a group of subjects (i.e., random effects analysis <xref ref-type="bibr" rid="pcbi.1002280-Stephan3">[32]</xref>). In this case, the total variability consists of within- and between-subject sources of variation. So far, our approach consists of optimizing the experimental design by controlling the variability at the within-subject level. This is done by optimizing the discriminability of models included in the comparison set. In essence, this is similar to design optimization for classical GLM analyses, where optimality is defined in relation to the reliability of maximum likelihood estimators. In this context, one can find an optimal balance between the number of subjects and the sampling size per subject <xref ref-type="bibr" rid="pcbi.1002280-Moerbeek1">[33]</xref>. This balance strives for a principled way of choosing, for example, between a study with twenty subjects scanned for fifteen minutes each versus a study with ten subjects scanned for half an hour each. In <xref ref-type="bibr" rid="pcbi.1002280-Maus1">[34]</xref>, authors demonstrate how this balance depends upon the ratio of within- and between- subject variances. Our analysis of the empirical data seems to disclose a similar dependency (<xref ref-type="fig" rid="pcbi-1002280-g017">Figure 17</xref>). In brief, the relationship between the average error rate and the sharing of degrees of freedom (across the within- and between-subject levels) depends upon the design type (i.e. blocked versus event-related). The results in sections “Laplace-Chernoff risk for canonical network identification questions” and “Investigating psycho-physiological interactions with DCM” imply that it may depend upon the comparison set as well. In addition, one has to consider two sorts of random effects here: variability in the model parameters (for a fixed model), and variability in the hidden model itself. Future work will consider these issues when extending the present approach to a multi-level random effects analysis for group data.</p>
      <p>Third, the Laplace-Chernoff bound relies upon the derivation of the prior predictive density of each model included in the comparison set. For nonlinear models, it relies upon a local linearization around the prior mean of the parameters; similarly to classical procedures for design optimization (see, e.g., <xref ref-type="bibr" rid="pcbi.1002280-Maus2">[35]</xref> for an application to estimating the hemodynamic response function). We are currently evaluating the potential benefit of using variants of the unscented transform <xref ref-type="bibr" rid="pcbi.1002280-Topsoe1">[22]</xref>, which may yield a more accurate approximation to the prior predictive density. We have not, however, accounted for uncertainty on hyperparameters; e.g., moments of the prior density on noise precision. Note that we do not expect this to be crucial because the contribution of the prior uncertainty on these hyperparameters is negligible, when compared to the variability already induced in the prior predictive densities.</p>
      <p>Nevertheless, the above approximations induce potential limitations for the current approach. For example, numerical simulations in sections “Tightness of the Laplace-Chernoff bounds” and “Results” demonstrate that the Laplace approximation might cause the bound to “break”, i.e. the Laplace-Chernoff risk might become an over-optimistic estimate of the model selection error rate. More precisely, this happens in situations where the exact model selection error rate is already very low (typically below 0.2, see <xref ref-type="fig" rid="pcbi-1002280-g003">Figure 3</xref>). Having said this, the relationship between the Laplace-Chernoff risk and the exact model selection error rate always remained monotonic. This means that the design that minimizes the Laplace-Chernoff risk is the one that would have minimized the exact model selection error rate, had we been able to quantify it. This monotonic relationship remains to be empirically verified for classes of models that are more complex than DCMs.</p>
      <p>From a practical perspective, if the aim is to quantify the actual model selection error rate (or a conservative upper bound on it), then the Laplace-Chernoff risk will yield an accurate estimation only for poorly discriminative designs (importantly, the upper bound on the true model selection error rate becomes tightest for the least decisive model comparisons, i.e., the approximation by the Laplace-Chernoff risk is most accurate when it is most needed). However, in most practical applications the aim is simply to select the most discriminative design amongst several alternatives. In this case, the Laplace-Chernoff risk can be used for any model comparison.</p>
      <p>Fourth, one may consider other applications for the Laplace-Chernoff risk. For example, given an experiment whose design is fixed or cannot be specified <italic>a priori</italic> (e.g., the presence of epileptic spikes, or successful vs. failed retrieval of encoded memories), one can use our approach to distinguish between statistical questions for which the design is suitable and those for which they are not. This can be done by evaluating the Laplace-Chernoff risk for different comparison sets or partitions of the same comparison sets. This could also be useful to motivate the <italic>a priori</italic> pruning of competing hypotheses in a principled way. One could also think of using an adaptive design strategy where the paradigm is optimized online as the experiment progresses (see <xref ref-type="bibr" rid="pcbi.1002280-Grabowski1">[36]</xref>–<xref ref-type="bibr" rid="pcbi.1002280-Xie1">[37]</xref> for similar applications to fMRI). Even though such procedures will not lead to a major gain in efficiency for linear models, this can be quite different for nonlinear models of the sort employed in DCM <xref ref-type="bibr" rid="pcbi.1002280-Lewi1">[38]</xref>. This is because the progressive accumulation of information corrects the predictive densities that are required to compute the Laplace-Chernoff risk. In turn, this can be exploited to improve the overall model discriminability <xref ref-type="bibr" rid="pcbi.1002280-Cavagnaro1">[39]</xref>.</p>
      <p>Fifth, we would like to highlight some important properties of the biophysical models when optimizing the experimental design for identifying networks with DCM for fMRI data. Consider the increase in selection error rate at short epoch durations. This is likely to arise from the hemodynamic impulse response function, which induces strong correlations in the fMRI data at fast time scales, relative to its own (about 16 to 32 seconds). Such loss of discriminative power in high frequencies has been discussed in the context of design optimization for classical fMRI studies <xref ref-type="bibr" rid="pcbi.1002280-Wager1">[12]</xref>. This effect worsens at very short epoch durations, due to <italic>hemodynamic refractoriness</italic>; i.e., the response to a second stimulus is reduced if it follows the preceding stimulus with a short delay <xref ref-type="bibr" rid="pcbi.1002280-Miezin1">[40]</xref>. This saturation effect is known to be captured by the hemodynamic Balloon model that is part of DCM <xref ref-type="bibr" rid="pcbi.1002280-Friston6">[28]</xref>. Interestingly, the effect of these known phenomena on statistical efficiency depends on which particular scientific question is asked. For example, the identification of feedback connections within the network is facilitated by epoch durations that are much shorter than required for addressing other questions about effective connectivity or in conventional GLM analyses (cf. <xref ref-type="fig" rid="pcbi-1002280-g008">Figure 8</xref>). This is because a feedback connection expresses itself mainly when the system goes back to steady-state, through an asymmetrical increase in node-to-node correlation (cf. <xref ref-type="fig" rid="pcbi-1002280-g009">Figure 9</xref>). In other terms, a feedback connection manifests itself by a higher reproducibility of network decay dynamics across repetitions, which is why its detection requires short epoch durations and thus a more frequent repetition of the transient that discloses its effect on the data.</p>
      <p>Sixth, our preliminary results show that the use of interventional techniques such as TMS could be highly beneficial for reducing the selection error rate (<xref ref-type="fig" rid="pcbi-1002280-g012">Figure 12</xref>). However, the expected gain is strongly dependent upon its physiological effects, which are still not fully known <xref ref-type="bibr" rid="pcbi.1002280-Hampson1">[41]</xref>. For example, different stimulation frequencies target different populations of neurons and can therefore either have a net excitatory or inhibitory effect. Such effects can be modelled easily within the framework of DCM <xref ref-type="bibr" rid="pcbi.1002280-Marreiros1">[42]</xref> and would constitute a straightforward extension to the example given in this paper (see <xref ref-type="bibr" rid="pcbi.1002280-Husain1">[43]</xref> for related work). In the future, such extensions could allow one to ask which TMS technique one should use to maximally improve sensitivity in disclosing network mechanisms by model selection. Such combinations of experimental techniques and model-based analysis are starting to emerge in the field <xref ref-type="bibr" rid="pcbi.1002280-Sarfeld1">[44]</xref> and hold great promises for the identification of directed influences in the brain, provided that one understands the impact of the experimental design used.</p>
      <p>Lastly, numerical simulations showed that the optimal design depends upon the choice of priors on the model's parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e360" xlink:type="simple"/></inline-formula>. This is of course expected, because <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e361" xlink:type="simple"/></inline-formula> partly determines the model's prior predictive density over data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e362" xlink:type="simple"/></inline-formula> (c.f. equations 1–2). Strictly speaking, we cannot use noninformative priors when optimizing the design for model comparison. This is because, in most cases, this would induce flat prior predictive densities for all models, which would prevent any design optimization procedure. This means that we have to choose mildly informative priors for the model's parameters. However, the precise way in which the priors affect the efficiency of the design depends upon the comparison set. For example, increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e363" xlink:type="simple"/></inline-formula> (the prior mean over the connectivity parameters) either increases model discriminability (e.g., <xref ref-type="fig" rid="pcbi-1002280-g010">Figure 10</xref>, for the feedback/no feedback comparison) or decreases it (e.g., <xref ref-type="fig" rid="pcbi-1002280-g010">Figure 10</xref>, when deciding where the input enters the network). Recall that a (generative) model is defined by all the (probabilistic) assumptions that describe how the data are generated, including the prior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e364" xlink:type="simple"/></inline-formula>. This means that when using different values for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e365" xlink:type="simple"/></inline-formula>, we are effectively defining different models. Thus, varying both <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e366" xlink:type="simple"/></inline-formula> and the connectivity structure implicitly augments the comparison set in a factorial way. Assuming that one is only interested in selecting the connectivity structure (irrespective of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e367" xlink:type="simple"/></inline-formula>), one has to resort to family inference (see <xref ref-type="supplementary-material" rid="pcbi.1002280.s003">Text S3</xref>), where each family is composed of members that share the same connectivity structure but differ in their <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e368" xlink:type="simple"/></inline-formula>. This simply means deriving the Laplace-Chernoff risk after marginalizing over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e369" xlink:type="simple"/></inline-formula>. This basically treats <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002280.e370" xlink:type="simple"/></inline-formula> as a nuisance effect, and de-sensitizes the design parameter of interest to mathematical variations in the implementation of the model. We have shown examples of such a “family level” extension of optimal designs when inspecting canonical PPI models (section “Investigating psycho-physiological interactions with DCM”) and analyzing experimental data (section “Empirical validation”).</p>
      <p>Similarly, one might wonder how sensitive the optimal design is to variations of the neuronal and biophysical state equations used in the DCM framework. Preliminary results (not shown here) indicate that the effects of design parameters such as epoch duration are not very sensitive to such variations, e.g., two-state DCM <xref ref-type="bibr" rid="pcbi.1002280-Marreiros1">[42]</xref> or stochastic DCM <xref ref-type="bibr" rid="pcbi.1002280-Daunizeau3">[45]</xref>–<xref ref-type="bibr" rid="pcbi.1002280-Li1">[46]</xref>. However, the latter class of DCM asks for a slight modification in the derivation of the prior predictive density <xref ref-type="bibr" rid="pcbi.1002280-Daunizeau4">[47]</xref>. This is because the presence of neural noise induces additional variability at the level of hidden states. Typically, neural noise expresses itself through a decrease in lagged (intra- and inter-node) covariances. This might therefore induce noticeable changes in optimal design parameters for specific comparison sets. A general solution to this is to include the DCM variant as a factor in the model comparison set, and then again, use family level inference to marginalize over it.</p>
      <p>We envisage that the present approach will be useful for a wide range of practical applications in neuroimaging and beyond. It may be particularly helpful in a clinical context, where the ability to disambiguate alternative diseases mechanisms with high sensitivity is of great diagnostic importance. One particular application domain we have in mind for future studies concerns the classification of patients from spectrum diseases such as schizophrenia using mechanistically interpretable models <xref ref-type="bibr" rid="pcbi.1002280-Stephan4">[48]</xref>. Another potential future application concerns model-based prediction of individual treatment responses, based on experimentally elicited physiological responses (e.g., to pharmacological challenges <xref ref-type="bibr" rid="pcbi.1002280-Moran1">[49]</xref>). Either approach will greatly benefit from methods for optimizing experimental design, such as the one introduced here.</p>
      <sec id="s4a">
        <title>Software note</title>
        <p>All the routines and ideas described in this paper will be implemented in the academic freeware SPM (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm" xlink:type="simple">http://www.fil.ion.ucl.ac.uk/spm</ext-link>).</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002280.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.s001" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <p>
            <bold>The Laplace approximation to the Jensen-Shannon bound.</bold>
          </p>
          <p>(DOCX)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002280.s002" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.s002" xlink:type="simple">
        <label>Text S2</label>
        <caption>
          <p>
            <bold>The Laplace-Chernoff risk for the general linear model and its frequentist limit.</bold>
          </p>
          <p>(DOCX)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002280.s003" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002280.s003" xlink:type="simple">
        <label>Text S3</label>
        <caption>
          <p>
            <bold>Extension to the comparison of model families.</bold>
          </p>
          <p>(DOCX)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002280-Friston1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>             <year>2011</year>             <article-title>Functional and Effective Connectivity: A Review.</article-title>             <source>Brain Connectivity</source>             <volume>1</volume>             <fpage>13</fpage>             <lpage>36</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Stephan1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name></person-group>             <year>2004</year>             <article-title>On the role of general system theory for functional neuroimaging.</article-title>             <source>J Anat</source>             <volume>205</volume>             <fpage>443</fpage>             <lpage>470</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-McIntosh1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>McIntosh</surname><given-names>AR</given-names></name><name name-style="western"><surname>Gonzalez-Lima</surname><given-names>F</given-names></name></person-group>             <year>1994</year>             <article-title>Structural equation modeling and its application to network analysis in functional brain imaging.</article-title>             <source>Hum Brain Mapp</source>             <volume>2</volume>             <fpage>2</fpage>             <lpage>22</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Friston2">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><name name-style="western"><surname>Harrison</surname><given-names>L</given-names></name><name name-style="western"><surname>Penny</surname><given-names>WD</given-names></name></person-group>             <year>2003</year>             <article-title>Dynamic Causal Modelling.</article-title>             <source>Neuroimage</source>             <volume>19</volume>             <fpage>1273</fpage>             <lpage>1302</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Penny1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Penny</surname><given-names>W</given-names></name><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name><name name-style="western"><surname>Mechelli</surname><given-names>A</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>             <year>2004</year>             <article-title>Comparing Dynamic Causal Models.</article-title>             <source>Neuroimage</source>             <volume>22</volume>             <fpage>1157</fpage>             <lpage>1172</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Josephs1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Josephs</surname><given-names>O</given-names></name><name name-style="western"><surname>Henson</surname><given-names>RN</given-names></name></person-group>             <year>1999</year>             <article-title>Event-related functional magnetic resonance imaging: modelling, inference and optimization.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>354</volume>             <fpage>1215</fpage>             <lpage>1228</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Liu1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>TT</given-names></name><name name-style="western"><surname>Frank</surname><given-names>LR</given-names></name><name name-style="western"><surname>Wong</surname><given-names>EC</given-names></name><name name-style="western"><surname>Buxton</surname><given-names>RB</given-names></name></person-group>             <year>2001</year>             <article-title>Detection power, estimation efficiency, and predictability in event-related fMRI.</article-title>             <source>Neuroimage</source>             <volume>13</volume>             <fpage>759</fpage>             <lpage>773</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Zahran1">
        <label>8</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zahran</surname><given-names>AR</given-names></name></person-group>             <year>2002</year>             <article-title>On the efficiency of designs for linear models in non-regular regions and the use of standard designs for generalized linear models.</article-title>             <comment>PhD thesis, Virginia Polytechnic Institute and State University, USA</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Mechelli1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mechelli</surname><given-names>A</given-names></name><name name-style="western"><surname>Price</surname><given-names>CJ</given-names></name><name name-style="western"><surname>Henson</surname><given-names>RN</given-names></name></person-group>             <year>2003</year>             <article-title>The effect of high-pass filtering on the efficiency of response estimation: a comparison between blocked and randomised designs.</article-title>             <source>Neuroimage</source>             <volume>18</volume>             <fpage>798</fpage>             <lpage>805</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Henson1">
        <label>10</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Henson</surname><given-names>R</given-names></name></person-group>             <year>2007</year>             <article-title>Efficient experimental design for fMRI.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><name name-style="western"><surname>Ashburner</surname><given-names>JT</given-names></name><name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Nichols</surname><given-names>TE</given-names></name><name name-style="western"><surname>Penny</surname><given-names>WD</given-names></name></person-group>             <source>Statistical Parametric Mapping</source>             <publisher-name>Academic Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Friston3">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name></person-group>             <year>2002</year>             <article-title>Beyond phrenology: what can neuroimaging tell us about distributed circuitry?</article-title>             <source>Annu Rev Neurosci</source>             <volume>25</volume>             <fpage>221</fpage>             <lpage>250</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Wager1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wager</surname><given-names>TD</given-names></name><name name-style="western"><surname>Nichols</surname><given-names>TE</given-names></name></person-group>             <year>2003</year>             <article-title>Optimization of experimental design in fMRI: a general framework using a genetic algorithm.</article-title>             <source>Neuroimage</source>             <volume>18</volume>             <fpage>293</fpage>             <lpage>309</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-McIntosh2">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>McIntosh</surname><given-names>AR</given-names></name></person-group>             <year>2000</year>             <article-title>Towards a network theory of cognition.</article-title>             <source>Neural Netw</source>             <volume>13</volume>             <fpage>861</fpage>             <lpage>870</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Daunizeau1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><name name-style="western"><surname>David</surname><given-names>O</given-names></name><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name></person-group>             <year>2010</year>             <article-title>Dynamic Causal Modelling: a critical review of the biophysical and statistical foundations.</article-title>             <source>Neuroimage</source>             <volume>58</volume>             <fpage>312</fpage>             <lpage>322</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Smith1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>SM</given-names></name><name name-style="western"><surname>Miller</surname><given-names>KL</given-names></name><name name-style="western"><surname>Salimi-Khorshidi</surname><given-names>G</given-names></name><name name-style="western"><surname>Webster</surname><given-names>M</given-names></name><etal/></person-group>             <year>2011</year>             <article-title>Network modelling methods for FMRI.</article-title>             <source>Neuro Image</source>             <volume>54</volume>             <fpage>875</fpage>             <lpage>891</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Myung1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Myung</surname><given-names>JI</given-names></name><name name-style="western"><surname>Pitt</surname><given-names>MA</given-names></name></person-group>             <year>2009</year>             <article-title>Optimal experimental design for model discrimination.</article-title>             <source>Psychol Rev</source>             <volume>116</volume>             <fpage>499</fpage>             <lpage>518</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Chaloner1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chaloner</surname><given-names>K</given-names></name><name name-style="western"><surname>Verdini</surname><given-names>I</given-names></name></person-group>             <year>1995</year>             <article-title>Bayesian experimental design: a review.</article-title>             <source>Statist Sci</source>             <volume>10</volume>             <fpage>273</fpage>             <lpage>304</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Lindley1">
        <label>18</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lindley</surname><given-names>DV</given-names></name></person-group>             <year>1972</year>             <source>Bayesian statistics – a review</source>             <publisher-loc>Philadelphia</publisher-loc>             <publisher-name>SIAM</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Robert1">
        <label>19</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Robert</surname><given-names>C</given-names></name></person-group>             <year>1992</year>             <source>L'analyse statistique Bayesienne</source>             <publisher-loc>Paris</publisher-loc>             <publisher-name>Economica</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Neyman1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Neyman</surname><given-names>J</given-names></name><name name-style="western"><surname>Pearson</surname><given-names>E</given-names></name></person-group>             <year>1933</year>             <article-title>On the problem of the most efficient tests of statistical hypotheses.</article-title>             <source>Philos Trans Roy Soc Lond A</source>             <volume>231</volume>             <fpage>289</fpage>             <lpage>337</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Lin1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>J</given-names></name></person-group>             <year>1991</year>             <article-title>Divergence measures based on the Shannon entropy.</article-title>             <source>IEEE Trans Inform Theory</source>             <volume>37</volume>             <fpage>151</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Topsoe1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Topsoe</surname><given-names>F</given-names></name></person-group>             <year>2000</year>             <article-title>Some inequalities for information divergence and related measures of discrimination.</article-title>             <source>IEEE Trans Inform Theory</source>             <volume>46</volume>             <fpage>1602</fpage>             <lpage>1609</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Penny2">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Penny</surname><given-names>W</given-names></name><name name-style="western"><surname>Joao</surname><given-names>M</given-names></name><name name-style="western"><surname>Flandin</surname><given-names>G</given-names></name><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Comparing Families of Dynamic Causal Models.</article-title>             <source>PLoS Comp Biol</source>             <volume>6</volume>             <fpage>e1000709</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Stephan2">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name><name name-style="western"><surname>Kasper</surname><given-names>L</given-names></name><name name-style="western"><surname>Harrison</surname><given-names>L</given-names></name><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><etal/></person-group>             <year>2008</year>             <article-title>Nonlinear dynamic causal models for fMRI.</article-title>             <source>Neuroimage</source>             <volume>42</volume>             <fpage>649</fpage>             <lpage>662</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Friston4">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><name name-style="western"><surname>Mattout</surname><given-names>J</given-names></name><name name-style="western"><surname>Trujillo-Barreto</surname><given-names>N</given-names></name><name name-style="western"><surname>Ashburner</surname><given-names>J</given-names></name><etal/></person-group>             <year>2007</year>             <article-title>Variational free energy and the Laplace approximation.</article-title>             <source>Neuroimage</source>             <volume>34</volume>             <fpage>220</fpage>             <lpage>234</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Friston5">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><name name-style="western"><surname>Büchel</surname><given-names>C</given-names></name><name name-style="western"><surname>Fink</surname><given-names>GR</given-names></name><name name-style="western"><surname>Morris</surname><given-names>J</given-names></name><etal/></person-group>             <year>1997</year>             <article-title>Psychophysiological and modulatory interactions in neuroimaging.</article-title>             <source>Neuro Image</source>             <volume>6</volume>             <fpage>218</fpage>             <lpage>229</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Gitelman1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gitelman</surname><given-names>DR</given-names></name><name name-style="western"><surname>Penny</surname><given-names>WD</given-names></name><name name-style="western"><surname>Ashburner</surname><given-names>J</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>             <year>2003</year>             <article-title>Modeling regional and psychophysiologic interactions in fMRI: the importance of hemodynamic deconvolution.</article-title>             <source>Neuroimage</source>             <volume>19</volume>             <fpage>200</fpage>             <lpage>207</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Friston6">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><name name-style="western"><surname>Mechelli</surname><given-names>R</given-names></name><name name-style="western"><surname>Turner</surname><given-names>R</given-names></name><name name-style="western"><surname>Price</surname><given-names>CJ</given-names></name></person-group>             <year>2002</year>             <article-title>Nonlinear response in fMRI: the balloon model, Volterra kernels and other hemodynamics.</article-title>             <source>Neuroimage</source>             <volume>12</volume>             <fpage>466</fpage>             <lpage>477</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Daunizeau2">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><name name-style="western"><surname>Den Ouden</surname><given-names>HEM</given-names></name><name name-style="western"><surname>Pessiglione</surname><given-names>M</given-names></name><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Observing the observer (I): meta-Bayesian models of learning and decision making.</article-title>             <source>PLoS ONE</source>             <volume>5</volume>             <fpage>e15554</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Busetto1">
        <label>30</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Busetto</surname><given-names>AG</given-names></name><name name-style="western"><surname>Ong</surname><given-names>CS</given-names></name><name name-style="western"><surname>Buhmann</surname><given-names>JM</given-names></name></person-group>             <year>2009</year>             <article-title>Optimized expected information gain for nonlinear dynamical systems.</article-title>             <fpage>97</fpage>             <lpage>104</lpage>             <comment>In: Association for Computing Machinery (ACM) Int. Conf. Proc. Series 382 Proc. of the 26th Int. Conf. on Machine Learning (ICML 09); 14–18 June 2009; Montreal, Quebec, Canada</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Busetto2">
        <label>31</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Busetto</surname><given-names>AG</given-names></name><name name-style="western"><surname>Buhmann</surname><given-names>JM</given-names></name></person-group>             <year>2009</year>             <article-title>Structure Identification by Optimized Interventions.</article-title>             <fpage>57</fpage>             <lpage>64</lpage>             <comment>J. Machine Learn. Res. (JMLR) Proc. of the 12th Int. Conf. on Artific. Intell. and Stat. (AISTATS 09);16–18 April 2009; Florida, United States</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Stephan3">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name><name name-style="western"><surname>Penny</surname><given-names>WD</given-names></name><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><name name-style="western"><surname>Moran</surname><given-names>RJ</given-names></name><etal/></person-group>             <year>2009a</year>             <article-title>Bayesian model selection for group studies.</article-title>             <source>Neuroimage</source>             <volume>46</volume>             <fpage>1004</fpage>             <lpage>1017</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Moerbeek1">
        <label>33</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Moerbeek</surname><given-names>M</given-names></name><name name-style="western"><surname>Van Breukelen</surname><given-names>GJP</given-names></name><name name-style="western"><surname>Berger</surname><given-names>MPF</given-names></name></person-group>             <year>2008</year>             <article-title>Optimal designs for multilevel studies.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>de Leeuw</surname><given-names>J</given-names></name><name name-style="western"><surname>Meijer</surname><given-names>E</given-names></name></person-group>             <source>Handbook of Multilevel Analysis</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Springer</publisher-name>             <fpage>177</fpage>             <lpage>206</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Maus1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Maus</surname><given-names>B</given-names></name><name name-style="western"><surname>Van Breukelen</surname><given-names>GJP</given-names></name><name name-style="western"><surname>Goebel</surname><given-names>R</given-names></name><name name-style="western"><surname>Berger</surname><given-names>MPF</given-names></name></person-group>             <year>2011a</year>             <article-title>Optimal design of multi-subject blocked fMRI experiments.</article-title>             <source>Neuroimage</source>             <volume>56</volume>             <fpage>1338</fpage>             <lpage>1352</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Maus2">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Maus</surname><given-names>B</given-names></name><name name-style="western"><surname>Van Breukelen</surname><given-names>GJP</given-names></name><name name-style="western"><surname>Goebel</surname><given-names>R</given-names></name><name name-style="western"><surname>Berger</surname><given-names>MPF</given-names></name></person-group>             <year>2011b</year>             <article-title>Optimal design for nonlinear estimation of the hemodynamic response function.</article-title>             <source>Hum Brain Mapp</source>             <comment>in press. doi: 10.1002/hbm.21289</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Grabowski1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Grabowski</surname><given-names>TJ</given-names></name><name name-style="western"><surname>Bauer</surname><given-names>MD</given-names></name><name name-style="western"><surname>Foreman</surname><given-names>D</given-names></name><name name-style="western"><surname>Mehta</surname><given-names>S</given-names></name><etal/></person-group>             <year>2006</year>             <article-title>Adaptive pacing of visual stimulation for fMRI studies involving overt speech.</article-title>             <source>Neuro Image</source>             <volume>29</volume>             <fpage>1023</fpage>             <lpage>1030</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Xie1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>J</given-names></name><name name-style="western"><surname>Clare</surname><given-names>S</given-names></name><name name-style="western"><surname>Gallichan</surname><given-names>D</given-names></name><name name-style="western"><surname>Gunn</surname><given-names>RN</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Real-time adaptive sequential design for optimal acquisition of arterial spin labeling MRI data.</article-title>             <source>Magn Reson Med</source>             <volume>64</volume>             <fpage>203</fpage>             <lpage>10</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Lewi1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lewi</surname><given-names>J</given-names></name><name name-style="western"><surname>Butera</surname><given-names>R</given-names></name><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name></person-group>             <year>2009</year>             <article-title>Sequential optimal design of neurophysiology experiments.</article-title>             <source>Neural Comp</source>             <volume>21</volume>             <fpage>619</fpage>             <lpage>687</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Cavagnaro1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cavagnaro</surname><given-names>DR</given-names></name><name name-style="western"><surname>Myung</surname><given-names>JL</given-names></name><name name-style="western"><surname>Pitt</surname><given-names>MA</given-names></name></person-group>             <year>2010</year>             <article-title>Adaptive design optimization: a mutual information-based approach to model discrimination in cognitive sciences.</article-title>             <source>Neural Comp</source>             <volume>22</volume>             <fpage>887</fpage>             <lpage>905</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Miezin1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Miezin</surname><given-names>FM</given-names></name><name name-style="western"><surname>Maccotta</surname><given-names>L</given-names></name><name name-style="western"><surname>Ollinger</surname><given-names>JM</given-names></name></person-group>             <year>2000</year>             <article-title>Characterizing the hemodynamic response: effects of presentation rate, sampling procedure, and the possibility of ordering brain activity based on relative timing.</article-title>             <source>Neuroimage</source>             <volume>11</volume>             <fpage>735</fpage>             <lpage>759</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Hampson1">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hampson</surname><given-names>M</given-names></name><name name-style="western"><surname>Hoffman</surname><given-names>RE</given-names></name></person-group>             <year>2010</year>             <article-title>Transcranial magnetic stimulation and connectivity mapping: tools for studying the neural bases of brain disorders.</article-title>             <source>Front Syst Neurosci</source>             <volume>4</volume>             <fpage>40</fpage>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnsys.2010.00040" xlink:type="simple">10.3389/fnsys.2010.00040</ext-link></comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Marreiros1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Marreiros</surname><given-names>AC</given-names></name><name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>             <year>2008</year>             <article-title>Dynamic causal modelling for fMRI: A two-state model.</article-title>             <source>Neuro Image</source>             <volume>39</volume>             <fpage>269</fpage>             <lpage>278</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Husain1">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Husain</surname><given-names>FT</given-names></name><name name-style="western"><surname>Nandipati</surname><given-names>G</given-names></name><name name-style="western"><surname>Braun</surname><given-names>AR</given-names></name><name name-style="western"><surname>Cohen</surname><given-names>LG</given-names></name><etal/></person-group>             <year>2002</year>             <article-title>Simulating transcranial magnetic stimulation during PET with a large-scale neural network model of the prefrontal cortex and the visual system.</article-title>             <source>Neuroimage</source>             <volume>15</volume>             <fpage>58</fpage>             <lpage>73</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Sarfeld1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sarfeld</surname><given-names>AS</given-names></name><name name-style="western"><surname>Diekhoff</surname><given-names>S</given-names></name><name name-style="western"><surname>Wang</surname><given-names>LE</given-names></name><name name-style="western"><surname>Liuzzi</surname><given-names>G</given-names></name><etal/></person-group>             <year>2011</year>             <article-title>Convergence of human brain mapping tools: Neuronavigated TMS Parameters and fMRI activity in the hand motor area.</article-title>             <source>Hum Brain Mapp</source>             <comment>in press. doi: 10.1002/hbm.21272</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Daunizeau3">
        <label>45</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><name name-style="western"><surname>Kiebel</surname><given-names>S</given-names></name><name name-style="western"><surname>Lemieux</surname><given-names>L</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Stochastic nonlinear DCM for fMRI: neural noise and network dynamics.</article-title>             <comment>Proc. of the Hum. Brain Mapp. Conf. (OHBM 2010)</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Li1">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>B</given-names></name><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name><name name-style="western"><surname>Penny</surname><given-names>W</given-names></name><etal/></person-group>             <year>2011</year>             <article-title>Stochastic DCM and generalized filtering.</article-title>             <source>Neuro Image</source>             <volume>58</volume>             <fpage>442</fpage>             <lpage>457</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Daunizeau4">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name></person-group>             <year>2009</year>             <article-title>Variational Bayesian identification and prediction of stochastic nonlinear dynamic causal models.</article-title>             <source>Physica D</source>             <volume>238</volume>             <fpage>2089</fpage>             <lpage>2118</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Stephan4">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><name name-style="western"><surname>Frith</surname><given-names>CD</given-names></name></person-group>             <year>2009b</year>             <article-title>Dysconnection in schizophrenia: From abnormal synaptic plasticity to failures of self-monitoring.</article-title>             <source>Schizophr Bull</source>             <volume>35</volume>             <fpage>509</fpage>             <lpage>527</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002280-Moran1">
        <label>49</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Moran</surname><given-names>RJ</given-names></name><name name-style="western"><surname>Symmonds</surname><given-names>M</given-names></name><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><etal/></person-group>             <year>2011</year>             <article-title>An in vivo assay of synaptic function mediating human cognition.</article-title>             <source>Curr Biol</source>             <volume>21</volume>             <fpage>1320</fpage>             <lpage>1325</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>