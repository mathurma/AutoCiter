<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-01811</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002691</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Spike-based Decision Learning of Nash Equilibria in Two-Player Games</article-title><alt-title alt-title-type="running-head">Decision Learning in Two-Player Games</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Friedrich</surname>
            <given-names>Johannes</given-names>
          </name>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Senn</surname>
            <given-names>Walter</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1">
        <addr-line>Department of Physiology and Center for Cognition, Learning and Memory, University of Bern, Switzerland</addr-line>
      </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">
        <addr-line>Indiana University, United States of America</addr-line>
      </aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">senn@pyl.unibe.ch</email></corresp>
        <fn fn-type="conflict">
          <p>The authors have declared that no competing interests exist.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: JF WS. Performed the experiments: JF. Analyzed the data: JF WS. Wrote the paper: JF WS.</p>
        </fn>
      </author-notes><pub-date pub-type="collection">
        <month>9</month>
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>27</day>
        <month>9</month>
        <year>2012</year>
      </pub-date><volume>8</volume><issue>9</issue><elocation-id>e1002691</elocation-id><history>
        <date date-type="received">
          <day>5</day>
          <month>12</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>26</day>
          <month>7</month>
          <year>2012</year>
        </date>
      </history><permissions>
        
        <copyright-holder>Friedrich, Senn</copyright-holder>
        <license xlink:type="simple">
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
        </license>
      </permissions><abstract>
        <p>Humans and animals face decision tasks in an uncertain multi-agent environment where an agent's strategy may change in time due to the co-adaptation of others strategies. The neuronal substrate and the computational algorithms underlying such adaptive decision making, however, is largely unknown. We propose a population coding model of spiking neurons with a policy gradient procedure that successfully acquires optimal strategies for classical game-theoretical tasks. The suggested population reinforcement learning reproduces data from human behavioral experiments for the blackjack and the inspector game. It performs optimally according to a pure (deterministic) and mixed (stochastic) Nash equilibrium, respectively. In contrast, temporal-difference(TD)-learning, covariance-learning, and basic reinforcement learning fail to perform optimally for the stochastic strategy. Spike-based population reinforcement learning, shown to follow the stochastic reward gradient, is therefore a viable candidate to explain automated decision learning of a Nash equilibrium in two-player games.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Socio-economic interactions are captured in a game theoretic framework by multiple agents acting on a pool of goods to maximize their own reward. Neuroeconomics tries to explain the agent's behavior in neuronal terms. Classical models in neuroeconomics use temporal-difference(TD)-learning. This algorithm incrementally updates values of state-action pairs, and actions are selected according to a value-based policy. In contrast, policy gradient methods do not introduce values as intermediate steps, but directly derive an action selection policy which maximizes the total expected reward. We consider a decision making network consisting of a population of neurons which, upon presentation of a spatio-temporal spike pattern, encodes binary actions by the population output spike trains and a subsequent majority vote. The action selection policy is parametrized by the strengths of synapses projecting to the population neurons. A gradient learning rule is derived which modifies these synaptic strengths and which depends on four factors, the pre- and postsynaptic activities, the action and the reward. We show that for classical game-theoretical tasks our decision making network endowed with the four-factor learning rule leads to Nash-optimal action selections. It also mimics human decision learning for these same tasks.</p>
      </abstract><funding-group>
        <funding-statement>This work was supported by a grant of the Swiss SystemsX.ch initiative (Neurochoice, evaluated by the SNSF). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group><counts>
        <page-count count="12"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Neuroeconomics is an interdisciplinary research field that tries to explain human decision making in neuronal terms. Behavioral outcomes are construed as results of brain activity and the neuronal correlates of the quantities relevant for the decision making process are identified. Humans, as economic agents, attempt to optimize some reward function by participating in the production, exchange and maintenance of goods. Reward for the individuals will depend in general not merely upon their own actions but also on those of the other players and, furthermore, these will adapt their own strategies.</p>
      <p>Classical models in neuroeconomics are based on temporal difference (TD) learning <xref ref-type="bibr" rid="pcbi.1002691-Dayan1">[1]</xref>, an algorithm to maximize the total expected reward <xref ref-type="bibr" rid="pcbi.1002691-Sutton1">[2]</xref> with potential neuronal implementations <xref ref-type="bibr" rid="pcbi.1002691-Seymour1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Potjans1">[4]</xref>. It assumes that the environment can be described as a Markov decision process (MDP), i.e. by a finite number of states with fixed transition probabilities <xref ref-type="bibr" rid="pcbi.1002691-Howard1">[5]</xref>. Multi-agent games, however, are not Markovian as the evolution of the environment typically does not only depend on the current state, but also on the history and on the adaptation of the other agents. Such games can be described as partially observable Markov decision processes (POMDP, <xref ref-type="bibr" rid="pcbi.1002691-Smallwood1">[6]</xref>) by embedding the sequences and the learning strategies of the other agents into a large state space. We have presented a policy gradient method for population reinforcement learning which, unlike TD-learning, can cope with POMDPs and can be implemented in neuronal terms <xref ref-type="bibr" rid="pcbi.1002691-Friedrich1">[7]</xref>. Yet, since a human learner would need to successfully explore the large state space of the POMDP, this appears to be an unrealistic scenario for explaining decision making in a multi-agent environment. A more realistic learning scenario is that humans transiently conceive the other players to follow a fixed strategy, and try to find their optimal counter strategy under this stationarity approximation. Maximizing one's own payoff while assuming stationarity in the opponents strategy is called a fictitious play and conditions are studied when this play effectively converges to a stationary (Nash) equilibrium <xref ref-type="bibr" rid="pcbi.1002691-Fudenberg1">[8]</xref>.</p>
      <p>Here we show that for classical two-player games <xref ref-type="bibr" rid="pcbi.1002691-VonNeumann1">[9]</xref> a simplified population reinforcement learning approach <xref ref-type="bibr" rid="pcbi.1002691-Friedrich1">[7]</xref>, which is policy gradient under the stationarity approximation, can reproduce human data. We consider two games, blackjack <xref ref-type="bibr" rid="pcbi.1002691-Hewig1">[10]</xref> and the inspector game <xref ref-type="bibr" rid="pcbi.1002691-Dorris1">[11]</xref>, as examples for which the optimal strategy is either deterministic or stochastic, respectively. Optimality is expressed in terms of the Nash equilibrium, a solution concept for games involving two or more players. It is reached when no player has anything to gain by changing its strategy unilaterally. Each player is making the best decision it can, taking into account the decisions of the other(s), hence the Nash equilibrium constitutes an optimum. Our algorithm is consistent with behavioral experiments for these games <xref ref-type="bibr" rid="pcbi.1002691-Hewig1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Dorris1">[11]</xref> while performing optimally according to the Nash equilibrium. We also show that TD-learning as well as covariance learning fail to find the stochastic Nash equilibrium for the inspector game.</p>
      <p>The current paper follows a long tradition of explaining human and animal behavior by simple models of reward-based learning, starting from Thorndike's law of effect <xref ref-type="bibr" rid="pcbi.1002691-Thorndike1">[12]</xref> and Pavlovian conditioning paradigms <xref ref-type="bibr" rid="pcbi.1002691-Rescorla1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Dayan2">[14]</xref> up to more recent theories of reinforcement learning <xref ref-type="bibr" rid="pcbi.1002691-Dayan1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Sutton1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Fiete1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Frmaux1">[16]</xref>. Basic reinforcement learning with simple models of a few free parameters have also been applied to games. It has been shown for a wide set of two-player games that these simple algorithms well approximate human performance <xref ref-type="bibr" rid="pcbi.1002691-Erev1">[17]</xref>.Yet, we show that basic reinforcement learning does not follow the reward gradient, and in fact it does not fit human data on the inspector game as well as our gradient rule. Obviously, playing games involves cognitive reasoning, as for instance captured by the theory of ‘adaptive control of thought–rational’ (ACT-R, <xref ref-type="bibr" rid="pcbi.1002691-Anderson1">[18]</xref>). Within such a theory, our model represents a neuronal implementation of a ‘production rule’ which initiates a behavioral pattern in response to sensory and possibly cognitive input.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>Model</title>
        <p>The network representing a player consists of a population of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e001" xlink:type="simple"/></inline-formula> Spike-Response-Model (SRM) neurons with escape noise <xref ref-type="bibr" rid="pcbi.1002691-Gerstner1">[19]</xref>, driven by a common presynaptic stimulus encoding the current state of the game (the player's hand value in blackjack, and a fixed stimulus for the inspector game). Each input spike pattern (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e002" xlink:type="simple"/></inline-formula>) is composed of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e003" xlink:type="simple"/></inline-formula> afferent spike trains generated once by independent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e004" xlink:type="simple"/></inline-formula> Poisson processes with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e005" xlink:type="simple"/></inline-formula> duration, and then repeatedly presented with the same fixed spike timings. The population neurons integrate the afferent presynaptic input spike trains and produce an output spike pattern (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e006" xlink:type="simple"/></inline-formula>, see <xref ref-type="fig" rid="pcbi-1002691-g001">Fig. 1</xref>). The decision of an individual postsynaptic neuron is denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e007" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e008" xlink:type="simple"/></inline-formula>, if the considered neuron does not spike, otherwise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e009" xlink:type="simple"/></inline-formula>. Behavioral decisions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e010" xlink:type="simple"/></inline-formula> are stochastically made based on the population activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e011" xlink:type="simple"/></inline-formula> defined as sum of the individual decisions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e012" xlink:type="simple"/></inline-formula> across the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e013" xlink:type="simple"/></inline-formula> population neurons: if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e014" xlink:type="simple"/></inline-formula> is small, the population decision is likely <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e015" xlink:type="simple"/></inline-formula>, and the larger <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e016" xlink:type="simple"/></inline-formula> is, the more likely is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e017" xlink:type="simple"/></inline-formula>. At the end of a game involving either a single decision (like in the inspector game) or a sequence of decisions (like in blackjack), a reward signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e018" xlink:type="simple"/></inline-formula> is delivered by an external critic which either informs about winning or losing (with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e019" xlink:type="simple"/></inline-formula> like in blackjack) or delivers a specific payoff (like in the inspector game).</p>
        <fig id="pcbi-1002691-g001" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002691.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Neuronal architecture implementing the two players.</title>
            <p>Each player <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e020" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e021" xlink:type="simple"/></inline-formula>) is represented by a population of decision making neurons (shown <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e022" xlink:type="simple"/></inline-formula> of each) which receive an input spike pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e023" xlink:type="simple"/></inline-formula> and generate an output spike pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e024" xlink:type="simple"/></inline-formula>. The population decision <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e025" xlink:type="simple"/></inline-formula> is represented by a readout unit, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e026" xlink:type="simple"/></inline-formula> being more likely when more decision making neurons fire at least one output spike. The synaptic weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e027" xlink:type="simple"/></inline-formula> are adapted as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e028" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e029" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e030" xlink:type="simple"/></inline-formula> and the reward signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e031" xlink:type="simple"/></inline-formula> delivered by a critic.</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002691.g001" xlink:type="simple"/>
        </fig>
        <p>The synapses feeding the stimulating spike pattern to the population neurons are updated according to a multi-factor plasticity rule involving the reward, the behavioral decision, the single neuron decision and the eligibility trace which depends on the post- and pre-synaptic activity:<disp-formula id="pcbi.1002691.e032"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e032" xlink:type="simple"/><label>(1)</label></disp-formula>Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e033" xlink:type="simple"/></inline-formula> is the reward signal encoding the reward prediction error <xref ref-type="bibr" rid="pcbi.1002691-Schultz1">[20]</xref> (see <xref ref-type="disp-formula" rid="pcbi.1002691.e273">Eq. 5</xref> in <xref ref-type="sec" rid="s4">Methods</xref>), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e034" xlink:type="simple"/></inline-formula> is the global feedback signal informing the synapses about the population decision <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e035" xlink:type="simple"/></inline-formula> weighted by the population activity (<xref ref-type="disp-formula" rid="pcbi.1002691.e284">Eq. 6</xref> in <xref ref-type="sec" rid="s4">Methods</xref>), and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e036" xlink:type="simple"/></inline-formula> is the neuronal decision (spike/no spike). The eligibility trace <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e037" xlink:type="simple"/></inline-formula> is a synaptic buffer roughly encoding the covariance between the past pre- and postsynaptic activity relevant for learning (<xref ref-type="disp-formula" rid="pcbi.1002691.e295">Eq. 8</xref> in <xref ref-type="sec" rid="s4">Methods</xref>). Technically, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e038" xlink:type="simple"/></inline-formula> is the derivative of the log-likelihood of producing the postsynaptic spike train. The learning rule can be shown to perform gradient ascent in the expected reward (Supporting <xref ref-type="supplementary-material" rid="pcbi.1002691.s002">Text S2</xref>).</p>
        <p>While most of the terms in <xref ref-type="disp-formula" rid="pcbi.1002691.e032">Eq. 1</xref> may have their standard biological counterpart <xref ref-type="bibr" rid="pcbi.1002691-Frmaux1">[16]</xref>, there is less experimental evidence for assigning the decision feedback <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e039" xlink:type="simple"/></inline-formula>' to one specific neuromodulator. Yet, be the population neurons recurrently connected <xref ref-type="bibr" rid="pcbi.1002691-Wang1">[21]</xref> or not, decision learning based on a population always requires that a global population signal is fed back to the individual neurons, as otherwise learning would quickly degrade with increasing population size <xref ref-type="bibr" rid="pcbi.1002691-Friedrich1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Urbanczik1">[22]</xref>. By the same performance reasons it is not possible to replace the other factors ‘<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e040" xlink:type="simple"/></inline-formula>’ in <xref ref-type="disp-formula" rid="pcbi.1002691.e032">Eq. 1</xref> by a classical spike-timing dependent plasticity (STDP) implementation endowed with the multiplicative reward signal ‘<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e041" xlink:type="simple"/></inline-formula>’ <xref ref-type="bibr" rid="pcbi.1002691-Izhikevich1">[23]</xref>. In fact, reward-modulated STDP is only able to learn multiple stimulus-response associations when the reward factor averages out to zero for each stimulus individually, requiring an additional reward-prediction network <xref ref-type="bibr" rid="pcbi.1002691-Frmaux1">[16]</xref>.</p>
        <p>Our neuronal implementation is as simple as possible to provide the required computational properties. The lack of feedback connectivity avoids issues relating to population spike correlations <xref ref-type="bibr" rid="pcbi.1002691-Averbeck1">[24]</xref>, and the neural mechanisms supporting the readout of the decision and the population feedback signal are not considered here. Similarly, the fixed spike trains representing an input pattern is a biological simplification which does not fundamentally restrict the suggested approach.</p>
      </sec>
      <sec id="s2b">
        <title>Blackjack</title>
        <p>The simplified version of blackjack considered here was played in 18th century France and is the precursor of the version played in casinos nowadays. The card decks used consist of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e042" xlink:type="simple"/></inline-formula> cards. Ace counts eleven, jack, queen and king ten points and the numbers two to ten according to their written value. The player (gambler) draws one card after the other, starting with an initial two card hand, with the object of bringing the hand value (total across drawn cards) as close as possible to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e043" xlink:type="simple"/></inline-formula>, but stopping early enough so that it does not exceed this number, in which case he immediately loses. Afterwards the croupier does the same for the bank. The player wins if its score is higher than that of the croupier or if the croupier exceeds <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e044" xlink:type="simple"/></inline-formula>, otherwise the croupier wins. The winner's payoff is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e045" xlink:type="simple"/></inline-formula>, the loser's <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e046" xlink:type="simple"/></inline-formula>. We assume that both player and croupier base their decision whether to draw another card or not only on their current hand value. Player and bank follow a strategy defined by the hand value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e047" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e048" xlink:type="simple"/></inline-formula>, respectively, from which on they stop to draw another card.</p>
        <p>The described rules of the game result in the payoff-matrix (<xref ref-type="table" rid="pcbi-1002691-t001">Table 1</xref>) comprising the average payoff of the bank as a function of the strategies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e049" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e050" xlink:type="simple"/></inline-formula> of the player and bank, respectively (<xref ref-type="sec" rid="s4">Methods</xref>). The gambler loses whatever the bank wins, therefore the game is an example of a zero sum game. For zero sum games a Nash equilibrium corresponds to a minimax solution <xref ref-type="bibr" rid="pcbi.1002691-VonNeumann2">[25]</xref>. If the pay-off matrix has a saddle point (an entry which is the maximum in its row and the minimum in its column) the corresponding strategy pair is a minimax solution which represents a pure Nash equilibrium. In blackjack there is a unique such pair, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e051" xlink:type="simple"/></inline-formula>, and hence there is a unique Nash equilibrium at all. For this optimal strategy pair the gambler stops drawing another card as soon as he has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e052" xlink:type="simple"/></inline-formula> points or more, while the croupier stops at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e053" xlink:type="simple"/></inline-formula> or more. The entry represents the lowest loss for the gambler given the strategy of the bank (minimum in the column), and the maximal payoff obtainable by the bank given the strategy of the gambler (maximum in the row). The Nash equilibrium is asymmetric because in the case of a standoff (equal final hand values) the croupier always obtains reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e054" xlink:type="simple"/></inline-formula> and the player <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e055" xlink:type="simple"/></inline-formula>. For hand values smaller than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e056" xlink:type="simple"/></inline-formula> it is safe to draw another card whereas for more than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e057" xlink:type="simple"/></inline-formula> drawing another card leads to certain loss due to exceeding <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e058" xlink:type="simple"/></inline-formula>. While we do not model these trivial actions, we address the learning problem for hand values between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e059" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e060" xlink:type="simple"/></inline-formula>.</p>
        <table-wrap id="pcbi-1002691-t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002691.t001</object-id><label>Table 1</label><caption>
            <title>Average bank payoff for our version of blackjack.</title>
          </caption><alternatives>
            <graphic id="pcbi-1002691-t001-1" mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002691.t001" xlink:type="simple"/>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e061" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">13</td>
                  <td align="left" colspan="1" rowspan="1">14</td>
                  <td align="left" colspan="1" rowspan="1">15</td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>16</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">17</td>
                  <td align="left" colspan="1" rowspan="1">18</td>
                  <td align="left" colspan="1" rowspan="1">19</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" colspan="1" rowspan="1">11</td>
                  <td align="left" colspan="1" rowspan="1">0.2982</td>
                  <td align="left" colspan="1" rowspan="1">0.3164</td>
                  <td align="left" colspan="1" rowspan="1">0.3027</td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>0.2544</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">0.1689</td>
                  <td align="left" colspan="1" rowspan="1">0.0436</td>
                  <td align="left" colspan="1" rowspan="1">−0.1237</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">12</td>
                  <td align="left" colspan="1" rowspan="1">0.1635</td>
                  <td align="left" colspan="1" rowspan="1">0.2015</td>
                  <td align="left" colspan="1" rowspan="1">0.2076</td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>0.1791</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">0.1130</td>
                  <td align="left" colspan="1" rowspan="1">0.0066</td>
                  <td align="left" colspan="1" rowspan="1">−0.1427</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">13</td>
                  <td align="left" colspan="1" rowspan="1">0.1052</td>
                  <td align="left" colspan="1" rowspan="1">0.1587</td>
                  <td align="left" colspan="1" rowspan="1">0.1806</td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>0.1679</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">0.1176</td>
                  <td align="left" colspan="1" rowspan="1">0.0266</td>
                  <td align="left" colspan="1" rowspan="1">−0.1077</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">14</td>
                  <td align="left" colspan="1" rowspan="1">0.0438</td>
                  <td align="left" colspan="1" rowspan="1">0.1134</td>
                  <td align="left" colspan="1" rowspan="1">0.1536</td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>0.1597</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">0.1282</td>
                  <td align="left" colspan="1" rowspan="1">0.0560</td>
                  <td align="left" colspan="1" rowspan="1">−0.0598</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>15</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>0.0119</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>0.0706</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>0.1289</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <bold>0.1555</bold>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>0.1450</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>0.0940</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>−0.0008</italic>
                  </td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">16</td>
                  <td align="left" colspan="1" rowspan="1">0.0143</td>
                  <td align="left" colspan="1" rowspan="1">0.0607</td>
                  <td align="left" colspan="1" rowspan="1">0.1085</td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>0.1557</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">0.1685</td>
                  <td align="left" colspan="1" rowspan="1">0.1411</td>
                  <td align="left" colspan="1" rowspan="1">0.0702</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">17</td>
                  <td align="left" colspan="1" rowspan="1">0.0543</td>
                  <td align="left" colspan="1" rowspan="1">0.0893</td>
                  <td align="left" colspan="1" rowspan="1">0.1254</td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>0.1628</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">0.1989</td>
                  <td align="left" colspan="1" rowspan="1">0.1980</td>
                  <td align="left" colspan="1" rowspan="1">0.1539</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">18</td>
                  <td align="left" colspan="1" rowspan="1">0.1349</td>
                  <td align="left" colspan="1" rowspan="1">0.1598</td>
                  <td align="left" colspan="1" rowspan="1">0.1854</td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>0.2120</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">0.2394</td>
                  <td align="left" colspan="1" rowspan="1">0.2651</td>
                  <td align="left" colspan="1" rowspan="1">0.2509</td>
                </tr>
              </tbody>
            </table>
          </alternatives><table-wrap-foot>
            <fn id="nt101">
              <label/>
              <p>The values show the calculated mean gains of the bank, respectively losses of the gambler, dependent on the strategy of the gambler (stopping to draw a card at hand values equal or larger than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e062" xlink:type="simple"/></inline-formula>) and the croupier (stopping at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e063" xlink:type="simple"/></inline-formula>). The strategies are described by the hand values from which on the players stop to draw another card. Formatted typesetting is used to highlight the Nash equilibrium.</p>
            </fn>
          </table-wrap-foot></table-wrap>
        <sec id="s2b1">
          <title>pRL and TD-learning converge to a pure Nash equilibrium</title>
          <p>We first simulated two neural networks playing against each other. Each hand value between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e064" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e065" xlink:type="simple"/></inline-formula> was represented by a fixed spatio-temporal spike pattern generated by 6 Hz Poisson processes, with different (but fixed) patterns distinguishing numbers, gambler and croupier. Since initially no ordering information is associated to the Poisson spike train encoding of the hand values, the learning process has yet to assign this information by trial and error. The drawing probabilities for each hand value learned by the gambler after <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e066" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e067" xlink:type="simple"/></inline-formula> games, averaged over ten runs, are shown in <xref ref-type="fig" rid="pcbi-1002691-g002">Fig. 2A</xref>. The colored dashed lines in the plot indicate the decision boundaries of Nash equilibrium above which no further card is drawn by the gambler or croupier, respectively. Initially, both players randomly decide with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e068" xlink:type="simple"/></inline-formula> chance to draw (black dashed line). After about <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e069" xlink:type="simple"/></inline-formula> games both players have learned to not exceed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e070" xlink:type="simple"/></inline-formula> and do not draw further cards for high hand values, being still undetermined about what action to take for low hand values. After <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e071" xlink:type="simple"/></inline-formula> games both have learned successfully to draw another card for low hand values and tend to play according to the Nash equilibrium.</p>
          <fig id="pcbi-1002691-g002" orientation="portrait" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002691.g002</object-id>
            <label>Figure 2</label>
            <caption>
              <title>Playing blackjack with pRL converges toward pure Nash equilibrium.</title>
              <p>(A) Average strategy (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e072" xlink:type="simple"/></inline-formula>) after <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e073" xlink:type="simple"/></inline-formula> (open circles) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e074" xlink:type="simple"/></inline-formula> (filled circles) games where the gambler (blue) is a neural net as well as the croupier (black). The dotted vertical lines left of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e075" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e076" xlink:type="simple"/></inline-formula> show the separation line of drawing/not drawing another card for the optimal Nash strategy pair. (B) Average strategy (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e077" xlink:type="simple"/></inline-formula>) after <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e078" xlink:type="simple"/></inline-formula> games for a neural net as gambler playing against a croupier that follows a given strategy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e079" xlink:type="simple"/></inline-formula> (blue), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e080" xlink:type="simple"/></inline-formula> (red) or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e081" xlink:type="simple"/></inline-formula> (green). The colored dotted lines left of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e082" xlink:type="simple"/></inline-formula> show the separation line of drawing/not drawing another card for the optimal strategy given that the croupier stops drawing at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e083" xlink:type="simple"/></inline-formula> (from left to right). (C) Average reward (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e084" xlink:type="simple"/></inline-formula>) of the gambler for the scenario described in (B). The colored dotted lines show the maximal reachable average reward. (D) Average strategy (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e085" xlink:type="simple"/></inline-formula>) over the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e086" xlink:type="simple"/></inline-formula> out of a total of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e087" xlink:type="simple"/></inline-formula> games for a neural net (red) or human (green) as gambler playing against a croupier that follows a given strategy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e088" xlink:type="simple"/></inline-formula>. The initial weights of the network were chosen such that the strategy in the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e089" xlink:type="simple"/></inline-formula> trials (blue) mimics the strategy of humans instructed about the game rules (black).</p>
            </caption>
            <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002691.g002" xlink:type="simple"/>
          </fig>
          <p>We next simulated the gambler by a neural net and the croupier by a computer algorithm which follows right from the beginning a fixed strategy. The resulting drawing probabilities after <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e090" xlink:type="simple"/></inline-formula> games are shown in <xref ref-type="fig" rid="pcbi-1002691-g002">Fig. 2B</xref> for three different strategies of the croupier, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e091" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e092" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e093" xlink:type="simple"/></inline-formula>. The gambler learns the perfect response strategies. The neural net exploits deviations of the croupier from the Nash equilibrium <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e094" xlink:type="simple"/></inline-formula> by also deviating and thus increasing its reward. If the croupier stops earlier at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e095" xlink:type="simple"/></inline-formula> the gambler continues until <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e096" xlink:type="simple"/></inline-formula>, trying to exceed the croupier's hand value (blue), whereas if the croupier stops later at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e097" xlink:type="simple"/></inline-formula> the gambler stops already at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e098" xlink:type="simple"/></inline-formula> (green), taking advantage of the fact that the croupier likely exceeds <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e099" xlink:type="simple"/></inline-formula>. For the case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e100" xlink:type="simple"/></inline-formula> (red), the gambler does not learn the optimal strategy as well as for the two others. This is due to the fact that here the true mean rewards for the strategies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e101" xlink:type="simple"/></inline-formula> are close to the one for the optimal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e102" xlink:type="simple"/></inline-formula>, cf. <xref ref-type="table" rid="pcbi-1002691-t001">Table 1</xref>, and cannot be distinguished based on merely <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e103" xlink:type="simple"/></inline-formula> samples. Instead of just looking at the strategy we hence consider the maximally possible and the actually obtained reward for the three croupier strategies. <xref ref-type="fig" rid="pcbi-1002691-g002">Fig. 2C</xref> depicts the low pass filtered reward which approaches the theoretical optimum indicated by the dashed lines and read out from the corresponding columns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e104" xlink:type="simple"/></inline-formula> in <xref ref-type="table" rid="pcbi-1002691-t001">Table 1</xref>. For the non-optimal croupier strategies (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e105" xlink:type="simple"/></inline-formula>) the maximally possible reward is significantly higher.</p>
          <p>Hence, from playing against another network and against the croupier, we conclude that a neuronal population endowed with the plasticity rule (1) is able to learn the optimal strategies in blackjack, determined either by the pure Nash equilibrium, or by the croupier's fixed strategy. Replacement of the neural net by a TD-learner yields similar results for both scenarios (Supporting <xref ref-type="supplementary-material" rid="pcbi.1002691.s001">Text S1</xref>).</p>
        </sec>
        <sec id="s2b2">
          <title>pRL fits human data on blackjack</title>
          <p>Human behavior in blackjack was studied in <xref ref-type="bibr" rid="pcbi.1002691-Hewig1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Hewig2">[26]</xref>, though with a deck of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e106" xlink:type="simple"/></inline-formula> cards. The instruction of the subjects about the rules of the game already induces a prior in the drawing behavior with a preference for drawing or stopping in the case of low or high hand values, respectively. Neither pRL nor TD can reproduce this type of learning by insight. Moreover, the network needs first to learn the ordering of the stimuli by trial-and-error, and hence much more learning trials are required for the network. To still allow for a comparison with the human data, we used the same deck of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e107" xlink:type="simple"/></inline-formula> cards and simulated a neural network with initial weights chosen in such a way, that the initial strategy mimics the one of human's (<xref ref-type="fig" rid="pcbi-1002691-g002">Fig. 2D</xref>). After playing the same number of games as humans did, the network's final strategy agrees with the experimental data of humans, showing the same shift to a slightly less risky drawing behavior.</p>
        </sec>
      </sec>
      <sec id="s2c">
        <title>Inspector game</title>
        <p>The inspector game <xref ref-type="bibr" rid="pcbi.1002691-Avenhaus1">[27]</xref> has been widely studied in neuroeconomics <xref ref-type="bibr" rid="pcbi.1002691-Glimcher1">[28]</xref>. The economic story surrounding the game is that a lazy employee prefers not to work. An employer knows this and sometimes ‘inspects’, but has to pay some cost ‘<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e108" xlink:type="simple"/></inline-formula>’for inspection. The payoffs for employee and employer are shown in <xref ref-type="table" rid="pcbi-1002691-t002">Table 2</xref>. The inspector game shows only a mixed Nash equilibrium in which decisions are taken stochastically with a fixed probability. At the equilibrium, the players mix pure strategies, each with the same payoff: had these pure strategies different payoffs, then it would be better to just follow the pure strategy with the highest expected payoff.</p>
        <table-wrap id="pcbi-1002691-t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002691.t002</object-id><label>Table 2</label><caption>
            <title>Payoff matrix of the inspector game.</title>
          </caption><alternatives>
            <graphic id="pcbi-1002691-t002-2" mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002691.t002" xlink:type="simple"/>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" colspan="1" rowspan="1">employee\ <italic>employer</italic></td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>inspect</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>Don't inspect</italic>
                  </td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" colspan="1" rowspan="1">work</td>
                  <td align="left" colspan="1" rowspan="1">0.5, <italic>2-i</italic></td>
                  <td align="left" colspan="1" rowspan="1">0.5, <italic>2</italic></td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">shirk</td>
                  <td align="left" colspan="1" rowspan="1">0, <italic>1-i</italic></td>
                  <td align="left" colspan="1" rowspan="1">1, <italic>0</italic></td>
                </tr>
              </tbody>
            </table>
          </alternatives><table-wrap-foot>
            <fn id="nt102">
              <label/>
              <p>The variables in the left of each cell determine the employee's payoffs, and the variables in the right determine the employer's payoffs for each combination of player's responses. ‘<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e109" xlink:type="simple"/></inline-formula>’is the cost of inspection to the employer.</p>
            </fn>
          </table-wrap-foot></table-wrap>
        <p>For each value of the inspection cost <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e110" xlink:type="simple"/></inline-formula>, there is a unique mixed Nash equilibrium in which the probability with which the employee is shirking just corresponds to the inspection cost, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e111" xlink:type="simple"/></inline-formula>, and the probability with which the employer is inspecting is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e112" xlink:type="simple"/></inline-formula>. In this case, neither player can improve its expected payoff by only unilaterally changing its strategy. In fact, using <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e113" xlink:type="simple"/></inline-formula>, the expected payoff for the employer is always<disp-formula id="pcbi.1002691.e114"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e114" xlink:type="simple"/></disp-formula>independently of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e115" xlink:type="simple"/></inline-formula>. Likewise, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e116" xlink:type="simple"/></inline-formula>, the expected payoff for the employee is always <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e117" xlink:type="simple"/></inline-formula>, independently of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e118" xlink:type="simple"/></inline-formula>.</p>
        <sec id="s2c1">
          <title>pRL reproduces inspector game data and Nash equilibria</title>
          <p>We played with our neural network as employee against the algorithm presented in <xref ref-type="bibr" rid="pcbi.1002691-Dorris1">[11]</xref> as employer, and we also simulated two neural nets playing against each other. <xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3A</xref> shows a running average over the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e119" xlink:type="simple"/></inline-formula> trials of the shirk and inspection rates for the neuronal net employee (green) and neuronal net employer (red), overlaid with the corresponding data for a human employee (black) playing against a human employer (grey, data from <xref ref-type="bibr" rid="pcbi.1002691-Dorris1">[11]</xref>). The inspection cost was held constant during three blocks of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e120" xlink:type="simple"/></inline-formula> trials and stepped from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e121" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e122" xlink:type="simple"/></inline-formula> and finally to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e123" xlink:type="simple"/></inline-formula>. The averaged shirk rate of the neuronal net employee is in striking agreement with the one of the human employee across the whole rate of inspection costs (<xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3B</xref>). There is also good agreement with the experimental data for the employee's reward (<xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3C</xref>).</p>
          <fig id="pcbi-1002691-g003" orientation="portrait" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002691.g003</object-id>
            <label>Figure 3</label>
            <caption>
              <title>pRL but not TD-learning fits data and follows a mixed Nash equilibrium.</title>
              <p>(A) Choice behavior for pRL versus pRL (employee green, employer red) and human versus human (employee black, employer gray) <xref ref-type="bibr" rid="pcbi.1002691-Dorris1">[11]</xref>. The cost of inspection was stepped from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e124" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e125" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e126" xlink:type="simple"/></inline-formula>, respectively, and this does also correspond to the shirk rate in Nash equilibrium (thick black lines). The inspection rate in the Nash equilibrium would always be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e127" xlink:type="simple"/></inline-formula>. (B) Average choice behavior of pRL vs pRL (dark green circles) and TD vs TD (light green circles), pRL for the employee vs computer algorithm for the employer (blue squares), human vs human (black), human as an employee vs computer algorithm (orange) and monkey vs computer algorithm (cyan) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e128" xlink:type="simple"/></inline-formula> trials/block as function of the inspection cost. The solid line indicates the Nash equilibrium. (C) Reward as function of the inspection cost for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e129" xlink:type="simple"/></inline-formula> trials/block. Coloring as in (B). pRL simulations are more similar to the experimental data than the TD simulations. (D) Average choice behavior as in (B) but for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e130" xlink:type="simple"/></inline-formula> trials/block. The inspect rates for pRL vs pRL (TD vs TD) (dark (light) red circles) and pRL vs computer algorithm (purple squares) are shown too. The lines indicate the Nash equilibrium for the employee (diagonal) and the employer (horizontal). pRL behaves according to the Nash equilibrium, whereas TD does not. (E) Time course of the probability to shirk with inspection cost <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e131" xlink:type="simple"/></inline-formula> for pRL vs algorithm (blue line) and pRL vs pRL (TD vs TD) (dark (light) green line). For the latter the probability of the employer to inspect is shown too (dark (light) red line). pRL oscillates around the Nash equilibrium (drawn lines), whereas TD completely deviates from Nash. (F) Time course of the probability to shirk or inspect respectively with inspection cost <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e132" xlink:type="simple"/></inline-formula> for pRL vs pRL (green respectively red, solid) as in E, but shifted up for clarity and overlaid with the negative change in the shirk rate (green dashed) and the change in the inspect rate (red dashed) to show the counteractive behavior.</p>
            </caption>
            <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002691.g003" xlink:type="simple"/>
          </fig>
          <p>To check whether the good fit of the human data is due to the gradient property alone or whether the population boost is also necessary we considered single neurons playing against each other (by setting the number of neurons in the population to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e133" xlink:type="simple"/></inline-formula>, without changing the learning rule). In this case our learning rule becomes equivalent to the policy gradient rule for single escape rate neurons <xref ref-type="bibr" rid="pcbi.1002691-Pfister1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Florian1">[30]</xref>. With only a single neuron learning turns out to be too slow to match the transient behavior in the human data (<xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3A</xref>), even after optimizing the learning rate (data not shown). We have previously shown that the speeding up learning in a population of spiking neurons is only possible with an additional population signal modulating synaptic plasticity <xref ref-type="bibr" rid="pcbi.1002691-Urbanczik1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Friedrich2">[31]</xref>. We conclude that population learning is necessary, and that other spike-based gradient rules which do not exploit a population signal <xref ref-type="bibr" rid="pcbi.1002691-Fiete1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Seung1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Werfel1">[33]</xref> will also be too slow.</p>
          <p>During the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e134" xlink:type="simple"/></inline-formula> trials across an experimental block, the shirk rates (but not the inspection rates) tended towards the corresponding value of the Nash equilibrium (diagonal line in <xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3B</xref>), and so did the employee's reward (horizontal line in <xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3C</xref>), although without reaching them. In the simulation we extended the block size to check the asymptotic behavior. We found that for block sizes of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e135" xlink:type="simple"/></inline-formula> trials, the average shirk <italic>and</italic> the inspection rates closely reached the Nash equilibrium (match of simulation points with the two lines in <xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3D</xref>).</p>
          <p>Despite the match of the average rates with the mixed Nash equilibria, the running means oscillate around the corresponding equilibria (shown in <xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3E</xref> for inspection cost <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e136" xlink:type="simple"/></inline-formula>), as predicted by the theory <xref ref-type="bibr" rid="pcbi.1002691-Crawford1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Stahl1">[35]</xref>. In the asymmetric case, when our neuronal employee plays against the (apparently not optimal) computer algorithm, the oscillation vanish and the employee's shirk rate reaches the optimal Nash equilibrium (blue), as expected for a neuronal network endowed by synaptic modifications following the reward gradient (Supporting <xref ref-type="supplementary-material" rid="pcbi.1002691.s002">Text S2</xref>). When two reward maximizing networks play against each other, however, each tries to exploit any deviation of the other from his Nash equilibrium, pushing him even further away. This leads to oscillations around the Nash equilibrium where a change in strategy of one player is oppositely directed to the deviation from the Nash equilibrium of the other player. In fact, when superimposing one rate with the negative change of the other rate, a close match is observed (<xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3F</xref>).</p>
          <p>As we are studying a policy gradient algorithm, one may ask how robust the described properties are in view of a possibly improper biological implementation. The eligibility trace <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e137" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1002691.e295">Eq. 8</xref> in <xref ref-type="sec" rid="s4">Methods</xref>) depends on the presynaptic spike timing and contains a positive term that depends itself on the postsynaptic spike timing and a negative term depending on the postsynaptic potential. Due to the policy gradient property, the two terms are balanced and the eligibility trace is zero on average. To check for robustness we performed simulations where this balance is perturbed. The above results still qualitatively hold true if the negative term in the eligibility trace is twice as large, or even if it is neglected completely, yielding STDP with plasticity for pre-post spike pairing only. The robustness can be attributed to the factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e138" xlink:type="simple"/></inline-formula> in the learning rule (<xref ref-type="disp-formula" rid="pcbi.1002691.e032">Eq. 1</xref>) which averages out to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e139" xlink:type="simple"/></inline-formula> due to the subtraction of the reward prediction (since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e140" xlink:type="simple"/></inline-formula>, see <xref ref-type="disp-formula" rid="pcbi.1002691.e273">Eq. 5</xref> in <xref ref-type="sec" rid="s4">Methods</xref>) and hence neutralizes any bias in the estimate of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e141" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002691-Frmaux1">[16]</xref>.</p>
        </sec>
        <sec id="s2c2">
          <title>TD-learning is inconsistent with data and Nash</title>
          <p>To value the match generated by our synaptic plasticity rule with experimental and theoretical data, we also trained a TD-learner on the inspector game. Yet, the parameter optimized TD-algorithm roughly reproduces only the humans average shirk rate (<xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3B</xref>), but less well the subjects' rewards (<xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3C</xref>). More strikingly, two opposing TD-learners simulated across the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e142" xlink:type="simple"/></inline-formula> trial blocks do not behave according to the Nash equilibrium (<xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3D</xref>), but adopt a deterministic strategy within such a block (<xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3E</xref>). When simulating even longer, oscillations emerge as well, but without convergence of the long-term average to the Nash equilibrium.</p>
          <p>There are principled reasons why TD-learning must generally fail to find an optimum solution, and in particular a mixed Nash equilibrium. First, TD-learning assumes that the underlying process is Markovian, but for multiplayer games this assumption is in general not satisfied. In fact, because the policy of the other player may change in time during learning, the optimal decision probabilities depend on past actions. Values which are assumed to be a function of the current action only, may therefore be incorrectly estimated. Second, for a mixed Nash equilibrium, TD-learning may also fail to correctly map values to decision probabilities at the steady-state after learning. This is because each policy imposes some predefined mapping from action values to decision probabilities, and this adhoc mapping may not reflect the true relationship between expected payoffs and decision probabilities defining the Nash equilibrium. For the inspector game with inspection costs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e143" xlink:type="simple"/></inline-formula>, for instance, the softmax policy, which selects action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e144" xlink:type="simple"/></inline-formula> with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e145" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e146" xlink:type="simple"/></inline-formula> is the value of action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e147" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e148" xlink:type="simple"/></inline-formula> a parameter (inverse temperature) regulating the amount of stochasticity in the decision making, does never reflect the Nash probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e149" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e150" xlink:type="simple"/></inline-formula>, whatever the parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e151" xlink:type="simple"/></inline-formula> is (see Supporting <xref ref-type="supplementary-material" rid="pcbi.1002691.s001">Text S1</xref>).</p>
          <p>In other learning tasks, animals and humans <italic>transiently</italic> match the choice probabilities with the average payoffs of the corresponding actions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e152" xlink:type="simple"/></inline-formula> (for a discussion of probability matching see <xref ref-type="bibr" rid="pcbi.1002691-Shanks1">[36]</xref>–<xref ref-type="bibr" rid="pcbi.1002691-Loewenstein2">[38]</xref>). In these cases too, TD-learning with softmax will not be able to find the solutions neither, unless again all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e153" xlink:type="simple"/></inline-formula>- and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e154" xlink:type="simple"/></inline-formula>-values are each the same, or alternatively, the choice policy is redefined (to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e155" xlink:type="simple"/></inline-formula>, see <xref ref-type="bibr" rid="pcbi.1002691-Sakai1">[39]</xref>). For other examples where TD-learning fails in each of the two ways of either learning the values or inferring the choice probabilities see <xref ref-type="bibr" rid="pcbi.1002691-Friedrich1">[7]</xref>.</p>
        </sec>
        <sec id="s2c3">
          <title>Not all covariance-rules lead to Nash equilibria</title>
          <p>Covariance-based learning rules change the synaptic strengths <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e156" xlink:type="simple"/></inline-formula> according to the covariance between reward prediction error and some measure of neuronal activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e157" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e158" xlink:type="simple"/></inline-formula>, see e.g. <xref ref-type="bibr" rid="pcbi.1002691-Loewenstein1">[37]</xref>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e159" xlink:type="simple"/></inline-formula> denotes expectation. pRL which follows the stochastic gradient of the expected reward, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e160" xlink:type="simple"/></inline-formula>, is a special instance of a covariance rule where the quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e161" xlink:type="simple"/></inline-formula> corresponds to the eligibility trace <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e162" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1002691.e032">Eq. 1</xref>). Steady-states of covariance rules satisfy Herrnstein's <italic>matching law</italic> <xref ref-type="bibr" rid="pcbi.1002691-Loewenstein1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Herrnstein1">[40]</xref>. This law states that the number of times an action is chosen is proportional to the reward accumulated from choosing that action. Formally, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e163" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e164" xlink:type="simple"/></inline-formula> is the number of times an action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e165" xlink:type="simple"/></inline-formula> is chosen, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e166" xlink:type="simple"/></inline-formula> is the action-independent proportionality constant, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e167" xlink:type="simple"/></inline-formula> is the reward accumulated by action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e168" xlink:type="simple"/></inline-formula> across its <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e169" xlink:type="simple"/></inline-formula> choices.</p>
          <p>Pure strategies, where only a single action is chosen, trivially satisfy the matching property since for non-chosen actions both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e170" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e171" xlink:type="simple"/></inline-formula> vanish. In contrast, stochastic strategies only satisfy matching in the case that the selected options provide the same average reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e172" xlink:type="simple"/></inline-formula>. The mixed (and trivially the pure) Nash equilibrium represents a special case of matching. If in a two-player game, for instance, player 2 adopts a mixed Nash strategy then, by definition, player 1 receives the same average reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e173" xlink:type="simple"/></inline-formula> from any action (see also <xref ref-type="bibr" rid="pcbi.1002691-Loewenstein2">[38]</xref>).</p>
          <p>Both the steady-state of a covariance rule and the Nash equilibrium imply matching. But a steady-state of the covariance rule does not necessarily need to be a Nash equilibrium. This can be seen by generalizing the classical covariance rule <xref ref-type="bibr" rid="pcbi.1002691-Loewenstein1">[37]</xref> to population learning, and applying this rule to the inspector game. To do so we replaced the eligibility trace <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e174" xlink:type="simple"/></inline-formula> in the pRL rule (<xref ref-type="disp-formula" rid="pcbi.1002691.e032">Eq. 1</xref>) by the deviation of the neuronal response <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e175" xlink:type="simple"/></inline-formula> from its mean, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e176" xlink:type="simple"/></inline-formula>. The emerging population covariance (pCOV) learning rule,<disp-formula id="pcbi.1002691.e177"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e177" xlink:type="simple"/><label>(2)</label></disp-formula>with positive learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e178" xlink:type="simple"/></inline-formula> and estimates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e179" xlink:type="simple"/></inline-formula> (see <xref ref-type="disp-formula" rid="pcbi.1002691.e273">Eq. 5</xref> in <xref ref-type="sec" rid="s4">Methods</xref>) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e180" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e181" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e182" xlink:type="simple"/></inline-formula> respectively, does not follow the reward gradient and does not reliably converge to the unique mixed Nash equilibrium of the inspector game (<xref ref-type="fig" rid="pcbi-1002691-g004">Fig. 4</xref>). To keep simulation time short we did not use spiking neurons but merely binary neurons that produce output <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e183" xlink:type="simple"/></inline-formula> with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e184" xlink:type="simple"/></inline-formula> for a given binary input pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e185" xlink:type="simple"/></inline-formula>. This yields an expected neural response <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e186" xlink:type="simple"/></inline-formula>. Notice that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e187" xlink:type="simple"/></inline-formula> can be considered as the personalized reward for the specific neuron in consideration: if the sign of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e188" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e189" xlink:type="simple"/></inline-formula> coincide, the population reward signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e190" xlink:type="simple"/></inline-formula> elicited in response to the population decision can be taken as a personal reward signal for that neuron; otherwise the neuron's personal reward has reversed sign of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e191" xlink:type="simple"/></inline-formula>. Personalizing this way the neuronal rewards within the population solves the spatial credit-assignment problem and boosts learning <xref ref-type="bibr" rid="pcbi.1002691-Urbanczik1">[22]</xref>. <xref ref-type="fig" rid="pcbi-1002691-g004">Fig. 4 (A–D)</xref> shows the results for the pCOV rule applied to the inspector game, once with only the employee playing according to pCOV against an employer playing according to the algorithm presented in <xref ref-type="bibr" rid="pcbi.1002691-Dorris1">[11]</xref>, and once for pCOV versus pCOV. Only in a fraction of the simulated runs is the mixed Nash equilibrium reached, while in the other runs, a deterministic (non-Nash) strategy pair emerges.</p>
          <fig id="pcbi-1002691-g004" orientation="portrait" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002691.g004</object-id>
            <label>Figure 4</label>
            <caption>
              <title>Covariance learning rules may lead to a mixed Nash equilibrium, but also to deterministic non-Nash strategies. pRL fits data better than basic reinforcement models.</title>
              <p>Time course of the probability to shirk (A,C) and inspect (B,D) with inspection cost <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e192" xlink:type="simple"/></inline-formula> for pCOV vs algorithm (A,B) and pCOV vs pCOV (C,D). In each panel the horizontal lines depict the Nash equilibrium, and for 10 simulation runs inspection and shirk rates are shown (same color in (A,B) and (C,D), respectively, correspond to the same run). Only a small fraction of all runs converge or oscillate around the Nash equilibrium, while the other runs result in a deterministic strategy pair. The initial distribution of synaptic weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e193" xlink:type="simple"/></inline-formula> was Gauss with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e194" xlink:type="simple"/></inline-formula> and standard deviation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e195" xlink:type="simple"/></inline-formula>. The learning rate was set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e196" xlink:type="simple"/></inline-formula>, but <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e197" xlink:type="simple"/></inline-formula> did not change the proportion of runs converging to the pure strategy. (E) Average choice behavior of pRL vs pRL (green), RE1 vs RE1 (blue), RE3 vs RE3 (red) and human vs human (black) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e198" xlink:type="simple"/></inline-formula> trials/block as function of the inspection cost. The light red circles show the average choice behavior for RE3 vs RE3 and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e199" xlink:type="simple"/></inline-formula> trials/block. Individual runs converged to a pure strategy, hence the shown averages over 200 runs reflect the percentage of runs converging to a pure shirk strategy. (F) Reward as function of the inspection cost for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e200" xlink:type="simple"/></inline-formula> trials/block. Coloring as in (E). The solid lines indicate the Nash equilibrium.</p>
            </caption>
            <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002691.g004" xlink:type="simple"/>
          </fig>
          <p>As check for robustness we performed further simulations showing that these negative results hold true also for other (non-gradient) covariance rules. We considered the version where the mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e201" xlink:type="simple"/></inline-formula> is not calculated analytically but determined as a running average (as done for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e202" xlink:type="simple"/></inline-formula>), and where the neuronal activity in the covariance rule is equal to the binary output, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e203" xlink:type="simple"/></inline-formula>, without mean subtraction (yielding the simple update rule <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e204" xlink:type="simple"/></inline-formula>). Moreover, considering only a single neuron and taking its response <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e205" xlink:type="simple"/></inline-formula> as the behavioral decision did not qualitatively change the results, demonstrating that the failure is not due to the population framework (data not shown).</p>
          <p>In <xref ref-type="bibr" rid="pcbi.1002691-Loewenstein3">[41]</xref> the author further elaborates on the relationship of covariance based rules to the Replicator dynamics. The latter is described by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e206" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e207" xlink:type="simple"/></inline-formula> is the effective learning rate and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e208" xlink:type="simple"/></inline-formula> is the average reward for choosing action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e209" xlink:type="simple"/></inline-formula>. The effective learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e210" xlink:type="simple"/></inline-formula> depends on the details of the decision making network and is given in Eq.(14) of <xref ref-type="bibr" rid="pcbi.1002691-Loewenstein3">[41]</xref>. If synaptic changes are driven by the covariance of reward and neural activity, then according to the average velocity approximation, learning behavior is described by the differential equation above, but the effective learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e211" xlink:type="simple"/></inline-formula> is not guaranteed to be positive. Indeed, for the binary neurons one gets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e212" xlink:type="simple"/></inline-formula>, which can be negative due to negative components <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e213" xlink:type="simple"/></inline-formula>. Hence, the convergence statements in <xref ref-type="bibr" rid="pcbi.1002691-Loewenstein3">[41]</xref> do not apply to our decision making network, and there is no contradiction with the finding that the covariance rule fails to reproduce the mixed Nash equilibrium. Nevertheless, in the special case of a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e214" xlink:type="simple"/></inline-formula> coding of the inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e215" xlink:type="simple"/></inline-formula> such that the effective learning rate becomes positive, and the specific covariance rule <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e216" xlink:type="simple"/></inline-formula> with a single postsynaptic neuron (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e217" xlink:type="simple"/></inline-formula>), we can also fit the human data (for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e218" xlink:type="simple"/></inline-formula> learning trials) and obtain the oscillations around the Nash equilibrium (for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e219" xlink:type="simple"/></inline-formula> trials).</p>
          <p>Whereas we do not consider the neural mechanisms supporting the readout of the decision, such a mechanism has been studied in the context of matching <xref ref-type="bibr" rid="pcbi.1002691-Soltani1">[42]</xref>. There, the probability for decision making is well described by a logistic function, which is also our choice for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e220" xlink:type="simple"/></inline-formula>. For an increasing amount of stochasticity in the decision making they report an increasing deviation from matching towards more uniform choice probabilities. For choice probabilities larger than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e221" xlink:type="simple"/></inline-formula> this leads to lower choice probabilities than predicted by the matching law, a phenomenon called undermatching. We also varied the amount of stochasticity by changing the slope of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e222" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e223" xlink:type="simple"/></inline-formula>. We find that increasing and decreasing the slope by a factor of two still robustly leads to matching for the considered inspection costs, but when decreasing it by a factor of ten we also observe undermatching. Due to the stochasticity in the decision making the range of choice probabilities our network can represent is limited. With increasing the amount of stochasticity the range becomes smaller and extreme probabilities close to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e224" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e225" xlink:type="simple"/></inline-formula> predicted by the matching law cannot be represented. Instead, choice probabilities lie closer to uniform randomness, i.e. undermatching occurs.</p>
        </sec>
        <sec id="s2c4">
          <title>pRL fits data and Nash better than basic reinforcement models</title>
          <p>The Replicator equation is widely used in evolutionary game theory and provides a good phenomenological description of choice behavior in many repeated-choice experiments. A different phenomenological description has been suggested in <xref ref-type="bibr" rid="pcbi.1002691-Erev1">[17]</xref>. Starting from Luce's basic reinforcement learning (RE1, see also <xref ref-type="bibr" rid="pcbi.1002691-Luce1">[43]</xref>), the authors adapt this rule to take account of a generalization and recency effect (RE3, <xref ref-type="sec" rid="s4">Methods</xref>). They show that both the basic and extended reinforcement learning reproduces the behavior of humans in many different games. However, we find that these rules poorly match human behavior for the inspector game. In fact, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e226" xlink:type="simple"/></inline-formula> trials/block both models of Erev and Roth fit the experimental data significantly worse than pRL (<xref ref-type="fig" rid="pcbi-1002691-g004">Fig. 4 E and F</xref>). After <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e227" xlink:type="simple"/></inline-formula> trials RE3 converged to a pure (non-Nash) strategy, and for RE1 the inspection rate diverged away from the Nash equilibrium of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e228" xlink:type="simple"/></inline-formula>. This non-optimal equilibrium performance is consistent with the fact that RE1 and RE3 are not gradient procedures (see Supporting <xref ref-type="supplementary-material" rid="pcbi.1002691.s003">Text S3</xref>). Whether pRL fits the behavioral data of humans also better in the other games Erev and Roth considered remains to be tested. In any case, the models have to cope with the fact that humans show a variety of behaviors in two-player matrix games, although in many settings they eventually play according to Nash (for a discussion see <xref ref-type="bibr" rid="pcbi.1002691-Shanks2">[44]</xref>).</p>
        </sec>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We considered a population of spiking neurons which represent an adaptive agent in a dynamic environment including other adaptive agents. The agent's adaptation was implemented as population reinforcement learning algorithm (pRL) which was previously shown to perform stochastic gradient ascent in the reward for partially observable Markov decision processes (POMDPs) <xref ref-type="bibr" rid="pcbi.1002691-Friedrich1">[7]</xref>. Here we showed with blackjack and the inspector game that pRL can also cope with a dynamic multi-agent environment and that the performance is comparable to human data in both these games. In fact, when two neuronal populations play against each other, they learn to behave according to the optimal (but unstable) Nash equilibrium. By definition, no further increase in an agent's expected payoff is possible in the Nash equilibrium by only changing its own strategy while the environment remains stationary. In these steady-state conditions – where the opponent's strategy is assumed to be stationary – pRL is proven to maximize the expected reward (Supporting <xref ref-type="supplementary-material" rid="pcbi.1002691.s002">Text S2</xref>). The simulations show that the equilibrium is indeed reached by two pRL agents playing against each other, with a pure (deterministic) Nash equilibrium in blackjack and a mixed (stochastic) Nash equilibrium in the inspector game. As predicted by the theory <xref ref-type="bibr" rid="pcbi.1002691-Crawford1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Stahl1">[35]</xref>, the strategies oscillated around the mixed Nash equilibrium when both players used the same gradient algorithm based on the others stationarity assumption, i.e. when one network played against another both using pRL (with a small learning rate). Averaging over long enough time windows, i.e. long compared to the oscillation period, yields the Nash equilibrium values. However, when implementing only the employee by a gradient pRL network and the employer by a non-gradient computer algorithm <xref ref-type="bibr" rid="pcbi.1002691-Dorris1">[11]</xref>, the two players do not play exactly equally well. In this case no oscillations occurred and both converged to and stayed at the optimal Nash equilibrium.</p>
      <p>For mathematical clarity we presented the spike-based pRL for an episodic learning scenario. But a biologically plausible implementation of a fully online scheme is also possible: to avoid an explicit separation of stimuli in time, the rectangular window function used to temporally integrate the eligibility trace (<xref ref-type="disp-formula" rid="pcbi.1002691.e295">Eq. 8</xref> in <xref ref-type="sec" rid="s4">Methods</xref>) can be replaced by an exponentially decaying window function to get a low-pass filtered eligibility trace, and concentrations of neuromodulators can be used to encode feedback about the population decision and the global reward signal (e.g. acetylcholine or dopamine) <xref ref-type="bibr" rid="pcbi.1002691-Urbanczik1">[22]</xref>. We considered reward delivery immediately after stimulus presentation, but reward could also be substantially delayed when considering a further eligibility trace incorporating the population decision <xref ref-type="bibr" rid="pcbi.1002691-Friedrich1">[7]</xref>. Moreover, since learning in general speeds up with population size (up to 1-shot learning for stimulus-response associations <xref ref-type="bibr" rid="pcbi.1002691-Friedrich2">[31]</xref>) we expect that the convergence for pRL towards the Nash equilibrium can be much faster than in our example where parameters were fit to reproduce human data.</p>
      <p>The mixed Nash equilibrium represents a special case of Herrnstein's matching law <xref ref-type="bibr" rid="pcbi.1002691-Herrnstein1">[40]</xref>, according to which the number of times an action is chosen is proportional to the reward accumulated from choosing that action. This is true both for the pure and mixed Nash optimum. In the special case that the current reward only depends on the current action, but not on past actions, reward maximization always implies matching. (In fact, if one action would yield a higher (average) payoff per choice, then this action must be chosen with probability 1 to maximize expected reward, and matching (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e229" xlink:type="simple"/></inline-formula>) is trivially satisfied (since for the non-chosen action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e230" xlink:type="simple"/></inline-formula>). If both actions yield the same payoff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e231" xlink:type="simple"/></inline-formula> per choice (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e232" xlink:type="simple"/></inline-formula>), then matching is again trivially satisfied.) In turn, a reward-based learning rule which only empirically maximizes reward in this case leads to only an approximated matching <xref ref-type="bibr" rid="pcbi.1002691-Soltani1">[42]</xref>. Choice probabilities which maximize the expected reward are trivially also fixed points of any learning rule defined by the covariance between reward and neuronal activity. (In fact, at the reward maximum there is no change in neuronal activity which, in average, would lead to an increase (and in the opposite direction to a decrease) of the expected reward, and hence the covariance between activity and reward must vanish.) The other direction, again, is not true: a covariance-based rule does not necessarily lead to reward maximization or a Nash equilibrium <xref ref-type="bibr" rid="pcbi.1002691-Loewenstein1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1002691-Loewenstein2">[38]</xref>. Indeed, our simulations of the inspector game with the canonical covariance-based plasticity rules show that these rules do not necessarily lead to the mixed Nash equilibrium, but instead can result in deterministic (non-Nash) strategies. Similarly, basic reinforcement rules studied in the context of economics and human decision making <xref ref-type="bibr" rid="pcbi.1002691-Erev1">[17]</xref> are neither compatible with the mixed Nash equilibrium for the inspector game.</p>
      <p>The performance of spike-based pRL is also superior to TD-learning <xref ref-type="bibr" rid="pcbi.1002691-Sutton1">[2]</xref> which is often discussed in the neuro-economical context <xref ref-type="bibr" rid="pcbi.1002691-Dayan1">[1]</xref>. With the parameter values for which TD-learners came closest to human data (although without matching them as closely as pRL), the mixed Nash equilibrium in the inspector game was not reached within the long learning times. Instead, TD-learner first adopted a deterministic strategy, transiently switched their behavior, and swapped back to the same deterministic strategy. We attributed this mismatch to a general failing of TD-learning in correctly mapping action values to choice probabilities in probabilistic decision making tasks. TD-learning with the softmax choice policy, in particular, fails when matching of choice probabilities with average payoff is required <xref ref-type="bibr" rid="pcbi.1002691-Herrnstein1">[40]</xref>.</p>
      <p>Different generalizations have been considered to approach the shortcomings of algorithms in socio-economic games. TD-learning has been extended to not only assign values to its own decisions, but to pairs of own and opponent decisions. This enables the learning of minimax strategies where reward is maximized for the worst of the opponents actions <xref ref-type="bibr" rid="pcbi.1002691-Littman1">[45]</xref>. While for zero-sum games minimax may realize a mixed Nash equilibrium, it results in a deterministic strategy in the inspector game: minimizing the maximal loss implies for the employee to always work (to prevent being caught while shirking), and for the employer to always inspect (to prevent undetected shirking). Another approach is to separately learn its own and the opponents action values and then calculate the Nash equilibrium <xref ref-type="bibr" rid="pcbi.1002691-Hu1">[46]</xref>, but such explicit calculations do not seem to be the typical human behavior in socio-economic interactions. Instead, it is tempting to consider pRL with long eligibility traces which, as it performs policy gradient in POMDPs <xref ref-type="bibr" rid="pcbi.1002691-Friedrich1">[7]</xref>, should find cooperative strategies with, on average, higher than Nash payoffs for all agents. For the inspector game such a co-operative strategy is that the employer should let the employee sporadically shirk (say with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e233" xlink:type="simple"/></inline-formula>) without inspection, but with the common agreement that shirking will not prevail (leading to average payoffs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e234" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e235" xlink:type="simple"/></inline-formula> for the employee and employer, respectively).</p>
      <p>Although under the specific experimental conditions of the inspector game humans did not show cooperation, they often do so in other game-theoretic paradigms, as e.g. in the prisoner's dilemma, and hence deviate from the Nash equilibrium (for a review see <xref ref-type="bibr" rid="pcbi.1002691-Holt1">[47]</xref>). It remains a challenge for future modeling work to capture such cooperative behavior. Likely, this will involve modeling the prediction of other player's reactions in response to ones own actions, as considered in the theory of mind <xref ref-type="bibr" rid="pcbi.1002691-Yoshida1">[48]</xref> and as being a hallmark of successful socio-economic behavior.</p>
      <p>Given the difficulties of modeling genuine social behavior, and the difficulties humans effectively have in stacked reflexive reasoning, the assumption of the opponent's stationarity considered here appears as a reasonable approximation for decision making even in complex situations. In view of its success in matching behavioral and theoretical data we may ask how far human decision making is in fact determined by cognitive reasoning, or whether decisions should rather be attributed to automated neuronal processes steered e.g. by pRL (which can also encompass input from a cognitive module as it is suggested for the production rules in the ACT-R theory, <xref ref-type="bibr" rid="pcbi.1002691-Anderson1">[18]</xref>). In fact, daily experience tells us that decisions are often more appropriate when we listen to our gut feeling, while we tend to merely add justifications post-hoc. Or put in Schopenhauer's words, “that in spite of all his resolutions and reflections he does not change his conduct” <xref ref-type="bibr" rid="pcbi.1002691-Schopenhauer1">[49]</xref>.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Model details</title>
        <p>Focusing on one neuron we denote by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e236" xlink:type="simple"/></inline-formula> its input, which is a spike pattern made up of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e237" xlink:type="simple"/></inline-formula> spike trains, and by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e238" xlink:type="simple"/></inline-formula> its output spike train. The membrane potential can be written as<disp-formula id="pcbi.1002691.e239"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e239" xlink:type="simple"/><label>(3)</label></disp-formula>The postsynaptic kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e240" xlink:type="simple"/></inline-formula> and the reset kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e241" xlink:type="simple"/></inline-formula> vanish for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e242" xlink:type="simple"/></inline-formula>. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e243" xlink:type="simple"/></inline-formula> they are given by<disp-formula id="pcbi.1002691.e244"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e244" xlink:type="simple"/></disp-formula>For the resting potential we use <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e245" xlink:type="simple"/></inline-formula> (arbitrary units). Further, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e246" xlink:type="simple"/></inline-formula> is used for the membrane time constant and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e247" xlink:type="simple"/></inline-formula> for the synaptic time constant. Action potential generation is controlled by an instantaneous firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e248" xlink:type="simple"/></inline-formula> which increases with the membrane potential. So, at each point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e249" xlink:type="simple"/></inline-formula> in time, the neuron fires with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e250" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e251" xlink:type="simple"/></inline-formula> represents an infinitesimal time window (we use <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e252" xlink:type="simple"/></inline-formula> in the simulations). Our firing rate function is<disp-formula id="pcbi.1002691.e253"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e253" xlink:type="simple"/></disp-formula>with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e254" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e255" xlink:type="simple"/></inline-formula> (parameter values taken from <xref ref-type="bibr" rid="pcbi.1002691-Pfister1">[29]</xref>, see also <xref ref-type="bibr" rid="pcbi.1002691-Gerstner1">[19]</xref>).</p>
        <p>We consider a population of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e256" xlink:type="simple"/></inline-formula> neurons and an input layer of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e257" xlink:type="simple"/></inline-formula> for each player that is represented by a neural net. We assume that each population neuron synapses onto a site in the input layer with probability of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e258" xlink:type="simple"/></inline-formula>, leading to many shared input spike trains between the neurons. The population response is read out by the decision making unit based on a spike/no-spike code. We introduce the coding function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e259" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e260" xlink:type="simple"/></inline-formula>, if neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e261" xlink:type="simple"/></inline-formula> does not spike, otherwise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e262" xlink:type="simple"/></inline-formula>. The population activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e263" xlink:type="simple"/></inline-formula> being read out by the decision making unit is:<disp-formula id="pcbi.1002691.e264"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e264" xlink:type="simple"/></disp-formula>Note that such a formal summation could be implemented in terms of a neuronal integrator (forming a ‘line attractor’) which continuously integrates excitatory and inhibitory input and keeps the neuronal activity at a constant level in the absence of input <xref ref-type="bibr" rid="pcbi.1002691-Seung2">[50]</xref>. Using this activity readout, the behavioral decision <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e265" xlink:type="simple"/></inline-formula> is made probabilistically, with likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e266" xlink:type="simple"/></inline-formula> given by the logistic function<disp-formula id="pcbi.1002691.e267"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e267" xlink:type="simple"/><label>(4)</label></disp-formula>and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e268" xlink:type="simple"/></inline-formula> being the counter probability. The normalization of the activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e269" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e270" xlink:type="simple"/></inline-formula> ensures that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e271" xlink:type="simple"/></inline-formula>, thus being of same order as the noise in the decision readout.</p>
        <p>We now describe the terms, modulating synaptic plasticity in <xref ref-type="disp-formula" rid="pcbi.1002691.e032">Eq. (1)</xref>. The reward feedback <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e272" xlink:type="simple"/></inline-formula> encodes the reward prediction error, as observed in experiments <xref ref-type="bibr" rid="pcbi.1002691-Schultz1">[20]</xref>,<disp-formula id="pcbi.1002691.e273"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e273" xlink:type="simple"/><label>(5)</label></disp-formula>Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e274" xlink:type="simple"/></inline-formula> is a running mean estimate of the expected reward, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e275" xlink:type="simple"/></inline-formula>, where we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e276" xlink:type="simple"/></inline-formula>. The parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e277" xlink:type="simple"/></inline-formula> is the positive learning rate which, for notational convenience, we absorb into the reward signal. In all pRL simulations we used the value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e278" xlink:type="simple"/></inline-formula>. Both values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e279" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e280" xlink:type="simple"/></inline-formula> (rounded) were chosen to minimize the Mean Squared Error (MSE) between the average model and human data (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e281" xlink:type="simple"/></inline-formula> for the shirk rate and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e282" xlink:type="simple"/></inline-formula> for the employee's reward in the inspector game). All other parameter values were taken from <xref ref-type="bibr" rid="pcbi.1002691-Friedrich1">[7]</xref>.</p>
        <p>The decision feedback <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e283" xlink:type="simple"/></inline-formula> is given by<disp-formula id="pcbi.1002691.e284"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e284" xlink:type="simple"/><label>(6)</label></disp-formula>which is the derivative of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e285" xlink:type="simple"/></inline-formula>, see <xref ref-type="disp-formula" rid="pcbi.1002691.e267">Eq. (4)</xref>, with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e286" xlink:type="simple"/></inline-formula>; so decision feedback measures how sensitive the decision is to changes in activity.</p>
        <p>As shown in <xref ref-type="bibr" rid="pcbi.1002691-Pfister1">[29]</xref>, the probability density, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e287" xlink:type="simple"/></inline-formula>, that a neuron actually produces the output spike train <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e288" xlink:type="simple"/></inline-formula> in response to the stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e289" xlink:type="simple"/></inline-formula> during a decision period lasting from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e290" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e291" xlink:type="simple"/></inline-formula> satisfies:<disp-formula id="pcbi.1002691.e292"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e292" xlink:type="simple"/><label>(7)</label></disp-formula>The derivative of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e293" xlink:type="simple"/></inline-formula> with respect to the strength of synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e294" xlink:type="simple"/></inline-formula> is known as characteristic eligibility in reinforcement learning <xref ref-type="bibr" rid="pcbi.1002691-Williams1">[51]</xref>. For our choice of the firing rate function one obtains for the last term in (1)<disp-formula id="pcbi.1002691.e295"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e295" xlink:type="simple"/><label>(8)</label></disp-formula></p>
        <p>In all the simulations initial values for the synaptic strength were picked from a Gaussian distribution with mean zero and standard deviation equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e296" xlink:type="simple"/></inline-formula>, independently for each afferent and each neuron. In the Supporting <xref ref-type="supplementary-material" rid="pcbi.1002691.s002">Text S2</xref> we show that the plasticity rule (1) composed of the factors (5, 6, 8) and the decision <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e297" xlink:type="simple"/></inline-formula> follows the stochastic gradient of the expected reward.</p>
      </sec>
      <sec id="s4b">
        <title>TD-learning</title>
        <p>For TD-learning we used the SARSA control algorithm <xref ref-type="bibr" rid="pcbi.1002691-Sutton1">[2]</xref> which estimates the values of state-action pairs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e298" xlink:type="simple"/></inline-formula>. At each point in time, the value estimates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e299" xlink:type="simple"/></inline-formula> are updated according to<disp-formula id="pcbi.1002691.e300"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e300" xlink:type="simple"/></disp-formula>Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e301" xlink:type="simple"/></inline-formula> is similar to a learning rate and has values between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e302" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e303" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e304" xlink:type="simple"/></inline-formula> is the reward immediately obtained after performing action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e305" xlink:type="simple"/></inline-formula>. In the case of blackjack it is defined as zero if the game is not over and the player chooses to draw another card, otherwise it is determined by the payoffs of the considered game. When in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e306" xlink:type="simple"/></inline-formula>, the next action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e307" xlink:type="simple"/></inline-formula> is chosen using softmax, i.e. according to the probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e308" xlink:type="simple"/></inline-formula>. In all simulations we used the rounded values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e309" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e310" xlink:type="simple"/></inline-formula> as they minimized the MSE between averaged model and human data (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e311" xlink:type="simple"/></inline-formula> for the shirk rate and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e312" xlink:type="simple"/></inline-formula> for the employee's reward in the inspector game). Note that in both TD-learning and pRL we adapted the same number of free parameters (TD: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e313" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e314" xlink:type="simple"/></inline-formula>; pRL: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e315" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e316" xlink:type="simple"/></inline-formula>), making it possible to directly compare the quality of the fit.</p>
      </sec>
      <sec id="s4c">
        <title>Basic reinforcement models</title>
        <p>In both Roth-Erev models <xref ref-type="bibr" rid="pcbi.1002691-Erev1">[17]</xref> the probabilistic choice rule is parametrized using propensities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e317" xlink:type="simple"/></inline-formula>. The probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e318" xlink:type="simple"/></inline-formula> that a specific player (who's index is omitted) plays his <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e319" xlink:type="simple"/></inline-formula>th pure strategy is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e320" xlink:type="simple"/></inline-formula>.</p>
        <sec id="s4c1">
          <title>RE1 model</title>
          <p>The propensity of the chosen action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e321" xlink:type="simple"/></inline-formula> is incremented by the received reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e322" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e323" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e324" xlink:type="simple"/></inline-formula> denotes the Kronecker delta. The initial propensities are set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e325" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e326" xlink:type="simple"/></inline-formula> is the average reward for that player under uniformly distributed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e327" xlink:type="simple"/></inline-formula>'s, and the strength parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e328" xlink:type="simple"/></inline-formula> is the one parameter that is optimized.</p>
        </sec>
        <sec id="s4c2">
          <title>RE3 model</title>
          <p>In addition to the strength parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e329" xlink:type="simple"/></inline-formula>, the generalization and forgetting parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e330" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e331" xlink:type="simple"/></inline-formula> are introduced. The propensities are updated according to<disp-formula id="pcbi.1002691.e332"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e332" xlink:type="simple"/></disp-formula>The parameters were chosen to minimize the mean squared error (MSE) between the average model and human data (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e333" xlink:type="simple"/></inline-formula> (RE1) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e334" xlink:type="simple"/></inline-formula> (RE3) for the shirk rate and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e335" xlink:type="simple"/></inline-formula> (RE1) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e336" xlink:type="simple"/></inline-formula> (RE3) for the employee's reward in the inspector game).</p>
        </sec>
      </sec>
      <sec id="s4d">
        <title>Blackjack details</title>
        <p>In blackjack we assume an infinite number of card decks. Independently of the history, the drawing probability therefore remains constant, with a probability to draw a card with value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e337" xlink:type="simple"/></inline-formula> being <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e338" xlink:type="simple"/></inline-formula>, and the probability to draw any other value from 2 to 11 being <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e339" xlink:type="simple"/></inline-formula>. For a strategy determined by the stopping value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e340" xlink:type="simple"/></inline-formula> we calculated analytically the probability distribution of hand values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e341" xlink:type="simple"/></inline-formula> after drawing the last card. The drawing process is iterated for those hand values that are smaller than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e342" xlink:type="simple"/></inline-formula> until there is only probability mass on hand values greater than or equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e343" xlink:type="simple"/></inline-formula>. Because the lowest card value on the desk remains always <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e344" xlink:type="simple"/></inline-formula>, drawing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e345" xlink:type="simple"/></inline-formula> times in a row yields a lowest possible hand value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e346" xlink:type="simple"/></inline-formula>. Hence up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e347" xlink:type="simple"/></inline-formula> cards are drawn in order to obtain a hand value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e348" xlink:type="simple"/></inline-formula> greater or equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e349" xlink:type="simple"/></inline-formula>. Let us denote the value of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e350" xlink:type="simple"/></inline-formula>th card by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e351" xlink:type="simple"/></inline-formula> and its probability distribution by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e352" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1002691.e353"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e353" xlink:type="simple"/></disp-formula>To obtain the probability distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e354" xlink:type="simple"/></inline-formula> we sum up the probabilities of all possible combinations to draw <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e355" xlink:type="simple"/></inline-formula> cards that yield hand value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e356" xlink:type="simple"/></inline-formula>, with the condition that the sum of the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e357" xlink:type="simple"/></inline-formula> drawn cards is smaller than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e358" xlink:type="simple"/></inline-formula>, such that a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e359" xlink:type="simple"/></inline-formula>th card is actually drawn under the stopping strategy.<disp-formula id="pcbi.1002691.e360"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e360" xlink:type="simple"/></disp-formula>where II is the indicator function which is one if its argument is true and zero else. The product of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e361" xlink:type="simple"/></inline-formula> is the joint probability that the first card has value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e362" xlink:type="simple"/></inline-formula>, the second <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e363" xlink:type="simple"/></inline-formula> and so on. The first indicator function ensures that all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e364" xlink:type="simple"/></inline-formula> drawn cards sum up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e365" xlink:type="simple"/></inline-formula>, the second that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e366" xlink:type="simple"/></inline-formula> cards are drawn, i.e. the sum of the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e367" xlink:type="simple"/></inline-formula> cards has to be smaller than the stopping value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e368" xlink:type="simple"/></inline-formula>, because otherwise no further card would be drawn. For instance in the case of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e369" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e370" xlink:type="simple"/></inline-formula> one obtains the distributions in <xref ref-type="table" rid="pcbi-1002691-t003">Table 3.</xref></p>
        <table-wrap id="pcbi-1002691-t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002691.t003</object-id><label>Table 3</label><caption>
            <title>Probability distribution of hand values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e371" xlink:type="simple"/></inline-formula> after drawing the last card.</title>
          </caption><alternatives>
            <graphic id="pcbi-1002691-t003-3" mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002691.t003" xlink:type="simple"/>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e372" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">&lt;15</td>
                  <td align="left" colspan="1" rowspan="1">15</td>
                  <td align="left" colspan="1" rowspan="1">16</td>
                  <td align="left" colspan="1" rowspan="1">17</td>
                  <td align="left" colspan="1" rowspan="1">18</td>
                  <td align="left" colspan="1" rowspan="1">19</td>
                  <td align="left" colspan="1" rowspan="1">20</td>
                  <td align="left" colspan="1" rowspan="1">21</td>
                  <td align="left" colspan="1" rowspan="1">&gt;21</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e373" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">0</td>
                  <td align="left" colspan="1" rowspan="1">0.1206</td>
                  <td align="left" colspan="1" rowspan="1">0.1247</td>
                  <td align="left" colspan="1" rowspan="1">0.1194</td>
                  <td align="left" colspan="1" rowspan="1">0.1138</td>
                  <td align="left" colspan="1" rowspan="1">0.1078</td>
                  <td align="left" colspan="1" rowspan="1">0.1546</td>
                  <td align="left" colspan="1" rowspan="1">0.0944</td>
                  <td align="left" colspan="1" rowspan="1">0.1648</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e374" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">0</td>
                  <td align="left" colspan="1" rowspan="1">0</td>
                  <td align="left" colspan="1" rowspan="1">0.1247</td>
                  <td align="left" colspan="1" rowspan="1">0.1287</td>
                  <td align="left" colspan="1" rowspan="1">0.1231</td>
                  <td align="left" colspan="1" rowspan="1">0.1170</td>
                  <td align="left" colspan="1" rowspan="1">0.1638</td>
                  <td align="left" colspan="1" rowspan="1">0.1036</td>
                  <td align="left" colspan="1" rowspan="1">0.2390</td>
                </tr>
              </tbody>
            </table>
          </alternatives></table-wrap>
        <p>Denoting the hand value of the gambler by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e375" xlink:type="simple"/></inline-formula> and that of the croupier by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e376" xlink:type="simple"/></inline-formula> the payoff of the bank is<disp-formula id="pcbi.1002691.e377"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002691.e377" xlink:type="simple"/></disp-formula>Averaging of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e378" xlink:type="simple"/></inline-formula> with respect to the joint distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e379" xlink:type="simple"/></inline-formula> yields the entry in the average payoff matrix <xref ref-type="table" rid="pcbi-1002691-t001">Table 1</xref> for the strategy pair (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e380" xlink:type="simple"/></inline-formula>). For instance for (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e381" xlink:type="simple"/></inline-formula>), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e382" xlink:type="simple"/></inline-formula>.</p>
        <p>We defined the drawing probabilities in <xref ref-type="fig" rid="pcbi-1002691-g002">Fig. 2</xref> for a hand value at a certain game number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e383" xlink:type="simple"/></inline-formula> as the frequency with which another card has been drawn upon the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e384" xlink:type="simple"/></inline-formula> presentations prior to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e385" xlink:type="simple"/></inline-formula> of the corresponding stimulus. The evolution of the average reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e386" xlink:type="simple"/></inline-formula> in time in <xref ref-type="fig" rid="pcbi-1002691-g002">Fig. 2C</xref> are the low pass filtered reward sequences, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e387" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e388" xlink:type="simple"/></inline-formula> is the reward in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e389" xlink:type="simple"/></inline-formula>-th game and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e390" xlink:type="simple"/></inline-formula> was used. The initial value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e391" xlink:type="simple"/></inline-formula> was calculated assuming a random <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e392" xlink:type="simple"/></inline-formula> choice behavior prior to learning.</p>
        <p>The initial weights mimicking the prior strategy of instructed humans were obtained by training our network to make a decision with a certain probability. This is possible by adapting pRL to perform regression (as will be published elsewhere).</p>
      </sec>
      <sec id="s4e">
        <title>Inspector game details</title>
        <p>The evolution of the rates in time in <xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3E</xref> are the low pass filtered decision sequences, e.g. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e393" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e394" xlink:type="simple"/></inline-formula> if the employee shirks in trial <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e395" xlink:type="simple"/></inline-formula>, otherwise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e396" xlink:type="simple"/></inline-formula>. We used a value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e397" xlink:type="simple"/></inline-formula> and assumed again an initial random <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e398" xlink:type="simple"/></inline-formula> choice behavior. The rate change in <xref ref-type="fig" rid="pcbi-1002691-g003">Fig. 3F</xref> was determine by binning the obtained time course of the rate into bins of width <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e399" xlink:type="simple"/></inline-formula>, calculating the mean of each bin, and the differences between succeeding bins. The result was further low pass filtered once more with an exponential running mean (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002691.e400" xlink:type="simple"/></inline-formula>) to reduce the noise.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002691.s001" mimetype="application/pdf" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002691.s001" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <p>We present further results for temporal-difference learning and elaborate on its failure to learn mixed Nash equilibria.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002691.s002" mimetype="application/pdf" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002691.s002" xlink:type="simple">
        <label>Text S2</label>
        <caption>
          <p>We show how the plasticity rule presented in the main text is based on a gradient ascent procedure maximizing the average reward.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002691.s003" mimetype="application/pdf" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002691.s003" xlink:type="simple">
        <label>Text S3</label>
        <caption>
          <p>We demonstrate that the heuristic rules of Erev and Roth <xref ref-type="bibr" rid="pcbi.1002691-Erev1">[17]</xref> are no gradient procedures.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank Robert Urbanczik for helpful discussions, Michael C. Dorris for providing the code of the computer algorithm used in <xref ref-type="bibr" rid="pcbi.1002691-Dorris1">[11]</xref>, Johannes Hewig for the data of humans playing blackjack <xref ref-type="bibr" rid="pcbi.1002691-Hewig1">[10]</xref>, and Yonatan Loewenstein for helpful feedback on the covariance rules.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002691-Dayan1">
        <label>1</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name> (<year>2008</year>) <article-title>Decision theory, reinforcement learning, and the brain</article-title>. <source>Cogn Affect Behav Ne</source> <volume>8</volume>: <fpage>429</fpage>–<lpage>453</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Sutton1">
        <label>2</label>
        <mixed-citation publication-type="other" xlink:type="simple">Sutton RS, Barto AG (1998) Reinforcement Learning: An Introduction. Cambridge, MA: MIT Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Seymour1">
        <label>3</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seymour</surname><given-names>B</given-names></name>, <name name-style="western"><surname>O'Doherty</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Koltzenburg</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>AK</given-names></name>, <etal>et al</etal>. (<year>2004</year>) <article-title>Temporal difference models describe higher-order learning in humans</article-title>. <source>Nature</source> <volume>429</volume>: <fpage>664</fpage>–<lpage>7</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Potjans1">
        <label>4</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Potjans</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Morrison</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>A spiking neural network model of an actor-aritic learning agent</article-title>. <source>Neural Comput</source> <volume>21</volume>: <fpage>301</fpage>–<lpage>339</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Howard1">
        <label>5</label>
        <mixed-citation publication-type="other" xlink:type="simple">Howard RA (1960) Dynamic programming and Markov processes. Cambridge, MA: MIT Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Smallwood1">
        <label>6</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smallwood</surname><given-names>RD</given-names></name>, <name name-style="western"><surname>Sondik</surname><given-names>EJ</given-names></name> (<year>1973</year>) <article-title>The optimal control of partially observable markov processes over a finite horizon</article-title>. <source>Oper Res</source> <volume>21</volume>: <fpage>1071</fpage>–<lpage>1088</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Friedrich1">
        <label>7</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friedrich</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Urbanczik</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname><given-names>W</given-names></name> (<year>2011</year>) <article-title>Spatio-temporal credit assignment in neuronal population learning</article-title>. <source>PLoS Comput Biol</source> <volume>7</volume>: <fpage>e1002092</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Fudenberg1">
        <label>8</label>
        <mixed-citation publication-type="other" xlink:type="simple">Fudenberg D, Levine DK (1998) Theory of Learning in Games. Cambridge, MA: MIT Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-VonNeumann1">
        <label>9</label>
        <mixed-citation publication-type="other" xlink:type="simple">Von Neumann J, Morgenstern O (1944) Theory of games and economic behavior. Princeton: Princeton University Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Hewig1">
        <label>10</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hewig</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Trippe</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Hecht</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Coles</surname><given-names>GH</given-names></name>, <name name-style="western"><surname>Holroyd</surname><given-names>CB</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Decision-making in blackjack: An electrophysiological analysis</article-title>. <source>Cereb Cortex</source> <volume>17</volume>: <fpage>865</fpage>–<lpage>877</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Dorris1">
        <label>11</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dorris</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>Glimcher</surname><given-names>PW</given-names></name> (<year>2004</year>) <article-title>Activity in posterior parietal cortex is correlated with the relative subjective desirability of action</article-title>. <source>Neuron</source> <volume>44</volume>: <fpage>365</fpage>–<lpage>378</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Thorndike1">
        <label>12</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorndike</surname><given-names>EL</given-names></name> (<year>1898</year>) <article-title>Animal Intelligence: An Experimental Study of the Associative Processes in Animals</article-title>. <source>Psychol Monogr</source> <volume>2</volume>: <fpage>321</fpage>–<lpage>330</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Rescorla1">
        <label>13</label>
        <mixed-citation publication-type="other" xlink:type="simple">Rescorla R, Wagner A (1972) A theory of Pavlovian conditioning: variations in the effectiveness of reinforecement and nonreinforcement. In: Black AH, Prokasy WF, editors. Classical Conditioning II: current research and theory. New York: Appleton Century Crofts. pp. 64–99.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Dayan2">
        <label>14</label>
        <mixed-citation publication-type="other" xlink:type="simple">Dayan P, Long T (1998) Statistical models of conditioning. Adv Neural Inf Process Syst 10. pp. 117–123.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Fiete1">
        <label>15</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiete</surname><given-names>IR</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name> (<year>2006</year>) <article-title>Gradient learning in spiking neural networks by dynamic perturbation of conductances</article-title>. <source>Phys Rev Lett</source> <volume>97</volume>: <fpage>048104</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Frmaux1">
        <label>16</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frémaux</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Sprekeler</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>Functional requirements for reward-modulated spike- timing-dependent plasticity</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>13326</fpage>–<lpage>13337</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Erev1">
        <label>17</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Erev</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Roth</surname><given-names>AE</given-names></name> (<year>1998</year>) <article-title>Predicting how people play games: reinforcement learning in experimental games with unique, mixed strategy equilibria</article-title>. <source>Amer Econ Rev</source> <volume>88</volume>: <fpage>848</fpage>–<lpage>881</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Anderson1">
        <label>18</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Bothell</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Byrne</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Douglass</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Lebiere</surname><given-names>C</given-names></name>, <etal>et al</etal>. (<year>2004</year>) <article-title>An integrated theory of the mind</article-title>. <source>Psychol Rev</source> <volume>111</volume>: <fpage>1036</fpage>–<lpage>1060</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Gerstner1">
        <label>19</label>
        <mixed-citation publication-type="other" xlink:type="simple">Gerstner W, Kistler WM (2002) Spiking Neuron Models. Cambridge, UK: Cambridge University Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Schultz1">
        <label>20</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name> (<year>1997</year>) <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source> <volume>275</volume>: <fpage>1593</fpage>–<lpage>1599</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Wang1">
        <label>21</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name> (<year>2008</year>) <article-title>Decision making in recurrent neuronal circuits</article-title>. <source>Neuron</source> <volume>60</volume>: <fpage>215</fpage>–<lpage>234</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Urbanczik1">
        <label>22</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Urbanczik</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>Reinforcement learning in populations of spiking neurons</article-title>. <source>Nat Neurosci</source> <volume>12</volume>: <fpage>250</fpage>–<lpage>252</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Izhikevich1">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izhikevich</surname><given-names>EM</given-names></name> (<year>2007</year>) <article-title>Solving the distal reward problem through linkage of STDP and dopamine signaling</article-title>. <source>Cereb Cortex</source> <volume>17</volume>: <fpage>2443</fpage>–<lpage>2452</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Averbeck1">
        <label>24</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Averbeck</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name> (<year>2006</year>) <article-title>Neural correlations, population coding and computation</article-title>. <source>Nat Rev Neurosci</source> <volume>7</volume>: <fpage>358</fpage>–<lpage>3666</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-VonNeumann2">
        <label>25</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Von Neumann</surname><given-names>J</given-names></name> (<year>1928</year>) <article-title>Zur Theorie der Gesellschaftsspiele</article-title>. <source>Math Ann</source> <volume>100</volume>: <fpage>295</fpage>–<lpage>320</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Hewig2">
        <label>26</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hewig</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Trippe</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Hecht</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Coles</surname><given-names>GH</given-names></name>, <name name-style="western"><surname>Holroyd</surname><given-names>CB</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>An electrophysiological analysis of coaching in blackjack</article-title>. <source>Cortex</source> <volume>44</volume>: <fpage>1197</fpage>–<lpage>1205</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Avenhaus1">
        <label>27</label>
        <mixed-citation publication-type="other" xlink:type="simple">Avenhaus R, Von Stengel B, Zamir S (2002) Inspection games. In: Aumann RJ, Hart S, editors. Handbook of Game Theory with Economic Applications. pp. 1947–1987.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Glimcher1">
        <label>28</label>
        <mixed-citation publication-type="other" xlink:type="simple">Glimcher PW, Camerer C, Fehr E, Poldrack R, editors (2008) Neuroeconomics: decision making and the brain. Amsterdam: Elsevier.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Pfister1">
        <label>29</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pfister</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Toyoizumi</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Barber</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2006</year>) <article-title>Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning</article-title>. <source>Neural Comput</source> <volume>18</volume>: <fpage>1318</fpage>–<lpage>1348</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Florian1">
        <label>30</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Florian</surname><given-names>RV</given-names></name> (<year>2007</year>) <article-title>Reinforcement learning through modulation of spike-timing-dependent synaptic plasticity</article-title>. <source>Neural Comput</source> <volume>19</volume>: <fpage>1468</fpage>–<lpage>1502</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Friedrich2">
        <label>31</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friedrich</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Urbanczik</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>Learning spike-based population codes by reward and population feedback</article-title>. <source>Neural Comput</source> <volume>22</volume>: <fpage>1698</fpage>–<lpage>1717</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Seung1">
        <label>32</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name> (<year>2003</year>) <article-title>Learning in spiking neural networks by reinforcement of stochastic synaptic transmission</article-title>. <source>Neuron</source> <volume>40</volume>: <fpage>1063</fpage>–<lpage>1073</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Werfel1">
        <label>33</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Werfel</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Xie</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name> (<year>2005</year>) <article-title>Learning curves for stochastic gradient descent in linear feedfor-ward networks</article-title>. <source>Neural Comput</source> <volume>17</volume>: <fpage>2699</fpage>–<lpage>2718</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Crawford1">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Crawford</surname><given-names>VP</given-names></name> (<year>1985</year>) <article-title>Learning behavior and mixed-strategy Nash equilibria</article-title>. <source>J Econ Behav Organ</source> <volume>6</volume>: <fpage>69</fpage>–<lpage>78</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Stahl1">
        <label>35</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stahl</surname><given-names>DO</given-names></name> (<year>1988</year>) <article-title>On the instability of mixed-strategy Nash equilibria</article-title>. <source>J Econ Behav Organ</source> <volume>9</volume>: <fpage>59</fpage>–<lpage>69</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Shanks1">
        <label>36</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shanks</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Tunney</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>McCarthy</surname><given-names>JD</given-names></name> (<year>2002</year>) <article-title>A re-examination of probability matching and ra-tional choice</article-title>. <source>J Behav Decis Making</source> <volume>15</volume>: <fpage>233</fpage>–<lpage>250</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Loewenstein1">
        <label>37</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name> (<year>2006</year>) <article-title>Operant matching is a generic outcome of synaptic plasticity based on the covariance between reward and neural activity</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>103</volume>: <fpage>15224</fpage>–<lpage>15229</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Loewenstein2">
        <label>38</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Prelec</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name> (<year>2009</year>) <article-title>Operant matching as a Nash equilibrium of an in-tertemporal game</article-title>. <source>Neural Comput</source> <volume>21</volume>: <fpage>2755</fpage>–<lpage>2773</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Sakai1">
        <label>39</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sakai</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Fukai</surname><given-names>T</given-names></name> (<year>2008</year>) <article-title>The actor-critic learning is behind the matching law: matching versus optimal behaviors</article-title>. <source>Neural Comput</source> <volume>20</volume>: <fpage>227</fpage>–<lpage>251</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Herrnstein1">
        <label>40</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herrnstein</surname><given-names>RJ</given-names></name> (<year>1961</year>) <article-title>Relative and absolute strength of response as a function of frequency of reinforcement</article-title>. <source>J Exp Anal Behav</source> <volume>4</volume>: <fpage>267</fpage>–<lpage>272</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Loewenstein3">
        <label>41</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name> (<year>2010</year>) <article-title>Synaptic theory of replicator-like melioration</article-title>. <source>Front Comput Neurosci</source> <volume>4</volume>: <fpage>17</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Soltani1">
        <label>42</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Soltani</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name> (<year>2006</year>) <article-title>A biophysically based neural model of matching law behavior: melio-ration by stochastic synapses</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>3731</fpage>–<lpage>3744</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Luce1">
        <label>43</label>
        <mixed-citation publication-type="other" xlink:type="simple">Luce R (1959) Individual Choice Behavior: A Theoretical Analysis. New York: Wiley.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Shanks2">
        <label>44</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shanks</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Tunney</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>McCarthy</surname><given-names>JD</given-names></name> (<year>1998</year>) <article-title>Do people play Nash equilibrium? Lessons from evolutionary game theory</article-title>. <source>J Econ Lit</source> <volume>36</volume>: <fpage>1</fpage>–<lpage>28</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Littman1">
        <label>45</label>
        <mixed-citation publication-type="other" xlink:type="simple">Littman ML (1994) Markov games as a framework for multi-agent reinforcement learning. In: Proceedings of the International Conference on Machine Learning. San Francisco, CA. pp. 157–163.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Hu1">
        <label>46</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hu</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wellman</surname><given-names>MP</given-names></name> (<year>2003</year>) <article-title>Nash Q-learning for general-sum stochastic games</article-title>. <source>J Mach Learn Res</source> <volume>4</volume>: <fpage>1039</fpage>–<lpage>1069</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Holt1">
        <label>47</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Holt</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Roth</surname><given-names>AE</given-names></name> (<year>2004</year>) <article-title>The Nash equilibrium: a perspective</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>101</volume>: <fpage>3999</fpage>–<lpage>4002</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Yoshida1">
        <label>48</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yoshida</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2008</year>) <article-title>Game theory of mind</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>: <fpage>e1000254</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Schopenhauer1">
        <label>49</label>
        <mixed-citation publication-type="other" xlink:type="simple">Schopenhauer A (1890) The wisdom of life. London: S. Sonnenschein. 147 pp.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Seung2">
        <label>50</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name> (<year>1996</year>) <article-title>How the brain keeps the eyes still</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>93</volume>: <fpage>13339</fpage>–<lpage>13344</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002691-Williams1">
        <label>51</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williams</surname><given-names>RJ</given-names></name> (<year>1992</year>) <article-title>Simple statistical gradient-following algorithms for connectionist reinforcement learning</article-title>. <source>Mach Learn</source> <volume>8</volume>: <fpage>229</fpage>–<lpage>256</lpage>.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>