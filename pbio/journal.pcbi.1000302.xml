<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">08-PLCB-RA-0819R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000302</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology</subject><subject>Neuroscience/Behavioral Neuroscience</subject><subject>Neuroscience/Sensory Systems</subject><subject>Neuroscience/Psychology</subject><subject>Neuroscience/Experimental Psychology</subject></subj-group></article-categories><title-group><article-title>The Modulation Transfer Function for Speech Intelligibility</article-title><alt-title alt-title-type="running-head">Modulation Transfer Function of Speech</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Elliott</surname><given-names>Taffeta M.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Theunissen</surname><given-names>Frédéric E.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Helen Wills Neuroscience Institute, University of California Berkeley, Berkeley, California, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Department of Psychology, University of California Berkeley, Berkeley, California, United States of America</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">theunissen@berkeley.edu</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: TME FET. Performed the experiments: TME FET. Analyzed the data: TME FET. Contributed reagents/materials/analysis tools: TME FET. Wrote the paper: TME FET.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>3</month><year>2009</year></pub-date><pub-date pub-type="epub"><day>6</day><month>3</month><year>2009</year></pub-date><volume>5</volume><issue>3</issue><elocation-id>e1000302</elocation-id><history>
<date date-type="received"><day>18</day><month>9</month><year>2008</year></date>
<date date-type="accepted"><day>23</day><month>1</month><year>2009</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2009</copyright-year><copyright-holder>Elliott, Theunissen</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>We systematically determined which spectrotemporal modulations in speech are necessary for comprehension by human listeners. Speech comprehension has been shown to be robust to spectral and temporal degradations, but the specific relevance of particular degradations is arguable due to the complexity of the joint spectral and temporal information in the speech signal. We applied a novel modulation filtering technique to recorded sentences to restrict acoustic information quantitatively and to obtain a joint spectrotemporal modulation transfer function for speech comprehension, the <italic>speech MTF</italic>. For American English, the speech MTF showed the criticality of low modulation frequencies in both time and frequency. Comprehension was significantly impaired when temporal modulations &lt;12 Hz or spectral modulations &lt;4 cycles/kHz were removed. More specifically, the MTF was bandpass in temporal modulations and low-pass in spectral modulations: temporal modulations from 1 to 7 Hz and spectral modulations &lt;1 cycles/kHz were the most important. We evaluated the importance of spectrotemporal modulations for vocal gender identification and found a different region of interest: removing spectral modulations between 3 and 7 cycles/kHz significantly increases gender misidentifications of female speakers. The determination of the speech MTF furnishes an additional method for producing speech signals with reduced bandwidth but high intelligibility. Such compression could be used for audio applications such as file compression or noise removal and for clinical applications such as signal processing for cochlear implants.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>The sound signal of speech is rich in temporal and frequency patterns. These fluctuations of power in time and frequency are called modulations. Despite their acoustic complexity, spoken words remain intelligible after drastic degradations in either time or frequency. To fully understand the perception of speech and to be able to reduce speech to its most essential components, we need to completely characterize how modulations in amplitude and frequency contribute together to the comprehensibility of speech. Hallmark research distorted speech in either time or frequency but described the arbitrary manipulations in terms limited to one domain or the other, without quantifying the remaining and missing portions of the signal. Here, we use a novel sound filtering technique to systematically investigate the joint features in time and frequency that are crucial for understanding speech. Both the modulation-filtering approach and the resulting characterization of speech have the potential to change the way that speech is compressed in audio engineering and how it is processed in medical applications such as cochlear implants.</p>
</abstract><funding-group><funding-statement>The research was funded by NIDCD07293 to FET. The National Institute on Deafness and Other Communication Disorders played no other role in the study.</funding-statement></funding-group><counts><page-count count="14"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Human speech, like most animal vocalizations, is a complex signal whose amplitude envelope fluctuates timbrally in frequency and rhythmically in time. Horizontal cross-sections of the speech spectrogram as in <xref ref-type="fig" rid="pcbi-1000302-g001">Figure 1A</xref> describe the time-varying envelope for a particular frequency while vertical cross-sections at various time points show spectral contrasts, or variation in the spectral envelope shape (<xref ref-type="supplementary-material" rid="pcbi.1000302.s001">Audio S1</xref>). Indeed, the structure in the spectrogram of speech is not characterized by isolated spectrotemporal events but instead by sinusoidal patterns that extend in time and frequency over larger time windows and many frequency bands. It is well known that it is these patterns that carry important phonological information, such as syllable boundaries in the time domain, formant and pitch information in the spectral domain, and formant transitions in the spectrotemporal domain as a whole <xref ref-type="bibr" rid="pcbi.1000302-Liberman1">[1]</xref>. In order to quantify the power in these temporal and spectral modulations, the two-dimensional (2D) Fourier transform of the spectrogram can be analyzed to obtain the modulation power spectrum (MPS) of speech <xref ref-type="bibr" rid="pcbi.1000302-Chi1">[2]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Singh1">[3]</xref>. In this study, first we repeated this analysis using a time-frequency representation that emphasized differences in formant structure and pitch structure. Then we used a novel filtering method to investigate which spectral and temporal modulation frequencies were the most important for speech intelligibility. In this manner we obtained the speech modulation transfer function (speech MTF). We were then able to compare the speech MTF with the speech MPS in order to interpret the effect of modulation filters on perception of linguistic features of speech.</p>
<fig id="pcbi-1000302-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000302.g001</object-id><label>Figure 1</label><caption>
<title>Component spectrotemporal modulations make up the modulation spectrum.</title>
<p>(A) Spectrogram of a control condition sentence, “The radio was playing too loudly,” reveals the acoustic complexity of speech (<xref ref-type="supplementary-material" rid="pcbi.1000302.s001">Audio S1</xref>). All supporting sound files have been compressed as .mp3 files for the purpose of publication; original .wav files were used as stimuli. (B) Example spectrotemporal modulation patterns circled in the sentence (A) can be described as a time-varying weighted sum of component modulations. (C) The MPS shows the spectral and temporal modulation power in 100 sentences. The outer, middle, and inner black contour lines delineate the modulations contained in 95%, 90%, and 85% of the modulation power, respectively. Down-sweeps in frequency appear in the right quadrant, whereas upward drifts in frequency are in the left quadrant. Slower temporal changes lie near zero on the axis, while faster changes result in higher temporal modulations towards the left and right of the graph.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.g001" xlink:type="simple"/></fig>
<p>Our study both complements and unifies previous speech perception experiments that have shown speech intelligibility to depend on both spectral and temporal modulation cues, but to be surprisingly robust to significant spectral or temporal degradations. Speech can be understood with either very coarse spectral information <xref ref-type="bibr" rid="pcbi.1000302-Silipo1">[4]</xref>–<xref ref-type="bibr" rid="pcbi.1000302-terKeurs1">[8]</xref> or very coarse temporal information <xref ref-type="bibr" rid="pcbi.1000302-Drullman1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1000302-Arai2">[11]</xref>. Our goal was to unify spectral and temporal degradation experiments by performing both types of manipulations in the same space, namely, the space of joint spectrotemporal modulations given by the speech MPS. The approach makes advances in the rigor of signal processing, in the specificity of the manipulations allowed, and in the comparison with speech signal statistics. First, the approach depicts visually and quantifies the concomitant effects that temporal manipulations have on the spectral structure of the signal, and that spectral filtering has on temporal structure. Second, the technique offers the possibility of notch filtering in the spectral modulation domain, something which has not been done before. Whereas degradation by low-pass filtering can reveal the minimum spectral or temporal resolution required for comprehension, notch filtering can distinguish more limited regions of spectrotemporal modulations that differ in levels of importance for comprehension. Third, the modulation filtering technique can be used to target specific joint spectral and temporal modulations. In this study, this advantage was exploited in a two-step filtering procedure to measure the effects of precise temporal and spectral degradations in the range of modulations most important for intelligibility. In this procedure, we first removed potentially redundant information in higher spectral <italic>and</italic> temporal modulations, and then we applied notch spectral or temporal filters within the remaining modulation space. Finally, we were able to compare the results of the speech filtering experiments to the MPS of speech, in order to make an initial characterization of the speech MTF in humans. As far as we know, this is the first such comparison using a linear frequency axis and a modulation transfer function obtained directly from speech intelligibility experiments. The resultant speech MTF could be used to design more optimal speech compression such as that required by cochlear implants.</p>
<p>Neurophysiological research on animal perception of modulations inspired our study. While the cochlea and peripheral auditory neurons represent acoustic signals in a time-frequency decomposition (a cochleogram), higher auditory neurons acquire novel response properties that are best described by tuning sensitivity to temporal amplitude modulations and spectral amplitude modulations (reviewed in <xref ref-type="bibr" rid="pcbi.1000302-Shamma1">[12]</xref> and <xref ref-type="bibr" rid="pcbi.1000302-Eggermont1">[13]</xref>). By designing human psychological experiments using the same representations used in neurophysiological research, we can begin to link brain mechanisms and human perception.</p>
<p>Speech signals carry information about a speaker's emotion and identity in addition to the message content. As a final thrust of investigation, we tested whether modulations corresponding to acoustic features embedded in the speech signal enabled listeners to detect the gender of the speaker. Vocal gender identity has been shown to depend on some spectral features in common with, and some distinct from, the spectral features conferring speech intelligibility <xref ref-type="bibr" rid="pcbi.1000302-Fu1">[14]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Remez1">[15]</xref>.</p>
</sec><sec id="s2">
<title>Results</title>
<p>Spectrotemporal modulations underlying speech intelligibility and gender recognition were assessed in psychophysical experiments using sentences in which spectrotemporal modulations had been systematically filtered. Since our psychophysical experiments were in large part inspired by our analysis of the spectrotemporal modulations of speech, we begin by reporting the resulting modulation space. We will describe the characteristics of the MPS of speech and emphasize which characteristics are general to natural sounds, which are general to animal vocal communication signals, and which ones are more unique to human speech. The goal of the psychophysical experiments was to determine the subset of perceptible modulations that contribute exceptionally to speech intelligibility.</p>
<sec id="s2a">
<title>Modulation Power Spectrum of Speech</title>
<p>The MPS of American English (<xref ref-type="fig" rid="pcbi-1000302-g001">Figure 1C</xref>) was calculated from a corpus of 100 sentences (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). This speech modulation spectrum shares key features observed in other natural sounds. As in all natural sounds, most of the power is found for low modulation frequencies and decays along the modulation axes following a power law <xref ref-type="bibr" rid="pcbi.1000302-Singh1">[3]</xref>. Moreover, as typical of animal vocalizations, the MPS is not separable; most of the energy in high spectral modulations occurs only at low temporal modulation, and most high temporal modulation power is found at low spectral modulation <xref ref-type="bibr" rid="pcbi.1000302-Singh1">[3]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Cohen1">[16]</xref>. This characteristic non-separability of the MPS is due to the fact that animal vocalizations contain two kinds of sounds: short sounds with little spectral structure but fast temporal changes (contributing power along the x-axis at intermediate to high temporal frequencies), and slow sounds with rich spectral structure (found along the y-axis at intermediate to high spectral frequencies). In normal speech, this grouping of sounds corresponds roughly to the vocalic (slow sounds with spectral structure, produced with phonation) and non-vocalic acoustic contrasts (fast sounds with less spectral structure, produced without phonation). Animal vocalizations and human speech do have sound elements at intermediate spectrotemporal modulations, but these have less power (or in other words are less frequent) than expected from the power (or average occurrence) of spectral or temporal modulations taken separately, reflecting the non-separability of the MPS.</p>
<p>An additional aspect of human speech is that modulations separate into three independent areas of energy along the axis of spectral modulation, at low temporal modulation (<xref ref-type="fig" rid="pcbi-1000302-g001">Figures 1C</xref> and <xref ref-type="fig" rid="pcbi-1000302-g002">2</xref>). First, the triangular energy area at the lower spectral modulation frequencies corresponds to the coarse spectral amplitude fluctuations imposed by the upper vocal tract, namely the formants and formant transitions (labeled in <xref ref-type="fig" rid="pcbi-1000302-g002">Figure 2B</xref>). The other two areas of spectral modulation energy, found at higher levels, correspond to the harmonic structure of vocalic phones produced by the glottal pulse; this energy diverges into two areas because of the difference in pitch between the low male voice (highest spectral modulations) and the higher female voice (more intermediate spectral modulations). The lower register of the male voice produces higher spectral modulations because of the finer spacing of harmonics over that low fundamental. Equivalent pitches corresponding to the spectral modulations are labeled parenthetically in white on the y-axis of <xref ref-type="fig" rid="pcbi-1000302-g002">Figure 2</xref>. The MPS can also be estimated from time-frequency representations that have a logarithmic frequency axis (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>, and <xref ref-type="supplementary-material" rid="pcbi.1000302.s008">Figure S1</xref>). Although log-frequency representations are better models of the auditory periphery, the linear-frequency representation is more useful for describing the harmonic structure present in sounds. For example, the separation of the spectral structure of vocalic phones into three regions is a property that is observed only in the linear frequency representation (<xref ref-type="supplementary-material" rid="pcbi.1000302.s008">Figure S1</xref>).</p>
<fig id="pcbi-1000302-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000302.g002</object-id><label>Figure 2</label><caption>
<title>Spectral modulations differ in male and female speech.</title>
<p>(A,B) The MPS of the 50 corpus sentences spoken by males (A), and of the 50 spoken by females (B), with black contour lines as in <xref ref-type="fig" rid="pcbi-1000302-g001">Figure 1</xref>. White parenthetical labels on the y-axes of (A) and (B) show related frequencies demarcating the male and female vocal registers; they correspond to spectral modulations based on harmonic spacing. (C,D) Modulation filters that resulted in misidentification of the speaker's gender. (C) the speech MPS for female speakers is overlapped with the boundaries of the low-pass spectrotemporal filter. In this condition, speaker gender was misidentified in a quarter of the sentences, with 91% of those errors being females misidentified as male. (D) the same female speech MPS overlapped with a notch filter that removed modulations from 3 to 7 cycles/kHz. Of the 21% gender errors in this condition, 95% were female speakers misidentified as male.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.g002" xlink:type="simple"/></fig>
<p>Thus, in the speech MPS with linear frequency, not only do vocalic and non-vocalic sounds occupy different regions within the modulation space, but the spectral modulations for vocalic sounds corresponding to formants and male and female pitch occupy distinct regions. Also, human speech is symmetric between positive and negative temporal modulation frequencies, showing that there is equal power for upward frequency modulations (<xref ref-type="fig" rid="pcbi-1000302-g001">Figure 1C, left quadrant</xref>) and downward frequency modulations (right quadrant).</p>
</sec><sec id="s2b">
<title>Psychophysical Experiments in Spectrotemporal Modulation Filtering</title>
<p>Our modulation filtering methodology allowed us not only to rigorously degrade speech within its spectral and temporal structure but also to relate the results from the degradation to acoustic features of the signal that are important for different percepts, as described above. Our psychophysical experiments are organized in three sections. We first report results from the two sets of modulation filters applied to the whole spectrotemporal modulation spectrum of speech—low-pass filters and notch filters—which indicated a subset of modulations that are critical for speech understanding, thereafter designated the “core” modulations. Subsequently, we report results from notch filters applied to sentences containing only core modulations, further refining our identification of crucial spectrotemporal modulations.</p>
<sec id="s2b1">
<title>Low-pass modulation filtering</title>
<p>We scored the number of words reported correctly from sentences with low-pass filtered spectral or temporal modulations (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for the modulation filtering procedure) at cutoff frequencies roughly logarithmically distributed across the speech MPS (<xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3</xref>). Sentences were embedded in noise and played back at 3 different levels of signal-to-noise ratio (SNR). Comprehension dropped off significantly at around 4 cycles/kHz low-pass cutoff spectral frequency, and at 12 Hz in the temporal domain, with a further significant decrease at 6 Hz. Gray shading in the thumbnails of the modulation spectrum show the modulations of speech that were low-pass filtered spectrally (<xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3A</xref>), or temporally (<xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3B</xref>). The line graphs (<xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3C and 3D</xref>) show mean±s.e. performance on the sentence comprehension test for the spectral and the temporal conditions, at the three SNRs. Spectrograms of the example sentence from <xref ref-type="fig" rid="pcbi-1000302-g001">Figure 1</xref> show extreme spectral (0.5 cycles/kHz, <xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3E</xref>, <xref ref-type="supplementary-material" rid="pcbi.1000302.s002">Audio S2</xref>) and temporal smearing (3 Hz, <xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3F</xref>, <xref ref-type="supplementary-material" rid="pcbi.1000302.s003">Audio S3</xref>), in addition to the spectral smearing (4 cycles/kHz, <xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3G</xref>, <xref ref-type="supplementary-material" rid="pcbi.1000302.s004">Audio S4</xref>) and temporal smearing (12 Hz, <xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3H</xref>, <xref ref-type="supplementary-material" rid="pcbi.1000302.s005">Audio S5</xref>) conditions at which comprehension decreased significantly in comparison to control.</p>
<fig id="pcbi-1000302-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000302.g003</object-id><label>Figure 3</label><caption>
<title>Comprehension of low-pass modulation filtered sentences.</title>
<p>(A,B) Grayed areas of thumbnails show spectrotemporal modulations removed by low-pass modulation filtering in the spectral (A) or temporal (B) domain. Units and axis ranges are the same as in <xref ref-type="fig" rid="pcbi-1000302-g002">Figure 2</xref>. Each thumbnail represents a stimulus set analyzed in (C,D). (C,D) Mean±s.e. performance in transcribing words from the low-pass modulation filtered sentences. Cutoff frequencies on the x-axes of the two graphs are presented in units appropriate to the spectral or temporal domain, but could equally well be viewed on one continuous scale in either unit. Symbols show SNR levels. Dashed line shows control performance at +2 dB SNR; dotted line shows control performance at −3 dB SNR. Points at cutoff frequencies which share no capital letters in common (above line plots) are significantly different (repeated measures ANOVA, Bonferroni post-hoc correction, p&lt;0.0008) at the +2 dB SNR condition. (E and G) Spectrograms of an example sentence (same as in <xref ref-type="fig" rid="pcbi-1000302-g001">Figure 1</xref>) with the most extreme spectral modulation filtering (with a low-pass cutoff of 0.5 cycles/kHz; <xref ref-type="supplementary-material" rid="pcbi.1000302.s002">Audio S2</xref>) and the spectral modulation filtering at which comprehension became significantly worse (4 cycles/kHz; <xref ref-type="supplementary-material" rid="pcbi.1000302.s003">Audio S3</xref>), respectively. LP = Low-pass. (F and H) Spectrograms of the example sentence with the most extreme temporal modulation filtering tested (having a low-pass cutoff of ∼3 Hz; <xref ref-type="supplementary-material" rid="pcbi.1000302.s004">Audio S4</xref>), and the temporal modulation filtering at which comprehension became significantly worse (cutoff 12 Hz; <xref ref-type="supplementary-material" rid="pcbi.1000302.s005">Audio S5</xref>).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.g003" xlink:type="simple"/></fig>
<p>Together, the results from the spectral and temporal domains suggested that there exists a region, or “core”, of modulations below ∼4 cycles/kHz and ∼8 Hz that are essential for comprehension. Sentences containing only these core modulations served afterwards as a control condition and as starting material for further notch filtering. In a separate experiment, we also applied low-pass spectral filtering using spectral modulations obtained from a logarithmic frequency axis in the time-frequency representation (see <xref ref-type="supplementary-material" rid="pcbi.1000302.s008">Figure S1</xref>). Those data show that spectral modulations below 2 cycles/octave are important (for a center frequency of 500 Hz, 2 cycles/octave = 4 cycles/kHz). Finally, we also examined the effect of low-pass modulation filtering on nuclear vowel (h/V/d) and consonant discrimination (/C/a). Vowels were less affected by temporal filtering and consonants were less affected by spectral filtering (data not shown).</p>
</sec><sec id="s2b2">
<title>Notch modulation filtering</title>
<p>Next, we tested the effect of notch filters on speech comprehension. The widths of the notch filters were designed to be logarithmically proportional because the modulation power in the signal decreases following a power law from the origin in both the spectral and temporal dimensions <xref ref-type="bibr" rid="pcbi.1000302-Singh1">[3]</xref>. Also, psychophysical experiments suggest that the Q factor of the human temporal modulation filter is constant for frequencies up to 64 Hz <xref ref-type="bibr" rid="pcbi.1000302-Ewert1">[17]</xref>, and comparative judgments of auditory duration follow Weber-Fechner's law <xref ref-type="bibr" rid="pcbi.1000302-Creelman1">[18]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Stevens1">[19]</xref>. All notch filtering experiments were performed at the intermediate SNR level of +2 dB.</p>
<p>As in <xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3</xref>, thumbnails of the modulation spectrum in <xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4</xref> depict filtered areas layered in transparent gray. Note that temporal notch filters removed both positive and negative modulations, appearing as symmetric grayed areas (<xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4B</xref>). Bar graphs (<xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4D and 4E</xref>) show average±s.e. word comprehension. Light gray bars in these graphs denote the control condition (spectrogram inversion without modulation filtering) and the core condition in which inessential modulations were removed, by first low-pass filtering at 3.75 cycles/kHz and then at 7.75 Hz (<xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4C and 4G</xref>). Dark gray bars in the graphs show the performance for each of the five spectral (<xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4A</xref>; see one example in <xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4F</xref>, <xref ref-type="supplementary-material" rid="pcbi.1000302.s006">Audio S6</xref>) and five temporal notch modulation filters (<xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4B</xref>).</p>
<fig id="pcbi-1000302-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000302.g004</object-id><label>Figure 4</label><caption>
<title>Comprehension of speech with notch-filtered modulations or “core” modulations.</title>
<p>(A–C) The speech modulation spectrum with filtered modulations denoted by grayed areas as in <xref ref-type="fig" rid="pcbi-1000302-g005">Figure 5A</xref>. (A) Spectral notch modulation filters. (B) Temporal notch modulation filters. (C) Core modulations most essential to comprehension in <xref ref-type="fig" rid="pcbi-1000302-g005">Figure 5</xref> are depicted in full and zoomed-in thumbnail plots. Stimuli for the core condition were obtained by low-pass filtering in both the spectral and temporal modulation domains. (D,E) Mean±s.e. comprehension when either spectral (D) or temporal (E) modulation filters were applied to the sentences, along with control sentences (lighter gray bars) containing all or only core modulations (C). Stimulus conditions which share no lower case letters (above plots) in common are significantly different, as in <xref ref-type="fig" rid="pcbi-1000302-g005">Figure 5</xref> (repeated measures ANOVA). (F) Spectrogram of the example sentence after spectral modulations between 3 and 7 cycles/kHz were filtered out (<xref ref-type="supplementary-material" rid="pcbi.1000302.s006">Audio S6</xref>). (G) Spectrogram of the example sentence containing only the core of essential modulations below 7.75 Hz and 3.75 cycles/kHz (<xref ref-type="supplementary-material" rid="pcbi.1000302.s007">Audio S7</xref>).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.g004" xlink:type="simple"/></fig>
<p>Comprehension of core modulations was 75% word recognition (example sentence spectrogram in <xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4G</xref>, <xref ref-type="supplementary-material" rid="pcbi.1000302.s007">Audio S7</xref>). Of the spectrally delimited filtering, only the removal of modulations below 1 cycles/kHz significantly decreased sentence comprehension relative to control performance (<xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4D</xref>). In the temporal domain, the 7–15 Hz notch filter caused a small but significant decrease in intelligibility, yielding performance that was at a level similar to the core condition (<xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4E</xref>). More importantly, the removal of intermediate temporal modulations (either from 1–3 Hz or from 3–7 Hz) produced a significantly greater decrement in performance (<xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4E and 4F</xref>).</p>
</sec><sec id="s2b3">
<title>Notch filtering of the “core” modulations</title>
<p>Since the initial low-pass filtering experiment had revealed that spectral modulations below ∼4 cycles/kHz and temporal modulations below ∼8 Hz are essential for comprehension, we limited modulations to this core spectrotemporal range (&lt;3.75 cycles/kHz and &lt;7.75 Hz) and further applied notch filters to test which core modulations contribute most to comprehension. This dual filtering allowed us to remove potentially redundant information found at modulations outside of the core.</p>
<p><xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4C</xref> shows the core of modulations (right thumbnail is a zoom-in of left thumbnail; the magnified scale is used in the thumbnails of <xref ref-type="fig" rid="pcbi-1000302-g005">Figure 5A and 5B</xref>). Sentences limited to the core modulations provided the control condition since in this experiment the notch filters were applied to them (<xref ref-type="fig" rid="pcbi-1000302-g005">Figure 5A and 5B</xref>) instead of to sentences without any of the perceptible modulation spectrum previously removed (as in the other experiments; <xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3A and 3B</xref>). As explained above, the grayed areas in the thumbnail modulation spectra (<xref ref-type="fig" rid="pcbi-1000302-g005">Figure 5A and 5B</xref>) show which modulations were removed in each condition. Notch boundaries were again logarithmically spaced. There were only four spectral notches because the core modulations are already more limited in the spectral than the temporal domain.</p>
<fig id="pcbi-1000302-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000302.g005</object-id><label>Figure 5</label><caption>
<title>Comprehension of “core” modulations in speech with notch filtering.</title>
<p>(A,B) Notch filters in the spectral (A) or temporal (B) modulation domain removed modulations from sentences that contained only core modulations after having been low-pass filtered in both domains. As depicted in <xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4C</xref>, x- and y-axes are 0 to ±7.75 Hz and 0 to 3.75 cycles/kHz, respectively. (C,D) Comprehension when spectral (C) or temporal (D) notch filters were applied to sentences containing only core modulations. See <xref ref-type="fig" rid="pcbi-1000302-g006">Figure 6C</xref> for a thumbnail of the core modulations. As in <xref ref-type="fig" rid="pcbi-1000302-g005">Figures 5</xref> and <xref ref-type="fig" rid="pcbi-1000302-g006">6</xref>, conditions which share no lower-case labels in common are significantly different (repeated measures ANOVA).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.g005" xlink:type="simple"/></fig>
<p>Notch filters removing any of the core spectral modulations resulted in a decrease in intelligibility but this was especially true for the notch at the lowest modulation frequency (below 0.25 cycles/kHz) (<xref ref-type="fig" rid="pcbi-1000302-g005">Figure 5C</xref>). In the temporal dimension, any of the three temporal modulation notch filters above 0.75 Hz resulted in a decrease in performance, but in this case the effect was greater for higher temporal modulations (above 1.75 Hz), significantly decreased comprehension (<xref ref-type="fig" rid="pcbi-1000302-g005">Figure 5D</xref>).</p>
<p>The results of the notch filtering experiments show firstly that intermediate temporal modulations between 1 and 7 Hz are critical for speech intelligibility, whereas lower or higher temporal modulations are less critical. Secondly, the very low spectral modulations that we tested appeared to be critical. The human speech intelligibility transfer function appears therefore to show a band-pass tuning in the temporal domain and a low-pass tuning in the spectral domain.</p>
</sec></sec><sec id="s2c">
<title>Gender Identification</title>
<p>Subjects reported the gender of the speakers of the notch-filtered sentences. Even though sentences having modulations restricted to the “core” (<xref ref-type="fig" rid="pcbi-1000302-g004">Figures 4D</xref> and <xref ref-type="fig" rid="pcbi-1000302-g005">5C</xref>) were well comprehended, gender identification of the speakers of these sentences fell to 77%, where chance would be 50%. Of the gender errors, 91% occurred when the speaker was female. When modulations outside the core were spared, the notch-filter of spectral modulations between 3 and 7 cycles/kHz (<xref ref-type="fig" rid="pcbi-1000302-g002">Figures 2C</xref> and <xref ref-type="fig" rid="pcbi-1000302-g004">4F</xref>) significantly decreased gender identification (to 79%). Of these misidentified speakers, 95% were female. Both the core condition and this spectral notch condition lacked modulations in the 3–7 cycles/kHz range, where female speech has more power (core spectral modulations are below 3.75 cycles/kHz). Male speech has more power shifted to higher spectral modulations (6–11 cycles/kHz). Thus, spectral modulation filters in the uniquely male range produced no significant decrease in gender identification. These results can be explained by the fact that whenever the filtered sentences lacked spectral modulation information unique to the female vocal register, subjects guessed that the speaker was male.</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<p>This study attempts to use dynamic properties of sound, rather than the traditionally stereotyped cues of acoustic phonetics, to refashion a parsimonious account of speech perception. Specifically, we used a novel filtering technique to remove spectrotemporal modulations from spoken sentences in order to isolate the acoustic properties critical for identifying linguistic features and for recognizing gender as a personal attribute of the voice.</p>
<p>We first systematically degraded sentences by filtering specific temporal and spectral modulation frequencies and then examined the effect on the number of words comprehended. As we will discuss in more detail below, our study replicates, but also has several advantages over, previous degradations performed in the temporal or spectral domain alone. First, it provides a rigorous mathematical framework to precisely quantify what is being removed from the speech signal. Second, the framework unifies manipulations across two lines of research that are otherwise described orthogonally, namely AM and FM filtering <xref ref-type="bibr" rid="pcbi.1000302-Zeng1">[20]</xref>. Finally, we can make a direct connection between the acoustical space we manipulated and the information-bearing features of speech distributed in each particular region <xref ref-type="bibr" rid="pcbi.1000302-Mesgarani1">[21]</xref>.</p>
<p>In the MPS of American English (our prototype for human speech), the distinctive non-separable distribution of energy—namely, close to the x and y axes—corresponds roughly to a division between vocalic and non-vocalic sounds <xref ref-type="bibr" rid="pcbi.1000302-Singh1">[3]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Cohen1">[16]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Mesgarani1">[21]</xref>. Non-vocalic phones in speech tend to be rapid and to have little spectral structure whereas vocalic phones are longer and have more spectral structure. Our categorization of speech sounds along the spectral and temporal axes of the MPS remains, of course, rather coarse. For example, voicing is associated with multiple acoustic properties and is only one of the linguistic features (e.g., place of articulation, manner, rounding) needed in order to categorize phonemes <xref ref-type="bibr" rid="pcbi.1000302-Pickett1">[22]</xref>. A more detailed analysis of the MPS of individual phonemes or simple combinations of phonemes would further illustrate the usefulness of this methodology for speech analysis <xref ref-type="bibr" rid="pcbi.1000302-Mesgarani1">[21]</xref>.</p>
<p>Within the spectral structure especially associated with vocalic sounds, we also found a clear separation between pitch information and phonetic information (formants and formant transitions). The separation of the formant spectral frequencies from the pitch spectral frequencies had been described before and is one reason that cepstral analysis works well for the determination of formant frequencies <xref ref-type="bibr" rid="pcbi.1000302-Gold1">[23]</xref>. In the discussion that follows, we will relate performance on the comprehension task to the acoustic features of speech we filtered from the MPS.</p>
<p>Our low-pass spectral-modulation filtering experiment shows that speech intelligibility begins to degrade significantly when modulations below 4 cycles/kHz are removed. Not surprisingly, this definitive point corresponds to the upper extent of the area in the speech MPS occupied by energy associated with formants (<xref ref-type="fig" rid="pcbi-1000302-g001">Figures 1</xref> and <xref ref-type="fig" rid="pcbi-1000302-g002">2</xref>). The separation between formant peaks in English vowels is greater than 500 Hz (or 2 cycles/kHz) <xref ref-type="bibr" rid="pcbi.1000302-Hillenbrand1">[24]</xref>, but finer spectral resolution (up to 4 cycles/kHz) would be beneficial to capture further the overall spectral shape of the formant filters and to detect formant transitions.</p>
<p>There is a large literature on how spectral degradation affects speech comprehension, the most similar studies being those of Shannon and Dorman and colleagues <xref ref-type="bibr" rid="pcbi.1000302-Dorman1">[5]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Shannon1">[6]</xref> who have investigated speech intelligibility with very limited spectral resolution as one would experience with a cochlear implant. Shannon et al. <xref ref-type="bibr" rid="pcbi.1000302-Shannon1">[6]</xref> reported that speech intelligibility in a noise-free setting was fully preserved with spectral structure present in only 4 frequency channels below 4 kHz. These spectral bands would correspond in our implementation to a low-pass filter cutoff of approximately 1 cycles/kHz, or 1.7 cycles/octave, which is below the level needed for fully resolving formant spectral peaks and considerably below our cutoff of 4 cycles/kHz (or 2 cycles/octave as shown in <xref ref-type="supplementary-material" rid="pcbi.1000302.s008">Figure S1</xref>). However, when noise is present, Friesen et al. have shown that intelligibility increased with additional spectral channels <xref ref-type="bibr" rid="pcbi.1000302-Friesen1">[25]</xref>. In that study, for 0 dB SNR, 16 channels spaced below 6 kHz (or approximately 3.75 cycles/kHz) yielded significant additional comprehension over that of more degraded speech with fewer frequency channels. Our results are consistent with that result, and our study brings several additional insights to this analysis. First, the notch filtering experiments unequivocally demonstrate that the spectral MTF is truly low-pass. Removing lower (or intermediate) spectral modulations while preserving higher spectral modulations results in significant decreases in speech intelligibility. In other words, there does not appear to be information that is redundant between the spectral modulations below 4 cycles/kHz and higher spectral modulations (further details of the notch filtering results are discussed below). Second, our comparison between the speech MTF and the speech MPS offers an obvious explanation for the critical spectral frequency cutoff: it corresponds to the modulation power boundary of formants and formant transitions. Finally, by examining how much modulation power was removed in the filtering operations, we can also say that the crucial modulation areas are not simply the ones with the higher power in the speech MPS. For example, the region of the core notch filter between 0.25 and 0.75 cycles/kHz contributes less to intelligibility than the 0–0.25 cycles/kHz area, although the former contains higher power. Humans appear to be particularly adept at detecting very low modulations in the spectral envelope and at using that information for speech intelligibility.</p>
<p>In the temporal dimension, we showed that filtering the amplitude envelope of the speech signal below 12 Hz results in significant intelligibility deficits. Our results are similar to experiments in which the temporal envelope of speech was low-pass filtered or degraded. For Dutch, English and Japanese, it was shown that the region below 8 Hz is critical for speech comprehension <xref ref-type="bibr" rid="pcbi.1000302-Drullman1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1000302-Arai2">[11]</xref>. This critical modulation is somewhere between the temporal modulations corresponding to the rate of syllables, at around 2 to 5 Hz <xref ref-type="bibr" rid="pcbi.1000302-Pickett1">[22]</xref>, and those corresponding to phonemes, at around 15–30 Hz <xref ref-type="bibr" rid="pcbi.1000302-Liberman2">[26]</xref>–<xref ref-type="bibr" rid="pcbi.1000302-Greenberg1">[28]</xref>. In the MPS, we observe that frequencies below 10 Hz account for approximately 85% of all the spectrotemporal modulation power. Examining the temporal modulation spectrum as a function of frequency bands (rather than as a function of spectral modulations, as shown in the MPS), Greenberg and Arai showed that the peak in power lies between 4 and 6 Hz <xref ref-type="bibr" rid="pcbi.1000302-Greenberg2">[29]</xref>. By preserving frequencies below 8 Hz, one therefore retains most of the power in the temporal modulation spectrum. Qualitatively, the speech sounds that were heavily temporally filtered (below 5 Hz) sounded like reverberated speech, consistent with the observation that it is the higher temporal modulation frequencies that are affected in reverberant environments <xref ref-type="bibr" rid="pcbi.1000302-Kusumoto1">[30]</xref>.</p>
<p>Interpreting which modulations proved crucial in the low-pass spectral or temporal filtering results is problematic because each relative lowering of the cutoff frequency removed increasingly more modulations. Furthermore, comparisons between low-pass cutoffs do not exclude the possibility that higher intelligibility could be achieved with isolated regions of the MPS. To obtain something akin to a modulation transfer function (MTF) for speech intelligibility, low-pass filtering manipulation must be complemented with high-pass filtering. Alternatively, a transfer function can be obtained directly from notch filtering experiments. We chose the latter approach and based the design of our notch filters on the results from the low-pass experiments. The combination of notch filtering and low-pass filtering also allowed us to examine areas in the speech MPS that carry redundant information.</p>
<p>Two conclusions can be made from the notch filtering experiments. First, the results show a low-pass spectral tuning with most of the gain between 0 and 1 cycles/kHz, and a band-pass temporal tuning with most of the gain between 1 and 7 Hz. Second, the results show the high level of redundancy in the speech signal. The intelligibility of most notch-filtered stimuli remained excellent. This is even more remarkable considering that tests were done with a SNR of 2 dB. Redundancy is evident also when one examines the difference in results obtained from the low-pass and notch filters. Notably, the low-pass cutoff spectral frequency of 2 cycles/kHz significantly reduced performance as compared to the 4 cycles/kHz condition, whereas the 1–3 and 3–7 cycles/kHz notch filters straddling that range of modulations produced no significant decrease in performance. This discrepancy suggests that some of the information about formant structure in the 1–4 cycles/kHz range can also be found at higher spectral modulation frequencies. For this reason, we conducted the second notch filter experiment that operated on the core modulations (modulations below ∼4 cycles/kHz and ∼8 Hz). This second notch experiment allowed us to obtain a more detailed MTF.</p>
<p>The final speech MTF was obtained by combining the results of the spectral and temporal notch filters applied to the whole MPS. For this purpose, we calculated the average percentage error in word comprehension, and divided by the average control comprehension. Then we multiplied the normalized comprehension errors from the spectral notch filters (<xref ref-type="fig" rid="pcbi-1000302-g006">Figure 6A, vertical stripes</xref>), and the temporal notch filters (<xref ref-type="fig" rid="pcbi-1000302-g006">Figure 6A, horizontal stripes</xref>). The resulting color plot indicates which MPS areas are more important (red) for speech comprehension, and which are less important (blue). For comparison, we also generated a summary plot from the low-pass spectral and temporal modulation filters (<xref ref-type="fig" rid="pcbi-1000302-g006">Figure 6B</xref>). In this case, the subsequent increases in error caused by each lowering of the cutoff modulation frequency were used. A similar analysis of the notch filters applied to sentences containing only core modulations (<xref ref-type="fig" rid="pcbi-1000302-g006">Figure 6C</xref>, redness indicates importance for comprehension) gave an overall impression in general agreement with the respective areas of <xref ref-type="fig" rid="pcbi-1000302-g006">Figure 6A</xref>.</p>
<fig id="pcbi-1000302-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000302.g006</object-id><label>Figure 6</label><caption>
<title>Combination of results from low-pass and notch modulation filtering.</title>
<p>(A,B) To combine the spectral and temporal results from low-pass (A) and notch (B) modulation filtering, we calculated the average percent error in word comprehension, and divided by the average control comprehension. Then we multiplied the normalized errors from the spectral and temporal notch filters. Black lines indicate the contours of modulation power, as in <xref ref-type="fig" rid="pcbi-1000302-g001">Figure 1</xref>. Red areas are more important for speech comprehension than blue. The summary plot of the low-pass spectral and temporal modulation filters used the additional error caused by each subsequent lowering of the cutoff. Notch and low-pass experiments had somewhat different results in the spectral domain. The notch filtering implicated modulations closer to the origin, but still intermediate in temporal modulation, as most crucial. This discrepancy suggests a non-linearity in the relative contribution of modulations: the removal of intermediate spectral modulations matters more when higher spectral modulations are missing as well. (Dropping the low-pass cutoff spectral frequency from 4 to 2 cycles/kHz significantly reduced performance, but the 1–3 and 3–7 cycles/kHz notch filters straddling that range produced no significant difference.) (C) Schematic of modulations underlying comprehension and gender identification. The summary cartoon shows a region of low spectral and intermediate temporal modulations is of the greatest importance for speech intelligibility (red), while a separate band of higher spectral modulations (blue) make a speaker sound female. Yellow outlines the modulations that did not significantly contribute to sentence comprehension in any experiment. (D) Sentence modulation transfer function. When compression design, speech recognition by machines, and cochlear implant applications impose constraints on the bandwidth of a speech signal, modulation filtering could reduce a speech signal to only the modulation components needed for comprehension (red area). Depending on the bandwidth permitted, increasingly more of the orange and then yellow areas of the modulation spectrum could be included to add to the perception of vocal source characteristics.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.g006" xlink:type="simple"/></fig>
<p>It should be noted that, to generate this initial speech MTF, we assumed that spectral and temporal degradations affect the speech signal independently, which allowed us to multiply the normalized comprehension errors. We know, however, from the discrepancy between the comprehension after notch-filtering of core modulations, and the comprehension after notch-filtering of all modulations (namely, removal of intermediate spectral modulations is more detrimental to performance if higher spectral modulations have been removed as well), that there must exist some spectrotemporal interdependence. We also assumed that the MTF is symmetric along positive and negative modulation frequencies, in other words, that the gain in the MTF for joint spectrotemporal modulations corresponding to down-sweeps equals the gain for up-sweeps. Although we have not further explored the interdependence of the spectral and temporal modulations, our joint spectrotemporal modulation filtering technique opens the door to future studies directly assessing the degree of interdependence and potential asymmetry.</p>
<p>The shape of our final speech MTF (temporally band-pass and spectrally low-pass) approximately matches the shape of a psychophysical MTF that was obtained from detection thresholds for broadband moving ripples (corresponding to a single point in the MPS) in white noise <xref ref-type="bibr" rid="pcbi.1000302-Chi1">[2]</xref>, but with some important differences. Chi et al. found that the human MTF was low-pass for spectral and temporal modulations, with increases in threshold detection for modulations greater than 2 cycles/octave and 16 Hz (Footnote: Chi et al. state that their MTF is low-pass in the temporal domain but their psychometric function does show that detection at the very low temporal modulations is somewhat more difficult than at the low intermediate temporal modulations). In comparison, if we examined only our low-pass filtering results, we would find modulation cutoff values around 4 cycles/kHz and 12 Hz. (Note that 4 cycles/kHz corresponds to 2 cycles/octave for center frequencies of 500 Hz, and that we too obtained a cutoff value of 2 cycles/octave using log frequency as shown in <xref ref-type="supplementary-material" rid="pcbi.1000302.s008">Figure S1</xref>). The estimation of these upper boundaries is therefore very similar between the two studies. However, our complete speech MTF based on the combination of notch and low-pass filters shows a much more restricted area of high gain. For example, while the MTF of Chi et al. is relatively flat all the way to 2 cycles/octave, our speech MTF shows that the lowest spectral modulations (&lt;0.25 cycles/kHz) play a more important role than the higher ones (&gt;0.5 cycles/kHz). There are therefore important differences between the MTF obtained by measuring detections of ripple sounds in noise and the one obtained by performing notch filtering operations on speech. While humans might be equally good at detecting low and intermediate spectral modulations, the lower ones carry more information for speech intelligibility. The intermediate modulations should carry more information for other auditory tasks such as pitch perception.</p>
<p>While animal models of speech perception remain a stretched analogy, models of animal sensitivity to relevant modulations hold more immediate potential. The shape of our speech MTF also resembles the MTFs that have been obtained for mammalian <xref ref-type="bibr" rid="pcbi.1000302-Miller1">[31]</xref> and avian <xref ref-type="bibr" rid="pcbi.1000302-Woolley1">[32]</xref> high-level auditory neurons. This correlation between the power of the spectrotemporal modulations in speech (the speech MPS), the MTF resulting from tests of speech intelligibility, the MTF derived from detection of synthetic sounds <xref ref-type="bibr" rid="pcbi.1000302-Chi1">[2]</xref>, and the tuning properties of auditory neuron ensembles suggests a match between the speech signal and the receiver. The most informative modulations in speech, and in other animal communication signals, occur in regions of the modulation spectrum where humans show high sensitivity and where animals' high-level auditory neurons have the highest gain <xref ref-type="bibr" rid="pcbi.1000302-Eggermont1">[13]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Joris1">[33]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Gill1">[34]</xref>.</p>
<p>We also examined the role of modulations in the task of recognizing the gender of a speaker. The MPSs of male and female speech differ in the frequency rate at which power is concentrated in the higher spectral modulations (<xref ref-type="fig" rid="pcbi-1000302-g002">Figure 2</xref>). In our MPS representation, the pitch-associated spectral frequencies of male and female speakers showed a bimodal distribution: the two modes correspond to the glottal action of the vocal cords pulsing at ∼150 Hz in adult male speakers and at above 200 Hz in females <xref ref-type="bibr" rid="pcbi.1000302-Pickett1">[22]</xref>. The spectral notch filter that removed the high spectral modulation power unique to the female voice confused listeners' percept of gender, such that half of the female stimuli notch filtered between 3–7 cycles/kHz sounded male to subjects. Control stimuli containing only the core modulations, which likewise lack the female-specific modulation power, similarly confused listeners. We conclude that modulations between 3 and 7 cycles/kHz give rise to the percept of female vocal pitch. It is interesting that removal of the modulations underlying the male vocal register did not appear to detract from perception of speaker masculinity. Although fundamental frequencies provide the basis for gender recognition particularly in vowels <xref ref-type="bibr" rid="pcbi.1000302-Peterson1">[35]</xref>, it has also been shown that the fundamental and the second formant frequency are equally good predictors of speaker gender <xref ref-type="bibr" rid="pcbi.1000302-Childers1">[36]</xref>. Therefore the lower spectral modulations could carry additional gender information, but the acoustic distinction fails to explain the bias for male identification. Alternatively, the perception of vocal masculinity could depend more on gender-specific articulatory behaviors which account for social “dialectal” gender cues distinguishing even pre-pubescent speakers <xref ref-type="bibr" rid="pcbi.1000302-Johnson1">[37]</xref>.</p>
<p>Our results have implications for speech representation purposes including compression, cochlear design, and speech recognition by machines. In both speech compression applications and signal processing for cochlear design, the redundancy of the speech signal allows a reduction in the bandwidth of a channel through which the signal is represented. For this purpose, limiting spectral resolution has been a favorite solution both because of the robustness of the signal to such deteriorations <xref ref-type="bibr" rid="pcbi.1000302-Shannon1">[6]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Greenberg2">[29]</xref> and because of engineering design constraints for cochlear implants. However, in noisy environments, additional spectral information results in significant speech hearing improvement <xref ref-type="bibr" rid="pcbi.1000302-Zeng1">[20]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Friesen1">[25]</xref>. Our approach provides a guided solution: after determining the speech MTF, one can selectively reduce the bandwidth of the signal by first representing key spectral modulations and then systematically including the most important adjacent spectrotemporal modulations to capture the greatest overall space within constraints, as illustrated in cartoon form in <xref ref-type="fig" rid="pcbi-1000302-g006">Figure 6</xref> (see also <xref ref-type="bibr" rid="pcbi.1000302-Chi1">[2]</xref>). Our initial experiment with gender identification, together with research in music perception <xref ref-type="bibr" rid="pcbi.1000302-McDermott1">[38]</xref>, shows that the most advantageous solution will depend on the task and the desired percept. Finally, the speech MTF could also be used as a template for filtering out broadband noise: a modulation filtering procedure can be used to emphasize the modulations important for speech and to de-emphasize all others. Both the speech compression and the speech filtering operation require a decomposition of the sound in terms of spectrotemporal modulations, as well as a re-synthesis. These are not particularly simple operations (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>), but with advances in signal processing they will become possible in real time. After all, a similar operation appears to happen in real time in the auditory system <xref ref-type="bibr" rid="pcbi.1000302-Shamma1">[12]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Mesgarani1">[21]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Versnel1">[39]</xref>.</p>
</sec><sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Ethics Statement</title>
<p>Subjects gave written consent as approved by the Committee for the Protection of Human Subjects at University of California, Berkeley.</p>
</sec><sec id="s4b">
<title>Subjects</title>
<p>Native American-English speakers of mixed gender (20 in the low-pass experiment, aged 18–34 yr; and 17 in the notch experiment, age range 18–36 yr) volunteered to participate in the study. Audiograms showed that their hearing thresholds were normal from 30 to 15,000 Hz; one subject was excluded due to high-frequency hearing loss.</p>
</sec><sec id="s4c">
<title>Stimuli Materials</title>
<p>Acoustically clean recordings of spoken sentences were obtained from the soundtrack of the Iowa Audiovisual Speech Perception videotape <xref ref-type="bibr" rid="pcbi.1000302-Tyler1">[40]</xref>. The soundtrack was digitized at 32 kHz sampling rate in our laboratory using TDT System II hardware. This corpus consists of 100 short complete sentences read without emotion by six adult male and female American-English speakers. See <xref ref-type="fig" rid="pcbi-1000302-g001">Figure 1</xref> for the spectrogram of one example, “The radio was playing too loudly.” The corpus has been used by previous studies of speech perception <xref ref-type="bibr" rid="pcbi.1000302-Dorman1">[5]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Shannon1">[6]</xref>. The original speech sentences were normalized for power. The synthetic degraded speech signals were generated from this original set by a novel filtering procedure performed on the log spectrogram, as described below.</p>
</sec><sec id="s4d">
<title>The Modulation Power Spectrum</title>
<p>The modulation power spectrum (MPS) of a sound is the amplitude spectrum of the 2D Fourier Transform of a time-frequency representation of the sound's pressure waveform <xref ref-type="bibr" rid="pcbi.1000302-Singh1">[3]</xref>. The MPS can be estimated for a single sound (e.g. one sentence) or for an ensemble of sounds (e.g. 50 sentences from adult male speakers). In our analysis, the time-frequency representation is the log amplitude of a spectrogram obtained with Gaussian windows. Gaussian windows are used because of their symmetry in time-frequency and because they result in time-frequency representations that are more easily invertible <xref ref-type="bibr" rid="pcbi.1000302-Cohen2">[41]</xref>. As in cepstral analysis <xref ref-type="bibr" rid="pcbi.1000302-Gold1">[23]</xref>, the logarithm of the amplitude of the spectrogram is used to disentangle multiplicative spectral or temporal modulations into separate terms. For example, in speech sounds, the spectral modulations that constitute the formants in vowels (timbre) separate from those that constitute the pitch of the voice (<xref ref-type="fig" rid="pcbi-1000302-g002">Figure 2B</xref>). The MPS is then the amplitude squared as a function of the Fourier pairs of the time and frequency axis of the spectrogram of the log amplitude of this spectrographic representation. We call these two axes the temporal modulations (in Hz) and the spectral modulations (in cycles/kHz). One of these two axes must have positive and negative frequency modulations to distinguish upward frequency modulations (e.g., cos(ω<sub>s</sub>f-ω<sub>t</sub>t)) from downward modulations (e.g., cos(ω<sub>f</sub>f+ω<sub>t</sub>t)). By convention, we use positive and negative temporal modulations. The time-frequency resolution scale of the spectrogram (given by the width of the Gaussian window) determines the upper bounds of the temporal and spectral modulation in an inverse relationship as a result of the uncertainty principle or time-frequency tradeoff. The time-frequency scale must therefore be chosen carefully so that modulation frequencies of interest are considered. The choice of time-frequency scale can be made in a somewhat systematic fashion by using a value for which the shape of the modulation spectrum does not change very much. At these values of time-frequency scale, most of the energy in the modulation spectrum would be far from the boundaries determined by the time-frequency tradeoff <xref ref-type="bibr" rid="pcbi.1000302-Singh1">[3]</xref>. For analyzing our original and filtered signals, we used a time-frequency scale given by a Gaussian window of 10 ms in the time domain or 16 Hz in the frequency domain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000302.e001" xlink:type="simple"/></inline-formula>. For obtaining the MPS of sound ensembles, sounds in their spectrographic representation were divided into segments of 1 s and the MPS for each segment was estimated before averaging to obtain a power density function. The boundaries of the modulation spectrum at the time-frequency scale of 10 ms–16 Hz are 50 Hz and 31 cyc/kHz. At this time-frequency scale, approximately 90% of the power in the modulation spectrum was found for temporal modulations below 25 Hz and for spectral modulations below 16 cycles/kHz, justifying the choice (<xref ref-type="fig" rid="pcbi-1000302-g001">Figure 1</xref>). Moreover, the temporal and spectral modulation cutoffs correspond approximately to the critical modulation frequency at which amplitude modulated tones and noise start to promote a pitch percept <xref ref-type="bibr" rid="pcbi.1000302-Joris1">[33]</xref>. Thus, when we use this particular time-frequency scale, the temporal modulation frequencies analyzed are perceived predominantly as temporal changes, while higher temporal modulations (those above 50 Hz) which would mediate a percept of pitch are found along the spectral modulation axis. Using wider frequency filters might cause spectral modulation power that is plotted high on the ordinate (e.g., 5 cycles/kHz corresponding to a 200 Hz pitch) to appear instead at a correspondingly high temporal modulation (200 Hz) on the abscissa.</p>
<p>For the modulation filtering operation described below we used other time-frequency scales which were adapted to the filter's cutoff frequencies and thus improved the required spectrogram inversion step in that process.</p>
<p>The MPS can be obtained from a time-frequency decomposition with a linear frequency axis (resulting in spectral modulations in units of cycles/kHz), or from a decomposition with a log frequency axis (resulting in spectral modulation in units of cycles/octave). The log frequency axis is a better model of the decomposition that occurs in the auditory periphery, but we found that the linear-frequency scale is a better decomposition for describing sounds that have harmonic structure. We suggest that higher level neurons may be equally well described as representing either linear or log scale frequency <xref ref-type="bibr" rid="pcbi.1000302-Gill2">[42]</xref>. In any case, both representations are useful. To be able to compare our results to other published work, we additionally obtained the speech MPS and psychometric curves using the log-frequency representation. These results are shown in <xref ref-type="supplementary-material" rid="pcbi.1000302.s008">Figure S1</xref>.</p>
</sec><sec id="s4e">
<title>Synthesis of Degraded Speech</title>
<p>The sentences were degraded by a novel modulation filtering procedure. In brief, the sound is first represented in its spectrographic representation using a log-spectrogram calculated with Gaussian windows as described above. Then a new log-spectrogram is obtained by a 2D filtering operation. This filtering operation is performed in the Fourier domain of the modulation amplitude and phase in the following way. First the 2D FFT of the log spectrogram is calculated. Then the amplitudes of specific temporal and spectral modulations that we want to filter out are set to zero. The inverse 2D FFT yields the desired filtered log-spectrogram. After exponentiation, the spectrogram is then inverted using an iterative spectrogram inversion algorithm <xref ref-type="bibr" rid="pcbi.1000302-Griffin1">[43]</xref>. We then verified the procedure by calculating the spectrogram and MPS of the filtered sound. For a measure of the errors introduced by spectrogram inversion, we squared the differences between the desired spectrogram and the spectrogram obtained, and divided by the desired spectrogram power, summing the resulting values over time and frequency. Across the 100 stimulus sentences in the control condition, the residual error at the end of 20 algorithm iterations averaged 2.5%. When the 100 sentences were low-pass filtered in one step to create stimuli with only the core modulations, the average residual error after the 20 algorithm iterations was 6.3%. The modulation filtering was written in Matlab using modified code from Malcolm Slaney's Auditory Toolbox for the spectrogram inversion routine <xref ref-type="bibr" rid="pcbi.1000302-Slaney1">[44]</xref>. The complete program is available upon demand. The iterative method improves upon earlier overlap-and-add methods that had to compensate for the retention of phase information that unintentionally preserves some spectral information targeted for removal <xref ref-type="bibr" rid="pcbi.1000302-Baer1">[7]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-terKeurs1">[8]</xref>.</p>
<p>For the low-pass modulation filtering procedure, the time-frequency scale of the spectrogram was adjusted depending on the desired modulation frequency cutoffs of the modulation filter. For example, if the amplitude of spectral modulation frequencies above 2 cycles/kHz was to be set to zero, then using a time-frequency scale where spectral modulations were represented only up to values approaching 2 cycles/kHz gave better results. In this example, one could use a time-frequency window in the spectrogram of 1.25 ms–128 Hz to obtain a MPS with boundaries at 402 Hz and 3.9 cycles/kHz. Such adjustments made the inverting process much more efficient. Moreover, for low-pass filtering only, one could take this procedure to the extreme and calculate the spectrogram at a time-frequency scale that corresponds exactly to the modulation frequency cut-off of the filter. In that case, the spectrogram would not require any additional filtering and the spectrogram inversion routine can be by-passed altogether. One can instead directly obtain the filtered sounds by using the amplitude envelopes in each frequency band of the spectrogram and using these to modulate a set of narrowband signals of the same bandwidth and center frequency but unitary amplitude. These unit-amplitude narrowband signals can be obtained from Gaussian white-noise that is decomposed through the same spectrographic filter bank <xref ref-type="bibr" rid="pcbi.1000302-Smith1">[45]</xref> or, equivalently, by generating them directly using an analytic signal representation <xref ref-type="bibr" rid="pcbi.1000302-Theunissen1">[46]</xref>. In the analytical representation the amplitude is set to 1 and the instantaneous phase is random but band limited so that the instantaneous frequency remains within the band. In this study, this direct method was used to generate the low-pass modulation-filtered sentences. The modulation filtering that involved notch or band-stop filtering was done with the complete spectrogram filtering and inverting procedure. In the direct methods, the frequency cutoff for temporal frequencies is inversely related to the frequency cutoff for spectral frequencies but the conjugate boundary was always far from the limits being considered here. For example, a 49 Hz low-pass temporal filter had a conjugate spectral frequency cutoff of 32 cycles/kHz and any temporal filtering with cutoff frequencies below 49 Hz has spectral modulations cutoffs higher than 32 cycles/kHz (<xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3A and 3B</xref>). Because of this relationship the panels C and D of <xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3</xref> could be merged into one plot that would show a unimodal (inverted U) psychometric curve as a function of a spectrotemporal cutoff (as in <xref ref-type="supplementary-material" rid="pcbi.1000302.s008">Figure S1</xref>). More details on these sound synthesis procedures and on time-frequency scale effects can be found in <xref ref-type="bibr" rid="pcbi.1000302-Theunissen1">[46]</xref> and <xref ref-type="bibr" rid="pcbi.1000302-Singh1">[3]</xref>. A control (unfiltered) speech sentence was obtained by inverting the unfiltered log-spectrogram obtained with the 10 ms–16 Hz time-frequency scale (low-pass experiment) or 5 ms–32 Hz scale (notch experiment). The control sentences sounded very similar to the original sentences and yielded high levels of intelligibility.</p>
<p>Errors calculated during resynthesis depend on the bandwidth of the time-frequency scale. Residual errors in the control case of spectrogram inversion without filtering would barely be affected by changing the time-frequency scale from 5 ms–32 Hz to 1.25 ms–128 Hz (2.92% vs. 2.52% after 20 iterations, averaged over all 100 sentences). Similarly, in the case of temporal and spectral low-pass filtering leaving only core modulations, this time-frequency change would make a minimal improvement in the residual errors (5.49% vs. 6.29%). However, in the case of low-pass spectral modulation filtering with a 2 cycles/kHz cutoff, the 128 Hz time-frequency scale would double residual errors (12.18% vs. 6.41%). Using the 128 Hz time-frequency scale for temporal low-pass filtering with a 6 Hz cutoff would similarly increase residual error (5.64% vs. 2.02%).</p>
</sec><sec id="s4f">
<title>Experimental Procedures</title>
<p>All sounds were presented through headphones (Sennheiser HD265 Linear) to subjects who sat in a sound attenuated chamber. An audiogram from 30 Hz to 15 kHz was obtained initially for each subject, using an adaptive staircase procedure (Tucker Davis Technologies software PsychoSig) and subjects who had thresholds of 20 dB above normal were excluded.</p>
<p>For the comprehension test, the sentences were embedded in Gaussian white noise (0–20 kHz). The white-noise lasted 6 seconds and the sentences (filtered and control) started at random times between 300 ms and 2 s after the onset of the noise. The white noise was played at a level of 65 dB SPL (B&amp;K Level Meter, A-weighting, measured with headphone coupler from B&amp;K). The modulated speech sentences were played at 3 different levels: 72 dB, 67 dB, and 62 dB SPL (B&amp;K level meter, A-weighting, peak level with slow integration, headphone coupler). The 5 dB attenuation steps were obtained using a programmable attenuator (Tucker Davis Technologies). The signal to noise ratios (SNR) calculated from the SPL measurements of the speech and noise signals were therefore +7, +2 and −3 dB. We also calculated the SNR in terms of the RMS values of the sound pressure waveform of the noise and speech and found almost identical values (6.7 dB, 1.7 dB and −3.3 dB). These SNRs were chosen in pilot data to yield complete sigmoidal psychometric tuning curves in the low-pass filtered conditions, and almost perfect speech intelligibility for the control condition <xref ref-type="bibr" rid="pcbi.1000302-Plomp1">[47]</xref>. Furthermore, these SNRs cover the 3 dB SNR level that presents little difficulty for normal listeners but reduces comprehension in the hearing impaired <xref ref-type="bibr" rid="pcbi.1000302-Plomp2">[48]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Glasberg1">[49]</xref>.</p>
<p>Subjects listened to the sentences at their own pace, pressing a button to elicit the next stimulus. They were instructed to type whatever words they heard followed by whether they perceived the speaker's gender to be male or female. Subjects were asked to guess if necessary, but not to force sentences into making sense if any words did not make sense together. The typed response files were scored for the percentage of words reported correctly, with an algorithm to compensate for small spelling errors. Baseline performance under control conditions and with +2 dB SNR was around 90%.</p>
<p>During an experiment each subject heard all 100 sentences in the corpus without repetitions, so that each sentence was pseudorandomly assigned only to one normal (control) or filtered condition at one level. The SNR levels and the filtering conditions were presented in pseudorandom order. The notch-filtered sentences were presented only at +2 dB SNR.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1000302.s001" mimetype="video/mpeg" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.s001" xlink:type="simple"><label>Audio S1</label><caption>
<p>Example sentence under control condition. Mp3 file after conversion from the original wave file of an example stimulus sentence in <xref ref-type="fig" rid="pcbi-1000302-g001">Figure 1A</xref>. No modulation filtering was performed under this condition controlling for spectrogram inversion.</p>
<p>(0.09 MB MPG)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000302.s002" mimetype="video/mpeg" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.s002" xlink:type="simple"><label>Audio S2</label><caption>
<p>Low-pass modulation filtering at 0.5 cyc/kHz. Mp3 of an example sentence (<xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3E</xref>) with the most extreme spectral modulation filtering (with a low-pass cutoff of 0.5 cyc/kHz).</p>
<p>(0.09 MB MPG)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000302.s003" mimetype="video/mpeg" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.s003" xlink:type="simple"><label>Audio S3</label><caption>
<p>Low-pass modulation filtering at 3 Hz. Mp3 of the example sentence with the most extreme temporal modulation filtering tested (having a low-pass cutoff of about 3 Hz; <xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3F</xref>).</p>
<p>(0.09 MB MPG)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000302.s004" mimetype="video/mpeg" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.s004" xlink:type="simple"><label>Audio S4</label><caption>
<p>Low-pass modulation filtering at 4 cyc/kHz. Mp3 of the example sentence with the spectral modulation filtering at which comprehension became significantly worse (cutoff 4 cyc/kHz; <xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3G</xref>).</p>
<p>(0.09 MB MPG)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000302.s005" mimetype="video/mpeg" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.s005" xlink:type="simple"><label>Audio S5</label><caption>
<p>Low-pass modulation filtering at 12 Hz. Mp3 of example sentence with the temporal modulation filtering at which comprehension became significantly worse (cutoff 12 Hz; <xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3H</xref>).</p>
<p>(0.09 MB MPG)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000302.s006" mimetype="video/mpeg" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.s006" xlink:type="simple"><label>Audio S6</label><caption>
<p>Example sentence with core modulations. Mp3 of the example sentence containing only the core of essential modulations below 7.75 Hz and 3.75 cyc/kHz (<xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4G</xref>).</p>
<p>(0.09 MB MPG)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000302.s007" mimetype="video/mpeg" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.s007" xlink:type="simple"><label>Audio S7</label><caption>
<p>Spectral notch filter producing gender misidentification. Mp3 of the example sentence after spectral modulations between 3 and 7 cyc/kHz were filtered out (<xref ref-type="fig" rid="pcbi-1000302-g004">Figure 4F</xref>). Listeners misreported the gender of about half the female speakers.</p>
<p>(0.09 MB MPG)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000302.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000302.s008" xlink:type="simple"><label>Figure S1</label><caption>
<p>Modulation power spectrum and performance with linear time-frequency scale. (A) The top panels in the figure show the modulation power spectrum (MPS) of speech (American English) calculated from a time-frequency representation of the sound using a logarithmic frequency filter bank (log-f). The modulation spectrum is shown for male and female speakers. As was the case for the modulation spectrum estimate with a linear frequency filter bank (<xref ref-type="fig" rid="pcbi-1000302-g001">Figures 1</xref> and <xref ref-type="fig" rid="pcbi-1000302-g002">2</xref>), the log-f speech modulation spectrum shows a power law distribution of energy and some degree of non-separability between spectral and temporal modulations. However, in the linear modulation spectrum, the spectral modulations in cycles/Hz distribute into clearly separate regions corresponding to pitch and formant energy (<xref ref-type="fig" rid="pcbi-1000302-g002">Figure 2</xref>), whereas in the log-f modulation spectrum the corresponding modulations overlap in a single triangular region below 4 cycles/octave. In addition, in this speech corpus at this time-frequency scale, the harmonic structure of women's vocalic sounds creates a repeated pattern of spectral modulations. The log-f spectrogram was obtained with logarithmically-spaced Gaussian filters with a bandwidth of 0.0138 octaves. (B) The line graph replots on a linear spectral modulation axis the comprehension of sentences after log-f low-pass filtering. The resulting psychometric curve includes low-pass filter cutoffs from 1/4 cycles/octave to 256 cycles/octave, but these can be interpreted as low-pass spectral filtering on the left side of the peak and low-pass temporal filtering on the right side of the peak, as follows. The sound pressure re-synthesis of these sentences used the direct method, where the filtered amplitude was obtained by decomposing the sound into a set of narrowband signals with the frequency bandwidth given by the modulation frequency cutoff, and the filtered phase was obtained from Gaussian white-noise that is decomposed through the same filter bank <xref ref-type="bibr" rid="pcbi.1000302-Tyler1">[40]</xref>,<xref ref-type="bibr" rid="pcbi.1000302-Cohen2">[41]</xref>. For high modulation frequency cutoffs, because of the time-frequency tradeoff, this method effectively low-pass filters the amplitude envelope. In a log frequency representation, the temporal frequency cutoff depends on the center frequency. We show the corresponding temporal cutoff for the frequency band centered at 1 kHz in parentheses under the relevant x-axis labels. The left side of the figure can therefore be compared to the psychometric curve shown in <xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3C</xref>, and the right side to <xref ref-type="fig" rid="pcbi-1000302-g003">Figure 3D</xref>. The left side shows that speech comprehension remains very good with representations having filter bands as wide as 0.25 octaves (the sigma parameter corresponding to 2 cycles/octave cutoff <xref ref-type="bibr" rid="pcbi.1000302-Fu1">[14]</xref>) but that it degrades rapidly with wider frequency bands, particularly in noisy conditions. As in our interpretation of the linear frequency results, this steep decline occurs when spectral modulations that correspond to formants and formant transitions are filtered out. On the right side of the curve, the critical temporal modulation cutoffs are approximately twice as large in this plot as in the linear frequency plot, suggesting that humans cannot easily use the faster temporal information that is present in filters above 500 Hz to compensate for the loss of that information in the lower frequency bands.</p>
<p>(4.37 MB TIF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank Deep Ganguli and Bradley Voytek who assisted F. Theunissen in writing the modulation filtering routines in Matlab. Ervin Hafter, Psyche Loui, and Thomas Wickens provided or shared sound booth space for these experiments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1000302-Liberman1"><label>1</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Liberman</surname><given-names>AM</given-names></name>
</person-group>             <year>1996</year>             <source>Speech: A Special Code</source>             <publisher-loc>Cambridge, Massachusetts</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000302-Chi1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chi</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Gao</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Guyton</surname><given-names>MC</given-names></name>
<name name-style="western"><surname>Ru</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name>
</person-group>             <year>1999</year>             <article-title>Spectro-temporal modulation transfer functions and speech intelligibility.</article-title>             <source>J Acoust Soc Am</source>             <volume>106</volume>             <fpage>2719</fpage>             <lpage>2732</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Singh1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Singh</surname><given-names>NC</given-names></name>
<name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name>
</person-group>             <year>2003</year>             <article-title>Modulation spectra of natural sounds and ethological theories of auditory processing.</article-title>             <source>J Acoust Soc Am</source>             <volume>114</volume>             <fpage>3394</fpage>             <lpage>3411</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Silipo1"><label>4</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Silipo</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Greenberg</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Arai</surname><given-names>T</given-names></name>
</person-group>             <year>1999</year>             <article-title>Temporal constraints on speech intelligibility as deduced from exceedingly sparse spectral representations.</article-title>             <comment>Eurospeech-99, Budapest, Hungary</comment>          </element-citation></ref>
<ref id="pcbi.1000302-Dorman1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dorman</surname><given-names>MF</given-names></name>
<name name-style="western"><surname>Loizou</surname><given-names>PC</given-names></name>
<name name-style="western"><surname>Rainey</surname><given-names>D</given-names></name>
</person-group>             <year>1997</year>             <article-title>Speech intelligibility as a function of the number of channels of stimulation for signal processors using sine-wave and noise-band outputs.</article-title>             <source>J Acoust Soc Am</source>             <volume>102</volume>             <fpage>2403</fpage>             <lpage>2411</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Shannon1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Shannon</surname><given-names>RV</given-names></name>
<name name-style="western"><surname>Zeng</surname><given-names>FG</given-names></name>
<name name-style="western"><surname>Kamath</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Wygonski</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Ekelid</surname><given-names>M</given-names></name>
</person-group>             <year>1995</year>             <article-title>Speech recognition with primarily temporal cues.</article-title>             <source>Science</source>             <volume>270</volume>             <fpage>303</fpage>             <lpage>304</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Baer1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Baer</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Moore</surname><given-names>BCJ</given-names></name>
</person-group>             <year>1993</year>             <article-title>Effects of spectral smearing on the intelligibility of sentences in noise.</article-title>             <source>J Acoust Soc Am</source>             <volume>94</volume>             <fpage>1229</fpage>             <lpage>1241</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-terKeurs1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>ter Keurs</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Festen</surname><given-names>JM</given-names></name>
<name name-style="western"><surname>Plomp</surname><given-names>R</given-names></name>
</person-group>             <year>1992</year>             <article-title>Effect of spectral envelope smearing on speech reception.</article-title>             <source>I. J Acoust Soc Am</source>             <volume>91</volume>             <fpage>2872</fpage>             <lpage>2880</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Drullman1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Drullman</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Festen</surname><given-names>JM</given-names></name>
<name name-style="western"><surname>Plomp</surname><given-names>R</given-names></name>
</person-group>             <year>1994</year>             <article-title>Effect of temporal envelope smearing on speech reception.</article-title>             <source>J Acoust Soc Am</source>             <volume>95</volume>             <fpage>1053</fpage>             <lpage>1064</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Arai1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Arai</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Pavel</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Hermansky</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Avendano</surname><given-names>C</given-names></name>
</person-group>             <year>1999</year>             <article-title>Syllable intelligibility for temporally filtered LPC cepstral trajectories.</article-title>             <source>J Acoust Soc Am</source>             <volume>105</volume>             <fpage>2783</fpage>             <lpage>2791</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Arai2"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Arai</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Greenberg</surname><given-names>S</given-names></name>
</person-group>             <year>1998</year>             <article-title>Speech intelligibility in the presence of cross-channel spectral asynchrony.</article-title>             <source>Proc IEEE Int Conf Acoust Speech Signal Process ICASSP</source>             <volume>2</volume>             <fpage>933</fpage>             <lpage>936</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Shamma1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name>
</person-group>             <year>2001</year>             <article-title>On the role of space and time in auditory processing.</article-title>             <source>Trends Cogn Sci</source>             <volume>5</volume>             <fpage>340</fpage>             <lpage>348</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Eggermont1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Eggermont</surname><given-names>JJ</given-names></name>
</person-group>             <year>2001</year>             <article-title>Between sound and perception: reviewing the search for a neural code.</article-title>             <source>Hear Res</source>             <volume>157</volume>             <fpage>1</fpage>             <lpage>42</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Fu1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fu</surname><given-names>Q-J</given-names></name>
<name name-style="western"><surname>Chinchilla</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Nogaki</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Galvin</surname><given-names>JJ</given-names><suffix>III</suffix></name>
</person-group>             <year>2005</year>             <article-title>Voice gender identification by cochlear implant users: the role of spectral and temporal resolution.</article-title>             <source>J Acoust Soc Am</source>             <volume>118</volume>             <fpage>1711</fpage>             <lpage>1718</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Remez1"><label>15</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Remez</surname><given-names>RE</given-names></name>
</person-group>             <article-title>Spoken expression of individual identity and the listener.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Morsella</surname><given-names>E</given-names></name>
</person-group>             <source>Expressing Onself/Expressing One's Self: A Festschrift in Honor of Robert M Krauss</source>             <publisher-loc>London</publisher-loc>             <publisher-name>Taylor &amp; Francis</publisher-name>             <comment>In press</comment>          </element-citation></ref>
<ref id="pcbi.1000302-Cohen1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cohen</surname><given-names>YE</given-names></name>
<name name-style="western"><surname>Theunissen</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Russ</surname><given-names>BE</given-names></name>
<name name-style="western"><surname>Gill</surname><given-names>P</given-names></name>
</person-group>             <year>2007</year>             <article-title>Acoustic features of rhesus vocalizations and their representation in the ventrolateral prefrontal cortex.</article-title>             <source>J Neurophysiol</source>             <volume>97</volume>             <fpage>1470</fpage>             <lpage>1484</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Ewert1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ewert</surname><given-names>SD</given-names></name>
<name name-style="western"><surname>Dau</surname><given-names>T</given-names></name>
</person-group>             <year>2000</year>             <article-title>Characterizing frequency selectivity for envelope fluctuations.</article-title>             <source>J Acoust Soc Am</source>             <volume>108</volume>             <fpage>1181</fpage>             <lpage>1196</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Creelman1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Creelman</surname><given-names>CD</given-names></name>
</person-group>             <year>1962</year>             <article-title>Human discrimination of auditory duration.</article-title>             <source>J Acoust Soc Am</source>             <volume>34</volume>             <fpage>582</fpage>             <lpage>593</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Stevens1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Stevens</surname><given-names>SS</given-names></name>
</person-group>             <year>1957</year>             <article-title>On the psychophysical law.</article-title>             <source>Psychol Rev</source>             <volume>64</volume>             <fpage>153</fpage>             <lpage>181</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Zeng1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zeng</surname><given-names>FG</given-names></name>
<name name-style="western"><surname>Nie</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Stickney</surname><given-names>GS</given-names></name>
<name name-style="western"><surname>Kong</surname><given-names>YY</given-names></name>
<name name-style="western"><surname>Vongphoe</surname><given-names>M</given-names></name>
<etal/></person-group>             <year>2005</year>             <article-title>Speech recognition with amplitude and frequency modulations.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>102</volume>             <fpage>2293</fpage>             <lpage>2298</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Mesgarani1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mesgarani</surname><given-names>N</given-names></name>
<name name-style="western"><surname>David</surname><given-names>SV</given-names></name>
<name name-style="western"><surname>Fritz</surname><given-names>JB</given-names></name>
<name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name>
</person-group>             <year>2008</year>             <article-title>Phoneme representation and classification in primary auditory cortex.</article-title>             <source>J Acoust Soc Am</source>             <volume>123</volume>             <fpage>899</fpage>             <lpage>909</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Pickett1"><label>22</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pickett</surname><given-names>JM</given-names></name>
</person-group>             <year>1999</year>             <source>The Acoustics of Speech Communication: Fundamentals, Speech Perception Theory, and Technology</source>             <publisher-loc>Boston, Massachusetts</publisher-loc>             <publisher-name>Allyn &amp; Bacon</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000302-Gold1"><label>23</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gold</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Morgan</surname><given-names>N</given-names></name>
</person-group>             <year>2000</year>             <source>The cepstrum as a spectral analyzer. Speech and Audio Signal Processing</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>John Wiley</publisher-name>             <fpage>271</fpage>             <lpage>279</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Hillenbrand1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hillenbrand</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Getty</surname><given-names>LA</given-names></name>
<name name-style="western"><surname>Clark</surname><given-names>MJ</given-names></name>
<name name-style="western"><surname>Wheeler</surname><given-names>K</given-names></name>
</person-group>             <year>1995</year>             <article-title>Acoustic characteristics of American English vowels.</article-title>             <source>J Acoust Soc Am</source>             <volume>97</volume>             <fpage>3099</fpage>             <lpage>3111</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Friesen1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friesen</surname><given-names>LM</given-names></name>
<name name-style="western"><surname>Shannon</surname><given-names>RV</given-names></name>
<name name-style="western"><surname>Baskent</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Wang</surname><given-names>X</given-names></name>
</person-group>             <year>2001</year>             <article-title>Speech recognition in noise as a function of the number of spectral channels: comparison of acoustic hearing and cochlear implants.</article-title>             <source>J Acoust Soc Am</source>             <volume>110</volume>             <fpage>1150</fpage>             <lpage>1163</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Liberman2"><label>26</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Liberman</surname><given-names>AM</given-names></name>
</person-group>             <year>1970</year>             <article-title>Some characteristics of perception in the speech mode.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Hamburg</surname><given-names>DA</given-names></name>
<name name-style="western"><surname>Pribram</surname><given-names>KH</given-names></name>
<name name-style="western"><surname>Stunkard</surname><given-names>AJ</given-names></name>
</person-group>             <source>Perception and Its Disorders.</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Williams &amp; Wilkins Company</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000302-Liberman3"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Liberman</surname><given-names>AM</given-names></name>
<name name-style="western"><surname>Cooper</surname><given-names>FS</given-names></name>
<name name-style="western"><surname>Shankweiler</surname><given-names>DP</given-names></name>
<name name-style="western"><surname>Studdert-Kennedy</surname><given-names>M</given-names></name>
</person-group>             <year>1967</year>             <article-title>Perception of the speech code.</article-title>             <source>Psychol Rev</source>             <volume>74</volume>             <fpage>431</fpage>             <lpage>461</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Greenberg1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Greenberg</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Arai</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Kingsbury</surname><given-names>BE</given-names></name>
<name name-style="western"><surname>Morgan</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Shire</surname><given-names>M</given-names></name>
<etal/></person-group>             <year>1999</year>             <article-title>Syllable-based speech recognition using auditorylike features.</article-title>             <source>J Acoust Soc Am</source>             <volume>105</volume>             <fpage>1157</fpage>             <lpage>1158</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Greenberg2"><label>29</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Greenberg</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Arai</surname><given-names>T</given-names></name>
</person-group>             <year>2001</year>             <article-title>The relation between speech intelligibility and the complex modulation spectrum.</article-title>             <fpage>473</fpage>             <lpage>476</lpage>             <comment>Eurospeech-2001, Aalborg, Denmark</comment>          </element-citation></ref>
<ref id="pcbi.1000302-Kusumoto1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kusumoto</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Arai</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Kinoshita</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Hodoshima</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Vaughan</surname><given-names>N</given-names></name>
</person-group>             <year>2005</year>             <article-title>Modulation enhancement of speech by a preprocessing algorithm for improving intelligibility in reverberant environments.</article-title>             <source>Speech Commun</source>             <volume>45</volume>             <fpage>101</fpage>             <lpage>113</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Miller1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Miller</surname><given-names>LM</given-names></name>
<name name-style="western"><surname>Escabi</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Read</surname><given-names>HL</given-names></name>
<name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name>
</person-group>             <year>2002</year>             <article-title>Spectrotemporal receptive fields in the lemniscal auditory thalamus and cortex.</article-title>             <source>J Neurophysiol</source>             <volume>87</volume>             <fpage>516</fpage>             <lpage>527</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Woolley1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Woolley</surname><given-names>SM</given-names></name>
<name name-style="western"><surname>Fremouw</surname><given-names>TE</given-names></name>
<name name-style="western"><surname>Hsu</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name>
</person-group>             <year>2005</year>             <article-title>Tuning for spectro-temporal modulations as a mechanism for auditory discrimination of natural sounds.</article-title>             <source>Nat Neurosci</source>             <volume>8</volume>             <fpage>1371</fpage>             <lpage>1379</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Joris1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Joris</surname><given-names>PX</given-names></name>
<name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name>
<name name-style="western"><surname>Rees</surname><given-names>A</given-names></name>
</person-group>             <year>2004</year>             <article-title>Neural processing of amplitude-modulated sounds.</article-title>             <source>Physiol Rev</source>             <volume>84</volume>             <fpage>541</fpage>             <lpage>577</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Gill1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gill</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Woolley</surname><given-names>SMN</given-names></name>
<name name-style="western"><surname>Fremouw</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name>
</person-group>             <year>2008</year>             <article-title>What's that sound? Auditory area CLM encodes stimulus surprise, not intensity or intensity changes.</article-title>             <source>J Neurophysiol</source>             <volume>99</volume>             <fpage>2809</fpage>             <lpage>2820</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Peterson1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Peterson</surname><given-names>GE</given-names></name>
<name name-style="western"><surname>Barney</surname><given-names>HL</given-names></name>
</person-group>             <year>1952</year>             <article-title>Control methods used in a study of the vowels.</article-title>             <source>J Acoust Soc Am</source>             <volume>24</volume>             <fpage>175</fpage>             <lpage>184</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Childers1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Childers</surname><given-names>DG</given-names></name>
<name name-style="western"><surname>Wu</surname><given-names>K</given-names></name>
</person-group>             <year>1991</year>             <article-title>Gender recognition from speech. Part II: fine analysis.</article-title>             <source>J Acoust Soc Am</source>             <volume>90</volume>             <fpage>1841</fpage>             <lpage>1856</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Johnson1"><label>37</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Johnson</surname><given-names>KA</given-names></name>
</person-group>             <year>2005</year>             <article-title>Speaker normalization in speech perception.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Pisoni</surname><given-names>DB</given-names></name>
<name name-style="western"><surname>Remez</surname><given-names>RE</given-names></name>
</person-group>             <source>The Handbook of Speech Perception.</source>             <publisher-loc>Malden, Massachusetts</publisher-loc>             <publisher-name>Blackwell Publishing</publisher-name>             <fpage>363</fpage>             <lpage>389</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-McDermott1"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>McDermott</surname><given-names>HJ</given-names></name>
</person-group>             <year>2004</year>             <article-title>Music perception with cochlear implants: a review.</article-title>             <source>Trends Amplif</source>             <volume>8</volume>             <fpage>49</fpage>             <lpage>82</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Versnel1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Versnel</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name>
</person-group>             <year>1998</year>             <article-title>Spectral-ripple representation of steady-state vowels in primary auditory cortex.</article-title>             <source>J Acoust Soc Am</source>             <volume>103</volume>             <fpage>2502</fpage>             <lpage>2514</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Tyler1"><label>40</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tyler</surname><given-names>RS</given-names></name>
<name name-style="western"><surname>Preece</surname><given-names>JP</given-names></name>
<name name-style="western"><surname>Lowder</surname><given-names>MW</given-names></name>
</person-group>             <year>1987</year>             <source>The Iowa audiovisual speech perception laser videodisc. Laser Videodisc and Laboratory Report</source>             <publisher-loc>Iowa City, Iowa</publisher-loc>             <publisher-name>Department of Otolaryngology, University of Iowa</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000302-Cohen2"><label>41</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cohen</surname><given-names>L</given-names></name>
</person-group>             <year>1995</year>             <source>Time-Frequency Analysis</source>             <publisher-loc>Englewood Cliffs, New Jersey</publisher-loc>             <publisher-name>Prentice Hall</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000302-Gill2"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gill</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Zhang</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Woolley</surname><given-names>SM</given-names></name>
<name name-style="western"><surname>Fremouw</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name>
</person-group>             <year>2006</year>             <article-title>Sound representation methods for spectro-temporal receptive field estimation.</article-title>             <source>J Comput Neurosci</source>             <volume>22</volume>             <fpage>22</fpage>          </element-citation></ref>
<ref id="pcbi.1000302-Griffin1"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Griffin</surname><given-names>DW</given-names></name>
<name name-style="western"><surname>Lim</surname><given-names>JS</given-names></name>
</person-group>             <year>1984</year>             <article-title>Signal estimation from modified short-time Fourier transform.</article-title>             <source>IEEE Trans Acoust</source>             <volume>32</volume>             <fpage>236</fpage>             <lpage>243</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Slaney1"><label>44</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Slaney</surname><given-names>M</given-names></name>
</person-group>             <year>1994</year>             <article-title>An introduction to auditory model inversion.</article-title>             <comment>Interval Technical Report IRC1994</comment>          </element-citation></ref>
<ref id="pcbi.1000302-Smith1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Smith</surname><given-names>ZM</given-names></name>
<name name-style="western"><surname>Delgutte</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Oxenham</surname><given-names>AJ</given-names></name>
</person-group>             <year>2002</year>             <article-title>Chimaeric sounds reveal dichotomies in auditory perception.</article-title>             <source>Nature</source>             <volume>416</volume>             <fpage>87</fpage>             <lpage>90</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Theunissen1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name>
<name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name>
</person-group>             <year>1998</year>             <article-title>Temporal and spectral sensitivity of complex auditory neurons in the nucleus HVc of male zebra finches.</article-title>             <source>J Neurosci</source>             <volume>18</volume>             <fpage>3786</fpage>             <lpage>3802</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Plomp1"><label>47</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Plomp</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Mimpen</surname><given-names>AM</given-names></name>
</person-group>             <year>1979</year>             <article-title>Improving the reliability of testing the speech reception threshold for sentences.</article-title>             <source>Audiology</source>             <volume>18</volume>             <fpage>43</fpage>             <lpage>52</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Plomp2"><label>48</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Plomp</surname><given-names>R</given-names></name>
</person-group>             <year>1986</year>             <article-title>A signal-to-noise ratio model for the speech-reception threshold of the hearing impaired.</article-title>             <source>J Speech Hear Res</source>             <volume>29</volume>             <fpage>146</fpage>             <lpage>154</lpage>          </element-citation></ref>
<ref id="pcbi.1000302-Glasberg1"><label>49</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Glasberg</surname><given-names>BR</given-names></name>
<name name-style="western"><surname>Moore</surname><given-names>BC</given-names></name>
</person-group>             <year>1989</year>             <article-title>Psychoacoustic abilities of subjects with unilateral and bilateral cochlear hearing impairments and their relationship to the ability to understand speech.</article-title>             <source>Scand Audiol Suppl</source>             <volume>32</volume>             <fpage>1</fpage>             <lpage>25</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>