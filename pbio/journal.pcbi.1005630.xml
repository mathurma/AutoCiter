<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005630</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-00074</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Developmental neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Monte Carlo method</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Monte Carlo method</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Optimization</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Noise reduction</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Probability estimation</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Optimal structure of metaplasticity for adaptive learning</article-title>
<alt-title alt-title-type="running-head">Optimal structure of metaplasticity for adaptive learning</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2874-6536</contrib-id>
<name name-style="western">
<surname>Khorsand</surname>
<given-names>Peyman</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4386-8486</contrib-id>
<name name-style="western">
<surname>Soltani</surname>
<given-names>Alireza</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Department of Psychological and Brain Sciences, Dartmouth College, New Hampshire, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Maloney</surname>
<given-names>Laurence T.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>New York University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"><list-item>
<p><bold>Conceptualization:</bold> PK AS.</p></list-item> <list-item>
<p><bold>Data curation:</bold> PK AS.</p></list-item> <list-item>
<p><bold>Formal analysis:</bold> PK AS.</p></list-item> <list-item>
<p><bold>Investigation:</bold> PK AS.</p></list-item> <list-item>
<p><bold>Validation:</bold> PK AS.</p></list-item> <list-item>
<p><bold>Visualization:</bold> PK AS.</p></list-item> <list-item>
<p><bold>Writing – original draft:</bold> PK AS.</p></list-item> <list-item>
<p><bold>Writing – review &amp; editing:</bold> PK AS.</p></list-item></list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">peyman@dartmouth.edu</email> (PK); <email xlink:type="simple">soltani@dartmouth.edu</email> (AS)</corresp>
</author-notes>
<pub-date pub-type="epub">
<day>28</day>
<month>6</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<month>6</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>6</issue>
<elocation-id>e1005630</elocation-id>
<history>
<date date-type="received">
<day>16</day>
<month>1</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>9</day>
<month>6</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Khorsand, Soltani</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005630"/>
<abstract>
<p>Learning from reward feedback in a changing environment requires a high degree of adaptability, yet the precise estimation of reward information demands slow updates. In the framework of estimating reward probability, here we investigated how this tradeoff between adaptability and precision can be mitigated via metaplasticity, i.e. synaptic changes that do not always alter synaptic efficacy. Using the mean-field and Monte Carlo simulations we identified ‘superior’ metaplastic models that can substantially overcome the adaptability-precision tradeoff. These models can achieve both adaptability and precision by forming two separate sets of meta-states: reservoirs and buffers. Synapses in reservoir meta-states do not change their efficacy upon reward feedback, whereas those in buffer meta-states can change their efficacy. Rapid changes in efficacy are limited to synapses occupying buffers, creating a bottleneck that reduces noise without significantly decreasing adaptability. In contrast, more-populated reservoirs can generate a strong signal without manifesting any observable plasticity. By comparing the behavior of our model and a few competing models during a dynamic probability estimation task, we found that superior metaplastic models perform close to optimally for a wider range of model parameters. Finally, we found that metaplastic models are robust to changes in model parameters and that metaplastic transitions are crucial for adaptive learning since replacing them with graded plastic transitions (transitions that change synaptic efficacy) reduces the ability to overcome the adaptability-precision tradeoff. Overall, our results suggest that ubiquitous unreliability of synaptic changes evinces metaplasticity that can provide a robust mechanism for mitigating the tradeoff between adaptability and precision and thus adaptive learning.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Successful learning from our experience and feedback from the environment requires that the reward value assigned to a given option or action to be updated by a precise amount after each feedback. In the standard model for reward-based learning known as reinforcement learning, the learning rates determine the strength of such update. A large learning rate allows fast update of values (large adaptability) but introduces noise (small precision), whereas a small learning rate does the opposite. Thus, learning seems to be bounded by a tradeoff between adaptability and precision. Here, we asked whether there are synaptic mechanisms that are capable of adjusting the brain’s level of plasticity according to reward statistics, and, therefore, allow the learning process to be adaptive. We showed that metaplasticity, changes in the synaptic state that shape future synaptic modifications without any observable changes in the strength of synapses, could provide such a mechanism and furthermore, identified the optimal structure of such metaplasticity. We propose that metaplasticity, which sometimes causes no observable changes in behavior and thus could be perceived as a lack of learning, can provide a robust mechanism for adaptive learning.</p>
</abstract>
<funding-group>
<funding-statement>The authors received no specific funding for this work.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="0"/>
<page-count count="22"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2017-07-13</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>To successfully learn from reward feedback, the brain must adjust how it responds to and integrates reward outcomes, since reward contingencies can unpredictably change over time [<xref ref-type="bibr" rid="pcbi.1005630.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005630.ref002">2</xref>]. At the heart of this learning problem is a tradeoff between adaptability and precision. On the one hand, the brain must rapidly update reward values in response to changes in the environment; on the other hand, in the absence of any such changes, it must obtain accurate estimates of those values. This tradeoff, which we refer to as the adaptability—precision tradeoff [<xref ref-type="bibr" rid="pcbi.1005630.ref003">3</xref>,<xref ref-type="bibr" rid="pcbi.1005630.ref004">4</xref>], can be easily demonstrated in the framework of reinforcement learning [<xref ref-type="bibr" rid="pcbi.1005630.ref005">5</xref>]. According to this framework, larger learning rates result in higher adaptability but lower precision, and smaller learning rates give rise to lower adaptability but higher precision. In recent years, the failure of conventional reinforcement learning (RL) models to capture the level of adaptability and precision demonstrated by humans and animals has led to alternative explanations for how we deal with uncertainty and volatility in the environment [<xref ref-type="bibr" rid="pcbi.1005630.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005630.ref006">6</xref>,<xref ref-type="bibr" rid="pcbi.1005630.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1005630.ref008">8</xref>]. However, most of these solutions for adjusting learning require complicated calculations, and their underlying neural substrates are unknown.</p>
<p>Given the central role of synapses in learning, we asked whether there are local synaptic mechanisms that can adjust the level of plasticity according to reward statistics and, therefore, allow the learning process to be adaptable. A candidate mechanism for such adjustment is metaplasticity, defined as changes in the synaptic state that shape the direction, magnitude, and duration of future synaptic changes without any observable change in the efficacy of synaptic transmission [<xref ref-type="bibr" rid="pcbi.1005630.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1005630.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1005630.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1005630.ref012">12</xref>]. Extending our recent heuristic model of reward-dependent metaplasticity, which enables adjustment of learning to reward uncertainty [<xref ref-type="bibr" rid="pcbi.1005630.ref003">3</xref>], we examined a general class of metaplastic models to identify features that are beneficial for mitigating the adaptability-precision tradeoff (APT) during the estimation of the probability of binary reward.</p>
<p>Using the mean-field and Monte Carlo simulations, we identified optimal metaplastic models that can substantially overcome the APT. These models, which we refer to as ‘superior’ models, achieve both adaptability and precision by forming two separate sets of meta-states: reservoirs and buffers. Synapses in reservoir meta-states do not change their efficacy upon reward feedback, whereas those in buffer meta-states can change their efficacy. In superior models, rapid changes in efficacy are limited to synapses occupying buffers, creating a bottleneck that reduces noise without significantly decreasing adaptability. In contrast, more-populated reservoirs can generate a strong signal without manifesting any observable plasticity. Comparison of the behavior of our model and a few competing models during a dynamic probability estimation task revealed that superior metaplastic models perform close to optimally for a wider range of model parameters. However, superior models were suboptimal when precision was defined in the absolute term (absolute value of the difference between estimated and actual probabilities) rather than relative (the ability to distinguish neighboring values of probability), indicating that the brain could use different objectives to deal with reward uncertainty. Finally, we showed that metaplasticity provides a robust mechanism for mitigating the APT, and that metaplastic transitions are crucial, since replacing these transitions with plastic ones reduces the ability of the model to mitigate the APT. Altogether, our results illustrate how metaplasticity can mitigate one of the most fundamental tradeoffs in learning and, moreover, reveal the critical features of metaplasticity that contribute to adaptive learning.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>The adaptability-precision tradeoff</title>
<p>To study the relationship between adaptability and precision, we considered a general problem of estimating reward probability from a stream of binary outcomes (reward, no reward). We defined adaptability and precision in the context of this estimation task in order to quantify the adaptability-precision tradeoff (APT). We assumed that the estimation of reward probability is performed by a set of synapses and, thus, reward probability is stored in the strength of these synapses. As a result, adaptability in estimation of reward probability requires these synapses to change their strengths quickly whereas precision requires that the strength of synapses can discriminate between neighboring values of reward probability (see below).</p>
<p>The estimation of reward probability can be preformed with a model consisting of synapses that follow a stochastic reward-dependent plasticity rule [<xref ref-type="bibr" rid="pcbi.1005630.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1005630.ref015">15</xref>] (<xref ref-type="fig" rid="pcbi.1005630.g001">Fig 1a</xref>). In this model, which we refer to as the ‘plastic’ model, synapses are binary so they can be in the weak or strong state [<xref ref-type="bibr" rid="pcbi.1005630.ref016">16</xref>,<xref ref-type="bibr" rid="pcbi.1005630.ref017">17</xref>]. Weak synapses can be potentiated on rewarded trials with a probability <italic>t</italic><sup>+</sup> (potentiation rate), whereas strong synapses can be depressed on unrewarded trials with a probability <italic>t</italic><sup>−</sup> (depression rate). The difference between the fractions of synapses that are in the strong and weak states determines the signal stored in these synapses, reflecting the model’s estimate of reward probability. This model is equivalent to a simple RL model based on reward prediction error and can provide an unbiased estimate of reward probability when potentiation and depression rates are equal (<xref ref-type="supplementary-material" rid="pcbi.1005630.s001">S1 Text</xref>).</p>
<fig id="pcbi.1005630.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005630.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The adaptability-precision tradeoff in the plastic and metaplastic models.</title>
<p>(<bold>a</bold>) The APT in estimating reward probability for the binary plastic model. Plotted is the signal in the binary plastic model in response to a sudden change in reward probability from 0.3 to 0.8 on trial 20. The blue curve shows an example instance of estimation in response to a reward sequence (tick marks on top and bottom indicate reward and no reward, respectively). The black curve shows the average over many instances and the green shade indicates the standard deviation over those instances. Decreasing the transition rates (from <italic>t</italic><sup>+</sup> = <italic>t</italic><sup>−</sup> = 0.07 in the left panel to <italic>t</italic><sup>+</sup> = <italic>t</italic><sup>−</sup> = 0.03 in the right) resulted in noise reduction in the asymptotic value of the signal, but at the expense of slower convergence to this asymptotic value. The inset shows the model with binary plastic synapses (W: weak; S: strong). (<bold>b</bold>) The APT manifests itself for different learning rates for different reward probabilities. Plotted is the adaptability, as a function of the precision for different values of <italic>p</italic><sub><italic>r</italic></sub>. Each dot corresponds to a specific set of parameter values. The APT is stronger as <italic>p</italic><sub><italic>r</italic></sub> becomes closer to 0.5 (<bold>c</bold>) A schematic of a general model of metaplasticity with four ordered meta-states (<italic>N</italic> = 4). Synapses can transition between different meta-states with either weak or strong synaptic efficacies. (<bold>d</bold>) The APT in the binary plastic model (<italic>N</italic> = 2), and superior metaplastic models with different numbers of meta-states. Plotted is the average adaptability (over different values of <italic>p</italic><sub><italic>r</italic></sub>) as a function of the average precision in different models. For the binary plastic model, each dot corresponds to a specific set of parameter values. For metaplastic models (<italic>N</italic> = 4,6,8), each outline connects models with optimized adaptability for a given value of average precision.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.g001" xlink:type="simple"/>
</fig>
<p>For a given value of reward probability, <italic>p</italic><sub><italic>r</italic></sub>, the steady state of the model can be used to calculate the signal, and the weighted average change in signal due to single potentiation and depression events (‘one-step’ noise) provides a good proxy for noise (see <xref ref-type="sec" rid="sec010">Methods</xref>). We measured precision with the ability of the model to differentiate between adjacent reward probabilities instead of a more conventional definition based on the difference between the estimated and actual probabilities. We adopted the former definition because encoding and representation of reward information are inherently relative in the brain, since the probability estimated by a set of synapses can be easily scaled and biased by changes in the input neural firing to these synapses. Therefore, we defined the ‘precision’ (<inline-formula id="pcbi.1005630.e001"><alternatives><graphic id="pcbi.1005630.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mi>ℙ</mml:mi></mml:math></alternatives></inline-formula>) as equal to the sensitivity of the model’s signal to changes in reward probabilities (‘sensitivity’), divided by noise in the signal. The ‘adaptability’ (<inline-formula id="pcbi.1005630.e002"><alternatives><graphic id="pcbi.1005630.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mi mathvariant="double-struck">A</mml:mi></mml:math></alternatives></inline-formula>) of a model in estimating reward probability was defined as the rate at which the fractions of meta-states approach their final values (see <xref ref-type="sec" rid="sec010">Methods</xref>).</p>
<p>Because of the simplicity of the plastic and RL models, adaptability and precision can be analytically computed for these models (<xref ref-type="supplementary-material" rid="pcbi.1005630.s002">S2 Text</xref>). Both these models show a strict APT, since the product of adaptability and precision is independent of model parameters and only depends on reward probability (<xref ref-type="fig" rid="pcbi.1005630.g001">Fig 1a and 1b</xref>). Importantly, adopting different values for the potentiation and depression rates (or equivalently the learning rates in RL) cannot improve the APT. Rather, adoption of different values slightly alters the average values of adaptability and precision over a set of reward probabilities (<xref ref-type="fig" rid="pcbi.1005630.g001">Fig 1d</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005630.s003">S1 Fig</xref>).</p>
</sec>
<sec id="sec004">
<title>Metaplasticity can mitigate the adaptability-precision tradeoff</title>
<p>Here, we considered a general model of metaplasticity and used optimization to identify the superior metaplastic models for mitigating the APT. Our general model of metaplasticity consisted of multiple meta-states associated with one of the two values of synaptic efficacy (weak and strong), and all possible transitions between these meta-states (<xref ref-type="fig" rid="pcbi.1005630.g001">Fig 1c</xref>; see <xref ref-type="sec" rid="sec010">Methods</xref>). In this model, the difference between the fractions of synapses that are in the strong and weak meta-states determines the signal (<italic>S</italic>) stored in these synapses, reflecting the model’s estimate of reward probability. Importantly, we assumed that metaplastic transitions have a consistent order, and thus, within the set of weak and strong meta-states, there are multiple meta-states with different levels of depth (<xref ref-type="fig" rid="pcbi.1005630.g001">Fig 1c</xref>). Since we were interested in conditions under which metaplasticity can improve the APT, we examined ‘superior’ metaplastic models (i.e. those which optimized <inline-formula id="pcbi.1005630.e003"><alternatives><graphic id="pcbi.1005630.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mtext> </mml:mtext><mml:mi>ℙ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> for a given value of <inline-formula id="pcbi.1005630.e004"><alternatives><graphic id="pcbi.1005630.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mi>ℙ</mml:mi></mml:math></alternatives></inline-formula>).</p>
<p>We found that for many model parameters, the APT can be mitigated by superior metaplastic models that consist of as few as four meta-states (<xref ref-type="fig" rid="pcbi.1005630.g001">Fig 1d</xref>; <xref ref-type="supplementary-material" rid="pcbi.1005630.s004">S2 Fig</xref>). These superior models overcame the APT by exhibiting three important characteristics: differential adjustments of learning based on reward probability; matching of sensitivity to noise; and optimal adaptability. Firstly, the learning on rewarded and unrewarded trials was differentially adjusted according to reward probability. Secondly, the sensitivity of the signal to reward probability matched the level of noise (sensitivity-to-noise matching), and this matching was improved with larger numbers of meta-states. Thirdly, the adaptability of the models was optimized for a given level of noise (see below).</p>
<p>The first characteristic of superior models is that learning was naturally adjusted according to reward probability without any changes in the model’s parameters. To show this adjustment, we computed the ‘effective’ learning rates for potentiation and depression events for a given value of <italic>p</italic><sub><italic>r</italic></sub> (see <xref ref-type="sec" rid="sec010">Methods</xref>). The effective learning rate assigned a single rate to transitions between the weak and strong meta-states or vice versa (plastic transitions, <xref ref-type="fig" rid="pcbi.1005630.g001">Fig 1c</xref>), which are the only transitions that can change synaptic efficacy and thus the signal. We found that the effective learning rate on rewarded trials (<inline-formula id="pcbi.1005630.e005"><alternatives><graphic id="pcbi.1005630.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>) was close to zero for small values of <italic>p</italic><sub><italic>r</italic></sub> but monotonically increased as <italic>p</italic><sub><italic>r</italic></sub> increased (<xref ref-type="fig" rid="pcbi.1005630.g002">Fig 2a</xref>). At the same time, the effective learning rate on unrewarded trials (<inline-formula id="pcbi.1005630.e006"><alternatives><graphic id="pcbi.1005630.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>) was large when <italic>p</italic><sub><italic>r</italic></sub> was close to zero and decreased as <italic>p</italic><sub><italic>r</italic></sub> increased. The effective learning rates on rewarded and unrewarded trials crossed over at 0.5 due to the symmetry in models parameters with respect to reward and no reward.</p>
<fig id="pcbi.1005630.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005630.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Adjustment of learning to reward probability in the superior metaplastic models.</title>
<p>(<bold>a</bold>) Plotted are the effective learning rates for potentiation (<inline-formula id="pcbi.1005630.e007"><alternatives><graphic id="pcbi.1005630.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>) and depression (<inline-formula id="pcbi.1005630.e008"><alternatives><graphic id="pcbi.1005630.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>) events as a function of <italic>p</italic><sub><italic>r</italic></sub>. The effective learning rate on potentiation (depression) events increases (decreases) as reward probability increases, with a crossover at <italic>p</italic><sub><italic>r</italic></sub> = 0.5. (<bold>b</bold>) Signal in superior metaplastic models. (<bold>c</bold>) Matching of the sensitivity to noise in the metaplastic models. Plotted are the normalized sensitivity (d<italic>S</italic>/d<italic>p</italic><sub><italic>r</italic></sub>, denoted as (Δ<italic>S</italic>)<sub><italic>norm</italic></sub>) and one-step noise (<italic>η</italic>) as a function of <italic>p</italic><sub><italic>r</italic></sub> for three examples of superior metaplastic models with different numbers of meta-states. The sensitivity profile better matches the noise profile as <italic>N</italic> increases.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.g002" xlink:type="simple"/>
</fig>
<p>To understand why these adjustments are beneficial for mitigating the APT, one should note that in the RL model, the convergence to the final estimate of reward probability (when <italic>p</italic><sub><italic>r</italic></sub> is small) slows down as more negative outcomes (no reward) are observed, since reward prediction error becomes smaller for unrewarded trials. This property limits adaptability. At the same time, the response to a positive outcome (reward) increases since reward prediction error increases on rewarded trials, and this property increases noise. In contrast, metaplastic models increase <inline-formula id="pcbi.1005630.e009"><alternatives><graphic id="pcbi.1005630.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> as the models receive more negative outcomes allowing them to slow their convergence to the final value of probability estimate to a lesser degree. On the other hand, decreasing <inline-formula id="pcbi.1005630.e010"><alternatives><graphic id="pcbi.1005630.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> makes the estimate more robust against sporadic positive outcomes (noise). The opposite happens when <italic>p</italic><sub><italic>r</italic></sub> becomes closer to one.</p>
<p>These complementary adjustments in learning resulted in a sigmoid-shape signal for superior metaplastic models (<xref ref-type="fig" rid="pcbi.1005630.g002">Fig 2b</xref>), which in turn, gives rise to the second characteristic of superior models, the match between the sensitivity and the noise level (<xref ref-type="fig" rid="pcbi.1005630.g002">Fig 2c</xref>). More specifically, the maximum sensitivity (d<italic>S</italic>/d<italic>p</italic><sub><italic>r</italic></sub>) for superior models occurred at <italic>p</italic><sub><italic>r</italic></sub> = 0.5, such that the steepest part of the signal matched the maximum level of noise (<xref ref-type="fig" rid="pcbi.1005630.g002">Fig 2c</xref>). Additionally, for a given level of precision, the signal became a steeper function of reward probability and the maximum sensitivity increased as the number of meta-states increased (<xref ref-type="fig" rid="pcbi.1005630.g002">Fig 2b</xref>). Importantly, the slope of the signal (i.e. sensitivity) at <italic>p</italic><sub><italic>r</italic></sub> = 0.5 was linearly proportional to the ratio of effective learning rates around <italic>p</italic><sub><italic>r</italic></sub> = 0.5, indicative of a direct relationship between the sensitivity-to-noise matching and adjustment of learning to reward probability. The adjustments occur in metaplastic models without any changes in parameters; as reward probability deviates from 0.5 (say when <italic>p</italic><sub><italic>r</italic></sub> &gt; 0.5), more synapses move to shallower weak meta-states, increasing the effective potentiation rate above the effective depression rate (<xref ref-type="fig" rid="pcbi.1005630.g002">Fig 2a</xref>). As the ratio of effective potentiation to depression rates increases, however, the fraction of synapses in weak meta-states decreases. Consequently, sensitivity to reward probability decreases as <italic>p</italic><sub><italic>r</italic></sub> becomes larger or smaller than 0.5.</p>
<p>As noted above, in addition to the sensitivity-to-noise matching, adaptability of the superior models was optimized for a given level of noise. This optimization occurred because metaplasticity enabled superior models to form two separate sets of meta-states: reservoirs and buffers. Reservoirs, which are unique to metaplastic models, are the deepest sets of meta-states that cannot change their efficacy upon potentiation or depression events; they can only undergo metaplastic transitions (<xref ref-type="fig" rid="pcbi.1005630.g003">Fig 3a</xref>). Buffers, on the other hand, are the shallowest meta-states, and are able to undergo plastic transitions that change their synaptic efficacy. We refer to the remainder of the meta-states as ‘transient’. Because the superior models had reservoirs and buffers, they were able to keep a large proportion of their synapses in the weak or strong reservoirs (<xref ref-type="fig" rid="pcbi.1005630.g003">Fig 3b</xref>). Synapses within reservoirs were protected against changes in efficacy upon potentiation or depression events, and as a result, the signal could increase without increasing the level of noise.</p>
<fig id="pcbi.1005630.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005630.g003</object-id>
<label>Fig 3</label>
<caption>
<title/>
<p>(<bold>a</bold>) Schematic of the reservoirs, buffers, and transient meta-states, and how synapses occupy these meta-states according to reward probability. (<bold>b</bold>) Plotted are the fractions of synapses in the weak reservoir (dashed lines) and buffer (solid lines) as a function of reward probability. The inset shows the fraction of transient meta-states for <italic>N</italic> = 6 and <italic>N</italic> = 8 models. Fractions in the strong reservoir, buffer, and transient meta-states are the mirror image (along <italic>p</italic><sub><italic>r</italic></sub> = 0.5) of their weak counterparts. As reward probability approaches 0.5, more synapses occupy transient and buffer meta-states, making the model more adaptable. As <italic>p</italic><sub><italic>r</italic></sub> deviates from 0.5, more synapses transition to reservoirs, enabling the model to protect the signal.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.g003" xlink:type="simple"/>
</fig>
<p>The adaptability in the model depends on the rates of transitions between all subsets of meta-states, whereas noise (in reward estimation) depends on the flow across the plastic boundary (i.e. transitions between weak and strong meta-states and vice versa). Therefore, to understand how the model’s adaptability is optimized for a given level of noise, we computed the ‘effective transition rate’ for all subsets of meta-states. The effective transition rate was defined as the outward flow of synapses out of that subset divided by the fraction of synapses in that subset. This quantity, which is closely related to the concept of conductance in Markov chains [<xref ref-type="bibr" rid="pcbi.1005630.ref018">18</xref>], measures how easily synapses could leave a subset of meta-states (<xref ref-type="fig" rid="pcbi.1005630.g004">Fig 4a</xref>; see <xref ref-type="sec" rid="sec010">Methods</xref>). Importantly, the model’s adaptability is constrained by its slowest effective transition rate.</p>
<fig id="pcbi.1005630.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005630.g004</object-id>
<label>Fig 4</label>
<caption>
<title/>
<p>(<bold>a</bold>) Schematic of possible subsets of meta-states for <italic>N</italic> = 4 model. Overall, there are (2<sup><italic>N</italic></sup> − 2) non-trivial subsets of meta-states for a given metaplastic model. (<bold>b-d</bold>) Plotted is a subset of meta-states with the fastest and slowest effective transition rates for different values of reward probability for the metaplastic models with <italic>N</italic> = 4,6,8. For superior models, the bottleneck (slowest) subset is always across the plastic boundary to minimize noise for a given level of adaptability, whereas the rapidly mixing (fastest) subsets consist of only transient meta-states in order to a build a quick connection between reservoir to buffer meta-states.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.g004" xlink:type="simple"/>
</fig>
<p>In superior models, to reduce noise with a minimum cost to the adaptability, the slowest transition rates should be at the plastic boundary. We found that this was the case for all superior models (<xref ref-type="fig" rid="pcbi.1005630.g004">Fig 4</xref>). Interestingly, having the minimum effective transition rates at plastic transitions created a ‘bottleneck’ for the flow between weak and strong meta-states. This bottleneck helped reduce noise without significantly reducing the adaptability. The superior models with <italic>N</italic> &gt; 4 also contained transient meta-states, with the fastest effective transition rates between buffers and reservoirs, resulting in improved adaptability (<xref ref-type="fig" rid="pcbi.1005630.g004">Fig 4c and 4d</xref>).</p>
<p>This specific arrangement of meta-states and transitions between them, as well as the adjustment of the metaplastic model to reward probability, enabled metaplastic models to be more adaptable than corresponding binary plastic models. To demonstrate this superior adaptability, we used the effective learning rates for a given value of <italic>p</italic><sub><italic>r</italic></sub> to define an equivalent binary plastic model (<italic>N</italic> = 2 model) for any metaplastic model. We found that metaplastic models showed larger sensitivity to reward probability than equivalent plastic models (<xref ref-type="fig" rid="pcbi.1005630.g005">Fig 5</xref>). Moreover, metaplastic models were more adaptable and more precise than their equivalent plastic models. These results demonstrate that the dynamic adjustment of learning in metaplastic models is crucial for improving the APT, and that this adjustment cannot be achieved by simply replacing the learning rates in corresponding plastic models with the effective learning rates based on the superior metaplastic models.</p>
<fig id="pcbi.1005630.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005630.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Comparisons between the behavior of the metaplastic models and equivalent binary plastic models with the effective learning rates for a given value of <italic>p</italic><sub><italic>r</italic></sub>.</title>
<p>(<bold>a</bold>) Plotted is sensitivity in three superior metaplastic models (solid curves) and their equivalent binary plastic models (dashed curves) as a function of <italic>p</italic><sub><italic>r</italic></sub>. The equivalent plastic models are constructed using the effective learning rates for a given value of <italic>p</italic><sub><italic>r</italic></sub> and a metaplastic model. (<bold>b-c</bold>) Plotted is the adaptability and precision as a function of <italic>p</italic><sub><italic>r</italic></sub> for the same models presented in (a). The metaplastic models outperform equivalent binary plastic models in terms of sensitivity, precision, and adaptability for all values of reward probability.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Superior family of metaplastic models for mitigating the APT</title>
<p>To further study the characteristics of superior metaplastic models, we next examined the transition probabilities in these models. We found that most transition probabilities were very close to zero, allowing for the creation of reservoirs and buffers, while non-zero transition probabilities varied proportionally to create models with different levels of adaptability and precision (<xref ref-type="fig" rid="pcbi.1005630.g006">Fig 6a–6c</xref>). For example, in metaplastic models with four meta-states (<xref ref-type="fig" rid="pcbi.1005630.g006">Fig 6a</xref>), three of six transition probabilities for potentiation were zero, two others were equal, and the last one was very close to those two other non-zero probabilities.</p>
<fig id="pcbi.1005630.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005630.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Transition probabilities in the superior metaplastic models and a superior family of metaplastic models with only one parameter.</title>
<p>(<bold>a-c</bold>) Plotted are the transition probabilities for superior models with different numbers of meta-states (<italic>N</italic> = 4, 6, 8) for a given value of average precision. Only a few transition probabilities are non-zero, and the rest vary together, revealing the specific structure of metaplasticity that is useful for overcoming the APT. Finding optimal transition probabilities is more difficult for models with larger numbers of meta-states because the performance of those models are more robust against fluctuations in the model parameters. (<bold>d</bold>) The structure of the special family of metaplastic models with only one parameter (for <italic>N</italic> = 4,6,8 meta-states). (<bold>e</bold>) Adaptability as a function of the precision for the simple metaplastic models using the mean-field approach. (<bold>f</bold>) <inline-formula id="pcbi.1005630.e011"><alternatives><graphic id="pcbi.1005630.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi><mml:mo> </mml:mo><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mtext> </mml:mtext><mml:mi>ℙ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> as a function of the precision using the mean-field approach.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.g006" xlink:type="simple"/>
</fig>
<p>Based on these observations, we constructed a superior family of metaplastic models using a single parameter (transition probability). This was done to test whether such metaplastic models with only a single transition probability can significantly mitigate the APT. We found that even such simple metaplastic models can overcome the APT, and this ability was improved with additional meta-states (<xref ref-type="fig" rid="pcbi.1005630.g006">Fig 6d–6f</xref>). Overall, these results show that metaplastic models outperform plastic models, not because they have more parameters, but because they have a structure that allows for strong adjustment of learning.</p>
<p>These results illustrate that having more meta-states can improve the ability of metaplasticity to overcome the APT even for superior one-parameter models. The basic mechanism for this improvement is the existence of reservoirs, buffers, and a bottleneck for changing synaptic efficacy. Additional meta-states provide intermediate transitions between reservoirs and buffers that could increase signal and reduce noise without significantly decreasing the adaptability (<xref ref-type="fig" rid="pcbi.1005630.g006">Fig 6d–6f</xref>). As a result, models with larger numbers of intermediate meta-states show better matching of sensitivity to noise as well as more optimized adaptability for a given level of noise. Essentially, the specific structure for changing synaptic efficacy allows the models with a large number of meta-states to collect evidence (by transitioning synapses to shallower meta-states) before making a change.</p>
</sec>
<sec id="sec006">
<title>Validity of the mean-field approach</title>
<p>The results above were obtained using the mean-field (MF) approach. Although the MF approach could accurately estimate the signal, there are two components of the MF approach that could yield different results from the Monte Carlo (MC) simulations: adaptability and noise. As we show below, only the estimation of noise based on the MF approach is significantly different than noise based on the MC simulations. Nevertheless, our main findings based on MF also hold using MC simulations.</p>
<p>In the MF approach, adaptability is measured by the eigenvalue of the slowest decay mode of the transition matrix. However, what influences the estimation of reward probability is the synaptic strength (signal), since the synaptic efficacies of all weak meta-states or all strong meta-states are the same. That is, the asymptomatic rate of convergence to the new equilibrium or steady state of the synaptic strength could be different. Reaching steady state based on meta-states provides a lower bound for adaptability, since such equilibrium guarantees reaching steady state based on the synaptic strength but not vice versa. Comparing adaptability computed by the two methods, however, revealed only a small difference due to finite-size effects in the MC simulations (<xref ref-type="fig" rid="pcbi.1005630.g007">Fig 7a</xref>).</p>
<fig id="pcbi.1005630.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005630.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Comparison of results based on the mean-field approach and the Monte Carlo simulations.</title>
<p>(<bold>a</bold>) Match between the adaptability calculated using the MF approach and the MC simulations for superior metaplastic models with four meta-states (<italic>N</italic> = 4). Adaptability in the MC simulations corresponds to the asymptomatic rate of convergence to the new equilibrium synaptic strength (i.e. signal). The convergence rate is computed over a jump in probability from 0.3 to 0.8 for different values of the transition probability in the model (from 0.05 to 0.25). The difference between adaptability computed with the two methods is very small. (<bold>b</bold>) Plotted is the noise computed using the MC simulations as a function of one-step noise for the binary plastic model and superior metaplastic models with different numbers of meta-states. One-step noise sets a lower bound for simulation noise. The MF approximation for noise becomes more accurate for higher adaptability. (<bold>c</bold>) Comparison of sensitivity-to-noise matching based on one-step noise and the MC simulations. Conventions are the same as in <xref ref-type="fig" rid="pcbi.1005630.g002">Fig 2c</xref>. (<bold>d</bold>) The APT in the binary plastic model and superior metaplastic models with different numbers of meta-states using the MC simulations. The inset shows the results for 1-parameter family using the MC simulations.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.g007" xlink:type="simple"/>
</fig>
<p>The only difference between the MF approach and MC simulations was the estimation of noise. Using the MF approach, the estimated noise was set to one-step noise, which is equal to the weighted average of changes in the steady state of synaptic strength due to a potentiation and depression event. The one-step noise converges to the actual noise if the adaptability is equal to 1. When the adaptability is different from 1, one-step noise underestimates the actual level of noise measured by real simulations (<xref ref-type="fig" rid="pcbi.1005630.g007">Fig 7b</xref>). Intuitively, this underestimation occurs because of the extra noise in the MC simulations due to fluctuation of the fractions of synapses in different meta-states around their steady-state values. While underestimation of noise in the MF approach increases with the number of meta-states, this effect is not strong enough to change the sensitivity-to-noise matching (<xref ref-type="fig" rid="pcbi.1005630.g007">Fig 7c</xref>). Moreover, the MC simulations showed the same order of models in their ability to overcome the APT (compare Figs <xref ref-type="fig" rid="pcbi.1005630.g007">7d</xref>, <xref ref-type="fig" rid="pcbi.1005630.g001">1d</xref> and <xref ref-type="fig" rid="pcbi.1005630.g006">6d</xref>).</p>
</sec>
<sec id="sec007">
<title>Comparison with other models and metrics</title>
<p>In order to obtain the optimal structure of metaplasticity based on a general model, independently of a given task or set of task parameters, we measured adaptability as the rate at which the signal in the model approaches its steady state. Moreover, we measured precision as the ability of the model to differentiate between adjacent values of probability while considering noise. This “relative” definition of precision was adopted because any information stored at the synaptic level can be amplified (and thus be biased) by a change in the input firing rate.</p>
<p>Although superior metaplastic models are optimal in mitigating the APT based on the adopted definitions, a certain task or set of task parameters could favor certain models or certain model parameters. Alternatively, one could measure precision in an absolute fashion, for example as the difference between the estimated and actual reward probability. Therefore, we tested the performance of one-parameter superior models and a few competing models during a dynamic probability estimation task using both relative and absolute definitions of precision (see <xref ref-type="sec" rid="sec010">Methods</xref>). More specifically, we computed the performance of various models using the average difference between the transient signal and the steady state of the signal based on the actual value of reward probability at each time point in the task. We also computed the estimation error as the difference between the estimated and actual reward probability.</p>
<p>As expected, for a simple environment defined by the value <italic>L</italic> (the number of trials before reward probability is changed), the estimation error depends on the model parameter (transition probability or learning rate), except for when using the Bayesian model (<xref ref-type="fig" rid="pcbi.1005630.g008">Fig 8</xref>). For a large value of <italic>L</italic>, the RL model can achieve its optimal performance for a small value of learning rate, but the estimation error increases sharply for other values of the learning rate above that of the Bayesian model (<xref ref-type="fig" rid="pcbi.1005630.g008">Fig 8a</xref>). In contrast, the one-parameter superior family shows a small estimation error for a wide range of transition probability values. The cascade model shows larger estimation error since this model was designed to preserve its signal [<xref ref-type="bibr" rid="pcbi.1005630.ref019">19</xref>]. The performance of our previous heuristic metaplasticity model (RDMP) [<xref ref-type="bibr" rid="pcbi.1005630.ref003">3</xref>] falls between the superior and cascade models for larger values of the transition probability. Qualitatively, similar behavior was observed for performance in a simple environment with a smaller value of <italic>L</italic> (<xref ref-type="fig" rid="pcbi.1005630.g008">Fig 8b</xref>) and in a complex environment in which the value of <italic>L</italic> changes between blocks of trials (<xref ref-type="fig" rid="pcbi.1005630.g008">Fig 8c</xref>). Overall, these results illustrate that superior metaplastic models, which are identified by optimizing for the APT, can perform near-optimally over a wide range of parameter values during a dynamic estimation task.</p>
<fig id="pcbi.1005630.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005630.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Comparison of the one-parameter superior models with competing models during a dynamic probability estimation task.</title>
<p>Plotted are the average estimation errors as a function of the model parameter for the one-parameter superior model with six meta-states (<italic>N</italic> = 6), the heuristic RDMP model, the RL model with one learning rate, and the cascade model. The dotted black line shows the average estimation error for a hierarchical Bayesian model. Panels (a) and (b) show the results for a simple environment with <italic>L</italic> = 100 and 20, respectively. Panel (c) plots the performance for a complex environment with <italic>L</italic> between 10 and 100.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.g008" xlink:type="simple"/>
</fig>
<p>In contrast, performance based on the absolute measure of estimation error (i.e. difference between the estimated and actual reward probability) revealed that one-parameter superior models perform worse than competing models (<xref ref-type="supplementary-material" rid="pcbi.1005630.s005">S3 Fig</xref>). This suboptimality of one-parameter superior models, however, stems from the steady-state signal (i.e. estimated reward probability) that strongly deviates from the actual reward probability (<xref ref-type="supplementary-material" rid="pcbi.1005630.s006">S4 Fig</xref>). This deviation, which allows the superior models to be very sensitive to changes in reward probability near 0.5, was much less pronounced in the cascade and RDMP models (signal in the RL model is equal to the actual reward probability). Overall, these results illustrate that superior metaplastic models, which are identified by optimizing for the APT, can perform close to optimally over a wide range of parameter values during a dynamic probability estimation task. However, these models can be suboptimal when an absolute metric is used for measuring precision (see <xref ref-type="sec" rid="sec009">Discussion</xref>).</p>
</sec>
<sec id="sec008">
<title>Robustness and importance of metaplasticity</title>
<p>In order to test the robustness of the metaplasticity solution, we examined how sensitive the superior solutions were with respect to changes in transition probabilities. To do so, we randomly perturbed the non-zeros elements in the potentiation and depression matrices of the superior models by a specific amount. We found that superior models show a high degree of robustness in their adaptability and precision against changes in their potentiation and depression transfer matrices as long as their transition topologies are not altered (i.e. the zero elements of transition matrices are kept zero) (<xref ref-type="fig" rid="pcbi.1005630.g009">Fig 9</xref>).</p>
<fig id="pcbi.1005630.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005630.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Robustness of metaplasticity to perturbations in model parameters.</title>
<p>(<bold>a</bold>) Plotted is the average adaptability versus average precision in the superior models (blue dots) and superior models with six meta-states (<italic>N</italic> = 6) with perturbed transition matrices (red dots). The black dots show the binary plastic models for comparison (<italic>N</italic> = 2). To implement perturbation, the non-zeros elements in the potentiation and depression matrices of the superior models were independently perturbed by 10% of their original values (with Gaussian noise), with the constraint that the transition probabilities are positive. (<bold>b</bold>) The same as in (a) but with 50% perturbation.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.g009" xlink:type="simple"/>
</fig>
<p>The ultimate test for whether metaplastic transitions are crucial for mitigating the APT is to replace these transitions with plastic ones (transitions that change synaptic efficacy) while keeping the same number of states and transitions. Therefore, we examined the APT in the simple family of metaplastic models, but with different values of synaptic efficacy assigned to different meta-states (<xref ref-type="fig" rid="pcbi.1005630.g010">Fig 10a</xref>; see <xref ref-type="sec" rid="sec010">Methods</xref>). This ‘graded’ plastic model could be reduced to the metaplastic model by setting equal values of synaptic efficacy for different weak or strong states. We found that <inline-formula id="pcbi.1005630.e012"><alternatives><graphic id="pcbi.1005630.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi><mml:mo> </mml:mo><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mtext> </mml:mtext><mml:mi>ℙ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> monotonically increased as the graded plastic model became more similar to the metaplastic model, reflecting the importance of metaplasticity to overcome the APT (<xref ref-type="fig" rid="pcbi.1005630.g010">Fig 10b–10d</xref>). Nevertheless, additional states in the plastic models improved the ability of these models to mitigate the APT beyond binary plastic synapses (<xref ref-type="supplementary-material" rid="pcbi.1005630.s007">S5 Fig</xref>) similar to improvement of memory storage capacity with more states [<xref ref-type="bibr" rid="pcbi.1005630.ref020">20</xref>]. Overall, these results demonstrate that metaplastic transitions are crucial for mitigating the APT.</p>
<fig id="pcbi.1005630.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005630.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Graded plasticity reduces the ability to overcome the APT, indicating that metaplasticity is crucial for adaptive learning.</title>
<p>(<bold>a</bold>) Schematic of the simple-family graded plastic models. This model has an equal number of states and transitions as the metaplastic model, but with different values of synaptic efficacy assigned to different states. (<bold>b</bold>) Plotted is the average <inline-formula id="pcbi.1005630.e013"><alternatives><graphic id="pcbi.1005630.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi><mml:mo> </mml:mo><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mtext> </mml:mtext><mml:mi>ℙ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> in the graded plastic models with four states as a function of the single transition rate and the efficacy of the least weak state (<italic>efficacy</italic> = 1 is equivalent to the <italic>N</italic> = 4 meta-plastic model). (<bold>c</bold>) Plotted is the average <inline-formula id="pcbi.1005630.e014"><alternatives><graphic id="pcbi.1005630.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi><mml:mo> </mml:mo><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mtext> </mml:mtext><mml:mi>ℙ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> in the graded plastic models with six states as a function of the efficacy of the weaker states when the single transition rate was set to 0.2 (<italic>efficacy</italic><sub><italic>1</italic></sub> = <italic>efficacy</italic><sub><italic>2</italic></sub> = 1 is equivalent to a superior meta-plastic model with six meta-states). (<bold>d</bold>) The same as in (c) but for the transition rate equal to 0.7. Overall, <inline-formula id="pcbi.1005630.e015"><alternatives><graphic id="pcbi.1005630.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi><mml:mo> </mml:mo><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mtext> </mml:mtext><mml:mi>ℙ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> monotonically increased as the graded plastic model became similar to the metaplastic model.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.g010" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Discussion</title>
<p>The demands of learning in a changing world require a high degree of adaptability, which comes at the cost of low precision [<xref ref-type="bibr" rid="pcbi.1005630.ref004">4</xref>]. Here we show how metaplasticity, which is reflected in the unreliability of synaptic plasticity, can provide a solution for substantially overcoming the APT. More specifically, by optimizing the APT for a given level of precision, we identify crucial characteristics of superior metaplastic models. The superior models contain reservoir and buffer meta-states; synapses in reservoir meta-states do not change their efficacy upon reward feedback, whereas those in buffer meta-states can change their efficacy. Moreover, rapid changes in efficacy are limited to synapses occupying buffers, which provides a bottleneck that reduces noise without significantly decreasing adaptability. In contrast, more-populated reservoirs can generate a strong signal without manifesting any observable plasticity. The generation of reservoirs and buffers by metaplastic synapses results in the adjustments of learning, or the degree of plasticity, according to recent reward history. For example, when synapses occupy reservoir meta-states, which occurs with consecutive rewarded or unrewarded trials in a stable environment, the behavior should become less adaptable. However, when reward history changes over time, synapses mainly occupy buffer meta-states, causing more adaptable behavior. Overall, the model predicts that learning should be more sensitive to the reward sequence than what has previously been assumed (also see [<xref ref-type="bibr" rid="pcbi.1005630.ref003">3</xref>]).</p>
<p>Importantly, the results of one-parameter superior models and the MC simulations show that more meta-states can improve ability to overcome the APT and, in addition, give rise to more robust models for adaptive learning. These results are compatible with similar improvements in memory storage capacity with larger numbers of states [<xref ref-type="bibr" rid="pcbi.1005630.ref020">20</xref>]. Interestingly, the signal in the metaplastic models is a sigmoid-like function of reward probability. This illustrates that, for intermediate values of <italic>p</italic><sub><italic>r</italic></sub> (around 0.5), learning based on metaplastic synapses is more sensitive to changes in reward probability than learning based on plastic synapses. This sensitivity increases with the number of meta-states. Future experiments that can measure the sensitivity of probability learning can test this prediction of metaplasticity.</p>
<p>The basic mechanism for improvement with more meta-states is also related to the generation of reservoirs and buffers that create a bottleneck for changing synaptic efficacy, and not merely because of having a larger number of states as in graded synapses [<xref ref-type="bibr" rid="pcbi.1005630.ref021">21</xref>]. More specifically, additional meta-states provide intermediate transitions between reservoirs and buffers that could reduce noise without significantly compromising adaptability. Interestingly, it has been shown that in the framework of Markov chains, the eigenvalues and eigenvectors of models with bigger spectral gaps (i.e. more adaptable models) are less sensitive to perturbation of transition probabilities [<xref ref-type="bibr" rid="pcbi.1005630.ref022">22</xref>–<xref ref-type="bibr" rid="pcbi.1005630.ref025">25</xref>]. In other words, more adaptable models can produce signals without fine-tuning. Superior metaplastic models require only a few parameters, and their behavior is not very sensitive to these parameters.</p>
<p>As a higher-order form of plasticity, metaplasticity has been successfully used to explain paradoxical observations regarding synaptic plasticity by considering prior synaptic activity [<xref ref-type="bibr" rid="pcbi.1005630.ref012">12</xref>]. At the cognitive level, however, the computational power of metaplastic synapses has been mainly explored to address memory retention [<xref ref-type="bibr" rid="pcbi.1005630.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1005630.ref026">26</xref>,<xref ref-type="bibr" rid="pcbi.1005630.ref027">27</xref>]. For example, Fusi and colleagues proposed the so-called cascade model to explain how memory could be protected from synaptic changes due to ongoing activity over large timescales [<xref ref-type="bibr" rid="pcbi.1005630.ref019">19</xref>]. By having multiple timescales associated with different meta-states, the cascade model can achieve high levels of both memory storage and retention time, and therefore, mitigate the ‘storage-retention’ tradeoff (i.e. a system which is good at storage would be poor at retention, and vice versa). A more recent study has shown that this tradeoff in memory systems can be further improved by having a large number of states that initially store memory quickly and then transfer memories to slower states [<xref ref-type="bibr" rid="pcbi.1005630.ref028">28</xref>]. This storage-retention tradeoff is exactly the opposite of the adaptability-precision tradeoff studied here, since memory systems are concerned with maintaining the signal whereas learning systems need to be adaptable. Nevertheless, it is encouraging that metaplasticity can mitigate two very different tradeoffs. Moreover, these results suggest that metaplasticity can be useful for estimating signals other than reward probability and is generalizable to other domains of learning for which adaptability and precision are both important.</p>
<p>Our results could also explain why plasticity protocols are unreliable and outcome plasticity is heterogeneous [<xref ref-type="bibr" rid="pcbi.1005630.ref029">29</xref>]. As we showed, superior metaplastic models create bottlenecks for changing synaptic efficacy, since such a property can reduce noise with minimal decrease in adaptability. However, limiting plastic transitions to those that occur from buffers would make many transitions invisible to measurement of change in synaptic efficacy. Therefore, until such a structure is specifically tested, plasticity protocols will be perceived as noisy and unreliable. Besides recent behavioral evidence [<xref ref-type="bibr" rid="pcbi.1005630.ref003">3</xref>], there is no direct electrophysiological evidence for the structure of metaplasticity proposed here. We hope that our study stimulates experimentalists to investigate this structure.</p>
<p>Using the difference between the estimated and actual reward probability as a measure of precision, we find that superior metaplastic models are suboptimal. This occurs because the signal in these models is biased to allow maximal sensitivity to reward probability for intermediate values of reward probability. However, if reward estimates have to be ultimately used for making choices as in binary decision-making tasks, it is more desirable to have a higher accuracy near such values of probabilities (0.5) where the outcome of the decision is more sensitive to the estimated value. Moreover, reward information has to be encoded and represented in the brain in a relative fashion in order to deal with a limited range of neural firing rates. Accordingly, we adopted a relative definition for precision that measures the ability to distinguish neighboring values of reward probability. Therefore, our results suggest that some of the suboptimality in the estimation of reward probability could be due to the biophysical limitations of the nervous system in encoding values.</p>
<p>Our proposal provides a new approach for studying synaptic plasticity and its contribution to brain computations. Our model predicts that a previous reward outcome (learning experience) not only contributes to learning and behavioral changes, but also affects subsequent induction of such changes within a specific time window. On the one hand, certain sequences of reward feedback cause the nervous system to become more receptive to subsequent similar feedback. On the other hand, consecutive feedback can shape future learning such that it is not responsive to feedback in the opposite direction. Understanding such propensity for and unresponsiveness to reward feedback could provide new insights into habit and addiction, respectively. Therefore, further investigations into metaplasticity, both at the behavioral and synaptic levels, could help researchers discover tools for improving learning, especially with respect to habits and addiction [<xref ref-type="bibr" rid="pcbi.1005630.ref030">30</xref>,<xref ref-type="bibr" rid="pcbi.1005630.ref031">31</xref>]. Overall, our work highlights an overlooked contribution of synaptic mechanisms to solving complex cognitive problems [<xref ref-type="bibr" rid="pcbi.1005630.ref032">32</xref>].</p>
</sec>
<sec id="sec010" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec011">
<title>Metaplastic model</title>
<p>Our general model of metaplasticity consisted of multiple meta-states associated with two values of synaptic efficacy (weak and strong) and all possible transitions between these meta-states (<xref ref-type="fig" rid="pcbi.1005630.g001">Fig 1a</xref>). The metaplastic models have <italic>N</italic> distinct meta-states, half of which are associated with strong synaptic efficacy and half with weak. The model is completely specified with two transition matrices, one for a potentiation event (<inline-formula id="pcbi.1005630.e016"><alternatives><graphic id="pcbi.1005630.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>) and one for a depression event (<inline-formula id="pcbi.1005630.e017"><alternatives><graphic id="pcbi.1005630.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>) corresponding to rewarded and unrewarded trials, respectively. Here, we assumed that metaplastic transitions have a consistent order such that potentiation and depression events (on rewarded and unrewarded trials, respectively) create flows in opposite directions. This assumption also establishes weak and strong meta-states with different ‘depths’ such that deeper states are further from the plastic boundary (<xref ref-type="fig" rid="pcbi.1005630.g001">Fig 1a</xref>). Moreover, we assumed symmetry between information by reward and no-reward feedback, and thus only focused on mirror-symmetric flows. This assumption put another constraint on the potentiation and depression matrices:
<disp-formula id="pcbi.1005630.e018">
<alternatives>
<graphic id="pcbi.1005630.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e018" xlink:type="simple"/>
<mml:math display="block" id="M18">
<mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msubsup></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula></p>
<p>Based on these assumptions, transition matrices for potentiation and depression events can be represented by lower-triangular and upper-triangular matrices:
<disp-formula id="pcbi.1005630.e019">
<alternatives>
<graphic id="pcbi.1005630.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e019" xlink:type="simple"/>
<mml:math display="block" id="M19">
<mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>−</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula></p>
<p>There are <italic>N</italic>(<italic>N</italic> − 1)/2 unique transition probabilities for models with <italic>N</italic> meta-states. The probability conservation was dictated by the transition flows out of any meta-state summing up to 1.</p>
<disp-formula id="pcbi.1005630.e020">
<alternatives>
<graphic id="pcbi.1005630.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e020" xlink:type="simple"/>
<mml:math display="block" id="M20">
<mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mstyle><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo>∀</mml:mo><mml:mi>i</mml:mi></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
</sec>
<sec id="sec012">
<title>Mean-field approach</title>
<p>We assumed that the estimation of reward probability was performed by a set of synapses and, thus, reward probability was stored in the strength of these synapses. At any point in time, the signal (<italic>S</italic>) was defined as the difference between the fractions of synapses in the strong and weak meta-states,
<disp-formula id="pcbi.1005630.e021">
<alternatives>
<graphic id="pcbi.1005630.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e021" xlink:type="simple"/>
<mml:math display="block" id="M21">
<mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>Ψ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>Ψ</mml:mi><mml:mo>−</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
where <inline-formula id="pcbi.1005630.e022"><alternatives><graphic id="pcbi.1005630.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:msub><mml:mi>Ψ</mml:mi><mml:mo>−</mml:mo></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mi>Ψ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005630.e023"><alternatives><graphic id="pcbi.1005630.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:msub><mml:mi>Ψ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mtext> </mml:mtext><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>Ψ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula> are fractions of synapses in the weak and strong meta-states, respectively. In the mean-field (MF) approximation approach, the average system dynamics is fully described by the average transition matrix for a given value of reward probability (<inline-formula id="pcbi.1005630.e024"><alternatives><graphic id="pcbi.1005630.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>). The eigenvector, <italic>Ψ</italic>, with an eigenvalue <italic>λ</italic> = 1 (the largest eigenvalue according to Perron-Frobenius theorem) of average transition matrix, <inline-formula id="pcbi.1005630.e025"><alternatives><graphic id="pcbi.1005630.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, provided the steady state of the model from which the average signal was calculated using <xref ref-type="disp-formula" rid="pcbi.1005630.e021">Eq 4</xref>.</p>
<p>As a proxy for signal fluctuations around its average value, we introduced the concept of ‘one-step noise’ as the mean magnitude deviation from the average signal due to one potentiation or depression event:
<disp-formula id="pcbi.1005630.e026">
<alternatives>
<graphic id="pcbi.1005630.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e026" xlink:type="simple"/>
<mml:math display="block" id="M26">
<mml:mrow><mml:mi>η</mml:mi><mml:mo> </mml:mo><mml:mo>≡</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mi>S</mml:mi><mml:mo>〉</mml:mo></mml:mrow><mml:mo> </mml:mo><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo> </mml:mo><mml:mo>+</mml:mo><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mi>S</mml:mi><mml:mo>〉</mml:mo></mml:mrow><mml:mo> </mml:mo><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mo>−</mml:mo></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
where 〈<italic>S</italic>〉 is the average signal based on the steady-state solution, and <italic>S</italic><sub>+</sub> and <italic>S</italic><sub>−</sub> are the signal values after the application of the potentiation or depression transition matrices on the steady-state solution, respectively. In general, noise at time (<italic>t</italic> + 1) is a combination of several components: (1) the attenuated transferred noise from the state of the system at time <italic>t</italic>; (2) the amount of noise generated in one step, from <italic>t</italic> to (<italic>t</italic> + 1); (3) the inherent noise involved in translating <italic>p</italic>(<italic>t</italic>) to a binary representation with potentiation and depression events; and finally (4) a finite size effect when dealing with a limited number of identical synapses. The one-step noise measures the second component and always underestimates the level of the noise in the model. The Monte Carlo simulations, however, contain the sum of the first three components mentioned above and thus capture the overall noise.</p>
<p>We defined precision as the ratio of the signal sensitivity and the one-step noise:
<disp-formula id="pcbi.1005630.e027">
<alternatives>
<graphic id="pcbi.1005630.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e027" xlink:type="simple"/>
<mml:math display="block" id="M27">
<mml:mrow><mml:mi>ℙ</mml:mi><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>S</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>η</mml:mi></mml:mrow>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula></p>
<p>Therefore, precision measures the discriminability between two adjacent reward probabilities based on their resulting signals. We chose this measure instead of the difference between the estimated and actual reward probability because the firing rate of neurons, which represents reward values, can be differentially scaled by their input firing rates. Therefore, the absolute difference may be irrelevant for the nervous system.</p>
<p>Finally, the adaptability of the model was defined as the rate of the decaying mode in the system, and was estimated using the difference between the second-largest eigenvalues (slowest decaying mode) of the average transition matrix and 1 (<inline-formula id="pcbi.1005630.e028"><alternatives><graphic id="pcbi.1005630.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>), also known as the spectral gap in the Markov chains literature. We chose this definition because it is not possible to reduce the dynamics of metaplasticity to arrive at one equation for the synaptic strength. As a result, adaptability measures the lower bound for the rate of convergence to the final steady state of the synaptic strength. Nevertheless, we found that our definition provides a good approximation for this rate (<xref ref-type="fig" rid="pcbi.1005630.g007">Fig 7a</xref>).</p>
<p>By focusing on the steady-state solution, the concept of learning rates in the binary plastic models (<italic>N</italic> = 2) can be generalized to higher <italic>N</italic> as the effective learning rates, <inline-formula id="pcbi.1005630.e029"><alternatives><graphic id="pcbi.1005630.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>±</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. The effective learning rates were defined as the relative change in the fraction of synapses in the weak or strong meta-states after a potentiation or depression event:
<disp-formula id="pcbi.1005630.e030">
<alternatives>
<graphic id="pcbi.1005630.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e030" xlink:type="simple"/>
<mml:math display="block" id="M30">
<mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>±</mml:mo></mml:msup><mml:mo> </mml:mo><mml:mo>Ψ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>±</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo>Ψ</mml:mo><mml:mo>±</mml:mo></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>±</mml:mo></mml:msup><mml:msub><mml:mo>Ψ</mml:mo><mml:mo>∓</mml:mo></mml:msub></mml:mrow>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
where (<italic>T</italic><sup>±</sup> Ψ)<sub>±</sub> is the sum of the fraction of strong/weak meta-states. To examine transitions from a given subset of meta-states, we also defined the ‘effective transition rate’ as the outward flow of synapses from that subset, divided by the fraction of synapses in that subset (<xref ref-type="fig" rid="pcbi.1005630.g003">Fig 3a</xref>). The effective transition rate (<inline-formula id="pcbi.1005630.e031"><alternatives><graphic id="pcbi.1005630.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>) assigns a single rate for outward transition from a set of meta-states <italic>a</italic> to a set of meta-states <italic>b</italic>. There are (2<sup><italic>N</italic></sup> − 2) non-trivial ways that <italic>N</italic> meta-states can be partitioned into two disjoint, complementary subsets.</p>
<p>A closely related concept of conductance, <italic>C</italic>(<italic>S</italic>), for a given subset <italic>S</italic> in a Markov chain is defined as the outward flow from that subset divided by the minimum of occupancy in that subset, <italic>π</italic>(<italic>S</italic>), and occupancy in its complementary set <italic>π</italic>(<italic>S</italic><sup><italic>c</italic></sup>). The magnitude of one-step noise is directly related to the effective transition rate when the two subsets are chosen based on their synaptic efficacy. The value of spectral gap (i.e. the difference between the second-largest eigenvalues of the average transition matrix and 1) is constrained by the minimum conductance among all possible subsets of meta-states [<xref ref-type="bibr" rid="pcbi.1005630.ref018">18</xref>].</p>
</sec>
<sec id="sec013">
<title>Monte Carlo simulations</title>
<p>The Monte Carlo simulations were performed by running multiple trials starting from a given initial state in environments with identical reward statistics (reward probability was the same but the reward sequence varied across different simulations). Data from an initial relaxation period was discarded to remove dependence on the initial state, and the relevant quantities were computed by averaging over the ensemble at a given time step or across time. Moreover, to further reduce the relaxation time, we started from the steady-state solution of the mean-field equation for the initial environment.</p>
<p>To measure the decay rate in the Monte Carlo simulations, we simulated the dynamics of signal in one-parameter superior models (with <italic>N</italic> = 4, and 6) in response to a sudden jump from reward probability of 0.3 to 0.8. We averaged over 100000 different instances of such simulations to obtain the asymptotic convergence of the signal. The asymptotic signal was then fit to an exponential function and the best fit for the time constant was computed using minimum squared error methods. We performed these simulations for different values of model parameters and transition probabilities between 0.05 and 0.25 (with 0.01 increments). The results of the Monte Carlo simulations were then compared with the models’ slowest mode using the mean-field approach.</p>
</sec>
<sec id="sec014">
<title>Finding the optimal solutions</title>
<p>The optimal solutions (i.e. upper-boundary in adaptability × precision vs. precision plot) were found in two stages. An initial upper envelope in the <inline-formula id="pcbi.1005630.e032"><alternatives><graphic id="pcbi.1005630.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi><mml:mo> </mml:mo><mml:mo>×</mml:mo><mml:mtext> </mml:mtext><mml:mi>ℙ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> vs. <inline-formula id="pcbi.1005630.e033"><alternatives><graphic id="pcbi.1005630.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mi>ℙ</mml:mi></mml:math></alternatives></inline-formula> (using discretization for <inline-formula id="pcbi.1005630.e034"><alternatives><graphic id="pcbi.1005630.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mi>ℙ</mml:mi></mml:math></alternatives></inline-formula>) was constructed by random sampling of 10<sup>7</sup> transition matrices. The transition matrices were divided into <italic>n</italic> bins according to their precision, <inline-formula id="pcbi.1005630.e035"><alternatives><graphic id="pcbi.1005630.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mi>ℙ</mml:mi></mml:math></alternatives></inline-formula>, and the transition matrix with the highest value of <inline-formula id="pcbi.1005630.e036"><alternatives><graphic id="pcbi.1005630.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005630.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi><mml:mo> </mml:mo><mml:mo>×</mml:mo><mml:mtext> </mml:mtext><mml:mi>ℙ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> in each bin was selected. These transition matrices were then used as the initial points for our optimization process. To avoid local minima, at the beginning of each iteration, a duplicated copy of the initial transition matrix with added small jitters was generated. All the resulting 2<italic>n</italic> transition matrices were used as the starting point of our optimization. At the end of each optimization iteration, the best solutions in each bin were selected out of all initial transition matrices, and the final outcome of our optimization procedure was used for the initial samples of the next iteration. For models with a large number of meta-states (<italic>N</italic> &gt; 4), we conducted multiple iterations of the optimization process. The higher dimensional solutions are more robust against fluctuations, and optimized solutions can be found by increasing the bin numbers (initial points) and the number of optimization iterations. The optimization was constrained by keeping the sum of every column in transition matrices with positive elements to one. We used MATLAB’s ‘fminsearch’ function for the basic optimization process.</p>
</sec>
<sec id="sec015">
<title>Dynamic probability estimation task</title>
<p>To compare superior metaplastic models and a few competing models, we measured the performance of these models in a dynamic probability estimation task. In this task, the reward is provided on each trial based on a fixed probability. This probability, however, increases or decreases (with equal probability) by 0.1 every <italic>L</italic> number of trials, resulting in 11 different values of reward probability ([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]). Therefore, parameter <italic>L</italic> defines the level of volatility in this task. We simulated the behavior of various models in simple environments (environments with a fixed value of <italic>L</italic>) and in a complex environment where <italic>L</italic> can also change. For the complex environment, after each change in the reward probability, the value of <italic>L</italic> was selected randomly from the following set of values: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100], subject to the constraint that overall lengths of blocks with a given value of <italic>L</italic> are similar.</p>
<p>We used two methods to compare the performance of various models (see below) in this task in terms of estimation error. In the first method, we computed the average difference between the transient signal and the steady state of the signal based on the actual value of reward probability at each time point during the task (relative estimation error). This was done because the signal (or the estimate of reward probability) in each model is relative, and there is a one-to-one mapping between the signal in a given model and the actual reward probability. In the second method, we computed the estimation error by the absolute value of the difference between the estimated and actual reward probability at each time point (absolute estimation error).</p>
</sec>
<sec id="sec016">
<title>Alternative models</title>
<p>In order to compare the performance of our model with competing models, we simulated four models and measured their performances in a dynamic probability estimation task. The first model was an RL model based on reward prediction error (see <xref ref-type="supplementary-material" rid="pcbi.1005630.s001">S1 Text</xref>). This model is equivalent to the binary plastic model (<italic>N</italic> = 2) and can be quantified with one or two parameters (learning rates). The second model was the so-called cascade model of Fusi et al. (2005)[<xref ref-type="bibr" rid="pcbi.1005630.ref019">19</xref>]. This cascade model also assumes metaplasticity and order for transitions between different meta-states similarly to our superior models. However, the cascade model has a different structure for transition than our simple family model and, moreover, transition probabilities become smaller for deeper meta-states. The third model was a heuristic model of reward-dependent metaplasticity (RDMP), which we have proposed to capture behavioral data during a dynamic learning and decision-making task [<xref ref-type="bibr" rid="pcbi.1005630.ref003">3</xref>]. Finally, we also simulated a hierarchical Bayesian model that directly estimates volatility and change in volatility in order to determine the amount of update based on reward feedback [<xref ref-type="bibr" rid="pcbi.1005630.ref001">1</xref>].</p>
</sec>
</sec>
<sec id="sec017">
<title>Supporting information</title>
<supplementary-material id="pcbi.1005630.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Equivalence of the RL model to the model with plastic synapses.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005630.s002" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.s002" xlink:type="simple">
<label>S2 Text</label>
<caption>
<title>Adaptability-precision tradeoff in the model with plastic synapses.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005630.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.s003" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>The APT in the binary plastic model (<italic>N</italic> = 2).</title>
<p>Characteristics of the binary plastic model measured using different quantities as a function of reward probability for two sets of learning rates (<italic>t</italic><sup>+</sup> = 2 × <italic>t</italic><sup>−</sup> = 0.4 and <italic>t</italic><sup>+</sup> = 2 × <italic>t</italic><sup>−</sup> = 0.3). Adopting different learning rates improves the adaptability for certain values of <italic>p</italic><sub><italic>r</italic></sub> and improves precision for complementary values of <italic>p</italic><sub><italic>r</italic></sub>, resulting in a strict tradeoff between adaptability and precision. A simple RL model based on RPE behaves similarly to the binary plastic model shown here.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005630.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.s004" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Example traces of the signal in response to a sudden change in reward probability in the binary plastic model (a, e), and three superior metaplastic models with different numbers of meta-states (<italic>N</italic> = 4, 6, and 8 in b-d and f-h, respectively).</title>
<p>In each plot, the blue trace is an example estimate based on the shown reward sequence (tick marks on the top and bottom correspond to rewarded and unrewarded trials, respectively). Reward probability changed from 0.3 to 0.8 on trial 20. The black curve shows the average signal, and the green shade shows the signal plus/minus its s.e.m. Models in (a-d) are more adaptable, whereas models in (e-h) are more precise. For these simulations, example models were selected to have the same average precision. Overall, metaplastic models can improve adaptability without increasing noise in the signal (thinner green lines).</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005630.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.s005" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Comparison of the performance of one-parameter superior models with competing models during a dynamic probability estimation task.</title>
<p>Plotted are the absolute estimation errors as a function of the model parameter for the one-parameter superior model (<italic>N</italic> = 6), the heuristic RDMP model, the RL model with one learning rate, and the cascade model. The dotted black line shows the average estimation error for a hierarchical Bayesian model. Panels (a) and (b) show the results for a simple environment with <italic>L</italic> = 100 and 20, respectively. Panel (c) plots the performance for a complex environment with <italic>L</italic> between 10 and 100.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005630.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.s006" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Relationship between signal and actual reward probability in different models.</title>
<p>Plotted are the steady state of signals in the superior metaplastic models (a), the cascade model (b), and the heuristic RDMP model (c) as a function of the actual reward probability, <italic>p</italic><sub><italic>r</italic></sub>, for <italic>N</italic> = 6 number of meta-states and different transition probability and model parameters. Blue, red and golden curves correspond to transition probabilities equal to 0.1, 0.2, and 0.4, respectively. The superior metaplastic model deviates the most from the actual reward probability. Note that the signal in superior metaplastic model is independent of transition probability.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005630.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005630.s007" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>The effect of graded synaptic efficacy on the ability to mitigate the APT.</title>
<p>Plotted is the average APT as a function of the average precision using the Monte Carlo simulations in a family of graded plastic models with the same architecture as the one-parameter superior models. Panels (a) and (b) correspond to models with <italic>N</italic> = 4 and <italic>N</italic> = 6 graded states, respectively. The single transition probability is set to 0.1, 0.3, 0.5, 0.7, or 0.9 as indicated in the legend. The graded synaptic efficacies for different synaptic states for <italic>N</italic> = 4 models (a) were set to [-1, -<italic>w</italic><sub><italic>1</italic></sub>, <italic>w</italic><sub><italic>1</italic></sub>, 1]. Points with the same color in (a) correspond to values of <italic>w</italic><sub><italic>1</italic></sub> between 0.1 and 1 with 0.1 increments. The star represents <italic>w</italic><sub><italic>1</italic></sub> = 1 and corresponds to the superior metaplastic model. The graded synaptic efficacies for <italic>N</italic> = 6 models (b) were set to [-1, -<italic>w</italic><sub><italic>1</italic></sub>, -<italic>w</italic><sub><italic>2</italic></sub>, <italic>w</italic><sub><italic>2</italic></sub>, <italic>w</italic><sub><italic>1</italic></sub>, 1]. Points with the same color in (b) correspond to values of <italic>w</italic><sub><italic>1</italic></sub> and <italic>w</italic><sub><italic>2</italic></sub> between 0.1 and 1 with 0.1 increments. The star represents <italic>w</italic><sub><italic>1</italic></sub> = <italic>w</italic><sub><italic>2</italic></sub> = 1 and corresponds to the superior metaplastic model. The black dots are the RL model with different values of the learning rate.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We would like to thank Brad Duchaine, Clara Guo, Daeyeol Lee, Katherine Rowe, and Matt van der Meer for helpful comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005630.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Behrens</surname> <given-names>TEJ</given-names></name>, <name name-style="western"><surname>Woolrich</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Walton</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MFS</given-names></name>. <article-title>Learning the value of information in an uncertain world</article-title>. <source>Nat. Neurosci</source>. <year>2007</year>; <volume>10</volume>(<issue>9</issue>), <fpage>1214</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1954" xlink:type="simple">10.1038/nn1954</ext-link></comment> <object-id pub-id-type="pmid">17676057</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rushworth</surname> <given-names>MFS</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TEJ</given-names></name>. <article-title>Choice, uncertainty and value in prefrontal and cingulate cortex</article-title>. <source>Nat Neurosci</source>. <year>2008</year>; <volume>11</volume>(<issue>4</issue>): <fpage>389</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn2066" xlink:type="simple">10.1038/nn2066</ext-link></comment> <object-id pub-id-type="pmid">18368045</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Farashahi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Donahue</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Khorsand</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Seo</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Soltani</surname> <given-names>A</given-names></name>. <article-title>Metaplasticity as a Neural Substrate for Adaptive Learning and Choice under Uncertainty</article-title>. <source>Neuron</source>, <year>2017</year>; <volume>94</volume>(<issue>2</issue>), <fpage>401</fpage>–<lpage>414</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2017.03.044" xlink:type="simple">10.1016/j.neuron.2017.03.044</ext-link></comment> <object-id pub-id-type="pmid">28426971</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Farashahi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rowe</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Aslami</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Soltani</surname> <given-names>A</given-names></name>. <article-title>Your favorite color makes learning more adaptable and precise</article-title>. <year>2017</year>; <source>bioRxiv</source>, <fpage>097741</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005630.ref005"><label>5</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <source>Reinforcement Learning: An Introduction</source>, <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, MA</publisher-loc>; <year>1998</year>.</mixed-citation></ref>
<ref id="pcbi.1005630.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krugel</surname> <given-names>LK</given-names></name>, <name name-style="western"><surname>Biele</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Mohr</surname> <given-names>PNC</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Heekeren</surname> <given-names>HR</given-names></name>. <article-title>Genetic variation in dopaminergic neuromodulation influences the ability to rapidly and flexibly adapt decisions</article-title>. <source>Proc. Natl. Acad. Sci</source>. <year>2009</year>; <volume>106</volume>(<issue>42</issue>), <fpage>17951</fpage>–<lpage>17956</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0905191106" xlink:type="simple">10.1073/pnas.0905191106</ext-link></comment> <object-id pub-id-type="pmid">19822738</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Payzan-LeNestour</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Bossaerts</surname> <given-names>P</given-names></name>. <article-title>Risk, unexpected uncertainty, and estimation uncertainty: Bayesian learning in unstable settings</article-title>. <year>2011</year>; <source>PLoS Comput Biol</source> <volume>7</volume>, <fpage>e1001048</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1001048" xlink:type="simple">10.1371/journal.pcbi.1001048</ext-link></comment> <object-id pub-id-type="pmid">21283774</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Costa</surname> <given-names>VD</given-names></name>, <name name-style="western"><surname>Tran</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>Turchi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Averbeck</surname> <given-names>BB</given-names></name>. <article-title>Reversal Learning and Dopamine: A Bayesian Perspective</article-title>. <source>J. Neurosci</source>. <year>2015</year>; <volume>35</volume>(<issue>6</issue>), <fpage>2407</fpage>–<lpage>2416</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1989-14.2015" xlink:type="simple">10.1523/JNEUROSCI.1989-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25673835</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abraham</surname> <given-names>WC</given-names></name>, <name name-style="western"><surname>Bear</surname> <given-names>MF</given-names></name>. <article-title>Metaplasticity: the plasticity of synaptic plasticity</article-title>. <source>Trends Neurosci</source>. <year>1996</year>; <volume>19</volume>(<issue>4</issue>), <fpage>126</fpage>–<lpage>130</lpage>. <object-id pub-id-type="pmid">8658594</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abraham</surname> <given-names>WC</given-names></name>. <article-title>Metaplasticity: tuning synapses and networks for plasticity</article-title>. <source>Nat. Rev. Neurosci</source>. <year>2008</year>; <volume>9</volume>(<issue>5</issue>), <fpage>387</fpage>–<lpage>399</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn2356" xlink:type="simple">10.1038/nrn2356</ext-link></comment> <object-id pub-id-type="pmid">18401345</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Müller-Dahlhaus</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ziemann</surname> <given-names>U</given-names></name>. <article-title>Metaplasticity in human cortex</article-title>. <source>Neurosci</source>. <year>2015</year>; <volume>21</volume>(<issue>2</issue>), <fpage>185</fpage>–<lpage>202</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005630.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yger</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Gilson</surname> <given-names>M</given-names></name>. <article-title>Models of metaplasticity: a review of concepts</article-title>. <source>Front. Comput. Neurosci</source>. <year>2015</year>; <volume>9</volume>, <fpage>138</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2015.00138" xlink:type="simple">10.3389/fncom.2015.00138</ext-link></comment> <object-id pub-id-type="pmid">26617512</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Soltani</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Neural Mechanism for Stochastic Behavior During a Competitive Game</article-title>. <source>Neural Networks</source>, <year>2006</year>; <volume>19</volume>, <fpage>1075</fpage>–<lpage>1090</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neunet.2006.05.044" xlink:type="simple">10.1016/j.neunet.2006.05.044</ext-link></comment> <object-id pub-id-type="pmid">17015181</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Soltani</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>A biophysically based neural model of matching law behavior: melioration by stochastic synapses</article-title>. <source>The Journal of Neuroscience</source>, <year>2006</year>; <volume>26</volume>(<issue>14</issue>), <fpage>3731</fpage>–<lpage>3744</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5159-05.2006" xlink:type="simple">10.1523/JNEUROSCI.5159-05.2006</ext-link></comment> <object-id pub-id-type="pmid">16597727</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Soltani</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Synaptic computation underlying probabilisticinference</article-title>. <source>Nat. Neurosci</source>. <year>2010</year> <volume>13</volume>, <fpage>112</fpage>–<lpage>119</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2450" xlink:type="simple">10.1038/nn.2450</ext-link></comment> <object-id pub-id-type="pmid">20010823</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Petersen</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Malenka</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Nicoll</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name>. <article-title>All-or-none potentiation at CA3-CA1 synapses</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <year>1998</year>; <volume>95</volume>(<issue>8</issue>), <fpage>4732</fpage>–<lpage>4737</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005630.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O'Connor</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Wittenberg</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>SSH</given-names></name>. <article-title>Graded bidirectional synaptic plasticity is composed of switch-like unitary events</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <year>2005</year>; <volume>102</volume>(<issue>27</issue>), <fpage>9679</fpage>–<lpage>9684</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0502332102" xlink:type="simple">10.1073/pnas.0502332102</ext-link></comment> <object-id pub-id-type="pmid">15983385</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sinclair</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Jerrum</surname> <given-names>M</given-names></name>. <article-title>Approximate counting, uniform generation and rapidly mixing Markov chains</article-title>. <source>Information and Computation</source>, <year>1989</year>; <volume>82</volume>(<issue>1</issue>), <fpage>93</fpage>–<lpage>133</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005630.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Drew</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Cascade models of synaptically stored memories</article-title>. <source>Neuron</source> <year>2005</year>; <volume>45</volume>(<issue>4</issue>), <fpage>599</fpage>–<lpage>611</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2005.02.001" xlink:type="simple">10.1016/j.neuron.2005.02.001</ext-link></comment> <object-id pub-id-type="pmid">15721245</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Limits on the memory storage capacity of bounded synapses</article-title>. <source>Nature neuroscience</source>, <year>2007</year>; <volume>10</volume>(<issue>4</issue>), <fpage>485</fpage>–<lpage>493</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1859" xlink:type="simple">10.1038/nn1859</ext-link></comment> <object-id pub-id-type="pmid">17351638</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Enoki</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>YL</given-names></name>, <name name-style="western"><surname>Hamilton</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Fine</surname> <given-names>A</given-names></name>. <article-title>Expression of long-term plasticity at individual synapses in hippocampus is graded, bidirectional, and mainly presynaptic: optical quantal analysis</article-title>. <source>Neuron</source>, <year>2009</year>; <volume>62</volume>(<issue>2</issue>), <fpage>242</fpage>–<lpage>253</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.02.026" xlink:type="simple">10.1016/j.neuron.2009.02.026</ext-link></comment> <object-id pub-id-type="pmid">19409269</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Funderlic</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Meyer</surname> <given-names>CD</given-names></name>. <article-title>Sensitivity of the stationary distribution vector for an ergodic Markov chain</article-title>. <source>Linear Algebra and its Applications</source>, <year>1986</year>; <volume>76</volume>, <fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005630.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seneta</surname> <given-names>E</given-names></name>. <article-title>Sensitivity of finite Markov chains under perturbation</article-title>. <source>Statistics &amp; probability letters</source>, <year>1993</year>; <volume>17</volume>(<issue>2</issue>), <fpage>163</fpage>–<lpage>168</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005630.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meyer</surname> <given-names>CD</given-names></name>. <article-title>Sensitivity of the stationary distribution of a Markov chain</article-title>. <source>SIAM Journal on Matrix Analysis and Applications</source>, <year>1994</year>; <volume>15</volume>(<issue>3</issue>), <fpage>715</fpage>–<lpage>728</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005630.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cho</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Meyer</surname> <given-names>CD</given-names></name>. <article-title>Comparison of perturbation bounds for the stationary distribution of a Markov chain</article-title>. <source>Linear Algebra and its Applications</source>, <year>2001</year>; <volume>335</volume>(<issue>1</issue>), <fpage>137</fpage>–<lpage>150</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005630.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barrett</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>van Rossum</surname> <given-names>MC</given-names></name>. <article-title>Optimal learning rules for discrete synapses</article-title>. <source>PLoS Comput Biol</source>, <year>2008</year>; <volume>4</volume>(<issue>11</issue>), <fpage>e1000230</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000230" xlink:type="simple">10.1371/journal.pcbi.1000230</ext-link></comment> <object-id pub-id-type="pmid">19043540</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref027"><label>27</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Lahiri</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ganguli</surname> <given-names>S</given-names></name>. <chapter-title>A memory frontier for complex synapses</chapter-title>. In <source>Advances in Neural Information Processing Systems</source> <year>2013</year>; (pp. <fpage>1034</fpage>–<lpage>1042</lpage>).</mixed-citation></ref>
<ref id="pcbi.1005630.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benna</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>. <article-title>Computational principles of synaptic memory consolidation</article-title>. <source>Nature neuroscienc</source> <year>2016</year>; <volume>19</volume>(<issue>12</issue>). <fpage>1697</fpage>–<lpage>1706</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005630.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Montgomery</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Madison</surname> <given-names>DV</given-names></name>. <article-title>State-dependent heterogeneity in synaptic depression between pyramidal cell pairs</article-title>. <source>Neuron</source>, <year>2002</year>; <volume>33</volume>(<issue>5</issue>), <fpage>765</fpage>–<lpage>777</lpage>. <object-id pub-id-type="pmid">11879653</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moussawi</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Pacchioni</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Moran</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Olive</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Gass</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Lavin</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>N-Acetylcysteine reverses cocaine-induced metaplasticity</article-title>. <source>Nat. Neurosci</source>. <year>2009</year>; <volume>12</volume>(<issue>2</issue>), <fpage>182</fpage>–<lpage>189</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2250" xlink:type="simple">10.1038/nn.2250</ext-link></comment> <object-id pub-id-type="pmid">19136971</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hulme</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>OD</given-names></name>, <name name-style="western"><surname>Abraham</surname> <given-names>WC</given-names></name>. <article-title>Emerging roles of metaplasticity in behaviour and disease</article-title>. <source>Trends in neurosciences</source> <year>2013</year>; <volume>36</volume>(<issue>6</issue>): <fpage>353</fpage>–<lpage>362</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2013.03.007" xlink:type="simple">10.1016/j.tins.2013.03.007</ext-link></comment> <object-id pub-id-type="pmid">23602195</object-id></mixed-citation></ref>
<ref id="pcbi.1005630.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mongillo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name>. <article-title>Synaptic theory of working memory</article-title>. <source>Science</source> <year>2008</year>; <volume>319</volume>, <fpage>1543</fpage>–<lpage>1546</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1150769" xlink:type="simple">10.1126/science.1150769</ext-link></comment> <object-id pub-id-type="pmid">18339943</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>