<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">10-PLCB-RA-1723R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000894</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology/Computational Neuroscience</subject><subject>Neuroscience/Behavioral Neuroscience</subject><subject>Neuroscience/Theoretical Neuroscience</subject><subject>Neuroscience/Natural and Synthetic Vision</subject></subj-group></article-categories><title-group><article-title>Reinforcement Learning on Slow Features of High-Dimensional Input Streams</article-title><alt-title alt-title-type="running-head">Reinforcement Learning on Slow Features</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Legenstein</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Wilbert</surname><given-names>Niko</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Wiskott</surname><given-names>Laurenz</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Institute for Theoretical Computer Science, Graz University of Technology, Graz, Austria</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Institute for Theoretical Biology, Humboldt-Universität zu Berlin, Berlin, Germany</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Bernstein Center for Computational Neuroscience, Berlin, Germany</addr-line>       </aff><aff id="aff4"><label>4</label><addr-line>Institut für Neuroinformatik, Ruhr-Universität Bochum, Bochum, Germany</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Morrison</surname><given-names>Abigail</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">RIKEN Brain Science Institute, Japan</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">legi@igi.tugraz.at</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: RL NW LW. Performed the experiments: RL NW. Analyzed the data: RL NW LW. Wrote the paper: RL NW LW.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>8</month><year>2010</year></pub-date><pub-date pub-type="epub"><day>19</day><month>8</month><year>2010</year></pub-date><volume>6</volume><issue>8</issue><elocation-id>e1000894</elocation-id><history>
<date date-type="received"><day>26</day><month>1</month><year>2010</year></date>
<date date-type="accepted"><day>16</day><month>7</month><year>2010</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2010</copyright-year><copyright-holder>Legenstein et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>Humans and animals are able to learn complex behaviors based on a massive stream of sensory information from different modalities. Early animal studies have identified learning mechanisms that are based on reward and punishment such that animals tend to avoid actions that lead to punishment whereas rewarded actions are reinforced. However, most algorithms for reward-based learning are only applicable if the dimensionality of the state-space is sufficiently small or its structure is sufficiently simple. Therefore, the question arises how the problem of learning on high-dimensional data is solved in the brain. In this article, we propose a biologically plausible generic two-stage learning system that can directly be applied to raw high-dimensional input streams. The system is composed of a hierarchical slow feature analysis (SFA) network for preprocessing and a simple neural network on top that is trained based on rewards. We demonstrate by computer simulations that this generic architecture is able to learn quite demanding reinforcement learning tasks on high-dimensional visual input streams in a time that is comparable to the time needed when an explicit highly informative low-dimensional state-space representation is given instead of the high-dimensional visual input. The learning speed of the proposed architecture in a task similar to the Morris water maze task is comparable to that found in experimental studies with rats. This study thus supports the hypothesis that slowness learning is one important unsupervised learning principle utilized in the brain to form efficient state representations for behavioral learning.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>Humans and animals are able to learn complex behaviors based on a massive stream of sensory information from different modalities. Early animal studies have identified learning mechanisms that are based on reward and punishment such that animals tend to avoid actions that lead to punishment whereas rewarded actions are reinforced. It is an open question how sensory information is processed by the brain in order to learn and perform rewarding behaviors. In this article, we propose a learning system that combines the autonomous extraction of important information from the sensory input with reward-based learning. The extraction of salient information is learned by exploiting the temporal continuity of real-world stimuli. A subsequent neural circuit then learns rewarding behaviors based on this representation of the sensory input. We demonstrate in two control tasks that this system is capable of learning complex behaviors on raw visual input.</p>
</abstract><funding-group><funding-statement>This work was supported by the Austrian Science Fund FWF [S9102-N13, to RL]. NW has been supported by the German Federal Ministry of Education and Research through a grant to the Bernstein Center of Computational Neuroscience Berlin. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="13"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>The nervous system of vertebrates continuously generates decisions based on a massive stream of complex multimodal sensory input. The strength of this system is based on its ability to adapt and learn suitable decisions in novel situations. Early animal studies have identified learning mechanisms that are based on reward and punishment such that animals tend to avoid actions that lead to punishment whereas rewarded actions are reinforced. The study of such reward-based learning goes back to Thorndikes law of effect <xref ref-type="bibr" rid="pcbi.1000894-Thorndike1">[1]</xref>. Later, the mathematically well-founded theory of reinforcement learning, which describes learning by reward, has been developed <xref ref-type="bibr" rid="pcbi.1000894-Bertsekas1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Sutton1">[3]</xref>.</p>
<p>In a general reinforcement learning problem, an agent senses the environment at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e001" xlink:type="simple"/></inline-formula> via a state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e002" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e003" xlink:type="simple"/></inline-formula> is the state space of the problem. The agent then chooses an action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e004" xlink:type="simple"/></inline-formula>, which leads to state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e005" xlink:type="simple"/></inline-formula> according to some (in general probabilistic) state-transition relation. The agent also receives some reward signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e006" xlink:type="simple"/></inline-formula>, which depends probabilistically on the state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e007" xlink:type="simple"/></inline-formula>. By choosing an action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e008" xlink:type="simple"/></inline-formula> the agent aims at maximizing the expected discounted future reward<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e009" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e010" xlink:type="simple"/></inline-formula> denotes the expectation and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e011" xlink:type="simple"/></inline-formula> is some discount rate. This general theory has a huge influence on psychology, systems neuroscience, machine learning, and engineering and numerous algorithms have been developed for the reinforcement learning problem. By utilizing these algorithms, many impressive control applications have been developed. Several experimental studies connect the neural basis for reward-based learning in animals to well-known reinforcement learning algorithms. It has been shown that the activity of dopaminergic neurons in the ventral tegmental area is related to the reward-prediction error <xref ref-type="bibr" rid="pcbi.1000894-Schultz1">[4]</xref>, a signal that is needed for parameter updates in temporal difference learning <xref ref-type="bibr" rid="pcbi.1000894-Sutton1">[3]</xref>. These neurons in turn have dense diffuse projections to several important areas including the striatum. In the striatum it was shown that dopamine influences synaptic plasticity <xref ref-type="bibr" rid="pcbi.1000894-Reynolds1">[5]</xref>. Hence, the principal basis of reward-based learning in this sub-system, although not well understood yet, could be related to well-known reinforcement learning algorithms. However, the learning capabilities of animals such as rodents are still far from reach with current reinforcement learning algorithms. Since physiological experiments are consistent with quite standard reward-based learning schemes, it is reasonable to speculate that the superior learning capabilities of animals is to a high degree based on the ability to autonomously extract relevant features from the input stream such that subsequent reward-based learning is highly simplified (We note that the distinction between feature extraction and reward-based learning is most likely not so strict in the brain. For example, acetylcholine is a prominent neuromodulator in sensory cortical areas which could be utilized for task-dependent feature extraction). In fact, one of the most crucial design questions in the design of a reinforcement learning system is the definition of the state space <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e012" xlink:type="simple"/></inline-formula>. Most reinforcement learning algorithms are only applicable if the state space of the problem is sufficiently small. Thus, if the sensory input to a controller is complex and high-dimensional, the first task of the designer is to extract from this high-dimensional input stream a highly compressed representation that encodes the current state of the environment in a suitable way such that the agent can learn to solve the task. In contrast, the nervous system is able to learn good decisions from high-dimensional visual, auditory, tactile, olfactory, and other sensory inputs autonomously. The autonomous extraction of relevant features in the nervous system is commonly attributed to neocortex. The way how neocortex extracts features from the sensory input is still unknown and a matter of debate. Several principles with biologically plausible neural implementations have been postulated. Possible candidates are for example principal component analysis (PCA) <xref ref-type="bibr" rid="pcbi.1000894-Gerstner1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Buesing1">[7]</xref>, independent component analysis <xref ref-type="bibr" rid="pcbi.1000894-Hyvrinen1">[8]</xref>–<xref ref-type="bibr" rid="pcbi.1000894-Klampfl1">[10]</xref>, and information bottleneck optimization <xref ref-type="bibr" rid="pcbi.1000894-Klampfl1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Tishby1">[11]</xref>. One learning algorithm that exploits slowness information is slow feature analysis (SFA) <xref ref-type="bibr" rid="pcbi.1000894-Wiskott1">[12]</xref>. SFA extracts the most slowly varying features in the input stream (see below). One important property of SFA is that it can be applied in a hierarchical fashion, first extracting local features on the raw input data which are then integrated to more and more global and abstract features. This hierarchical organization is similar to cortical organization for example in the visual system (we note however that the characteristic recurrent organization of cortex where multiple loops provide feedback from higher-level to lower-level processing is not yet exploited in hierarchical SFA architectures). Furthermore, the features that emerge from SFA have been shown to resemble the stimulus tunings of neurons both at low and high levels of sensory representation such as various types of complex cells in the visual system <xref ref-type="bibr" rid="pcbi.1000894-Berkes1">[13]</xref> as well as hippocampal place cells, head-direction cells, and spatial-view cells <xref ref-type="bibr" rid="pcbi.1000894-Franzius1">[14]</xref>.</p>
<p>These features have been extracted from visual input. This hints at the usefulness of SFA for autonomous learning on high-dimensional input streams. In fact, it was shown in <xref ref-type="bibr" rid="pcbi.1000894-Franzius2">[15]</xref> that important stimulus features such as object category, the position of objects, or their orientation can be easily extracted by supervised training with high precision from the slow features of a high-dimensional visual input stream. It should be noted that the SFA algorithm is only one particular implementation of learning based on slowness, and there have been various earlier approaches, e.g., <xref ref-type="bibr" rid="pcbi.1000894-Fldik1">[16]</xref>–<xref ref-type="bibr" rid="pcbi.1000894-Becker1">[19]</xref>. Slowness has previously been used in some hierarchical models as well <xref ref-type="bibr" rid="pcbi.1000894-Wallis1">[20]</xref>–<xref ref-type="bibr" rid="pcbi.1000894-Wyss1">[22]</xref>.</p>
<p>Unsupervised learning based on the slowness principle (i.e., learning that exploits temporal continuity of real-world stimuli) has recently attracted the attention of experimentalists <xref ref-type="bibr" rid="pcbi.1000894-Li1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Miyashita1">[24]</xref>. It was shown in monkey experiments, that features in monkey infero temporal cortex are adapted in a way that is consistent with the slowness principle <xref ref-type="bibr" rid="pcbi.1000894-Li1">[23]</xref>.</p>
<p>In this article, we propose a learning system where the state space representation is constituted autonomously by SFA. A subsequent neural circuit is then trained by a reward-based synaptic learning rule that is related to policy gradient methods or Q-learning in classical reinforcement learning. We apply this system to two closed-loop control tasks where the input to the system is high-dimensional raw pixel data and the output are motor commands. We thus show in this article for two control tasks on high-dimensional visual input streams that the representation of the SFA output is well suited to serve as a state-representation for reward-based learning in a subsequent neural circuit.</p>
</sec><sec id="s2" sec-type="methods">
<title>Methods</title>
<p>The learning system considered in this article consists of two components, a hierarchical SFA network and a subsequent control network, see <xref ref-type="fig" rid="pcbi-1000894-g001">Figure 1</xref>. The SFA network reduces the dimensionality of the state-space from 24025 to a small number <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e013" xlink:type="simple"/></inline-formula> that was chosen to be 64 or less in this article. The decisions of the subsequent control network are based solely on the features extracted by the SFA network.</p>
<fig id="pcbi-1000894-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000894.g001</object-id><label>Figure 1</label><caption>
<title>The learning system and the simulation setup.</title>
<p>The learning system (gray box) consists of a hierarchical slow-feature analysis network, which reduces the dimensionality of the high-dimensional visual input. This reduction is trained in an unsupervised manner. The extracted features from the SFA network serve as inputs for a small neural network that produces the control commands. This network is trained by simple reward-modulated learning. We tested the learning system in a closed-loop setup. The system controlled an agent in an environment (universe). The state of the environment was accessible to the learning system via a visual sensory stream of dimension 155<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e014" xlink:type="simple"/></inline-formula>155. A reward signal was made accessible to the control network for learning.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.g001" xlink:type="simple"/></fig><sec id="s2a">
<title>The environment</title>
<p>We tested this learning system on two different control tasks where an agent (a fish) navigates in a 2D environment with analog state- and action-space: a task similar to the Morris water-maze task <xref ref-type="bibr" rid="pcbi.1000894-Morris1">[25]</xref> and a variable-targets task, see section “Tasks”. The state of the universe at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e015" xlink:type="simple"/></inline-formula> (see below for details) was used to render a 155 <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e016" xlink:type="simple"/></inline-formula> 155 dimensional 2D visual scene that showed the agent (a fish; for one of the tasks two fish-types with different visual appearance were used) at a position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e017" xlink:type="simple"/></inline-formula> and potentially other objects, see <xref ref-type="fig" rid="pcbi-1000894-g002">Figure 2</xref>. This visual scene constituted the input to the learning system. These tasks are to be seen as generic control tasks of reasonable complexity. The bird's eye perspective used here is of course not realistic for animal agents. As demonstrated in <xref ref-type="bibr" rid="pcbi.1000894-Franzius1">[14]</xref> our model should also be able to deal with a first-person perspective, especially in the Morris water-maze. For the variable-targets task this would introduce some complications like the target not being in the field of view or being hidden behind the distractor. On the other hand it would simplify the task, since the agent would not need to know its own position and angle (it could simply center its field of view on the target).</p>
<fig id="pcbi-1000894-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000894.g002</object-id><label>Figure 2</label><caption>
<title>Examples for the visual input to the learning system for the variable-targets task.</title>
<p>The scene consists of three objects, the agent (fish), an object that indicates the location of the target, and a second object that acts as a distractor. As indicated in the figure the target object depends on the fish identity. For the fish identity shown in the upper panels the target is always the disk, whereas the for the other fish identity, the target is the cross. In the visual input for the water-maze task the target and the distractor are not present, and the agent representation is the non-rotated image of the fish-type shown in the upper panels.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.g002" xlink:type="simple"/></fig>
<p>For the training of the system, we distinguish two different phases. In a first phase the SFA network is trained. In this phase, the fish, the target, and the distractor are floating slowly over the 2D space of the environment. The type of fish is changed from time to time (see section “Training stimuli of the hierarchical network”).</p>
<p>In a second phase the control circuit is trained. This phase consists of several learning episodes, an episode being one trial to reach a defined target from the initial fish-position. An episode ends when the target is reached or when a maximum number of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e018" xlink:type="simple"/></inline-formula> time-steps is exceeded.</p>
</sec><sec id="s2b">
<title>Slow feature analysis</title>
<p>The hierarchical network described in the next section is based on the Slow Feature Analysis Algorithm (SFA) <xref ref-type="bibr" rid="pcbi.1000894-Wiskott2">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Wiskott3">[27]</xref>. SFA solves the following learning task: Given a multidimensional input signal we want to find instantaneous scalar input-output functions that generate output signals that vary as slowly as possible but still carry significant information. To ensure the latter we require the output signals to be uncorrelated and have unit variance. In mathematical terms, this can be stated as follows:</p>
<p><bold>Optimization problem:</bold> <italic>Given a function space</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e019" xlink:type="simple"/></inline-formula> <italic>and an I-dimensional input signal</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e020" xlink:type="simple"/></inline-formula> <italic>find a set of</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e021" xlink:type="simple"/></inline-formula> <italic>real-valued input-output functions</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e022" xlink:type="simple"/></inline-formula> <italic>such that the output signals</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e023" xlink:type="simple"/></inline-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e024" xlink:type="simple"/><label>(1)</label></disp-formula><italic>under the constraints</italic><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e025" xlink:type="simple"/><label>(2)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e026" xlink:type="simple"/><label>(3)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e027" xlink:type="simple"/><label>(4)</label></disp-formula><italic>with</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e028" xlink:type="simple"/></inline-formula> <italic>and</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e029" xlink:type="simple"/></inline-formula> <italic>indicating temporal averaging and the derivative of</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e030" xlink:type="simple"/></inline-formula>, <italic>respectively.</italic></p>
<p>Equation (1) introduces the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e031" xlink:type="simple"/></inline-formula>-value, which is a measure of the temporal slowness (or rather fastness) of the signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e032" xlink:type="simple"/></inline-formula>. It is given by the mean square of the signal's temporal derivative, so that small <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e033" xlink:type="simple"/></inline-formula>-values indicate slowly varying signals. The constraints (2) and (3) avoid the trivial constant solution and constraint (4) ensures that different functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e034" xlink:type="simple"/></inline-formula> code for different aspects of the input. Because of constraint (4) the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e035" xlink:type="simple"/></inline-formula> are also ordered according to their slowness, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e036" xlink:type="simple"/></inline-formula> having the smallest <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e037" xlink:type="simple"/></inline-formula>.</p>
<p>It is important to note that although the objective is slowness, the functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e038" xlink:type="simple"/></inline-formula> are instantaneous functions of the input, so that slowness cannot be achieved by low-pass filtering. Slow output signals can only be obtained if the input signal contains slowly varying features that can be extracted instantaneously by the functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e039" xlink:type="simple"/></inline-formula>. Note also that for the same reason, once trained, the system works fast, not slowly.</p>
<p>In the computationally relevant case where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e040" xlink:type="simple"/></inline-formula> is finite-dimensional the solution to the optimization problem can be found by means of Slow Feature Analysis (SFA) <xref ref-type="bibr" rid="pcbi.1000894-Wiskott2">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Wiskott3">[27]</xref>. This algorithm, which is based on an eigenvector approach, is guaranteed to find the global optimum. Biologically more plausible learning rules for the optimization problem exist <xref ref-type="bibr" rid="pcbi.1000894-Hashimoto1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Sprekeler1">[29]</xref>.</p>
</sec><sec id="s2c">
<title>Hierarchical network model</title>
<p>The visual system is, to a first approximation, structured in a hierarchical fashion, first extracting local features which are then integrated to more and more global and abstract features. We apply SFA in a similar hierarchical manner to the raw visual input data. First, the slow features of small local image patches are extracted. The integration of spatially local information exploits the local correlation structure of visual data. A second layer extracts slow features of these features (again integrating spatially local patches), and so on. Such hierarchical architecture is promising because SFA has been applied successfully to visual data in a hierarchical fashion previously <xref ref-type="bibr" rid="pcbi.1000894-Franzius2">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Franzius3">[30]</xref>. A hierarchical organization also turns out to be crucial for the applicability of the approach for computational reasons. The application of non-linear SFA on the whole high-dimensional input would be computationally infeasible. Efficient use of resources is also an issue in biological neural circuits. It has been suggested that connectivity is the main constraint there <xref ref-type="bibr" rid="pcbi.1000894-Legenstein1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Chklovskii1">[32]</xref>. Since a hierarchical organization requires nearly exclusively local communication, it avoids extensive connectivity.</p>
<p>The hierarchical network consists of a converging hierarchy of layers of SFA nodes, and the network structure is identical to that used in <xref ref-type="bibr" rid="pcbi.1000894-Franzius3">[30]</xref> (there this part of our model is also discussed in greater length). All required building blocks for the hierarchical network are available in the “Modular toolkit for Data Processing” (MDP) library <xref ref-type="bibr" rid="pcbi.1000894-Zito1">[33]</xref>.</p>
<sec id="s2c1">
<title>Network structure</title>
<p>The detailed network structure is shown in <xref ref-type="fig" rid="pcbi-1000894-g003">Figure 3</xref>. It consists of four layers of SFA nodes, connected topographically in a feed-forward manner. We first describe the internal organization of each individual SFA node before we give a detailed description of the connection architecture below. In each SFA node, first additive Gaussian white noise (with a variance of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e041" xlink:type="simple"/></inline-formula>) is introduced for numerical reasons, to avoid possible singularities in the subsequent SFA step. Then a linear SFA is performed for a first reduction of the input dimensionality. In a subsequent quadratic expansion, the incoming data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e042" xlink:type="simple"/></inline-formula> is mapped with a basis of the space of polynomials with degree up to two. So in addition to the original data, all quadratic combinations like <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e043" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e044" xlink:type="simple"/></inline-formula> are concatenated to the data block. Another linear SFA stage is applied on the expanded data. The solutions of linear SFA on this expanded data is equivalent to those of SFA in the space of polynomials up to degree two. After the second SFA stage we apply a clipping at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e045" xlink:type="simple"/></inline-formula>. This clipping removes extreme values that can occur on test data due to the divergence of the quadratic functions for larger values. However, both the additive noise and the clipping are mostly just technical safeguards and have typically no effect on the network performance.</p>
<fig id="pcbi-1000894-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000894.g003</object-id><label>Figure 3</label><caption>
<title>Model architecture and stimuli.</title>
<p>An input image is fed into the hierarchical network. The circles in each layer symbolize the overlapping receptive fields, which converge towards the top layer. The same set of steps is applied on each layer, which is visualized on the right hand side.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.g003" xlink:type="simple"/></fig>
<p>The number of SFA components used from the first linear SFA stage in each node depends on the layer in which the SFA node is situated. The first linear SFA stage in each node reduces the dimensionality to 32 in the first two layers, 42 in the third layer, and 52 in the fourth layer (the increase in dimensionality across layers leads to a small performance increase). Accordingly, the quadratic expansion then increases dimensionality to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e046" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e047" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e048" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e049" xlink:type="simple"/></inline-formula>, in the first, second, third, and fourth layer respectively. The second linear SFA stage reduces the dimensionality of the expanded signal to 32, except for the top layer, where the output is reduced to 64 dimensions. One can then choose how many of these outputs are actually used in the reinforcement learning (for the variable-targets task the 32 slowest outputs were used).</p>
<p>We now describe how the nodes are connected (see <xref ref-type="fig" rid="pcbi-1000894-g004">Figure 4</xref>). We use a layered feed-forward architecture, i.e., the nodes in the first layer receive inputs only from the input image and nodes in higher layers receive inputs exclusively from the previous layer. Additionally, connections are topographically structured such that a node receives inputs from neighboring nodes in the previous layer. In the following, the part of the input image that influences a node's output is denoted as its receptive field. On the lowest layer, the receptive field of each node consists of an image patch of 10 by 10 grayscale pixels. The receptive fields jointly cover the input image of 155 by 155 pixels. The nodes form a regular (i.e., non-foveated) 30 by 30 grid with partially overlapping receptive fields, resulting in an overlap of five pixels in each direction. The second layer contains 14 by 14 nodes, each receiving input from 4 by 4 layer 1 nodes with neighboring receptive fields, resembling a retinotopic layout (the overlap is two nodes in each direction). The third layer contains 6 by 6 nodes, each receiving input from 4 by 4 layer 2 nodes with neighboring receptive fields, again in a retinotopic layout (with 2 nodes overlap in each direction, as shown in <xref ref-type="fig" rid="pcbi-1000894-g004">Figure 4</xref>). All 6 by 6 layer 3 outputs converge onto a single node in layer 4, whose output we call SFA-output. This organization is summarized in <xref ref-type="table" rid="pcbi-1000894-t001">Table 1</xref>.</p>
<fig id="pcbi-1000894-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000894.g004</object-id><label>Figure 4</label><caption>
<title>Receptive field of nodes in layer 3.</title>
<p>Each dot represents the 32 dimensional SFA output from one node. The field overlap is 2 nodes and the borders of the receptive fields are represented by the black lines between the dots.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.g004" xlink:type="simple"/></fig><table-wrap id="pcbi-1000894-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000894.t001</object-id><label>Table 1</label><caption>
<title>Overview of the network architecture.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000894-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Layer</td>
<td align="left" colspan="1" rowspan="1">Number of nodes</td>
<td align="left" colspan="1" rowspan="1">Input area of node</td>
<td align="left" colspan="1" rowspan="1">Overlap per direction</td>
<td align="left" colspan="1" rowspan="1">SFA outputs per node</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">0 (Image)</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e050" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">(1 pixel)</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e051" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e052" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">5</td>
<td align="left" colspan="1" rowspan="1">32</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">2</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e053" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e054" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">2</td>
<td align="left" colspan="1" rowspan="1">32</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">3</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e055" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e056" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">2</td>
<td align="left" colspan="1" rowspan="1">32</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">4</td>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e057" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">64</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt101"><label/><p>Layer 0 denotes the input image, a node corresponds to a pixel in that image. The input area denotes the number of nodes in the previous layer from which a node receives input, this is also called the receptive field. An example for layer 3 is visualized in <xref ref-type="fig" rid="pcbi-1000894-g004">Figure 4</xref>.</p></fn></table-wrap-foot></table-wrap>
<p>Thus, the hierarchical organization of the model captures two important aspects of cortical visual processing: increasing receptive field sizes and accumulating computational power at higher layers. The latter is due to the quadratic expansion in each layer, so that each layer computes a subset of higher polynomials than its predecessor. The SFA-outputs at the top layer compute subsets of polynomials of degree <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e058" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2c2">
<title>Network training</title>
<p>For each of the two tasks discussed in this paper (Morris water-maze and variable-targets) we trained a dedicated hierarchical network. The number of training samples and the training itself was done in the same way for both tasks, only the content of the training samples was different (this is described in the next section).</p>
<p>The network layers were trained sequentially from bottom to top. We used 50,000 time points for the training of the two lower layers and 200,000 for the two top layers. These training sequences were generated with a random walk procedure, which is described in the next section. The random walk parameters of the training data were identical for all layers. The larger training set for the top layers is motivated by the smaller multiplicative effect of the weight-sharing and by the slower time scales towards the top (though one has to combine this factor with the complexity of the data structure).</p>
<p>For computational efficiency, we train only one node with stimuli from all node locations in its layer and replicate this node throughout the layer. For example this means that the node in the lowest layer sees <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e059" xlink:type="simple"/></inline-formula> times as much data as if it was only trained at a single location. This mechanism effectively increases the number of training samples and implements a weight-sharing constraint. However, the system performance does not depend on this mechanism. The statistics of the training data are approximately identical for all receptive fields, so individually learned nodes would lead to the same results (but at higher computational cost). While the weight-sharing does ease the emergence of translation invariance it is not at all sufficient.</p>
<p>The simulated views are generated from their configuration (position, angles, and object identity) with floating point precision and are not artificially discretized.</p>
</sec><sec id="s2c3">
<title>Training stimuli of the hierarchical network</title>
<p>The training sequences for the two tasks were created with the same random walk procedure that was used in <xref ref-type="bibr" rid="pcbi.1000894-Franzius3">[30]</xref>. The configuration of the objects shown (i.e. the agent in the water-maze task, and for the variable-targets task also target and distractor) was updated in each timestep. Such an update consists of adding a random term to the current spatial velocities of the objects and to the in-plane angular velocity for the agent object (the fish). The velocities are then used to calculate the new positions of the objects, which are in the interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e060" xlink:type="simple"/></inline-formula>, and the new angle of the agent. The velocity distribution was the same for all objects (max. velocity of 0.06 and a max. update of 0.01). For the in-plane angle of the agent the max. velocity was 0.04 with a max. update of 0.01 (in radiant measure).</p>
<p>For the variable-targets task training the objects were given a radius so that they bounce off each other. The radii were chosen such that there could be only a small visible overlap between any two objects (radius of 0.4 for the agent, 0.2 for target and distractor). In each time step the agent identity was switched with a probability of 0.002.</p>
</sec></sec><sec id="s2d">
<title>Neural circuits for reward-based learning</title>
<p>We employed neural implementations of two reinforcement learning algorithms, one is based on Q-learning and one is a policy-gradient method.</p>
<p>Neural versions of Q-learning have been used in various previous works on biological reward-based learning, see e.g. <xref ref-type="bibr" rid="pcbi.1000894-Foster1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Sheynikhovich1">[35]</xref>. The popularity of Q-learning stems from the finding that the activity of dopaminergic neurons in the ventral tegmental area is related to the reward-prediction error <xref ref-type="bibr" rid="pcbi.1000894-Schultz1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Montague1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Schultz2">[37]</xref>, a signal that is needed in Q-learning <xref ref-type="bibr" rid="pcbi.1000894-Sheynikhovich1">[35]</xref>. In Q-learning, decisions are based on a so-called Q-function that maps state-action pairs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e061" xlink:type="simple"/></inline-formula> onto values that represent the current estimate of the expected total discounted reward given that action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e062" xlink:type="simple"/></inline-formula> is executed at state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e063" xlink:type="simple"/></inline-formula>. For a given state, the action with highest associated Q-value is preferred by the agent. However, to ensure exploration, a random action may be chosen with some probability. We implemented the neural version of Q-learning from <xref ref-type="bibr" rid="pcbi.1000894-Sheynikhovich1">[35]</xref> where the Q-function is represented by a small ensemble of neurons and parametrized by the connection weights from the inputs to these neurons. The system learns by adaptation of the Q-function via the network weights. In the implementation used in this article, this is achieved by a local synaptic learning rule at the synapses of the neurons in the neuron ensemble. The global signal that modulates local learning is the temporal difference error (TD-error). We do not address in this article the question how this signal is computed by a neuronal network. Several possible mechanisms have been suggested in the literature <xref ref-type="bibr" rid="pcbi.1000894-Schultz2">[37]</xref>–<xref ref-type="bibr" rid="pcbi.1000894-Berns1">[39]</xref>.</p>
<p>The Q-function was represented by a set of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e064" xlink:type="simple"/></inline-formula> linear neurons that receive information about the current state from the output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e065" xlink:type="simple"/></inline-formula> of the SFA circuit. The output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e066" xlink:type="simple"/></inline-formula> of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e067" xlink:type="simple"/></inline-formula> is hence given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e068" xlink:type="simple"/></inline-formula>.</p>
<p>Each neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e069" xlink:type="simple"/></inline-formula> has a dedicated preferred direction <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e070" xlink:type="simple"/></inline-formula>. The Q-value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e071" xlink:type="simple"/></inline-formula> of a movement in direction <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e072" xlink:type="simple"/></inline-formula> for the given state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e073" xlink:type="simple"/></inline-formula> is hence given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e074" xlink:type="simple"/></inline-formula>. The activities of these neurons imply a proposed action for the agent which is a movement in the direction given by the population vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e075" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e076" xlink:type="simple"/></inline-formula> is the angle of the vector<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e077" xlink:type="simple"/><label>(5)</label></disp-formula>where the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e078" xlink:type="simple"/></inline-formula> is the unit vector in direction <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e079" xlink:type="simple"/></inline-formula>.</p>
<p>The Q-function is parametrized by the weight values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e080" xlink:type="simple"/></inline-formula> and it is learned by adapting these weights according to the Q-learning algorithm (see <xref ref-type="bibr" rid="pcbi.1000894-Sheynikhovich1">[35]</xref>):</p>
<list list-type="order"><list-item>
<p>For time step <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e081" xlink:type="simple"/></inline-formula>, compute the Q-values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e082" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p>Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e083" xlink:type="simple"/></inline-formula> be a movement in direction of the population vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e084" xlink:type="simple"/></inline-formula></p>
</list-item><list-item>
<p>Choose the next action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e085" xlink:type="simple"/></inline-formula> to be <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e086" xlink:type="simple"/></inline-formula> with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e087" xlink:type="simple"/></inline-formula> or a movement in a random direction with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e088" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p>A Gaussian profile around the chosen action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e089" xlink:type="simple"/></inline-formula> is enforced in the neural ensemble resulting in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e090" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p>The eligibility trace is updated according to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e091" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e092" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p>Action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e093" xlink:type="simple"/></inline-formula> is executed and time is updated <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e094" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p>The reward prediction error is calculated as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e095" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e096" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p>Update the weights of the neuron population according to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e097" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e098" xlink:type="simple"/></inline-formula> being a small decaying learning rate.</p>
</list-item></list>
<p>See Supporting <xref ref-type="supplementary-material" rid="pcbi.1000894.s003">Text S1</xref> for parameter settings.</p>
<p>The second learning algorithm employed was a policy gradient method. In this case, the action is directly given by the output of a neural network. Hence, the network (which receives as input the state-representation from the SFA network) represents a policy (i.e., a mapping from a state to an action). Most theoretical studies of such biologically plausible policy-gradient learning algorithms are based on point-neuron models where synaptic inputs are weighted by the synaptic efficacies to obtain the membrane voltage. The output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e099" xlink:type="simple"/></inline-formula> of the neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e100" xlink:type="simple"/></inline-formula> is then essentially obtained by the application of a nonlinear function to the membrane voltage. A particularly simple example of such a neuron model is a simple pseudo-linear rate-based model where a nonlinear activation function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e101" xlink:type="simple"/></inline-formula> (commonly sigmoidal) is applied to the weighted sum of inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e102" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e103" xlink:type="simple"/><label>(6)</label></disp-formula>Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e104" xlink:type="simple"/></inline-formula> denotes the synaptic efficacy (weight) of synapse <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e105" xlink:type="simple"/></inline-formula> that projects from neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e106" xlink:type="simple"/></inline-formula> to neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e107" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e108" xlink:type="simple"/></inline-formula> is a bias, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e109" xlink:type="simple"/></inline-formula> denotes some noise signal. We assume that a reward signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e110" xlink:type="simple"/></inline-formula> indicates the amount of reward that the system receives at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e111" xlink:type="simple"/></inline-formula>. Good actions will be rewarded, which will lead to weight changes that in turn make such actions more probable. Reinforcement learning demands exploration of the agent, i.e., the agent has to explore new actions. Thus, any neural system that is subject to reward-based learning needs some kind of stochasticity for exploration. In neuron model (6) exploration is implemented via the noise term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e112" xlink:type="simple"/></inline-formula>. Reward-based learning rules for this model can easily be obtained by changing the weights in the direction of the gradient of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e113" xlink:type="simple"/></inline-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e114" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e115" xlink:type="simple"/></inline-formula> denotes the low-pass filtered version of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e116" xlink:type="simple"/></inline-formula> with an exponential kernel, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e117" xlink:type="simple"/></inline-formula> is a small learning rate. In our simulations we used <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e118" xlink:type="simple"/></inline-formula> for the filtered reward. The update equations for the bias is analogous with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e119" xlink:type="simple"/></inline-formula>.</p>
<p>A single neuron of type (6) turns out to be too weak for some of the control tasks considered in this article. The standard way to increase the expressive power is to use networks of such neurons. The learning rule for the network is then unchanged, each neuron tries to optimize the reward independently from the others <xref ref-type="bibr" rid="pcbi.1000894-Seung1">[40]</xref>, but see <xref ref-type="bibr" rid="pcbi.1000894-Urbanczik1">[41]</xref>. It can be shown that such a greedy strategy still performs gradient ascent on the reward signal. However, the time needed to converge to a good solution is often too long for practical applications as shown in <xref ref-type="sec" rid="s3">Results</xref>. We therefore propose a learning rule that is based on a more complex neuron model with nonlinear dendritic interactions within neurons <xref ref-type="bibr" rid="pcbi.1000894-Poirazi1">[42]</xref> and the possibility to adapt dendritic conductance properties <xref ref-type="bibr" rid="pcbi.1000894-Losonczy1">[43]</xref>.</p>
<p>In this model, the total somatic input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e120" xlink:type="simple"/></inline-formula> to neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e121" xlink:type="simple"/></inline-formula> is modeled as a noisy weighted linear sum of signals from dendritic branches<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e122" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e123" xlink:type="simple"/></inline-formula> describes the coupling strength between branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e124" xlink:type="simple"/></inline-formula> and the soma and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e125" xlink:type="simple"/></inline-formula> is a bias. Again, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e126" xlink:type="simple"/></inline-formula> models exploratory noise. At each time step, an independent sample from the zero mean distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e127" xlink:type="simple"/></inline-formula> is drawn as the exploratory signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e128" xlink:type="simple"/></inline-formula>. In our simulations, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e129" xlink:type="simple"/></inline-formula> is the uniform distribution over the interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e130" xlink:type="simple"/></inline-formula>. The output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e131" xlink:type="simple"/></inline-formula> of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e132" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e133" xlink:type="simple"/></inline-formula> is modeled as a nonlinear function of the total somatic input:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e134" xlink:type="simple"/><label>(9)</label></disp-formula>Each dendritic branches <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e135" xlink:type="simple"/></inline-formula> itself sums weighted synaptic inputs followed by a sigmoidal nonlinearity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e136" xlink:type="simple"/></inline-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e137" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e138" xlink:type="simple"/></inline-formula> denotes the synaptic weight from input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e139" xlink:type="simple"/></inline-formula> to the dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e140" xlink:type="simple"/></inline-formula> of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e141" xlink:type="simple"/></inline-formula>. Update equations that perform gradient ascent on a reward-signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e142" xlink:type="simple"/></inline-formula> are derived in Supporting <xref ref-type="supplementary-material" rid="pcbi.1000894.s004">Text S2</xref>. The derived update rules for the parameters are<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e143" xlink:type="simple"/><label>(11)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e144" xlink:type="simple"/><label>(12)</label></disp-formula>where and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e145" xlink:type="simple"/></inline-formula> are small learning rates. The update rules can be extended to use eligibility traces that collect the information about recent pre-and postsynaptic states at the synapse in a single scalar value. In this way, previous states of the synapse can be incorporated in the weight change at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e146" xlink:type="simple"/></inline-formula>, which is driven by the momentary reward signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e147" xlink:type="simple"/></inline-formula>. In this article however, we rely on the update rules (11) and (12) without eligibility traces. See <xref ref-type="bibr" rid="pcbi.1000894-Urbanczik1">[41]</xref> for an alternative rule of similar flavor.</p>
<p>In our simulations, we needed two control variables, one to control the speed <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e148" xlink:type="simple"/></inline-formula> of the agent and one for its angular velocity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e149" xlink:type="simple"/></inline-formula>. Each control variable was computed by a single neuron of this type where each neuron had <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e150" xlink:type="simple"/></inline-formula> branches. The nonlinearity in the branches was the tangens hyperbolicus function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e151" xlink:type="simple"/></inline-formula>. Also a logistic sigmoidal was tested which is a scaled version of the tangens hyperbolicus to the image set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e152" xlink:type="simple"/></inline-formula>. Results were similar with a slight increase in learning time. The nonlinearity at the soma was the tangens hyperbolicus for the angular velocity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e153" xlink:type="simple"/></inline-formula> and a logistic sigmoid <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e154" xlink:type="simple"/></inline-formula> for the speed <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e155" xlink:type="simple"/></inline-formula>. The noise signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e156" xlink:type="simple"/></inline-formula> was drawn independently for each neuron and at each time step from a uniform distribution in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e157" xlink:type="simple"/></inline-formula>. Detailed parameter settings used for the simulations can be found in Supporting <xref ref-type="supplementary-material" rid="pcbi.1000894.s003">Text S1</xref>.</p>
</sec><sec id="s2e">
<title>Tasks</title>
<p>We tested the system on two different control tasks: a task similar to the Morris water-maze task and a variable-targets task.</p>
<sec id="s2e1">
<title>Morris water maze task</title>
<p>In the experimental setup of a Morris water maze task <xref ref-type="bibr" rid="pcbi.1000894-Morris1">[25]</xref>, a rat swims in a milky liquid with a hidden platform underneath the liquid surface. Because the rodent tries to avoid swimming in the liquid, it searches for the platform. This task has been modeled several times <xref ref-type="bibr" rid="pcbi.1000894-Foster1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Sheynikhovich1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Vasilaki1">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Potjans1">[45]</xref>.</p>
<p>In order to be able to compare the results to previous studies, we modeled the Morris water maze task in our standard setup in the following way: We used only a single fish type and a fixed target position at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e158" xlink:type="simple"/></inline-formula>. Only the fish but not the target was visible in the visual input to the learning system. There was only one control signal which controlled the direction of the next movement. At each time step, the fish was moved by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e159" xlink:type="simple"/></inline-formula> length units in the direction given by the controller. The position of the fish was hard-bounded by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e160" xlink:type="simple"/></inline-formula> from below and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e161" xlink:type="simple"/></inline-formula> from above after each update such that it stayed within <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e162" xlink:type="simple"/></inline-formula>. In this setup, the fish was always oriented in same direction (facing to the right), i. e., the fish was not rotated in the visual input. The target was reached by the agent if it was within a radius of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e163" xlink:type="simple"/></inline-formula> of the target position.</p>
<p>The reward signal was defined such that reaching the target at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e164" xlink:type="simple"/></inline-formula> resulted in a positive reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e165" xlink:type="simple"/></inline-formula>, hitting the wall at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e166" xlink:type="simple"/></inline-formula> resulted in a negative reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e167" xlink:type="simple"/></inline-formula>, and the reward signal was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e168" xlink:type="simple"/></inline-formula> at other times. Hence this is a setup with sparse rewards. An episode ended when the target was reached or after <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e169" xlink:type="simple"/></inline-formula> time-steps have evolved (this is consistent with <xref ref-type="bibr" rid="pcbi.1000894-Vasilaki1">[44]</xref> where a simulation time step was interpreted as a 200 msec time interval).</p>
</sec><sec id="s2e2">
<title>Variable-targets task</title>
<p>In order to explore the general applicability of the system we investigated a more demanding task with several objects in the visual input and two different types of fish of varying orientation.</p>
<p>In this task, the state of the agent at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e170" xlink:type="simple"/></inline-formula> was defined by its identity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e171" xlink:type="simple"/></inline-formula> (this corresponds to two types of fish, each with a unique visual appearance in the visual input stream), its position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e172" xlink:type="simple"/></inline-formula>, and its orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e173" xlink:type="simple"/></inline-formula>. Additionally to the agent, there were two objects in the universe, one of them acting as the target and the other as a distractor. One object had appeared as a “cross” in the visual scene and the other object as a “disk” (see <xref ref-type="fig" rid="pcbi-1000894-g002">Figure 2</xref>). The state of object <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e174" xlink:type="simple"/></inline-formula> was defined by its position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e175" xlink:type="simple"/></inline-formula>. The current state of the universe at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e176" xlink:type="simple"/></inline-formula> was given by the collection of these variables.</p>
<p>The output of the learning system were two control variables to control the agent in the environment, a speed signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e177" xlink:type="simple"/></inline-formula> and a signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e178" xlink:type="simple"/></inline-formula> for angular velocity. These signals were used to update the orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e179" xlink:type="simple"/></inline-formula> and position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e180" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e181" xlink:type="simple"/></inline-formula> of the fish<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e182" xlink:type="simple"/><label>(13)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e183" xlink:type="simple"/><label>(14)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e184" xlink:type="simple"/><label>(15)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e185" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e186" xlink:type="simple"/></inline-formula> are scaling constants. When the agent hit the boundaries of the environment (i.e., when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e187" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e188" xlink:type="simple"/></inline-formula> were below <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e189" xlink:type="simple"/></inline-formula> or above <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e190" xlink:type="simple"/></inline-formula>), the movement was mirrored. For each training episode, object positions, fish orientation, and fish identity were initially chosen randomly from the uniform distribution in their range. However, when an object was less than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e191" xlink:type="simple"/></inline-formula> away from the other object or the fish (which likely produced a visual overlap), a new initial state was drawn. The object positions were then fixed. Each fish identity had a different object serving as the target, such that the fish of type A was associated with the “cross” whereas fish-type B was associated with the “disk”. The task was to navigate the fish to the target object for the given fish identity. The current episode ended when the fish reached the target location within some predefined radius (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e192" xlink:type="simple"/></inline-formula>) or after a maximum of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e193" xlink:type="simple"/></inline-formula> time steps were exceeded). Although the other object did not influence the outcome of the task, it was still visible as a distracting stimulus.</p>
<p>The reward signal indicated whether the last action was successful in bringing the agent closer to the target:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e194" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e195" xlink:type="simple"/></inline-formula> denotes the Euclidean norm and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e196" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e197" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e198" xlink:type="simple"/></inline-formula> otherwise. This is a relatively informative reward signal (see <xref ref-type="sec" rid="s4">Discussion</xref>).</p>
</sec></sec></sec><sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Morris water maze task</title>
<p>We implemented this task with our learning system where the decision circuit consisted of the Q-learning circuit described above. In this task, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e199" xlink:type="simple"/></inline-formula> slowest components as extracted by the hierarchical SFA network were used by the subsequent decision network. The results of training are shown in <xref ref-type="fig" rid="pcbi-1000894-g005">Figure 5</xref>. The performance of the system was measured by the time needed to reach the target (escape latency). The system learns quite fast with convergence after about 40 training episodes. The results are comparable to previously obtained simulation results <xref ref-type="bibr" rid="pcbi.1000894-Foster1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Sheynikhovich1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Vasilaki1">[44]</xref> that were based on a state representation by neurons with place-cell-like behavior. <xref ref-type="fig" rid="pcbi-1000894-g005">Figure 5B</xref> shows the direction the system chooses with high probability at various positions in the water maze (navigation map) after training. Using only the 16 slowest SFA components for reinforcement learning, the system has rapidly learned a near-optimal strategy in this task. This result shows that the use of SFA as preprocessing makes it possible to apply reinforcement learning to raw image data in the Morris water maze task.</p>
<fig id="pcbi-1000894-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000894.g005</object-id><label>Figure 5</label><caption>
<title>Performance of the learning system in the Morris water maze task with Q-learning.</title>
<p>A) Mean escape latency (in simulation time steps) as a function of learning episodes for 10 independent sets of episodes (full thick line). The thin dashed line indicates the standard deviation. B) The navigation map of the system after training. The vectors indicate the movement directions the system would most likely choose at the given positions in the water maze. An episode ended successfully when the center of the fish reached the area indicated by the gray disk.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.g005" xlink:type="simple"/></fig></sec><sec id="s3b">
<title>Variable-targets task</title>
<p>The Morris water maze task is relatively simple and does not provide rich visual input. We therefore tested the learning system on the variable-targets task described above, a control task where two types of fish navigate in a 2D environment. In the environment, two object positions were marked by a cross and a disk, and these positions were different in each learning episode. A target object was defined for each fish type and the task was to navigate the current fish to its target by controlling the forward speed and the change in movement direction (angular velocity). The control of angular velocity, the arbitrary target position, and the dependence of the target object on the fish identity complicates the control task such that the Q-learning algorithm used in the water-maze task as well as a simple linear decision neuron like the one of equation (6) would not succeed in this task. We therefore trained the leaning system with the more powerful policy gradient algorithm described above on the slowest 32 components extracted by the hierarchical SFA network.</p>
<p>In order to compute the SFA output fast, we had to perform the training of the control network in batches of 100 parallel traces in this task (i.e., 100 training episodes with different initial conditions are simulated in parallel with a given weight vector. After the simulation of a single time step in all 100 episodes, weight changes over these 100 traces are averaged and implemented. Then, the next time step in each of the 100 traces is simulated and weights are updated). When the agent in one of the traces arrived at the target, a new learning episode was initiated in this trace while other traces simply continued. As will be shown below, the training in batches has no significant influence on the learning dynamics.</p>
<p>Results are shown in <xref ref-type="fig" rid="pcbi-1000894-g006">Figure 6A,B</xref>. The reward converges to a mean reward above <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e200" xlink:type="simple"/></inline-formula> which means that the agent nearly always takes the best step towards the target despite the high amount of noise in the control neurons. <xref ref-type="fig" rid="pcbi-1000894-g007">Figure 7</xref> shows that the trajectories after training were very good. Interestingly, the network does not learn the optimal strategy with respect to the forward speed output. Although it would be beneficial to reduce the forward speed when the agent is directed away from the target, first rotate the agent, and only then move forward, the output of the speed neuron is nearly always close to the maximum value. A possible reason for this is that the agent is directed towards the target most of the time. Thus, the gain in reward is very small and a relatively small fraction of training examples demands low speed.</p>
<fig id="pcbi-1000894-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000894.g006</object-id><label>Figure 6</label><caption>
<title>Rewards and escape latencies during training of the control task with target and distractor.</title>
<p>A) Evolution of reward during training. A simulation step for all 100 parallel traces corresponds to 100 time-steps at the x-axis. The plotted values are averages over consecutive 20,000 time steps. B) Evolution of escape latencies (measured in time steps) during training. The number of episodes on the x-axis is the number of completed traces. The plotted values are averages over 1,200 consecutive episodes. C,D) Same as panels A and B, but learning was performed on a highly condensed and precise state-encoding instead of the SFA network output. Shown is the performance for learning on 100 parallel traces (black, full line) and without parallel traces (gray, dashed line). Convergence is comparable to learning on SFA outputs. The results without parallel traces are very similar to the results with parallel traces.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.g006" xlink:type="simple"/></fig><fig id="pcbi-1000894-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000894.g007</object-id><label>Figure 7</label><caption>
<title>Three representative trajectories after training of the control task with target and distractor.</title>
<p>Each row summarizes one representative learning trial. Shown is the visual input at start position (left column), the visual input when the goal was reached (middle column), and the whole trajectory (right column). In the trajectory, fish positions (small black discs), target region (large circle), and distractor location (gray rectangle) are shown.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.g007" xlink:type="simple"/></fig>
<p>We compared the results to a learning system with the same control circuit, but with SFA replaced by a vector which directly encoded the state-space in a straight-forward way. For this task with two fish identities and two objects, we encoded the state-space by a vector<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e201" xlink:type="simple"/><label>(17)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e202" xlink:type="simple"/></inline-formula> is the position of the agent, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e203" xlink:type="simple"/></inline-formula> is its orientation, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e204" xlink:type="simple"/></inline-formula> is its identity, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e205" xlink:type="simple"/></inline-formula> is the position of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e206" xlink:type="simple"/></inline-formula> object. <xref ref-type="fig" rid="pcbi-1000894-g006">Figure 6C,D</xref> shows the results when the control network was trained with identical parameters but with this state-vector as input. The Performance with the SFA network is comparable to the performance of the system with a highly informative and precise state encoding.</p>
<p>For efficiency reasons, we had to perform the training of the control network in batches of 100 traces (see above). Because no SFA is needed in the setup with the direct state-vector as input, we can compare learning performance of the control network to performance without batches. The result is shown in in <xref ref-type="fig" rid="pcbi-1000894-g006">Figure 6C,D</xref> (gray dashed lines). The use of small batches does not influence the learning dynamics significantly.</p>
<p>In the environment considered, movement is mirrored if the agent hits a boundary. Since this helps to avoid getting stuck in corners we performed control experiments where the movement in the direction of the boundary is simply cut off but no reflection happens (i.e., the dynamics of the position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e207" xlink:type="simple"/></inline-formula> of the fish is given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e208" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e209" xlink:type="simple"/></inline-formula>, compare to equations (14),(15)). Results are shown in <xref ref-type="supplementary-material" rid="pcbi.1000894.s001">Figure S1</xref>. As expected, the system starts with lower performance and convergence takes about twice as long compared to the environment with mirrored movements at boundaries. Interestingly, in this slightly more demanding environment, the SFA network is converging faster than the system with a highly informative and precise state encoding.</p>
<p>In another series of experiments we tested how the performance depends on the number of outputs from the SFA network that are used as input for the reinforcement learning. Since the outputs of the SFA network are naturally ordered by their slowness one can pick only the first <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e210" xlink:type="simple"/></inline-formula> outputs and train the reinforcement learning network on those. For the variable-targets task we tested the performance for 16, 22, 28, 32, and 64 outputs. For 16 outputs the average reward value always stayed below <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e211" xlink:type="simple"/></inline-formula> and rose much slower than in the case of 32 outputs. For 28 outputs the performance was already very close to that of the 32 outputs. Going from 32 outputs to 64 did not change the average reward, but in the case of 64 outputs the trajectories of the agent occasionally showed some errors (e.g., the agent initially chose a wrong direction and took therefore longer to reach the target).</p>
<p>We compared performance of the system to a system where the control network is a two-layer feed-forward network of simpler neurons without dendritic branches, see Equation (6). We used two networks with identical architecture, one for each control variable. Each network consisted of 50 neurons in the first layer connected to one output neuron (increasing the number of neurons in the first layer to 100 did not change the results). Every neuron in the first layer received input from all SFA outputs. The learning rates of all neurons were identical. See Supporting <xref ref-type="supplementary-material" rid="pcbi.1000894.s003">Text S1</xref> for details on parameters and their determination. Results are shown in <xref ref-type="supplementary-material" rid="pcbi.1000894.s002">Figure S2</xref>. The network of simple neurons can solve the problem in principle, but it converges much slower.</p>
<p>We also compared performance of the system with SFA to systems where the dimensionality of the visual input was reduced by PCA. In one experiment the SFA nodes in the hierarchical network were simply replaced by PCA nodes. We then used 64 outputs from the network for the standard reinforcement learning training. As shown in <xref ref-type="fig" rid="pcbi-1000894-g008">Figure 8</xref> the control network was hardly able to learn the control task. This is also evident in the test trajectories, which generally look erratic.</p>
<fig id="pcbi-1000894-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000894.g008</object-id><label>Figure 8</label><caption>
<title>Performance of a PCA based hierarchical network.</title>
<p>Rewards (A) and escape latencies (B) in the variable-targets control experiment with a PCA based hierarchical network. The control network is not able to learn the task based on this state representation. Note the larger scaling factor for the time-axis in panel A.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.g008" xlink:type="simple"/></fig>
<p>In another experiment we used PCA on the whole images. Because of the high dimensionality we first had to downsample the image data by averaging over two by two pixels (reducing the dimensionality by a factor of four) before using linear PCA. The performance was very similar to the hierarchical PCA experiment (the average reward hovered below <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e212" xlink:type="simple"/></inline-formula>). A direct analysis of the PCA output with linear regression <xref ref-type="bibr" rid="pcbi.1000894-Franzius2">[15]</xref> indicates that except for the agent identity, no important features such as position of the agent or the targets can be extracted in a linear way from the reduced state representation. For hierarchical SFA, such an extraction is often possible <xref ref-type="bibr" rid="pcbi.1000894-Franzius2">[15]</xref>. This hints at the possibility that the state representation given by PCA cannot be exploited by the control network because the implicit encoding of relevant variables is either too complex or too much important information has been discarded.</p>
</sec></sec><sec id="s4">
<title>Discussion</title>
<p>Several theoretical studies have investigated biologically plausible reward-based learning rules <xref ref-type="bibr" rid="pcbi.1000894-Mazzoni1">[46]</xref>–<xref ref-type="bibr" rid="pcbi.1000894-Legenstein2">[55]</xref>. On the synaptic level, such rules are commonly of the reward-modulated Hebbian type, also called three-factor rules. In traditional Hebbian learning rules, changes of synaptic plasticity at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e213" xlink:type="simple"/></inline-formula> are based on the history of the presynaptic and the postsynaptic activity, such that the weight change <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e214" xlink:type="simple"/></inline-formula> of a synapse from a presynaptic neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e215" xlink:type="simple"/></inline-formula> to a postsynaptic neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e216" xlink:type="simple"/></inline-formula> is the product between some function of the presynaptic activity history and some function of the postsynaptic activity history. A third signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000894.e217" xlink:type="simple"/></inline-formula> that models the local concentration of some neuromodulator which in turn signals some reward, is in many models modulating these Hebbian updates. Such update rules are either purely phenomenological <xref ref-type="bibr" rid="pcbi.1000894-Izhikevich1">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Legenstein2">[55]</xref> or derived from a reward-maximization principle <xref ref-type="bibr" rid="pcbi.1000894-Baxter1">[47]</xref>–<xref ref-type="bibr" rid="pcbi.1000894-Florian1">[51]</xref>. From the viewpoint of classical reinforcement learning, the latter approach is related to policy-gradient methods. Since the learning algorithms in these previous works are based on simple neuron models, they are too weak for the variable-targets task considered in this article. The policy-gradient method used in this article extends the classical single-neuron based policy-gradient approach in the sense that it is based on a more expressive neuron model with nonlinear branches. In this model, both, synaptic weights and branch strengths are adapted through learning. Our approach is motivated by recent experimental findings where it has been shown that not only synaptic efficacies but also the strengths of individual dendritic branches are plastic <xref ref-type="bibr" rid="pcbi.1000894-Losonczy1">[43]</xref>. Furthermore, it was shown that this type of plasticity is dependent on neuromodulatory signals. Our results (compare <xref ref-type="fig" rid="pcbi-1000894-g006">Figure 6</xref> to <xref ref-type="supplementary-material" rid="pcbi.1000894.s002">Figure S2</xref>) indicate that the neuron model with nonlinear branches can be trained much faster than networks of point-neuron models. This hints at a possible role of nonlinear branches in the context of reward-based learning.</p>
<p>The Morris water-maze task has been modeled before. In <xref ref-type="bibr" rid="pcbi.1000894-Potjans1">[45]</xref>, a network of spiking neurons was trained on a relatively small discrete state-space that explicitly coded the current position of the agent on a two-dimensional grid. The authors used a neural implementation of temporal difference learning. In contrast to the algorithms used in this article, their approach demands a discrete state space. This algorithm is therefore not directly applicable to the continuous state-space representation that is achieved through SFA. In <xref ref-type="bibr" rid="pcbi.1000894-Foster1">[34]</xref> and <xref ref-type="bibr" rid="pcbi.1000894-Vasilaki1">[44]</xref> the input to the reinforcement learning network was explicitly coded similar to the response of hippocampal place-cells. In <xref ref-type="bibr" rid="pcbi.1000894-Sheynikhovich1">[35]</xref>, the state-representation was also governed by place-cell-like response that were learned from the input data. This approach was however tailored to the problem at hand, whereas we claim that SFA can be used in a much broader application domain since it is not restricted to visual input. Furthermore, in this article SFA was not only used to extract position of an agent in space but also for position of other objects, for object identity, and for orientation. We thus claim that the learning architecture presented is very general only relying on temporal continuity of important state variables.</p>
<p>Although the variable-targets task considered above is quite demanding, the learning system gets immediate feedback of its performance via the reward signal defined by equation (16). By postulating such a reward signal one has to assume that some system can evaluate that “getting closer to the target” is good. Such prior knowledge could have been acquired by earlier learning or it could be encoded genetically. An example of a learning system that probably involves such a circuitry (the critique) is the song-learning system in the songbird. In this system, it is believed that a critique can evaluate similarity between the own song and a memory copy of a tutor song <xref ref-type="bibr" rid="pcbi.1000894-Troyer1">[56]</xref>. However, there is no evidence that such higher-level critique is involved for example in navigational learning of rodents. Instead, it is more natural to assume that an internal reward signal is produced for example when some food-reward is delivered to the animal. One experimental setup with sparse rewards is the Morris water maze task <xref ref-type="bibr" rid="pcbi.1000894-Morris1">[25]</xref> considered above. In principle, this sparse reward situation could also be learned if the learning rules (11), (12) are amended with eligibility traces <xref ref-type="bibr" rid="pcbi.1000894-Xie1">[48]</xref>. However, the learning would probably take much longer.</p>
<p>Given the high-dimensional visual encoding of the state-space accessible to the learning system, it is practically impossible that any direct reinforcement learning approach is able to solve the variable-targets task directly on the visually-induced state-space. Additionally, in order to scale down the visual input to viable sizes, a hierarchical approach is most promising. Here, hierarchical SFA is one of the few approaches that have been proven to work well. Linear unsupervised techniques such as principal component analysis (PCA) or independent component analysis (ICA) are less suited to be applied hierarchically. To understand the results, it is important to note that SFA is quite different from PCA or other more elaborate dimensionality reduction techniques <xref ref-type="bibr" rid="pcbi.1000894-Antoulas1">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Tenenbaum1">[58]</xref>. Dimensionality reduction in general tries to produce a faithful low-dimensional representation of the data. The aim of SFA is not to produce a faithful representation in the sense that the original data can be reconstructed with small error. Instead, it tries to extract slow features by taking the temporal dimension of the data into account (this dimension is not exploited by PCA) and disregards many details of the input. Although it is in general not guaranteed that slowly varying features are also important for the control task, slowly varying features such as object identities and positions are important in many tasks. In fact, the removal of details may underlie the success of the generic architecture, since it allows the subsequent decision circuit to concentrate on a few important features of the input. This may also explain the failure of PCA. The encoding of the visual input produced by PCA can be used to reconstruct a “blurred” version of the input image. However, it is very hard to extract from this information the relevant state variables such as object identity or position. But this information can easily be extracted from the SFA output, see <xref ref-type="bibr" rid="pcbi.1000894-Franzius2">[15]</xref>.</p>
<p>We compared the preprocessing with SFA to PCA preprocessing but not to more elaborate techniques <xref ref-type="bibr" rid="pcbi.1000894-Antoulas1">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Tenenbaum1">[58]</xref> since the focus of this paper is on simple techniques for which some biological evidence exists. Another candidate for sensory preprocessing instead of SFA is ICA. However, ICA does not provide a natural ordering of extracted components. It is thus not clear which components to disregard in order to reduce the dimensionality of the sensory input stream. One interesting possibility would be to order the ICA components by kurtosis in order to extract those components which are most non-Gaussian. Another interesting possibility not pursued in this paper would be to sparsify the SFA output by ICA. This has led to place-cell like behavior in <xref ref-type="bibr" rid="pcbi.1000894-Franzius1">[14]</xref> and might be beneficial for subsequent reward-based learning. Information bottleneck optimization (IB) is another candidate learning mechanism for cortical feature extraction. However, IB is not unsupervised, it needs a relevance signal. It would be interesting to investigate whether a useful relevance signal could be constructed for example from the reward signal. Finally, the problem of state space reduction has also been considered in the reinforcement learning literature. There, the main approach is either to reduce the size of a discrete state space or to discretize a continuous state-space <xref ref-type="bibr" rid="pcbi.1000894-AndrewMoore1">[59]</xref>,<xref ref-type="bibr" rid="pcbi.1000894-Munos1">[60]</xref>. In contrast, SFA preserves the continuous nature of the state-space by representing it with a few highly informative continuous variables. This circumvents many problems of state-space discretization such as the question of state-space granularity. Thus, there are multiple benefits of SFA in the problem studied: It can be trained in a fully unsupervised manner (as compared to IB). By taking the temporal dimension into account, it is able to compress the state-space significantly without the need to discretize the continuous state-space (as compared to <xref ref-type="bibr" rid="pcbi.1000894-AndrewMoore1">[59]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Munos1">[60]</xref>). It provides a highly abstract representation that can be utilized by simple subsequent reward-based learning (compare to the discussion of PCA). The possibility to apply SFA in a hierarchical fashion renders it computationally efficient even on high-dimensional input streams, both in conventional computers and in biological neural circuits where it allows for mainly local communication and thus avoids extensive connectivity <xref ref-type="bibr" rid="pcbi.1000894-Legenstein1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Chklovskii1">[32]</xref>. The natural ordering of features based on their slowness implies a simple criterion on the basis of which information can be discarded in each node of the hierarchical network (compare to ICA), resulting in a significant reduction of information that has to be processed by higher-level circuits. Finally, SFA is relatively simple, its complexity is comparable to PCA and it is considerably simpler than other approaches for state-space reduction <xref ref-type="bibr" rid="pcbi.1000894-Antoulas1">[57]</xref>–<xref ref-type="bibr" rid="pcbi.1000894-Munos1">[60]</xref>. Accordingly, biologically plausible implementations of SFA exist <xref ref-type="bibr" rid="pcbi.1000894-Hashimoto1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1000894-Sprekeler1">[29]</xref>. Together with the fact that experimental evidence for slowness learning exists in the visual system <xref ref-type="bibr" rid="pcbi.1000894-Li1">[23]</xref>, this renders SFA an important candidate mechanism for unsupervised feature extraction in sensory cortex.</p>
<p>In this article, we provided a proof of concept that a learning system with an unsupervised preprocessing and subsequent simple biologically realistic reward-based learning can learn quite complex control tasks on high-dimension visual input streams without the need for hand-design of a reduced state-space. We applied the proposed learning system to two control tasks. In the Morris water maze task, we showed that the system can find an optimal strategy in a number of learning episodes that is comparable to experimental results with rats <xref ref-type="bibr" rid="pcbi.1000894-Morris1">[25]</xref>. The application of the learning system to the variable targets task shows that also much more complex tasks with rich visual inputs can be solved by the system. We propose in this article that slowness-learning in combination with reward-based learning may provide a generic (although not exclusive) principle for behavioral learning in the brain. This hypothesis predicts that slowness learning should be a major unsupervised learning mechanism in sensory cortices of any modality. Currently, such evidence exists for the visual pathway only <xref ref-type="bibr" rid="pcbi.1000894-Li1">[23]</xref>. We showed that learning performance of the system in this task is comparable to a system where the state-representation extracted by SFA is replaced by a highly compressed and precise hand-crafted state-space. Finally, our simulation results suggest that performance of the system is quite insensitive to the number of SFA components that is chosen for further processing by the reinforcement learning network as long as enough informative features are chosen.</p>
<p>Altogether this study provides, on the one hand, further support that slowness learning could be one important (but not necessarily exclusive) unsupervised learning principle utilized in the brain to form efficient state representations of the environment. On the other hand, this work shows that autonomous learning of state-representations with SFA should be further pursued in the search for autonomous learning systems that do not - or much less - have to rely on expensive tuning by human experts.</p>
</sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1000894.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.s001" xlink:type="simple"><label>Figure S1</label><caption>
<p>Rewards and escape latencies during training of the control task with target and distractor without mirrored movements at boundaries. A) Evolution of reward during training. A simulation step for all 100 parallel traces corresponds to 100 time-steps at the x-axis. The plotted values are averages over consecutive 50,000 time steps. B) Evolution of escape latencies (measured in time steps) during training. The number of episodes on the x-axis is the number of completed traces. The plotted values are averages over 3,000 consecutive episodes. C,D) Same as panels A and B, but learning was performed on a highly condensed and precise state-encoding instead of the SFA network output. Shown is the performance for learning on 100 parallel traces (black, full line) and without parallel traces (gray, dashed line). Convergence is slower compared to learning on SFA outputs.</p>
<p>(0.02 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000894.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.s002" xlink:type="simple"><label>Figure S2</label><caption>
<p>Rewards and escape latencies during training of a feed-forward network of simple neurons on the control task with target and distractor. A) Evolution of reward during training. A simulation step for all 100 parallel traces corresponds to 100 time-steps at the x-axis. The plotted values are averages over consecutive 150,000 time steps. B) Evolution of escape latencies (measured in time steps) during training. The number of episodes on the x-axis is the number of completed traces. The plotted values are averages over 8,000 consecutive episodes.</p>
<p>(0.02 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000894.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.s003" xlink:type="simple"><label>Text S1</label><caption>
<p>Detailed parameters for reward-based learning.</p>
<p>(0.02 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000894.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000894.s004" xlink:type="simple"><label>Text S2</label><caption>
<p>Derivation of the policy-gradient update rule.</p>
<p>(0.02 MB PDF)</p>
</caption></supplementary-material></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1000894-Thorndike1"><label>1</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Thorndike</surname><given-names>E</given-names></name>
</person-group>             <year>1911</year>             <source>Animal Intelligence</source>             <publisher-loc>CT</publisher-loc>             <publisher-name>Hafner, Darien</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000894-Bertsekas1"><label>2</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bertsekas</surname><given-names>DP</given-names></name>
<name name-style="western"><surname>Tsitsiklis</surname><given-names>J</given-names></name>
</person-group>             <year>1996</year>             <source>Neuro-Dynamic Programming</source>             <publisher-name>Athena Scientific</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000894-Sutton1"><label>3</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name>
<name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name>
</person-group>             <year>1998</year>             <source>Reinforcement Learning: An Introduction</source>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000894-Schultz1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Montague</surname><given-names>P</given-names></name>
</person-group>             <year>1997</year>             <article-title>A neural substrate of prediction and reward.</article-title>             <source>Science</source>             <volume>275</volume>             <fpage>1593</fpage>             <lpage>9</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Reynolds1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Reynolds</surname><given-names>JN</given-names></name>
<name name-style="western"><surname>Wickens</surname><given-names>JR</given-names></name>
</person-group>             <year>2002</year>             <article-title>Dopamine-dependent plasticity of corticostriatal synapses.</article-title>             <source>Neural Netw</source>             <volume>15</volume>             <fpage>507</fpage>             <lpage>521</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Gerstner1"><label>6</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Kistler</surname><given-names>WM</given-names></name>
</person-group>             <year>2002</year>             <source>Spiking Neuron Models</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000894-Buesing1"><label>7</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Buesing</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>
</person-group>             <year>2008</year>             <article-title>Simplified rules and theoretical analysis for information bottleneck optimization and PCA with spiking neurons.</article-title>             <fpage>193</fpage>             <lpage>200</lpage>             <comment>In: Proc. of NIPS 2007, Advances in Neural Information Processing Systems. MIT Press, volume 20</comment>          </element-citation></ref>
<ref id="pcbi.1000894-Hyvrinen1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Oja</surname><given-names>E</given-names></name>
</person-group>             <year>1996</year>             <article-title>Simple neuron models for independent component analysis.</article-title>             <source>Int J Neural Syst</source>             <volume>7</volume>             <fpage>671</fpage>             <lpage>687</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Hyvrinen2"><label>9</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Karhunen</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Oja</surname><given-names>E</given-names></name>
</person-group>             <year>2001</year>             <source>Independent Component Analysis</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>J. Wiley</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000894-Klampfl1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Klampfl</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Legenstein</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>
</person-group>             <year>2009</year>             <article-title>Spiking neurons can learn to solve information bottleneck problems and to extract independent components.</article-title>             <source>Neural Comput</source>             <volume>21</volume>             <fpage>911</fpage>             <lpage>959</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Tishby1"><label>11</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tishby</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Pereira</surname><given-names>FC</given-names></name>
<name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>
</person-group>             <year>1999</year>             <article-title>The information bottleneck method.</article-title>             <fpage>368</fpage>             <lpage>377</lpage>             <comment>In: Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing</comment>          </element-citation></ref>
<ref id="pcbi.1000894-Wiskott1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name>
</person-group>             <year>2002</year>             <article-title>Slow feature analysis: unsupervised learning of invariances.</article-title>             <source>Neural Comput</source>             <volume>14</volume>             <fpage>715</fpage>             <lpage>770</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Berkes1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name>
</person-group>             <year>2005</year>             <article-title>Slow feature analysis yields a rich repertoire of complex cell properties.</article-title>             <source>J Vision</source>             <volume>5</volume>             <fpage>579</fpage>             <lpage>602</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Franzius1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Franzius</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Sprekeler</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name>
</person-group>             <year>2007</year>             <article-title>Slowness and sparseness lead to place, head-direction, and spatial-view cells.</article-title>             <source>PLoS Comput Biol</source>             <volume>3</volume>             <fpage>e166</fpage>          </element-citation></ref>
<ref id="pcbi.1000894-Franzius2"><label>15</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Franzius</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Wilbert</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name>
</person-group>             <year>2008</year>             <article-title>Invariant object recognition with slow feature analysis.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Kurková</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Neruda</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Koutník</surname><given-names>J</given-names></name>
</person-group>             <fpage>961</fpage>             <lpage>970</lpage>             <comment>Proc. 18th Intl. Conf. on Artificial Neural Networks, ICANN'08, Prague. Springer, volume 5163 of <italic>Lecture Notes in Computer Science</italic></comment>          </element-citation></ref>
<ref id="pcbi.1000894-Fldik1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Földiák</surname><given-names>P</given-names></name>
</person-group>             <year>1991</year>             <article-title>Learning invariance from transformation sequences.</article-title>             <source>Neural Comput</source>             <volume>3</volume>             <fpage>194</fpage>             <lpage>200</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Mitchison1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mitchison</surname><given-names>G</given-names></name>
</person-group>             <year>1991</year>             <article-title>Removing time variation with the anti-Hebbian differential synapse.</article-title>             <source>Neural Comput</source>             <volume>3</volume>             <fpage>312</fpage>             <lpage>320</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Hinton1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name>
</person-group>             <year>1989</year>             <article-title>Connectionist learning procedures.</article-title>             <source>Artif Intell</source>             <volume>40</volume>             <fpage>185</fpage>             <lpage>234</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Becker1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Becker</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name>
</person-group>             <year>1992</year>             <article-title>Self-organizing neural network that discovers surfaces in random-dot stereograms.</article-title>             <source>Nature</source>             <volume>355</volume>             <fpage>161</fpage>             <lpage>163</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Wallis1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wallis</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name>
</person-group>             <year>1997</year>             <article-title>Invariant face and object recognition in the visual system.</article-title>             <source>Prog Neurobiol</source>             <volume>51</volume>             <fpage>167</fpage>             <lpage>194</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Einhuser1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Einhäuser</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Hipp</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Eggert</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Körner</surname><given-names>E</given-names></name>
<name name-style="western"><surname>König</surname><given-names>P</given-names></name>
</person-group>             <year>2005</year>             <article-title>Learning viewpoint invariant object representations using a temporal coherence principle.</article-title>             <source>Biol Cybern</source>             <volume>93</volume>             <fpage>79</fpage>             <lpage>90</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Wyss1"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wyss</surname><given-names>R</given-names></name>
<name name-style="western"><surname>König</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Verschure</surname><given-names>PFMJ</given-names></name>
</person-group>             <year>2006</year>             <article-title>A Model of the Ventral Visual System Based on Temporal Stability and Local Memory.</article-title>             <source>PLoS Biol</source>             <volume>4</volume>          </element-citation></ref>
<ref id="pcbi.1000894-Li1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Li</surname><given-names>N</given-names></name>
<name name-style="western"><surname>DiCarlo</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>Unsupervised natural experience rapidly alters invariant object representation in visual cortex.</article-title>             <source>Science</source>             <volume>321</volume>             <fpage>1502</fpage>             <lpage>1507</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Miyashita1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Miyashita</surname><given-names>Y</given-names></name>
</person-group>             <year>1988</year>             <article-title>Neuronal correlate of visual associative long-term memory in the primate temporal cortex.</article-title>             <source>Nature</source>             <volume>335</volume>             <fpage>817</fpage>             <lpage>820</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Morris1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Morris</surname><given-names>RG</given-names></name>
<name name-style="western"><surname>Garrud</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Rawlins</surname><given-names>JN</given-names></name>
<name name-style="western"><surname>O'Keefe</surname><given-names>J</given-names></name>
</person-group>             <year>1982</year>             <article-title>Place navigation impaired in rats with hippocampal lesions.</article-title>             <source>Nature</source>             <volume>297</volume>             <fpage>681</fpage>             <lpage>683</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Wiskott2"><label>26</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name>
</person-group>             <year>1998</year>             <article-title>Learning invariance manifolds.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Niklasson</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Bodén</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Ziemke</surname><given-names>T</given-names></name>
</person-group>             <source>Proceedings of the 8th International Conference on Artificial Neural Networks, ICANN'98, Skövde</source>             <publisher-loc>London</publisher-loc>             <publisher-name>Springer, Perspectives in Neural Computing</publisher-name>             <fpage>555</fpage>             <lpage>560</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Wiskott3"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name>
</person-group>             <year>2002</year>             <article-title>Slow feature analysis: Unsupervised learning of invariances.</article-title>             <source>Neural Comput</source>             <volume>14</volume>             <fpage>715</fpage>             <lpage>770</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Hashimoto1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hashimoto</surname><given-names>W</given-names></name>
</person-group>             <year>2003</year>             <article-title>Quadratic forms in natural images.</article-title>             <source>Netw Comput Neural Syst</source>             <volume>14</volume>             <fpage>765</fpage>             <lpage>788</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Sprekeler1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sprekeler</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Michaelis</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name>
</person-group>             <year>2007</year>             <article-title>Slowness: An objective for spike-timing-plasticity?</article-title>             <source>PLoS Comput Biol</source>             <volume>3</volume>             <fpage>e112</fpage>          </element-citation></ref>
<ref id="pcbi.1000894-Franzius3"><label>30</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Franzius</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Wilbert</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name>
</person-group>             <year>2010</year>             <article-title>Invariant object recognition and pose estimation with slow feature analysis.</article-title>             <comment>submitted</comment>          </element-citation></ref>
<ref id="pcbi.1000894-Legenstein1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Legenstein</surname><given-names>RA</given-names></name>
<name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>
</person-group>             <year>2005</year>             <article-title>Wire length as a circuit complexity measure.</article-title>             <source>J Comput Syst Sci</source>             <volume>70</volume>             <fpage>53</fpage>             <lpage>72</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Chklovskii1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chklovskii</surname><given-names>DB</given-names></name>
<name name-style="western"><surname>Koulakov</surname><given-names>AA</given-names></name>
</person-group>             <year>2000</year>             <article-title>A wire length minimization approach to ocular dominance patterns in mammalian visual cortex.</article-title>             <source>Physica A</source>             <volume>284</volume>             <fpage>318</fpage>             <lpage>334</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Zito1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zito</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Wilbert</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name>
</person-group>             <year>2008</year>             <article-title>Modular toolkit for Data Processing (MDP): a Python data processing framework.</article-title>             <source>Front Neuroinformatics</source>             <volume>2</volume>          </element-citation></ref>
<ref id="pcbi.1000894-Foster1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Foster</surname><given-names>DJ</given-names></name>
<name name-style="western"><surname>Morris</surname><given-names>RGM</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
</person-group>             <year>2000</year>             <article-title>Models of hippocampally dependent navigation, using the temporal difference learning rule.</article-title>             <source>Hippocampus</source>             <volume>10</volume>             <fpage>1</fpage>             <lpage>16</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Sheynikhovich1"><label>35</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sheynikhovich</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Chavarriaga</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Strösslin</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>
</person-group>             <year>2005</year>             <article-title>Spatial Representation and Navigation in a Bio-inspired Robot.</article-title>             <fpage>245</fpage>             <lpage>264</lpage>             <comment>In: Biomimetic Neural Learning for Intelligent Robots: Intelligent Systems, Cognitive Robotics, and Neuroscience</comment>          </element-citation></ref>
<ref id="pcbi.1000894-Montague1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>J</surname><given-names>ST</given-names></name>
</person-group>             <year>1996</year>             <article-title>A framework for mesencephalic dopamine systems based on predictive Hebbian learning.</article-title>             <source>J Neurosci</source>             <volume>16</volume>             <fpage>1936</fpage>             <lpage>1947</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Schultz2"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>1998</year>             <article-title>Predictive reward signal of dopamine neurons.</article-title>             <source>J Neurophys</source>             <volume>80</volume>             <fpage>1</fpage>             <lpage>27</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Houk1"><label>38</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Houk</surname><given-names>JC</given-names></name>
<name name-style="western"><surname>Adams</surname><given-names>JL</given-names></name>
<name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name>
</person-group>             <year>1995</year>             <article-title>A model of how the basal ganglia generate and use neural signals that predict reinforcement.</article-title>             <source>Models of information processing in the basal ganglia</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>249</fpage>             <lpage>270</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Berns1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Berns</surname><given-names>GS</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name>
</person-group>             <year>1998</year>             <article-title>A computational model of how the basal ganglia produces sequences.</article-title>             <source>J Cognitive Neurosci</source>             <volume>10</volume>             <fpage>108</fpage>             <lpage>121</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Seung1"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seung</surname><given-names>HD</given-names></name>
</person-group>             <year>2003</year>             <article-title>Learning in spiking neural networks by reinforcement of stochastic synaptic transmission.</article-title>             <source>Neuron</source>             <volume>40</volume>             <fpage>1063</fpage>             <lpage>1073</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Urbanczik1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Urbanczik</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Senn</surname><given-names>W</given-names></name>
</person-group>             <year>2009</year>             <article-title>Reinforcement learning in populations of spiking neurons.</article-title>             <source>Nat Neurosci</source>             <volume>12</volume>             <fpage>250</fpage>             <lpage>252</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Poirazi1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Poirazi</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Brannon</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Mel</surname><given-names>BW</given-names></name>
</person-group>             <year>2003</year>             <article-title>Pyramidal neuron as two-layer neural network.</article-title>             <source>Neuron</source>             <volume>37</volume>             <fpage>989</fpage>             <lpage>999</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Losonczy1"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Losonczy</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Makara</surname><given-names>JK</given-names></name>
<name name-style="western"><surname>Magee</surname><given-names>JC</given-names></name>
</person-group>             <year>2008</year>             <article-title>Compartmentalized dendritic plasticity and input feature storage in neurons.</article-title>             <source>Nature</source>             <volume>452</volume>             <fpage>436</fpage>             <lpage>441</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Vasilaki1"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Frémaux</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Urbanczik</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Senn</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>
</person-group>             <year>2009</year>             <article-title>Spike-based reinforcement learning in continuous state and action space: When policy gradient methods fail.</article-title>             <source>PLoS Comput Biol</source>             <volume>5</volume>             <fpage>e1000586</fpage>          </element-citation></ref>
<ref id="pcbi.1000894-Potjans1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Potjans</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Morrison</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name>
</person-group>             <year>2009</year>             <article-title>A spiking neural network model of an actor-critic learning agent.</article-title>             <source>Neural Comp</source>             <volume>21</volume>             <fpage>1</fpage>             <lpage>39</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Mazzoni1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mazzoni</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Andersen</surname><given-names>RA</given-names></name>
<name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name>
</person-group>             <year>1991</year>             <article-title>A more biologically plausible learning rule for neural networks.</article-title>             <source>P Natl Acad Sci USA</source>             <volume>88</volume>             <fpage>4433</fpage>             <lpage>4437</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Baxter1"><label>47</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Baxter</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Bartlett</surname><given-names>PL</given-names></name>
</person-group>             <year>1999</year>             <article-title>Direct gradient-based reinforcement learning: I. gradient estimation algorithms.</article-title>             <comment>Technical report, Research School of Information Sciences and Engineering, Australian National University</comment>          </element-citation></ref>
<ref id="pcbi.1000894-Xie1"><label>48</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Xie</surname><given-names>X</given-names></name>
<name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name>
</person-group>             <year>2004</year>             <article-title>Learning in neural networks by reinforcement of irregular spiking.</article-title>             <source>Phys Rev E</source>             <volume>69</volume>          </element-citation></ref>
<ref id="pcbi.1000894-Pfister1"><label>49</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pfister</surname><given-names>JP</given-names></name>
<name name-style="western"><surname>Toyoizumi</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Barber</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>
</person-group>             <year>2006</year>             <article-title>Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning.</article-title>             <source>Neural Comput</source>             <volume>18</volume>             <fpage>1318</fpage>             <lpage>1348</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Fiete1"><label>50</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fiete</surname><given-names>IR</given-names></name>
<name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name>
</person-group>             <year>2006</year>             <article-title>Gradient learning in spiking neural networks by dynamic perturbation of conductances.</article-title>             <source>Phys Rev Lett</source>             <volume>97</volume>             <fpage>048104-1</fpage>             <lpage>048104-4</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Florian1"><label>51</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Florian</surname><given-names>RV</given-names></name>
</person-group>             <year>2007</year>             <article-title>Reinforcement learning through modulation of spike-timing-dependent synaptic plasticity.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>1468</fpage>             <lpage>1502</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Farries1"><label>52</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Farries</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Fairhall</surname><given-names>AL</given-names></name>
</person-group>             <year>2007</year>             <article-title>Reinforcement learning with modulated spike timing-dependent synaptic plasticity.</article-title>             <source>J Neurophysiol</source>             <volume>98</volume>             <fpage>3648</fpage>             <lpage>3665</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Izhikevich1"><label>53</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Izhikevich</surname><given-names>EM</given-names></name>
</person-group>             <year>2007</year>             <article-title>Solving the distal reward problem through linkage of STDP and dopamine signaling.</article-title>             <source>Cereb Cortex</source>             <volume>17</volume>             <fpage>2443</fpage>             <lpage>2452</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Baras1"><label>54</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Baras</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Meir</surname><given-names>R</given-names></name>
</person-group>             <year>2007</year>             <article-title>Reinforcement learning, spike-time-dependent plasticity, and the BCM rule.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>2245</fpage>             <lpage>2279</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Legenstein2"><label>55</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Legenstein</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Pecevski</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>
</person-group>             <year>2008</year>             <article-title>A learning theory for reward-modulated spike-timing-dependent plasticity with application to biofeedback.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <fpage>1</fpage>             <lpage>27</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Troyer1"><label>56</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Troyer</surname><given-names>TW</given-names></name>
<name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name>
</person-group>             <year>2000</year>             <article-title>An associational model of birdsong sensorimotor learning ii. temporal hierarchies and the learning of song sequence.</article-title>             <source>J Neurophysiol</source>             <volume>84</volume>             <fpage>1224</fpage>             <lpage>1239</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Antoulas1"><label>57</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Antoulas</surname><given-names>AC</given-names></name>
<name name-style="western"><surname>Sorensen</surname><given-names>DC</given-names></name>
</person-group>             <year>2001</year>             <article-title>Approximation of large-scale dynamical systems: An overview.</article-title>             <source>Int J Appl Math Comp</source>             <volume>11</volume>             <fpage>1093</fpage>             <lpage>1121</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-Tenenbaum1"><label>58</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name>
<name name-style="western"><surname>de Silva</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Langford</surname><given-names>JC</given-names></name>
</person-group>             <year>2000</year>             <article-title>A global geometric framework for nonlinear dimensionality reduction.</article-title>             <source>Science</source>             <volume>290</volume>             <fpage>2319</fpage>             <lpage>2323</lpage>          </element-citation></ref>
<ref id="pcbi.1000894-AndrewMoore1"><label>59</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Andrew Moore</surname><given-names>CA</given-names></name>
</person-group>             <year>1995</year>             <article-title>The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces.</article-title>             <source>Mach Learn</source>             <volume>21</volume>          </element-citation></ref>
<ref id="pcbi.1000894-Munos1"><label>60</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Munos</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Moore</surname><given-names>A</given-names></name>
</person-group>             <year>2002</year>             <article-title>Variable resolution discretization in optimal control.</article-title>             <source>Mach Learn</source>             <volume>49</volume>             <fpage>291</fpage>             <lpage>323</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>