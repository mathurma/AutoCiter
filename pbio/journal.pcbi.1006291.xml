<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-00445</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006291</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject><subj-group><subject>Ganglion cells</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject><subj-group><subject>Ganglion cells</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Ocular system</subject><subj-group><subject>Ocular anatomy</subject><subj-group><subject>Retina</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Ocular system</subject><subj-group><subject>Ocular anatomy</subject><subj-group><subject>Retina</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Neuroanatomy</subject><subj-group><subject>Neural pathways</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Neuroanatomy</subject><subj-group><subject>Neural pathways</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroanatomy</subject><subj-group><subject>Neural pathways</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Algebra</subject><subj-group><subject>Linear algebra</subject><subj-group><subject>Eigenvectors</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>White noise</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject><subj-group><subject>Afferent neurons</subject><subj-group><subject>Retinal ganglion cells</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject><subj-group><subject>Afferent neurons</subject><subj-group><subject>Retinal ganglion cells</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject><subj-group><subject>Ganglion cells</subject><subj-group><subject>Retinal ganglion cells</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject><subj-group><subject>Ganglion cells</subject><subj-group><subject>Retinal ganglion cells</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Optimization</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Inferring hidden structure in multilayered neural circuits</article-title>
<alt-title alt-title-type="running-head">Inferring hidden structure in multilayered neural circuits</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3946-4705</contrib-id>
<name name-style="western">
<surname>Maheswaranathan</surname> <given-names>Niru</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="fn" rid="currentaff001"><sup>¤a</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9681-425X</contrib-id>
<name name-style="western">
<surname>Kastner</surname> <given-names>David B.</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="fn" rid="currentaff002"><sup>¤b</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Baccus</surname> <given-names>Stephen A.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Ganguli</surname> <given-names>Surya</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Neurosciences Graduate Program, Stanford University, Stanford, California, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Neurobiology, Stanford University, Stanford, California, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Department of Applied Physics, Stanford University, Stanford, California, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Latham</surname> <given-names>Peter E.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>UCL, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<label>¤a</label>
<p>Current address: Google Brain, Mountain View, California, United States of America</p>
</fn>
<fn fn-type="current-aff" id="currentaff002">
<label>¤b</label>
<p>Current address: Department of Psychiatry, UCSF, San Francisco, California, United States of America</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">sganguli@stanford.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>8</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="epub">
<day>23</day>
<month>8</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>8</issue>
<elocation-id>e1006291</elocation-id>
<history>
<date date-type="received">
<day>20</day>
<month>3</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>9</day>
<month>6</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Maheswaranathan et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006291"/>
<abstract>
<p>A central challenge in sensory neuroscience involves understanding how neural circuits shape computations across cascaded cell layers. Here we attempt to reconstruct the response properties of experimentally unobserved neurons in the interior of a multilayered neural circuit, using cascaded linear-nonlinear (LN-LN) models. We combine non-smooth regularization with proximal consensus algorithms to overcome difficulties in fitting such models that arise from the high dimensionality of their parameter space. We apply this framework to retinal ganglion cell processing, learning LN-LN models of retinal circuitry consisting of thousands of parameters, using 40 minutes of responses to white noise. Our models demonstrate a 53% improvement in predicting ganglion cell spikes over classical linear-nonlinear (LN) models. Internal nonlinear subunits of the model match properties of retinal bipolar cells in both receptive field structure and number. Subunits have consistently high thresholds, supressing all but a small fraction of inputs, leading to sparse activity patterns in which only one subunit drives ganglion cell spiking at any time. From the model’s parameters, we predict that the removal of visual redundancies through stimulus decorrelation across space, a central tenet of efficient coding theory, originates primarily from bipolar cell synapses. Furthermore, the composite nonlinear computation performed by retinal circuitry corresponds to a boolean OR function applied to bipolar cell feature detectors. Our methods are statistically and computationally efficient, enabling us to rapidly learn hierarchical non-linear models as well as efficiently compute widely used descriptive statistics such as the spike triggered average (STA) and covariance (STC) for high dimensional stimuli. This general computational framework may aid in extracting principles of nonlinear hierarchical sensory processing across diverse modalities from limited data.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Computation in neural circuits arises from the cascaded processing of inputs through multiple cell layers. Each of these cell layers performs operations such as filtering and thresholding in order to shape a circuit’s output. It remains a challenge to describe both the computations and the mechanisms that mediate them given limited data recorded from a neural circuit. A standard approach to describing circuit computation involves building quantitative encoding models that predict the circuit response given its input, but these often fail to map in an interpretable way onto mechanisms within the circuit. In this work, we build two layer linear-nonlinear cascade models (LN-LN) in order to describe how the retinal output is shaped by nonlinear mechanisms in the inner retina. We find that these LN-LN models, fit to ganglion cell recordings alone, identify filters and nonlinearities that are readily mapped onto individual circuit components inside the retina, namely bipolar cells and the bipolar-to-ganglion cell synaptic threshold. This work demonstrates how combining simple prior knowledge of circuit properties with partial experimental recordings of a neural circuit’s output can yield interpretable models of the entire circuit computation, including parts of the circuit that are hidden or not directly observed in neural recordings.</p>
</abstract>
<funding-group>
<funding-statement>Stephen A. Baccus received funding from National Institutes of Health (NIH) R01EY025087, R01EY022933. Surya Ganguli received funding from the Burroughs-Wellcome, Simons, McKnight, and James S. McDonnell Foundations. Stephen A. Baccus and Surya Ganguli received a grant from Stanford University’s Cracking the Neural Code program. Niru Maheswaranathan was supported by an NSF Graduate Research Fellowship (GRFP) and the Stanford Mind, Brain, and Computation program. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="1"/>
<page-count count="30"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-09-05</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All code and data used in this paper are available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/baccuslab/inferring-hidden-structure-retinal-circuits" xlink:type="simple">https://github.com/baccuslab/inferring-hidden-structure-retinal-circuits</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<sec id="sec002">
<title>Motivation</title>
<p>Computational models of neural responses to sensory stimuli have played a central role in addressing fundamental questions about the nervous system, including how sensory stimuli are encoded and represented, the mechanisms that generate such a neural code, and the theoretical principles governing both the sensory code and underlying mechanisms. These models often begin with a statistical description of the stimuli that precede a neural response such as the spike-triggered average (STA) [<xref ref-type="bibr" rid="pcbi.1006291.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref002">2</xref>] or covariance (STC) [<xref ref-type="bibr" rid="pcbi.1006291.ref003">3</xref>–<xref ref-type="bibr" rid="pcbi.1006291.ref008">8</xref>]. These statistical measures characterize to some extent the set of effective stimuli that drive a response, but do not necessarily reveal how these statistical properties relate to cellular mechanisms or neural pathways. Going beyond descriptive statistics, an explicit representation of the neural code can be obtained by building a model to predict neural responses to sensory stimuli.</p>
<p>A classic approach involves a single stage of spatiotemporal filtering and a time-independent or static nonlinearity; these models include linear-nonlinear (LN) models with single or multiple pathways [<xref ref-type="bibr" rid="pcbi.1006291.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1006291.ref011">11</xref>] or generalized linear models (GLMs) with spike history feedback [<xref ref-type="bibr" rid="pcbi.1006291.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref013">13</xref>]. However, these models do not directly map onto circuit anatomy and function. As a result, the interpretation of such phenomenological models, as well as how they precisely relate to underlying cellular mechanisms, remains unclear. Ideally, one would like to generate more biologically interpretable models of sensory circuits, in which sub-components of the model map in a one-to-one fashion onto cellular components of neurobiological circuits [<xref ref-type="bibr" rid="pcbi.1006291.ref014">14</xref>]. For example, model components such as spatiotemporal filtering, thresholding, and summation are readily mapped onto photoreceptor or membrane voltage dynamics, synaptic and spiking thresholds, and dendritic pooling, respectively.</p>
<p>A critical aspect of sensory circuits is that they operate in a hierarchical fashion in which sensory signals propagate through multiple nonlinear cell layers [<xref ref-type="bibr" rid="pcbi.1006291.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1006291.ref017">17</xref>]. Fitting models that capture this widespread structure using neural data recorded from one layer of a circuit in response to controlled stimuli raises significant statistical and computational challenges [<xref ref-type="bibr" rid="pcbi.1006291.ref018">18</xref>–<xref ref-type="bibr" rid="pcbi.1006291.ref022">22</xref>]. A key issue is the high dimensionality of both stimulus and parameter space, as well as the existence of hidden, unobserved neurons in intermediate cell layers. The high dimensionality of parameter space can necessitate prohibitively large amounts of data and computational time required to accurately fit the model. One approach to address these difficulties is to incorporate prior knowledge about the structure and components of circuits to constrain the model [<xref ref-type="bibr" rid="pcbi.1006291.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref023">23</xref>–<xref ref-type="bibr" rid="pcbi.1006291.ref025">25</xref>]. Although prior knowledge of the exact network architecture and sequence of nonlinear transformations would greatly constrain the number of possible circuit solutions, such prior knowledge is typically minimal for most neural circuits.</p>
<p>In this work, we learn hierarchical nonlinear models from recordings of ganglion cells in the salamander retina, with the goal of building more interpretable models. In particular, we focus on models with two stages of linear-nonlinear processing (LN-LN models), analogous to the specific cell layers the retina. LN-LN models have been previously proposed to describe cascaded nonlinear computation in sensory pathways such as in V1 [<xref ref-type="bibr" rid="pcbi.1006291.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref021">21</xref>] and retina [<xref ref-type="bibr" rid="pcbi.1006291.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref027">27</xref>] (we elaborate on differences across these studies and our work below). In the retina, it has been proposed that there is a nonlinear transformation between bipolar and ganglion cells, however, building models that capture these nonlinearities has been a challenge due to the issues described above. Here, we find that with appropriate regularization, we are able to learn LN-LN models from recordings of ganglion cells alone that are both more accurate and more interpretable than their LN counterparts. In particular, inferred LN-LN model subunits quantitatively match properties of bipolar cells in the retina. Moreover, although the focus of this paper is on LN-LN models, we demonstrate that the algorithms we use to fit them are also useful in directly learning the STA and STC eigenvectors using very little data.</p>
<p>Further analysis of our learned LN-LN models reveals novel insights into retinal function, namely that: transmission between every subunit and ganglion cell pair is well described by a high threshold expansive nonlinearity (suppressing all but a small fraction of inputs), bipolar cell terminals are sparsely active, visual inputs are most decorrelated at the subunit layer, pre-synaptic to ganglion cells, and finally the composite computation performed by the retinal ganglion cell output corresponds to a boolean OR function of bipolar cell feature detectors. Collectively, these results shed light on the nature of hierarchical nonlinear computation in the retina. Our computational framework is general, however, and we hope it will aid in providing insights into hierarchical nonlinear computations across the nervous system.</p>
</sec>
<sec id="sec003">
<title>Background</title>
<p>The retina is a classic system for exploring the relationship between quantitative encoding models and measurements of neurobiological circuit properties [<xref ref-type="bibr" rid="pcbi.1006291.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref029">29</xref>]. Signals in the retina flow from photoreceptors through populations of horizontal, bipolar, and amacrine cells before reaching the ganglion cell layer.</p>
<p>To characterize this complex multilayered circuitry, many studies utilize descriptive statistics such as the spike-triggered average, interpreted as the average feature encoded by a ganglion cell [<xref ref-type="bibr" rid="pcbi.1006291.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1006291.ref003">3</xref>]. Responses are often then modeled using a linear-nonlinear (LN) framework (schematized in <xref ref-type="fig" rid="pcbi.1006291.g001">Fig 1A</xref>). A major reason for the widespread adoption of LN models is their high level of tractability; learning their parameters can be accomplished by solving a simple convex optimization problem [<xref ref-type="bibr" rid="pcbi.1006291.ref002">2</xref>], or alternatively, estimated using straightforward reverse correlation analyses [<xref ref-type="bibr" rid="pcbi.1006291.ref001">1</xref>]. However, LN models have two major drawbacks: it is difficult to map them onto biophysical mechanisms in retinal circuitry, and they do not accurately describe ganglion cell responses across diverse stimuli. Regarding mechanisms, the spatiotemporal linear filter of the LN model is typically interpreted as mapping onto the aggregate sequential mechanisms of phototransduction, signal filtering and transmission through bipolar and amacrine cell pathways, and summation at the ganglion cell, while the nonlinearity is mapped onto the spiking threshold of ganglion cells. Regarding accuracy, while previous studies have found that these simple models can, for some neurons, capture most of the variance of the responses to low-resolution spatiotemporal white noise [<xref ref-type="bibr" rid="pcbi.1006291.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref020">20</xref>], they do not describe responses to stimuli with more structure such as natural scenes [<xref ref-type="bibr" rid="pcbi.1006291.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref030">30</xref>–<xref ref-type="bibr" rid="pcbi.1006291.ref033">33</xref>].</p>
<fig id="pcbi.1006291.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006291.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Schematic of the LN-LN model and corresponding retinal circuitry.</title>
<p>(a) The LN-LN cascade contains a bank of linear-nonlinear (LN) subunits, whose outputs are pooled at a second linear stage before being passed through a final nonlinearity. (b,c) The LN-LN model mapped on to a retinal circuit. The first LN stage consists of bipolar cell subunits and the bipolar-to-ganglion cell synaptic threshold. The second LN stage is pooling at the ganglion cell, plus a spiking threshold. The contribution of inhibitory amacrine cells is omitted here.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006291.g001" xlink:type="simple"/>
</fig>
<p>A likely reason for these drawbacks are the nonlinearities within the retina. There can be strong rectification of signals that occurs pre-synaptic to ganglion cells [<xref ref-type="bibr" rid="pcbi.1006291.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1006291.ref036">36</xref>], breaking the assumption of composite linearity in the pathway from photoreceptors just up to the ganglion cell spiking threshold [<xref ref-type="bibr" rid="pcbi.1006291.ref017">17</xref>]. Indeed, nonlinear spatial integration within ganglion cell receptive fields was first described in the cat retina [<xref ref-type="bibr" rid="pcbi.1006291.ref037">37</xref>] in <italic>Y</italic>-type ganglion cells. A hypothetical model for this computation was proposed as a cascade of two layers of linear-nonlinear operations (LN-LN) [<xref ref-type="bibr" rid="pcbi.1006291.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref027">27</xref>]. If one keeps the mean luminance constant, avoiding light adaptation in photoreceptors, the first major nonlinearity is thought to lie at the presynaptic terminal of the bipolar to ganglion cell synapse. Ganglion cells pool over multiple bipolar cell inputs, each of which can be approximated as linear-nonlinear components, termed subunits of the ganglion cell. Due to the roughly linear integration [<xref ref-type="bibr" rid="pcbi.1006291.ref009">9</xref>] that occurs at bipolar cells, we (computationally) distill mechanisms in photoreceptors and inhibitory horizontal cells into a single spatiotemporal filter with positive and negative elements that gives rise to bipolar cell signals. The second LN layer corresponds to summation or pooling across multiple subunits at the ganglion cell soma, followed by a spiking threshold. The subunit nonlinearities in these models have been shown to underlie many retinal computations including latency encoding [<xref ref-type="bibr" rid="pcbi.1006291.ref029">29</xref>], object motion sensitivity [<xref ref-type="bibr" rid="pcbi.1006291.ref038">38</xref>], and sensitivity to fine spatial structure (such as edges) in natural scenes [<xref ref-type="bibr" rid="pcbi.1006291.ref035">35</xref>]. <xref ref-type="fig" rid="pcbi.1006291.g001">Fig 1</xref> shows a schematic of the LN-LN cascade and its mapping onto retinal anatomy. Functionally, these models with multiple nonlinear pathways both provide a more accurate description of ganglion cell responses and are more amenable to interpretation.</p>
<p>Early work on characterizing these multiple pathways motivated the use of the significant eigenvectors of the spike-triggered covariance (STC) matrix as the set of features that drives a cell, focusing on low-dimensional full field flicker stimuli [<xref ref-type="bibr" rid="pcbi.1006291.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref039">39</xref>] to reduce the amount of data required for accurately estimating these eigenvectors. Significant STC eigenvectors will span the same linear subspace as the true biological filters that make up the pathways feeding onto a ganglion cell [<xref ref-type="bibr" rid="pcbi.1006291.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref040">40</xref>–<xref ref-type="bibr" rid="pcbi.1006291.ref042">42</xref>]. However, the precise relationship between these eigenvectors (which obey a biologically implausible orthogonality constraint) and the individual spatiotemporal filtering properties intrinsic to multiple parallel pathways in a neural circuit remains unclear.</p>
<p>Instead, we take the approach of directly fitting a hierarchical, nonlinear, neural model, enabling us to jointly learn a set of non-orthogonal, biophysically plausible set of pathway filters, as well as an arbitrary, flexible nonlinearity for each pathway. Much recent and complementary work on fitting such models make simplifying assumptions in order to make model fitting tractable. For example, assuming the subunits are shifted copies of a template results in models with a single convolutional subunit filter [<xref ref-type="bibr" rid="pcbi.1006291.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref024">24</xref>]. However, this obscures individual variability in the spatiotemporal filters of subunits of the same type across visual space, which has been shown to be functionally important in increasing retinal resolution [<xref ref-type="bibr" rid="pcbi.1006291.ref043">43</xref>]. Another common assumption is that the subunit nonlinearities have a particular form, such as quadratic [<xref ref-type="bibr" rid="pcbi.1006291.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref025">25</xref>] or sigmoidal [<xref ref-type="bibr" rid="pcbi.1006291.ref044">44</xref>]. Fitting multi-layered models with convolutional filters and fixed nonlinearities has also been successfully used to describe retinal responses to natural scenes [<xref ref-type="bibr" rid="pcbi.1006291.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref033">33</xref>], although this work maximizes predictive accuracy at the expense of a one-to-one mapping of model components onto retinal circuit elements. Finally, other work focuses on particular ganglion cell types with a small number of inputs [<xref ref-type="bibr" rid="pcbi.1006291.ref022">22</xref>], constrains the input stimulus to a low-dimensional subspace (such as two halves of the receptive field [<xref ref-type="bibr" rid="pcbi.1006291.ref045">45</xref>]), or constrains the coefficients of receptive fields to be non-negative [<xref ref-type="bibr" rid="pcbi.1006291.ref046">46</xref>], thus discarding known properties of the inhibitory surround. Our approach is most similar to MacFarland et. al. [<xref ref-type="bibr" rid="pcbi.1006291.ref018">18</xref>], who formulate LN-LN models for describing nonlinearities in sensory pathways. Their model formulation uses spatiotemporal filters and smooth parameterized nonlinearities with an additional sparsity regularization penalty on the filters (encouraging filters to contain few non-zero elements), fit using gradient descent. They demonstrate that these methods recover the parameters of a simulated model cell with two subunit pathways. Our work differs technically in the types of regularization penalties we apply and our use of high-dimensional spatiotemporal stimuli, and scientifically in our focus on gaining insight into the nonlinear computations underlying spatiotemporal processing in the retina.</p>
<p>In this work, we do not make assumptions about or place restrictions on the number or tiling of subunit filters, the shapes of the subunit nonlinearities, the sign of receptive field elements, or the stimulus dimensionality. We additionally use a low-rank regularization penalty, that encourages approximately spatiotemporally separable filters [<xref ref-type="bibr" rid="pcbi.1006291.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref048">48</xref>], a property that is common to receptive fields in a wide variety of sensory systems. In order to fit these models, we use methods based on proximal consensus algorithms (described in <xref ref-type="sec" rid="sec018">Methods</xref>). These allow us to use this prior knowledge about model parameters to not only fit hierarchical nonlinear models, but also perform spike-triggered analyses using much less data than otherwise required.</p>
</sec>
</sec>
<sec id="sec004" sec-type="results">
<title>Results</title>
<sec id="sec005">
<title>Learning hierarchical nonlinear models of the retinal response</title>
<p>Our LN-LN model architecture (schematized in <xref ref-type="fig" rid="pcbi.1006291.g001">Fig 1</xref>) follows previous work [<xref ref-type="bibr" rid="pcbi.1006291.ref018">18</xref>]. The stimulus is first passed through a set of LN subunits. Each subunit filter is a spatiotemporal stimulus filter, constrained to have unit norm. The subunit nonlinearity is parameterized using a set of basis functions (Gaussian bumps) that tile the input space [<xref ref-type="bibr" rid="pcbi.1006291.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref020">20</xref>] (see <xref ref-type="sec" rid="sec018">Methods</xref>). This parameterization is flexible enough that we could learn, for each individual subunit, any smooth nonlinearity that can be expressed as a linear combination of our basis functions. The second LN layer pools subunits through weighted summation, followed by a spiking nonlinearity that we model using a parameterized soft rectifying function <italic>r</italic>(<italic>x</italic>) = <italic>g</italic> log(1 + <italic>e</italic><sup><italic>x</italic>−<italic>θ</italic></sup>). Here <italic>g</italic> is an overall gain, and <italic>θ</italic> is a threshold. The full set of parameters for the model consists of the spatiotemporal subunit filters, the subunit nonlinearity parameters, and the gain and threshold of the final nonlinearity.</p>
<sec id="sec006">
<title>Model fitting and performance</title>
<p>We recorded ganglion cell responses to a 40 minute stimulus consisting of a 1-D spatiotemporal white noise bars stimulus. For both LN and LN-LN models we fit model parameters by optimizing the sum of the log-likelihood of recorded spikes under a Poisson noise model [<xref ref-type="bibr" rid="pcbi.1006291.ref012">12</xref>] with additional regularization terms. We found that some form of regularization was critical to prevent over-fitting of the LN-LN model to the subset of data used for training. In particular, the regularization penalties we used were <italic>ℓ</italic><sub>1</sub> and nuclear norm penalties applied to the spatiotemporal subunit filters. The <italic>ℓ</italic><sub>1</sub> norm encourages filters to be sparse (have few non-zero coefficients) and the nuclear norm penalty encourages the space-time filter to be low rank (comprised of a small number of features separable in space and time). We chose the weights of the <italic>ℓ</italic><sub>1</sub> and nuclear norm regularization penalties, both for the LN and LN-LN models, through cross-validation on a small subset of cells, and then held these weights constant across all cells. Our subsequent results indicate that we do not have to fine tune these hyperparameters on a cell by cell basis to achieve good predictive performance. Finally, because different cells may have different numbers of functional subunits, for the LN-LN models, we chose the optimal number of subunits on a cell-by-cell basis by maximizing performance on held-out data through cross-validation. No additional structure was imposed on the subunits such as spatial repetition, overlap, or non-negativity. We optimize the log-likelihood and regularization terms using proximal algorithms (see <xref ref-type="sec" rid="sec018">Methods</xref> for an overview).</p>
<p>We find that the LN-LN model significantly outperforms the LN model at describing responses of ganglion cells, for all recorded cells. <xref ref-type="fig" rid="pcbi.1006291.g002">Fig 2A</xref> shows firing rate traces for an example cell, comparing the recorded response (gray) with an LN model (blue) and an LN-LN model (red). We quantify the similarity between predicted and recorded firing rate traces using either the Pearson correlation coefficient or the log-likelihood of held-out data under the model. All log-likelihood values are reported as an increase over the log-likelihood of a fixed mean firing rate model, scaled by the firing rate (yielding units of bits/spike). Summarized across <italic>n</italic> = 23 recorded ganglion cells, we find that the LN-LN model yields a consistent improvement over the LN model using either metric (<xref ref-type="fig" rid="pcbi.1006291.g002">Fig 2B and 2C</xref>). Overall, this demonstrated performance improvement indicates that nonlinear spatial integration is fundamental in driving ganglion cell responses, even to white noise stimuli, and that an LN model is not sufficient to capture the response to spatiotemporal white noise [<xref ref-type="bibr" rid="pcbi.1006291.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref049">49</xref>]. This salient, intermediate rectification that we identify computationally is consistent with previous measurements of bipolar-to-ganglion cell transmission in the retina [<xref ref-type="bibr" rid="pcbi.1006291.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref050">50</xref>].</p>
<fig id="pcbi.1006291.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006291.g002</object-id>
<label>Fig 2</label>
<caption>
<title>LN-LN models predict held-out response data better than LN models.</title>
<p>(a) Firing rates for an example neuron. The recorded firing rate (shaded, gray), is shown along with the LN model prediction (dashed, green) and the LN-LN prediction (solid, red). (b) LN-LN performance on held out data vs. the LN model, measured using correlation coefficient between the model and held out data. Note that all cells are above the diagonal. (c) Same as in (b), but with the performance metric of log-likelihood improvement over the mean rate in bits per spike.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006291.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Internal structure of learned models</title>
<p>Given the improved performance of our hierarchical nonlinear subunit models, we examined the internal structure of the model for insights into retinal structure and computation. <xref ref-type="fig" rid="pcbi.1006291.g003">Fig 3</xref> shows a visualization of the parameters learned for an example cell. <xref ref-type="fig" rid="pcbi.1006291.g003">Fig 3A and 3B</xref> shows the parameters for the classical LN model, for comparison, while <xref ref-type="fig" rid="pcbi.1006291.g003">Fig 3C and 3D</xref> shows the corresponding subunit filters and subunit nonlinearities in the first stage of the LN-LN model, fit to the same cell. The subunit filters had a similar temporal structure, but smaller spatial profiles compared to that of the LN model and the nonlinearities associated with each subunit were roughly monotonic with high thresholds (quantified below). These qualitative properties were consistent across all cells.</p>
<fig id="pcbi.1006291.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006291.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Example LN-LN model parameters fit to a recording of an OFF retinal ganglion cell.</title>
<p>(a and b): LN-model parameters, consisting of a single spatial filter (a) and nonlinearity (b). (c and d) LN-LN model parameters. (c) First layer filters (top) and nonlinearities (bottom) of an LN-LN model fit to the same cell. Spatial profiles of filters are shown in gray to the right of the filters. The subunit filters have a much smaller spatial extent compared to the LN filter, but similar temporal profiles.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006291.g003" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec008">
<title>Physiological properties of learned LN-LN models</title>
<p>Here we examine, in more detail, quantitative properties of learned LN-LN models that can be compared to physiological properties of the retina. We find that model subunits quantitatively resemble bipolar cells in terms of receptive field properties and number, and that these intermediate subunits consistently have high-threshold nonlinearities.</p>
<sec id="sec009">
<title>Inferred hidden units quantitatively resemble bipolar cell receptive fields</title>
<p>Mapping the LN-LN model onto retinal anatomy leads us to believe that the first layer filters (some examples of which are shown in <xref ref-type="fig" rid="pcbi.1006291.g003">Fig 3C</xref>) should mimic or capture filtering properties pre-synaptic to bipolar cells in the inner retina. To examine this possibility, we compared the first layer model filters to properties of bipolar cell receptive fields. An example learned model subunit receptive field is shown in <xref ref-type="fig" rid="pcbi.1006291.g004">Fig 4A</xref>, while a bipolar cell receptive field, obtained from direct intracellular recording of a bipolar cell, is shown in <xref ref-type="fig" rid="pcbi.1006291.g004">Fig 4B</xref>. Qualitatively, we found that the filters in the LN-LN model matched these bipolar cell RFs, as well as previously reported bipolar cell receptive fields [<xref ref-type="bibr" rid="pcbi.1006291.ref050">50</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref051">51</xref>]: both had center-surround receptive fields with similar spatial extents. We further quantified the degree of space-time separability of the filters using the numerical or stable rank [<xref ref-type="bibr" rid="pcbi.1006291.ref052">52</xref>], which is a measure of rank insensitive to small amounts of noise in the matrix (a stable rank of one indicates the filter is exactly space-time separable). We found that the degree of space-time separability of recorded bipolar cell receptive fields and inferred model subunits were quite similar (1.28 ± 0.01 and 1.39 ± 0.03, respectively), indicating that the nuclear norm penalty was not artificially reducing the rank of our model filters. This also demonstrates the advantage of using a soft rank penalty, such as the nuclear norm, as opposed to explicitly constraining the rank to any specific integer.</p>
<fig id="pcbi.1006291.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006291.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Comparison of subunit filter parameters with intracellular bipolar cell recordings.</title>
<p>(a) An example subunit bipolar cell. (b) A recorded bipolar cell receptive field. (c) Receptive field centers sizes for subunit filters (blue), LN model filters (green), and recorded bipolar cells (black point). (c) Same as in (b), but with receptive field surround sizes.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006291.g004" xlink:type="simple"/>
</fig>
<p>To quantitatively compare these model-derived and ground-truth bipolar cell receptive fields, we fit the spatial receptive field with a difference of Gaussians function to estimate the RF center and surround sizes. We find the RF centers for the LN-LN subunit filters are much smaller than the corresponding LN model filter. Furthermore, the size of these LN-LN subunit centers matched the size of the RF center measured from intracellular recordings of <italic>n</italic> = 8 bipolar cells (<xref ref-type="fig" rid="pcbi.1006291.g004">Fig 4C</xref>). The recorded bipolar cells, LN model filters (ganglion cells), and LN-LN subunit filters all had similar surround sizes (<xref ref-type="fig" rid="pcbi.1006291.g004">Fig 4D</xref>). More example bipolar cell receptive fields are provided in <xref ref-type="supplementary-material" rid="pcbi.1006291.s001">S1 Fig</xref>.</p>
<p>Note that this match between LN-LN model subunits and the RF properties of bipolar cells was not a pre-specified constraint placed on our model, but instead arose as an emergent property of predicting ganglion cell responses to white noise stimuli. These results indicate that our modeling framework not only enables higher performing predictive models of the retinal response, but can also reconstruct important aspects of the unobserved interior of the retina.</p>
</sec>
<sec id="sec010">
<title>Number of inferred subunits</title>
<p>The number of subunits utilized in the LN-LN model for any individual cell was chosen to optimize model predictive performance on a held-out data set via cross-validation. That is, we fit models with different numbers of subunits and selected the one with the best performance on a validation set. We find that for models with more subunits than necessary, extra subunits are ignored (the learned nonlinearity for these subunits is flat, thus they do not modulate the firing rate).</p>
<p><xref ref-type="fig" rid="pcbi.1006291.g005">Fig 5A</xref> shows the model performance, quantified as the difference between the LN-LN model and the LN model, across a population of cells as a function of the number of subunits included in the LN-LN model. We find that models with four to six subunits maximized model performance on held-out data. Note that the stimuli used here are one-dimensional spatiotemporal bars that have constant luminance across one spatial dimension. Thus each model subunit likely corresponds to the combination of multiple bipolar cell inputs whose receptive fields overlap a particular bar in the stimulus. Previous anatomical studies of bipolar cell density and axonal branching width [<xref ref-type="bibr" rid="pcbi.1006291.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref054">54</xref>] as well as functional studies [<xref ref-type="bibr" rid="pcbi.1006291.ref046">46</xref>] suggest that a typical ganglion cell in the salamander retina receives input from 10–50 bipolar cells whose receptive fields are tiled across two dimensional space. The number of independently activated groups of such a two dimensional array of bipolar cells, in response to a one dimensional bar stimulus is then expected to be reduced from the total number of bipolar cells, roughly, by a square root factor: i.e. <inline-formula id="pcbi.1006291.e001"><alternatives><graphic id="pcbi.1006291.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mn>25</mml:mn> <mml:mo>→</mml:mo> <mml:msqrt><mml:mn>25</mml:mn></mml:msqrt> <mml:mo>=</mml:mo> <mml:mn>5</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. This estimate is largely consistent with the typical number of subunits in <xref ref-type="fig" rid="pcbi.1006291.g005">Fig 5A</xref>, required to optimize model predictive performance. This estimate also suggests that the large majority of bipolar-to-ganglion cell synapses are rectifying (strongly nonlinear), as linear connections are not uniquely identifiable in an LN-LN cascade. Indeed, in the salamander retina, strong rectification appears to be the norm [<xref ref-type="bibr" rid="pcbi.1006291.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref050">50</xref>].</p>
<fig id="pcbi.1006291.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006291.g005</object-id>
<label>Fig 5</label>
<caption>
<title>LN-LN model parameter analysis.</title>
<p>(a) Performance improvement (increase in correlation coefficient relative to an LN model) as a function of the number of subunits used in the LN-LN model. Error bars indicate the standard error across 23 cells. (b) Subunit nonlinearities learned across all ganglion cells. For reference the white noise input to a subunit nonlinearity has standard deviation 1, which sets the scale of the x-axis. Red line and shaded fill indicate the mean and s.e.m. of nonlinearity thresholds (see text for details). (c) Visualization of the principal axes of variation in subunit nonlinearities by adding or subtracting principal components from the mean nonlinearity. (top) The principal axis of variation in subunit nonlinearities results in a gain change, while (bottom) the second principal axis corresponds to a threshold shift. These two dimensions captured 63% of the nonlinearity variability across cells.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006291.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec011">
<title>LN-LN models have subunit nonlinearities with high thresholds</title>
<p>The nonlinearities for all of the measured subunits are overlaid in <xref ref-type="fig" rid="pcbi.1006291.g005">Fig 5B</xref>. Each LN-LN subunit nonlinearity takes as input the projection of the stimulus onto the corresponding subunit spatiotemporal filter. Since the stimulus components are white noise with unit standard deviation, and the spatiotemporal filter is constrained to have unit norm, the projection of the stimulus onto the filter has a standard Normal distribution, thus we can compare nonlinearities on a common axis. Despite the fact that the model could separately learn an arbitrary function over the input for each subunit nonlinearity, we find that the nonlinearities are fairly consistent across the different subunits of many cells. Subunit nonlinearities look roughly like thresholding functions, relatively flat for most inputs but then increasing sharply after a threshold. We quantified the threshold as input for which the nonlinearity reaches 40% of the maximum output, across <italic>n</italic> = 92 model-identified subunits the mean threshold was 3.09 ± 0.14 (s.e.m.) standard deviations. We additionally computed LN thresholds for ganglion cells and found that they were similarly consistent across the population (3.22 ± 0.11 standard deviations). We decomposed the set of nonlinearities using principal components analysis and show the two primary axes of variation in <xref ref-type="fig" rid="pcbi.1006291.g005">Fig 5C</xref>. The primary axis of variation results in a gain change, while the secondary axis induces a threshold shift. Due to the high thresholds of these nonlinearities, subunits only impact ganglion cell firing probability for large input values. The slight rise on the left side of the nonlinearities is likely due to weak ON- inputs to the ganglion cell, and a stimulus ensemble that drives the ON- pathways more strongly [<xref ref-type="bibr" rid="pcbi.1006291.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref055">55</xref>] may be necessary to uniquely identify them. Our ganglion cell population consisted of 18 Fast-Off, 4 Slow-Off, and 1 On cell (classification shown in <xref ref-type="supplementary-material" rid="pcbi.1006291.s002">S2 Fig</xref>). We did not find significant differences across cell types in terms of the number of identified subunits or the subunit thresholds.</p>
</sec>
</sec>
<sec id="sec012">
<title>Computational properties of learned LN-LN models</title>
<p>We now turn from a quantitative analysis of the physiological properties of the retina, described above, to their implications in terms of the computational function of the retina in processing visual stimuli. In particular, in the next two sub-sections we predict that the dominant contribution to stimulus decorrelation in efficient coding theory occurs at the bipolar cell synaptic threshold, and that the composite function computed by a retinal ganglion cell corresponds to a logical OR of its bipolar cell inputs.</p>
<sec id="sec013">
<title>Stimulus decorrelation at different stages in hierarchical retinal processing</title>
<p>Natural stimuli have highly redundant structure. Efficient coding theories [<xref ref-type="bibr" rid="pcbi.1006291.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref057">57</xref>] state that sensory systems ought to remove these redundancies in order to efficiently encode natural stimuli. The simplest such redundancy is that nearby points in space and time contain similar, or correlated, luminance levels [<xref ref-type="bibr" rid="pcbi.1006291.ref058">58</xref>]. The transmission of such correlated structure would thus be highly inefficient. Efficient coding has been used to explain why responses are much less correlated than natural scenes, although the mechanistic underpinnings of decorrelation in the retina remain unclear.</p>
<p>Early work [<xref ref-type="bibr" rid="pcbi.1006291.ref059">59</xref>] suggested a simple mechanism: the linear center-surround receptive field of ganglion cells (and more recently, of bipolar cells [<xref ref-type="bibr" rid="pcbi.1006291.ref060">60</xref>]) could contribute to redundancy reduction simply by transmitting only differences in stimulus intensity across nearby positions in space. However, it was recently shown [<xref ref-type="bibr" rid="pcbi.1006291.ref061">61</xref>] using LN models that most of the decorrelation of naturalistic stimuli in the retina could be attributed to ganglion cell nonlinearities, as opposed to linear filtering. Given that we fit an entire layer of subunits pre-synaptic to each ganglion cell layer, we can analyze the spatial representation of naturalistic images at different stages of hierarchical retinal processing, thereby localizing the computation of decorrelation to a particular stage in the model.</p>
<p>To do so, we generated the response of the entire population of model subunits to a spatial stimulus similar to previous work [<xref ref-type="bibr" rid="pcbi.1006291.ref061">61</xref>], namely spatially pink noise, low pass filtered in time. We computed the correlation of stimulus intensities as a function of spatial distance, as well as the correlation between pairs of model units as a function of spatial distance. We examined pairs of units across different stages of the LN-LN model: after linear filtering by the subunits, after the subunit nonlinearity, and finally at the ganglion cell firing rates (the final stage). <xref ref-type="fig" rid="pcbi.1006291.g006">Fig 6</xref> shows that the correlation at these stages drops off with distance between either the subunits or ganglion cells (with distance measured between receptive field centers). Pitkow &amp; Meister [<xref ref-type="bibr" rid="pcbi.1006291.ref061">61</xref>] found that the nonlinearity in an LN model was primarily responsible for most of the decorrelation (overall suppression of the correlation curve towards zero). However, without the ability to map the LN nonlinearity to biophysical components in the retina, it is hard to map this result onto a particular biophysical mechanism. Instead, our model predicts that this decorrelation is primarily due to the <italic>subunit</italic> nonlinearities, as opposed to ganglion cell spiking nonlinearities. In fact, the correlation between the ganglion cell model firing rates slightly increases after pooling across subunits. The most decorrelated representation occurs just after thresholding at the subunit layer. In this manner, our modeling framework suggests a more precisely localized mechanistic origin for a central tenet of efficient coding theory. Namely, our results predict that the removal of visual redundancies, through stimulus decorrelation across space, originates primarily from high-threshold nonlinearities associated with bipolar cell synapses.</p>
<fig id="pcbi.1006291.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006291.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Decorrelation in LN-LN subunit models.</title>
<p>A naturalistic (pink noise) stimulus was shown to a population of nonlinear subunits. The correlation in the population after filtering at the subunit layer (blue), after the subunit nonlinearity (green), and after pooling and thresholding at the ganglion cell layer (red), in addition to the stimulus correlations (gray) are shown. Left: the correlation as a function of distance on the retina for Off-Off cell pairs. Right: correlation for Off-On cell pairs. For each plot, distances were binned every 70<italic>μm</italic>, and error bars are the s.e.m. within each bin.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006291.g006" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec014">
<title>The nature of nonlinear spatial integration across retinal subunits</title>
<p>Retinal ganglion cells emit responses in sparse, temporally precise patterns [<xref ref-type="bibr" rid="pcbi.1006291.ref062">62</xref>], presumably to keep firing rates low thereby preserving energy [<xref ref-type="bibr" rid="pcbi.1006291.ref061">61</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref063">63</xref>]. LN models can emulate sparse, precise firing in only one way: by using nonlinearities with high thresholds relative to the distribution of stimuli projected onto their linear filter. This way, only a small fraction of stimuli will cause the model to generate a response. Indeed, nonlinearities in LN models fit to ganglion cells have high thresholds [<xref ref-type="bibr" rid="pcbi.1006291.ref009">9</xref>]. LN-LN models, in contrast, can generate sparse responses using two qualitatively distinct operating regimes: either the subunit thresholds (first nonlinearity) could be high and the ganglion cell or spiking threshold (second nonlinearity) could be low, or the subunit thresholds could be low and spiking thresholds could be high. Both of these scenarios give rise to sparse firing at the ganglion cell output. However, they correspond to categorically distinct functional computations.</p>
<p>These various scenarios are diagrammed in <xref ref-type="fig" rid="pcbi.1006291.g007">Fig 7A–7C</xref>. Each panel shows the response of a model in a two-dimensional space defined by the projection of the stimulus onto two subunit filters pre-synaptic to the ganglion cell (the two-dimensional space is easier for visualization, but the same picture holds for multiple subunits). We show the response as contours where the firing probability is constant (iso-response contours). Here, the subunit nonlinearities play a key role in shaping the geometry of the response contours, and therefore shape the computation performed by the cell. Note that the ganglion cell nonlinearity would act to rescale the output, but cannot change the shape of the contours. Therefore, it is fundamentally the subunit nonlinearities alone that determine the geometry of the response contours.</p>
<fig id="pcbi.1006291.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006291.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Visualization of subunit contours.</title>
<p>Contours of equal firing probability are shown in a 2D space defined by the projection of the visual stimulus along each of two subunits. (a) Example contour plots for a model with low threshold subunit nonlinearities (inset) has concave contours. (b) A model with high threshold subunit nonlinearities has convex contours. (c &amp; d) Contours from a model for two example ganglion cells, for three different pairs of subunits (left to right). In each panel, a histogram of the recorded firing rate is shown (red squares) as well as the stimulus distribution (gray oval).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006291.g007" xlink:type="simple"/>
</fig>
<p>Low-threshold subunit nonlinearities give rise to concave contours (<xref ref-type="fig" rid="pcbi.1006291.g007">Fig 7B</xref>), whereas high-threshold subunits give rise to convex contours (<xref ref-type="fig" rid="pcbi.1006291.g007">Fig 7C</xref>). Because final output rate is determined by the subunit and final thresholds, both of these descriptions could yield sparse firing output with the same overall rate (by adjusting the final threshold), but correspond to different computations. Low-threshold subunits can be simultaneously active across many stimuli, and thus yield spiking when subunits are simultaneously active (an AND-like combination of inputs). On the other hand, high-threshold subunits are rarely simultaneously active and thus usually only one subunit is active during a ganglion cell firing event, giving rise to an OR-like combination of inputs. By comparison, a cell that linearly integrates its inputs would have linear contours (<xref ref-type="fig" rid="pcbi.1006291.g007">Fig 7A</xref>).</p>
<p>In our models fit to retinal ganglion cells, we find all cells are much more consistent with the high threshold OR-like model. Subunit nonlinearities tend to have high thresholds, and therefore result in convex contours (shown for different pairs of subunits for two example cells in <xref ref-type="fig" rid="pcbi.1006291.g007">Fig 7D and 7E</xref>). For each example ganglion cell, we show the corresponding model contours along with the 2 standard deviation contour of the stimulus distribution (gray oval) and the empirical firing histogram (red checkers) in the 2D space defined by the projection of the stimulus onto a given pair of subunit filters identified by the LN-LN cascade model. Note that while the stimulus is uncorrelated (i.e. white, or circular), non-orthogonality of subunit filters themselves yield correlations in the subunit activations obtained by applying each subunit filter to the stimulus. Hence the stimulus distribution in the space of subunit activations (grey shaded ovals) is not circular. In all recorded cells, we find that the composite computation implemented by retinal ganglion cell circuitry corresponds to an OR function associated with high subunit thresholds (as schematized in <xref ref-type="fig" rid="pcbi.1006291.g007">Fig 7C</xref>). Moreover, both the AND computation and the linear model are qualitatively ruled out by the shape of the model response contours as well as the empirical firing histogram over subunit activations, which closely tracks the model response contours (i.e. the boundaries of the red histograms are well captured by the model contours).</p>
<p>These results are consistent with previous studies of nonlinear spatial integration in the retina. For example, Bollinger et. al. [<xref ref-type="bibr" rid="pcbi.1006291.ref045">45</xref>] discovered convex iso-response contours for a very simple two dimensional spatial stimulus, and Kaardal et. al. [<xref ref-type="bibr" rid="pcbi.1006291.ref044">44</xref>] performed an explicit hypothesis test between an AND-like and OR-like nonlinear integration over a low dimensional subspace obtained via the un-regularized STC eigenvectors, finding that OR outperformed AND. However, the techniques of [<xref ref-type="bibr" rid="pcbi.1006291.ref045">45</xref>] can only explore a low-dimensional stimulus space, whereas our methods enable the discovery of iso-response contours for high-dimensional stimuli. Moreover, in contrast to the hypothesis testing approach taken in [<xref ref-type="bibr" rid="pcbi.1006291.ref044">44</xref>], our general methods to learn LN-LN models reveal that an OR-model of nonlinear integration is a good model on an absolute scale of performance amongst all models in the LN-LN family, rather than simply being better than an AND-model.</p>
</sec>
<sec id="sec015">
<title>A multi-dimensional view of cascaded retinal computation</title>
<p>A simple, qualitative schematic of the distinct computational regime in which retinal ganglion cells operate in response to white noise stimuli can be obtained by considering the geometry of the spike triggered ensemble in <italic>N</italic> dimensional space. In particular, the distribution of stimuli concentrates on a constant radius sphere in <italic>N</italic> dimensional stimulus space. More precisely, any high dimensional random stimulus realization <bold>x</bold> has approximately the same vector length, because the fluctuations in length across realizations, relative to the mean length, is <inline-formula id="pcbi.1006291.e002"><alternatives><graphic id="pcbi.1006291.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Thus we can think of all likely white-noise stimuli as occurring on the <italic>N</italic> − 1 dimensional surface of a sphere in <italic>N</italic> dimensional space. Each subunit filter can be thought of as a vector pointing in a particular direction in <italic>N</italic> dimensional stimulus space. The corresponding input to the subunit nonlinearity for any stimulus is the inner-product of the stimulus with the subunit filter, when both are viewed as <italic>N</italic> dimensional vectors. The high threshold of the subunit nonlinearity means that the subunit only responds to a small subset of stimuli on the sphere, corresponding to a small cap centered around the subunit filter. For a single subunit model (i.e. an LN model), the set of stimuli that elicit a spike then corresponds simply to this one cap (<xref ref-type="fig" rid="pcbi.1006291.g008">Fig 8A</xref>). In contrast, the OR like computation implemented by an LN-LN model with high subunit thresholds responds to stimuli in a region consisting of a union of small caps, one for each subunit (<xref ref-type="fig" rid="pcbi.1006291.g008">Fig 8B</xref>).</p>
<fig id="pcbi.1006291.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006291.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Stimulus selectivity in LN and LN-LN models.</title>
<p>Each panel shows the raw stimulus distribution (gray contours) projected onto the top two principal components of the spike-triggered subunit activations (with subunits identified by the LN-LN model). The LN model (a) fires in response to stimuli in a single region, or cap, of stimulus space (indicated by the arrow and dashed threshold), whereas the LN-LN model (b) fires in response to a union of caps, each defined by an individual subunit. (c) Spike-triggered subunit activations for three representative cells are shown as colored histograms (colors indicate which model-identified subunit was maximally active during the spike), with the corresponding subunit filter directions shown as colored arrows (see text for details). Color intensity of the histogram indicates the probability density of the spike-triggered ensemble (STE), thus drops in intensity between changes in color indicate a multimodal STE, with high density modes centered near subunit filter directions.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006291.g008" xlink:type="simple"/>
</fig>
<p>To verify this conceptual picture, in <xref ref-type="fig" rid="pcbi.1006291.g008">Fig 8C</xref> we visualize a two-dimensional projection of the spike-triggered stimulus ensemble for three example ganglion cells, using principal components analysis of the spike-triggered subunit activations. That is, we project the spike-triggered ensemble onto the subunit filters identified in the LN-LN model, and subsequently project those subunit activations onto the two dimensions that capture the most variance in subunit activations. Note that this is different from just taking the top two principal components of the STC matrix, as the top STC component is typically the <italic>average</italic> of the subunit filters [<xref ref-type="bibr" rid="pcbi.1006291.ref010">10</xref>], which does not differentiate the subunit activations. This procedure identifies a subspace that captures the radial spread of subunit filters in high-dimensional stimulus space. We find that the spike-triggered ensemble projected onto this subspace (<xref ref-type="fig" rid="pcbi.1006291.g008">Fig 8C</xref>) curves around the radial shell defined by the stimulus distribution, and matches the conceptual picture shown in <xref ref-type="fig" rid="pcbi.1006291.g008">Fig 8B</xref>. For ease of visualization, we colored elements in the spike-triggered ensemble by which LN-LN model subunit was maximally active during that spike, and we normalize the spike-triggered histogram by the raw stimulus distribution (gray ovals). This picture provides a simple, compelling view for why LN models are insufficient to capture the retinal response to white noise, and further visualizes the aspect of retinal computation LN-LN models capture that the LN model does not: ganglion cells encode the union of different types of stimuli, with each stimulus type having large overlap with precisely one subunit filter.</p>
</sec>
</sec>
<sec id="sec016">
<title>Regularized spike-triggered analysis</title>
<p>Given a mathematical model of a multilayered neural circuit, we can connect the pathways in such models back to descriptive statistics, namely, spike-triggered statistics such as the spike-triggered average (STA) and covariance (STC). That is, we can show that the STA and the STC eigenvectors of a general LN-LN model are linear combinations its pathway filters (see <xref ref-type="sec" rid="sec018">Methods</xref> and [<xref ref-type="bibr" rid="pcbi.1006291.ref064">64</xref>]). Therefore, we expect certain types of structure in said pathways to persist after the linear combination, assuming the number of pathways is small relative to the stimulus dimension. This immediately suggests that the same proximal algorithms and regularization terms we used to fit LN-LN models can be used to regularize the STA and the STC eigenvectors directly (for situations where one is interested in the descriptive statistics, but not the full encoding model).</p>
<p>To illustrate the benefits of the regularization terms used to fit the LN-LN models, we apply these penalties to perform regularized spike-triggered analysis. We formulate optimization problems for regularizing the spike-triggered average and covariance which only require access to the un-regularized estimates (see <xref ref-type="sec" rid="sec018">Methods</xref>). This is useful for the situation where working with the full spike-triggered ensemble or raw dataset is prohibitive due to computational time or memory constraints.</p>
<p><xref ref-type="fig" rid="pcbi.1006291.g009">Fig 9A</xref> compares a regularized spike-triggered average with the raw, un-regularized STA for an example recorded ganglion cell in response to a 1-D spatiotemporal white noise stimulus. For long recordings, the regularized STA closely matches the raw STA, while for short recordings the regularized STA has less high frequency noise and retains much of the structure observed if the STA had been estimated using more data. <xref ref-type="fig" rid="pcbi.1006291.g009">Fig 9B</xref> shows the held-out performance of the regularized STA for an example cell across different regularization weights, scanned over a broad range, demonstrating that performance is largely insensitive to the strengths of the weights of the <italic>ℓ</italic><sub>1</sub> and nuclear norm penalty functions. Thus regularization weights need not be fine tuned to achieve superior performance. We further quantified the performance of the regularized STA by using it as the linear filter of an LN model, and found that with regularization, about 5 minutes of recording was sufficient to achieve the performance (on held-out data) obtained through 40 minutes of recording without regularization (<xref ref-type="fig" rid="pcbi.1006291.g009">Fig 9C</xref>).</p>
<fig id="pcbi.1006291.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006291.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Regularization for estimating receptive fields (via a regularized spike-triggered-average).</title>
<p>(a) Top row: the raw spike-triggered average computed using different amounts of data (from left to right, 30s to 40min), bottom row: the regularized spike-triggered average computed using the same amount of data as the corresponding column. (b) Performance (held-out log-likelihood) as a function of two regularization weights, the nuclear norm (x-axis, encourages low-rank structure) and the <italic>ℓ</italic><sub>1</sub>-norm (y-axis, encourages sparsity), for an example cell. (c) Correlation coefficient (on held-out data) between the firing rate of a retinal ganglion cell and LN model whose filter is fixed to be a regularized or raw (un-regularized) STA, as a function of the amount of training data for estimating the STA (length of recording).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006291.g009" xlink:type="simple"/>
</fig>
<p><xref ref-type="fig" rid="pcbi.1006291.g010">Fig 10</xref> demonstrates the improvement in our ability to estimate the relevant subspace spanned by significant STC eigenvectors, both in terms of the qualitative improvement in eigenvectors for an example cell (<xref ref-type="fig" rid="pcbi.1006291.g010">Fig 10A</xref>) and quantified across the population (<xref ref-type="fig" rid="pcbi.1006291.g010">Fig 10B</xref>). In <xref ref-type="fig" rid="pcbi.1006291.g010">Fig 10A</xref>, we show the top regularized STC eigenvectors for different values of the nuclear norm (<italic>γ</italic><sub>*</sub>) and <italic>ℓ</italic><sub>1</sub>-norm (<italic>γ</italic><sub>1</sub>) regularization penalties (<xref ref-type="disp-formula" rid="pcbi.1006291.e034">Eq 11</xref> in <xref ref-type="sec" rid="sec018">Methods</xref> and <xref ref-type="table" rid="pcbi.1006291.t001">Table 1</xref>). We score the performance of the STC subspace in <xref ref-type="fig" rid="pcbi.1006291.g010">Fig 10B</xref> in terms of how well stimuli, after projection onto the subspace, can be used to predict spikes, by computing the subspace overlap (defined in <xref ref-type="sec" rid="sec018">Methods</xref>) between the raw or regularized STC subspace and the best fit LN-LN subspace. This quantity ranges between zero for orthogonal subspaces and one for overlapping subspaces.</p>
<fig id="pcbi.1006291.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006291.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Regularized spike-triggered covariance.</title>
<p>(a) Example panels of the output of our regularized spike-triggered covariance algorithm. Each panel contains the five most significant regularized eigenvectors of the STC matrix, reshaped as spatiotemporal filters. The bottom panel shows the result with no regularization added, and the upper panels show the result with increasing weights on the regularization penalties. Here <italic>γ</italic><sub>1</sub> is the regularization weight applied to an <italic>ℓ</italic><sub>1</sub> penalty encouraging sparsity, and <italic>γ</italic><sub>*</sub> is a regularization weight applied to a nuclear norm penalty, encouraging approximate spatiotemporal separability of the eigenvectors, when reshaped as spatiotemporal filters. (b) Summary across a population of cells. The heatmap shows the held-out performance of regularized STC (measured as the subspace overlap with the best fit LN-LN subspace, see text for details). The y-axis in (b) represents a line spanning 3 orders of magnitude in two-dimensional regularization parameter space (<italic>γ</italic><sub>*</sub>, <italic>γ</italic><sub>1</sub>), ranging from the point (<italic>γ</italic><sub>*</sub> = 10<sup>−4</sup>, <italic>γ</italic><sub>1</sub> = 10<sup>−3</sup>) to (<italic>γ</italic><sub>*</sub> = 10<sup>−1</sup>, <italic>γ</italic><sub>1</sub> = 1).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006291.g010" xlink:type="simple"/>
</fig>
<table-wrap id="pcbi.1006291.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006291.t001</object-id>
<label>Table 1</label>
<caption>
<title>Common regularization penalties and their proximal operators (in closed form).</title>
</caption>
<alternatives>
<graphic id="pcbi.1006291.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006291.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Penalty function, <italic>ϕ</italic>(x)</th>
<th align="center">Proximal operator, <inline-formula id="pcbi.1006291.e003"><alternatives><graphic id="pcbi.1006291.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi> <mml:mi>ϕ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">v</mml:mi> <mml:mo>,</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></th>
<th align="center">Computational complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><italic>ℓ</italic><sub>2</sub>-norm, <italic>γ</italic>‖<bold>x</bold>‖<sub>2</sub></td>
<td align="center">
<inline-formula id="pcbi.1006291.e004">
<alternatives>
<graphic id="pcbi.1006291.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e004" xlink:type="simple"/>
<mml:math display="inline" id="M4">
<mml:mfrac>
<mml:mi mathvariant="bold">v</mml:mi>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>/</mml:mo>
<mml:mi>ρ</mml:mi>
</mml:mrow>
</mml:mfrac>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="center">
<inline-formula id="pcbi.1006291.e005">
<alternatives>
<graphic id="pcbi.1006291.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e005" xlink:type="simple"/>
<mml:math display="inline" id="M5">
<mml:mrow>
<mml:mi mathvariant="script">O</mml:mi>
<mml:mo>(</mml:mo>
<mml:mi>n</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>
</td>
</tr>
<tr>
<td align="center"><italic>ℓ</italic><sub>1</sub>-norm, <italic>γ</italic>‖<bold>x</bold>‖<sub>1</sub></td>
<td align="center">
<inline-formula id="pcbi.1006291.e006">
<alternatives>
<graphic id="pcbi.1006291.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e006" xlink:type="simple"/>
<mml:math display="inline" id="M6">
<mml:mo>{</mml:mo>
<mml:mtable>
<mml:mtr>
<mml:mtd columnalign="left">
<mml:mrow>
<mml:msub>
<mml:mi>v</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>-</mml:mo>
<mml:mi>γ</mml:mi>
<mml:mo>/</mml:mo>
<mml:mi>ρ</mml:mi>
</mml:mrow>
</mml:mtd>
<mml:mtd columnalign="left">
<mml:mrow>
<mml:msub>
<mml:mi>v</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>≥</mml:mo>
<mml:mi>γ</mml:mi>
<mml:mo>/</mml:mo>
<mml:mi>ρ</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="left">
<mml:mrow>
<mml:msub>
<mml:mi>v</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mi>γ</mml:mi>
<mml:mo>/</mml:mo>
<mml:mi>ρ</mml:mi>
</mml:mrow>
</mml:mtd>
<mml:mtd columnalign="left">
<mml:mrow>
<mml:msub>
<mml:mi>v</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>≤</mml:mo>
<mml:mo>-</mml:mo>
<mml:mi>γ</mml:mi>
<mml:mo>/</mml:mo>
<mml:mi>ρ</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="left">
<mml:mn>0</mml:mn>
</mml:mtd>
<mml:mtd columnalign="left">
<mml:mtext>otherwise</mml:mtext>
</mml:mtd>
</mml:mtr>
</mml:mtable>
<mml:mo/>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="center">
<inline-formula id="pcbi.1006291.e007">
<alternatives>
<graphic id="pcbi.1006291.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e007" xlink:type="simple"/>
<mml:math display="inline" id="M7">
<mml:mrow>
<mml:mi mathvariant="script">O</mml:mi>
<mml:mo>(</mml:mo>
<mml:mi>n</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>
</td>
</tr>
<tr>
<td align="center">Nuclear norm, ‖<italic>X</italic>‖<sub>*</sub></td>
<td align="center"><bold>X</bold> = <bold>USV</bold><sup><italic>T</italic></sup>, <inline-formula id="pcbi.1006291.e008"><alternatives><graphic id="pcbi.1006291.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi> <mml:mi>ϕ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">v</mml:mi> <mml:mo>,</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">U</mml:mi> <mml:msup><mml:mi mathvariant="bold">S</mml:mi> <mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msup> <mml:msup><mml:mi mathvariant="bold">V</mml:mi> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1006291.e009"><alternatives><graphic id="pcbi.1006291.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:msup><mml:mi mathvariant="bold">S</mml:mi> <mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msup> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>/</mml:mo> <mml:mi>ρ</mml:mi></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>≥</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>/</mml:mo> <mml:mi>ρ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:math></alternatives></inline-formula></td>
<td align="center">
<inline-formula id="pcbi.1006291.e010">
<alternatives>
<graphic id="pcbi.1006291.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e010" xlink:type="simple"/>
<mml:math display="inline" id="M10">
<mml:mrow>
<mml:mi mathvariant="script">O</mml:mi>
<mml:mo>(</mml:mo>
<mml:mtext>min</mml:mtext>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>m</mml:mi>
<mml:msup>
<mml:mi>n</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo>,</mml:mo>
<mml:mi>n</mml:mi>
<mml:msup>
<mml:mi>m</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>
</td>
</tr>
<tr>
<td align="center">Non-negativity, <inline-formula id="pcbi.1006291.e011"><alternatives><graphic id="pcbi.1006291.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mi mathvariant="script">I</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>&gt;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></td>
<td align="center">
<inline-formula id="pcbi.1006291.e012">
<alternatives>
<graphic id="pcbi.1006291.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e012" xlink:type="simple"/>
<mml:math display="inline" id="M12">
<mml:mo>{</mml:mo>
<mml:mtable>
<mml:mtr>
<mml:mtd columnalign="left">
<mml:msub>
<mml:mi>v</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:mtd>
<mml:mtd columnalign="left">
<mml:mrow>
<mml:msub>
<mml:mi>v</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>&gt;</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="left">
<mml:mn>0</mml:mn>
</mml:mtd>
<mml:mtd columnalign="left">
<mml:mrow>
<mml:msub>
<mml:mi>v</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>≤</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
<mml:mo/>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="center">
<inline-formula id="pcbi.1006291.e013">
<alternatives>
<graphic id="pcbi.1006291.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e013" xlink:type="simple"/>
<mml:math display="inline" id="M13">
<mml:mrow>
<mml:mi mathvariant="script">O</mml:mi>
<mml:mo>(</mml:mo>
<mml:mi>n</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>
</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Since the LN-LN subspace is the best subspace found by the LN-LN model for predicting spiking, a large subspace overlap between the regularized STC and LN-LN subspaces indicates the ability of regularized STC to find stimulus subspaces predictive of neural firing without actually fitting a model of the neuron. With appropriate regularization, one can recover the best predictive subspace using about 10 minutes of data; without regularization, one requires 40 minutes of data to recover a subspace with comparative predictive accuracy. Note that even for the full length of this experiment (40 minutes), regularization still improves our regularized STC estimate. Thus, we find the subspace spanned by regularized STC eigenvectors becomes very similar to the subspace obtained from the filters in an LN-LN model, as predicted by our theoretical analysis.</p>
</sec>
</sec>
<sec id="sec017" sec-type="conclusions">
<title>Discussion</title>
<p>In summary, we combine proximal algorithms with non-smooth regularization terms to model stimulus driven neural processing in circuits with multiple parallel, hierarchical nonlinear pathways using limited experimental data. We found that models employing two stages of linear and nonlinear computation, namely LN-LN models, demonstrated a robust improvement over the classical standard of LN models at predicting responses to white noise across a population of ganglion cells. Beyond performance considerations alone, the gross architecture of the LN-LN model maps directly onto the hierarchical, cascaded, anatomy of the retina, thereby enabling the possibility that we can generate quantitative hypotheses about neural signal propagation and computation in the unobserved interior of the retina simply by examining the structure of our model’s interior. Since learning our model only requires measurements of the inputs and outputs to the retinal circuit, this approach is tantamount to the computational reconstruction of unobserved hidden layers of a neural circuit. The advantage of applying this method in the retina is that we can experimentally validate aspects of this computational reconstruction procedure.</p>
<p>Indeed, using intracellular recordings of bipolar cells, we found that our learned subunits matched properties of bipolar cells, both in terms of their receptive field center-surround structure, and in terms of the approximate number of bipolar cells connected to a ganglion-cell. However care must be taken not to directly identify the learned subunits in our model with bipolar cells in the retina. Instead, they should be thought of as functional subunits that reflect the combined contribution of not only bipolar cells, but also horizontal cells and amacrine cells that sculpt the composite response of retinal ganglion cells to stimuli. Nevertheless, the correspondence between subunits and bipolar cell RFs (which are also shaped by horizontal cells), suggests that even for Gaussian white noise stimuli, it is important to learn functional subunits that loosely correspond to the composite effect that bipolar cells and associated circuitry have on ganglion-cell responses.</p>
<p>The interior of our models also reveal several functional principles underlying retinal processing. First, all subunits across all cells had strikingly consistent nonlinearities corresponding to monotonically increasing threshold-like functions with very high thresholds. This inferred biophysical property yields several important consequences for neural signal processing in the inner retina. First, it predicts that subunit activation patterns are sparse across the ensemble of stimuli, with typically only one subunit actively contributing to any given ganglion cell spike. Second, it predicts that the dominant source of stimulus decorrelation, a central tenet of efficient coding theory, has its mechanistic origin at the first strongly nonlinear processing stage of the retina, namely in the synapse from bipolar cells to ganglion cells. Third, it implies that the composite function computed by individual retinal ganglion cells corresponds to a Boolean OR function of bipolar cell feature detectors.</p>
<p>Taken together, the proximal algorithms framework provides a unified way to both estimate hierarchical nonlinear models of sensory processing and compute spike-triggered statistics using limited data. When applied to the retina, these techniques recover aspects of the interior of the retina without requiring direct measurements of retinal interneurons. Moreover, by identifying candidate mechanisms for cascaded nonlinear computation in retinal circuitry, our results provide a higher resolution view of retinal processing compared to classic LN models, thereby setting the stage for the next generation of efficient coding theories that may provide a normative explanation for such processing. For example, considerations of efficient coding have been employed to explain aspects of the linear filter [<xref ref-type="bibr" rid="pcbi.1006291.ref065">65</xref>] and nonlinearity [<xref ref-type="bibr" rid="pcbi.1006291.ref061">61</xref>] of retinal ganglion cells when viewed through the coarse lens of an LN model. An important direction for future research would be the extension of these basic theories to more sophisticated ones that can explain the higher resolution view of retinal processing uncovered by our learned LN-LN models. Principles that underlie such theories of LN-LN processing might include subthreshold noise rejection [<xref ref-type="bibr" rid="pcbi.1006291.ref066">66</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref067">67</xref>], sensitivity to higher order statistical structure in natural scenes, and energy efficiency [<xref ref-type="bibr" rid="pcbi.1006291.ref063">63</xref>]. Indeed the ability to extract these models from data in both a statistically and computationally efficient manner constitutes an important step in the genesis and validation of such a theory.</p>
<p>Another phenomenon robustly observed in the retina is adaptation to the luminance and contrast of the visual scene. Adaptation is thought to be a critical component of the retinal response to natural scenes [<xref ref-type="bibr" rid="pcbi.1006291.ref030">30</xref>], and a promising direction for extensions of our work would be to include luminance and contrast adaptation in subunit models. Luminance adaptation (adapting to the mean light intensity) is mediated by photoreceptor cells, and could be modeled by prepending a simple photoreceptor model (e.g. [<xref ref-type="bibr" rid="pcbi.1006291.ref068">68</xref>]) to an LN-LN model. There are two major sites of contrast adaptation, at the bipolar-to-ganglion cell synapse [<xref ref-type="bibr" rid="pcbi.1006291.ref069">69</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref070">70</xref>] and at the spiking mechanism of ganglion cells [<xref ref-type="bibr" rid="pcbi.1006291.ref069">69</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref071">71</xref>]. Extending the simple thresholding nonlinearities in our model with a dynamical model of adaptation (e. g. [<xref ref-type="bibr" rid="pcbi.1006291.ref019">19</xref>]) is a first step towards understanding the interaction between nonlinear subunits and adaptation.</p>
<p>While our work utilized white noise stimuli, the methods do not require any particular form of stimulus and will thus generalize to other stimulus distributions. In particular, stimuli that differentially activate subunits will be the most effective at differentiating LN and LN-LN models. Stimuli with coarse spatial resolution will not differentially activate subunits within the receptive field, thus are a poor choice for studying nonlinear spatial integration. However, fine textures as present in natural stimuli, are very likely to activate these nonlinear mechanisms in the retina, and thus are a critical component for understanding vision in the context of ethologically relevant stimuli.</p>
<p>The computational motifs identified by LN-LN models are likely to generalize across different species because they rely on a few key properties. For example, our predictions about the primary source of decorrelation in the retina rely on three features of the underlying circuitry identified by LN-LN models: (a) bipolar cell receptive fields are smaller than those of ganglion cells, (b) bipolar cell receptive field centers are largely non-overlapping, and (c) bipolar cell synapses have high thresholds. In addition, the logical OR combination of features relies on high thresholds and bipolar receptive fields that are (largely) non-overlapping. These properties (high threshold subunits with smaller, non-overlapping receptive fields) are common across multiple species.</p>
<p>Beyond the retina, multiple stages of cascaded nonlinear computation constitutes a ubiquitous motif in the structure and function of neural circuits. The tools we have applied here to elucidate hierarchical nonlinear processing in the retina are similarly applicable across neural systems more generally. Thus we hope our work provides mathematical and computational tools for efficiently extracting and analyzing both informative descriptive statistics and hierarchical nonlinear models across many different sensory modalities, brain regions, and stimulus ensembles, thereby furthering our understanding of general principles underlying nonlinear neural computation.</p>
</sec>
<sec id="sec018" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec019">
<title>Ethics statement</title>
<p>This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health, and the Stanford institutional animal care and use committee (IACUC) protocol (11619).</p>
</sec>
<sec id="sec020">
<title>Experiments</title>
<p>Experimental data was collected from the tiger salamander retina using a multi-electrode array (Multi-Channel Systems), as described elsewhere [<xref ref-type="bibr" rid="pcbi.1006291.ref009">9</xref>]. Isolated ganglion cells were identified using custom spike sorting software. The stimulus used was a 100 Hz white noise bars stimulus, where the luminance of each bar was drawn independently from a Gaussian distribution. Spatially, the stimulus spanned approximately 2.8 mm on the retina (50 bars at 55.5 <italic>μm</italic> / bar). Intracellular recordings were performed as described elsewhere [<xref ref-type="bibr" rid="pcbi.1006291.ref072">72</xref>]. Off bipolar cells were identified by their flash response, receptive field size, and level in the retina. Data were analyzed using the <monospace>pyret</monospace> package [<xref ref-type="bibr" rid="pcbi.1006291.ref073">73</xref>]. The experimental data, as well as general purpose code to fit hierarchical LN-LN models and perform regularized STA and STC analysis, are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/baccuslab/inferring-hidden-structure-retinal-circuits" xlink:type="simple">https://github.com/baccuslab/inferring-hidden-structure-retinal-circuits</ext-link>.</p>
</sec>
<sec id="sec021">
<title>LN-LN models</title>
<p>In this section, we specify the mathematical formulation of our LN-LN models. The model takes a spatiotemporal stimulus, represented as a vector <bold>x</bold>, and generates a predicted firing rate, <italic>r</italic>(<bold>x</bold>). First, the stimulus is projected onto a number of subunit filters. The number of subunits is a hyper-parameter of the model, chosen through cross validation (we repeatedly fit models with increasing numbers of subunits until held-out performance on a validation set decreases). If we have <italic>k</italic> subunits, then the stimulus is projected onto each of the <italic>k</italic> filters: <bold>w</bold><sub><italic>i</italic></sub> for <italic>i</italic> = 1, …, <italic>k</italic>. These projections are then passed through separate subunit nonlinearities. The nonlinearities are parameterized using a set of Gaussian basis functions (or bumps) that tile the relevant input space [<xref ref-type="bibr" rid="pcbi.1006291.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref020">20</xref>]. This parameterization enforces smoothness of the nonlinearity. We typically use <italic>p</italic> = 30 evenly spaced Gaussian bumps that tile the range spanned by the projection of the stimulus onto the linear filter (results were not sensitive to the number of bumps over a range of 10–30 bumps). For example, a nonlinearity <italic>h</italic>(<italic>u</italic>) is parameterized as
<disp-formula id="pcbi.1006291.e014"><alternatives><graphic id="pcbi.1006291.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>h</mml:mi> <mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>p</mml:mi></mml:munderover> <mml:msub><mml:mi>a</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>p</mml:mi></mml:munderover> <mml:msub><mml:mi>a</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mi>ϕ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mo>Δ</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>ϕ</italic> is the basis function, e.g. <italic>ϕ</italic>(<italic>x</italic>) = exp(−<italic>x</italic><sup>2</sup>), Δ<sub><italic>j</italic></sub> indicates the spacing between the basis functions, <italic>p</italic> is the number of bases used, and <italic>a</italic><sub><italic>j</italic></sub> is a weight on that particular basis function. Since the basis functions and spacings are fixed beforehand, the only free parameters are the <italic>a</italic><sub><italic>j</italic></sub>’s. For subunit <italic>i</italic>, the corresponding nonlinearity has a set of weights <italic>a</italic><sub><italic>ij</italic></sub> for <italic>j</italic> = 1, …, <italic>p</italic>.</p>
<p>The output of the <italic>k</italic> subunits is then summed and passed through a final nonlinearity. This final nonlinearity is parameterized as a soft rectifying function <italic>r</italic>(<italic>x</italic>) = <italic>g</italic> log(1 + <italic>e</italic><sup><italic>x</italic>−<italic>θ</italic></sup>), with two parameters: <italic>g</italic> is an overall gain and <italic>θ</italic> is the threshold. The full LN-LN model is then given by:
<disp-formula id="pcbi.1006291.e015"><alternatives><graphic id="pcbi.1006291.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>g</mml:mi> <mml:mo form="prefix">log</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>[</mml:mo> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>k</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>p</mml:mi></mml:munderover> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mi>ϕ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>i</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mo>Δ</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>]</mml:mo> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where the parameters to optimize are the subunit filters <bold>w</bold><sub><italic>i</italic></sub> for <italic>i</italic> = 1, …, <italic>k</italic>, subunit nonlinearity weights <italic>a</italic><sub><italic>ij</italic></sub> for <italic>j</italic> = 1, …, <italic>p</italic>, and final nonlinearity parameters <italic>θ</italic> and <italic>g</italic>.</p>
<p>We optimize the parameters using a maximum likelihood objective assuming a Poisson noise model for spiking. Rather than optimize all of the parameters simultaneously, we alternate between optimizing blocks of parameters (joint optimization using gradient descent was prone to getting stuck at solutions that were less accurate). That is, we alternate between optimizing three blocks of parameters: the subunit filters <bold>w</bold><sub><italic>i</italic></sub>, the subunit nonlinearities <italic>a</italic><sub><italic>ij</italic></sub>, and the final nonlinearity parameterized by <italic>θ</italic> and <italic>g</italic>. We optimize each block of parameters by minimizing the negative log-likelihood of the data plus any regularization terms using proximal algorithms. The subunit filters are the only parameters with regularization penalties (the nuclear norm applied to the filter reshaped as a spatiotemporal matrix and the <italic>ℓ</italic><sub>1</sub> norm), to encourage space-time separability and sparseness of the filters. The proximal operator for each of these regularization penalties is given in <xref ref-type="table" rid="pcbi.1006291.t001">Table 1</xref>, and the proximal operator for the log-likelihood term (which does not have a closed-form solution) is solved using gradient descent. In addition, after optimizing the block of parameters corresponding to the subunit filters, we rescale them to have unit norm before continuing the alternating minimization scheme. This ensures that the distribution of input to the nonlinearities spans the same range, and gets rid of an ambiguity between the scale of the subunit filters and the scale of the domain of the subunit nonlinearity. We find that the parameters converge after several rounds of alternating minimization, and are robust with respect to random initialization of the parameters.</p>
</sec>
<sec id="sec022">
<title>Proximal operators and algorithms</title>
<p>The framework of <italic>proximal algorithms</italic> allows us to efficiently optimize functions with non-smooth terms. The name <italic>proximal</italic> comes from the fact that these algorithms utilize the <italic>proximal operator</italic> (defined below) as subroutines or steps in the optimization algorithm. For brevity, we skip the derivation of these algorithms, instead referring the reader to the more thorough treatment by Parikh and Boyd [<xref ref-type="bibr" rid="pcbi.1006291.ref074">74</xref>] or Polson et al. [<xref ref-type="bibr" rid="pcbi.1006291.ref075">75</xref>]. The proximal operator for a function <italic>ϕ</italic> given a starting point <italic>v</italic> is defined as:
<disp-formula id="pcbi.1006291.e016"><alternatives><graphic id="pcbi.1006291.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi> <mml:mi>ϕ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>v</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>argmin</mml:mtext> <mml:mi>x</mml:mi></mml:msub> <mml:mo>[</mml:mo> <mml:mi>ϕ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi>ρ</mml:mi> <mml:mn>2</mml:mn></mml:mfrac> <mml:msubsup><mml:mrow><mml:mo>∥</mml:mo> <mml:mi>x</mml:mi> <mml:mo>-</mml:mo> <mml:mi>v</mml:mi> <mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>]</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
The proximal operator is a mapping from a starting point <italic>v</italic> to a new point <italic>x</italic> that tries to minimize the function <italic>ϕ</italic>(<italic>x</italic>) (first term above) but stays close to the starting point <italic>v</italic> (second term), where the parameter <italic>ρ</italic> trades off between these two objectives. The proximal operator is a building block that we will use to create more complicated algorithms. We will take advantage of the fact that for many functions <italic>ϕ</italic> of interest to us, we can analytically compute their proximal operators, thus making these operators a computationally cheap building block.</p>
<p>We used these building blocks to solve optimization problems involving the sum of a number of simpler terms:
<disp-formula id="pcbi.1006291.e017"><alternatives><graphic id="pcbi.1006291.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mo form="prefix" movablelimits="true">min</mml:mo> <mml:mi>x</mml:mi></mml:munder> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>k</mml:mi></mml:munderover> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where in our application the <italic>ϕ</italic><sub><italic>i</italic></sub>’s represent either a model fitting objective (e.g. a log-likelihood) or different regularization penalties on the parameters, <italic>x</italic>. For example, for learning the parameters of a linear filter in an LN model, the objective consists of a log-likelihood <italic>f</italic>(<italic>x</italic>) along with regularization penalties that impose prior beliefs about the filter, <italic>x</italic>. We focus on two main penalties. Sparsity, which encodes the belief that many filter coefficients are zero, is penalized by the <italic>ℓ</italic><sub>1</sub>-norm (<italic>ϕ</italic><sub>1</sub>(<italic>x</italic>) = ‖<italic>x</italic>‖<sub>1</sub>). Additionally, spatiotemporal filters are often approximately space-time separable (they are well modeled as the outer product of a few spatial and temporal factors). We encoded this penalty by the nuclear norm, <italic>ℓ</italic><sub>*</sub>, which encourages the parameters <italic>x</italic>, when reshaped to form a spatiotemporal matrix, to be a low-rank matrix (the nuclear norm <italic>ℓ</italic><sub>*</sub> of a matrix is simply the sum of its singular values). Another natural penalty would be one that encourages the parameters to be smooth in space and/or time, which could be accomplished by applying an <italic>ℓ</italic><sub>1</sub> or <italic>ℓ</italic><sub>2</sub> penalty to the spatial or temporal differences in parameters. As shown below, these types of penalties are easy to incorporate into the proximal algorithm framework. Other commonly used regularization penalties, and their corresponding proximal operators, are listed in <xref ref-type="table" rid="pcbi.1006291.t001">Table 1</xref>.</p>
<p>The proximal consensus algorithm is an iterative algorithm for solving (<xref ref-type="disp-formula" rid="pcbi.1006291.e017">2</xref>) that takes a series of proximal operator steps. It first creates a copy of the variable <italic>x</italic> for each term <italic>ϕ</italic><sub><italic>i</italic></sub> in the objective. The algorithm proceeds by alternating between taking proximal steps for each function <italic>ϕ</italic><sub><italic>i</italic></sub> using that variable copy <italic>x</italic><sub><italic>i</italic></sub>, and then enforcing all of the different variable copies to agree (reach consensus) by averaging them. The algorithm is:
<disp-formula id="pcbi.1006291.e018"><alternatives><graphic id="pcbi.1006291.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>u</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>k</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>k</mml:mi></mml:munderover> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi>u</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>i</italic> indexes each of the terms in the objective function, <italic>x</italic><sub><italic>i</italic></sub> is a copy of the variable, <inline-formula id="pcbi.1006291.e019"><alternatives><graphic id="pcbi.1006291.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is the average of the variable copies, and <italic>u</italic><sub><italic>i</italic></sub> is a dual variable that can be thought of as keeping a running average of the error between each variable copy and the average. Intuitively, we can think of each variable copy <italic>x</italic><sub><italic>i</italic></sub> as trying to minimize a single term <italic>ϕ</italic><sub><italic>i</italic></sub> in the objective, and the average, or consensus <inline-formula id="pcbi.1006291.e020"><alternatives><graphic id="pcbi.1006291.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> forces the different copies to agree. After convergence, each copy <italic>x</italic><sub><italic>i</italic></sub> will be close to the mean value <inline-formula id="pcbi.1006291.e021"><alternatives><graphic id="pcbi.1006291.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, which is the set of parameters that minimizes the original composite objective.</p>
<p>This algorithm has a number of desirable properties. First, the updates for each term <italic>x</italic><sub><italic>i</italic></sub> can be carried out in parallel, therefore allowing for speedups when run on a cluster or multi-core computer. Second, it converges even when terms in the objective are non-differentiable. Due to the repeated application of the proximal operator, this algorithm works best when the terms <italic>ϕ</italic><sub><italic>i</italic></sub> have proximal operators that are easy to compute.</p>
<p>This is exactly the case for the regularization terms described above: for the <italic>ℓ</italic><sub>1</sub> norm, the proximal operator corresponds to soft thresholding of the parameters. For the nuclear norm, the proximal operator corresponds to soft thresholding of the singular values of parameters reshaped as a matrix. Occasionally, the proximal operator may not have a closed form solution. In this case, the proximal step can be carried out through gradient based optimization of (<xref ref-type="disp-formula" rid="pcbi.1006291.e016">1</xref>) directly. This is the case for some log-likelihoods, such as the log-likelihood of a particular firing rate under Poisson spiking. In this case, gradient step based optimization of (<xref ref-type="disp-formula" rid="pcbi.1006291.e016">1</xref>) often dominates the computational cost of the algorithm. As many methods for fitting neural models involve gradient step updates on the log-likelihood, such methods can then be augmented with additional regularization terms with no appreciable effect on runtime, by using proximal consensus algorithms for optimization. Our code for solving formulating and solving optimization problems using proximal algorithms is provided online at <ext-link ext-link-type="uri" xlink:href="https://github.com/ganguli-lab/proxalgs" xlink:type="simple">https://github.com/ganguli-lab/proxalgs</ext-link>.</p>
</sec>
<sec id="sec023">
<title>Relationship between descriptive statistics and encoding models</title>
<p>Here, we derive the relationship between the pathways of any differentiable encoding model and spike-triggered statistics under Gaussian noise stimulation. We represent a visual stimulus as an <italic>N</italic> dimensional vector <bold>x</bold>. We view a functional neural model as an arbitrary nonlinear function <italic>r</italic> = <italic>f</italic>(<bold>x</bold>), over <italic>N</italic> dimensional stimulus space, where <italic>r</italic> determines the probability that the neuron fires in a small time window following a stimulus <bold>x</bold>: <italic>r</italic>(<bold>x</bold>) = <italic>p</italic>(spike ∣ <bold>x</bold>). The derivation will show how the STA is related to the <italic>gradient</italic> of the model ∇<italic>r</italic>(<bold>x</bold>), and the STC is related to the <italic>Hessian</italic>, ∇<sup>2</sup><italic>r</italic>(<bold>x</bold>).</p>
<p>The STA and STC are the mean and covariance, respectively, of the spike-triggered stimulus ensemble, which reflects the collection of stimuli preceding each spike [<xref ref-type="bibr" rid="pcbi.1006291.ref003">3</xref>]. This distribution over stimuli, conditioned on a spike occurring, can be expressed via Bayes rule,
<disp-formula id="pcbi.1006291.e022"><alternatives><graphic id="pcbi.1006291.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∣</mml:mo> <mml:mtext>spike</mml:mtext> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mtext>spike</mml:mtext> <mml:mo>∣</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mtext>spike</mml:mtext> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where <italic>p</italic>(<bold>x</bold>) is the prior distribution over stimuli and <italic>p</italic>(spike) is the average firing probability over all stimuli. Here, we assume a white noise stimulus distribution, in which each component of <bold>x</bold> is chosen independently from a Gaussian distribution with zero mean and unit variance. The STA and STC are given by
<disp-formula id="pcbi.1006291.e023"><alternatives><graphic id="pcbi.1006291.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>s</mml:mi> <mml:mi>p</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi> <mml:mi>e</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula> <disp-formula id="pcbi.1006291.e024"><alternatives><graphic id="pcbi.1006291.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold">C</mml:mi> <mml:mtext>STC</mml:mtext></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>s</mml:mi> <mml:mi>p</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi> <mml:mi>e</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
Focusing first on the STA:
<disp-formula id="pcbi.1006291.e025"><alternatives><graphic id="pcbi.1006291.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∫</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∣</mml:mo> <mml:mtext>spike</mml:mtext> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>μ</mml:mi></mml:mfrac> <mml:mo>∫</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>μ</mml:mi></mml:mfrac> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>μ</mml:mi></mml:mfrac> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mo>∇</mml:mo> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
where <italic>μ</italic> = <italic>p</italic>(spike) is the overall probability of spiking. The last step in the derivation uses Stein’s lemma, which states that <inline-formula id="pcbi.1006291.e026"><alternatives><graphic id="pcbi.1006291.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mo>∇</mml:mo> <mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> if the expectation is taken over a multivariate Gaussian distribution with identity covariance matrix, corresponding to our white noise stimulus assumption. This calculation thus yields the simple statement that the spike-triggered average is proportional to the gradient (or gain) of the response function, averaged over the input distribution [<xref ref-type="bibr" rid="pcbi.1006291.ref064">64</xref>]. Applying Stein’s lemma again yields an expression for the STC matrix:
<disp-formula id="pcbi.1006291.e027"><alternatives><graphic id="pcbi.1006291.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold">C</mml:mi> <mml:mtext>STC</mml:mtext></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∫</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∣</mml:mo> <mml:mtext>spike</mml:mtext> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>μ</mml:mi></mml:mfrac> <mml:mo>∫</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>μ</mml:mi></mml:mfrac> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>μ</mml:mi></mml:mfrac> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msup><mml:mo>∇</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
Intuitively, these results state that the STA is related to the slope (first derivative) and the STC is related to the Hessian curvature (matrix of second derivatives) of the multi-dimensional nonlinear response function <italic>r</italic>(<bold>x</bold>).</p>
<p>For example, consider a linear-nonlinear model <italic>r</italic> = <italic>f</italic>(<bold>w</bold><sup><italic>T</italic></sup> <bold>x</bold>) which has the following gradient: ∇<italic>r</italic>(<bold>x</bold>) = <italic>f</italic>′(<bold>w</bold><sup><italic>T</italic></sup> <bold>x</bold>)<bold>w</bold> and Hessian: ∇<sup>2</sup><italic>r</italic>(<bold>x</bold>) = <italic>f</italic>″(<bold>w</bold><sup><italic>T</italic></sup> <bold>x</bold>)<bold>w</bold><bold>w</bold><sup><italic>T</italic></sup>. Plugging these expressions into Eqs (<xref ref-type="disp-formula" rid="pcbi.1006291.e025">6</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1006291.e027">7</xref>) reveals that the STA is proportional to <bold>w</bold> and the STC is proportional to <bold>w</bold><bold>w</bold><sup><italic>T</italic></sup>. Therefore, we recover the known result [<xref ref-type="bibr" rid="pcbi.1006291.ref002">2</xref>] that the STA of the LN model is proportional to the linear filter, and there will be one significant direction in the STC, which is also proportional to the linear filter (with mild assumptions on the nonlinearity, <italic>f</italic>, to ensure that slope and curvature terms in (<xref ref-type="disp-formula" rid="pcbi.1006291.e025">6</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1006291.e027">7</xref>) are non-zero).</p>
<p>We can extend this to the case of a multilayered circuit with <italic>k</italic> pathways, each of which first filters the stimulus with a filter <bold>w</bold><sub>1</sub> … <bold>w</bold><sub><italic>k</italic></sub>. Regardless of how these pathways are then combined, we can write this circuit computation as <italic>r</italic> = <italic>f</italic>(<bold>W</bold><sup><italic>T</italic></sup> <bold>x</bold>) where <bold>W</bold> is a matrix whose columns are the <italic>k</italic> pathway filters, and <italic>f</italic> is a <italic>k</italic>-dimensional time-independent (static) nonlinear function. We can think of the <italic>k</italic> dimensional vector <bold>u</bold> = <bold>W</bold><sup><italic>T</italic></sup> <bold>x</bold> as the activity pattern across each of the <italic>k</italic> pathways before any nonlinearity. The gradient for such a model is ∇<italic>r</italic>(<bold>x</bold>) = <bold>W</bold><sup><italic>T</italic></sup><bold>∇f</bold>(<bold>u</bold>), where <bold>∇f</bold>(<bold>u</bold>) is the gradient of the <italic>k</italic>-dimensional nonlinearity. Using <xref ref-type="disp-formula" rid="pcbi.1006291.e025">Eq (6)</xref>, the STA is then a linear combination of the pathway filters:
<disp-formula id="pcbi.1006291.e028"><alternatives><graphic id="pcbi.1006291.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>μ</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>k</mml:mi></mml:munderover> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi mathvariant="bold">i</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where the weights are given by
<disp-formula id="pcbi.1006291.e029"><alternatives><graphic id="pcbi.1006291.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>∂</mml:mi> <mml:msub><mml:mi>u</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>]</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
and correspond to the average sensitivity, or slope of the neural response <italic>r</italic> with respect to changes in the activity of the <italic>i</italic><sup><italic>th</italic></sup> filter.</p>
<p>The Hessian for the multilayered model is ∇<sup>2</sup><italic>r</italic>(<bold>x</bold>) = <bold>W∇</bold><sup><bold>2</bold></sup><bold>fW</bold><sup><bold>T</bold></sup>, where <bold>∇</bold><sup><bold>2</bold></sup><bold>f</bold> is the <italic>k</italic>-by-<italic>k</italic> matrix of second derivatives of the <italic>k</italic>-dimensional nonlinearity <italic>f</italic>(<bold>u</bold>). From <xref ref-type="disp-formula" rid="pcbi.1006291.e027">Eq (7)</xref>, the STC is then given by:
<disp-formula id="pcbi.1006291.e030"><alternatives><graphic id="pcbi.1006291.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e030" xlink:type="simple"/><mml:math display="block" id="M30"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">C</mml:mi> <mml:mtext>STC</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mi>μ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:mi mathvariant="bold">W</mml:mi> <mml:mi mathvariant="bold">H</mml:mi> <mml:msup><mml:mi mathvariant="bold">W</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
where the <italic>k</italic>-by-<italic>k</italic> matrix <bold>H</bold> is:
<disp-formula id="pcbi.1006291.e031"><alternatives><graphic id="pcbi.1006291.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">H</mml:mi> <mml:mo>=</mml:mo> <mml:mi>μ</mml:mi> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msup><mml:mo>∇</mml:mo> <mml:mn mathvariant="bold">2</mml:mn></mml:msup> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mo>∇</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mo>∇</mml:mo> <mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
This expression implies that nontrivial directions in the column space of <bold>C</bold><sub>STC</sub> correspond to (span the same space as) the column space of <bold>W</bold>. Therefore, the significant eigenvectors of the STC matrix will be linear combinations of the <italic>k</italic> pathway filters, and the number of significant eigenvectors is at most <italic>k</italic>.</p>
<p>Note that Eqs (<xref ref-type="disp-formula" rid="pcbi.1006291.e025">6</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1006291.e027">7</xref>) are valid for <italic>any</italic> differentiable model, including those with more than two layers, divisive interactions, feedback, and so on.</p>
<sec id="sec024">
<title>Regularized STA</title>
<p>To compute a regularized STA, without explicitly building an encoding model, we can form an optimization problem that directly denoises the STA:
<disp-formula id="pcbi.1006291.e032"><alternatives><graphic id="pcbi.1006291.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e032" xlink:type="simple"/><mml:math display="block" id="M32"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mtext>STA</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">min</mml:mo> <mml:mi mathvariant="bold">x</mml:mi></mml:munder> <mml:msubsup><mml:mrow><mml:mo>∥</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>STA</mml:mtext></mml:msub> <mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:msub><mml:mi>γ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(9)</label></disp-formula>
Here, <italic>ϕ</italic><sub><italic>i</italic></sub>(<italic>x</italic>) are the regularization penalty functions, with an associated regularization weight <italic>γ</italic><sub><italic>i</italic></sub>, and <bold>x</bold><sub>STA</sub> is the raw (sample) STA from recorded data (which is noisy due to finite sampling). Note that we use mean squared error to quantify distance from the raw estimate, but other loss functions may be also used. For the penalty functions <italic>ϕ</italic><sub><italic>i</italic></sub>, we use an <italic>ℓ</italic><sub>1</sub> penalty that encourages the estimated filter to be sparse (few non-zero coefficients), and a nuclear norm penalty, which is the sum of the singular values of the spatiotemporal filter <bold>x</bold> when viewed as a spatiotemporal matrix. The nuclear norm penalty is advantageous compared to explicitly forcing the spacetime filter <bold>x</bold> to be low-rank, as it is a “soft” penalty which allows for many small singular values, whereas explicitly forcing the filter to be low-rank forces those to be zero.</p>
</sec>
<sec id="sec025">
<title>Regularized STC analysis</title>
<p>The STC eigenvectors are obtained by an eigendecomposition of the STC matrix <bold>C</bold> [<xref ref-type="bibr" rid="pcbi.1006291.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1006291.ref076">76</xref>], which is equivalent to solving an optimization problem:
<disp-formula id="pcbi.1006291.e033"><alternatives><graphic id="pcbi.1006291.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e033" xlink:type="simple"/><mml:math display="block" id="M33"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>maximize</mml:mtext> <mml:mspace width="1.em"/><mml:mtext>Tr</mml:mtext> <mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">U</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">C</mml:mi> <mml:mi mathvariant="bold">U</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>subject</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext> <mml:mspace width="1.em"/><mml:msup><mml:mi mathvariant="bold">U</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">U</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">I</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
where <bold>U</bold> denotes a matrix whose columns are the orthonormal eigenvectors of <bold>C</bold>. In order to regularize these eigenvectors, we wish to add penalty terms to (<xref ref-type="disp-formula" rid="pcbi.1006291.e033">10</xref>), which precludes a closed form solution to the problem. We circumvent this by reformulating the problem using a convex relaxation. First, we consider the matrix <bold>X</bold> = <bold>UU</bold><sup><italic>T</italic></sup>, corresponding to the outer product of the eigenvectors. Because of the cyclic property of the trace, namely that Tr(<bold>U</bold><sup><italic>T</italic></sup><bold>CU</bold>) = Tr(<bold>UU</bold><sup><italic>T</italic></sup><bold>C</bold>) = Tr(<bold>XC</bold>), the function to be optimized in (<xref ref-type="disp-formula" rid="pcbi.1006291.e033">10</xref>) depends on the eigenvector matrix <bold>U</bold> only through the combination <bold>X</bold> = <bold>UU</bold><sup><italic>T</italic></sup>. Thus we can directly optimize over the variable <bold>X</bold>. However, the non-convex equality constraint <bold>U</bold><sup><italic>T</italic></sup><bold>U</bold> = <bold>I</bold> in (<xref ref-type="disp-formula" rid="pcbi.1006291.e033">10</xref>) is not easily expressible in terms of <bold>X</bold>. <bold>X</bold> is however a projection operator, obeying <bold>X</bold><sup>2</sup> = <bold>X</bold>. We replace this with the constraint that <bold>X</bold> should be contained within the convex hull of the set of rank-<italic>d</italic> projection matrices. This space of matrices is a convex body known as the <italic>fantope</italic> [<xref ref-type="bibr" rid="pcbi.1006291.ref077">77</xref>].</p>
<p>The advantage of this formulation is that we obtain a convex optimization problem which can be further augmented with additional functions that penalize the columns of <bold>X</bold> to impose prior knowledge about the structure of the eigenvectors of <bold>C</bold>. Columns of <bold>X</bold> are linear combinations of the eigenvectors of <bold>C</bold>, which are themselves linear combinations of the small set of spatiotemporal filters we are interested in identifying. Therefore, if we expect the spatiotemporal filters of individual biological pathways to have certain structure (for example, smooth, low-rank, or sparse), then we also expect to see those properties in both the eigenvectors and in the columns of <bold>X</bold>.</p>
<p>Putting this logic together, to obtain <italic>regularized</italic> STC eigenvectors, we solve the following convex optimization problem:
<disp-formula id="pcbi.1006291.e034"><alternatives><graphic id="pcbi.1006291.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mi mathvariant="bold">X</mml:mi></mml:munder> <mml:mspace width="1.em"/><mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mi mathvariant="bold">C</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>γ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>subject</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext> <mml:mspace width="1.em"/><mml:mi>X</mml:mi> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="script">F</mml:mi> <mml:mi>d</mml:mi></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
Here <bold>C</bold> is the raw (sample) STC matrix, which is again noisy due to limited recorded data, and <inline-formula id="pcbi.1006291.e035"><alternatives><graphic id="pcbi.1006291.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:msup><mml:mi mathvariant="script">F</mml:mi> <mml:mi>d</mml:mi></mml:msup></mml:math></alternatives></inline-formula> denotes the fantope, or convex hull of all rank <italic>d</italic> projection matrices. Each <italic>ϕ</italic><sub><italic>i</italic></sub> is a regularization penalty function applied to each of the columns of <bold>X</bold>; i.e. <inline-formula id="pcbi.1006291.e036"><alternatives><graphic id="pcbi.1006291.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>j</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where <bold>x</bold><sup><italic>j</italic></sup> denotes the <italic>j</italic>’th column of <bold>X</bold>. Again, we can solve this optimization problem efficiently using proximal consensus algorithms, described above. Common regularization penalties and their corresponding proximal operators are shown in <xref ref-type="table" rid="pcbi.1006291.t001">Table 1</xref>. The optimization yields a matrix <inline-formula id="pcbi.1006291.e037"><alternatives><graphic id="pcbi.1006291.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> in the fantope <inline-formula id="pcbi.1006291.e038"><alternatives><graphic id="pcbi.1006291.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:msup><mml:mi mathvariant="script">F</mml:mi> <mml:mi>d</mml:mi></mml:msup></mml:math></alternatives></inline-formula>, which may itself have rank higher than <italic>d</italic>, so we perform a final eigendecomposition of this matrix to obtain its top-eigenvectors. These eigenvectors constitute our regularized estimate of the eigenvectors of the significant (expansive) STC eigenvectors (to find suppressive directions, one could invert <bold>C</bold> in (<xref ref-type="disp-formula" rid="pcbi.1006291.e034">11</xref>)). A major computational advantage of this formulation is that we only need to store and work with the <italic>N</italic> by <italic>N</italic> raw STC covariance matrix itself, without ever needing access to the spike-triggered ensemble, an <italic>N</italic> by <italic>M</italic> matrix where <italic>M</italic> (the number of spikes) is typically much greater than <italic>N</italic>.</p>
</sec>
</sec>
<sec id="sec026">
<title>Subspace overlap</title>
<p>We quantify the overlap between two subspaces as the average of the cosine of the principal (or canonical) angles between the subspaces. The principal angles between two subspaces <inline-formula id="pcbi.1006291.e039"><alternatives><graphic id="pcbi.1006291.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="script">R</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>×</mml:mo> <mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006291.e040"><alternatives><graphic id="pcbi.1006291.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="script">R</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>×</mml:mo> <mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> generalize the idea of angles between vectors. Here we describe a pair of <italic>p</italic> and <italic>q</italic> dimensional subspaces in <italic>n</italic> dimensional space as the span of the columns of the matrices <bold>X</bold> and <bold>Y</bold>. Assuming without loss of generality that <italic>p</italic> ≤ <italic>q</italic>, then we have <italic>p</italic> principal angles <italic>θ</italic><sub>1</sub>, …, <italic>θ</italic><sub><italic>p</italic></sub> that are defined recursively for <italic>k</italic> = 1, …, <italic>p</italic> as:
<disp-formula id="pcbi.1006291.e041"><alternatives><graphic id="pcbi.1006291.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e041" xlink:type="simple"/><mml:math display="block" id="M41"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo form="prefix">cos</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:munder> <mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:munder> <mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi mathvariant="bold">k</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mi>T</mml:mi></mml:msup> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">k</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
subject to the constraints that the vectors are unit vectors (<bold>x</bold><sup><italic>T</italic></sup> <bold>x</bold> = <bold>y</bold><sup><italic>T</italic></sup> <bold>y</bold> = 1) and are orthogonal to the previously identified vectors (<inline-formula id="pcbi.1006291.e042"><alternatives><graphic id="pcbi.1006291.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi mathvariant="bold">j</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi mathvariant="bold">j</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for <italic>j</italic> = 1, 2, …, <italic>k</italic> − 1). That is, the first principal angle is found by identifying a unit vector within each subspace such that the correlation, or dot product, between these vectors (these are known as the principal vectors) is maximized. This principal angle is then the inverse cosine of the dot product. Each subsequent principal angle is found by performing the same maximization but restricting each new pair of vectors to be orthogonal to the previous principal vectors in each subspace. The principal angles can be efficiently computed via the QR decomposition [<xref ref-type="bibr" rid="pcbi.1006291.ref078">78</xref>]. We define subspace overlap as the average of the cosine of the principal angles, <inline-formula id="pcbi.1006291.e043"><alternatives><graphic id="pcbi.1006291.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006291.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>p</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>p</mml:mi></mml:msubsup> <mml:mo form="prefix">cos</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. This quantity is at most 1 (for two subspaces that span the same space), and at least 0 (for two orthogonal subspaces that share no common directions).</p>
</sec>
</sec>
<sec id="sec027">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006291.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006291.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Example bipolar receptive fields.</title>
<p>Each panel shows a spatiotemporal receptive field of an OFF bipolar cell recorded intracellularly from the salamander retina.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006291.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006291.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Subunit and retinal ganglion cell types.</title>
<p>Cell type classification for salamander ganglion cell and subunit filters. (a) K-Means clustering applied to the temporal kernel (temporal component of the spatiotemporal receptive field) of n = 23 recorded retinal ganglion cells. (b) K-Means clustering applied to temporal kernels of n = 92 model-identified subunits. (c) Frequency of the different cell types, both for RGCs and subunits.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors would like to thank Ben Naecker, Ben Poole, and Lane McIntosh for discussions as well as Stéphane Deny, Tim Gollisch, Ben Naecker, and Alex Williams for comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006291.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chichilnisky</surname> <given-names>E</given-names></name>. <article-title>A simple white noise analysis of neuronal light responses</article-title>. <source>Network: Computation in Neural Systems</source>. <year>2001</year>;<volume>12</volume>(<issue>2</issue>):<fpage>199</fpage>–<lpage>213</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/713663221" xlink:type="simple">10.1080/713663221</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Convergence properties of three spike-triggered analysis techniques</article-title>. <source>Network: Comput Neural Syst</source>. <year>2003</year>;<volume>14</volume>:<fpage>437</fpage>–<lpage>464</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/0954-898X/14/3/304" xlink:type="simple">10.1088/0954-898X/14/3/304</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Spike-triggered neural characterization</article-title>. <source>Journal of Vision</source>. <year>2006</year>;<volume>6</volume>(<issue>4</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/6.4.13" xlink:type="simple">10.1167/6.4.13</ext-link></comment> <object-id pub-id-type="pmid">16889482</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Aljadeff</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lansdell</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Fairhall</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Kleinfeld</surname> <given-names>D</given-names></name>. <article-title>Analysis of neuronal spike trains, deconstructed</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>91</volume>(<issue>2</issue>):<fpage>221</fpage>–<lpage>259</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.05.039" xlink:type="simple">10.1016/j.neuron.2016.05.039</ext-link></comment> <object-id pub-id-type="pmid">27477016</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Van Steveninck</surname> <given-names>RDR</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Real-time performance of a movement-sensitive neuron in the blowfly visual system: coding and information transfer in short spike sequences</article-title>. <source>Proceedings of the Royal Society of London B: Biological Sciences</source>. <year>1988</year>;<volume>234</volume>(<issue>1277</issue>):<fpage>379</fpage>–<lpage>414</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rspb.1988.0055" xlink:type="simple">10.1098/rspb.1988.0055</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Agüera y Arcas</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Fairhall</surname> <given-names>AL</given-names></name>. <article-title>What causes a neuron to spike?</article-title> <source>Neural Computation</source>. <year>2003</year>;<volume>15</volume>(<issue>8</issue>):<fpage>1789</fpage>–<lpage>1807</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/08997660360675044" xlink:type="simple">10.1162/08997660360675044</ext-link></comment> <object-id pub-id-type="pmid">14511513</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brenner</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Van Steveninck</surname> <given-names>RdR</given-names></name>. <article-title>Adaptive rescaling maximizes information transmission</article-title>. <source>Neuron</source>. <year>2000</year>;<volume>26</volume>(<issue>3</issue>):<fpage>695</fpage>–<lpage>702</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0896-6273(00)81205-2" xlink:type="simple">10.1016/S0896-6273(00)81205-2</ext-link></comment> <object-id pub-id-type="pmid">10896164</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Characterizing neural gain control using spike-triggered covariance</article-title>. <source>Advances in neural information processing systems</source>. <year>2002</year>;<volume>1</volume>:<fpage>269</fpage>–<lpage>276</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Baccus</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name>. <article-title>Fast and slow contrast adaptation in retinal circuitry</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>36</volume>(<issue>5</issue>):<fpage>909</fpage>–<lpage>919</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0896-6273(02)01050-4" xlink:type="simple">10.1016/S0896-6273(02)01050-4</ext-link></comment> <object-id pub-id-type="pmid">12467594</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fairhall</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Burlingame</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Narasimhan</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Puchalla</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>MJ</given-names></name>. <article-title>Selectivity for Multiple Stimulus Features in Retinal Ganglion Cells</article-title>. <source>J Neurophysiol</source>. <year>2006</year>;<volume>96</volume>(<issue>5</issue>):<fpage>2724</fpage>–<lpage>2738</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00995.2005" xlink:type="simple">10.1152/jn.00995.2005</ext-link></comment> <object-id pub-id-type="pmid">16914609</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Spatiotemporal Elements of Macaque {V1} Receptive Fields</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>46</volume>(<issue>6</issue>):<fpage>945</fpage>–<lpage>956</lpage>. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2005.05.021" xlink:type="simple">http://dx.doi.org/10.1016/j.neuron.2005.05.021</ext-link>. <object-id pub-id-type="pmid">15953422</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pillow</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Shlens</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sher</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>E</given-names></name>, <etal>et al</etal>. <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source>. <year>2008</year>;<volume>454</volume>(<issue>7207</issue>):<fpage>995</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature07140" xlink:type="simple">10.1038/nature07140</ext-link></comment> <object-id pub-id-type="pmid">18650810</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Heitman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Brackbill</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Greschner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sher</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>E</given-names></name>. <article-title>Testing pseudo-linear models of responses to natural scenes in primate retina</article-title>. <source>bioRxiv</source>. <year>2016</year>; p. <fpage>045336</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref014">
<label>14</label>
<mixed-citation publication-type="other" xlink:type="simple">Latimer KW, Chichilnisky E, Rieke F, Pillow JW. Inferring synaptic conductances from spike trains with a biophysically inspired point process model. In: Advances in Neural Information Processing Systems; 2014. p. 954–962.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Demb</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Haarsma</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Freed</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Sterling</surname> <given-names>P</given-names></name>. <article-title>Functional circuitry of the retinal ganglion cell’s nonlinear receptive field</article-title>. <source>Journal of Neuroscience</source>. <year>1999</year>;<volume>19</volume>(<issue>22</issue>):<fpage>9756</fpage>–<lpage>9767</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.19-22-09756.1999" xlink:type="simple">10.1523/JNEUROSCI.19-22-09756.1999</ext-link></comment> <object-id pub-id-type="pmid">10559385</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>PH</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Greschner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ahn</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Gunning</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Mathieson</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Retinal representation of the elementary visual signal</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>81</volume>(<issue>1</issue>):<fpage>130</fpage>–<lpage>139</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.10.043" xlink:type="simple">10.1016/j.neuron.2013.10.043</ext-link></comment> <object-id pub-id-type="pmid">24411737</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gollisch</surname> <given-names>T</given-names></name>. <article-title>Features and functions of nonlinear spatial integration by retinal ganglion cells</article-title>. <source>Journal of Physiology-Paris</source>. <year>2013</year>;<volume>107</volume>(<issue>5</issue>):<fpage>338</fpage>–<lpage>348</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jphysparis.2012.12.001" xlink:type="simple">10.1016/j.jphysparis.2012.12.001</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>McFarland</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Cui</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Butts</surname> <given-names>DA</given-names></name>. <article-title>Inferring nonlinear neuronal computation based on physiologically plausible inputs</article-title>. <source>PLoS computational biology</source>. <year>2013</year>;<volume>9</volume>(<issue>7</issue>):<fpage>e1003143</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003143" xlink:type="simple">10.1371/journal.pcbi.1003143</ext-link></comment> <object-id pub-id-type="pmid">23874185</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ozuysal</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Baccus</surname> <given-names>SA</given-names></name>. <article-title>Linking the computational structure of variance adaptation to biophysical mechanisms</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>73</volume>(<issue>5</issue>):<fpage>1002</fpage>–<lpage>1015</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.12.029" xlink:type="simple">10.1016/j.neuron.2011.12.029</ext-link></comment> <object-id pub-id-type="pmid">22405209</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Keat</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Reinagel</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Reid</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name>. <article-title>Predicting Every Spike: A Model for the Responses of Visual Neurons</article-title>. <source>Neuron</source>. <year>2001</year>;<volume>30</volume>(<issue>3</issue>):<fpage>803</fpage>–<lpage>817</lpage>. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(01)00322-1" xlink:type="simple">http://dx.doi.org/10.1016/S0896-6273(01)00322-1</ext-link>. <object-id pub-id-type="pmid">11430813</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref021">
<label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Wu A, Park IM, Pillow JW. Convolutional spike-triggered covariance analysis for neural subunit models. In: Advances in Neural Information Processing Systems; 2015. p. 793–801.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Freeman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>PH</given-names></name>, <name name-style="western"><surname>Greschner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gunning</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Mathieson</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Mapping nonlinear receptive field structure in primate retina at single cone resolution</article-title>. <source>eLife</source>. <year>2015</year>;<volume>4</volume>:<fpage>e05241</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.05241" xlink:type="simple">10.7554/eLife.05241</ext-link></comment> <object-id pub-id-type="pmid">26517879</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Real</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Asari</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Gollisch</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name>. <article-title>Neural Circuit Inference from Function to Structure</article-title>. <source>Current Biology</source>. <year>2017</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2016.11.040" xlink:type="simple">10.1016/j.cub.2016.11.040</ext-link></comment> <object-id pub-id-type="pmid">28065610</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref024">
<label>24</label>
<mixed-citation publication-type="other" xlink:type="simple">Vintch B, Zaharia AD, Movshon JA, Simoncelli EP, et al. Efficient and direct estimation of a neural subunit model for sensory coding. In: NIPS; 2012. p. 3113–3121.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref025">
<label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Park IM, Pillow JW. Bayesian spike-triggered covariance analysis. In: Advances in neural information processing systems; 2011. p. 1692–1700.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hochstein</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Shapley</surname> <given-names>RM</given-names></name>. <article-title>Linear and nonlinear spatial subunits in Y cat retinal ganglion cells</article-title>. <source>The Journal of Physiology</source>. <year>1976</year>;<volume>262</volume>(<issue>2</issue>):<fpage>265</fpage>–<lpage>284</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1113/jphysiol.1976.sp011595" xlink:type="simple">10.1113/jphysiol.1976.sp011595</ext-link></comment> <object-id pub-id-type="pmid">994040</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Victor</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Shapley</surname> <given-names>RM</given-names></name>. <article-title>The nonlinear pathway of Y ganglion cells in the cat retina</article-title>. <source>The Journal of General Physiology</source>. <year>1979</year>;<volume>74</volume>(<issue>6</issue>):<fpage>671</fpage>–<lpage>689</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1085/jgp.74.6.671" xlink:type="simple">10.1085/jgp.74.6.671</ext-link></comment> <object-id pub-id-type="pmid">231636</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Baccus</surname> <given-names>SA</given-names></name>. <article-title>Timing and Computation in Inner Retinal Circuitry</article-title>. <source>Annual Review of Physiology</source>. <year>2007</year>;<volume>69</volume>(<issue>1</issue>):<fpage>271</fpage>–<lpage>290</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.physiol.69.120205.124451" xlink:type="simple">10.1146/annurev.physiol.69.120205.124451</ext-link></comment> <object-id pub-id-type="pmid">17059359</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gollisch</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name>. <article-title>Eye Smarter than Scientists Believed: Neural Computations in Circuits of the Retina</article-title>. <source>Neuron</source>. <year>2010</year>;<volume>65</volume>(<issue>2</issue>):<fpage>150</fpage>–<lpage>164</lpage>. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.12.009" xlink:type="simple">http://dx.doi.org/10.1016/j.neuron.2009.12.009</ext-link>. <object-id pub-id-type="pmid">20152123</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Rudd</surname> <given-names>ME</given-names></name>. <article-title>The challenges natural images pose for visual adaptation</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>64</volume>(<issue>5</issue>):<fpage>605</fpage>–<lpage>616</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.11.028" xlink:type="simple">10.1016/j.neuron.2009.11.028</ext-link></comment> <object-id pub-id-type="pmid">20005818</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Demb</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tolhurst</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Dan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <etal>et al</etal>. <article-title>Do we know what the early visual system does?</article-title> <source>The Journal of neuroscience</source>. <year>2005</year>;<volume>25</volume>(<issue>46</issue>):<fpage>10577</fpage>–<lpage>10597</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3726-05.2005" xlink:type="simple">10.1523/JNEUROSCI.3726-05.2005</ext-link></comment> <object-id pub-id-type="pmid">16291931</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref032">
<label>32</label>
<mixed-citation publication-type="other" xlink:type="simple">McIntosh LT, Maheswaranathan N, Nayebi A, Ganguli S, Baccus SA. Deep learning models of the retinal response to natural scenes. In: Advances in Neural Information Processing Systems; 2016. p. 1361–1369.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maheswaranathan</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>McIntosh</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Kastner</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Melander</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Brezovec</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Nayebi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ganguli</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Baccus</surname> <given-names>SA</given-names></name>. <article-title>Deep learning models reveal internal structure and diverse computations in the retina under natural scenes</article-title>. <source>bioRxiv</source>. <year>2018</year>; p. <fpage>340943</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Demb</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Zaghloul</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Haarsma</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sterling</surname> <given-names>P</given-names></name>. <article-title>Bipolar cells contribute to nonlinear spatial summation in the brisk-transient (Y) ganglion cell in mammalian retina</article-title>. <source>The Journal of neuroscience</source>. <year>2001</year>;<volume>21</volume>(<issue>19</issue>):<fpage>7447</fpage>–<lpage>7454</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.21-19-07447.2001" xlink:type="simple">10.1523/JNEUROSCI.21-19-07447.2001</ext-link></comment> <object-id pub-id-type="pmid">11567034</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Turner</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>. <article-title>Synaptic rectification controls nonlinear spatial integration of natural visual inputs</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>90</volume>(<issue>6</issue>):<fpage>1257</fpage>–<lpage>1271</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.05.006" xlink:type="simple">10.1016/j.neuron.2016.05.006</ext-link></comment> <object-id pub-id-type="pmid">27263968</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Werblin</surname> <given-names>FS</given-names></name>. <article-title>Six different roles for crossover inhibition in the retina: correcting the nonlinearities of synaptic transmission</article-title>. <source>Visual neuroscience</source>. <year>2010</year>;<volume>27</volume>(<issue>1-2</issue>):<fpage>1</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0952523810000076" xlink:type="simple">10.1017/S0952523810000076</ext-link></comment> <object-id pub-id-type="pmid">20392301</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Enroth-Cugell</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Robson</surname> <given-names>JG</given-names></name>. <article-title>The contrast sensitivity of retinal ganglion cells of the cat</article-title>. <source>The Journal of physiology</source>. <year>1966</year>;<volume>187</volume>(<issue>3</issue>):<fpage>517</fpage>–<lpage>552</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1113/jphysiol.1966.sp008107" xlink:type="simple">10.1113/jphysiol.1966.sp008107</ext-link></comment> <object-id pub-id-type="pmid">16783910</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ölveczky</surname> <given-names>BP</given-names></name>, <name name-style="western"><surname>Baccus</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name>. <article-title>Segregation of object and background motion in the retina</article-title>. <source>Nature</source>. <year>2003</year>;<volume>423</volume>(<issue>6938</issue>):<fpage>401</fpage>–<lpage>408</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature01652" xlink:type="simple">10.1038/nature01652</ext-link></comment> <object-id pub-id-type="pmid">12754524</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Geffen</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>de Vries</surname> <given-names>SEJ</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name>. <article-title>Retinal Ganglion Cells Can Rapidly Change Polarity from Off to On</article-title>. <source>PLoS Biol</source>. <year>2007</year>;<volume>5</volume>(<issue>3</issue>):<fpage>e65</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.0050065" xlink:type="simple">10.1371/journal.pbio.0050065</ext-link></comment> <object-id pub-id-type="pmid">17341132</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Dimensionality reduction in neural models: an information-theoretic generalization of spike-triggered average and covariance analysis</article-title>. <source>Journal of vision</source>. <year>2006</year>;<volume>6</volume>(<issue>4</issue>):<fpage>9</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/6.4.9" xlink:type="simple">10.1167/6.4.9</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Aljadeff</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Segev</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>MJ</given-names> <suffix>II</suffix></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name>. <article-title>Spike triggered covariance in strongly correlated Gaussian stimuli</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>(<issue>9</issue>):<fpage>e1003206</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003206" xlink:type="simple">10.1371/journal.pcbi.1003206</ext-link></comment> <object-id pub-id-type="pmid">24039563</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name>. <article-title>Computational identification of receptive fields</article-title>. <source>Annual review of neuroscience</source>. <year>2013</year>;<volume>36</volume>:<fpage>103</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-neuro-062012-170253" xlink:type="simple">10.1146/annurev-neuro-062012-170253</ext-link></comment> <object-id pub-id-type="pmid">23841838</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liu</surname> <given-names>YS</given-names></name>, <name name-style="western"><surname>Stevens</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name>. <article-title>Predictable irregularities in retinal receptive fields</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2009</year>;<volume>106</volume>(<issue>38</issue>):<fpage>16499</fpage>–<lpage>16504</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0908926106" xlink:type="simple">10.1073/pnas.0908926106</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kaardal</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Fitzgerald</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name>. <article-title>Identifying functional bases for multidimensional neural computations</article-title>. <source>Neural computation</source>. <year>2013</year>;<volume>25</volume>(<issue>7</issue>):<fpage>1870</fpage>–<lpage>1890</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00465" xlink:type="simple">10.1162/NECO_a_00465</ext-link></comment> <object-id pub-id-type="pmid">23607565</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bölinger</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Gollisch</surname> <given-names>T</given-names></name>. <article-title>Closed-Loop Measurements of Iso-Response Stimuli Reveal Dynamic Nonlinear Stimulus Integration in the Retina</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>73</volume>(<issue>2</issue>):<fpage>333</fpage>–<lpage>346</lpage>. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.10.039" xlink:type="simple">http://dx.doi.org/10.1016/j.neuron.2011.10.039</ext-link>. <object-id pub-id-type="pmid">22284187</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liu</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Schreyer</surname> <given-names>HM</given-names></name>, <name name-style="western"><surname>Onken</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rozenblit</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Khani</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Krishnamoorthy</surname> <given-names>V</given-names></name>, <etal>et al</etal>. <article-title>Inference of neuronal functional circuitry with spike-triggered non-negative matrix factorization</article-title>. <source>Nature Communications</source>. <year>2017</year>;<volume>8</volume>(<issue>1</issue>):<fpage>149</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-017-00156-9" xlink:type="simple">10.1038/s41467-017-00156-9</ext-link></comment> <object-id pub-id-type="pmid">28747662</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref047">
<label>47</label>
<mixed-citation publication-type="other" xlink:type="simple">Park M, Pillow JW. Bayesian inference for low rank spatiotemporal neural receptive fields. In: Advances in Neural Information Processing Systems; 2013. p. 2688–2696.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Thorson</surname> <given-names>IL</given-names></name>, <name name-style="western"><surname>Liénard</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>David</surname> <given-names>SV</given-names></name>. <article-title>The essential complexity of auditory receptive fields</article-title>. <source>PLoS computational biology</source>. <year>2015</year>;<volume>11</volume>(<issue>12</issue>):<fpage>e1004628</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004628" xlink:type="simple">10.1371/journal.pcbi.1004628</ext-link></comment> <object-id pub-id-type="pmid">26683490</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schwartz</surname> <given-names>GW</given-names></name>, <name name-style="western"><surname>Okawa</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Dunn</surname> <given-names>FA</given-names></name>, <name name-style="western"><surname>Morgan</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Kerschensteiner</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Wong</surname> <given-names>RO</given-names></name>, <etal>et al</etal>. <article-title>The spatial structure of a nonlinear receptive field</article-title>. <source>Nature neuroscience</source>. <year>2012</year>;<volume>15</volume>(<issue>11</issue>):<fpage>1572</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3225" xlink:type="simple">10.1038/nn.3225</ext-link></comment> <object-id pub-id-type="pmid">23001060</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Asari</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name>. <article-title>Divergence of visual channels in the inner retina</article-title>. <source>Nature neuroscience</source>. <year>2012</year>;<volume>15</volume>(<issue>11</issue>):<fpage>1581</fpage>–<lpage>1589</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3241" xlink:type="simple">10.1038/nn.3241</ext-link></comment> <object-id pub-id-type="pmid">23086336</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Asari</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name>. <article-title>The projective field of retinal bipolar cells and its modulation by visual context</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>81</volume>(<issue>3</issue>):<fpage>641</fpage>–<lpage>652</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.11.029" xlink:type="simple">10.1016/j.neuron.2013.11.029</ext-link></comment> <object-id pub-id-type="pmid">24507195</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rudelson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Vershynin</surname> <given-names>R</given-names></name>. <article-title>Sampling from large matrices: An approach through geometric functional analysis</article-title>. <source>Journal of the ACM (JACM)</source>. <year>2007</year>;<volume>54</volume>(<issue>4</issue>):<fpage>21</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/1255443.1255449" xlink:type="simple">10.1145/1255443.1255449</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wu</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Gao</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Maple</surname> <given-names>BR</given-names></name>. <article-title>Functional architecture of synapses in the inner retina: segregation of visual signals by stratification of bipolar cell axon terminals</article-title>. <source>The Journal of Neuroscience</source>. <year>2000</year>;<volume>20</volume>(<issue>12</issue>):<fpage>4462</fpage>–<lpage>4470</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.20-12-04462.2000" xlink:type="simple">10.1523/JNEUROSCI.20-12-04462.2000</ext-link></comment> <object-id pub-id-type="pmid">10844015</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wässle</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Puller</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Müller</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Haverkamp</surname> <given-names>S</given-names></name>. <article-title>Cone contacts, mosaics, and territories of bipolar cells in the mouse retina</article-title>. <source>The Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>1</issue>):<fpage>106</fpage>–<lpage>117</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4442-08.2009" xlink:type="simple">10.1523/JNEUROSCI.4442-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19129389</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jadzinsky</surname> <given-names>PD</given-names></name>, <name name-style="western"><surname>Baccus</surname> <given-names>SA</given-names></name>. <article-title>Synchronized amplification of local information transmission by peripheral retinal input</article-title>. <source>eLife</source>. <year>2015</year>;<volume>4</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.09266" xlink:type="simple">10.7554/eLife.09266</ext-link></comment> <object-id pub-id-type="pmid">26568312</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Attneave</surname> <given-names>F</given-names></name>. <article-title>Some informational aspects of visual perception</article-title>. <source>Psychological review</source>. <year>1954</year>;<volume>61</volume>(<issue>3</issue>):<fpage>183</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0054663" xlink:type="simple">10.1037/h0054663</ext-link></comment> <object-id pub-id-type="pmid">13167245</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref057">
<label>57</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Barlow</surname> <given-names>HB</given-names></name>. In: <source>Possible principles underlying the transformations of sensory messages</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1961</year>. p. <fpage>217</fpage>–<lpage>234</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref058">
<label>58</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Hyvärinen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hurri</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hoyer</surname> <given-names>PO</given-names></name>. <source>Natural Image Statistics: A Probabilistic Approach to Early Computational Vision</source>. <volume>vol. 39</volume>. <publisher-name>Springer Science &amp; Business Media</publisher-name>; <year>2009</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Atick</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Redlich</surname> <given-names>AN</given-names></name>. <article-title>Towards a theory of early visual processing</article-title>. <source>Neural Computation</source>. <year>1990</year>;<volume>2</volume>(<issue>3</issue>):<fpage>308</fpage>–<lpage>320</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.1990.2.3.308" xlink:type="simple">10.1162/neco.1990.2.3.308</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Franke</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schubert</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Euler</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Baden</surname> <given-names>T</given-names></name>. <article-title>Inhibition decorrelates visual feature representations in the inner retina</article-title>. <source>Nature</source>. <year>2017</year>;<volume>542</volume>(<issue>7642</issue>):<fpage>439</fpage>–<lpage>444</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature21394" xlink:type="simple">10.1038/nature21394</ext-link></comment> <object-id pub-id-type="pmid">28178238</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pitkow</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name>. <article-title>Decorrelation and efficient coding by retinal ganglion cells</article-title>. <source>Nature neuroscience</source>. <year>2012</year>;<volume>15</volume>(<issue>4</issue>):<fpage>628</fpage>–<lpage>635</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3064" xlink:type="simple">10.1038/nn.3064</ext-link></comment> <object-id pub-id-type="pmid">22406548</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Berry</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Warland</surname> <given-names>DK</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name>. <article-title>The structure and precision of retinal spike trains</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1997</year>;<volume>94</volume>(<issue>10</issue>):<fpage>5411</fpage>–<lpage>5416</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.94.10.5411" xlink:type="simple">10.1073/pnas.94.10.5411</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref063">
<label>63</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Sterling</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Laughlin</surname> <given-names>S</given-names></name>. <source>Principles of neural design</source>. <publisher-name>MIT Press</publisher-name>; <year>2015</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref064">
<label>64</label>
<mixed-citation publication-type="other" xlink:type="simple">Bialek W, van Steveninck RR. Features and dimensions: Motion estimation in fly vision. arXiv preprint q-bio/0505003. 2005.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Atick</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Redlich</surname> <given-names>AN</given-names></name>. <article-title>What does the retina know about natural scenes?</article-title> <source>Neural computation</source>. <year>1992</year>;<volume>4</volume>(<issue>2</issue>):<fpage>196</fpage>–<lpage>210</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.1992.4.2.196" xlink:type="simple">10.1162/neco.1992.4.2.196</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Field</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>. <article-title>Nonlinear signal transfer from mouse rods to bipolar cells and implications for visual sensitivity</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>34</volume>(<issue>5</issue>):<fpage>773</fpage>–<lpage>785</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0896-6273(02)00700-6" xlink:type="simple">10.1016/S0896-6273(02)00700-6</ext-link></comment> <object-id pub-id-type="pmid">12062023</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref067">
<label>67</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <source>Biophysics: searching for principles</source>. <publisher-name>Princeton University Press</publisher-name>; <year>2012</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref068">
<label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Clark</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Benichou</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>da Silveira</surname> <given-names>RA</given-names></name>. <article-title>Dynamical adaptation in photoreceptors</article-title>. <source>PLOS Comput Biol</source>. <year>2013</year>;<volume>9</volume>(<issue>11</issue>):<fpage>e1003289</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003289" xlink:type="simple">10.1371/journal.pcbi.1003289</ext-link></comment> <object-id pub-id-type="pmid">24244119</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref069">
<label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kim</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>. <article-title>Temporal contrast adaptation in the input and output signals of salamander retinal ganglion cells</article-title>. <source>The Journal of Neuroscience</source>. <year>2001</year>;<volume>21</volume>(<issue>1</issue>):<fpage>287</fpage>–<lpage>299</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.21-01-00287.2001" xlink:type="simple">10.1523/JNEUROSCI.21-01-00287.2001</ext-link></comment> <object-id pub-id-type="pmid">11150346</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Manookin</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Demb</surname> <given-names>JB</given-names></name>. <article-title>Presynaptic mechanism for slow contrast adaptation in mammalian retinal ganglion cells</article-title>. <source>Neuron</source>. <year>2006</year>;<volume>50</volume>(<issue>3</issue>):<fpage>453</fpage>–<lpage>464</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2006.03.039" xlink:type="simple">10.1016/j.neuron.2006.03.039</ext-link></comment> <object-id pub-id-type="pmid">16675399</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref071">
<label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Weick</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Demb</surname> <given-names>JB</given-names></name>. <article-title>Delayed-rectifier K channels contribute to contrast adaptation in mammalian retinal ganglion cells</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>71</volume>(<issue>1</issue>):<fpage>166</fpage>–<lpage>179</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.04.033" xlink:type="simple">10.1016/j.neuron.2011.04.033</ext-link></comment> <object-id pub-id-type="pmid">21745646</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref072">
<label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kastner</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Baccus</surname> <given-names>SA</given-names></name>. <article-title>Spatial segregation of adaptation and predictive sensitization in retinal ganglion cells</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>79</volume>(<issue>3</issue>):<fpage>541</fpage>–<lpage>554</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.06.011" xlink:type="simple">10.1016/j.neuron.2013.06.011</ext-link></comment> <object-id pub-id-type="pmid">23932000</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref073">
<label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Naecker</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Maheswaranathan</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ganguli</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Baccus</surname> <given-names>SA</given-names></name>. <article-title>Pyret: A Python package for analysis of neurophysiology data</article-title>. <source>The Journal of Open Source Software</source>. <year>2017</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.21105/joss.00137" xlink:type="simple">10.21105/joss.00137</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref074">
<label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Parikh</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Boyd</surname> <given-names>S</given-names></name>. <article-title>Proximal algorithms</article-title>. <source>Foundations and Trends in optimization</source>. <year>2013</year>;<volume>1</volume>(<issue>3</issue>):<fpage>123</fpage>–<lpage>231</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref075">
<label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Polson</surname> <given-names>NG</given-names></name>, <name name-style="western"><surname>Scott</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Willard</surname> <given-names>BT</given-names></name>, <etal>et al</etal>. <article-title>Proximal Algorithms in Statistics and Machine Learning</article-title>. <source>Statistical Science</source>. <year>2015</year>;<volume>30</volume>(<issue>4</issue>):<fpage>559</fpage>–<lpage>581</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1214/15-STS530" xlink:type="simple">10.1214/15-STS530</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref076">
<label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rajan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Marre</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>. <article-title>Learning quadratic receptive fields from neural responses to natural stimuli</article-title>. <source>Neural computation</source>. <year>2013</year>;<volume>25</volume>(<issue>7</issue>):<fpage>1661</fpage>–<lpage>1692</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00463" xlink:type="simple">10.1162/NECO_a_00463</ext-link></comment> <object-id pub-id-type="pmid">23607557</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006291.ref077">
<label>77</label>
<mixed-citation publication-type="other" xlink:type="simple">Vu VQ, Cho J, Lei J, Rohe K. Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA. In: Advances in Neural Information Processing Systems 26; 2013. p. 2670–2678.</mixed-citation>
</ref>
<ref id="pcbi.1006291.ref078">
<label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Björck</surname> <given-names>Ȧ</given-names></name>, <name name-style="western"><surname>Golub</surname> <given-names>GH</given-names></name>. <article-title>Numerical methods for computing angles between linear subspaces</article-title>. <source>Mathematics of computation</source>. <year>1973</year>;<volume>27</volume>(<issue>123</issue>):<fpage>579</fpage>–<lpage>594</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1090/S0025-5718-1973-0348991-3" xlink:type="simple">10.1090/S0025-5718-1973-0348991-3</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>