<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00551</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1007091</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Speech signal processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Phonetics</subject><subj-group><subject>Consonants</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Phonetics</subject><subj-group><subject>Vowels</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Grammar</subject><subj-group><subject>Phonology</subject><subj-group><subject>Syllables</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Acoustic signals</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Deep learning as a tool for neural data analysis: Speech classification and cross-frequency coupling in human sensorimotor cortex</article-title>
<alt-title alt-title-type="running-head">Deep learning as a tool for neural data analysis in human sensorimotor cortex</alt-title></title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0494-8758</contrib-id>
<name name-style="western">
<surname>Livezey</surname> <given-names>Jesse A.</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1974-4603</contrib-id>
<name name-style="western">
<surname>Bouchard</surname> <given-names>Kristofer E.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="fn" rid="econtrib001"><sup>‡</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref></contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2480-4700</contrib-id>
<name name-style="western">
<surname>Chang</surname> <given-names>Edward F.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
<xref ref-type="fn" rid="econtrib001"><sup>‡</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref></contrib></contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Biological Systems and Engineering Division, Lawrence Berkeley National Laboratory, Berkeley, California, United States of America</addr-line></aff>
<aff id="aff002">
<label>2</label>
<addr-line>Redwood Center for Theoretical Neuroscience, University of California, Berkeley, Berkeley, California, United States of America</addr-line></aff>
<aff id="aff003">
<label>3</label>
<addr-line>Helen Wills Neuroscience Institute, University of California, Berkeley, Berkeley, California, United States of America</addr-line></aff>
<aff id="aff004">
<label>4</label>
<addr-line>Department of Neurological Surgery and Department of Physiology, University of California, San Francisco, San Francisco, California, United States of America</addr-line></aff>
<aff id="aff005">
<label>5</label>
<addr-line>Center for Integrative Neuroscience, University of California, San Francisco, San Francisco, California, United States of America</addr-line></aff>
<aff id="aff006">
<label>6</label>
<addr-line>UCSF Epilepsy Center, University of California, San Francisco, San Francisco, California, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Miller</surname> <given-names>Kai</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib></contrib-group>
<aff id="edit1">
<addr-line>Stanford University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="other" id="econtrib001">
<label>‡</label>
<p>These authors also contributed equally to this work.</p></fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">kebouchard@lbl.gov</email> (KEB); <email xlink:type="simple">edward.chang@ucsf.edu</email> (EFC)</corresp></author-notes>
<pub-date pub-type="collection">
<month>9</month>
<year>2019</year></pub-date>
<pub-date pub-type="epub">
<day>16</day>
<month>9</month>
<year>2019</year></pub-date>
<volume>15</volume>
<issue>9</issue>
<elocation-id>e1007091</elocation-id>
<history>
<date date-type="received">
<day>10</day>
<month>4</month>
<year>2018</year></date>
<date date-type="accepted">
<day>10</day>
<month>5</month>
<year>2019</year></date></history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Livezey et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1007091"/>
<abstract>
<p>A fundamental challenge in neuroscience is to understand what structure in the world is represented in spatially distributed patterns of neural activity from multiple single-trial measurements. This is often accomplished by learning a simple, linear transformations between neural features and features of the sensory stimuli or motor task. While successful in some early sensory processing areas, linear mappings are unlikely to be ideal tools for elucidating nonlinear, hierarchical representations of higher-order brain areas during complex tasks, such as the production of speech by humans. Here, we apply deep networks to predict produced speech syllables from a dataset of high gamma cortical surface electric potentials recorded from human sensorimotor cortex. We find that deep networks had higher decoding prediction accuracy compared to baseline models. Having established that deep networks extract more task relevant information from neural data sets relative to linear models (i.e., higher predictive accuracy), we next sought to demonstrate their utility as a data analysis tool for neuroscience. We first show that deep network’s confusions revealed hierarchical latent structure in the neural data, which recapitulated the underlying articulatory nature of speech motor control. We next broadened the frequency features beyond high-gamma and identified a novel high-gamma-to-beta coupling during speech production. Finally, we used deep networks to compare task-relevant information in different neural frequency bands, and found that the high-gamma band contains the vast majority of information relevant for the speech prediction task, with little-to-no additional contribution from lower-frequency amplitudes. Together, these results demonstrate the utility of deep networks as a data analysis tool for basic and applied neuroscience.</p></abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>It has been demonstrated that deep networks can achieve state of the art results on a number of classic machine learning tasks, but it is not currently clear whether deep networks can enjoy the same success in science, where not only accuracy but scientific understanding is desired. For example, the relationship between neural features and features of speech is often examined through the use of single-layer statistical models. However, single-layer models are unlikely to be able to describe the complex representations in higher-order brain areas during speech production. In this study, we show that deep networks achieve state of the art accuracy when classifying speech syllables from the amplitude of cortical surface electrical potentials. Furthermore, deep networks reveal an articulatory speech hierarchy consistent with previously studies which used hand-designed features. We also report a novel positive coupling between the beta and high-gamma bands during speech production in “active” cortical areas. However, using deep networks we show that, compared to lower frequency bands, the high gamma amplitude is by far the most informative signal for classifying speech.</p></abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>National Institutes of Health (US)</institution></funding-source>
<award-id>R00-NS065120</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2480-4700</contrib-id>
<name name-style="western">
<surname>Chang</surname> <given-names>Edward F.</given-names></name></principal-award-recipient></award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000002</institution-id>
<institution>National Institutes of Health</institution></institution-wrap></funding-source>
<award-id>DP2-OD00862</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2480-4700</contrib-id>
<name name-style="western">
<surname>Chang</surname> <given-names>Edward F.</given-names></name></principal-award-recipient></award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000002</institution-id>
<institution>National Institutes of Health</institution></institution-wrap></funding-source>
<award-id>R01-DC012379</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2480-4700</contrib-id>
<name name-style="western">
<surname>Chang</surname> <given-names>Edward F.</given-names></name></principal-award-recipient></award-group>
<funding-statement>J.A.L. and K.E.B. were funded by Lawrence Berkeley National Laboratory-internal LDRD “Neuromorphic Kalman Filters” led by Paolo Calafiura and Lawrence Berkeley National Laboratory-internal LDRD “Deep Learning for Science” led by Prabhat (<ext-link ext-link-type="uri" xlink:href="https://science.energy.gov/lp/laboratory-directed-research-and-development/" xlink:type="simple">https://science.energy.gov/lp/laboratory-directed-research-and-development/</ext-link>). K.E.B. was funded by Lawrence Berkeley National Laboratory-internal LDRD “Neuro/Nano-Technology for BRAIN” led by Peter Denes. E.F.C. was funded by the US National Institutes of Health grants R00-NS065120, DP2-OD00862 and R01-DC012379 (<ext-link ext-link-type="uri" xlink:href="https://www.nih.gov/" xlink:type="simple">https://www.nih.gov/</ext-link>), and the Ester A. and Joseph Klingenstein Foundation (<ext-link ext-link-type="uri" xlink:href="http://www.klingfund.org/" xlink:type="simple">http://www.klingfund.org/</ext-link>). This project used resources of the National Energy Research Scientific Computing Center, which is supported by the Office of Science of the U.S. Department of Energy (<ext-link ext-link-type="uri" xlink:href="https://www.science.energy.gov/" xlink:type="simple">https://www.science.energy.gov/</ext-link>) under Contract No. DE-AC02-05CH11231. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group>
<counts>
<fig-count count="7"/>
<table-count count="1"/>
<page-count count="29"/></counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-09-26</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The code and model files are hosted at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.2646611" xlink:type="simple">https://doi.org/10.5281/zenodo.2646611</ext-link> and the raw data files are hosted at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/collections/Human_ECoG_speaking_consonant-vowel_syllables/4617263" xlink:type="simple">https://figshare.com/collections/Human_ECoG_speaking_consonant-vowel_syllables/4617263</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta></front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>A central goal of neuroscience is to understand what and how information about the external world (e.g., sensory stimuli or behaviors) is present in spatially distributed, dynamic patterns of brain activity. At the same time, neuroscience has been on an inexorable march away from the periphery (e.g., the retina, spinal cord), seeking to understand higher-order brain function (such as speech). The methods used by neuroscientists are typically based on simple linear transformations, which have been successful predictors in early processing stages of the nervous system for simple tasks [<xref ref-type="bibr" rid="pcbi.1007091.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref003">3</xref>]. However, linear methods are limited in their ability to represent complex, hierarchical, nonlinear relationships [<xref ref-type="bibr" rid="pcbi.1007091.ref004">4</xref>], which are likely present in the neural activity of higher-order brain areas. This linear restriction may not only limit the predictive accuracy of models but may also limit our ability to uncover structure in neural datasets.</p>
<p>Multilayer deep networks can combine features in nonlinear ways when making predictions. This gives them more expressive power in terms of the types of mappings they can learn at the cost of more model hyperparameters, more model parameters to train, and more difficult training dynamics [<xref ref-type="bibr" rid="pcbi.1007091.ref005">5</xref>]. Together with the recent success of deep learning in a number of fields including computer vision, text translation, and speech recognition [<xref ref-type="bibr" rid="pcbi.1007091.ref006">6</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref008">8</xref>], the ability of deep networks to learn nonlinear function from data motivates their use for understanding neural signals. The success of deep learning in classic machine learning tasks has spurred a growth of applications into new scientific fields. Other nonlinear methods such as random trees/forests can also be used on nonlinear neural data but often require more feature selection/reduction and are not typically used on data with thousands or tens of thousands of features [<xref ref-type="bibr" rid="pcbi.1007091.ref009">9</xref>]. Deep networks have recently been applied as classifiers for diverse types of physiological data including electromyographic (EMG), electroencephalographic (EEG), and spike rate signals [<xref ref-type="bibr" rid="pcbi.1007091.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref013">13</xref>], on stimulus reconstruction in sensory regions using electrocorticography (ECoG) [<xref ref-type="bibr" rid="pcbi.1007091.ref014">14</xref>], as models for sensory and motor systems [<xref ref-type="bibr" rid="pcbi.1007091.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref019">19</xref>]. Compared to datasets used in traditional machine learning, neuroscientif datasets tend to be very small. As such, models in neuroscience tend to be smaller (fewer layers, units per layers) and in this sense more similar to neural networks from previous decades. However, modern deep learning techniques such as ReLUs, Dropout, and optimization algorithms like Nesterov momentum are crucial to train them to high held-out performance. While these studies have demonstrated the superior performance of deep networks as black-box predictors, the utilization of deep networks to gain understanding into brain computations is rare. Whether deep networks can be used to elucidate the latent structure of scientific and neuroscientific datasets is still an open question.</p>
<p>Vocal articulation is a complex task requiring the coordinated orchestration of several parts of the vocal tract (e.g., the larynx, tongue, jaw, and lips). To study the neural basis of speech requires monitoring cortical activity at high spatio-temporal resolution (on the order of tens of milliseconds) over large areas of sensorimotor cortex (∼1300mm<sup>2</sup>) [<xref ref-type="bibr" rid="pcbi.1007091.ref020">20</xref>]. Electrocorticography (ECoG) is an ideal method to achieve the simultaneous high-resolution and broad coverage requirements in humans. Using such recordings, there has been a surge of recent efforts to understand the cortical basis of speech production [<xref ref-type="bibr" rid="pcbi.1007091.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref025">25</xref>]. For example, analyzing mean activity, Bouchard et. al. [<xref ref-type="bibr" rid="pcbi.1007091.ref020">20</xref>] demonstrated, much in the spirit of Penfield’s earlier work [<xref ref-type="bibr" rid="pcbi.1007091.ref026">26</xref>], that the ventral sensorimotor cortex (vSMC) has a spatial map of articulator representations (i.e. lips, jaw, tongue, and larynx) that are engaged during speech production. Additionally, it was found that spatial patterns of activity across the vSMC network (extracted from trial average activity with principal components analysis at specific time points) organized phonemes along phonetic features emphasizing the articulatory requirements of production.</p>
<p>Understanding how well cortical surface electrical potentials (CSEPs) capture the underlying neural processing involved in speech production is important for revealing the neural basis of speech and improving speech decoding for brain-computer interfaces [<xref ref-type="bibr" rid="pcbi.1007091.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref028">28</xref>]. Previous studies have used CSEPs and linear or single layer models to predict speech categories [<xref ref-type="bibr" rid="pcbi.1007091.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref029">29</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref032">32</xref>], or continuous aspects of speech production (e.g., vowel acoustics or vocal tract configurations) [<xref ref-type="bibr" rid="pcbi.1007091.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref025">25</xref>], with some success. However, given the challenge of collecting large number of samples across diverse speech categories, it is not clear that we should expect high performance from deep networks for speech classification. Exploring the use of deep networks to maximally extract information for speech prediction is not only important for brain machine interfaces which restore communication capabilities to people who are “locked-in”, but also for identifying cortical computations which are the underlying basis for speech production.</p>
<p>In general, understanding information content across neural signals, such as different frequency components of CSEPs, is an area of ongoing research [<xref ref-type="bibr" rid="pcbi.1007091.ref033">33</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref037">37</xref>]. A number of studies have found relationships between different frequency components in the brains electrical potentials. These can take the form of phase and amplitude structure of beta (<italic>β</italic>) waves [<xref ref-type="bibr" rid="pcbi.1007091.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref039">39</xref>] or correlations between lower frequency oscillations and spiking activity or high gamma (H<italic>γ</italic>) activity [<xref ref-type="bibr" rid="pcbi.1007091.ref037">37</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref040">40</xref>]. One observation is that <italic>β</italic> band (14-30Hz) amplitude and coherence [<xref ref-type="bibr" rid="pcbi.1007091.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref041">41</xref>] often decreases during behavior, when the state is changing [<xref ref-type="bibr" rid="pcbi.1007091.ref042">42</xref>]. This has lead to the interpretation that <italic>β</italic> may be serving a “maintenance of state” function. However, often these effects are not differentiated between functional areas that are active versus inactive during behavior. Indeed, in other contexts, aggregation has been shown to mask structure in neural signals [<xref ref-type="bibr" rid="pcbi.1007091.ref043">43</xref>]. The somatotopic organization of speech articulator control in human vSMC, and the differential engagement of these articulators by different speech sounds, potentially provides the opportunity to disentangle these issues. Furthermore, classifying behaviors, such as speech, from CSEPs can be used as a proxy for information content in a signal, obfuscating the interpretation of the results. However, this is often done using linear methods, which may not be able to take full advantage of the information in a signal. Since deep networks are able to maximize classification performance, they are an ideal candidate for comparing information content across neural signals.</p>
<p>In this work, we investigated deep networks as a data analytics framework for systems neuroscience, with a specific focus on the uniquely human capacity to produce spoken language. First, we show that deep networks achieve superior classification accuracy compared to linear models, with increased gains for increasing task complexity. We then “opened the black box” and used the deep network confusions to reveal the latent structure learned from single trials, which revealed a rich, hierarchical organization of linguistic features. Since deep networks classified speech production from H<italic>γ</italic> activity with higher accuracy that other methods, they are also candidates for determining the relative information content across neural signals. We explored the cross-frequency amplitude-amplitude structure in the CSEPs and discovered a novel signature of motor coordination in <italic>β</italic>-H<italic>γ</italic> coupling. Using deep networks, we then show that although there is information relevant to speech production in the lower frequency bands, it is small compared to H<italic>γ</italic>. Critically, the lower frequency bands do not add significant additional information about speech production about and beyond H<italic>γ</italic>. Furthermore, the correlations are not tightly related to overall information content and improvements in accuracy. Together, these results demonstrate the utilization of deep networks not only as an optimal black-box predictor, but as a powerful data analytics tool to reveal the latent structure of neural representations, and understanding the information content of different neural signals.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec003">
<title>Experimental data</title>
<p>The experimental protocol, collection, and processing of the data examined here have been described in detail previously [<xref ref-type="bibr" rid="pcbi.1007091.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref022">22</xref>]. The experimental protocol was approved by the Human Research Protection Program at the University of California, San Francisco. Briefly, four native English speaking human subjects underwent chronic implantation of a subdural electrocortigraphic (ECoG) array over the left hemisphere as part of their clinical treatment of epilepsy. The subjects gave their written informed consent before the day of surgery. The subjects read aloud consonant-vowel (CV) syllables composed of 19 consonants followed by one of three vowels (/a/, /i/ or /u/), for a total of 57 potential consonant-vowel syllables. Subjects did not produce each CV in an equal number of trials or produce all possible CVs. Across subjects, the number of repetitions per CV varied from 10 to 105, and the total number of usable trials per subject was S1: 2572, S2: 1563, S3: 5207, and S4: 1422. CVs for which there was not enough data to do cross-validation (fewer than 10 examples) were excluded per-subject.</p>
</sec>
<sec id="sec004">
<title>Signal processing</title>
<p>Cortical surface electrical potentials (CSEPs) were recorded directly from the cortical surface with a high-density (4mm pitch), 256-channel ECoG array and a multi-channel amplifier optically connected to a digital signal processor (Tucker-Davis Technologies [TDT], Alachua, FL). The time series from each channel was visually and quantitatively inspected for artifacts or excessive noise (typically 60 Hz line noise). These channels were excluded from all subsequent analysis and the raw CSEP signal from the remaining channels were downsampled to 400 Hz in the frequency domain and then common-average referenced and used for spectro-temporal analysis. For each useable channel, the time-varying analytic amplitude was extracted from 40 frequency domain, bandpass filters (Gaussian filters, logarithmically increasing center frequencies and semi-logarithmically increasing band-widths, equivalent to a frequency domain Morlet wavelet). The amplitude for each filter band was z-scored to a baseline window defined as a period of time in which the subject was silent, the room was silent, and the subject was resting. Finally, the amplitudes were downsampled to 200 Hz.</p>
<p>For each of the bands defined as: theta [4-7 Hz], alpha [8-14 Hz], beta [15-29 Hz], gamma [30-59 Hz], and high gamma [70-150 Hz], individual bands from the 40 Gaussian bandpassed amplitudes were grouped and averaged according to their center frequencies. The lower frequency features are all highly oversampled at the H<italic>γ</italic> rate of 200 Hz. To make comparisons across frequency bands more interpretable, control for potential overfitting from training on oversampled signals, and to reduce the computational complexity of training deep networks with concatenated input features, we downsampled each of the lower frequency bands in time so that the center frequency-to-sampling rate ratio was constant (ratio = 112.5/200) for each band. Given limited data, deep networks are tasked with deciding whether a change across input features is relevant or irrelevant for prediction. The lower frequency bands are highly oversampled at 200 Hz, however, the higher frequencies will not have exactly zero amplitude do to numerical noise even though these are irrelevant signals. Downsampling the bands to a fixed ratio makes comparing CV decoding accuracy across frequency bands more interpretable.</p>
<p>Based on previous results [<xref ref-type="bibr" rid="pcbi.1007091.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref022">22</xref>], we focused on the electrodes in the ventral sensorimotor cortex (vSMC). The activity for each of the examples in our data set was aligned to the acoustic onset of the consonant-to-vowel transition. For each example, a window 0.5 seconds preceding and 0.79 seconds following the acoustic onset of the consonant-to-vowel transition was extracted. The mean of the first and last ∼ 4% time samples was subtracted from the data per electrode and trial (another form of amplitude normalization that is very local in time). This defined the z-scored amplitude that is used for subsequent analyses.</p>
</sec>
<sec id="sec005">
<title>Deep networks</title>
<p>Supervised classification models often find their model parameters, <inline-formula id="pcbi.1007091.e001"><alternatives><graphic id="pcbi.1007091.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mover accent="true"><mml:mo>Θ</mml:mo> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, which minimize the negative log-likelihood of the training data and labels, {<italic>x</italic><sup>(<italic>i</italic>)</sup>, <italic>y</italic><sup>(<italic>i</italic>)</sup>}, under a model which gives the conditional probability of the labels given the input data
<disp-formula id="pcbi.1007091.e002"><alternatives><graphic id="pcbi.1007091.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mo>Θ</mml:mo> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:munder><mml:mrow><mml:mtext>arg</mml:mtext> <mml:mspace width="4pt"/><mml:mtext>min</mml:mtext></mml:mrow> <mml:mo>Θ</mml:mo></mml:munder> <mml:mspace width="4pt"/><mml:mo>−</mml:mo> <mml:mtext>log</mml:mtext> <mml:mspace width="2pt"/><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>Y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>X</mml:mi> <mml:mo>;</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mspace width="4pt"/><mml:mrow><mml:mo>{</mml:mo> <mml:msup><mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
Deep networks typically parametrize this conditional probability with a sequence of linear-nonlinear operations. Each layer in a fully-connected network consists of an affine transform followed by a nonlinearity:
<disp-formula id="pcbi.1007091.e003"><alternatives><graphic id="pcbi.1007091.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>h</mml:mi> <mml:mn>1</mml:mn></mml:msup></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>w</mml:mi> <mml:mn>1</mml:mn></mml:msup> <mml:mo>·</mml:mo> <mml:mi>x</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mi>b</mml:mi> <mml:mn>1</mml:mn></mml:msup> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msup></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mi>h</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>b</mml:mi> <mml:mi>i</mml:mi></mml:msup> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mspace width="4pt"/><mml:mtext>with</mml:mtext></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mo>Θ</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>x</italic> is a batch of input vectors, <italic>w</italic><sup><italic>i</italic></sup> and <italic>b</italic><sup><italic>i</italic></sup> are trainable parameters (weights and biases, respectively) for the <italic>i</italic>th layer, <italic>h</italic><sup><italic>i</italic></sup> is the <italic>i</italic>th hidden representation, and <italic>f</italic>(⋅) is a nonlinearity which can be chosen during hyperparameter selection. Single layer classification methods, such as multinomial logistic regression, are a special case of deep networks with no hidden representations and their corresponding hyperparameters.</p>
<p>For the fully-connected deep networks used here, the CSEP features were rasterized into a large feature vector per-trial in a window around CV production. These feature vectors are the input into the first layer of the fully connected network. The feature dimensionality is the number of electrodes by 258 time points which corresponds to Subject 1: 22,188, Subject 2: 20,124, Subject 3: 21,414, and Subject 4: 25,542 features. The final layer non-linearity is chosen to be the softmax function:
<disp-formula id="pcbi.1007091.e004"><alternatives><graphic id="pcbi.1007091.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mtext>softmax</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where <italic>h</italic><sub><italic>i</italic></sub> is the <italic>i</italic>th element of the hidden representation. This nonlinearity transforms a vector of real numbers into a vector which represents a one-draw multinomial distribution. It is the negative log-likelihood of this distribution over the training data which is minimized during training.</p>
<p>To train and evaluate the networks, the trials were organized into 10 groupings (folds) with mutually exclusive validation and test sets and 80-10-10% splits (training, validation, testing). Since some classes may have as few as 10 examples, it was important to split each class proportionally so that all classes were equally distributed. Training terminated when the validation accuracy did not improve for 10 epochs and typically lasted about 25 epochs. Theano, Pylearn2, and Scikit-learn [<xref ref-type="bibr" rid="pcbi.1007091.ref044">44</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref046">46</xref>] were used to train all deep and linear models.</p>
<p>As baseline models, we used multinomial logistic regression. Logistic regression required no additional dimensionality reduction and had the highest classification accuracy compared to other linear classifiers, i.e. linear support vector machines and linear discriminant analysis on the H<italic>γ</italic> features (10.4 ± 6.7% and 16.0 ± 10.0% respectively compared to 28.0 ± 12.9% for logistic regression). Additionally, the conditional class distribution used in logistic regression (multinomial) is the same as the one used for deep networks, which facilitated comparison of confusions.</p>
<sec id="sec006">
<title>Hyperparameter search</title>
<p>Deep networks have a number of hyperparameters that govern network architecture and optimization such as the number of layers, the layer nonlinearity, and the optimization parameters. The full list of hyperparameters and their ranges is listed in <xref ref-type="supplementary-material" rid="pcbi.1007091.s001">S1 Table</xref>.</p>
<p>For all results that are based on training networks, 400 models were trained with hyperparameters selected with random search [<xref ref-type="bibr" rid="pcbi.1007091.ref047">47</xref>]. For each set of hyperparameters, 10 copies of the network were trained on the respective 10 folds as described in Deep networks, for a total of 4000 networks per subject per task. For each task, optimal hyperparameters were selected by choosing the model with the best mean validation classification accuracy across 10 folds. Since our datasets were relatively small for training deep networks, we regularized the models in three ways: dropout, weight decay, and filter norm-clipping in all layers of the model. The dropout rate, activation-rescaling factor, max filter norm, and weight decay coefficient were all optimized hyperparameters. The optimal values for the hyperparameters were selected independently for each family of models in the paper, i.e. independently for each subject, model type (logistic or deep), input data type (frequency bands), and amount (data scaling experiment). The search space for hyperparameters was shared across all models, however, for the logistic regression models, the number of hidden layers was set to zero and no other hidden layer hyperparameters were used. The optimal hyperparameters for each model and links to trained model files and Docker images for running preprocessing and deep network training are available in <xref ref-type="supplementary-material" rid="pcbi.1007091.s004">S1 Appendix</xref>.</p>
</sec>
<sec id="sec007">
<title>Classification tasks</title>
<p>Each subject produced a subset of the 57 CV and the classification methods were trained to predict the subset. Each CV can also be classified as containing 1 of 19 consonants or 1 of 3 vowels. Similarly, a subset of the constants can be grouped into 1 of 3 vocal tract constriction location categories or 1 of 3 vocal tract constriction degree categories. The CV predictions were then tabulated within these restricted labelings in order to calculate the accuracy for consonant, vowel, constriction location, and constriction degree accuracies.</p>
<p>As there are drastically different numbers of classes between the different tasks (between 3 and 57), as well as subtle differences between subjects, classification accuracies and changes in accuracies are all normalized to chance. In each case, chance accuracy is estimated by assuming that test set predictions are drawn randomly from the training set distribution. This process was averaged across 100 random resamplings per fold, training fraction, subject, etc. Estimating chance accuracy by training models on data with shuffled labels was not possible for consonant constriction location and degree tasks since not all CVs were part of the restricted task and occasionally networks would predict 0 trials within the restricted task which would give undefined chance accuracy.</p>
<p>On the CV task, we compared performance scaling of different models by training on different fractions of the training set. For each fraction of the data, each class was subsampled individually to ensure all classes were present in the training set. The aggregate slopes were calculated with ordinary least-squares regression. The validation and test sets were not subsampled. Hyperparameters were chosen independently for each fraction of the training data.</p>
</sec>
<sec id="sec008">
<title>Information content in neural signals</title>
<p>For a given experimentally defined behavior, such as CV speech production, the information about the task is presumably present in the activity of the brain, which we coarsely measure with different frequency components of the recorded CSEPs. The information about the task in the measurements can be formalized by the mutual information between the task variable <italic>Y</italic> and the neural measurement variable <italic>X</italic> [<xref ref-type="bibr" rid="pcbi.1007091.ref048">48</xref>]
<disp-formula id="pcbi.1007091.e005"><alternatives><graphic id="pcbi.1007091.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>Y</mml:mi> <mml:mo>;</mml:mo> <mml:mi>X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>y</mml:mi> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi></mml:mrow></mml:munder> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="2pt"/><mml:msub><mml:mtext>log</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
It is not possible to calculate this quantity directly because we do not know the joint distribution of neural measurements and speech tokens, <italic>P</italic>(<italic>X</italic>, <italic>Y</italic>) and cannot easily approximate it due to the small number of samples (∼ 10<sup>3</sup>) compared to the dimensionality of each measurement (∼ 10<sup>4</sup>). However, we can classify the behavior from the neural data using statistical-machine learning methods, i.e. deep learning. For a supervised classification task, machine learning methods typically generate conditional probabilities <inline-formula id="pcbi.1007091.e006"><alternatives><graphic id="pcbi.1007091.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>Y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>|</mml:mo> <mml:mi>X</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Since we know the ground-truth behavior for each measurement, we can use the classifier to compute the mutual information between the behavior state, <italic>Y</italic>, and the predicted state, <inline-formula id="pcbi.1007091.e007"><alternatives><graphic id="pcbi.1007091.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mover accent="true"><mml:mi>Y</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>
<disp-formula id="pcbi.1007091.e008"><alternatives><graphic id="pcbi.1007091.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>Y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>;</mml:mo> <mml:mi>Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>y</mml:mi> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:munder> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>|</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="2pt"/><mml:msub><mml:mtext>log</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>|</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
The data processing inequality tells us that this quantity is a lower bound to <italic>I</italic>(<italic>Y</italic>;<italic>X</italic>).</p>
<p>Given this lower bound, if everything else is held constant, the classification method with highest accuracy will lead the tightest estimate of the mutual information between the task and neural data, <italic>I</italic>(<italic>Y</italic>; <italic>X</italic>), which is a quantity that is relevant for future experimental hardware, methods, and data preprocessing development.</p>
<p>This quantity is closely related to a second measure of classifier performance, the Channel Capacity (CC). To compare our results with previous speech classification studies, we report estimated CC, which is measured in bits per symbol, in addition to classification statistics. CC is a unified way of calculating the effectiveness of different speech classifiers, which can have differing numbers of classes and modalities. The channel capacity, <italic>CC</italic>, between the ground truth class, <italic>Y</italic>, and predicted class, <inline-formula id="pcbi.1007091.e009"><alternatives><graphic id="pcbi.1007091.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mover accent="true"><mml:mi>Y</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, is defined as:
<disp-formula id="pcbi.1007091.e010"><alternatives><graphic id="pcbi.1007091.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>C</mml:mi> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:munder><mml:mtext>supremum</mml:mtext> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:mspace width="2pt"/><mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>Y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>;</mml:mo> <mml:mi>Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mtext>supremum</mml:mtext> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="2pt"/><mml:msub><mml:mtext>log</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula></p>
<p>For previous work, we must approximate the channel capacity since we do not have access to the details of the classification performance, <inline-formula id="pcbi.1007091.e011"><alternatives><graphic id="pcbi.1007091.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>Y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>|</mml:mo> <mml:mi>Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Wolpaw et. al. [<xref ref-type="bibr" rid="pcbi.1007091.ref049">49</xref>] suggest an approximation that assumes all classes have the same accuracy as the mean accuracy and all errors are distributed equally (note that this second assumption is generally not true in speech, i.e. <xref ref-type="fig" rid="pcbi.1007091.g004">Fig 4C</xref>, also noted in [<xref ref-type="bibr" rid="pcbi.1007091.ref023">23</xref>]). To make a fair comparison, we compute this approximate value for our results in addition to the exact value. For our data, we find that the approximation underestimates the true channel capacity for the CV and consonant task. The Information Transfer Rate (ITR) is also commonly reported, which is the channel capacity divided by the symbol duration in time. Since we are considering fixed length measurements (1.3 s), we report channel capacity rather than ITR.</p>
</sec>
</sec>
<sec id="sec009">
<title>Structure of deep network predictions</title>
<p>Neuroscientists commonly study the model/confusions of linear analysis methods to gain insight into the structure of neural data. Deep networks can learn high dimensional, nonlinear features from data. Here, these features are learned by training the networks to perform classification, i.e. maximize <inline-formula id="pcbi.1007091.e012"><alternatives><graphic id="pcbi.1007091.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> where the subscript <italic>i</italic> indicates true class membership. It has been shown that these features contain more information than the thresholded multinomial classification prediction [<xref ref-type="bibr" rid="pcbi.1007091.ref050">50</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref051">51</xref>]. The off-diagonal values: <inline-formula id="pcbi.1007091.e013"><alternatives><graphic id="pcbi.1007091.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, <italic>i</italic> ≠ <italic>j</italic>, in this learned distribution represent prediction uncertainty for a given CSEP measurement. Uncertainty is learned during the training process and larger pairwise uncertainty between class labels means that the model has a harder time distinguishing those classes. Since the uncertainty (similarity) is not encoded in the supervised labels, this means that the neural data for those class labels is more similar.</p>
<p>To gain insight into the nature of the vSMC neural activity, we analyzed the structure of deep network predictions. The mean network prediction probabilities on the test set are used as features for each CV. A dendrogram was computed from the hierarchical clustering (Ward’s method) of these features. To aid visualization of these results, a threshold in the cluster distance was chosen by visual inspection of when the number of clusters as a function of distance rapidly increased, and the linguistic features were labeled by hand. The CV order from this clustering was used to order the features in the soft-confusion matrix and accuracy per CV. The soft confusion matrix shows mean network prediction probabilities on the test set rather than the aggregated thresholded predictions often shown in confusion matrices.</p>
<p>To compare the articulatory features and the deep network features quantitatively across subjects, pairwise distances between CVs were computed in both the articulatory and deep network spaces (see <xref ref-type="supplementary-material" rid="pcbi.1007091.s005">S1 Fig</xref> for articulatory features). These pairwise distances were then correlated per for each CV and subject and articulatory grouping.</p>
</sec>
<sec id="sec010">
<title>Cross-band amplitude-amplitude correlations</title>
<p>To examine the relationship between the amplitudes of different frequency components of recorded CSEPs, we first performed a correlation analysis. For this analysis, the data was trial-averaged per CV then organized into a data-tensor, <italic>D</italic><sub>CV,frequency,electrode,time</sub>. The frequency bands were then either used individually or aggregated into canonical frequency components, such as H<italic>γ</italic> <disp-formula id="pcbi.1007091.e014"><alternatives><graphic id="pcbi.1007091.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>H</mml:mtext> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mtext>CV</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>electrode</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>time</mml:mtext></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mtext>CV</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>frequency</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>electrode</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>time</mml:mtext></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mrow><mml:mtext>frequency</mml:mtext> <mml:mo>∈</mml:mo> <mml:mtext>H</mml:mtext> <mml:mi>γ</mml:mi></mml:mrow></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula></p>
<p><inline-formula id="pcbi.1007091.e015"><alternatives><graphic id="pcbi.1007091.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>H</mml:mtext> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mtext>CV</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>electrode</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>time</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> was correlated across time at 0 ms lag with each of the 40 Gaussian bandpassed amplitudes averaged across CVs and electrodes. The correlation between <inline-formula id="pcbi.1007091.e016"><alternatives><graphic id="pcbi.1007091.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>H</mml:mtext> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mtext>CV</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>electrode</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>time</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1007091.e017"><alternatives><graphic id="pcbi.1007091.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mtext>CV</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>electrode</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>time</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> was computed and histogrammed across CVs and electrodes. The average H<italic>γ</italic> power was averaged in a window 70 ms before and 140 ms after the CV acoustic transition and histogrammed across CVs and electrodes. This window was chosen as it is the most active and informative time period for consonants and vowels.</p>
</sec>
<sec id="sec011">
<title>Resolved cross-band amplitude-amplitude correlation</title>
<p>Since the ECoG grid covers a large functional area of vSMC and the CV task differentially engages articulators for different consonant and vowels, the correlations can be computed independently for “active” versus “inactive” electrodes for each CV (averaged across trials). To define active and inactive electrode groups for a band, <inline-formula id="pcbi.1007091.e018"><alternatives><graphic id="pcbi.1007091.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mi mathvariant="double-struck">B</mml:mi></mml:math></alternatives></inline-formula> and H<italic>γ</italic>, first, the <inline-formula id="pcbi.1007091.e019"><alternatives><graphic id="pcbi.1007091.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mi mathvariant="double-struck">B</mml:mi> <mml:mtext>-H</mml:mtext> <mml:mi>γ</mml:mi></mml:math></alternatives></inline-formula> amplitude-amplitude correlation, <inline-formula id="pcbi.1007091.e020"><alternatives><graphic id="pcbi.1007091.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:mi>C</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="double-struck">B</mml:mi> <mml:mo>,</mml:mo> <mml:mtext>H</mml:mtext> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and average H<italic>γ</italic> amplitude, <italic>A</italic>(H<italic>γ</italic>), with positive average amplitude (greater than baseline) are used to fit one linear model with ordinary least-squares regression
<disp-formula id="pcbi.1007091.e021"><alternatives><graphic id="pcbi.1007091.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>m</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:munder><mml:mrow><mml:mtext>arg</mml:mtext> <mml:mspace width="4pt"/><mml:mtext>max</mml:mtext></mml:mrow> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>b</mml:mi></mml:mrow></mml:munder> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="double-struck">B</mml:mi> <mml:mo>,</mml:mo> <mml:mtext>H</mml:mtext> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mtext>H</mml:mtext> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
for all electrodes, <italic>i</italic>, and CVs, <italic>j</italic>. The electrodes were then divided into “active” and “inactive” per CV by thresholding the average H<italic>γ</italic> activity where the linear fit predicted 0 correlation.
<disp-formula id="pcbi.1007091.e022"><alternatives><graphic id="pcbi.1007091.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>A</mml:mi> <mml:mtext>thresh</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mtext>H</mml:mtext> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>−</mml:mo> <mml:mfrac><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mover accent="true"><mml:mi>m</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
Electrodes with average H<italic>γ</italic> activity above threshold were active, and those with lower average H<italic>γ</italic> activity were inactive. The active and inactive electrodes per CV were separated and <inline-formula id="pcbi.1007091.e023"><alternatives><graphic id="pcbi.1007091.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007091.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>H</mml:mtext> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mtext>CV</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>electrode</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>time</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> was correlated across time at 0 ms lag with each of the 40 Gaussian bandpassed amplitudes averaged across CVs and electrodes independently for the active and inactive electrodes and for each subject.</p>
</sec>
<sec id="sec012">
<title>Classification from other frequency bands</title>
<p>An extended sets of lower frequency features per trial were used in addition to the H<italic>γ</italic> features for each of the theta, alpha, low beta, high beta, and gamma bands. The lower frequency amplitudes are highly oversampled at 200 Hz (the H<italic>γ</italic> sampling frequency), and overfitting due to this mismatch will confound the interpretations of signal content. To minimize overfitting, the lower frequency amplitudes were downsampled as described in the Signal processing subsection. For each frequency band, fully-connected deep networks were trained first on the individual bands’s features and then with the band’s features concatenated with the H<italic>γ</italic> features. Deep network training was done in the same manner at the networks trained solely on H<italic>γ</italic> features. The resulting classification accuracies were then compared with the baseline H<italic>γ</italic> classification accuracy and then with the band’s features concatenated with the H<italic>γ</italic> features.</p>
</sec>
</sec>
<sec id="sec013" sec-type="results">
<title>Results</title>
<p>A subset of the electrodes of the ECoG grid overlaid on the vSMC of Subject 1 is shown in <xref ref-type="fig" rid="pcbi.1007091.g001">Fig 1A</xref>. Cortical electric surface potentials (CSEPs) were recorded from the left hemisphere of 4 subjects during the production of a set of consonant-vowel syllables which engage different section of the vocal tract, as shown in <xref ref-type="fig" rid="pcbi.1007091.g001">Fig 1B</xref>, to produce acoustics which are shown in <xref ref-type="fig" rid="pcbi.1007091.g001">Fig 1C</xref>. The trial-averaged z-scored high gamma (H<italic>γ</italic>) amplitude recorded during the production of the syllables from <xref ref-type="fig" rid="pcbi.1007091.g001">Fig 1B</xref> show spatially and temporally distributed patterns of activity (<xref ref-type="fig" rid="pcbi.1007091.g001">Fig 1D</xref>). Here we see that cortical surface electrical potentials recorded from vSMC during the production of CVs consists of multiple spatially and temporally overlapping patterns.</p>
<fig id="pcbi.1007091.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007091.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Human ECoG recordings from ventral sensorimotor cortex (vSMC) during speech production.</title>
<p><bold>A</bold> Electrodes overlaid on vSMC. Electrodes are colored red-to-black with increasing distance from the Sylvian Fissure. <bold>B-D</bold> Task and data summary for three different consonant-vowel (CV) utterances. <bold>B</bold> Vocal tract configuration and point of constriction (orange dot) during the consonant for the production of /ba/ (lips), /da/ (coronal tongue), and /ga/ (dorsal tongue). <bold>C)</bold> The audio spectrogram aligned to the consonant-to-vowel acoustic transition (dashed line). <bold>D</bold> Mean across trials of the H<italic>γ</italic> amplitude from a subset of electrodes in vSMC aligned to CV transition. Traces are colored red-to-black with increasing distance from the Sylvian Fissure as in <bold>A</bold>. The syllables /ba/, /da/, and /ga/ are generated by overlapping yet distinct spatio-temporal patterns of activity across vSMC. <bold>E</bold> Logistic regression accuracy for consonants and vowels plotted against time aligned to the CV transition averaged across subjects and folds. Black and grey traces are average (± s.e.m., <italic>n</italic> = 40) accuracies for consonants (18–19 classes) and vowels (3 classes) respectively.</p></caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.g001" xlink:type="simple"/></fig>
<p>Spatiotemporal patterns of activity represent information about the produced syllables [<xref ref-type="bibr" rid="pcbi.1007091.ref020">20</xref>]. This is shown by training multinomial logistic regression models independently at each time point using all electrodes in vSMC (<xref ref-type="fig" rid="pcbi.1007091.g001">Fig 1E</xref>). Across subjects, the consonant classification accuracy rises from chance approximately 250 ms before the consonant-vowel acoustic transition at 0 ms, which precedes the acoustic production of the consonants, indicating the motor nature of the recordings. Consonant classification accuracy remains above chance for approximately 200 ms into vowel acoustics production. Vowel classification accuracy rises just before the transition to vowel acoustics production and remains above chance for approximately 500 ms. These results show that the consonant and vowel identity is encoded in the H<italic>γ</italic> amplitude in partially-overlapping temporal segments.</p>
<sec id="sec014">
<title>Deep learning for speech classification</title>
<sec id="sec015">
<title>Deep networks outperform standard methods for consonant-vowel classification from high gamma amplitude</title>
<p>It has been shown that CSEPs contain information about motor control [<xref ref-type="bibr" rid="pcbi.1007091.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref052">52</xref>] and variability [<xref ref-type="bibr" rid="pcbi.1007091.ref021">21</xref>]. Regressing CSEP time-frequency features onto behavioral features with linear methods has been used to elucidate the information content. Linear decoders can put a lower bound on the behaviorally relevant information in a measurement, but the restriction to linear mappings may limit the amount of information they are able to extract from the neural signal.</p>
<p>Deep networks can learn more complex, nonlinear mappings, which can potentially extract more information from a neural signal. Thus, they may be able to put a tighter lower bound on the information relevant for speech classification contained in CSEP features. To test this, fully connected deep networks and baseline multinomial logistic regression models were trained on z-scored <italic>Hγ</italic> amplitude from all electrodes in vSMC and time points in a window around CV production. <xref ref-type="fig" rid="pcbi.1007091.g002">Fig 2</xref> shows how the raw CSEP measurements are preprocessed into time-frequency features across behavioral trials, selected and grouped into datasets, and are used in the deep network hyperparameter cross-validation loop. The networks with the highest validation accuracy, averaged across 10 folds, were selected and their results on a held-out test set are reported.</p>
<fig id="pcbi.1007091.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007091.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Data processing and deep network training pipeline for ECoG data.</title>
<p><bold>A</bold> Cortical surface electrical potentials plotted against time for a subset of the vSMC electrodes segmented to the CV production window. Electrodes have an arbitrary vertical offset for visualization. <bold>B</bold> Voltage for one electrode. <bold>C</bold> The z-scored analytic amplitude is shown for a subset of the 40 frequency ranges used in the Hilbert Transform as a function of time. <bold>D</bold> The 40 ranges used in the Hilbert Transform are grouped and averaged according to whether their center frequency is part of each traditional neuroscience band. <bold>E</bold> For a particular analysis, a subset of the bands are chosen as features, and this process was repeated for each trial (sub-pane) and electrode (trace within each sub-pane) in vSMC. Each data sample consists of one trial’s H<italic>γ</italic> activity for all electrodes in vSMC. <bold>F</bold> Data were partitioned 10 times into training, validation, and testing subsets (80%, 10%, and 10% respectively) with independent testing subsets. We trained models that varied in a large hyper-parameter space, including network architecture and optimization parameters, symbolized by the 3 networks on the left with differing numbers of units and layers. The optimal model (right) is chosen based on the validation accuracy and results are reported on the test set.</p></caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.g002" xlink:type="simple"/></fig>
<p>Behaviorally, speech is organized across multiple levels. Even within the simple CV task examined here, there are multiple levels of attributes that can be associated with each CV syllable. The simplest description of the CVs correspond to the consonant constriction location, consonant constriction degree, or vowel labels (3-way tasks). <xref ref-type="fig" rid="pcbi.1007091.g003">Fig 3A–3C</xref> shows the accuracy in these cases respectively. For these tasks, subjects with baseline accuracy close to chance see little-to-no improvement and subjects with larger improvements are limited by the low complexity of the 3-way classification task. In order to partially normalize task complexity across tasks with very different numbers of outcome possibilities (and hence different chance levels), accuracy/chance is shown in <xref ref-type="fig" rid="pcbi.1007091.g003">Fig 3</xref>. This normalization highlights performance on tasks with higher complexity, e.g., CV classification, which would otherwise have lower accuracy than simpler tasks, e.g., vowel classification. <xref ref-type="supplementary-material" rid="pcbi.1007091.s006">S2 Fig</xref> shows this same data plotted as raw accuracy. An intermediate level of complexity is the consonant label (18 or 19-way, <xref ref-type="fig" rid="pcbi.1007091.g003">Fig 3D</xref>). The highest deep network accuracy for a single subject on the consonant task is for Subject 1 which is 59.0 ± 2.2% (11.1 times chance, 5.3%) and 51.2 ± 1.8% (9.7 times chance, 5.3%) for logistic regression and deep networks respectively, which is a 15.3% improvement. Mean consonant classification accuracy across subjects (19 way) with deep networks is 41.2 ± 14.3%. For logistic regression, it is 36.5 ± 12.3%.</p>
<fig id="pcbi.1007091.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007091.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Classification accuracy of logistic regression versus deep networks for different classification tasks.</title>
<p>For <bold>A</bold>-<bold>E</bold>, accuracies (± s.e.m., n = 10) are normalized to chance (chance = 1, dashed blue line) independently for each subject and task. Points on the left are multinomial logistic regression accuracy and are connected to the points on the right which are deep network accuracies for each subject. Subject accuracies have been left-right jittered to prevent visual overlap and demarcated with color (legend in <bold>E</bold>). <bold>A-D</bold> Classification accuracy when CV predictions are restricted to consonant constriction location (<bold>A</bold>), consonant constriction degree (<bold>B</bold>), vowel (<bold>C</bold>), or consonant (<bold>D</bold>) classification tasks. <bold>E</bold> Classification of entire consonant-vowel syllables from H<italic>γ</italic> amplitude features. *<italic>p</italic> &lt; 0.05, WSRT, Bonferroni corrected with <italic>n</italic> = 4. n.s., not significant. Significance was tested between deep network and logistic regression accuracies.</p></caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.g003" xlink:type="simple"/></fig>
<p>Finally, the most complex task is CV classification which has between 54 and 57 classes across subjects. The highest deep network accuracy for a single subject on the consonant vowel task is for Subject 1 which is 55.1 ± 2.3% (31.0 times chance, 1.8%) and 44.6 ± 3.2% (25.1 times chance, 1.8%) for logistic regression and deep networks respectively, which is a 24.0% improvement (<xref ref-type="fig" rid="pcbi.1007091.g003">Fig 3E</xref>). Mean consonant vowel classification accuracy across subjects (54-57 way) with deep networks is 33.7 ± 16.4%. For logistic regression, it is 28.0 ± 12.9%. Per subject improvements (change in accuracy normalized to chance) for Subjects 1 through 4 are 5.9x (<italic>p</italic> &lt; 0.05), 0.8x (n.s.), 1.6x (<italic>p</italic> &lt; 0.05), and 4.3x (<italic>p</italic> &lt; 0.05). For each subject, a Wilcoxon Signed-Rank Test (WSRT) was performed and the resulting p value was Bonferroni corrected (<italic>n</italic> = 4). For the 3 significant results, the p-value was at the floor for a WSRT with <italic>n</italic> = 10 samples and no equal differences.</p>
<p>The results described above contain many potential sources of variation. To test the significance of these variations, we use an ANOVA with subject, model type (deep network versus logistic regression), task complexity (CV versus consonant versus vowel, location, degree), and model-task complexity interaction as categorical groupings. This model is significant (f-statistic: 177.0, <italic>p</italic> &lt; 1 × 10<sup>−10</sup>) and all coefficients were significant at <italic>p</italic> &lt; 0.001 except for the deep network-consonant interaction which was significant at <italic>p</italic> &lt; 0.05 with Subject 1, CV task, and logistic regression categories as the reference treatment (see <xref ref-type="supplementary-material" rid="pcbi.1007091.s002">S2 Table</xref> for details). This shows that deep networks are able to provide better estimate of information contained in the H<italic>γ</italic> amplitude as compared to linear methods.</p>
<p>The number of speech tokens, duration of a task, and recording modality often differ from study to study [<xref ref-type="bibr" rid="pcbi.1007091.ref049">49</xref>]. This means that quantifying the quality of speech classification from neural signals using accuracy or accuracy normalized to chance can be misleading. The Information Transfer Rate (ITR, bits per second) is a quantity that combines both accuracy and speech in a single quantity [<xref ref-type="bibr" rid="pcbi.1007091.ref049">49</xref>]. Since we are comparing fixed length syllables, this is equivalent to calculating the number of bits per syllable which can be calculated with the Channel Capacity (CC, <xref ref-type="disp-formula" rid="pcbi.1007091.e010">Eq 6</xref>). The ITR can be calculated by diving the CC by the syllable duration. A summary of the accuracy results along with channel capacity estimates are summarized in <xref ref-type="table" rid="pcbi.1007091.t001">Table 1</xref> and compared against the results of Mugler et al. [<xref ref-type="bibr" rid="pcbi.1007091.ref023">23</xref>] which has a similar task and used linear discriminant analysis (LDA) as the classifier. Additional classification metrics are reported in <xref ref-type="supplementary-material" rid="pcbi.1007091.s003">S3 Table</xref>. Deep networks achieve state of the art classification accuracy and have the highest CC, and therefore ITR, on the full CV task. The state of the art accuracy and ITR are important quantities for brain-computer interfaces, which often limit communication rates in clinical applications.</p>
<table-wrap id="pcbi.1007091.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007091.t001</object-id>
<label>Table 1</label>
<caption>
<title>Classification and channel capacity results.</title></caption>
<alternatives>
<graphic id="pcbi.1007091.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/></colgroup>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Accuracy</th>
<th align="left">Acc./Chance</th>
<th align="left">CC (Bits/Syllable)</th></tr></thead>
<tbody>
<tr>
<td align="left"><bold>Deep network, 57 CV, single subj</bold>.</td>
<td align="left"><bold>55.1 ± 2.3%</bold></td>
<td align="left"><bold>31.0x</bold></td>
<td align="left"><bold>2.25 (3.92 exact)</bold></td></tr>
<tr>
<td align="left">Deep network, 57 CV, subj. average</td>
<td align="left">33.7 ± 16.4%</td>
<td align="left">18.6x</td>
<td align="left">1.15 (2.94 exact)</td></tr>
<tr>
<td align="left">Logistic Regression, 57 CV, single subj.</td>
<td align="left">44.6 ± 3.2%</td>
<td align="left">25.1x</td>
<td align="left">1.63 (3.49 exact)</td></tr>
<tr>
<td align="left">Logistic Regression, 57 CV, subj. average</td>
<td align="left">28.0 ± 12.9%</td>
<td align="left">15.5x</td>
<td align="left">0.86 (2.71 exact)</td></tr>
<tr>
<td align="left"><bold>Deep network, 19 cons., single subj</bold>.</td>
<td align="left"><bold>59.0 ± 2.2%</bold></td>
<td align="left"><bold>11.1x</bold></td>
<td align="left"><bold>1.6 (2.42 exact)</bold></td></tr>
<tr>
<td align="left">Deep network, 19 cons., subj. average</td>
<td align="left">41.2 ± 14.3%</td>
<td align="left">7.7x</td>
<td align="left">0.91 (1.63 exact)</td></tr>
<tr>
<td align="left">Logistic Regression, 19 cons., single subj.</td>
<td align="left">51.2 ± 1.8%</td>
<td align="left">9.7x</td>
<td align="left">1.25 (2.04 exact)</td></tr>
<tr>
<td align="left">Logistic Regression, 19 cons., subj. average</td>
<td align="left">36.5 ± 12.3%</td>
<td align="left">6.8x</td>
<td align="left">0.72 (1.41 exact)</td></tr>
<tr>
<td align="left">LDA [<xref ref-type="bibr" rid="pcbi.1007091.ref023">23</xref>], 24 cons., single subj.</td>
<td align="left">36.1%</td>
<td align="left">4.9x</td>
<td align="left">0.75</td></tr>
<tr>
<td align="left">LDA [<xref ref-type="bibr" rid="pcbi.1007091.ref023">23</xref>], 24 cons., subj. average</td>
<td align="left">20.4 ± 9.8%</td>
<td align="left">2.8x</td>
<td align="left">0.25</td></tr>
<tr>
<td align="left"><bold>Deep network, 3 vowels, single subj</bold>.</td>
<td align="left"><bold>85.9 ± 2.3%</bold></td>
<td align="left"><bold>2.6x</bold></td>
<td align="left"><bold>1.19 (0.87 exact)</bold></td></tr>
<tr>
<td align="left">Deep network, 3 vowels, subj. average</td>
<td align="left">67.3 ± 13.1%</td>
<td align="left">2.0x</td>
<td align="left">0.63 (0.45 exact)</td></tr>
<tr>
<td align="left">Logistic Regression, 3 vowels, single subj.</td>
<td align="left">78.7 ± 2.6%</td>
<td align="left">2.4x</td>
<td align="left">0.91 (0.64 exact)</td></tr>
<tr>
<td align="left">Logistic Regression, 3 vowels, subj. average</td>
<td align="left">61.3 ± 11.8%</td>
<td align="left">1.8x</td>
<td align="left">0.46 (0.32 exact)</td></tr>
<tr>
<td align="left">LDA [<xref ref-type="bibr" rid="pcbi.1007091.ref023">23</xref>], 15 vowels, single subj.</td>
<td align="left">23.9%</td>
<td align="left">1.9x</td>
<td align="left">0.22</td></tr>
<tr>
<td align="left">LDA [<xref ref-type="bibr" rid="pcbi.1007091.ref023">23</xref>], 15 vowels, subj. average</td>
<td align="left">19.2 ± 3.7%</td>
<td align="left">1.5x</td>
<td align="left">0.12</td></tr></tbody></table></alternatives></table-wrap>
<p>How the accuracy and precision of data analysis results scale with dataset size is an important metric for designing future experiments. This is especially true when working with human subjects and invasive or time consuming data collection methods. In the context of brain-computer interface (BCI) research, maximizing BCI performance is a central goal and so understanding how performance is limited by dataset size or decoding/classification methods is crucial for improving clinical use and understanding the potential role of deep networks in BCIs.</p>
<p>Deep networks are well known for their performance on enormous machine learning datasets. Since neural datasets are typically much smaller, we sought to explore the data efficiency of deep networks in the context of speech classification from CSEPs. We subsampled the training datasets by up to 50 percent in order to estimate accuracy improvements as a function of dataset size. The subsampled training dataset sizes and resulting classification accuracies were then used to estimate the slope of the accuracy as a function of dataset size.</p>
<p>As the fraction of the training set was changed from 0.5 to 1.0, deep network accuracies improve (<xref ref-type="supplementary-material" rid="pcbi.1007091.s007">S3 Fig</xref> panel A, solid lines). The accuracy relative to chance is higher for deep networks than for logistic regression (<xref ref-type="supplementary-material" rid="pcbi.1007091.s007">S3 Fig</xref> panel A, dotted lines) across dataset fractions for Subjects 1, 3, and 4. <xref ref-type="supplementary-material" rid="pcbi.1007091.s008">S4 Fig</xref> contains the same data plotted as raw accuracy and change in accuracy. Deep networks have slopes of Subject 1: 10.1 ± 0.9%, Subject 2: 8.1 ± 1.3%, Subject 3: 2.3 ± 0.3%, and 22.0 ± 2.9% change in accuracy per 1000 training examples (<xref ref-type="supplementary-material" rid="pcbi.1007091.s007">S3 Fig</xref> panel B) which are not significantly different from logistic regression slopes. There is no visual indication that performance has saturated for any subject which means the accuracy of classified speech production is in part limited by the amount of training data that can be collected.</p>
</sec>
</sec>
<sec id="sec016">
<title>Deep networks have classification confusions that reveal the latent articulatory organization of vSMC</title>
<p>Despite being able to mathematically specify the computations happening everywhere in the model, deep networks are often described as “black boxes”. What deep networks learn and how it depends on the structure of the dataset is not generally understood. This means that deep networks currently have limited value for scientific data analysis because their learned latent structure cannot be mapped back onto the structure of the data. Many current uses of deep networks in scientific applications rely on their high accuracy and do not inspect the network computations [<xref ref-type="bibr" rid="pcbi.1007091.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref054">54</xref>], although there are results in low dimensional networks [<xref ref-type="bibr" rid="pcbi.1007091.ref015">15</xref>] and early sensory areas [<xref ref-type="bibr" rid="pcbi.1007091.ref018">18</xref>]. Nevertheless, deep networks’ ability to consume huge datasets without saturating performance means that expanding their use in science is limited by our understanding of their ability to learn about the structure of data. For the dataset consider in this work, previous studies have shown that an articulatory hierarchy can be derived from the trial-averaged H<italic>γ</italic> amplitude using principal components analysis at hand-selected points in time [<xref ref-type="bibr" rid="pcbi.1007091.ref020">20</xref>]. Note that the articulatory structure of the consonants and vowels are not contained in the CV labels nor are the individual consonant or individual vowel labels due to the CVs being encoding in a one-hot fashion, i.e., /ba/ (label = 0) is as different from /bi/ (label = 1) as it is from /gu/ (label = 8) according to the CV labels even though they share a consonant (likewise for shared vowels).</p>
<p>To explore whether deep networks can infer this latent structure from the training data, we examined the structure of network output to better understand the organization of deep network syllable representations extracted from vSMC. Deep networks used for classification predict an entire distribution over class labels for each data sample. This learned distribution has been shown to be a useful training target in addition to the thresholded class labels [<xref ref-type="bibr" rid="pcbi.1007091.ref050">50</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref051">51</xref>]. We clustered these learned representations and compared them to articulatory representations of the CVs.</p>
<p>The dendrogram resulting from agglomerative hierarchical clustering on the trial averaged output of the softmax of the deep network (i.e., before thresholding for classification) averaged across subjects shows clusters spread across scales (<xref ref-type="fig" rid="pcbi.1007091.g004">Fig 4A</xref>). A threshold was chosen by inspection of when the number of clusters as a function of cutoff distance rapidly increased (<xref ref-type="fig" rid="pcbi.1007091.g004">Fig 4B</xref>) and used to color the highest levels of the hierarchy. At the highest level, syllables are confused only within the major articulator involved (lips, back tongue, or front tongue) in the syllable. This is followed by a characterization of the place of articulation within each articulator (bilabial, labio-dental, etc.). At the lowest level there seems to be a clustering across the consonant constriction degree and vowel categories that capture the general shape of the vocal tract in producing the syllable. When ordered by this clustering, the soft confusion matrix (<xref ref-type="fig" rid="pcbi.1007091.g004">Fig 4C</xref>) resulting from the average output of the final layer softmax shows block-diagonal structure corresponding to the articulatory hierarchy. In contrast, deep networks trained on the mel-cepstral coefficients and their time-differences (similar dimensionality to the H<italic>γ</italic> amplitude) show a largely inverted hierarchy, results which mirror those found in more general studies of deep network processing of spoken acoustics [<xref ref-type="bibr" rid="pcbi.1007091.ref055">55</xref>] (See <xref ref-type="supplementary-material" rid="pcbi.1007091.s009">S5 Fig</xref> for this analysis on the data presented here). There is a large amount of variation in the per-CV accuracies (<xref ref-type="fig" rid="pcbi.1007091.g004">Fig 4D</xref>).</p>
<fig id="pcbi.1007091.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007091.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Deep network predictions reveal a latent articulatory hierarchy from single-trial ECoG recordings.</title>
<p><bold>A</bold> The dendrogram from a hierarchical clustering of deep network predictions on the test set averaged across all subjects. The threshold for the colored clusters (dashed gray) is determined from inspection of the number of clusters as a function of distance cutoff shown in <bold>B</bold>. Clusters centroids are labeled with articulatory features shared by leaf CVs. DT: dorsal tongue, CT: coronal tongue, BL: bilabial, LD: labiodental, S: sibilant, A: alveolar. <bold>B</bold> Number of clusters (vertical axis) as a function of the minimum cutoff distance between cluster centroids (horizontal axis). <bold>C</bold> Average predicted probability per CV for Subject 1. CVs are ordered from clustering analysis in <bold>A</bold>. <bold>D</bold> Accuracy of individual CVs. <bold>E</bold> Correlation between pairwise distances in deep network similarity space from <bold>C</bold> compared to distances in an articulatory/phonetic feature space for Major Articulator, Consonant Constriction Location, Consonant Constriction Degree, and Vowel, aggregated across all subjects. Center bar is the median and boundaries are 50% confidence intervals. Colored circles indicate subject medians. **<italic>p</italic> &lt; 1 × 10<sup>−10</sup>, WSRT, *<italic>p</italic> &lt; 1 × 10<sup>−4</sup> t-test, both Bonferroni corrected with <italic>n</italic> = 4.</p></caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.g004" xlink:type="simple"/></fig>
<p>This hierarchy can be quantified by comparing the space of deep network prediction probabilities and the space of articulatory features associated with each CV. This comparison was made by correlating pairwise CV distances in these two features spaces across all pairs of CVs. The resulting structure of correlations is consistent with an articulatory organization in vSMC (<xref ref-type="fig" rid="pcbi.1007091.g004">Fig 4C</xref>). The major articulators feature distances are most correlated with the distances between CVs in deep network space, then consonant constriction location, and finally consonant constriction degree and vowel.</p>
<p>Together, these results show that deep networks trained to classify speech from H<italic>γ</italic> activity are learning an articulatory latent structure from the neural data. Qualitatively similar hierarchies can be derived using PCA and logistic regression. Indeed, this structure is in agreement with previous analyses of mean spatial patterns of activity at separate consonant and vowel time points [<xref ref-type="bibr" rid="pcbi.1007091.ref020">20</xref>] while allowing the consonants and vowels to be part of the same hierarchy. However, the deep network hierarchy has larger correlations and more separation between levels than the hierarchy derived from the Logistic regression model (shown in <xref ref-type="supplementary-material" rid="pcbi.1007091.s010">S6 Fig</xref>). Together, these results demonstrate the capacity of deep networks to reveal underlying structure in single-trial neural recordings.</p>
</sec>
<sec id="sec017">
<title>The high gamma and beta bands show a diversity of correlations across electrodes and CVs</title>
<p>Complex behaviors, such as speech, involve the coordination of multiple articulators on fast timescales. These articulators are controlled by spatially distributed functional areas of cortex. Lower frequency oscillations have been proposed as a coordinating signal in cortex. Previous studies have reported movement- or event-related beta (<italic>β</italic>)-H<italic>γ</italic> desynchronization or decorrelation [<xref ref-type="bibr" rid="pcbi.1007091.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref042">42</xref>]. The differential structure of these correlations across tasks and functions areas is not commonly analyzed. Since cortex often shows sparse and spatially-differentiated activity across tasks [<xref ref-type="bibr" rid="pcbi.1007091.ref022">22</xref>], averaging over electrodes and tasks may obscure structure in the cross-frequency relationships.</p>
<p>The CV task and grid coverage allow average neural spectrograms (zscored amplitude as a function of frequency and time) to be measured at two electrodes during the production of the syllable \ga\ (<xref ref-type="fig" rid="pcbi.1007091.g005">Fig 5A and 5B</xref>, median acoustic spectrogram is shown above). In order to investigate this, we measured cross frequency amplitude-amplitude coupling (correlation) for individual lower frequency bands and H<italic>γ</italic>. We also examine the aggregate <italic>β</italic> band. Some previous studies attempt to distinguish band-limited and broadband signals in lower frequencies, e.g., <italic>β</italic> [<xref ref-type="bibr" rid="pcbi.1007091.ref056">56</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref058">58</xref>]. However, methods for distinguishing these signals are generally not applied to high sampling rate signals and often require hand-tuning and is an ongoing area of research. As such, here, we are simply looking at correlations between bandpassed signals and not estimating and removing any broadband components. Further modeling would be needed to interpret these signals as correlations between biophysical sources (see <xref ref-type="sec" rid="sec019">Discussion</xref> for discussion). Initially, we pool results across all electrodes and CVs in order to replicate methods from previous studies. The H<italic>γ</italic> and <italic>β</italic> amplitudes show a diverse set of temporal relationships in these regions (<xref ref-type="fig" rid="pcbi.1007091.g005">Fig 5C and 5D</xref>). Across frequencies, H<italic>γ</italic> correlation is positive for low frequencies (&lt; 15Hz), then we see negative and near-zero correlations between H<italic>γ</italic> and the <italic>β</italic> range across subjects, and finally the correlation rises for the <italic>γ</italic> range (30–59 Hz) as the frequencies approach H<italic>γ</italic> (<xref ref-type="fig" rid="pcbi.1007091.g005">Fig 5E</xref>). However, these mean correlations mask a broad range of H<italic>γ</italic>-<italic>β</italic> correlations (<xref ref-type="fig" rid="pcbi.1007091.g005">Fig 5F</xref>) across H<italic>γ</italic> activity (across CVs and electrodes). This includes a large number of positive correlations. Similarly, although most of the amplitudes measured are smaller than baseline (<xref ref-type="fig" rid="pcbi.1007091.g005">Fig 5G</xref>), there is a long tail to amplitudes larger than baseline (above 0).</p>
<fig id="pcbi.1007091.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007091.g005</object-id>
<label>Fig 5</label>
<caption>
<title>H<italic>γ</italic> and <italic>β</italic> bands show diverse correlation structures across electrodes and CVs.</title>
<p><bold>A</bold>-<bold>B</bold> Average amplitude as a function of frequency and time for an electrode with large activity during /ga/ production and for an electrode with no activity during /ga/ production. <bold>C</bold> and <bold>D</bold> Normalized (-1 to 1) H<italic>γ</italic> (red) and <italic>β</italic> (black) activity from <bold>A</bold> and <bold>B</bold> respectively. Non-trivial temporal relationships can be seen in <bold>C</bold> which are not apparent in <bold>D</bold>. <bold>E</bold> The average correlation (± s.e.m.) between the H<italic>γ</italic> amplitude and the single frequency amplitude is plotted as a function of frequency for each subject. Thickened region of the horizontal axis indicates the <italic>β</italic> frequency range. <bold>F</bold> Histogram of the H<italic>γ</italic>-<italic>β</italic> correlation coefficients for all CVs and electrodes for Subject 1. <bold>G</bold> Histogram of the z-scored H<italic>γ</italic> power near the CV acoustic transition (time = 0) for all CVs and electrodes for Subject 1.</p></caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.g005" xlink:type="simple"/></fig>
<p>This diversity of correlations and amplitudes across CVs and electrodes indicates there is potentially substructure in the data that is being averaged over. This motivates a disaggregated analysis of the amplitude-amplitude correlations. Naïvely, one might expect to see different cross-frequency relationships in areas that are actively engaged in a task compared to area which are not engaged. The broad coverage of the ECoG grid and the diversity of articulatory movements across the consonants and vowels in the task allow us to investigate whether there is substructure in the amplitude-amplitude cross frequency correlations.</p>
<p>In order to investigate this, we grouped the H<italic>γ</italic> activity for each electrode and CV into “active” and “inactive” groups based on the average H<italic>γ</italic> power and computed correlations for these two groups. For the two subjects with high accuracy, we observe a positive correlation between H<italic>γ</italic> power and H<italic>γ</italic>-<italic>β</italic> correlation (<xref ref-type="fig" rid="pcbi.1007091.g006">Fig 6A</xref>). For the two subjects with low CV classification accuracy, we observe a generally negative correlation between H<italic>γ</italic> power and H<italic>γ</italic>-<italic>β</italic> amplitude (<xref ref-type="fig" rid="pcbi.1007091.g006">Fig 6B</xref>).</p>
<fig id="pcbi.1007091.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007091.g006</object-id>
<label>Fig 6</label>
<caption>
<title>H<italic>γ</italic> and <italic>β</italic> bands show positive correlations at active electrodes which are not found in inactive electrodes for subjects with high classification accuracy.</title>
<p><bold>A</bold> The trial-averaged H<italic>γ</italic>-<italic>β</italic> correlation coefficient across electrodes and CVs is plotted against the average H<italic>γ</italic> power near the CV acoustic transition for Subjects 1 and 4. Solid lines indicate the linear regression fit to the data with positive z-scored amplitude. The vertical dashed gray line indicates the division in average H<italic>γ</italic> power between ‘active’ and ‘inactive’ electrodes for subjects 1 and 4. Data is summarized in nine bins plotted (± s.e.m.) per subject. <bold>B</bold> Same as <bold>A</bold>, but for Subjects 2 and 3, which have a much lower classification accuracy. <bold>C</bold> For the two subjects in <bold>A</bold>, the average (± s.e.m.) correlation is plotted between the H<italic>γ</italic> amplitude and the single frequency amplitude as a function of frequency separately for active (white center line) and inactive (solid color) electrodes. Thickened region of the horizontal axis indicates the <italic>β</italic> frequency range. <bold>D</bold> Same as <bold>C</bold> for subjects in <bold>B</bold>.</p></caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.g006" xlink:type="simple"/></fig>
<p>The H<italic>γ</italic> correlation can be recomputed separately for active and inactive electrodes per CV. For the subjects with high CV classification accuracy (Subjects 1 and 4), we find a novel signature of motor coordination in the active electrodes: a positive correlation in the <italic>β</italic> frequency band (<xref ref-type="fig" rid="pcbi.1007091.g006">Fig 6C</xref>, lines with white centers). This is in contrast to the inactive electrodes, which show small or negative correlation (<xref ref-type="fig" rid="pcbi.1007091.g006">Fig 6C</xref>, solid lines) which is similar to the aggregated results (<xref ref-type="fig" rid="pcbi.1007091.g005">Fig 5E</xref>). For the two subjects with low CV classification accuracy (Subjects 2 and 3), the disaggregated results (<xref ref-type="fig" rid="pcbi.1007091.g006">Fig 6D</xref>) show less dichotomous structure.</p>
<p>Overall, we find that there is structure across bands in addition to cross-frequency relationship with the H<italic>γ</italic> band which has been used in the preceding classification analysis. As far as we are aware, this is the first observation of dichotomous amplitude-amplitude cross frequency correlation during behavior. This observation was only possible because of the broad functional coverage of the ECoG grid and the diverse behaviors represented in the CV task.</p>
</sec>
<sec id="sec018">
<title>Classification relevant information in lower frequency bands</title>
<p>The gamma and H<italic>γ</italic> band-passed CSEP amplitudes are commonly used both on their own and in conjunction with other frequency bands for decoding produced and perceived speech in humans due to their observed relation to motor and sensory tasks [<xref ref-type="bibr" rid="pcbi.1007091.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref059">59</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref060">60</xref>]. Other frequency bands have been shown to have amplitude or phase activity which is correlated with H<italic>γ</italic> amplitude or spiking activity [<xref ref-type="bibr" rid="pcbi.1007091.ref037">37</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref040">40</xref>]. Indeed, in the data used in this study, we find amplitude-amplitude correlation structure between H<italic>γ</italic> and lower frequency bands. Although these correlations imply that information is shared between H<italic>γ</italic> and other CSEP frequency bands, it is not known whether the other bands contain additional information about motor tasks beyond H<italic>γ</italic> or whether the information is redundant.</p>
<p>In order to understand the relative information content in CSEP frequency bands, we classified CVs from two different sets of features. Linear classification methods would not give a satisfactory answer to this question since they are limited to simple hyper-plane segmentation of the data which may trivially lead to the result of no information. Indeed, since we have shown that deep networks can outperform linear methods when classifying from H<italic>γ</italic>, they are also candidates for showing whether there is any relevant information in these bands. For the theta, alpha, beta, high beta, and gamma bands, each band’s features were first used for classification and then concatenated with the H<italic>γ</italic> features and used for classification. The raw classification accuracy and improvement beyond H<italic>γ</italic> are two measures that give insight into information content in the other bands.</p>
<p>
<xref ref-type="fig" rid="pcbi.1007091.g007">Fig 7</xref> shows the accuracies, normalized to chance, across the four subjects. <xref ref-type="fig" rid="pcbi.1007091.g007">Fig 7A</xref> shows the classification accuracies across subjects for single band features. Across subjects, all single bands have CV classification accuracies greater than chance, although subject-to-subject variation is observed. Although this is significantly above chance, the ranges of improvements for the subject means range between 1.5x to 2x chance, a small accuracies compared to H<italic>γ</italic> accuracies which ranged from 6x to 21x chance. For the single band features, accuracy above chance implies that there is relevant information about the task in the bands. <xref ref-type="fig" rid="pcbi.1007091.g007">Fig 7B</xref> shows the chance in classification accuracy relative to H<italic>γ</italic> accuracy, normalized to chance. No bands see a significant improvement in accuracy over the baseline accuracy obtained by classifying from H<italic>γ</italic>. Indeed, all measured mean changes in accuracy are smaller than the cross-validation standard deviations for the H<italic>γ</italic> accuracy. Together, these results show that there is task-relevant information in lower frequency bands, but the information is largely redundant to the information contained in the H<italic>γ</italic> amplitude.</p>
<fig id="pcbi.1007091.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007091.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Lower frequency bands do not contribute significant additional information to the CV classification task beyond H<italic>γ</italic>.</title>
<p><bold>A</bold> The average accuracy (± s.e.m., <italic>n</italic> = 10) normalized to chance (chance = 1, dashed blue line) is shown for each frequency band and subject. Subjects are left-right jittered to avoid visual overlap. The solid blue line is the mean across subjects for a single band. <bold>B</bold> Average change in accuracy (± s.e.m., <italic>n</italic> = 10) from H<italic>γ</italic> accuracy normalized to chance when band’s features are concatenated with the H<italic>γ</italic> features. The solid blue line is the mean across subjects for a single band. The H<italic>γ</italic> accuracy cross-validation standard deviation (<italic>n</italic> = 10) normalized to chance is plotted above and below zero in the right-most column for each subject for comparison. <bold>C</bold> Average accuracy (± s.e.m., <italic>n</italic> = 10) normalized to chance (dashed blue line, chance = 1) plotted against the correlation coefficient between H<italic>γ</italic> and the lower frequency band for active electrodes for each band and subject. The blue dashed line indicates chance accuracy. <bold>D</bold> Change in accuracy from H<italic>γ</italic> accuracy normalized to chance plotted against the correlation coefficient between H<italic>γ</italic> and the lower frequency band for active electrodes for each band and subject. The blue dashed line indicates no change in accuracy. **<italic>p</italic> &lt; 0.001, *<italic>p</italic> &lt; 0.01, WSRT, n.s., not significant. All Bonferroni corrected with <italic>n</italic> = 5.</p></caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.g007" xlink:type="simple"/></fig>
<p>The correlations observed in Figs <xref ref-type="fig" rid="pcbi.1007091.g005">5</xref> and <xref ref-type="fig" rid="pcbi.1007091.g006">6</xref> imply that there is some shared information between the lower frequency bands and the H<italic>γ</italic> band. However, the classification accuracies from H<italic>γ</italic> alone (<xref ref-type="fig" rid="pcbi.1007091.g003">Fig 3</xref>) are much higher than any other individual frequency band and are not improved by the addition of extra features from lower frequency bands. This shows that the high frequency CSEPs (H<italic>γ</italic> band), which is commonly used in motor decoding, are highly informative signals.</p>
</sec>
</sec>
<sec id="sec019" sec-type="conclusions">
<title>Discussion</title>
<p>The structure or information content of neural data is often estimated by regressing neural features against known features in the stimulus or behavior. Traditionally, this has been done with linear models, which are often poorly matched to the structure of this relationship. Here, we have shown that deep networks trained on high gamma (H<italic>γ</italic>) cortical surface electrical potentials (CSEPs) can classify produced speech with significantly higher accuracy than traditional linear or single layer models. When classifying syllables, deep networks achieved state-of-the-art accuracy and channel capacity: for the subject with higher accuracy, this was 55.1% and 3.92 bits per syllable. At word durations from Mugler et al. [<xref ref-type="bibr" rid="pcbi.1007091.ref023">23</xref>] and one CV syllable per word duration, 3.92 bits per syllable corresponds to 7.5 bits per second or 75 words per minute [<xref ref-type="bibr" rid="pcbi.1007091.ref061">61</xref>]. This could also be combined with a language model to improve accuracy in clinical applications [<xref ref-type="bibr" rid="pcbi.1007091.ref031">31</xref>] towards the eventual goal of natural spoken speech rates (250-600 words per minute). Generally, we expect that as neuroscientific datasets grow, more modern deep learning techniques and architectures will be used for higher precision such as Residual layers [<xref ref-type="bibr" rid="pcbi.1007091.ref006">6</xref>], variational auto-encoders [<xref ref-type="bibr" rid="pcbi.1007091.ref062">62</xref>], and recurrent models for timeseries [<xref ref-type="bibr" rid="pcbi.1007091.ref007">7</xref>]. Together, these results show that deep networks are a promising analytic platform for brain-computer interface (BCI) for speech prosthetics, an application where high accuracy and high training sample efficiency are crucial. Since deep networks are highly parameterized nonlinear models, their online interactions with learning may be more complex than typical methods [<xref ref-type="bibr" rid="pcbi.1007091.ref063">63</xref>]. Studying how deep networks behave in an online BCI will be important future step in integrating them into clinical settings.</p>
<p>Training the deep networks described here to high accuracy required an extensive hyperparameter search over model architectures including layer numbers, layer dimensions, and nonlinearity, along with optimization hyperparameters like learning rate, momentum decay, and dropout fraction. In general, the optimal hyperparameters may depend on the latent structure of the dataset being used, however, they may also depend strongly on the size of the dataset in terms of the number of samples or the dimensionality of each sample. Understanding the relationship between the optimal structure of deep networks and the structure of datasets is a future direction of research.</p>
<p>We observed classification accuracies were highest, both relative to chance and linear models, for consonant-vowel syllables compared to the consonants or vowels individually. This is consistent with previous reports on the presence of both anticipatory and perseverative coarticulation effects in vSMC (see also <xref ref-type="fig" rid="pcbi.1007091.g001">Fig 1E</xref>) [<xref ref-type="bibr" rid="pcbi.1007091.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref023">23</xref>]. Coarticulation refers to the fact that, at a behavioral level, the production of speech phonemes is systematically influenced by the surrounding phonemic context. For communication prosthetics, one might hope to decode the most atomic units, phonemes, and then express the combinatorial complexity of language through combinations of the small number of phonemes. Combined with other studies, the results presented here indicate that coarticulation is a feature of speech motor control that must be accounted for in BCIs.</p>
<p>In contrast to many commercial applications of deep learning, where optimizing prediction accuracy is often the primary goal, in science, it is also desirable to extract latent structure from the data to advance understanding. In the context of the current study, we used deep networks to determine which features of speech production were extracted from the neural activity to solve the classification task. Examination of the consonant-vowel confusions made by the deep networks reveal the underlying articulatory organization of speech production in the vSMC. At the highest level, the deep networks cluster the CVs into the major articulator involved in forming the consonant, i.e. lips, front tongue, or back tongue. The consonant constriction location, e.g. teeth-to-lips versus lips, is in the intermediate level of the hierarchy. Finally, consonant constriction degree and vowel are clustered at the lowest level of the hierarchy. Crucially, the consonant articulatory hierarchy is not present in the CV labels which means that the deep network is extracting this hierarchy from noisy, single-trial CSEPs during training. The articulatory organization we find is consistent with previous studies, which used PCA on the trial-averaged data at specific points in time [<xref ref-type="bibr" rid="pcbi.1007091.ref020">20</xref>]. However, we note that, while consistent with previous findings, the hierarchy observed here reflects structure across consonants and vowels together. This could not have been examined with the previous methodology, which required analyses at separate time points. In this way, deep networks were able to extract novel, more general structure from the data, and did so with much less human supervision.</p>
<p>As with many studies of human ECoG, there was substantial variability across subjects. Subjects 1 and 4 had the highest CV classification accuracy from H<italic>γ</italic> and also showed similar patterns of H<italic>γ</italic> correlations with lower frequencies (<xref ref-type="fig" rid="pcbi.1007091.g005">Fig 5</xref>) as well as H<italic>γ</italic>-<italic>β</italic> correlation distinctions in active versus inactive electrodes (<xref ref-type="fig" rid="pcbi.1007091.g006">Fig 6</xref>). Subjects 2 and 3 had lower accuracy and had less consistently structured H<italic>γ</italic> correlations. While the precise nature of cross-subject variability is unknown, likely extrinsic contributors are uncontrollable variation in the degree of contact of electrodes with cortex which could impact frequency specific SNR, differences in variance across recording sessions blocks, or degree of subject engagement in task. Further intrinsic sources of variability could include the lack or presence of particular articulator representations in the recorded activity or differing levels of broadband signals in lower frequency bands. However, the frequency specific bump in the <italic>β</italic> range observed in Subjects 1 and 4 is unlikely to be explained by a change in broadband power in active electrodes. This would require the power of the broadband signal to be mainly found in the <italic>β</italic> range and not in the frequencies on either side, which is not consistent with broadband power fluctuations. Interestingly, we found no clear relationship between CV decoding accuracy and the number of trials, suggesting that the variance is due to differences with the underlying signal and not overfitting. Developing machine learning techniques for training networks on CSEPs that generalize across subjects (ECoG grid placement, underlying functional organization, differences in spectral strucure, etc.) is an important direction of future research with broad applications for BCIs [<xref ref-type="bibr" rid="pcbi.1007091.ref064">64</xref>].</p>
<p>Previous studies of motor cortex have claimed the existence of “beta-desynchronization” (most commonly a decrease in beta amplitude) during motor production [<xref ref-type="bibr" rid="pcbi.1007091.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref041">41</xref>]. This has led to a variety of hypothesized functions of beta (<italic>β</italic>) band in motor preparation and control, with little consensus across studies. A common methodology in many of these previous studies (especially those done in humans, where the number of samples is small and function of cortex is often sub-sampled) is to aggregate data across all electrodes and tasks. For the two subjects for which there was high-quality decoding accuracy, and thus, likely higher quality CSEP recordings, we found a novel positive coupling, i.e., correlation, between the <italic>β</italic> band and the H<italic>γ</italic> band amplitudes. The positive correlation was band-limited, occurring in the <italic>β</italic> range with a peak near 23Hz, and present at electrode-syllable combinations in which the electrode was active. Thus, uncovering this correlation required that we disaggregate the relation between <italic>β</italic> and H<italic>γ</italic> according to whether an electrode, i.e., articulator, was engaged in the production of a given speech sound. The presence of this coupling is correlated with the classification accuracy from the H<italic>γ</italic> amplitude across subjects. The coupling in engaged functional areas is an example of the possible pitfalls of aggregation across functional areas and specific behaviors or stimuli when the combination of spatial specialization of function and task structure gives rise to sparse activation patterns.</p>
<p>The structure and biophysical origin of broadband and band-limited signals in cortex is an area of active research [<xref ref-type="bibr" rid="pcbi.1007091.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref057">57</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref059">59</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref065">65</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref066">66</xref>]. In neural power spectra, it has been reported that there are broadband fluctuations in power that can be considered as a separate signal from band-limited signals, e.g., <italic>β</italic> power [<xref ref-type="bibr" rid="pcbi.1007091.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref057">57</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref067">67</xref>]. Since this signal is broadband, it may mask or enhance cross-frequency correlations between underlying band-limited signals. Several methods have been proposed for estimating broadband signals and separating them from band-limited signals [<xref ref-type="bibr" rid="pcbi.1007091.ref056">56</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref058">58</xref>]. However, these methods are typically applied to ∼1 second windows with a step size that corresponds to ∼1-2 Hz. In this study, the cross-frequency analysis was performed at 200 Hz (although the lowest frequency bands have autocorrelations due to the choice of bandwidths that correspond to ∼4 Hz). At 200 Hz, across the 4 subjects analyzed here, and across all electrodes in vSMC, there are approximately 250 million points at which a broadband signal would need to be extracted. To our knowledge, methods for estimating broadband signals at this scale that are computationally efficient and do not require per-fit hand tuning have not been developed. Developing methods for estimating high sampling rate, continuous broadband signals is an important direction of future research.</p>
<p>Frequency bands besides H<italic>γ</italic> are known to contain information about stimuli, behavioral, and state variables [<xref ref-type="bibr" rid="pcbi.1007091.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1007091.ref059">59</xref>]. However, comparisons of task-relevant information across neural signals are rarely made. Information theory provides a way of measuring the amount of information about a task in a neural signal, the mutual information, but measuring mutual information across continuous, high dimensional signals is notoriously difficult. In the context of classifying discrete speech tokens, this information can be approximated through the information transfer rate. Being able to compare information across features is particularly useful for CSEPs which results from a variety of electrical processes in the brain [<xref ref-type="bibr" rid="pcbi.1007091.ref035">35</xref>]. Since they achieved higher accuracy then linear or single layer methods, deep networks optimized for accuracy can put a tighter bound on the task-relevant information in a set of neural features. We found that, for the amplitudes of frequency bands lower than H<italic>γ</italic>, it is possible to decode speech syllables with above chance accuracy, though at relatively modest levels. Furthermore, when combined with H<italic>γ</italic> features, the relative improvement in accuracy above H<italic>γ</italic> accuracy is small compared to the cross-validation variance. Thus, for BCIs, these results imply that, for the CV task examined here, only H<italic>γ</italic> activity (or higher frequency signals) need be acquired and analyzed: the other parts of the signal may profitably not be acquired to minimize data acquisition hardware and signal-processing in the decoder.</p>
<p>Although deep networks have shown the ability to maximize task performance across scientific and engineering fields, they are still largely black boxes [<xref ref-type="bibr" rid="pcbi.1007091.ref068">68</xref>]. While there has been some initial investigations [<xref ref-type="bibr" rid="pcbi.1007091.ref069">69</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref073">73</xref>], theoretical and empirical studies have not yet shown how deep networks disentangle the structure of a dataset during training. Currently, deep networks are most commonly used in science in cases where understanding of the deep network’s hidden representation is not needed. While we have taken some initial steps in that direction by examining the networks confusions, revealing how the deep networks disentangled articulatory features from the neural data will be an important extension of this work. An unresoved question is when we can expect deep networks to solve tasks through interpretable latent variables (like phonetic features in the context of speech) and how we can extract these variables from all layers of the learned deep network features (here we only use the learned output probabilities). In general, understanding the interaction between dataset structure and deep network training will make deep networks more broadly useful as a tool for data analytics in science.</p>
<p>Neuroscientists continue to create devices that measure more features in the brain while the stimuli or behavior during data collection become more complex and naturalistic. As the complexity of datasets increase, the tools needed to disentangle and understand these datasets must also evolve. Recently, deep networks have shown promise in analyzing and modeling neural responses in this work and others [<xref ref-type="bibr" rid="pcbi.1007091.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1007091.ref019">19</xref>]. Moving beyond their utility as high-accuracy regression methods will require a more profound understanding of how deep networks learn to represent complex structure from data sets, and tools to extract that structure so as to provide insights to humans. Indeed, many of the open theoretical and analytical challenges facing deep networks are also core to understanding the brain.</p>
</sec>
<sec id="sec020">
<title>Supporting information</title>
<supplementary-material id="pcbi.1007091.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.s001" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Deep network hyperparameter ranges.</title>
<p>Hyperparameters are listed in along with their type and range or options. Nesterov momentum was used as an optimizer for all networks with fixed initial momentum fraction (0.5). The momentum fraction was linearly increased per epoch, starting after the first epoch, to its saturation value. The initial learning rate was exponentially decayed per epoch to a minimum value. Many float hyperparameters were searched in log-space since they typically range over a few orders of magnitude.</p>
<p>(PDF)</p></caption></supplementary-material>
<supplementary-material id="pcbi.1007091.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.s002" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>ANOVA tables.</title>
<p>Summary tables for the ANOVA from subsection: Deep networks outperform standard methods for consonant-vowel classification from high gamma amplitude.</p>
<p>(PDF)</p></caption></supplementary-material>
<supplementary-material id="pcbi.1007091.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.s003" xlink:type="simple">
<label>S3 Table</label>
<caption>
<title>Classification metric comparison.</title>
<p>For each subject and both logistic and deep models, the accuracy, sensitivity, specificity, precision, and F1 score are tabulated for the CV task.</p>
<p>(PDF)</p></caption></supplementary-material>
<supplementary-material id="pcbi.1007091.s004" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.s004" xlink:type="simple">
<label>S1 Appendix</label>
<caption>
<title>Optimal deep network hyperparameters for all models, trained model files, and scripts.</title>
<p>The optimal hyperparameters for each subject and experiment stored in a YAML file. This also includes a README with links to Docker images which can run the preprocessing code and deep network code, trained model files, and plotting scripts and a link to download the raw data.</p>
<p>(ZIP)</p></caption></supplementary-material>
<supplementary-material id="pcbi.1007091.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.s005" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Articulatory features for comparison to deep network prediction features.</title>
<p>For each consonant vowel pair (labeled along top and bottom, respectively), a binary feature vector is shown (white indicates the presence of the feature). The grouping into major articulator, consonant constriction location, consonant constriction degree, and vowel features is shown on the right edge.</p>
<p>(TIF)</p></caption></supplementary-material>
<supplementary-material id="pcbi.1007091.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.s006" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Classification accuracy of logistic regression versus deep networks for different classification tasks.</title>
<p>For <bold>A</bold>-<bold>E</bold>, accuracies (± s.e.m., n = 10) are shown (chance is at the dashed line) independently for each subject and task. Points on the left are multinomial logistic regression accuracy and are connected to the points on the right which are deep network accuracies for each subject. Subject accuracies have been left-right jittered to prevent visual overlap and demarcated with color (legend in <bold>E</bold>). <bold>A-D</bold> Classification accuracy when CV predictions are restricted to consonant constriction location (<bold>A</bold>), consonant constriction degree (<bold>B</bold>), vowel (<bold>C</bold>), or consonant (<bold>D</bold>) classification tasks. <bold>E</bold> Classification of entire consonant-vowel syllables from H<italic>γ</italic> amplitude features.</p>
<p>(TIF)</p></caption></supplementary-material>
<supplementary-material id="pcbi.1007091.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.s007" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Classification accuracy improvement normalized to chance as a function of training dataset size for logistic regression versus deep networks.</title>
<p>Accuracies (± s.e.m., <italic>n</italic> = 10) are normalized to chance (chance = 1, dashed blue line) independently for each subject. Subject error bars have been left-right jittered to prevent visual overlap and demarcated with color (legend in <bold>A</bold>). <bold>A</bold> Average classification accuracy (± s.e.m., <italic>n</italic> = 10) normalized to chance for the CV task as a function of the fraction of training examples used for logistic regression (dotted lines) and deep networks (solid lines). <bold>B</bold> Change in classification accuracy normalized to chance per 1,000 training examples. The total training set sizes vary significantly between subjects so there is an additional per-subject normalization factor between the slopes in <bold>A</bold> and <bold>B</bold>. p-values were Bonferroni corrected with <italic>n</italic> = 4. n.s., not significant.</p>
<p>(TIF)</p></caption></supplementary-material>
<supplementary-material id="pcbi.1007091.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.s008" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Classification accuracy improvement as a function of training dataset size for logistic regression versus deep networks.</title>
<p>Accuracies (± s.e.m., <italic>n</italic> = 10) are shown (chance is at the dashed lines) independently for each subject. Subject error bars have been left-right jittered to prevent visual overlap and demarcated with color (legend in <bold>A</bold>). <bold>A</bold> Average classification accuracy (± s.e.m., <italic>n</italic> = 10) for the CV task as a function of the fraction of training examples used for logistic regression (dotted lines) and deep networks (solid lines). <bold>B</bold> Change in classification accuracy per 1,000 training examples. The total training set sizes vary significantly between subjects so there is an additional per-subject normalization factor between the slopes in <bold>A</bold> and <bold>B</bold>.</p>
<p>(TIF)</p></caption></supplementary-material>
<supplementary-material id="pcbi.1007091.s009" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.s009" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Deep network predictions reveal a latent acoustic hierarchy from single-trial acoustic recordings.</title>
<p>Similar analysis as <xref ref-type="fig" rid="pcbi.1007091.g005">Fig 5</xref> in the main text for networks trained on mel-cepstral coefficients from Subject 1. <bold>A</bold> The dendrogram from a hierarchical clustering of deep network predictions on the test set from Subject 1. The threshold for the colored clusters (dashed gray) is determined from inspection of the number of clusters as a function of distance cutoff shown in <bold>B</bold>. Clusters centroids are labeled with acoustic features shared by leaf CVs. <bold>B</bold> Number of clusters (vertical axis) as a function of the minimum cutoff distance between cluster centroids (horizontal axis). <bold>C</bold> Average predicted probability per CV for Subject 1. CVs are ordered from clustering analysis in <bold>A</bold>. <bold>D</bold> Accuracy of individual CVs for Subject 1. <bold>E</bold> Correlation between pairwise distances in deep network similarity space from <bold>C</bold> compared to distances in an articulatory/phonetic feature space for Major Articulator, Consonant Constriction Location, Consonant Constriction Degree, and Vowel, aggregated across all subjects. Center bar is the median and boundaries are 50% confidence intervals. Colored circles indicate subject medians.</p>
<p>(TIF)</p></caption></supplementary-material>
<supplementary-material id="pcbi.1007091.s010" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007091.s010" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Logistic regression predictions reveal a latent articulatory hierarchy from single-trial ECoG recordings to a lesser extent.</title>
<p>Similar analysis as <xref ref-type="fig" rid="pcbi.1007091.g005">Fig 5</xref> in the main text for Logistic regression. <bold>A</bold> The dendrogram from a hierarchical clustering of deep network predictions on the test set from Subject 1. The threshold for the colored clusters (dashed gray) is determined from inspection of the number of clusters as a function of distance cutoff shown in <bold>B</bold>. Clusters centroids are labeled with acoustic features shared by leaf CVs. <bold>B</bold> Number of clusters (vertical axis) as a function of the minimum cutoff distance between cluster centroids (horizontal axis). <bold>C</bold> Average predicted probability per CV for Subject 1. CVs are ordered from clustering analysis in <bold>A</bold>. <bold>D</bold> Accuracy of individual CVs for Subject 1. <bold>E</bold> Correlation between pairwise distances in deep network similarity space from <bold>C</bold> compared to distances in an articulatory/phonetic feature space for Major Articulator, Consonant Constriction Location, Consonant Constriction Degree, and Vowel, aggregated across all subjects. Center bar is the median and boundaries are 50% confidence intervals. Colored circles indicate subject medians.</p>
<p>(TIF)</p></caption></supplementary-material></sec></body>
<back>
<ack>
<p>We would like to thank Gopala Anumanchipalli for helpful discussion and feedback.</p></ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1007091.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>, <name name-style="western"><surname>Sen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Doupe</surname> <given-names>AJ</given-names></name>. <article-title>Spectral-temporal receptive fields of nonlinear auditory neurons obtained using natural sounds</article-title>. <source>Journal of Neuroscience</source>. <year>2000</year>;<volume>20</volume>(<issue>6</issue>):<fpage>2315</fpage>–<lpage>2331</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.20-06-02315.2000" xlink:type="simple">10.1523/JNEUROSCI.20-06-02315.2000</ext-link></comment> <object-id pub-id-type="pmid">10704507</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Demb</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tolhurst</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Dan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <etal>et al</etal>. <article-title>Do we know what the early visual system does?</article-title> <source>Journal of Neuroscience</source>. <year>2005</year>;<volume>25</volume>(<issue>46</issue>):<fpage>10577</fpage>–<lpage>10597</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3726-05.2005" xlink:type="simple">10.1523/JNEUROSCI.3726-05.2005</ext-link></comment> <object-id pub-id-type="pmid">16291931</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Spike-triggered neural characterization</article-title>. <source>Journal of vision</source>. <year>2006</year>;<volume>6</volume>(<issue>4</issue>):<fpage>13</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/6.4.13" xlink:type="simple">10.1167/6.4.13</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1007091.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Girosi</surname> <given-names>F</given-names></name>. <article-title>Networks for approximation and learning</article-title>. <source>Proceedings of the IEEE</source>. <year>1990</year>;<volume>78</volume>(<issue>9</issue>):<fpage>1481</fpage>–<lpage>1497</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/5.58326" xlink:type="simple">10.1109/5.58326</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1007091.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Larochelle</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Louradour</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lamblin</surname> <given-names>P</given-names></name>. <article-title>Exploring strategies for training deep neural networks</article-title>. <source>Journal of machine learning research</source>. <year>2009</year>;<volume>10</volume>(<issue>Jan</issue>):<fpage>1</fpage>–<lpage>40</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007091.ref006">
<label>6</label>
<mixed-citation publication-type="other" xlink:type="simple">He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2016. p. 770–778.</mixed-citation></ref>
<ref id="pcbi.1007091.ref007">
<label>7</label>
<mixed-citation publication-type="other" xlink:type="simple">Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:14090473. 2014.</mixed-citation></ref>
<ref id="pcbi.1007091.ref008">
<label>8</label>
<mixed-citation publication-type="other" xlink:type="simple">Amodei D, Ananthanarayanan S, Anubhai R, Bai J, Battenberg E, Case C, et al. Deep speech 2: End-to-end speech recognition in english and mandarin. In: International Conference on Machine Learning; 2016. p. 173–182.</mixed-citation></ref>
<ref id="pcbi.1007091.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Steyrl</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Scherer</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Faller</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Müller-Putz</surname> <given-names>GR</given-names></name>. <article-title>Random forests in non-invasive sensorimotor rhythm brain-computer interfaces: a practical and convenient non-linear classifier</article-title>. <source>Biomedical Engineering/Biomedizinische Technik</source>. <year>2016</year>;<volume>61</volume>(<issue>1</issue>):<fpage>77</fpage>–<lpage>86</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1515/bmt-2014-0117" xlink:type="simple">10.1515/bmt-2014-0117</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1007091.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wulsin</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Gupta</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Blanco</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Litt</surname> <given-names>B</given-names></name>. <article-title>Modeling electroencephalography waveforms with semi-supervised deep belief nets: fast classification and anomaly measurement</article-title>. <source>Journal of neural engineering</source>. <year>2011</year>;<volume>8</volume>(<issue>3</issue>):<fpage>036015</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/8/3/036015" xlink:type="simple">10.1088/1741-2560/8/3/036015</ext-link></comment> <object-id pub-id-type="pmid">21525569</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref011">
<label>11</label>
<mixed-citation publication-type="other" xlink:type="simple">Stober S, Cameron DJ, Grahn JA. Using Convolutional Neural Networks to Recognize Rhythm Stimuli from Electroencephalography Recordings. In: Advances in neural information processing systems; 2014. p. 1449–1457.</mixed-citation></ref>
<ref id="pcbi.1007091.ref012">
<label>12</label>
<mixed-citation publication-type="other" xlink:type="simple">Wand M, Schultz T. Pattern learning with deep neural networks in EMG-based speech recognition. In: Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual International Conference of the IEEE. IEEE; 2014. p. 4200–4203.</mixed-citation></ref>
<ref id="pcbi.1007091.ref013">
<label>13</label>
<mixed-citation publication-type="other" xlink:type="simple">Supratak A, Li L, Guo Y. Feature extraction with stacked autoencoders for epileptic seizure detection. In: Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual International Conference of the IEEE. IEEE; 2014. p. 4184–4187.</mixed-citation></ref>
<ref id="pcbi.1007091.ref014">
<label>14</label>
<mixed-citation publication-type="other" xlink:type="simple">Yang M, Sheth SA, Schevon CA, II GMM, Mesgarani N. Speech reconstruction from human auditory cortex with deep neural networks. In: Sixteenth Annual Conference of the International Speech Communication Association; 2015.</mixed-citation></ref>
<ref id="pcbi.1007091.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zipser</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>RA</given-names></name>. <article-title>A back-propagation programmed network that simulates response properties of a subset of posterior parietal neurons</article-title>. <source>Nature</source>. <year>1988</year>;<volume>331</volume>(<issue>6158</issue>):<fpage>679</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/331679a0" xlink:type="simple">10.1038/331679a0</ext-link></comment> <object-id pub-id-type="pmid">3344044</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yamins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Seibert</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year>;<volume>111</volume>(<issue>23</issue>):<fpage>8619</fpage>–<lpage>8624</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1403112111" xlink:type="simple">10.1073/pnas.1403112111</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1007091.ref017">
<label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">Agrawal P, Cheung B, Lescroart M, Stansbury D, Malik J, Gallant J. The Human Visual Hierarchy is Isomorphic to the Hierarchy learned by a Deep Convolutional Neural Network Trained for Object Recognition; 2015.</mixed-citation></ref>
<ref id="pcbi.1007091.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>McIntosh</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Maheswaranathan</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Nayebi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ganguli</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Baccus</surname> <given-names>S</given-names></name>. <article-title>Deep learning models of the retinal response to natural scenes</article-title>. In: <source>Advances in Neural Information Processing Systems</source>; <year>2016</year>. p. <fpage>1369</fpage>–<lpage>1377</lpage>. <object-id pub-id-type="pmid">28729779</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref019">
<label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Benjamin AS, Fernandes HL, Tomlinson T, Ramkumar P, VerSteeg C, Miller L, et al. Modern machine learning far outperforms GLMs at predicting spikes. bioRxiv. 2017; p. 111450.</mixed-citation></ref>
<ref id="pcbi.1007091.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bouchard</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Mesgarani</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>EF</given-names></name>. <article-title>Functional organization of human sensorimotor cortex for speech articulation</article-title>. <source>Nature</source>. <year>2013</year>;<volume>495</volume>(<issue>7441</issue>):<fpage>327</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature11911" xlink:type="simple">10.1038/nature11911</ext-link></comment> <object-id pub-id-type="pmid">23426266</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bouchard</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>EF</given-names></name>. <article-title>Control of spoken vowel acoustics and the influence of phonetic context in human speech sensorimotor cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2014</year>;<volume>34</volume>(<issue>38</issue>):<fpage>12662</fpage>–<lpage>12677</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1219-14.2014" xlink:type="simple">10.1523/JNEUROSCI.1219-14.2014</ext-link></comment> <object-id pub-id-type="pmid">25232105</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref022">
<label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Bouchard KE, Chang EF. Neural decoding of spoken vowels from human sensory-motor cortex with high-density electrocorticography. In: Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual International Conference of the IEEE. IEEE; 2014. p. 6782–6785.</mixed-citation></ref>
<ref id="pcbi.1007091.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mugler</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Patton</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Flint</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>ZA</given-names></name>, <name name-style="western"><surname>Schuele</surname> <given-names>SU</given-names></name>, <name name-style="western"><surname>Rosenow</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Direct classification of all American English phonemes using signals from functional speech motor cortex</article-title>. <source>Journal of neural engineering</source>. <year>2014</year>;<volume>11</volume>(<issue>3</issue>):<fpage>035015</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/11/3/035015" xlink:type="simple">10.1088/1741-2560/11/3/035015</ext-link></comment> <object-id pub-id-type="pmid">24836588</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lotte</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Brumberg</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Brunner</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Gunduz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ritaccio</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Guan</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Electrocorticographic representations of segmental features in continuous speech</article-title>. <source>Frontiers in human neuroscience</source>. <year>2015</year>;<volume>9</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2015.00097" xlink:type="simple">10.3389/fnhum.2015.00097</ext-link></comment> <object-id pub-id-type="pmid">25759647</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref025">
<label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Mugler EM, Tate MC, Livescu K, Templer JW, Goldrick MA, Slutzky MW. Differential representation of articulatory gestures and phonemes in motor, premotor, and inferior frontal cortices. bioRxiv. 2017; p. 220723.</mixed-citation></ref>
<ref id="pcbi.1007091.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Penfield</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Boldrey</surname> <given-names>E</given-names></name>. <article-title>Somatic motor and sensory representation in the cerebral cortex of man as studied by electrical stimulation</article-title>. <source>Brain: A journal of neurology</source>. <year>1937</year>;. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/brain/60.4.389" xlink:type="simple">10.1093/brain/60.4.389</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1007091.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Guenther</surname> <given-names>FH</given-names></name>, <name name-style="western"><surname>Brumberg</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Nieto-Castanon</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Tourville</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Panko</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>A wireless brain-machine interface for real-time speech synthesis</article-title>. <source>PloS one</source>. <year>2009</year>;<volume>4</volume>(<issue>12</issue>):<fpage>e8218</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0008218" xlink:type="simple">10.1371/journal.pone.0008218</ext-link></comment> <object-id pub-id-type="pmid">20011034</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Leuthardt</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Gaona</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Sharma</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Szrama</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Roland</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Freudenberg</surname> <given-names>Z</given-names></name>, <etal>et al</etal>. <article-title>Using the electrocorticographic speech network to control a brain–computer interface in humans</article-title>. <source>Journal of neural engineering</source>. <year>2011</year>;<volume>8</volume>(<issue>3</issue>):<fpage>036004</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/8/3/036004" xlink:type="simple">10.1088/1741-2560/8/3/036004</ext-link></comment> <object-id pub-id-type="pmid">21471638</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kellis</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Thomson</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>House</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Greger</surname> <given-names>B</given-names></name>. <article-title>Decoding spoken words using local field potentials recorded from the cortical surface</article-title>. <source>Journal of neural engineering</source>. <year>2010</year>;<volume>7</volume>(<issue>5</issue>):<fpage>056007</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/7/5/056007" xlink:type="simple">10.1088/1741-2560/7/5/056007</ext-link></comment> <object-id pub-id-type="pmid">20811093</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pei</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Barbour</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Leuthardt</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Schalk</surname> <given-names>G</given-names></name>. <article-title>Decoding vowels and consonants in spoken and imagined words using electrocorticographic signals in humans</article-title>. <source>Journal of neural engineering</source>. <year>2011</year>;<volume>8</volume>(<issue>4</issue>):<fpage>046028</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/8/4/046028" xlink:type="simple">10.1088/1741-2560/8/4/046028</ext-link></comment> <object-id pub-id-type="pmid">21750369</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Herff</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Heger</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>de Pesters</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Telaar</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Brunner</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schalk</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>Brain-to-text: decoding spoken phrases from phone representations in the brain</article-title>. <source>Frontiers in neuroscience</source>. <year>2015</year>;<volume>9</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnins.2015.00217" xlink:type="simple">10.3389/fnins.2015.00217</ext-link></comment> <object-id pub-id-type="pmid">26124702</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ramsey</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Salari</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Aarnoutse</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Vansteensel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bleichner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Freudenburg</surname> <given-names>Z</given-names></name>. <article-title>Decoding spoken phonemes from sensorimotor cortex with high-density ECoG grids</article-title>. <source>NeuroImage</source>. <year>2017</year>.</mixed-citation></ref>
<ref id="pcbi.1007091.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Crone</surname> <given-names>NE</given-names></name>, <name name-style="western"><surname>Miglioretti</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Gordon</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Sieracki</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Uematsu</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Functional mapping of human sensorimotor cortex with electrocorticographic spectral analysis. I. Alpha and beta event-related desynchronization</article-title>. <source>Brain: a journal of neurology</source>. <year>1998</year>;<volume>121</volume>(<issue>12</issue>):<fpage>2271</fpage>–<lpage>2299</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/brain/121.12.2271" xlink:type="simple">10.1093/brain/121.12.2271</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1007091.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Crone</surname> <given-names>NE</given-names></name>, <name name-style="western"><surname>Miglioretti</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Gordon</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lesser</surname> <given-names>RP</given-names></name>. <article-title>Functional mapping of human sensorimotor cortex with electrocorticographic spectral analysis. II. Event-related synchronization in the gamma band</article-title>. <source>Brain: a journal of neurology</source>. <year>1998</year>;<volume>121</volume>(<issue>12</issue>):<fpage>2301</fpage>–<lpage>2315</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007091.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Anastassiou</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>. <article-title>The origin of extracellular fields and currents—EEG, ECoG, LFP and spikes</article-title>. <source>Nature reviews neuroscience</source>. <year>2012</year>;<volume>13</volume>(<issue>6</issue>):<fpage>407</fpage>–<lpage>420</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3241" xlink:type="simple">10.1038/nrn3241</ext-link></comment> <object-id pub-id-type="pmid">22595786</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Michalareas</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Vezoli</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Van Pelt</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schoffelen</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Kennedy</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Fries</surname> <given-names>P</given-names></name>. <article-title>Alpha-beta and gamma rhythms subserve feedback and feedforward influences among human visual cortical areas</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>89</volume>(<issue>2</issue>):<fpage>384</fpage>–<lpage>397</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2015.12.018" xlink:type="simple">10.1016/j.neuron.2015.12.018</ext-link></comment> <object-id pub-id-type="pmid">26777277</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Richter</surname> <given-names>CG</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>WH</given-names></name>, <name name-style="western"><surname>Bosman</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Fries</surname> <given-names>P</given-names></name>. <article-title>Top-down beta enhances bottom-up gamma</article-title>. <source>Journal of Neuroscience</source>. <year>2017</year>; p. <fpage>3771</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007091.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rubino</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Robbins</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Hatsopoulos</surname> <given-names>NG</given-names></name>. <article-title>Propagating waves mediate information transfer in the motor cortex</article-title>. <source>Nature neuroscience</source>. <year>2006</year>;<volume>9</volume>(<issue>12</issue>):<fpage>1549</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1802" xlink:type="simple">10.1038/nn1802</ext-link></comment> <object-id pub-id-type="pmid">17115042</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Takahashi</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Coleman</surname> <given-names>TP</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Suminski</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Best</surname> <given-names>MD</given-names></name>, <etal>et al</etal>. <article-title>Large-scale spatiotemporal spike patterning consistent with wave propagation in motor cortex</article-title>. <source>Nature communications</source>. <year>2015</year>;<volume>6</volume>:<fpage>7169</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms8169" xlink:type="simple">10.1038/ncomms8169</ext-link></comment> <object-id pub-id-type="pmid">25994554</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Canolty</surname> <given-names>RT</given-names></name>, <name name-style="western"><surname>Ganguly</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kennerley</surname> <given-names>SW</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Koepsell</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Wallis</surname> <given-names>JD</given-names></name>, <etal>et al</etal>. <article-title>Oscillatory phase coupling coordinates anatomically dispersed functional cell assemblies</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2010</year>;<volume>107</volume>(<issue>40</issue>):<fpage>17356</fpage>–<lpage>17361</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1008306107" xlink:type="simple">10.1073/pnas.1008306107</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1007091.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pfurtscheller</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Da Silva</surname> <given-names>FL</given-names></name>. <article-title>Event-related EEG/MEG synchronization and desynchronization: basic principles</article-title>. <source>Clinical neurophysiology</source>. <year>1999</year>;<volume>110</volume>(<issue>11</issue>):<fpage>1842</fpage>–<lpage>1857</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S1388-2457(99)00141-8" xlink:type="simple">10.1016/S1388-2457(99)00141-8</ext-link></comment> <object-id pub-id-type="pmid">10576479</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Engel</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Fries</surname> <given-names>P</given-names></name>. <article-title>Beta-band oscillations—signalling the status quo?</article-title> <source>Current opinion in neurobiology</source>. <year>2010</year>;<volume>20</volume>(<issue>2</issue>):<fpage>156</fpage>–<lpage>165</lpage>. <object-id pub-id-type="pmid">20359884</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Latimer</surname> <given-names>KW</given-names></name>, <name name-style="western"><surname>Yates</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Huk</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>. <article-title>Single-trial spike trains in parietal cortex reveal discrete steps during decision-making</article-title>. <source>Science</source>. <year>2015</year>;<volume>349</volume>(<issue>6244</issue>):<fpage>184</fpage>–<lpage>187</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.aaa4056" xlink:type="simple">10.1126/science.aaa4056</ext-link></comment> <object-id pub-id-type="pmid">26160947</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pedregosa</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Varoquaux</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Gramfort</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Michel</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Thirion</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Grisel</surname> <given-names>O</given-names></name>, <etal>et al</etal>. <article-title>Scikit-learn: Machine learning in Python</article-title>. <source>Journal of machine learning research</source>. <year>2011</year>;<volume>12</volume>(<issue>Oct</issue>):<fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007091.ref045">
<label>45</label>
<mixed-citation publication-type="other" xlink:type="simple">Goodfellow IJ, Warde-Farley D, Lamblin P, Dumoulin V, Mirza M, Pascanu R, et al. Pylearn2: a machine learning research library. arXiv preprint arXiv:13084214. 2013.</mixed-citation></ref>
<ref id="pcbi.1007091.ref046">
<label>46</label>
<mixed-citation publication-type="other" xlink:type="simple">Al-Rfou R, Alain G, Almahairi A, Angermueller C, Bahdanau D, Ballas N, et al. Theano: A Python framework for fast computation of mathematical expressions. arXiv preprint. 2016.</mixed-citation></ref>
<ref id="pcbi.1007091.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bergstra</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>. <article-title>Random search for hyper-parameter optimization</article-title>. <source>Journal of Machine Learning Research</source>. <year>2012</year>;<volume>13</volume>(<issue>Feb</issue>):<fpage>281</fpage>–<lpage>305</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007091.ref048">
<label>48</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Cover</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Thomas</surname> <given-names>JA</given-names></name>. <source>Elements of information theory</source>. <publisher-name>John Wiley &amp; Sons</publisher-name>; <year>2012</year>.</mixed-citation></ref>
<ref id="pcbi.1007091.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wolpaw</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Birbaumer</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>McFarland</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Pfurtscheller</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Vaughan</surname> <given-names>TM</given-names></name>. <article-title>Brain–computer interfaces for communication and control</article-title>. <source>Clinical neurophysiology</source>. <year>2002</year>;<volume>113</volume>(<issue>6</issue>):<fpage>767</fpage>–<lpage>791</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S1388-2457(02)00057-3" xlink:type="simple">10.1016/S1388-2457(02)00057-3</ext-link></comment> <object-id pub-id-type="pmid">12048038</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref050">
<label>50</label>
<mixed-citation publication-type="other" xlink:type="simple">Warde-Farley D, Rabinovich A, Anguelov D. Self-informed neural network structure learning. arXiv preprint arXiv:14126563. 2014.</mixed-citation></ref>
<ref id="pcbi.1007091.ref051">
<label>51</label>
<mixed-citation publication-type="other" xlink:type="simple">Hinton G, Vinyals O, Dean J. Distilling the knowledge in a neural network. arXiv preprint arXiv:150302531. 2015.</mixed-citation></ref>
<ref id="pcbi.1007091.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kubanek</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ojemann</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wolpaw</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schalk</surname> <given-names>G</given-names></name>. <article-title>Decoding flexion of individual fingers using electrocorticographic signals in humans</article-title>. <source>Journal of neural engineering</source>. <year>2009</year>;<volume>6</volume>(<issue>6</issue>):<fpage>066001</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/6/6/066001" xlink:type="simple">10.1088/1741-2560/6/6/066001</ext-link></comment> <object-id pub-id-type="pmid">19794237</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Alipanahi</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Delong</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Weirauch</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Frey</surname> <given-names>BJ</given-names></name>. <article-title>Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning</article-title>. <source>Nature biotechnology</source>. <year>2015</year>;<volume>33</volume>(<issue>8</issue>):<fpage>831</fpage>–<lpage>838</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nbt.3300" xlink:type="simple">10.1038/nbt.3300</ext-link></comment> <object-id pub-id-type="pmid">26213851</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Baldi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sadowski</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Whiteson</surname> <given-names>D</given-names></name>. <article-title>Enhanced Higgs boson to <italic>τ</italic>+ <italic>τ</italic>- search with deep learning</article-title>. <source>Physical review letters</source>. <year>2015</year>;<volume>114</volume>(<issue>11</issue>):<fpage>111801</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.114.111801" xlink:type="simple">10.1103/PhysRevLett.114.111801</ext-link></comment> <object-id pub-id-type="pmid">25839260</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref055">
<label>55</label>
<mixed-citation publication-type="other" xlink:type="simple">Nagamine T, Seltzer ML, Mesgarani N. Exploring how deep neural networks form phonemic categories. In: Sixteenth Annual Conference of the International Speech Communication Association; 2015.</mixed-citation></ref>
<ref id="pcbi.1007091.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Zanos</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fetz</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Den Nijs</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ojemann</surname> <given-names>J</given-names></name>. <article-title>Decoupling the cortical power spectrum reveals real-time representation of individual finger movements in humans</article-title>. <source>Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>10</issue>):<fpage>3132</fpage>–<lpage>3137</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5506-08.2009" xlink:type="simple">10.1523/JNEUROSCI.5506-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19279250</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Manning</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Jacobs</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Fried</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Kahana</surname> <given-names>MJ</given-names></name>. <article-title>Broadband shifts in local field potential power spectra are correlated with single-neuron spiking in humans</article-title>. <source>Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>43</issue>):<fpage>13613</fpage>–<lpage>13620</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2041-09.2009" xlink:type="simple">10.1523/JNEUROSCI.2041-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19864573</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref058">
<label>58</label>
<mixed-citation publication-type="other" xlink:type="simple">Haller M, Donoghue T, Peterson E, Varma P, Sebastian P, Gao R, et al. Parameterizing neural power spectra. bioRxiv. 2018; p. 299859.</mixed-citation></ref>
<ref id="pcbi.1007091.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Leuthardt</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Schalk</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Rao</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>NR</given-names></name>, <name name-style="western"><surname>Moran</surname> <given-names>DW</given-names></name>, <etal>et al</etal>. <article-title>Spectral changes in cortical surface potentials during motor movement</article-title>. <source>Journal of Neuroscience</source>. <year>2007</year>;<volume>27</volume>(<issue>9</issue>):<fpage>2424</fpage>–<lpage>2432</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3886-06.2007" xlink:type="simple">10.1523/JNEUROSCI.3886-06.2007</ext-link></comment> <object-id pub-id-type="pmid">17329441</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Leonard</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Baud</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Sjerps</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>EF</given-names></name>. <article-title>Perceptual restoration of masked speech in human cortex</article-title>. <source>Nature communications</source>. <year>2016</year>;<volume>7</volume>:<fpage>13619</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms13619" xlink:type="simple">10.1038/ncomms13619</ext-link></comment> <object-id pub-id-type="pmid">27996973</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Reed</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Durlach</surname> <given-names>NI</given-names></name>. <article-title>Note on information transfer rates in human communication</article-title>. <source>Presence</source>. <year>1998</year>;<volume>7</volume>(<issue>5</issue>):<fpage>509</fpage>–<lpage>518</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/105474698565893" xlink:type="simple">10.1162/105474698565893</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1007091.ref062">
<label>62</label>
<mixed-citation publication-type="other" xlink:type="simple">Kingma DP, Welling M. Auto-encoding variational bayes. arXiv preprint arXiv:13126114. 2013.</mixed-citation></ref>
<ref id="pcbi.1007091.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Carmena</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Lebedev</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Crist</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Santucci</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Dimitrov</surname> <given-names>DF</given-names></name>, <etal>et al</etal>. <article-title>Learning to control a brain–machine interface for reaching and grasping by primates</article-title>. <source>PLoS biology</source>. <year>2003</year>;<volume>1</volume>(<issue>2</issue>):<fpage>e42</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.0000042" xlink:type="simple">10.1371/journal.pbio.0000042</ext-link></comment> <object-id pub-id-type="pmid">14624244</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref064">
<label>64</label>
<mixed-citation publication-type="other" xlink:type="simple">Huth AG, Griffiths TL, Theunissen FE, Gallant JL. PrAGMATiC: A probabilistic and generative model of areas tiling the cortex. arXiv preprint arXiv:150403622. 2015.</mixed-citation></ref>
<ref id="pcbi.1007091.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Crone</surname> <given-names>NE</given-names></name>, <name name-style="western"><surname>Korzeniewska</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Franaszczuk</surname> <given-names>PJ</given-names></name>. <article-title>Cortical gamma responses: searching high and low</article-title>. <source>International Journal of Psychophysiology</source>. <year>2011</year>;<volume>79</volume>(<issue>1</issue>):<fpage>9</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.ijpsycho.2010.10.013" xlink:type="simple">10.1016/j.ijpsycho.2010.10.013</ext-link></comment> <object-id pub-id-type="pmid">21081143</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ray</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Maunsell</surname> <given-names>JH</given-names></name>. <article-title>Different origins of gamma rhythm and high-gamma activity in macaque visual cortex</article-title>. <source>PLoS biology</source>. <year>2011</year>;<volume>9</volume>(<issue>4</issue>):<fpage>e1000610</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1000610" xlink:type="simple">10.1371/journal.pbio.1000610</ext-link></comment> <object-id pub-id-type="pmid">21532743</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref067">
<label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Sorensen</surname> <given-names>LB</given-names></name>, <name name-style="western"><surname>Ojemann</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Den Nijs</surname> <given-names>M</given-names></name>. <article-title>Power-law scaling in the brain surface electric potential</article-title>. <source>PLoS computational biology</source>. <year>2009</year>;<volume>5</volume>(<issue>12</issue>):<fpage>e1000609</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000609" xlink:type="simple">10.1371/journal.pcbi.1000609</ext-link></comment> <object-id pub-id-type="pmid">20019800</object-id></mixed-citation></ref>
<ref id="pcbi.1007091.ref068">
<label>68</label>
<mixed-citation publication-type="other" xlink:type="simple">Shwartz-Ziv R, Tishby N. Opening the black box of deep neural networks via information. arXiv preprint arXiv:170300810. 2017.</mixed-citation></ref>
<ref id="pcbi.1007091.ref069">
<label>69</label>
<mixed-citation publication-type="other" xlink:type="simple">Saxe AM, McClelland JL, Ganguli S. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:13126120. 2013.</mixed-citation></ref>
<ref id="pcbi.1007091.ref070">
<label>70</label>
<mixed-citation publication-type="other" xlink:type="simple">Zeiler MD, Fergus R. Visualizing and understanding convolutional networks. In: European conference on computer vision. Springer; 2014. p. 818–833.</mixed-citation></ref>
<ref id="pcbi.1007091.ref071">
<label>71</label>
<mixed-citation publication-type="other" xlink:type="simple">Li Y, Yosinski J, Clune J, Lipson H, Hopcroft J. Convergent learning: Do different neural networks learn the same representations? In: Feature Extraction: Modern Questions and Challenges; 2015. p. 196–212.</mixed-citation></ref>
<ref id="pcbi.1007091.ref072">
<label>72</label>
<mixed-citation publication-type="other" xlink:type="simple">Nguyen A, Dosovitskiy A, Yosinski J, Brox T, Clune J. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. In: Advances in Neural Information Processing Systems; 2016. p. 3387–3395.</mixed-citation></ref>
<ref id="pcbi.1007091.ref073">
<label>73</label>
<mixed-citation publication-type="other" xlink:type="simple">Achille A, Soatto S. On the emergence of invariance and disentangling in deep representations. arXiv preprint arXiv:170601350. 2017.</mixed-citation></ref></ref-list>
</back>
</article>