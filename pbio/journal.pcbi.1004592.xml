<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-00857</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004592</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Learning of Chunking Sequences in Cognition and Behavior</article-title>
<alt-title alt-title-type="running-head">Learning of Chunking Sequences in Cognition and Behavior</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Fonollosa</surname> <given-names>Jordi</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Neftci</surname> <given-names>Emre</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Rabinovich</surname> <given-names>Mikhail</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Biocircuits Institute, University of California, San Diego, La Jolla, California, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Institute for Bioengineering of Catalonia, Barcelona, Spain</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Department of Cognitive Sciences, University of California, Irvine, Irvine, California, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Sporns</surname> <given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Indiana University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: EN JF MR. Performed the experiments: EN JF. Analyzed the data: EN JF. Wrote the paper: EN JF MR.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">eneftci@uci.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>11</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>19</day>
<month>11</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>11</issue>
<elocation-id>e1004592</elocation-id>
<history>
<date date-type="received">
<day>27</day>
<month>5</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>5</day>
<month>10</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Fonollosa et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004592" xlink:type="simple"/>
<abstract>
<p>We often learn and recall long sequences in smaller segments, such as a phone number 858 534 22 30 memorized as four segments. Behavioral experiments suggest that humans and some animals employ this strategy of breaking down cognitive or behavioral sequences into chunks in a wide variety of tasks, but the dynamical principles of how this is achieved remains unknown. Here, we study the temporal dynamics of chunking for learning cognitive sequences in a chunking representation using a dynamical model of competing modes arranged to evoke hierarchical Winnerless Competition (WLC) dynamics. Sequential memory is represented as trajectories along a chain of metastable fixed points at each level of the hierarchy, and bistable Hebbian dynamics enables the learning of such trajectories in an unsupervised fashion. Using computer simulations, we demonstrate the learning of a chunking representation of sequences and their robust recall. During learning, the dynamics associates a set of modes to each information-carrying item in the sequence and encodes their relative order. During recall, hierarchical WLC guarantees the robustness of the sequence order when the sequence is not too long. The resulting patterns of activities share several features observed in behavioral experiments, such as the pauses between boundaries of chunks, their size and their duration. Failures in learning chunking sequences provide new insights into the dynamical causes of neurological disorders such as Parkinson’s disease and Schizophrenia.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Because chunking is a hallmark of the brain’s organization, efforts to understand its dynamics can provide valuable insights into the brain and its disorders. For identifying the dynamical principles of chunking learning, we hypothesize that perceptual sequences can be learned and stored as a chain of metastable fixed points in a low-dimensional dynamical system, similar to the trajectory of a ball rolling down a pinball machine. During a learning phase, the interactions in the network evolve such that the network learns a chunking representation of the sequence, as when memorizing a phone number in segments. In the example of the pinball machine, learning can be identified with the gradual placement of the pins. After learning, the pins are placed in a way that, at each run, the ball follows the same trajectory (recall of the same sequence) that encodes the perceptual sequence. Simulations show that the dynamics are endowed with the hallmarks of chunking observed in behavioral experiments, such as increased delays observed before loading new chunks.</p>
</abstract>
<funding-group>
<funding-statement>This work was funded by: NSF EFRI-1137279 (to EN), ONR MURI 14-13-1-0205 (to EN, JF, MR), ONR MURI  N00014-13-1-0678 (MR), and the Beatriu de Pinós grant BP-B-00190 (to JF). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="0"/>
<page-count count="24"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value> All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Sequence learning is a critical component of human intelligence. The ability to recognize and produce ordered sequences is a defining feature of the brain and a key component of many cognitive performances. Sequence learning and production is a hierarchical process, such as in speech organization, behavioral sequences, and thought processes. By segmenting a sequence of elements into blocks, or <italic>chunks</italic>, information becomes easier to retain and recall in the correct order [<xref ref-type="bibr" rid="pcbi.1004592.ref001">1</xref>]. Such chunking organization in memory has been investigated for more than half a century, when Bousfield formulated the idea that information-carrying items seem to be recalled in associated clusters [<xref ref-type="bibr" rid="pcbi.1004592.ref002">2</xref>], and Miller pointed out that limits in our working memory capacity for processing information necessitated the organization of items into chunks [<xref ref-type="bibr" rid="pcbi.1004592.ref003">3</xref>].</p>
<p>A chunk is often defined as a collection of elements having strong associations with each other, but weaker associations with elements within other chunks [<xref ref-type="bibr" rid="pcbi.1004592.ref004">4</xref>]. For example, complex motor movements are represented as a chain of subordinate movements, which are concatenated in a goal-specific fashion [<xref ref-type="bibr" rid="pcbi.1004592.ref005">5</xref>]. Behavioral visuo-motor sequence learning experiments suggest that action sequences are organized as chunks of information-carrying items [<xref ref-type="bibr" rid="pcbi.1004592.ref006">6</xref>–<xref ref-type="bibr" rid="pcbi.1004592.ref009">9</xref>]. Imaging and behavioral studies further suggest that chunking learning extends to language processing [<xref ref-type="bibr" rid="pcbi.1004592.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref011">11</xref>], visual perception [<xref ref-type="bibr" rid="pcbi.1004592.ref012">12</xref>], habit learning [<xref ref-type="bibr" rid="pcbi.1004592.ref013">13</xref>], and motor skills [<xref ref-type="bibr" rid="pcbi.1004592.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1004592.ref017">17</xref>].</p>
<p>Several studies provided models for chunking learning that explain some behavioral observations. For example, a model of chunking learning explains why skill improves with practice according to a power law [<xref ref-type="bibr" rid="pcbi.1004592.ref018">18</xref>]. Another example is that of competitive chunking [<xref ref-type="bibr" rid="pcbi.1004592.ref019">19</xref>], whereby a bottom-up perception process strengthens the chunks. Such computational models are informative as high-level descriptions of chunking learning, but do not incorporate temporal dynamics in a natural way. As a result, such models cannot provide principled insight into the temporal aspects of behavior. On the other hand, a dynamical systems approach naturally allows the study of temporal interactions [<xref ref-type="bibr" rid="pcbi.1004592.ref020">20</xref>], and can provide tight connections with biophysical models of neurons.</p>
<p>Experimental findings in imaging and behavioral studies provide the structure and dynamics of chunking in the brain at the mesoscopic level, allowing one to build theoretical models for the description of chunking in cognition and behavior [<xref ref-type="bibr" rid="pcbi.1004592.ref021">21</xref>]. These models are non-linear dynamical systems that describe the interaction of core components—or cognitive modes—participating in a specific mental function [<xref ref-type="bibr" rid="pcbi.1004592.ref022">22</xref>]. Here, we describe a dynamical model of the cognitive mechanisms for learning chunking representations of sequences. The dynamical system is based on the sequential competition between different information-carrying items that are represented as metastable states, such as saddle nodes. In the neighborhood of a saddle point, elementary volumes in the phase space are compressed along stable separatrices and stretched along an unstable separatrix. Saddle nodes can be chained such that the unstable separatrix of one node corresponds to the stable separatrix of the next node along the chain. If the compressing at the saddle node is larger than the stretching and all nodes in the chain are dissipative, the trajectories stably follow a channel [<xref ref-type="bibr" rid="pcbi.1004592.ref022">22</xref>]. Such channels are known as Stable Heteroclinic Channels (SHCs), and are argued to form the basis of sequential working memory through Winnerless Competition (WLC) dynamics [<xref ref-type="bibr" rid="pcbi.1004592.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref024">24</xref>].</p>
<p>The WLC principle depicts itinerant dynamics whereby a “winning” state transiently dominates the network in a sequential fashion. Its function is to transform inputs (<italic>e.g.</italic> a task input) into spatiotemporal outputs based on the intrinsic switching dynamics of an ensemble of modes [<xref ref-type="bibr" rid="pcbi.1004592.ref023">23</xref>]. As a concrete model of WLC, we employ a generalization of the Lotka-Volterra evolutionary prey-predator model [<xref ref-type="bibr" rid="pcbi.1004592.ref025">25</xref>], known as the Generalized Lotka-Volterra (GLV) model. GLVs represent a canonical non-linear model of non-equilibrium dissipative systems [<xref ref-type="bibr" rid="pcbi.1004592.ref026">26</xref>], and is widely used to study local bifurcations of SHCs. Many other models can be written in the form of GLV after some recasting [<xref ref-type="bibr" rid="pcbi.1004592.ref027">27</xref>], and its dynamical properties are consistent with a wide range of neuron models [<xref ref-type="bibr" rid="pcbi.1004592.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1004592.ref030">30</xref>].</p>
<p>Extending this idea, a dynamical image of <italic>chunking</italic> processing is a two-layer model describing a heteroclinic chain of heteroclinic chains. Under these dynamics, one metastable state in a “chunking layer” is associated to a heteroclinic sequence in another “elementary layer” [<xref ref-type="bibr" rid="pcbi.1004592.ref031">31</xref>]. In such representation, the chunks—or groups of elementary items—are learned in the “chunking layer”, whereas the elementary items are learned in the “elementary layer”. For example in the phone number 8585342230 broken down in four chunks, 858-534-22-30, each digit in a chunk is represented by a separate elementary unit, while every group of digits is represented by a chunking unit. This way, the chunking representation is a heteroclinic chain (in the chunking layer) of heteroclinic chains (in the elementary layer). Earlier work described a similar model for the recognition of sequences of sequences [<xref ref-type="bibr" rid="pcbi.1004592.ref032">32</xref>].</p>
<p>Our previous work demonstrated a model of sequential spatial memory learning based on the WLC principle [<xref ref-type="bibr" rid="pcbi.1004592.ref033">33</xref>]. The dynamics was endowed with learning dynamics which led to the self-organization of WLC. To learn chunking sequences, we extend our previous model with a hierarchical neural network [<xref ref-type="bibr" rid="pcbi.1004592.ref021">21</xref>], and augment it with bistable Hebbian plasticity dynamics [<xref ref-type="bibr" rid="pcbi.1004592.ref034">34</xref>] for unsupervised learning. Unsupervised here refers to the fact that learning is self-organized: During training, no external signal other than the perceptual information enters the dynamical system.</p>
<p>The competitive dynamics in the cognitive network and the plasticity rules interact to learn a chunking representation of the sequence. Within each layer, the couplings in the system are initialized to a state where the network performs Winner-Take-All (WTA): the node receiving the strongest input activates and all other node are silenced. When the couplings within a layer become sufficiently asymmetric, the dynamics within that layer switch from a WTA behavior [<xref ref-type="bibr" rid="pcbi.1004592.ref035">35</xref>] to a WLC behavior. At each layer the system learns chunks of information provided by the layer below it and stores syntactical information by modifying the couplings according to the directions indicated by the perceived items. After training, the system can reproduce the entire sequence by transitioning the activity of its corresponding modes in the same order.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Network model for sequence learning with chunks</title>
<p>Our dynamical model of chunking learning is composed of Perceptual Modes (PMs), Elementary Modes (EMs) and Chunking Modes (CMs). These are organized in a two-layer network plus a perceptual input layer, as shown in <xref ref-type="fig" rid="pcbi.1004592.g001">Fig 1</xref>. The activity of the PMs is dictated by a pre-determined sequence of patterns, presented multiple times as a repeated loop. The PM project to <italic>N</italic><sub><italic>X</italic></sub> EMs, according to a projection weight matrix <italic>P</italic>. The <italic>N</italic><sub><italic>Y</italic></sub> CMs receive excitatory input from the EMs according to a weight matrix <italic>Q</italic> and inhibit the EMs back through a weight matrix <italic>R</italic>. Here, we define inhibitory as couplings that result in a negative contribution to the node activity. Within the elementary and the chunking layer, the nodes have all-to-all inhibitory couplings, the weights of which are stored in competition matrices <italic>V</italic> and <italic>W</italic>, respectively.</p>
<fig id="pcbi.1004592.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004592.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Two-layer network for learning chunking dynamics.</title>
<p>In this example, the input sequence (<monospace>a</monospace>, <monospace>b</monospace>, <monospace>c</monospace>, <monospace>d</monospace>, <monospace>e</monospace>) is presented repeatedly. Initially, all the synaptic connections within a matrix are similar with small random variations. Through learning distinct elementary modes associate to each of the five patterns through weights of the projection matrix <italic>P</italic><sub><italic>ki</italic></sub>. In the elementary layer, the weights <italic>V</italic><sub><italic>ii</italic>′</sub> in the directions <monospace>a</monospace> to <monospace>b</monospace>, <monospace>b</monospace> to <monospace>c</monospace>, and <monospace>d</monospace> to <monospace>e</monospace> are weakened (arrow thickness denotes coupling strength), while the weights in the opposite direction are strengthened. The <italic>W</italic><sub><italic>jj</italic>′</sub> follow a similar learning rule to three chunks: <monospace>ab</monospace>, <monospace>c</monospace> and <monospace>de</monospace>. Chunking, <italic>i.e.</italic> the information specifying the association between CM and EM, is learned in the coupling matrices <italic>Q</italic><sub><italic>ij</italic></sub> and <italic>R</italic><sub><italic>ji</italic></sub>. The input in the perceptual layer is represented as non-overlapping binary patterns. For example, element <monospace>a</monospace> is the binary pattern <bold>s</bold><sup><italic>a</italic></sup> = [11000100], input <monospace>b</monospace> is the binary pattern <bold>s</bold><sup><italic>b</italic></sup> = [00100010], <italic>etc</italic>. Black circles represent inhibitory couplings, while arrowheads represent excitatory couplings. The number of elementary modes should be larger or equal to the number of patterns in a sequence. Note that there must be at least three units in each layer for a stable heteroclinic cycle to exist. It is not necessary that <italic>N</italic><sub><italic>y</italic></sub> &lt; <italic>N</italic><sub><italic>x</italic></sub>, and any value such that <italic>N</italic><sub><italic>y</italic></sub> &gt; 3, <italic>N</italic><sub><italic>x</italic></sub> &gt; 3 can be used. <italic>i</italic> = 1, …, <italic>N</italic><sub><italic>X</italic></sub>; <italic>j</italic> = 1, …, <italic>N</italic><sub><italic>Y</italic></sub>; <italic>k</italic> = 1…, <italic>M</italic>; <italic>N</italic><sub><italic>X</italic></sub> ≥ <italic>M</italic> &gt; 3.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004592.g001"/>
</fig>
<p>The two-layer chunking dynamics is a GLV system of the form:
<disp-formula id="pcbi.1004592.e001"><alternatives><graphic id="pcbi.1004592.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e001"/><mml:math id="M1" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mfrac><mml:mi mathvariant="normal">d</mml:mi> <mml:mrow><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:munderover> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>s</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2.em"/><mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>X</mml:mi></mml:msub></mml:munderover> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>x</mml:mi> <mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>Y</mml:mi></mml:msub></mml:munderover> <mml:msub><mml:mi>R</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:msub><mml:mi>η</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mfrac><mml:mi mathvariant="normal">d</mml:mi> <mml:mrow><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:msub><mml:mi>ξ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mi>z</mml:mi></mml:msub> <mml:mfrac><mml:mi mathvariant="normal">d</mml:mi> <mml:mrow><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mi>z</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>X</mml:mi></mml:msub></mml:munderover> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2.em"/><mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:msup><mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>Y</mml:mi></mml:msub></mml:munderover> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:msup><mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>y</mml:mi> <mml:msup><mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mi>z</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where state variables <italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>j</italic></sub> represent compositions of brain activities such as population firing rates [<xref ref-type="bibr" rid="pcbi.1004592.ref036">36</xref>], <italic>b</italic><sub><italic>x</italic></sub>, <italic>b</italic><sub><italic>y</italic></sub> are the respective constant growth rates and <italic>η</italic><sub><italic>i</italic></sub>(<italic>t</italic>), <italic>ξ</italic><sub><italic>j</italic></sub>(<italic>t</italic>) are random (Wiener) processes with amplitudes <italic>σ</italic><sub><italic>x</italic></sub> and <italic>σ</italic><sub><italic>y</italic></sub> respectively. Perceptual modes <italic>s</italic><sub><italic>k</italic></sub> (<italic>e.g.</italic> visual or auditory cues) stimulate the elementary modes <italic>x</italic><sub><italic>i</italic></sub>, which in turn drive the chunking modes <italic>y</italic><sub><italic>j</italic></sub> through variables <italic>z</italic><sub><italic>j</italic></sub>. Variables <italic>z</italic><sub><italic>j</italic></sub> convey the regulation between different brain domains or cognitive modes [<xref ref-type="bibr" rid="pcbi.1004592.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref037">37</xref>]. In our chunking model, we have used the simplest description that reminds the first order kinetic of synapses in spiking neuronal networks [<xref ref-type="bibr" rid="pcbi.1004592.ref038">38</xref>]. The <italic>τ</italic><sub><italic>z</italic></sub> is the characteristic time scale of <italic>z</italic><sub><italic>j</italic></sub> that determines the temporal distance between different informational units (<italic>i.e.</italic> those that would be part of different chunks) by delaying the competition between different CMs [<xref ref-type="bibr" rid="pcbi.1004592.ref039">39</xref>]. Finally, <italic>b</italic><sub><italic>z</italic></sub>(<italic>t</italic>) is a time-varying bias used to dynamically modulate chunking.</p>
<p>We construct a dynamical learning model that concatenates sequence elements within one layer, and segments longer sequence portions in multiple groups (chunks). Such two interacting processes are believed to be at the heart of chunking learning in the brain [<xref ref-type="bibr" rid="pcbi.1004592.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1004592.ref009">9</xref>].</p>
<p>The key components of the learning model can be separated in two parts: 1) An asymmetric, bistable Hebbian learning rule within the WLC network learns the sequence (order) of the activity of the subordinate layer, by potentiating the weights corresponding to the transitions occurring in the elementary layer. The effect of this operation is to “concatenate” informational items, such that, during recall the same order is reproduced in a robust fashion. Hebbian learning within the WLC layer has been previously demonstrated in [<xref ref-type="bibr" rid="pcbi.1004592.ref033">33</xref>], but the proposed learning rule had a single fixed point. By selecting the two fixed points of the bistable rule according to the bifurcation of the SHC (one above the bifurcation point, one below), bistability renders the learning much more robust and prevents the formation of spurious channels. 2) The connections between two consecutive layers are learned through a symmetric, bistable Hebbian rule. This rule causes a superordinate layer to associate one (or more) modes to a group of modes in a subordinate layer. The WLC dynamics in a superordinate layer causes the network to transition its active mode, causing it to associate one mode to a finite number of modes of a subordinate layer. The association to a finite number of modes guarantees the chunking process in the learning. The number of modes within one chunk depends on the learning dynamics and the WLC dynamics in each layer. In particular we show that the size of the chunk is further bounded by the ratios of the potentiation <italic>vs.</italic> depotentiation magnitudes. This effect is further explained and quantified in section <xref ref-type="sec" rid="sec010">Learning dynamics determine chunk size</xref>.</p>
<p>For these two learning rules, we used the bistable rule demonstrated in [<xref ref-type="bibr" rid="pcbi.1004592.ref034">34</xref>]. This rule has been demonstrated to reproduce many of the learning curves observed in experiments, and its dynamics are well understood. Similarly to [<xref ref-type="bibr" rid="pcbi.1004592.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref032">32</xref>], we can construct a hierarchy for chunking learning by setting the time constant of a superordinate layer larger than the time constant of the subordinate layer.</p>
<p>In addition to the learning rules above, the elementary layer learns to associate one mode to each element in the sequence through competitive learning [<xref ref-type="bibr" rid="pcbi.1004592.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref041">41</xref>]. Such learning has been extensively documented and shown to perform the Expectation-Maximization algorithm [<xref ref-type="bibr" rid="pcbi.1004592.ref041">41</xref>], and is thus robust to the noise in sensory modes.</p>
<p>
<xref ref-type="fig" rid="pcbi.1004592.g001">Fig 1</xref> illustrates chunking learning before and after training. In this example, a sequence composed of five patterns symbolized as <monospace>a, b, c, d</monospace>, and <monospace>e</monospace>, is presented multiple times during the learning phase. Distinct modes associate to each of the five patterns through weights of the projection matrix <italic>P</italic><sub><italic>ki</italic></sub>. For example, in <xref ref-type="fig" rid="pcbi.1004592.g001">Fig 1</xref> the weights in the directions <monospace>a</monospace> to <monospace>b</monospace>, <monospace>b</monospace> to <monospace>c</monospace>, and <monospace>d</monospace> to <monospace>e</monospace> are weakened (arrow thickness denotes coupling strength), while the weights in the opposite direction are strengthened. The same learning dynamics apply to the inhibitory couplings between the chunking modes. In this illustration, three chunks are learned: <monospace>ab</monospace>, <monospace>c</monospace> and <monospace>de</monospace>.</p>
<p>
<xref ref-type="fig" rid="pcbi.1004592.g002">Fig 2</xref> (right) shows a projection of the phase portrait of the chunking dynamics obtained after learning. Before learning, the network reaches stable fixed points, which appear as red “spikes” in <xref ref-type="fig" rid="pcbi.1004592.g002">Fig 2</xref> (left). This example illustrates how learning endows the network with a closed chunking sequence (black) that consists of several heteroclinic cycles that represent the chunks, which appear as red triangles in <xref ref-type="fig" rid="pcbi.1004592.g002">Fig 2</xref> (right). In general, the number of elementary items in each chunk are different and the chunking sequence can be open.</p>
<fig id="pcbi.1004592.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004592.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Projection of the phase portrait of the two-layer chunking hierarchical dynamics in the space of three auxiliary variables.</title>
<p>This example illustrates the dynamics of a system <italic>N</italic><sub><italic>X</italic></sub> = 24, <italic>N</italic><sub><italic>Y</italic></sub> = 3 before (left) and after learning (right) a sequence consisting of 24 patterns of <italic>M</italic> = 144 pixels. For visualization purposes, the variable space was projected according to <inline-formula id="pcbi.1004592.e002"><alternatives><graphic id="pcbi.1004592.e002g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e002"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>J</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn> <mml:mo stretchy="false">(</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mn>1</mml:mn> <mml:mi>i</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mn>2</mml:mn> <mml:mi>i</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mn>3</mml:mn> <mml:mi>i</mml:mi></mml:msubsup> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where superscript refers to the associated chunk. The plot is colored red when either of the chunks are active (<italic>y</italic><sub><italic>i</italic></sub> &gt; .9, ∀<italic>i</italic>). The traces were obtained from 12 runs starting from random initial conditions in the vicinity of the origin of the transformed space. Before learning, the network reaches stable fixed points. After learning, the network results in a closed chunking sequence (black) that consists of several heteroclinic cycles that represent the chunks (red). Each of the three chunks consist of EM, as the system visits the eight states in each chunk. Note however that the projection used here effectively reduces these to 9 (three states per chunk) for visualization purposes.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004592.g002"/>
</fig>
<p>In the three following paragraphs, we detail the learning dynamics between the sensory layer, the elementary layer, and the chunking layer.</p>
<sec id="sec004">
<title>Association of elementary modes with sensory modes</title>
<p>Initially, the connections between neurons are all-to-all with random variations in their weights. The couplings within each layer are symmetric and sufficiently strong such that the network behaves as a WTA [<xref ref-type="bibr" rid="pcbi.1004592.ref042">42</xref>]. The learning in the elementary layer associates one EM with each input pattern presented in the perceptual layer, according to a correlation-based rule with synaptic scaling [<xref ref-type="bibr" rid="pcbi.1004592.ref040">40</xref>]:
<disp-formula id="pcbi.1004592.e003"><alternatives><graphic id="pcbi.1004592.e003g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e003"/><mml:math id="M3" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mi>P</mml:mi></mml:msub> <mml:mfrac><mml:mi mathvariant="normal">d</mml:mi> <mml:mrow><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mi>t</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>s</italic><sub><italic>k</italic></sub> are the activities of the PMs, <italic>x</italic><sub><italic>i</italic></sub> are the activities of the EMs. When <italic>s</italic><sub><italic>k</italic></sub> is stronger than the current weight, <italic>P</italic><sub><italic>ki</italic></sub> is increased at a rate proportional to the activity of the elementary node <italic>x</italic><sub><italic>i</italic></sub>. Here, the negative term acts as a synaptic scaling term which prevents runaway potentiation in the weights [<xref ref-type="bibr" rid="pcbi.1004592.ref040">40</xref>]. When the inputs <italic>s</italic><sub><italic>k</italic></sub> are normalized, for example by feed-forward inhibition, the sum of the projection weights tends to a fixed value that is independent of the pattern [<xref ref-type="bibr" rid="pcbi.1004592.ref041">41</xref>].</p>
</sec>
<sec id="sec005">
<title>Concatenation of sequences of elementary modes</title>
<p>The learning dynamics modify the weights <italic>V</italic><sub><italic>ii</italic>′</sub> such that the order in which the EMs activate during recall is consistent with the order in the presented sequence. At each input transition, the inhibitory connections adapt such that the correct order of the presented patterns is learned in the network of elementary items. The learning rule implements a bistable Hebb rule [<xref ref-type="bibr" rid="pcbi.1004592.ref034">34</xref>]:
<disp-formula id="pcbi.1004592.e004"><alternatives><graphic id="pcbi.1004592.e004g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e004"/><mml:math id="M4" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi> <mml:mrow><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> </mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mo>∝</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>V</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mo>+</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mo>-</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"/> <mml:mtd columnalign="left"><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mo>+</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mrow><mml:mi>L</mml:mi> <mml:mi>T</mml:mi> <mml:mi>P</mml:mi></mml:mrow> <mml:mi>V</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"/> <mml:mtd columnalign="left"><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mo>-</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mrow><mml:mi>L</mml:mi> <mml:mi>T</mml:mi> <mml:mi>D</mml:mi></mml:mrow> <mml:mi>V</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mspace width="1.em"/><mml:mo>∀</mml:mo> <mml:mi>i</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>j</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where <italic>V</italic><sub><italic>ij</italic></sub> is the weight of the coupling between EMs from <italic>i</italic> to <italic>j</italic>. The first term endows the weight dynamics with two stable states, <italic>V</italic><sup>+</sup> and <italic>V</italic><sup>−</sup> at rest, and one unstable state <italic>V</italic>* such that 0 &lt; <italic>V</italic><sup>−</sup> &lt; <italic>V</italic>* &lt; <italic>V</italic><sup>+</sup>. The second and third terms implement the weight potentiation and depotentiation according to an asymmetric learning window (see <xref ref-type="sec" rid="sec017">Methods</xref>). The factors <italic>V</italic><sup>+</sup>−<italic>V</italic><sub><italic>ij</italic></sub> and <italic>V</italic><sup>−</sup>−<italic>V</italic><sub><italic>ij</italic></sub> ensure that, at rest, the weights remain in the range (<italic>V</italic><sup>−</sup>, <italic>V</italic><sup>+</sup>). When coupled with the network dynamics, an asymmetric learning window allows Long-Term Potentiation (LTP) and Long-Term Depression (LTD) to occur only when the activity transitions from one unit to another. As a result, the connection along the direction of the transition undergoes depression, while the connection in the opposite direction undergoes potentiation. The learning dynamics described above introduces asymmetry in the couplings to store the presented patterns and their order. The introduced asymmetry causes a bifurcation, changing the dynamics of the system to a WLC configuration [<xref ref-type="bibr" rid="pcbi.1004592.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref044">44</xref>]. Under these dynamics, once the learning process successfully induced a WLC configuration, the state of the system moves along a trajectory composed of the saddle nodes of an underlying SHC (see <xref ref-type="sec" rid="sec017">Methods</xref>).</p>
</sec>
<sec id="sec006">
<title>Segmentation of sequences of elementary modes into chunking modes</title>
<p>The information specifying the chunk, <italic>i.e.</italic> which EM belongs to which CM is stored in the coupling matrices <italic>Q</italic><sub><italic>ij</italic></sub> and <italic>R</italic><sub><italic>ji</italic></sub>. Learning dynamics at the chunking layer associates CMs to groups of consecutively active EMs. The rule governing the weight updates <italic>Q</italic><sub><italic>ij</italic></sub> is similar to <xref ref-type="disp-formula" rid="pcbi.1004592.e003">Eq (2)</xref>, but with soft boundaries:
<disp-formula id="pcbi.1004592.e005"><alternatives><graphic id="pcbi.1004592.e005g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e005"/><mml:math id="M5" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mfrac><mml:mi mathvariant="normal">d</mml:mi> <mml:mrow><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>∝</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi> <mml:mi>p</mml:mi> <mml:mi>Q</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>+</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>p</mml:mi> <mml:mi>Q</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mo>+</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi> <mml:mi>d</mml:mi> <mml:mi>Q</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>-</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>d</mml:mi> <mml:mi>Q</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mo>-</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi> <mml:mi>H</mml:mi></mml:msub> <mml:mo movablelimits="true" form="prefix">max</mml:mo> <mml:mo>(</mml:mo><mml:munder><mml:mo>∑</mml:mo> <mml:msup><mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:munder> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:msup><mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>H</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>f</italic><sub><italic>Q</italic></sub> is similar to the first term of <xref ref-type="disp-formula" rid="pcbi.1004592.e004">Eq (3)</xref>, Θ is a step (Heaviside) function and <italic>γ</italic><sub><italic>p</italic></sub> (<italic>γ</italic><sub><italic>d</italic></sub>) represents the rate of weight potentiation (depotentiation). This rule dictates potentiation when both elementary and chunking modes <italic>x</italic><sub><italic>i</italic></sub> and <italic>y</italic><sub><italic>j</italic></sub> are active, and depotentiation when only the CM is active. As a result, the couplings between the pair <italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>j</italic></sub> are strengthened, while all the other couplings targeting <italic>y</italic><sub><italic>j</italic></sub> are weakened. When the number of CMs is large, the elementary modes tend to form couplings with multiple chunking modes. This causes the CM to learn chunks consisting of only one EM. To prevent this, <xref ref-type="disp-formula" rid="pcbi.1004592.e005">Eq (4)</xref> includes heterosynaptic competition (last term), which imposes a limit <italic>m</italic><sub><italic>H</italic></sub> on the total efferent (outgoing) weights from each EM [<xref ref-type="bibr" rid="pcbi.1004592.ref045">45</xref>].</p>
<p>The dynamics for <italic>R</italic><sub><italic>ji</italic></sub> are of the form of <xref ref-type="disp-formula" rid="pcbi.1004592.e005">Eq (4)</xref>, but with parameters such that depression occurs when both elementary and chunking modes are active, and potentiation occurs when a CM is active. Hence, the connections from a CM to a EM that does not belong to the chunk become strongly inhibitory.</p>
<p>Finally, transitions between CMs are stored in the weights of the competition matrix <italic>W</italic><sub><italic>jj</italic>′</sub>, and follow the same dynamics as <xref ref-type="disp-formula" rid="pcbi.1004592.e004">Eq (3)</xref>.</p>
</sec>
</sec>
<sec id="sec007">
<title>Sequence learning and recall</title>
<p>We examined the ability to learn and recall sequence of patterns of a network with the architecture described above with 3 CMs, 24 EMs and 144 PMs, as well as its ability to perform chunking. The sensory input consisted of 24 different patterns that were presented sequentially. The patterns were composed of 144 pixels that were binary for presentation simplicity. Each input pattern was composed of 6 high-intensity pixels and 138 low-intensity pixels. The high/low pixels for each pattern were selected such that there was no overlap between inputs, meaning that the position of the high-intensity pixels were different than those of the low-intensity pixels. For simplicity, we chose a stimulus that consisted of 24, non-overlapping horizontal bars. A previous analysis of the learning rule of <italic>P</italic><sub><italic>ki</italic></sub> showed that the shape of the patterns can be arbitrary, but the overlap and the relative sizes of the patterns increases the difficulty of the learning task [<xref ref-type="bibr" rid="pcbi.1004592.ref041">41</xref>].</p>
<p>
<xref ref-type="fig" rid="pcbi.1004592.g003">Fig 3</xref> shows the input patterns and the activity of the EMs and CMs during learning and sequence recall. For visualization purposes we present the activity of the PMs grouped according to their activation time.</p>
<fig id="pcbi.1004592.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004592.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Input and network activities during learning and recall.</title>
<p><italic>s</italic><sub><italic>k</italic></sub>, <italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>j</italic></sub>, <italic>z</italic><sub><italic>k</italic></sub> during learning (after 5 presentations) (a) and during sequence recall (after 120 presentations) (b). Within each layer, different colors represent different modes (variables). The sensory input (presented only during learning) consisted of 24 different patterns presented sequentially. The patterns were composed of 144 binary (represented in black and white) pixels. During learning, the input drives the system dynamics. During recall, the elementary modes and the chunking modes activate in the same order as in learning. Each CM represents about 8 consecutively active elementary modes. The onset of each chunk is delayed and caused by the inhibition from the chunking layer. It is consistent with pauses before loading chunks observed in behavioral studies (highlighted in dashed line). (c) Duration that each EM remains active, with the same color codings as in (b). Three modes associated to the transitions between chunks remain active for a longer time than the others. Such pauses can be identified with pauses observed in behavioral experiments involving chunking [<xref ref-type="bibr" rid="pcbi.1004592.ref017">17</xref>].</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004592.g003"/>
</fig>
<p>While chunks can be formed of informational items that have some clear association with each other, chunking can also occur <italic>spontaneously</italic>, <italic>i.e.</italic> in the absence of clear structure in the stimuli [<xref ref-type="bibr" rid="pcbi.1004592.ref007">7</xref>]. In this section, we show chunking in the case of spontaneous chunking.</p>
<p>During the training phase, the sequence was repeatedly presented in a closed loop. After an initial transient in which EMs compete against each other, a given input pattern activates the same EM consistently (<xref ref-type="fig" rid="pcbi.1004592.g003">Fig 3</xref>, top). Similarly, the CMs always activate with the same subset of about 8 EMs. The resulting associations between PMs and EMs, and EMs and CMs are determined by the random variations present at the beginning of the learning. Therefore, each simulation run produced different association maps, similarly to the subject-specific chunking patterns during in behavioral experiments in the human [<xref ref-type="bibr" rid="pcbi.1004592.ref008">8</xref>].</p>
<p>After learning, the system is able to reproduce the sequence: EMs and CMs are driven with constant growth terms <italic>b</italic><sub><italic>x</italic></sub> and <italic>b</italic><sub><italic>y</italic></sub> to reproduce the activity in a periodic and continuous cycle (<xref ref-type="fig" rid="pcbi.1004592.g003">Fig 3</xref>, bottom). The order of the sequences were often reproduced perfectly, but the timing depends on the dynamics of the model. Namely, we observe the appearance of pauses in the EMs between chunks reminiscent of those observed in behavioral studies [<xref ref-type="bibr" rid="pcbi.1004592.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref008">8</xref>]. The weights of the competition matrices, <italic>V</italic> and <italic>W</italic>, transition from a WTA configuration at the beginning of the learning to a WLC dynamics after learning (see <xref ref-type="fig" rid="pcbi.1004592.g004">Fig 4</xref>). Initially, the couplings are all-to-all inhibitory, leading to WTA. After learning, <italic>V</italic> and <italic>W</italic> become asymmetric, leading to WLC in both layers. The arrows in <xref ref-type="fig" rid="pcbi.1004592.g004">Fig 4</xref> illustrate the succession of the state transitions in the resulting WLC. The matrices <italic>R</italic> and <italic>Q</italic> evolve to store the chunk association map. <xref ref-type="fig" rid="pcbi.1004592.g004">Fig 4</xref> (Bottom) shows that weights in the matrices <italic>Q</italic> and <italic>R</italic> form three groups with similar weights which correspond to the chunks. The patterns presented to the system are stored in the synaptic weights of the projection matrix <italic>P</italic>. Successive presentations of the input pattern modify <italic>P</italic> such that the presented patterns are stored (see <xref ref-type="fig" rid="pcbi.1004592.g005">Fig 5</xref>).</p>
<fig id="pcbi.1004592.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004592.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Synaptic weights before and after learning.</title>
<p>(a, b) Initially (<italic>t</italic><sub><italic>ini</italic></sub>), the recurrent weight matrices implement all-to-all symmetric inhibition, leading to WTA. After learning <italic>t</italic><sub><italic>fin</italic></sub> the matrices acquire an asymmetric component, leading to WLC. Superimposed white arrows in (b) indicate the resulting order of the recalled states. (c, d) The weights in the matrices <italic>Q</italic><sub><italic>ij</italic></sub> and <italic>R</italic><sub><italic>ji</italic></sub> learn which EM belongs to which chunk. The last three columns correspond to the elements that activate during chunk transitions.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004592.g004"/>
</fig>
<fig id="pcbi.1004592.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004592.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Input weights <italic>P</italic><sub><italic>ki</italic></sub> at the elementary modes.</title>
<p>(left) before and (right) after training. At the beginning, <italic>t</italic><sub><italic>ini</italic></sub>, the weights are random. The learning associates each of the 24 patterns to one EM.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004592.g005"/>
</fig>
</sec>
<sec id="sec008">
<title>The Dynamics of chunking learning</title>
<p>The results above used a small chunking layer (<italic>N</italic><sub><italic>y</italic></sub> = 3) in order to illustrate the model. However, the dynamics of chunking during learning are much more interesting for a large chunking layer, since the number of possible state trajectories grows factorially with the size of the network [<xref ref-type="bibr" rid="pcbi.1004592.ref023">23</xref>]. For this reason, in the results below, we test the model for <italic>N</italic><sub><italic>y</italic></sub> = 30 and <italic>N</italic><sub><italic>x</italic></sub> = 30.</p>
<p>The training of the model consisted of multiple epochs. Each epoch consisted of a full sequence presentation phase, immediately followed by a recall phase. After the sequence had ended, the recall phase was initiated by cueing the network with the first element of the sequence and observing the ensuing sequence of patterns in the elementary layer. During the recall phase, the parameters of the network were kept fixed (no learning).</p>
<p>We quantified recall by computing the normalized Levenshtein distance between the presented sequence and the reproduced one (see <xref ref-type="sec" rid="sec017">Methods</xref>—Characterizing Sequence Recall). Using the Levenshtein distance, we observe that overall 95% of the elements in the sequence were reproduced.</p>
<p>The progress of chunking learning is monitored by inspecting the magnitude of the chunking and the presence of sequential activity in the chunking layer during recall. The magnitude of the chunking is monitored by computing the <italic>chunking rate</italic> during learning, defined as the number of transitions taking place in the chunking layer during the presentation of each pattern in the sequence. A chunking rate equal to 1 signifies that a different CM was active for each pattern in the sequence (no chunking), while a chunking rate significantly smaller than one during training implies that chunks were formed. Note that a measure based on sequence recall only is not sufficient to characterize chunking since accurate recall is possible without the chunking layer.</p>
<p>To further assess the robustness of the chunking in the presence of noise in the sensory layer, a fixed noise drawn from a rectified Gaussian distribution was independently added to each pixel at each presentation of a sequence element (see also section 3 of <xref ref-type="supplementary-material" rid="pcbi.1004592.s001">S1 Text</xref>). Sequence recall accuracies (measured using the Levenshtein distance) and the chunking rates degraded gracefully as the noise magnitude was increased.</p>
<p>We observe that the boundaries of the chunks can change from trial to trial during training, and that chunks can undergo substantial reconfigurations throughout the learning, including the creation of new chunking modes. The dynamical nature of chunking was already observed in behavioral experiments, where chunk boundaries could vary substantially even after a large number of trials [<xref ref-type="bibr" rid="pcbi.1004592.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref046">46</xref>].</p>
<p>[<xref ref-type="bibr" rid="pcbi.1004592.ref046">46</xref>] use a Bayesian algorithm combining reaction time and error rates to reveal the chunking structure in humans performing a discrete sequence production. Interestingly, the chunking structure also evolves slowly over the course of the trials. A visual inspection of our model results suggests that this slow evolution might be caused by the enrollment of new chunking modes and the disenrollment of existing ones (see <xref ref-type="fig" rid="pcbi.1004592.g006">Fig 6</xref>, right panel).</p>
<fig id="pcbi.1004592.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004592.g006</object-id>
<label>Fig 6</label>
<caption>
<title>The dynamics of chunking.</title>
<p>The model is run 60 times, for 120 trials (<italic>N</italic><sub><italic>y</italic></sub> = 30) for different levels of noise. Each trial consisted of the presentation of one sequence, followed by a recall phase. (Top-Left) Sequence recall accuracy <italic>D</italic> averaged over all the runs. The sequence was determined by the identity of the most active mode in the elementary layer.<italic>D</italic> was computed using the Levenshtein distance (equal to the number of additions and subtractions between two sequences). In the noiseless and low noise cases, the distance between the presented sequence and the reproduced sequence reached about.05 (horizontal line), roughly corresponding to 1 addition/subtraction per sequence recall. The network was robust to noise, and sequence recall accuracy degraded gracefully as the amplitude of noise was increased. (Bottom-Left) Estimates of chunking rate measure <italic>CR</italic> for monitoring chunking in the noiseless case (blue curves).<italic>CR</italic> is defined as the number of transitions taking place in the chunking layer during the presentation of a pattern in the sequence. During an initial transient <italic>CR</italic> decreases as learning proceeds, indicating the formation of the chunks. (Right) Activity in the chunking layer for two representative runs, one with no noise, the other with no chunks, where learning of <italic>Q</italic><sub><italic>ij</italic></sub> and <italic>R</italic><sub><italic>ji</italic></sub> was turned off. The identity of the chunks is color-coded. Interestingly, the boundaries of the chunks can change during training, and the chunks can undergo substantial reconfigurations at the beginning of the training phase. In absence of learning in <italic>Q</italic><sub><italic>ij</italic></sub> and <italic>R</italic><sub><italic>ji</italic></sub>, the chunking rate did not diminish over the course of learning, indicating the absence of chunks. <xref ref-type="supplementary-material" rid="pcbi.1004592.s005">S4 Fig</xref> displays the evolution of the individual weights for the run shown in the top-right panel (No Noise).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004592.g006"/>
</fig>
</sec>
<sec id="sec009">
<title>Pauses in activity precede the recall of a chunk</title>
<p>Chunks in motor learning are often identified by the pauses between successive actions [<xref ref-type="bibr" rid="pcbi.1004592.ref049">49</xref>]. More specifically, psycholinguistic studies often focus on pauses between words and utterance-final syllable prolongations [<xref ref-type="bibr" rid="pcbi.1004592.ref050">50</xref>], which are indicative of a hierarchical organization of the overall speech production apparatus [<xref ref-type="bibr" rid="pcbi.1004592.ref010">10</xref>]. Other experiments also show the hierarchical organization of information in chunks when performing other visuo-motor tasks [<xref ref-type="bibr" rid="pcbi.1004592.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1004592.ref009">9</xref>]. The network activity in our model exhibits a temporal structure that is reminiscent of these studies. In the recall phase, the network activity is paused until the new chunk has been “loaded” (<xref ref-type="fig" rid="pcbi.1004592.g003">Fig 3(c)</xref>, dashed lines in <xref ref-type="fig" rid="pcbi.1004592.g003">Fig 3(b)</xref>). The pauses in the chunking are a result of the synchronization between elementary chunking layers. The duration of the EM and the CM activations depend on the magnitude of the growth terms <italic>b</italic><sub><italic>x</italic></sub> and <italic>b</italic><sub><italic>y</italic></sub>, but the two layers are bound to each other by the feedback connections <italic>Q</italic><sub><italic>ij</italic></sub> and <italic>R</italic><sub><italic>ji</italic></sub>. As a consequence, the EMs are delayed until the next chunk in the sequence is activated. The function of the pause is therefore to synchronize the activity of the CM and the sequential activity of the EM belonging to this chunk, and therefore depends on the relative speed between the elementary layer and the chunking layer. The duration of the pause is variable and did not depend on the number of items in each chunk.</p>
<p>In [<xref ref-type="bibr" rid="pcbi.1004592.ref007">7</xref>], the pause is assumed a direct result of two interacting processes running in parallel: one segmenting long sequential structures into shorter ones, and one process concatenating these same groups of motor elements into longer sequences. In our model, the ongoing competition within the layer and the cooperation between its layers are also two interacting parallel processes as in [<xref ref-type="bibr" rid="pcbi.1004592.ref007">7</xref>]. Concatenation in our model is performed by the competitive process along a given layer, while segmentation is performed by the cooperative couplings between layers. Our model is therefore consistent with the one described in [<xref ref-type="bibr" rid="pcbi.1004592.ref007">7</xref>].</p>
</sec>
<sec id="sec010">
<title>Learning dynamics determine chunk size</title>
<p>In the learned state, we find that the number of items in each chunk depends on the learning dynamics and the time constant in the synaptic dynamics <italic>z</italic> (<xref ref-type="fig" rid="pcbi.1004592.g007">Fig 7</xref>). The chunk size is the result of an equilibrium between competing learning processes in the dynamics. The size of the chunk is bounded by the magnitude of the <italic>Q</italic><sub><italic>ij</italic></sub> and <italic>R</italic><sub><italic>ij</italic></sub> potentiation when <italic>x</italic><sub><italic>i</italic></sub> and <italic>y</italic><sub><italic>j</italic></sub> are co-active, and the magnitude of the depotentiation when other elements <italic>x</italic><sub><italic>i</italic>′</sub>, <italic>i</italic>′ ≠ <italic>i</italic> belonging to the same chunk are active. This is because a coupling between a CM and EM undergoes depotentiation when other EM belonging to the same CM are active. The maximum number of elements in a chunk will therefore be limited by how much a CM and a EM potentiate when both are active versus the magnitude of the depotentiation when only the CM is active (and other EMs belonging to that chunk are active). This observation suggests the important result that the neural mechanisms for acquiring the chunking sequence also play a role in determining the capacity of chunking sequential memory, and lead to new experimental predictions. For example, there is evidence that dopamine modulates the cortico-striatal plasticity chunking during motor sequence learning in humans and monkeys. In monkeys the learning of new sequences was significantly affected by injection of a dopamine receptor antagonist, but did not affect sequences that were learned prior to the injection [<xref ref-type="bibr" rid="pcbi.1004592.ref047">47</xref>]. In the context of our model, this dopamine related modulation could translate into reducing <italic>γ</italic><sub><italic>p</italic></sub> or increasing <italic>γ</italic><sub><italic>d</italic></sub>. For example, if <italic>γ</italic><sub><italic>p</italic></sub> were gradually reduced, our model would predict a gradual decrease in chunk sizes in a chunking task such as those conducted in [<xref ref-type="bibr" rid="pcbi.1004592.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref008">8</xref>] (<italic>e.g.</italic> <xref ref-type="fig" rid="pcbi.1004592.g007">Fig 7</xref>, left). Note that not all of the chunking units are used to learn and recall the presented sequence, and therefore they remain available for the learning of other sequences.</p>
<fig id="pcbi.1004592.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004592.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Chunk size, number of EM in each chunk, (left) as a function of the potentiation scaling factor in Q, <inline-formula id="pcbi.1004592.e006"><alternatives><graphic id="pcbi.1004592.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e006"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>p</mml:mi><mml:mi>Q</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, (right) as a function of  the time constant in the synaptic dynamics, <italic>τ</italic><sub><italic>z</italic></sub>.</title>
<p>The number of information-carrying items contained in the chunks depends on the system dynamics, suggesting that they have impact on the total capacity of the memory. The initial random conditions lead the system to different structures after learning (number and size of chunks). The case <italic>τ</italic><sub><italic>z</italic></sub> = 0 corresponds to completely removing the synaptic dynamics. Although the chunking is present in the absence of <italic>z</italic><sub><italic>j</italic></sub>, the characteristic time scale of <italic>z</italic><sub><italic>j</italic></sub>, <italic>τ</italic><sub><italic>z</italic></sub> has a powerful effect on chunk size. Each point was evaluated 100 times and the mean and standard deviation are presented, suggesting a monotonically increasing relationship between chunk size and <inline-formula id="pcbi.1004592.e007"><alternatives><graphic id="pcbi.1004592.e007g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e007"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>p</mml:mi><mml:mi>Q</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> or <italic>τ</italic><sub><italic>z</italic></sub>. In total, 98.6% of the runs exhibited sequential activity in the chunking layer. Total number of available chunk modes, <italic>N</italic><sub><italic>Y</italic></sub> = 30; total number of elementary modes, <italic>N</italic><sub><italic>X</italic></sub> = 30.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004592.g007"/>
</fig>
<p>Chunk size can also be modulated within the sequence, by injecting a time-varying input into the synaptic variable <italic>z</italic><sub><italic>k</italic></sub>. We observe that the chunk size is proportional to the magnitude of this input <xref ref-type="supplementary-material" rid="pcbi.1004592.s003">S2 Fig</xref>. A neural analog of this modulation can be viewed as top-down attention [<xref ref-type="bibr" rid="pcbi.1004592.ref048">48</xref>], where sequential attention switching between multimodal mental activities depend on internal or external cues.</p>
</sec>
</sec>
<sec id="sec011" sec-type="conclusions">
<title>Discussion</title>
<p>Chunking is a naturally occurring process by which information-carrying items are grouped and these groups are related to each other according to a learned syntax. Chunking simplifies task performance and helps break down problems in order to think, understand, and compose more efficiently [<xref ref-type="bibr" rid="pcbi.1004592.ref001">1</xref>]. Several studies suggested that animals can effectively increase the capacity of their working memory by grouping multiple informational items into chunks [<xref ref-type="bibr" rid="pcbi.1004592.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref051">51</xref>]. Studying dynamical neural models capable of achieving chunking in a robust, scalable and efficient manner can shed light onto the organization of learning, memory and information processing in the brain.</p>
<p>In experimental studies, the markers of chunking are the pauses and reaction times observed during sequence production tasks. To provide a dynamical account of these studies, we presented a dynamical model capable of learning patterns and their order as metastable states of a hierarchical Stable Heteroclinic Channel (SHC). Our model provides the possible dynamical origin of delays (pauses) before a new chunk is initiated.</p>
<p>Recent work [<xref ref-type="bibr" rid="pcbi.1004592.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref032">32</xref>] described non-linear dynamical models of the chunking process (also called sequences of sequences [<xref ref-type="bibr" rid="pcbi.1004592.ref032">32</xref>]). Rigorous analysis further confirmed that chunking behavior in their suggested model corresponds to a hierarchical heteroclinic network in phase space [<xref ref-type="bibr" rid="pcbi.1004592.ref031">31</xref>]. We propose a model that builds on [<xref ref-type="bibr" rid="pcbi.1004592.ref021">21</xref>] by introducing a synaptic weight update rule that accommodates the unsupervised learning of the chunking process.</p>
<p>Our SHC-based approach guarantees robustness and sensitivity, which are two critical features for information processing with transient brain dynamics. Robust transients and sensitivity to inputs may be seen as contradictory requirements. However, previous work showed that spatiotemporal modes that contain metastable states can overcome this contradiction [<xref ref-type="bibr" rid="pcbi.1004592.ref052">52</xref>–<xref ref-type="bibr" rid="pcbi.1004592.ref054">54</xref>]. In our model, the activity in the system transitions from one metastable state to another along a SHC. The topology of the corresponding SHCs is strongly dependent on the stimuli, but the channel itself is structurally stable and robust against noise [<xref ref-type="bibr" rid="pcbi.1004592.ref022">22</xref>].</p>
<p>To demonstrate our findings, we used software simulations of the Generalized Lotka-Volterra (GLV). The GLV model is a non-linear dynamical system that is attractive for its mathematical simplicity: the existence of a SHC can be proven rigorously [<xref ref-type="bibr" rid="pcbi.1004592.ref044">44</xref>], and in the three-dimensional case its bifurcations have been extensively investigated [<xref ref-type="bibr" rid="pcbi.1004592.ref043">43</xref>]. Furthermore, the features of the GLVs relevant to this study can be replicated in dynamical systems that describe biological processes of neurons, such as integrate &amp; fire neurons [<xref ref-type="bibr" rid="pcbi.1004592.ref028">28</xref>], Hodgkin Huxley neurons [<xref ref-type="bibr" rid="pcbi.1004592.ref029">29</xref>], Wilson Cowan networks [<xref ref-type="bibr" rid="pcbi.1004592.ref030">30</xref>] and Fitzhugh Nagumo neurons [<xref ref-type="bibr" rid="pcbi.1004592.ref023">23</xref>].</p>
<p>Our model self-organizes to learn and recall sequences in a robust manner. Before learning the system has a single fixed point that depends on the applied stimulus and the initial conditions of the couplings. During training, the asymmetry in the inhibitory couplings increases and the network transitions from a Winner-Take-All (WTA) to a Winnerless Competition (WLC) configuration, such that the order in which the modes activate in the WLC is consistent with the presented sequence of patterns. Both the input patterns and their order are learned according to a hierarchical order: at a lower layer composed of elementary modes and at a higher level composed of chunking modes. When a chunk is recalled, the elementary layer incurs a pause that is similar to the delays observed at the boundaries of putative chunks observed when humans produced learned sequences [<xref ref-type="bibr" rid="pcbi.1004592.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1004592.ref009">9</xref>].</p>
<p>It is believed that chunking learning is a direct result of two separable interacting processes running in parallel: one segmenting long sequential patterns into shorter ones, and one process concatenating these same motor elements into longer sequences [<xref ref-type="bibr" rid="pcbi.1004592.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref055">55</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref056">56</xref>]. Our dynamical model naturally incorporates these two processes: Learning within the WLC dynamics within a layer concatenates the informational items through asymmetric Hebbian learning; while learning between WLC layers, combined with the competitive dynamics of the superordinate layer, mediate the segmentation the sequence of informational items. A direct consequence of two interacting layers are pauses in the activity: A subordinate layer is delayed until activity in the superordinate layer completes a transition.</p>
<sec id="sec012">
<title>Capacity of the WLC network</title>
<p>The number of sequences that can be stored simultaneously in the network is the total number of elements in all the learned sequences, since one unit is required for a single element of a sequence. In the case of a closed SHC, the number of different sequences that the SHC can store is equal to the number of distinct channels than can be formed with <italic>N</italic> nodes, which is of order exp(1) ⋅ (<italic>N</italic> − 1)! [<xref ref-type="bibr" rid="pcbi.1004592.ref023">23</xref>]. We note however, that under reasonable neuro-biological perturbations of the recurrent connectivity, the capacity is reduced. In that case, the maximal sequence length that can be stably recalled is about 7 [<xref ref-type="bibr" rid="pcbi.1004592.ref057">57</xref>]. Our model raises new questions on chunking capacity and recall under such perturbations. The benefit of chunking can be studied by comparing the maximal length of sequence in the presence or absence of chunking. This study is complicated by the fact that the average chunk size in the network is strongly dependent on the parameters of the learning dynamics (<xref ref-type="fig" rid="pcbi.1004592.g007">Fig 7</xref>), and is the target of future work.</p>
<p>Note that for simplicity, our current model cannot learn sequences that have recurring patterns. However this is possible in principle since other closely related work dealt with recurring patterns in sequences by retaining a memory of the past patterns in the sequence [<xref ref-type="bibr" rid="pcbi.1004592.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref059">59</xref>] or by using “template” connectivity matrices [<xref ref-type="bibr" rid="pcbi.1004592.ref032">32</xref>].</p>
</sec>
<sec id="sec013">
<title>Related hierarchical sequence learning models</title>
<p>The learning in the elementary layer of our model shares many features with models of competitive learning [<xref ref-type="bibr" rid="pcbi.1004592.ref060">60</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref061">61</xref>] and self-organizing maps [<xref ref-type="bibr" rid="pcbi.1004592.ref062">62</xref>]. In competitive learning, each stimulus is compared with a feature vector stored at each neuron. The neuron with the highest similarity is selected as the winner, and the feature vector is updated. This mechanism is similar to the effect of learning in the projection matrix <italic>P</italic> and the competitive dynamics in the WLC in our model. Our model extends this idea further by embedding the order of the stimuli in the network as winnerless competition dynamics.</p>
<p>Our model bears strong similarities with previous work in the recognition of sequences of sequences [<xref ref-type="bibr" rid="pcbi.1004592.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref063">63</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref064">64</xref>]. Kiebel et al. study the recognition of complex sequences, where the generative model is assumed <italic>a priori</italic> [<xref ref-type="bibr" rid="pcbi.1004592.ref032">32</xref>]. There, the within-layer connectivity matrix is modulated by activity in supra-ordinate levels. In contrast, feedback in our model is an additive term whose effect is to turn on or off circuits (SHCs) in the subordinate layers. This modeling choice comes at the cost of more nodes, but does not require the modulation of the connections. While the model presented in [<xref ref-type="bibr" rid="pcbi.1004592.ref064">64</xref>] addressed the learning of sound sequences, it did not address the learning of chunks (<italic>i.e</italic> sequences of sequences).</p>
<p>Other related methods for learning sequences in brain-inspired models are reservoir computers [<xref ref-type="bibr" rid="pcbi.1004592.ref065">65</xref>–<xref ref-type="bibr" rid="pcbi.1004592.ref067">67</xref>], synfire chains [<xref ref-type="bibr" rid="pcbi.1004592.ref068">68</xref>–<xref ref-type="bibr" rid="pcbi.1004592.ref070">70</xref>] and chains of WTA networks [<xref ref-type="bibr" rid="pcbi.1004592.ref071">71</xref>]. The idea of exploiting asymmetrically coupled networks for sequence learning was reported in multiple works based on attractor networks [<xref ref-type="bibr" rid="pcbi.1004592.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref065">65</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref069">69</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref072">72</xref>–<xref ref-type="bibr" rid="pcbi.1004592.ref074">74</xref>]. The novelty of our approach is the learning of the hierarchical dynamics as a sequence of metastable states. Hence, our model offers a non-linear dynamical perspective on the problem of hierarchical sequence learning in neural substrates that is fundamentally different from attractor networks.</p>
<p>Another attempt to map this type of dynamics on the cortex is the hierarchical temporal memory model [<xref ref-type="bibr" rid="pcbi.1004592.ref075">75</xref>], although that work does not address the dynamics of biologically inspired learning of hierarchical sequences.</p>
</sec>
<sec id="sec014">
<title>Stability of the learning dynamics and robustness to parameters</title>
<p>Stability can be viewed from two related perspectives: robustness of the dynamics to noise in the nodes and in the connections (structural stability); and stability of the metastable states, <italic>i.e.</italic> their Lyapunov exponents. In either case, the study of learning stability in the general case is notoriously difficult, because the addition of new information-carrying items can destroy existing metastable states for example by creating spurious attractors [<xref ref-type="bibr" rid="pcbi.1004592.ref076">76</xref>]. In the three dimensional case, the Lotka Volterra dynamics can be thoroughly analyzed. However, many more difficulties appear in four or more dimensions, such as new metastable states in the phase space of the system, making the analysis much more difficult [<xref ref-type="bibr" rid="pcbi.1004592.ref036">36</xref>].</p>
<p>However, it is possible to gain some insight in the asymptotic case where the time scales in the system are well separated. In our case these are arranged such that <italic>P</italic> reaches equilibrium before <italic>V</italic>, <italic>V</italic> before <italic>Q</italic>, <italic>W</italic> before <italic>R</italic>. The overall dynamics of the elementary <italic>P</italic> associates stimulus items to neurons through a competitive learning mechanism and can be thoroughly analyzed. Because <italic>P</italic> modulates the increment to the nodes, it does not interfere with the structure of the elementary network. As long as LTP and LTD in the couplings <italic>V</italic> and <italic>W</italic> are balanced and the transitions in the network are monotonic, the weights in the network tend to a WLC configuration (see section 1 of <xref ref-type="supplementary-material" rid="pcbi.1004592.s001">S1 Text</xref>).</p>
<p>The dynamics of the synapses between EM and CM capture the chunking behavior, and are very similar to the <italic>P</italic> dynamics. It segments the chain of activations in the elementary layer into chunks, by detecting change points in the sequence. Its function is comparable to sequence segmentation using the sliding window algorithm commonly used for online natural language processing [<xref ref-type="bibr" rid="pcbi.1004592.ref077">77</xref>].</p>
<p>In this asymptotic case, the parameters can be selected manually such that learning at each time scale progresses as described above.</p>
</sec>
<sec id="sec015">
<title>Failures to recall chunking sequences</title>
<p>In some cases, the model failed to recall the chunking sequences, especially when the parameters of learning dynamics were not appropriately chosen. The scenarios through which recall fails is of particular interest because these can provide insights into the dynamical causes of chunking deficits in neurodegenerative diseases, such as Parkinson’s disease.</p>
<p>The most common cause of failing to learn was that a transition between two EM’s did not form, or was not strong enough to drive it. As a result, the state of the network remained “stuck” and is reminiscent of certain motor disorders observed in Parkinson’s patients. The recall typically resumes by providing a stimulus corresponding to an item in the cue, which is consistent with how sensory cues can improve symptoms of bradykinesia [<xref ref-type="bibr" rid="pcbi.1004592.ref078">78</xref>].</p>
<p>Similar behavioral observations were made on elderly who could not learn motor chunks during a sequence production task [<xref ref-type="bibr" rid="pcbi.1004592.ref079">79</xref>]. In the elderly, reduced cognitive abilities impede the learning of motor chunks, although most of the tested individuals were capable of correctly reacting to the stimuli that indicated the sequence to recall. In our model, this is equivalent to a successful learning between the perceptual layer and the elementary layer, but failing to learn the weights within the elementary layer.</p>
<p>In other cases where learning failed, the chunking modes did not reach a WLC configuration, although the sequential structure was learned in the elementary layer. The result is that the activity in the chunking layer remained constant and did not affect the sequential structure of the EMs activations. This shortcoming was revealed in the elementary layer by the lack of pauses during the sequence recall.</p>
</sec>
<sec id="sec016">
<title>Conclusions</title>
<p>In this paper, we proposed a model of hierarchical chunking learning dynamics that can represent several forms of cognitive activities such as working memory and speech construction. This model is capable of learning patterns and their order as metastable states of a hierarchical SHC, and reproduces several key features observed in chunking behavior in humans.</p>
<p>The model and the results outlined in this paper sheds new light onto the formation of sequential working memory and chunking. Complex action (such as speech or song production) can be viewed as a chain of subordinate movements, which need to be combined according to a syntax in order to reach a goal.</p>
<p>Recent studies suggest that failures in reaching a functional configuration of the couplings is related to other diseases such as schizophrenia [<xref ref-type="bibr" rid="pcbi.1004592.ref039">39</xref>], obsessive-compulsive disorder [<xref ref-type="bibr" rid="pcbi.1004592.ref080">80</xref>], and Parkinson’s. Our model can generalize the dynamical image of these diseases by taking into account learning and chunking dynamics, in order to provide novel insights into treating them.</p>
</sec>
</sec>
<sec id="sec017" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec018">
<title>Transient brain dynamics: Hierarchical chunking</title>
<p>Our overarching hypothesis is that cognitive function in the brain is described by the non-linear interaction of brain “modes”. The number of these modes is assumed much smaller than the number of variables required to describe the state of the brain (<italic>e.g.</italic> membrane potentials, channel states). Backed by recent brain imaging techniques, we follow a top-down approach for identifying the nature of these modes, and how they interact in a transient, robust and scalable fashion to process information [<xref ref-type="bibr" rid="pcbi.1004592.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1004592.ref081">81</xref>].</p>
<p>In this context, a mode is defined as a metastable composition of elements from different brain areas that activate coherently to perform a specific cognitive task. Here, we focus on the cognitive task of recalling a sequence, which can be described by the sequential activation of brain modes. In particular, our approach is based on spatiotemporal mental modes that contain metastable states as equilibrium points since it resolves the contradiction by which the system must be robust to noise and, at the same time, sensitive to inputs [<xref ref-type="bibr" rid="pcbi.1004592.ref052">52</xref>–<xref ref-type="bibr" rid="pcbi.1004592.ref054">54</xref>].</p>
<p>Metastable states are semi-transient signals that can be represented as saddle nodes. These saddle nodes can be arranged to form a SHC, which consists of a sequence of successive states that are connected through their respective unstable separatrices (<xref ref-type="fig" rid="pcbi.1004592.g008">Fig 8</xref>). Under appropriate parametrizations, namely if the compressing of phase space around the saddle is larger than the stretching and if all saddles in the chain are dissipative, then the trajectories in the neighborhood of the metastable states that form the chain remain in the channel [<xref ref-type="bibr" rid="pcbi.1004592.ref022">22</xref>].</p>
<fig id="pcbi.1004592.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004592.g008</object-id>
<label>Fig 8</label>
<caption>
<p>(A) Stable heteroclinic chain with two connected metastable states (B) Stable heteroclinic channel (SHC)—robust sequence of metastable states. Adapted from [<xref ref-type="bibr" rid="pcbi.1004592.ref082">82</xref>]. (C) Transformation of the phase volume along trajectories in the neighborhood of unstable separatrix in the case when both coupled saddles are characterized by saddle values larger than one.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004592.g008"/>
</fig>
<p>The GLV dynamics is a canonical model for implementing a SHC [<xref ref-type="bibr" rid="pcbi.1004592.ref042">42</xref>]:
<disp-formula id="pcbi.1004592.e008"><alternatives><graphic id="pcbi.1004592.e008g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e008"/><mml:math id="M8" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi> <mml:mrow><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>X</mml:mi></mml:msub></mml:munderover> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>η</mml:mi> <mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mspace width="1.em"/><mml:mo>∀</mml:mo> <mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
The terms <italic>V</italic><sub><italic>ii</italic>′</sub> determine the interaction between the variables <italic>x</italic><sub><italic>i</italic></sub>, and <italic>η</italic><sub><italic>i</italic></sub> is an additive noise term. This asymmetry in <italic>V</italic><sub><italic>ii</italic>′</sub> installs metastable nodes in the network, which results in successive and temporary winners as in WLC dynamics [<xref ref-type="bibr" rid="pcbi.1004592.ref023">23</xref>]. The simplicity of this model enables theoretical study of the transient solutions representing sequential competition [<xref ref-type="bibr" rid="pcbi.1004592.ref042">42</xref>]. The dynamical features of the system <xref ref-type="disp-formula" rid="pcbi.1004592.e008">Eq (5)</xref> extend to a wide class of dynamical systems, known as Kolmogorov models [<xref ref-type="bibr" rid="pcbi.1004592.ref026">26</xref>]. The biological relevance of these models is confirmed by several previous works [<xref ref-type="bibr" rid="pcbi.1004592.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1004592.ref030">30</xref>].</p>
<p>The state variables in <xref ref-type="disp-formula" rid="pcbi.1004592.e008">Eq (5)</xref> are modes that represent abstract quantities that do not necessarily map directly or exactly onto individual neuron or populations activities. For instance, [<xref ref-type="bibr" rid="pcbi.1004592.ref029">29</xref>] show the existence of a SHC in a network of inhibitory Hodgkin Huxley-type (H&amp;H) neurons short-term synaptic depression, despite that the differential equations there differ significantly from <xref ref-type="disp-formula" rid="pcbi.1004592.e008">Eq (5)</xref>. Another example is given by [<xref ref-type="bibr" rid="pcbi.1004592.ref028">28</xref>], which describes the conditions under which the firing rate of leaky Integrate &amp; Fire (I&amp;F) neurons approximately map onto <xref ref-type="disp-formula" rid="pcbi.1004592.e008">Eq (5)</xref>.</p>
<p>The hierarchical chunking dynamics is represented by robust transient activity modes at each scale of the hierarchy. The above <xref ref-type="disp-formula" rid="pcbi.1004592.e008">Eq (5)</xref> serves as an elementary building block for each layer of the chunking dynamics. The two-layer chunking dynamics is a GLV system of the form of <xref ref-type="disp-formula" rid="pcbi.1004592.e001">Eq (1)</xref>. This model has slight modifications to the one presented in [<xref ref-type="bibr" rid="pcbi.1004592.ref021">21</xref>], which reflect the necessities for chunk formation during training. Firstly, the polarity of the couplings between the two layers is reversed (in [<xref ref-type="bibr" rid="pcbi.1004592.ref021">21</xref>] elementary modes inhibit chunking modes). This modification allows the elementary modes to directly drive a CM. Secondly, the synaptic dynamics represented by the dimension <italic>z</italic> are applied to the growth terms of the chunking layer (in contrast to [<xref ref-type="bibr" rid="pcbi.1004592.ref021">21</xref>], where only inhibitory couplings are subject to synaptic dynamics). The synaptic dynamics helps a single CM to remain active over several items in the stimulus.</p>
</sec>
<sec id="sec019">
<title>Synaptic plasticity model</title>
<p>The structure of the sequential activity is determined by the connectivity matrix among the respective modes. Within each layer, the amount of asymmetry in the couplings represents an order parameter that controls the dynamical behavior of the network. The inter-layer connections represent the association of the information-carrying items and chunks with the modes. After the presentation of the inputs, the network is run for a consolidation time, and the weights are held fixed to the values reached at the end of this time for recall.</p>
<p>The learning can be understood as the adjustment of this order parameter and the associations in a way that the recall dynamics of the elementary and the chunking modes is consistent with the training sequences.</p>
<sec id="sec020">
<title>Couplings <italic>P</italic><sub><italic>ki</italic></sub></title>
<p>The synapses between the PMs and the EMs follow a correlation rule with synaptic scaling [<xref ref-type="bibr" rid="pcbi.1004592.ref040">40</xref>] <xref ref-type="disp-formula" rid="pcbi.1004592.e003">Eq (2)</xref>. The input synapses learn which PMs are associated to a particular pattern. This rule can learn hidden causes of noisy sensory activations in a mixture model [<xref ref-type="bibr" rid="pcbi.1004592.ref041">41</xref>]. As in [<xref ref-type="bibr" rid="pcbi.1004592.ref041">41</xref>], we assume that a (unspecified) feedforward inhibition normalizes the intensity of the input patterns such that at steady state, ∑<sub><italic>k</italic></sub> <italic>s</italic><sub><italic>k</italic></sub> = <italic>C</italic> and ∑<sub><italic>k</italic></sub> <italic>P</italic><sub><italic>ki</italic></sub> = <italic>C</italic>.</p>
</sec>
<sec id="sec021">
<title>Couplings <italic>V</italic><sub><italic>ii</italic>′</sub> and <italic>W</italic><sub><italic>jj</italic>′</sub></title>
<p>The weight update of the coupling between EMs from <italic>i</italic> to <italic>i</italic>′ are dictated by a bistable synaptic plasticity rule with matched potentiation and depression according to <xref ref-type="disp-formula" rid="pcbi.1004592.e004">Eq (3)</xref>, where the potentiation and depotentiation terms are:
<disp-formula id="pcbi.1004592.e009"><alternatives><graphic id="pcbi.1004592.e009g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e009"/><mml:math id="M9" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi> <mml:mi>T</mml:mi> <mml:msub><mml:mi>P</mml:mi> <mml:mi>V</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>Θ</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>A</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:msub></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>p</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi> <mml:mi>T</mml:mi> <mml:msub><mml:mi>D</mml:mi> <mml:mi>V</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>Θ</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:msub> <mml:msub><mml:mi>A</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>A</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>A</mml:mi> <mml:mo>+</mml:mo></mml:msup> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:msub><mml:mi>K</mml:mi> <mml:mi>A</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="normal">d</mml:mi> <mml:mo>Δ</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>A</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>A</mml:mi> <mml:mo>-</mml:mo></mml:msup> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:msub><mml:mi>K</mml:mi> <mml:mi>A</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="normal">d</mml:mi> <mml:mo>Δ</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
where Θ is the Heaviside (step) function that returns 1 if its argument is positive and 0 otherwise, and <italic>θ</italic><sub><italic>p</italic></sub>, <italic>θ</italic><sub><italic>d</italic></sub> are constant potentiation and depotentiation thresholds. <italic>A</italic><sub><italic>x</italic><sub><italic>j</italic></sub></sub>, <italic>A</italic><sub><italic>x</italic><sub><italic>i</italic></sub></sub> are traces obtained by filtering the activities <italic>x</italic><sub><italic>i</italic></sub>, <italic>x</italic><sub><italic>j</italic></sub> with the learning window.</p>
<p><italic>V</italic><sup>+</sup>, <italic>V</italic><sup>−</sup>, and <italic>V</italic>* are the fixed points of the bistable learning rule [<xref ref-type="bibr" rid="pcbi.1004592.ref034">34</xref>].
<disp-formula id="pcbi.1004592.e010"><alternatives><graphic id="pcbi.1004592.e010g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e010"/><mml:math id="M10" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mspace width="1.em"/><mml:msup><mml:mi>V</mml:mi> <mml:mo>+</mml:mo></mml:msup> <mml:mo>&gt;</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>&gt;</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mo>-</mml:mo></mml:msup> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
Following this definition, the former are determined to be stable two are stable, while the latter is unstable. Once the weight <italic>V</italic><sub><italic>ii</italic>′</sub> crosses <italic>V</italic>*, in the absence of stimuli it is attracted towards <italic>V</italic><sup>+</sup> if <italic>V</italic><sub><italic>ii</italic>′</sub> &gt; <italic>V</italic>* and <italic>V</italic><sup>−</sup> otherwise.</p>
<p>When the activity transitions from one element to another, the synapse along the direction of the transition undergoes depotentiation, while the synapse in the opposite direction undergoes potentiation. At each state transition, this rule depotentiates the inhibitory synapse in the direction of the transition, and potentiates it in the opposite direction.</p>
<p>Initially each unit is associated with a stable fixed point. After a sufficient number of such updates, the stable fixed point becomes a saddle node, where the unstable separatrix leads to the unit associated with the subsequent item in the sequence. The number of updates required for this to occur depends on the magnitude of the synaptic updates, which plays the role of a learning rate. When synaptic potentiation and depression are matched, the weights are modified only when the activities of the modes change (see section 1 of <xref ref-type="supplementary-material" rid="pcbi.1004592.s001">S1 Text</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004592.s003">S2 Fig</xref>). The same synaptic dynamics apply for the couplings <italic>W</italic><sub><italic>jj</italic>′</sub> among the chunking modes.</p>
</sec>
<sec id="sec022">
<title>Couplings <italic>Q</italic><sub><italic>ij</italic></sub></title>
<p>The chunking layer takes the elementary modes’ activity as its inputs, and associates a group of elementary modes to a CM. The learning rule <xref ref-type="disp-formula" rid="pcbi.1004592.e005">Eq (4)</xref> is a bistable adaptation of <xref ref-type="disp-formula" rid="pcbi.1004592.e003">Eq (2)</xref>, where <italic>f</italic><sub><italic>Q</italic></sub>(<italic>Q</italic>) implements the bistable dynamics:
<disp-formula id="pcbi.1004592.e011"><alternatives><graphic id="pcbi.1004592.e011g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e011"/><mml:math id="M11" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>+</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>-</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
The duration of each chunk is strongly dependent on the potentiation and depotentiation scaling factors <inline-formula id="pcbi.1004592.e012"><alternatives><graphic id="pcbi.1004592.e012g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e012"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi> <mml:mi>p</mml:mi> <mml:mi>Q</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004592.e013"><alternatives><graphic id="pcbi.1004592.e013g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e013"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi> <mml:mi>d</mml:mi> <mml:mi>Q</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>A complete analysis of this learning rule is not possible because it involves the non-linear dynamics of both EM and CM. An intuition to the behavior of this rule can be obtained by comparing it to the rule governing <italic>P</italic>. In the case where <inline-formula id="pcbi.1004592.e014"><alternatives><graphic id="pcbi.1004592.e014g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e014"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>p</mml:mi><mml:mi>Q</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>θ</mml:mi><mml:mi>d</mml:mi><mml:mi>Q</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <italic>ϵ</italic><sub><italic>H</italic></sub> = 0, the rectifying function Θ becomes the identity function since <italic>x</italic><sub><italic>i</italic></sub> ≤ 0 and <italic>y</italic><sub><italic>j</italic></sub> ≤ 0. Choosing for clarity <italic>Q</italic><sup>−</sup> = 0, <italic>α</italic><sub><italic>Q</italic></sub> = 1, <inline-formula id="pcbi.1004592.e015"><alternatives><graphic id="pcbi.1004592.e015g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e015"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>γ</mml:mi> <mml:mi>d</mml:mi> <mml:mi>Q</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004592.e016"><alternatives><graphic id="pcbi.1004592.e016g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e016"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>γ</mml:mi> <mml:mi>p</mml:mi> <mml:mi>Q</mml:mi></mml:msubsup> <mml:mo>/</mml:mo> <mml:msubsup><mml:mi>γ</mml:mi> <mml:mi>d</mml:mi> <mml:mi>Q</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, the rule becomes:
<disp-formula id="pcbi.1004592.e017"><alternatives><graphic id="pcbi.1004592.e017g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e017"/><mml:math id="M17" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi> <mml:mrow><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>≅</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>(</mml:mo><mml:mi>r</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>+</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(8)</label></disp-formula>
which is identical to <xref ref-type="disp-formula" rid="pcbi.1004592.e003">Eq (2)</xref>, with the exception of an upper boundary on the weight <italic>Q</italic><sup>+</sup>. The conditioning of the stimulus ensures that switches in the chunking layer usually occur only when a new pattern is presented. At each activation of a EM, the active CM can persist or lose competition against another CM. The probability of either event taking place is dictated by the size of the chunk and the initial state of <italic>Q</italic>.</p>
</sec>
<sec id="sec023">
<title>Couplings <italic>R</italic><sub><italic>ij</italic></sub></title>
<p>The chunking modes inhibit the elementary network in a way that the activities of both layers coherently bind to each other. This inhibition is learned with a rule similar to the one above but with swapped boundaries. As a result, when both elementary modes and chunking modes are active, the weight depotentiates (inhibits less), but when only the CM is active, the weight potentiates.
<disp-formula id="pcbi.1004592.e018"><alternatives><graphic id="pcbi.1004592.e018g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e018"/><mml:math id="M18" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mfrac><mml:mi mathvariant="normal">d</mml:mi> <mml:mrow><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mi>R</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:msub><mml:mi>f</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>R</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi> <mml:mi>p</mml:mi> <mml:mi>R</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>R</mml:mi> <mml:mo>-</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>R</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>p</mml:mi> <mml:mi>R</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mo>+</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi> <mml:mi>d</mml:mi> <mml:mi>R</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>R</mml:mi> <mml:mo>+</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>R</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>d</mml:mi> <mml:mi>R</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
where,
<disp-formula id="pcbi.1004592.e019"><alternatives><graphic id="pcbi.1004592.e019g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004592.e019"/><mml:math id="M19" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>R</mml:mi> <mml:mo>+</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>R</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>R</mml:mi> <mml:mo>-</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>R</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>R</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>R</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
The effect of this rule is to learn a configuration where the EMs associated to the active CM are disinhibited.</p>
</sec>
</sec>
<sec id="sec024">
<title>Characterizing sequence recall</title>
<p>At the end of successful training, the network is able to recall the presented sequences. Successful recall is defined when the sequence order is produced with perfect accuracy. However, it occurred that the sequence was reproduced to a reasonable extent (<italic>e.g.</italic> missing elements, sequence reproduced correctly up to certain element). To take into account such events, we used a normalized Levenshtein distance to estimate the quality of the reproduction [<xref ref-type="bibr" rid="pcbi.1004592.ref083">83</xref>]. This distance computes the number of changes between two sequences (addition, subtraction), normalized by the length of the longest sequence. Note that sequence recall does not characterize chunking since accurate recall can be obtained without learning in the chunking layer.</p>
</sec>
</sec>
<sec id="sec025">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004592.s001" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004592.s001" mimetype="application/pdf" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Section 1, Details to the learning rule <xref ref-type="disp-formula" rid="pcbi.1004592.e004">Eq (3)</xref>.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004592.s002" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004592.s002" mimetype="image/tiff" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Asymmetric learning windows causes the weight to change when a transition between two units takes place.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004592.s003" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004592.s003" mimetype="image/tiff" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Network Dynamics Influence Chunking Rate.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004592.s004" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004592.s004" mimetype="image/tiff" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Chunking rate is modulated by a time-varying bias in the chunking layer.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004592.s005" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004592.s005" mimetype="image/tiff" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Examples of noisy stimuli.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Henry Abarbanel, Yury Sokolov and Thomas Nowotny for their helpful comments. We also thank and Brenton Maisel and Uriel Morone for reviewing an earlier version of this manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004592.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ericcson</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Chase</surname> <given-names>WG</given-names></name>, <name name-style="western"><surname>Faloon</surname> <given-names>S</given-names></name>. <article-title>Acquisition of a memory skill</article-title>. <source>Science</source>. <year>1980</year>;<volume>208</volume>(<issue>4448</issue>):<fpage>1181</fpage>–<lpage>1182</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.7375930" xlink:type="simple">10.1126/science.7375930</ext-link></comment> <object-id pub-id-type="pmid">7375930</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bousfield</surname> <given-names>WA</given-names></name>. <article-title>The occurrence of clustering in the recall of randomly arranged associates</article-title>. <source>The Journal of General Psychology</source>. <year>1953</year>;<volume>49</volume>(<issue>2</issue>):<fpage>229</fpage>–<lpage>240</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/00221309.1953.9710088" xlink:type="simple">10.1080/00221309.1953.9710088</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Miller</surname> <given-names>GA</given-names></name>. <article-title>The magical number seven, plus or minus two: some limits on our capacity for processing information</article-title>. <source>Psychological review</source>. <year>1956</year>;<volume>63</volume>(<issue>2</issue>):<fpage>81</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0043158" xlink:type="simple">10.1037/h0043158</ext-link></comment> <object-id pub-id-type="pmid">13310704</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gobet</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Lane</surname> <given-names>PC</given-names></name>, <name name-style="western"><surname>Croker</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cheng</surname> <given-names>PC</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Oliver</surname> <given-names>I</given-names></name>, <etal>et al</etal>. <article-title>Chunking mechanisms in human learning</article-title>. <source>Trends in cognitive sciences</source>. <year>2001</year>;<volume>5</volume>(<issue>6</issue>):<fpage>236</fpage>–<lpage>243</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S1364-6613(00)01662-4" xlink:type="simple">10.1016/S1364-6613(00)01662-4</ext-link></comment> <object-id pub-id-type="pmid">11390294</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Verwey</surname> <given-names>WB</given-names></name>. <article-title>Concatenating familiar movement sequences: the versatile cognitive processor</article-title>. <source>Acta psychologica</source>. <year>2001</year>;<volume>106</volume>(<issue>1</issue>):<fpage>69</fpage>–<lpage>95</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0001-6918(00)00027-5" xlink:type="simple">10.1016/S0001-6918(00)00027-5</ext-link></comment> <object-id pub-id-type="pmid">11256340</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pammi</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Miyapuram</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Samejima</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Bapi</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Changing the structure of complex visuo-motor sequences selectively activates the fronto-parietal network</article-title>. <source>Neuroimage</source>. <year>2012</year>;<volume>59</volume>(<issue>2</issue>):<fpage>1180</fpage>–<lpage>1189</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2011.08.006" xlink:type="simple">10.1016/j.neuroimage.2011.08.006</ext-link></comment> <object-id pub-id-type="pmid">21867758</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wymbs</surname> <given-names>NF</given-names></name>, <name name-style="western"><surname>Bassett</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Mucha</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Porter</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Grafton</surname> <given-names>ST</given-names></name>. <article-title>Differential recruitment of the sensorimotor putamen and frontoparietal cortex during motor chunking in humans</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>74</volume>(<issue>5</issue>):<fpage>936</fpage>–<lpage>946</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.03.038" xlink:type="simple">10.1016/j.neuron.2012.03.038</ext-link></comment> <object-id pub-id-type="pmid">22681696</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sakai</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kitaguchi</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Hikosaka</surname> <given-names>O</given-names></name>. <article-title>Chunking during human visuomotor sequence learning</article-title>. <source>Experimental brain research</source>. <year>2003</year>;<volume>152</volume>(<issue>2</issue>):<fpage>229</fpage>–<lpage>242</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-003-1548-8" xlink:type="simple">10.1007/s00221-003-1548-8</ext-link></comment> <object-id pub-id-type="pmid">12879170</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bo</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Seidler</surname> <given-names>RD</given-names></name>. <article-title>Visuospatial working memory capacity predicts the organization of acquired explicit motor sequences</article-title>. <source>Journal of neurophysiology</source>. <year>2009</year>;<volume>101</volume>(<issue>6</issue>):<fpage>3116</fpage>–<lpage>3125</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00006.2009" xlink:type="simple">10.1152/jn.00006.2009</ext-link></comment> <object-id pub-id-type="pmid">19357338</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gee</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Grosjean</surname> <given-names>F</given-names></name>. <article-title>Performance structures: A psycholinguistic and linguistic appraisal</article-title>. <source>Cognitive Psychology</source>. <year>1983</year>;<volume>15</volume>(<issue>4</issue>):<fpage>411</fpage>–<lpage>458</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/0010028583900142">http://www.sciencedirect.com/science/article/pii/0010028583900142</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0010-0285(83)90014-2" xlink:type="simple">10.1016/0010-0285(83)90014-2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ellis</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Sinclair</surname> <given-names>S</given-names></name>. <article-title>Working memory in the acquisition of vocabulary and syntax: Putting language in good order</article-title>. <source>The Quarterly Journal of Experimental Psychology: Section A</source>. <year>1996</year>;<volume>49</volume>(<issue>1</issue>):<fpage>234</fpage>–<lpage>250</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/713755604" xlink:type="simple">10.1080/713755604</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Luck</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Vogel</surname> <given-names>EK</given-names></name>. <article-title>The capacity of visual working memory for features and conjunctions</article-title>. <source>Nature</source>. <year>1997</year>;<volume>390</volume>(<issue>6657</issue>):<fpage>279</fpage>–<lpage>281</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/36846" xlink:type="simple">10.1038/36846</ext-link></comment> <object-id pub-id-type="pmid">9384378</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Graybiel</surname> <given-names>AM</given-names></name>. <article-title>The basal ganglia and chunking of action repertoires</article-title>. <source>Neurobiology of learning and memory</source>. <year>1998</year>;<volume>70</volume>(<issue>1</issue>):<fpage>119</fpage>–<lpage>136</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1006/nlme.1998.3843" xlink:type="simple">10.1006/nlme.1998.3843</ext-link></comment> <object-id pub-id-type="pmid">9753592</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Pammi</surname> <given-names>VC</given-names></name>, <name name-style="western"><surname>Miyapuram</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Bapi</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <chapter-title>Chunking phenomenon in complex sequential skill learning in humans</chapter-title>. In: <source>Neural Information Processing</source>. <publisher-loc>Springer</publisher-loc>; <year>2004</year>. p. <fpage>294</fpage>–<lpage>299</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Williams</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Staples</surname> <given-names>K</given-names></name>. <article-title>Syllable chunking in zebra finch song</article-title>. <source>Journal of Comparative Psychology</source>. <year>1992</year>;<volume>106</volume>(<issue>3</issue>):<fpage>278</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0735-7036.106.3.278" xlink:type="simple">10.1037/0735-7036.106.3.278</ext-link></comment> <object-id pub-id-type="pmid">1395497</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rosenbaum</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Kenny</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Derr</surname> <given-names>MA</given-names></name>. <article-title>Hierarchical control of rapid movement sequences</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>1983</year>;<volume>9</volume>(<issue>1</issue>):<fpage>86</fpage>. <object-id pub-id-type="pmid">6220126</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="other">Miyapuram KP, Bapi RS, Pammi CV, Doya K, et al. Hierarchical chunking during learning of visuomotor sequences. In: Neural Networks, 2006. IJCNN’06. International Joint Conference on. IEEE; 2006. p. 249–253.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Newell</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rosenbloom</surname> <given-names>PS</given-names></name>. <article-title>Mechanisms of skill acquisition and the law of practice</article-title>. <source>Cognitive skills and their acquisition</source>. <year>1981</year>;<volume>1</volume>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Servan-Schreiber</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>JR</given-names></name>. <article-title>Learning artificial grammars with competitive chunking</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>. <year>1990</year>;<volume>16</volume>(<issue>4</issue>):<fpage>592</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Van Gelder</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Port</surname> <given-names>RF</given-names></name>. <article-title>It’s about time: An overview of the dynamical approach to cognition</article-title>. <source>Mind as motion: Explorations in the dynamics of cognition</source>. <year>1995</year>;<volume>1</volume>:<fpage>43</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rabinovich</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Varona</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tristan</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Afraimovich</surname> <given-names>V</given-names></name>. <article-title>Chunking dynamics: heteroclinics in mind</article-title>. <source>Frontiers in computational neuroscience</source>. <year>2014</year>;<volume>8</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2014.00022" xlink:type="simple">10.3389/fncom.2014.00022</ext-link></comment> <object-id pub-id-type="pmid">24672469</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Huerta</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Varona</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Afraimovich</surname> <given-names>VS</given-names></name>. <article-title>Transient cognitive dynamics, metastability, and decision making</article-title>. <source>PLoS computational biology</source>. <year>2008</year>;<volume>4</volume>(<issue>5</issue>):<fpage>e1000072</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000072" xlink:type="simple">10.1371/journal.pcbi.1000072</ext-link></comment> <object-id pub-id-type="pmid">18452000</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rabinovich</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Volkovskii</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lecanda</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Huerta</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Abarbanel</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Laurent</surname> <given-names>G</given-names></name>. <article-title>Dynamical encoding by networks of competing neuron groups: winnerless competition</article-title>. <source>Physical Review Letters</source>. <year>2001</year>;<volume>87</volume>(<issue>6</issue>):<fpage>68102</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.87.068102" xlink:type="simple">10.1103/PhysRevLett.87.068102</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rabinovich</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Huerta</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Laurent</surname> <given-names>G</given-names></name>. <article-title>Transient dynamics for neural processing</article-title>. <source>Science</source>. <year>2008</year> <month>Jul</month>;<volume>321</volume>:<fpage>48</fpage>–<lpage>50</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.pubmed.org/18599763">http://www.pubmed.org/18599763</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1155564" xlink:type="simple">10.1126/science.1155564</ext-link></comment> <object-id pub-id-type="pmid">18599763</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Lotka</surname> <given-names>AJ</given-names></name>. <source>Elements of physical biology</source>. <publisher-name>Williams &amp; Wilkins Baltimore</publisher-name>; <year>1925</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Brauer</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Castillo-Chávez</surname> <given-names>C</given-names></name>. <source>Mathematical Models in Population Biology and Epidemiology</source>. <volume>vol. 40</volume>. <publisher-name>Springer</publisher-name>; <year>2001</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hernández-Bermejo</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Fairén</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Brenig</surname> <given-names>L</given-names></name>. <article-title>Algebraic recasting of nonlinear systems of ODEs into universal formats</article-title>. <source>Journal of Physics A: Mathematical and General</source>. <year>1998</year>;<volume>31</volume>(<issue>10</issue>):<fpage>2415</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0305-4470/31/10/016" xlink:type="simple">10.1088/0305-4470/31/10/016</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fukai</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tanaka</surname> <given-names>S</given-names></name>. <article-title>A simple neural network exhibiting selective activation of neuronal ensembles: from winner-take-all to winners-share-all</article-title>. <source>Neural Comput</source>. <year>1997</year>;<volume>9</volume>(<issue>1</issue>):<fpage>77</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1997.9.1.77" xlink:type="simple">10.1162/neco.1997.9.1.77</ext-link></comment> <object-id pub-id-type="pmid">9117902</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nowotny</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Rabinovich</surname> <given-names>M</given-names></name>. <article-title>Dynamical origin of independent spiking and bursting activity in neural microcircuits</article-title>. <source>Physical review letters</source>. <year>2007</year>;<volume>98</volume>(<issue>12</issue>):<fpage>128106</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.98.128106" xlink:type="simple">10.1103/PhysRevLett.98.128106</ext-link></comment> <object-id pub-id-type="pmid">17501162</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Huerta</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rabinovich</surname> <given-names>M</given-names></name>. <article-title>Reproducible sequence generation in random neural ensembles</article-title>. <source>Physical review letters</source>. <year>2004</year>;<volume>93</volume>(<issue>23</issue>):<fpage>238104</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.93.238104" xlink:type="simple">10.1103/PhysRevLett.93.238104</ext-link></comment> <object-id pub-id-type="pmid">15601209</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Afraimovich</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Rabinovich</surname> <given-names>M</given-names></name>. <article-title>Hierarchical Heteroclinics In Dynamical Model Of Cognitive Processes: Chunking</article-title>. <source>International Journal of Bifurcation and Chaos</source>. <year>2014</year>;(in press). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1142/S0218127414501326" xlink:type="simple">10.1142/S0218127414501326</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kiebel</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Von Kriegstein</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Recognizing sequences of sequences</article-title>. <source>PLoS computational biology</source>. <year>2009</year>;<volume>5</volume>(<issue>8</issue>):<fpage>e1000464</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000464" xlink:type="simple">10.1371/journal.pcbi.1000464</ext-link></comment> <object-id pub-id-type="pmid">19680429</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Seliger</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tsimring</surname> <given-names>LS</given-names></name>, <name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>. <article-title>Dynamics-based sequential memory: Winnerless competition of patterns</article-title>. <source>Phys Rev E</source>. <year>2003</year> <month>Jan</month>;<volume>67</volume>:<fpage>011905</fpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://link.aps.org/doi/10.1103/PhysRevE.67.011905">http://link.aps.org/doi/10.1103/PhysRevE.67.011905</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevE.67.011905" xlink:type="simple">10.1103/PhysRevE.67.011905</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Graupner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>Calcium-based plasticity model explains sensitivity of synaptic changes to spike pattern, rate, and dendritic location</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2012</year>;Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.pnas.org/content/early/2012/02/21/1109359109.abstract">http://www.pnas.org/content/early/2012/02/21/1109359109.abstract</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1109359109" xlink:type="simple">10.1073/pnas.1109359109</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Yuille</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Grzywacz</surname> <given-names>NM</given-names></name>. <article-title>A winner-take-all mechanism based on presynaptic inhibition feedback</article-title>. <source>Neural Comput</source>. <year>1989</year>;<volume>1</volume>(<issue>3</issue>):<fpage>334</fpage>–<lpage>347</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1989.1.3.334" xlink:type="simple">10.1162/neco.1989.1.3.334</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rabinovich</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tristan</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Varona</surname> <given-names>P</given-names></name>. <article-title>Neural dynamics of attentional cross-modality control</article-title>. <source>PloS one</source>. <year>2013</year>;<volume>8</volume>(<issue>5</issue>):<fpage>e64406</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0064406" xlink:type="simple">10.1371/journal.pone.0064406</ext-link></comment> <object-id pub-id-type="pmid">23696890</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Simmons</surname> <given-names>AN</given-names></name>, <name name-style="western"><surname>Varona</surname> <given-names>P</given-names></name>. <article-title>Dynamical bridge between brain and mind</article-title>. <source>Trends in cognitive sciences</source>. <year>2015</year>;<volume>19</volume>(<issue>8</issue>):<fpage>453</fpage>–<lpage>461</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2015.06.005" xlink:type="simple">10.1016/j.tics.2015.06.005</ext-link></comment> <object-id pub-id-type="pmid">26149511</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rabinovich</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Huerta</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Volkovskii</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Abarbanel</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Stopfer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Laurent</surname> <given-names>G</given-names></name>. <article-title>Dynamical coding of sensory information with competitive networks</article-title>. <source>Journal of Physiology-Paris</source>. <year>2000</year>;<volume>94</volume>(<issue>5</issue>):<fpage>465</fpage>–<lpage>471</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0928-4257(00)01092-5" xlink:type="simple">10.1016/S0928-4257(00)01092-5</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Sokolov</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kozma</surname> <given-names>R</given-names></name>. <article-title>Robust sequential working memory recall in heterogeneous cognitive networks</article-title>. <source>Frontiers in systems neuroscience</source>. <year>2014</year>;<volume>8</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnsys.2014.00220" xlink:type="simple">10.3389/fnsys.2014.00220</ext-link></comment> <object-id pub-id-type="pmid">25452717</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>SB</given-names></name>. <article-title>Synaptic plasticity: taming the beast</article-title>. <source>Nature Neuroscience</source>. <year>2000</year> <month>November</month>;<volume>3</volume>:<fpage>1178</fpage>–<lpage>1183</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/81453" xlink:type="simple">10.1038/81453</ext-link></comment> <object-id pub-id-type="pmid">11127835</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Keck</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Savin</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lücke</surname> <given-names>J</given-names></name>. <article-title>Feedforward Inhibition and Synaptic Scaling–Two Sides of the Same Coin?</article-title> <source>PLoS computational biology</source>. <year>2012</year>;<volume>8</volume>(<issue>3</issue>):<fpage>e1002432</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002432" xlink:type="simple">10.1371/journal.pcbi.1002432</ext-link></comment> <object-id pub-id-type="pmid">22457610</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Afraimovich</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tristan</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Huerta</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>. <article-title>Winnerless competition principle and prediction of the transient dynamics in a Lotka–Volterra model</article-title>. <source>Chaos: An Interdisciplinary Journal of Nonlinear Science</source>. <year>2008</year>;<volume>18</volume>(<issue>4</issue>):<fpage>043103</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1063/1.2991108" xlink:type="simple">10.1063/1.2991108</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Zeeman</surname> <given-names>ML</given-names></name>. <article-title>Hopf bifurcations in competitive three-dimensional Lotka–Volterra systems</article-title>. <source>Dynamics and Stability of Systems</source>. <year>1990</year>;<volume>8</volume>(<issue>3</issue>):<fpage>189</fpage>–<lpage>216</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/02681119308806158" xlink:type="simple">10.1080/02681119308806158</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Afraimovich</surname> <given-names>VS</given-names></name>, <name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Varona</surname> <given-names>P</given-names></name>. <article-title>Heteroclinic contours in neural ensembles and the winnerless competition principle</article-title>. <source>International Journal of Bifurcation and Chaos</source>. <year>2004</year>;<volume>14</volume>(<issue>04</issue>):<fpage>1195</fpage>–<lpage>1208</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1142/S0218127404009806" xlink:type="simple">10.1142/S0218127404009806</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fiete</surname> <given-names>IR</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>CZ</given-names></name>, <name name-style="western"><surname>Hahnloser</surname> <given-names>RH</given-names></name>. <article-title>Spike-time-dependent plasticity and heterosynaptic competition organize networks to produce long scale-free sequences of neural activity</article-title>. <source>Neuron</source>. <year>2010</year>;<volume>65</volume>(<issue>4</issue>):<fpage>563</fpage>–<lpage>576</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2010.02.003" xlink:type="simple">10.1016/j.neuron.2010.02.003</ext-link></comment> <object-id pub-id-type="pmid">20188660</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Acuna</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Wymbs</surname> <given-names>NF</given-names></name>, <name name-style="western"><surname>Reynolds</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Picard</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Turner</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Strick</surname> <given-names>PL</given-names></name>, <etal>et al</etal>. <article-title>Multifaceted aspects of chunking enable robust algorithms</article-title>. <source>Journal of neurophysiology</source>. <year>2014</year>;<volume>112</volume>(<issue>8</issue>):<fpage>1849</fpage>–<lpage>1856</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00028.2014" xlink:type="simple">10.1152/jn.00028.2014</ext-link></comment> <object-id pub-id-type="pmid">25080566</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tremblay</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Bedard</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Levesque</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Chebli</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Parent</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Courtemanche</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Motor sequence learning in primate: Role of the {D2} receptor in movement chunking during consolidation</article-title>. <source>Behavioural Brain Research</source>. <year>2009</year>;<volume>198</volume>(<issue>1</issue>):<fpage>231</fpage>–<lpage>239</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S0166432808006086">http://www.sciencedirect.com/science/article/pii/S0166432808006086</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.bbr.2008.11.002" xlink:type="simple">10.1016/j.bbr.2008.11.002</ext-link></comment> <object-id pub-id-type="pmid">19041898</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Itti</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>. <article-title>Computational Modeling of Visual Attention</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2001</year>;<volume>2</volume>(<issue>3</issue>):<fpage>194</fpage>–<lpage>203</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/35058500" xlink:type="simple">10.1038/35058500</ext-link></comment> <object-id pub-id-type="pmid">11256080</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Terrace</surname> <given-names>H</given-names></name>. <source>Chunking and serially organized behavior in pigeons, monkeys and humans</source>. <publisher-name>Comparative Cognition Press</publisher-name>) <publisher-loc>Medford, MA</publisher-loc>; <year>2001</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Zellner</surname> <given-names>B</given-names></name>. <chapter-title>Pauses and the temporal structure of speech</chapter-title>. Zellner, B (1994) Pauses and the temporal structure of speech, in <name name-style="western"><surname>Keller</surname> <given-names>E</given-names></name> (Ed) <source>Fundamentals of speech synthesis and speech recognition(pp 41–62)</source> <publisher-loc>Chichester</publisher-loc>: <publisher-name>John Wiley</publisher-name>. 1994;p. <fpage>41</fpage>–<lpage>62</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref051">
<label>51</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Matsuzaka</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Picard</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Strick</surname> <given-names>PL</given-names></name>. <article-title>Skill representation in the primary motor cortex after long-term practice</article-title>. <source>Journal of neurophysiology</source>. <year>2007</year>;<volume>97</volume>(<issue>2</issue>):<fpage>1819</fpage>–<lpage>1832</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00784.2006" xlink:type="simple">10.1152/jn.00784.2006</ext-link></comment> <object-id pub-id-type="pmid">17182912</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref052">
<label>52</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Kelso</surname> <given-names>JS</given-names></name>. <source>Dynamic patterns: The self-organization of brain and behavior</source>. <publisher-name>MIT press</publisher-name>; <year>1997</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref053">
<label>53</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Transients, metastability, and neuronal dynamics</article-title>. <source>Neuroimage</source>. <year>1997</year>;<volume>5</volume>(<issue>2</issue>):<fpage>164</fpage>–<lpage>171</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1006/nimg.1997.0259" xlink:type="simple">10.1006/nimg.1997.0259</ext-link></comment> <object-id pub-id-type="pmid">9345546</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref054">
<label>54</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Oullier</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Kelso</surname> <given-names>J</given-names></name>. <article-title>Neuroeconomics and the metastable brain</article-title>. <source>Trends in cognitive sciences</source>. <year>2006</year>;<volume>10</volume>(<issue>8</issue>):<fpage>353</fpage>–<lpage>354</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2006.06.009" xlink:type="simple">10.1016/j.tics.2006.06.009</ext-link></comment> <object-id pub-id-type="pmid">16828574</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref055">
<label>55</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Verwey</surname> <given-names>WB</given-names></name>, <name name-style="western"><surname>Abrahamse</surname> <given-names>EL</given-names></name>, <name name-style="western"><surname>Jiménez</surname> <given-names>L</given-names></name>. <article-title>Segmentation of short keying sequences does not spontaneously transfer to other sequences</article-title>. <source>Human Movement Science</source>. <year>2009</year>;<volume>28</volume>(<issue>3</issue>):<fpage>348</fpage>–<lpage>361</lpage>. Third European Workshop on Human Movement Science. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S0167945708001012">http://www.sciencedirect.com/science/article/pii/S0167945708001012</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.humov.2008.10.004" xlink:type="simple">10.1016/j.humov.2008.10.004</ext-link></comment> <object-id pub-id-type="pmid">19135276</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref056">
<label>56</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Verwey</surname> <given-names>WB</given-names></name>, <name name-style="western"><surname>Eikelboom</surname> <given-names>T</given-names></name>. <article-title>Evidence for lasting sequence segmentation in the discrete sequence-production task</article-title>. <source>Journal of motor behavior</source>. <year>2003</year>;<volume>35</volume>(<issue>2</issue>):<fpage>171</fpage>–<lpage>181</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/00222890309602131" xlink:type="simple">10.1080/00222890309602131</ext-link></comment> <object-id pub-id-type="pmid">12711587</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref057">
<label>57</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bick</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>. <source>Dynamical Origin of the Effective Storage Capacity in the Brain’s Working Memory</source>. <year>2009</year> <month>Nov</month>;<volume>103</volume>:<fpage>218101–1</fpage>–<lpage>218101–4</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://adsabs.harvard.edu/abs/2009PhRvL.103u8101B">http://adsabs.harvard.edu/abs/2009PhRvL.103u8101B</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref058">
<label>58</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Verduzco-Flores</surname> <given-names>SO</given-names></name>, <name name-style="western"><surname>Bodner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ermentrout</surname> <given-names>B</given-names></name>. <article-title>A model for complex sequence learning and reproduction in neural populations</article-title>. <source>Journal of computational neuroscience</source>. <year>2012</year>;<volume>32</volume>(<issue>3</issue>):<fpage>403</fpage>–<lpage>423</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10827-011-0360-x" xlink:type="simple">10.1007/s10827-011-0360-x</ext-link></comment> <object-id pub-id-type="pmid">21887499</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref059">
<label>59</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tully</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Hennig</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Lansner</surname> <given-names>A</given-names></name>. <article-title>Synaptic and nonsynaptic plasticity approximating probabilistic inference</article-title>. <source>Frontiers in synaptic neuroscience</source>. <year>2014</year>;<volume>6</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnsyn.2014.00008" xlink:type="simple">10.3389/fnsyn.2014.00008</ext-link></comment> <object-id pub-id-type="pmid">24782758</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref060">
<label>60</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rumelhart</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Zipser</surname> <given-names>D</given-names></name>. <article-title>Feature discovery by competitive learning*</article-title>. <source>Cognitive science</source>. <year>1985</year>;<volume>9</volume>(<issue>1</issue>):<fpage>75</fpage>–<lpage>112</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1207/s15516709cog0901_5" xlink:type="simple">10.1207/s15516709cog0901_5</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref061">
<label>61</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Grossberg</surname> <given-names>S</given-names></name>. <article-title>Competitive learning: From interactive activation to adaptive resonance</article-title>. <source>Cognitive science</source>. <year>1987</year>;<volume>11</volume>(<issue>1</issue>):<fpage>23</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1551-6708.1987.tb00862.x" xlink:type="simple">10.1111/j.1551-6708.1987.tb00862.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref062">
<label>62</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Kohonen</surname> <given-names>T</given-names></name>. <chapter-title>Self-Organization and Associative Memory</chapter-title>. <edition>2nd ed</edition>. <source>Springer Series in Information Sciences</source>. <publisher-name>Springer Verlag</publisher-name>; <year>1988</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref063">
<label>63</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kiebel</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Free energy and dendritic self-organization</article-title>. <source>Frontiers in systems neuroscience</source>. <year>2011</year>;<volume>5</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnsys.2011.00080" xlink:type="simple">10.3389/fnsys.2011.00080</ext-link></comment> <object-id pub-id-type="pmid">22013413</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref064">
<label>64</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Yildiz</surname> <given-names>IB</given-names></name>, <name name-style="western"><surname>von Kriegstein</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kiebel</surname> <given-names>SJ</given-names></name>. <article-title>From birdsong to human speech recognition: Bayesian inference on a hierarchy of nonlinear dynamical systems</article-title>. <source>PLoS computational biology</source>. <year>2013</year>;<volume>9</volume>:<fpage>e1003219</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003219" xlink:type="simple">10.1371/journal.pcbi.1003219</ext-link></comment> <object-id pub-id-type="pmid">24068902</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref065">
<label>65</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Liu</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Buonomano</surname> <given-names>DV</given-names></name>. <article-title>Embedding multiple trajectories in simulated recurrent neural networks in a self-organizing manner</article-title>. <source>The Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>42</issue>):<fpage>13172</fpage>–<lpage>13181</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2358-09.2009" xlink:type="simple">10.1523/JNEUROSCI.2358-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19846705</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref066">
<label>66</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Generating coherent patterns of activity from chaotic neural networks</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>63</volume>(<issue>4</issue>):<fpage>544</fpage>–<lpage>557</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.07.018" xlink:type="simple">10.1016/j.neuron.2009.07.018</ext-link></comment> <object-id pub-id-type="pmid">19709635</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref067">
<label>67</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Dominey</surname> <given-names>PF</given-names></name>. <article-title>Recurrent temporal networks and language acquisition-from corticostriatal neurophysiology to reservoir computing</article-title>. <source>Frontiers in psychology</source>. <year>2013</year>;<volume>4</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fpsyg.2013.00500" xlink:type="simple">10.3389/fpsyg.2013.00500</ext-link></comment> <object-id pub-id-type="pmid">23935589</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref068">
<label>68</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Abeles</surname> <given-names>M</given-names></name>. <chapter-title>Local cortical circuits</chapter-title>. <source>An electrophysiological study</source>. <publisher-name>Springer</publisher-name>, <publisher-loc>Berlin</publisher-loc>; <year>1982</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref069">
<label>69</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Jun</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Jin</surname> <given-names>DZ</given-names></name>. <article-title>Development of neural circuitry for precise temporal sequences through spontaneous activity, axon remodeling, and synaptic plasticity</article-title>. <source>PLoS One</source>. <year>2007</year>;<volume>2</volume>(<issue>8</issue>):<fpage>e723</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0000723" xlink:type="simple">10.1371/journal.pone.0000723</ext-link></comment> <object-id pub-id-type="pmid">17684568</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref070">
<label>70</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Diesmann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gewaltig</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Aertsen</surname> <given-names>A</given-names></name>. <article-title>Stable propagation of synchronous spiking in cortical neural networks</article-title>. <source>Nature</source>. <year>1999</year> <month>December</month>;<volume>402</volume>:<fpage>529</fpage>–<lpage>533</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/990101" xlink:type="simple">10.1038/990101</ext-link></comment> <object-id pub-id-type="pmid">10591212</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref071">
<label>71</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mostafa</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Indiveri</surname> <given-names>G</given-names></name>. <article-title>Sequential Activity in Asymmetrically Coupled Winner-Take-All Circuits</article-title>. <source>Neural Computation</source>. <year>2014</year>;p. <fpage>1</fpage>–<lpage>32</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref072">
<label>72</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Kanter</surname> <given-names>I</given-names></name>. <article-title>Temporal association in asymmetric neural networks</article-title>. <source>Physical Review Letters</source>. <year>1986</year>;<volume>57</volume>(<issue>22</issue>):<fpage>2861</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.57.2861" xlink:type="simple">10.1103/PhysRevLett.57.2861</ext-link></comment> <object-id pub-id-type="pmid">10033885</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref073">
<label>73</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sandamirskaya</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schöner</surname> <given-names>G</given-names></name>. <article-title>An embodied account of serial order: How instabilities drive sequence generation</article-title>. <source>Neural Networks</source>. <year>2010</year>;<volume>23</volume>(<issue>10</issue>):<fpage>1164</fpage>–<lpage>1179</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2010.07.012" xlink:type="simple">10.1016/j.neunet.2010.07.012</ext-link></comment> <object-id pub-id-type="pmid">20800989</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref074">
<label>74</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pascanu</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Jaeger</surname> <given-names>H</given-names></name>. <article-title>A neurodynamical model for working memory</article-title>. <source>Neural networks</source>. <year>2011</year>;<volume>24</volume>(<issue>2</issue>):<fpage>199</fpage>–<lpage>207</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2010.10.003" xlink:type="simple">10.1016/j.neunet.2010.10.003</ext-link></comment> <object-id pub-id-type="pmid">21036537</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref075">
<label>75</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>George</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hawkins</surname> <given-names>J</given-names></name>. <article-title>Towards a mathematical theory of cortical micro-circuits</article-title>. <source>PLoS computational biology</source>. <year>2009</year>;<volume>5</volume>(<issue>10</issue>):<fpage>e1000532</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000532" xlink:type="simple">10.1371/journal.pcbi.1000532</ext-link></comment> <object-id pub-id-type="pmid">19816557</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref076">
<label>76</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>. <source>Modeling brain function: The world of attractor neural networks</source>. <publisher-name>Cambridge University Press</publisher-name>; <year>1992</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref077">
<label>77</label>
<mixed-citation xlink:type="simple" publication-type="other">Keogh E, Chu S, Hart D, Pazzani M. An online algorithm for segmenting time series. In: Data Mining, 2001. ICDM 2001, Proceedings IEEE International Conference on. IEEE; 2001. p. 289–296.</mixed-citation>
</ref>
<ref id="pcbi.1004592.ref078">
<label>78</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Berardelli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rothwell</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Hallett</surname> <given-names>M</given-names></name>. <article-title>Pathophysiology of bradykinesia in Parkinson’s disease</article-title>. <source>Brain</source>. <year>2001</year>;<volume>124</volume>(<issue>11</issue>):<fpage>2131</fpage>–<lpage>2146</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/brain/124.11.2131" xlink:type="simple">10.1093/brain/124.11.2131</ext-link></comment> <object-id pub-id-type="pmid">11673316</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref079">
<label>79</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Verwey</surname> <given-names>WB</given-names></name>. <article-title>Diminished motor skill development in elderly: indications for limited motor chunk use</article-title>. <source>Acta psychologica</source>. <year>2010</year>;<volume>134</volume>(<issue>2</issue>):<fpage>206</fpage>–<lpage>214</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.actpsy.2010.02.001" xlink:type="simple">10.1016/j.actpsy.2010.02.001</ext-link></comment> <object-id pub-id-type="pmid">20189547</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref080">
<label>80</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Muezzinoglu</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Strigo</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bystritsky</surname> <given-names>A</given-names></name>. <article-title>Dynamical principles of emotion-cognition interaction: mathematical images of mental disorders</article-title>. <source>PloS one</source>. <year>2010</year>;<volume>5</volume>(<issue>9</issue>):<fpage>e12547</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0012547" xlink:type="simple">10.1371/journal.pone.0012547</ext-link></comment> <object-id pub-id-type="pmid">20877723</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref081">
<label>81</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rabinovich</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Varona</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Selverston</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Abarbanel</surname> <given-names>H</given-names></name>. <article-title>Dynamical principles in neuroscience</article-title>. <source>Reviews of modern physics</source>. <year>2006</year>;<volume>78</volume>(<issue>4</issue>):<fpage>1213</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/RevModPhys.78.1213" xlink:type="simple">10.1103/RevModPhys.78.1213</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref082">
<label>82</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Varona</surname> <given-names>P</given-names></name>. <article-title>Robust transient dynamics and brain functions</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2011</year>;<volume>5</volume>(<issue>24</issue>). Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.frontiersin.org/computational_neuroscience/10.3389/fncom.2011.00024/abstract">http://www.frontiersin.org/computational_neuroscience/10.3389/fncom.2011.00024/abstract</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2011.00024" xlink:type="simple">10.3389/fncom.2011.00024</ext-link></comment> <object-id pub-id-type="pmid">21716642</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004592.ref083">
<label>83</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Levenshtein</surname> <given-names>VI</given-names></name>. <article-title>Binary codes capable of correcting deletions, insertions, and reversals</article-title>. In: <source>Soviet physics doklady</source>. <volume>vol. 10</volume>; <year>1966</year>. p. <fpage>707</fpage>–<lpage>710</lpage>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>