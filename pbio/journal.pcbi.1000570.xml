<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">09-PLCB-RA-0920R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000570</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology/Systems Biology</subject><subject>Mathematics</subject><subject>Mathematics/Statistics</subject></subj-group></article-categories><title-group><article-title>A Novel Extended Granger Causal Model Approach Demonstrates Brain Hemispheric Differences during Face Recognition Learning</article-title><alt-title alt-title-type="running-head">Brain Asymmetry and Causality Analysis</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Ge</surname><given-names>Tian</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Kendrick</surname><given-names>Keith M.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Feng</surname><given-names>Jianfeng</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Centre for Computational Systems Biology, Fudan University, Shanghai, People's Republic of China</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Cognitive and Systems Neuroscience Group, The Babraham Institute, Cambridge, United Kingdom</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Centre for Scientific Computing, University of Warwick, Coventry, United Kingdom</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">jianfeng64@gmail.com</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: KMK. Performed the experiments: KMK. Analyzed the data: TG. Wrote the paper: TG KMK JF.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>11</month><year>2009</year></pub-date><pub-date pub-type="epub"><day>20</day><month>11</month><year>2009</year></pub-date><volume>5</volume><issue>11</issue><elocation-id>e1000570</elocation-id><history>
<date date-type="received"><day>3</day><month>8</month><year>2009</year></date>
<date date-type="accepted"><day>19</day><month>10</month><year>2009</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2009</copyright-year><copyright-holder>Ge et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>Two main approaches in exploring causal relationships in biological systems using time-series data are the application of Dynamic Causal model (DCM) and Granger Causal model (GCM). These have been extensively applied to brain imaging data and are also readily applicable to a wide range of temporal changes involving genes, proteins or metabolic pathways. However, these two approaches have always been considered to be radically different from each other and therefore used independently. Here we present a novel approach which is an extension of Granger Causal model and also shares the features of the bilinear approximation of Dynamic Causal model. We have first tested the efficacy of the extended GCM by applying it extensively in toy models in both time and frequency domains and then applied it to local field potential recording data collected from <italic>in vivo</italic> multi-electrode array experiments. We demonstrate face discrimination learning-induced changes in inter- and intra-hemispheric connectivity and in the hemispheric predominance of theta and gamma frequency oscillations in sheep inferotemporal cortex. The results provide the first evidence for connectivity changes between and within left and right inferotemporal cortexes as a result of face recognition learning.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>The right temporal cortex has previously been shown to play a greater role in the discrimination of faces in both sheep and humans. In the frequency domain, analysis of the relative causal contributions of low (theta 4–8Hz) and high (gamma 30–70Hz) frequency oscillations reveals that prior to learning, theta activity is more predominant in right than in left hemisphere processing, and that learning reduces this so that high frequency oscillations gain more control. We have been able to demonstrate that the frequency of connections increases in the right hemisphere and decreases between the left and right hemispheres after learning. The results are obtained based upon a way to combine aspects of both the Granger and Dynamic Causal Models, which can be used to establish significant causal relations in both time and frequency domains and applied to local field potential recordings from multiple (64 channel) electrodes implanted in the inferotemporal cortex of both sides of the brain in sheep in order to establish changes in causal connections within and between the two hemispheres as a result of learning to discriminate visually between pairs of faces. It is anticipated that this new approach to the measurement of causality will not only help reveal how the two brain hemispheres interact, but will also be applicable to many different types of biological data where variations in both frequency and temporal domains can be measured.</p>
</abstract><funding-group><funding-statement>JF was partially supported by grants from UK EPSRC CARMAN and an EU grant BION. KMK was supported by the UK BBSRC. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="13"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>In order to exploit the full potential of high-throughput data in biology we have to be able to convert them into the most appropriate framework for contributing to knowledge about how the biological system generating them is functioning. This process is best conceptualized as first building a nodal network derived from empirically derived knowledge of the biological structures and molecules involved (nodes) and then secondly to use computational-based steps to discover the nature, dynamics and directionality of connections (directed edges) between them.</p>
<p>Causality analysis based upon experimental data has become one of the most powerful and valuable tools in discovering connections between different elements in complex biological systems <xref ref-type="bibr" rid="pcbi.1000570-Cantone1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1000570-Seth2">[6]</xref>. However, despite some encouraging successes in various areas in systems or computational biology its development and application have been impeded by a number of issues about the meaning of causality. For example, in clinical science, the current emphasis on how to apply causality approaches mainly resides in resolving the problem of how clearly to define causality itself <xref ref-type="bibr" rid="pcbi.1000570-Simpson1">[7]</xref>. A typical problem cited is the so called “Simpson paradox” in which the successes of groups seem reversed when the groups are combined. This demonstrates the ambiguity that can result in determining causal relationships based only on frequency data. However, this issue disappears if we incorporate time into the definition of causality as Granger has done in the field of Economics <xref ref-type="bibr" rid="pcbi.1000570-Pearl1">[8]</xref>. Nevertheless there is still no accepted unified way to tackle this issue. Taking altered gene expression data using microarray analysis as an instance, there are three approaches one can use to deal with the time-series data obtained: the simple dynamical system approach, the dynamical Bayesian network approach and the Granger causality approach which is a generalization of the dynamical system one. In <xref ref-type="bibr" rid="pcbi.1000570-Zou2">[9]</xref>, we have discussed in detail the pros and cons of applying the latter two approaches and shown potential advantages in using Granger when sufficient repeated measurements are available. With brain activity data from functional magnetic resonance imaging (fMRI) experiments, two prominent techniques have been introduced to address temporal dependencies and directed causal influences: Dynamic Causal (DCM) and Granger Causal (GCM) models. These two models have always been considered to differ radically from each other <xref ref-type="bibr" rid="pcbi.1000570-Friston1">[10]</xref>,<xref ref-type="bibr" rid="pcbi.1000570-David1">[11]</xref>. DCM establishes state variables in the observed data and is believed to be a causal model in a true sense. On the other hand, GCM is a phenomenological model which just tests statistical dependencies among the observations to determine how the data may be caused <xref ref-type="bibr" rid="pcbi.1000570-Friston1">[10]</xref>–<xref ref-type="bibr" rid="pcbi.1000570-Friston2">[12]</xref>. The importance of the two approaches in interpreting fMRI data is demonstrated in <xref ref-type="bibr" rid="pcbi.1000570-Friston1">[10]</xref> and in 2008 there were around 450 papers published devoted to both approaches and excluding those relating to other types of biological data.</p>
<p>The key question we want to address in the current paper is whether we can develop an extended and biophysical constraint approach to share the features of the various approaches mentioned above, and in particular of the two causal models: DCM and GCM? The significance of such an approach is obvious and we would expect that its application could represent a powerful new tool in systems and computational biology, particularly in association with increasingly powerful genomic, proteomic and metabolic methodologies allowing time-series measurements of large numbers of putatively interacting molecules.</p>
<p>In this paper, we will show that GCM can be extended to a more biophysical constraint model by incorporating some features of the bilinear approximation of DCM. By setting up a conventional VAR model with additional deterministic inputs and observation variables, we can create a more general model: Extended Granger Causal Model (EGCM) which offers a new way to establish connectivity.</p>
<p>The EGCM is first tested in two toy models. With both state and observation variables, the interactions between nodes are successfully recovered using an extended Kalman filter approach and partial Granger to establish causality in both time (DCM) and frequency (GCM) domains respectively. The GCM approach itself is not tailored particularly well for biological experiments where we are often faced with the case of the data being recorded with and without a stimulus present. The time gap between two adjacent stimuli is very short and we would expect the network structure to remain unchanged during the whole experiment although the form and the intensity of the input may be unknown. This scenario is also the case for the gene network data considered in <xref ref-type="bibr" rid="pcbi.1000570-Cantone1">[1]</xref>,<xref ref-type="bibr" rid="pcbi.1000570-Camacho1">[2]</xref>, where the authors have treated the two situations separately, although there should be a common and true structure for both. We have therefore also used EGCM in toy models to establish its efficacy in revealing the true network structure when there is an intermittent input to affect state variables.</p>
<p>To exemplify the direct application of EGCM to establishing causality in a specific biological system, we have applied it to local field potential (LFP) data recorded in the sheep inferotemporal cortex (IT) of both left and right hemispheres before and after they learn a visual face discrimination task <xref ref-type="bibr" rid="pcbi.1000570-Kendrick1">[13]</xref>. There is electrophysiological, molecular neuroanatomical and behavioral evidence for asymmetrical processing of faces in the sheep brain similar to humans <xref ref-type="bibr" rid="pcbi.1000570-Peirce1">[14]</xref>–<xref ref-type="bibr" rid="pcbi.1000570-Tate1">[16]</xref> although cells in both the left and right IT respond selectively to faces <xref ref-type="bibr" rid="pcbi.1000570-Kendrick2">[15]</xref>. Learning also alters both local and population based encoding in sheep IT as well as theta-nested gamma frequency oscillations in both hemispheres and there is greater synchronization of theta across electrodes in the right IT than there is in the left IT <xref ref-type="bibr" rid="pcbi.1000570-Kendrick1">[13]</xref>. There is considerable interest in establishing functional differences between the ways the left and right brain hemispheres interact and process information <xref ref-type="bibr" rid="pcbi.1000570-Shinohara1">[17]</xref>–<xref ref-type="bibr" rid="pcbi.1000570-Turgeon1">[19]</xref>. It has recently been hypothesized that the left hemisphere specializes in controlling routine and tends to focus on local aspects of the stimulus while the right hemisphere specializes in responding to unexpected stimuli and tends to deal with the global environment <xref ref-type="bibr" rid="pcbi.1000570-MacNeilage1">[18]</xref>,<xref ref-type="bibr" rid="pcbi.1000570-Turgeon1">[19]</xref>. Establishing altered causal connections and frequency dependency within and between the two hemispheres during face recognition learning will help test this hypothesis.</p>
</sec><sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Ethics Statement</title>
<p>All animal experiments were performed in strict accordance with the UK 1986 Animals Scientific Procedures Act (including approval by the Babraham Institute Animal Welfare and Ethics Committee) and during them the animals were housed inside in individual pens and able to see and communicate with each other. Food and water were available ad libitum. Post-surgery all animals received both post-operative analgesia treatment to minimize discomfort and antibiotic treatment to prevent any possibility of infection.</p>
</sec><sec id="s2b">
<title>EGCM Model</title>
<p>The traditional and widely used Granger Causal Model takes the form <xref ref-type="bibr" rid="pcbi.1000570-Granger1">[20]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e001" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e002" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e003" xlink:type="simple"/></inline-formula> are coefficient matrices, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e004" xlink:type="simple"/></inline-formula> is the noise, and the model has a vector autoregressive representation with an order up to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e005" xlink:type="simple"/></inline-formula>.</p>
<p>In spite of its successful application, GCM requires the direct observation of the state variables and does not include designed experimental effects in the model which form some of its limitations. Here we extend GCM to a more reasonable and biophysical constraint model by incorporating additional deterministic inputs and observation variables, closely following equations which are the features in the Dynamical Causal Model and its bilinear approximation form <xref ref-type="bibr" rid="pcbi.1000570-Friston1">[10]</xref>. The extended Granger Causal Model takes the form:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e006" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e007" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e008" xlink:type="simple"/></inline-formula> are deterministic inputs, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e009" xlink:type="simple"/></inline-formula> are the observation variables which are the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e010" xlink:type="simple"/></inline-formula> of the state variables, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e011" xlink:type="simple"/></inline-formula> are the coefficients that allow the inputs to modulate the coupling of the state variables, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e012" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e013" xlink:type="simple"/></inline-formula> are intrinsic and observation noise and are mutually independent.</p>
<p>Now, if we can recover the state variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e014" xlink:type="simple"/></inline-formula> from the noise observation variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e015" xlink:type="simple"/></inline-formula>, all the problems can be considered in the framework of the traditional Granger causality. It's clear that the formal difference between EGCM and GCM is that we have included observation variables and deterministic inputs which are assumed to be known and will affect the connection of the state variables as well as the state variables directly. However, EGCM also has a strong connection with Dynamical Causal Model. We refer the readers to <xref ref-type="sec" rid="s4">Discussion</xref> section for a detailed discussion on the importance of this extension, in particular the relationship between Eq. (2) and the Volterra type series expansion <xref ref-type="bibr" rid="pcbi.1000570-Friston1">[10]</xref>.</p>
</sec><sec id="s2c">
<title>EGCM Algorithm</title>
<p>For the extended Granger Causal model Eq. (2), we now introduce an algorithm to estimate the state variables as well as all its parameters which will give us the first inspiration of the connection of the state variables.</p>
<p>Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e016" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e017" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e018" xlink:type="simple"/></inline-formula>.</p>
<p>Then, the VAR(<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e019" xlink:type="simple"/></inline-formula>) model can be reduced to a VAR(1) model which takes the form:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e020" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e021" xlink:type="simple"/></inline-formula> is the parameter vector to be estimated. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e022" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e023" xlink:type="simple"/></inline-formula> are both zero-mean uncorrelated Gaussian noise with covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e024" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e025" xlink:type="simple"/></inline-formula> respectively.</p>
<p>In order to apply the model to real data, we have to estimate both the states and parameters of the model from input variables and noise observations. A widely used method for this dual estimation is extended Kalman filter (EKF) <xref ref-type="bibr" rid="pcbi.1000570-Sun1">[21]</xref>,. Here we recursively approximate the nonlinear system by a linear model and use the traditional Kalman filter for the linearized model.</p>
<p>Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e026" xlink:type="simple"/></inline-formula> then<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e027" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e028" xlink:type="simple"/></inline-formula> is uncorrelated Gaussian noise with covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e029" xlink:type="simple"/></inline-formula>. Define <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e030" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e031" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e032" xlink:type="simple"/></inline-formula>. Then, the EKF algorithm for dual estimation consists of two steps: prediction and updating.</p>
<sec id="s2c1">
<title>Prediction</title>
<p>Given the estimated state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e033" xlink:type="simple"/></inline-formula>, the observation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e034" xlink:type="simple"/></inline-formula> and inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e035" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e036" xlink:type="simple"/></inline-formula>, we predict the state variables and the covariance matrix of prediction error of the system at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e037" xlink:type="simple"/></inline-formula>.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e038" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e039" xlink:type="simple"/></inline-formula> and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e040" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s2c2">
<title>Updating</title>
<p>We use the new observation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e041" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e042" xlink:type="simple"/></inline-formula> to update the state of the system.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e043" xlink:type="simple"/></disp-formula>where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e044" xlink:type="simple"/></disp-formula></p>
</sec></sec><sec id="s2d">
<title>EGCM Definition of Causality</title>
<p>After recovering the state variables using the EGCM algorithm above, we can define the causality with the idea proposed by Granger. The only difference is that, in our EGCM model, two deterministic inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e045" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e046" xlink:type="simple"/></inline-formula> are added to the normal autoregressive representation. Here, we provide the formulation of EGCM causality in both time domain and frequency domains.</p>
</sec><sec id="s2e">
<title>Causality in the Time Domain</title>
<p>For simplicity of notation, here we only formulate EGCM for two time series <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e047" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e048" xlink:type="simple"/></inline-formula>. To generalize them to more general case of multi time series, we refer the readers to <xref ref-type="bibr" rid="pcbi.1000570-Guo1">[23]</xref>,<xref ref-type="bibr" rid="pcbi.1000570-Schelter1">[24]</xref>. Assume that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e049" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e050" xlink:type="simple"/></inline-formula> in our EGCM have the following representation:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e051" xlink:type="simple"/><label>(3)</label></disp-formula>A joint representation in our EGCM that includes the past information of both processes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e052" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e053" xlink:type="simple"/></inline-formula> can be written as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e054" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e055" xlink:type="simple"/></inline-formula> is the maximum number of lagged observations included in the model. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e056" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e057" xlink:type="simple"/></inline-formula> are prediction errors with variance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e058" xlink:type="simple"/></inline-formula> and are uncorrelated over time. Then, according to the causality definition of Granger, if the prediction of one process can be improved by incorporating the past information of the second process, then the second process causes the first process. So, in the extended model here, we define that if the variance of prediction error for the process <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e059" xlink:type="simple"/></inline-formula> is reduced by the inclusion of the past information of the process <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e060" xlink:type="simple"/></inline-formula>, then, a causal relation from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e061" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e062" xlink:type="simple"/></inline-formula> exists. This can be quantified as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e063" xlink:type="simple"/><label>(5)</label></disp-formula>If <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e064" xlink:type="simple"/></inline-formula>, there is no causal influence from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e065" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e066" xlink:type="simple"/></inline-formula> and if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e067" xlink:type="simple"/></inline-formula>, there is. Similarly, we can define the causal influence from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e068" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e069" xlink:type="simple"/></inline-formula> as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e070" xlink:type="simple"/><label>(6)</label></disp-formula></p>
</sec><sec id="s2f">
<title>Causality in the Frequency Domain</title>
<p>Our EGCM also allows a frequency domain decomposition to detect the intrinsic causal influence which provides valuable information.</p>
<p>We define the lag operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e071" xlink:type="simple"/></inline-formula> to be <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e072" xlink:type="simple"/></inline-formula> and assume here that the input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e073" xlink:type="simple"/></inline-formula> is a constant, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e074" xlink:type="simple"/></inline-formula> to avoid the appearance of nonlinearity. Then, the joint representation of both processes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e075" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e076" xlink:type="simple"/></inline-formula> in equation (4) can be expressed as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e077" xlink:type="simple"/><label>(7)</label></disp-formula></p>
<p>Rewrite equation (7) in terms of lag operator, we have:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e078" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e079" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e080" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e081" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e082" xlink:type="simple"/></inline-formula>.</p>
<p>Since what we really care about is the causal relationship caused by the intrinsic connection of the state variables rather than the outside driving force, i.e. the input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e083" xlink:type="simple"/></inline-formula>, after fitting the model (7) and getting the covariance matrix of the prediction error, we just go on with the following model:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e084" xlink:type="simple"/><label>(9)</label></disp-formula>which means that after fitting the EGCM with input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e085" xlink:type="simple"/></inline-formula> to eliminate outside influence, we just focus on the intrinsic causal influence in the frequency domain.</p>
<p>After normalizing equation (9) using the transformation proposed by Geweke <xref ref-type="bibr" rid="pcbi.1000570-Geweke1">[25]</xref>,<xref ref-type="bibr" rid="pcbi.1000570-Geweke2">[26]</xref> to eliminate the cross term in the spectra, we assume that we have the normalized equation in the form:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e086" xlink:type="simple"/><label>(10)</label></disp-formula>Fourier transforming both sides of equation (10) leads to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e087" xlink:type="simple"/><label>(11)</label></disp-formula>Recasting equation (11) into the transfer function format we obtain<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e088" xlink:type="simple"/><label>(12)</label></disp-formula>After proper ensemble averaging we have the spectral matrix<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e089" xlink:type="simple"/><label>(13)</label></disp-formula>where * denotes the complex conjugate and matrix transpose and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e090" xlink:type="simple"/></inline-formula> is the covariance matrix of the prediction errors in equation (11). Hence, we can define the causal influence from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e091" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e092" xlink:type="simple"/></inline-formula> at frequency <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e093" xlink:type="simple"/></inline-formula> as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e094" xlink:type="simple"/><label>(14)</label></disp-formula>Similarly, we can define the causal influence from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e095" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e096" xlink:type="simple"/></inline-formula> at frequency <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e097" xlink:type="simple"/></inline-formula> as well.</p>
<p>Note that although here we just provide the definition of pairwise Granger causality for EGCM, it's obvious that similar methods can be easily applied to the definition of conditional, partial or complex Granger causality in both time and frequency domains <xref ref-type="bibr" rid="pcbi.1000570-Zou2">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1000570-Guo1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1000570-Schelter1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1000570-Guo2">[27]</xref>–<xref ref-type="bibr" rid="pcbi.1000570-Wu1">[30]</xref>. Since the explicit meaning of the parameters in the EGCM (i.e. the intrinsic coupling among state variables, the strength of the inputs to modulate the coupling and the influence of the inputs on the state variables directly), we can also get an idea of the connection of the state variables and how the inputs affect them from the fitted model before we translate it into a single number.</p>
</sec><sec id="s2g">
<title>Methods for LFP Recording Experiment</title>
<sec id="s2g1">
<title>Animals and visual discrimination training</title>
<p>Three female sheep were used (Ovis aries, one Clun Forest and two Dorsets). All experiments were performed in strict accordance with the UK 1986 Animals Scientific Procedures Act. During the experiments the animals were housed inside in individual pens. They were trained initially over several months to perform operant-based face (sheep) or non-face (objects) discrimination tasks with the animals making a choice between two simultaneously presented pictures, one of which was associated with a food reward. During stimulus presentations, animals stood in a holding trolley and indicated their choice of picture by pressing one of two touch panels located in the front of the trolley with their nose. The food reward was delivered automatically to a hopper between the two panels. The life-sized pictures were back projected onto a screen <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e098" xlink:type="simple"/></inline-formula> in front of the animal using a computer data projector. A white fixation spot on a black background was presented constantly in between trials to maintain attention and experimenters waited until the animals viewed this spot before triggering presentation of the image pairs. The stimulus images remained in view until the animal made an operant response (generally around <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e099" xlink:type="simple"/></inline-formula>). In each case, successful learning of a face or object pair required that a performance criterion of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e100" xlink:type="simple"/></inline-formula> correct choices over 40 trials (i.e. 40 pairs) was achieved consistently. By the end of training, animals were normally able to reach the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e101" xlink:type="simple"/></inline-formula> correct criterion after 40–80 trials and maintain this performance. For the current analysis extensive recordings taken during and after learning of novel face pairs were used (2 pairs for Sheep A; 2 pairs for Sheep B – only one of which was successfully learned – and 1 pair for Sheep C). In all cases recordings were made over 20–80 trials during learning and then during 46–170 trials after the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e102" xlink:type="simple"/></inline-formula> correct criterion was reached. Post learning trials ranged from within 5–10 minutes of the end of a learning trial session to 2 months after learning. For the face pairs, Sheep A and B were discriminating between the faces of different unfamiliar sheep faces (face identity discrimination) whereas for Sheep C, discrimination was between calm and stressed face expressions in the same animal (face emotion discrimination). With this latter animal, the calm face was the rewarded stimulus.</p>
</sec><sec id="s2g2">
<title>Electrophysiological recordings and analysis</title>
<p>Following initial behavioral training sheep were implanted under general anesthesia (fluothane) and full aseptic conditions with unilateral (Sheep A-right IT) or bilateral (Sheep B and C) planar 64-electrode arrays (epoxylite coated, etched, tungsten wires with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e103" xlink:type="simple"/></inline-formula> spacing - total array area around <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e104" xlink:type="simple"/></inline-formula> tip diameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e105" xlink:type="simple"/></inline-formula>, electrode impedence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e106" xlink:type="simple"/></inline-formula>) aimed at the IT. Holes (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e107" xlink:type="simple"/></inline-formula> diameter) were trephined in the skull and the dura beneath cut and reflected. The electrode bundles were introduced to a depth of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e108" xlink:type="simple"/></inline-formula> from the brain surface using a stereotaxic micromanipulator and fixed in place with dental acrylic and stainless-steel screws attached to the skull. Two of these screws acted as reference electrodes, one for each array. Electrode depths and placements were calculated with reference to X-rays, as previously described. Electrodes were connected to 34 pin female plugs (2 per array) which were cemented in place on top of the skull (using dental acrylic). Starting 3 weeks later, the electrodes were connected via male plugs and ribbon cables to a 128 channel electrophysiological recording system (Cerebus 128 Data Acquisition System - Cyberkinetics Neurotechnology Systems, USA) and recordings made during performance of the different face and non-face pair operant discrimination tasks. This system allowed simultaneous recordings of both neuronal spike and local event-related (LFP) activity from each electrode. Typically, individual recording sessions lasted around 30 min and for 80–200 individual trials. There was at least a week between individual recording sessions in each animal. The LFPs were sampled at 2kHz and digitized for storage from around 3 seconds prior to the stimulus onset to around 3 seconds after the stimulus onset (stimulus durations were generally <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e109" xlink:type="simple"/></inline-formula>).</p>
<p>For data analysis of the stored signals LFP data contaminated with noise such as from animal chewing food were excluded as were LFPs with unexpectedly high power. For LFPs, offline filtering was applied in the range of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e110" xlink:type="simple"/></inline-formula> and trend was removed before spectral analysis. Any trial having more than 5 points outside the mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e111" xlink:type="simple"/></inline-formula> standard deviation range were discarded before analysis. At the end of the experiments, animals were euthanized with an intravenous injection of sodium pentobarbitone and the brains removed for subsequent histological confirmation of X-rays that array placements were within the IT cortex region.</p>
</sec></sec></sec><sec id="s3">
<title>Results</title>
<p>In order to evaluate the performance of EGCM for the estimation of the state variables as well as the prediction of the parameters, we first applied the method to two toy models.</p>
<sec id="s3a">
<title>Toy Models</title>
<sec id="s3a1">
<title>Toy Model 1</title>
<p>The first toy model we used comes from a traditional VAR model which has been extensively applied in tests of Granger causality <xref ref-type="bibr" rid="pcbi.1000570-Gourvitch1">[31]</xref>. We modified the model by adding two deterministic inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e112" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e113" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e114" xlink:type="simple"/></inline-formula> was assumed to be a constant stimulation, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e115" xlink:type="simple"/></inline-formula> while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e116" xlink:type="simple"/></inline-formula> was assumed to a harmonic oscillator and had the form of a sinusoidal function since biological rhythms are a common phenomenon. Observation variables were also included in the toy model and assumed to be nonlinear functions of the state variables since it's a real challenge to uncover state variables with nonlinear mapping from states to measurements <xref ref-type="bibr" rid="pcbi.1000570-David2">[32]</xref>,<xref ref-type="bibr" rid="pcbi.1000570-Arulampalam1">[33]</xref>. We generated the time series according to the following equations:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e117" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e118" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e119" xlink:type="simple"/></inline-formula> were zero mean uncorrelated Gaussian noise with variance 0.5, 0.8 and 0.6 respectively. Hence, according to the general form of EGCM, in this toy model we have:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e120" xlink:type="simple"/></disp-formula>Inspection of the above equations reveals that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e121" xlink:type="simple"/></inline-formula> is a direct source to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e122" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e123" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e124" xlink:type="simple"/></inline-formula> share a feedback loop. There is no direct connection between the remaining pairs of the state variables.</p>
<p><xref ref-type="fig" rid="pcbi-1000570-g001">Fig. 1A</xref> is an example of the 2000 time-steps of the data and <xref ref-type="fig" rid="pcbi-1000570-g001">Fig. 1B</xref> shows the network structure.</p>
<fig id="pcbi-1000570-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000570.g001</object-id><label>Figure 1</label><caption>
<title>Results on Toy Model 1.</title>
<p>A. Traces of the time series. B. The causal relationships considered in Toy Model 1 between the three state variables. C. The estimated parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e125" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e126" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e127" xlink:type="simple"/></inline-formula> for the simulated data in Toy Model 1. The initial values of the three parameters are all set to 0. The covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e128" xlink:type="simple"/></inline-formula> is first set to decay slowly to achieve faster convergence and then set to decay faster after two hundred time points to ensure a better accuracy. D. Frequency decomposition of all kinds of relationships between the state variables. Significant causal influences are marked by red.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.g001" xlink:type="simple"/></fig>
<p>The observation variables were<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e129" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e130" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e131" xlink:type="simple"/></inline-formula> were zero mean uncorrelated Gaussian noise with variance 0.1 and also uncorrelated with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e132" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e133" xlink:type="simple"/></inline-formula>.</p>
<p>Now, we can apply the method to this toy model, i.e., to estimate all the parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e134" xlink:type="simple"/></inline-formula> and state variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e135" xlink:type="simple"/></inline-formula> from the deterministic input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e136" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e137" xlink:type="simple"/></inline-formula> and noise observations. Simulations were performed for 2 seconds (2000 equally spaced time points). <xref ref-type="fig" rid="pcbi-1000570-g001">Fig. 1C</xref> shows that the parameters converged to their true values with only small fluctuations after several hundred data points, even though no prior knowledge was included and the initial values of the parameters were assigned to zeros. It has already been pointed out that the covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e138" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s2">Methods</xref> section) of the noise in the parameter equation will affect the convergence rate and tracking performance <xref ref-type="bibr" rid="pcbi.1000570-Nelson1">[34]</xref>. In the situation here, a steep decay of the covariance matrix will lead to a better accuracy but the convergence is then slow. On the other hand, a slow decay will lead to a faster convergence but a larger fluctuation is observed. Hence, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e139" xlink:type="simple"/></inline-formula> was carefully controlled to reduce to zero as the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e140" xlink:type="simple"/></inline-formula> increased (see <xref ref-type="fig" rid="pcbi-1000570-g001">Fig. 1C</xref>).</p>
<p>After the state variables being recovered, we computed the partial Granger causality in both time and frequency domains (see <xref ref-type="fig" rid="pcbi-1000570-g001">Fig. 1D</xref>) and used the bootstrap approach to construct confidence intervals. Specifically, we simulated the fitted model to generate a data set of 1000 realizations of 2000 time points and use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e141" xlink:type="simple"/></inline-formula> as the confidence interval. In this result, a causal connection was illustrated as part of the network if, and only if, the lower bound of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e142" xlink:type="simple"/></inline-formula> confidence interval of the causality was greater than zero. The results show that our extended model can detect the causal relationship correctly in both time and frequency domains.</p>
</sec><sec id="s3a2">
<title>Toy Model 2</title>
<p>When dealing with real data it is quite common that we need to detect the causal influence between time series from several variables affected by some stimulus. The stimulus may be very complicated, or hard to measure, and it may be impossible to formulate its form explicitly. However, if we ignore the influence of these inputs and use a traditional VAR model to detect the causality it is quite probable that we will get a misleading structure even if we use a high-order VAR model.</p>
<p>We used the following toy model which has exactly the same connection coefficients between the three state variables considered in Toy model 1 with an additional simple constant input function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e143" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e144" xlink:type="simple"/><label>(15)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e145" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e146" xlink:type="simple"/></inline-formula> were zero mean uncorrelated Gaussian noise with variance 0.5, 0.8 and 0.6 respectively.</p>
<p>Here, we assumed that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e147" xlink:type="simple"/></inline-formula> and the observation variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e148" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e149" xlink:type="simple"/></inline-formula> were identical to the state variables with observation noise. The variance of the noise was 0.1. It is obvious that the network structure is the same one as shown in <xref ref-type="fig" rid="pcbi-1000570-g001">Fig. 1B</xref>. However, if we ignore the constant input and just use a VAR model to detect this structure, we obtain the structure shown in <xref ref-type="fig" rid="pcbi-1000570-g002">Fig. 2A</xref> and <xref ref-type="fig" rid="pcbi-1000570-g002">Fig. 2B</xref> with confidence intervals where two additional causal relationships (i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e150" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e151" xlink:type="simple"/></inline-formula>) are presented showing that the real causal influence can no longer be correctly detected. Furthermore, when the input is not taken into consideration, the coefficients of the connection matrix will be meaningless and no longer provide us with the correct estimation of the strength of connection strengths between the state variables. This illustrates why we consider it necessary to incorporate the stimulus into our model although sometimes we don't know its form or intensity.</p>
<fig id="pcbi-1000570-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000570.g002</object-id><label>Figure 2</label><caption>
<title>Results on Toy Model 2.</title>
<p>Network structures with and without stimulus. A. Confidence intervals of all links between units. The data is generated with Eq. (15), but we use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e152" xlink:type="simple"/></inline-formula> (without input) in our algorithms and a traditional VAR(10) model to detect the causal influence. B. The network structure of the state variables corresponding to A. Two additional causal relationships are marked by the dashed line. C. Confidence intervals of all links between units. The data is generated with Eq. (16) where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e153" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e154" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e155" xlink:type="simple"/></inline-formula> are generated with normal distribution (with input). D. The network structure of the state variables corresponding to C.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.g002" xlink:type="simple"/></fig>
<p>With our extended model we can, to some extent, solve the above issue and detect the causal influence correctly amongst those state variables affected by some unknown stimulus intermittently, although our model is originally set up for deterministic inputs.</p>
<p>We next generated a time series of 10000 time points which was composed of 10 segments with equal length, a.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e156" xlink:type="simple"/></inline-formula>. Each segment took the form:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e157" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e158" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e159" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e160" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e161" xlink:type="simple"/></inline-formula> were zero mean uncorrelated Gaussian noise with variance 0.5, 0.8 and 0.6 respectively.</p>
<p>The five segments <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e162" xlink:type="simple"/></inline-formula> were generated according to the above toy model without input, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e163" xlink:type="simple"/></inline-formula>, while the remaining five segments were assumed to include input of random intensity which would also affect the state variables randomly. Specifically, within each segment, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e164" xlink:type="simple"/></inline-formula> was assigned a random value which was generated with the normal distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e165" xlink:type="simple"/></inline-formula>, and the same was the case with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e166" xlink:type="simple"/></inline-formula>: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e167" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e168" xlink:type="simple"/></inline-formula>. Observation variables were still assumed to be identical to the state variables with the variation of observation as 0.1.</p>
<p>Hence, the network structure of the three state variables is still the same as shown in <xref ref-type="fig" rid="pcbi-1000570-g001">Fig. 1B</xref> while each state variable is affected by some input that we don't know the intensity of. <xref ref-type="fig" rid="pcbi-1000570-g002">Fig. 2C</xref> and <xref ref-type="fig" rid="pcbi-1000570-g002">Fig. 2D</xref> show the predicted network structure with confidence intervals using our extended model. The results show that we can still detect the causal influence correctly in this situation.</p>
</sec></sec><sec id="s3b">
<title>LFP from Left and Right Hemisphere</title>
<p>Local field potential data were obtained from 64-channel multielectrode arrays implanted in the right and left inferior temporal cortices of three sheep (one sheep only had electrodes in the right hemisphere) as previously described <xref ref-type="bibr" rid="pcbi.1000570-Kendrick1">[13]</xref>. Recordings were made while the animals were presented with pairs of faces which they were required to discriminate between using an operant response in order to obtain a food reward. In between face pair presentations the animals were presented with a visual fixation stimulus (a white spot on a black screen). Recordings were made during sessions of 20–40 trials where they were either still learning the discrimination or had successfully achieved the learning criterion (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e169" xlink:type="simple"/></inline-formula> correct choice of rewarded face). In both the left and right IT the main oscillatory frequencies recorded are in the theta (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e170" xlink:type="simple"/></inline-formula>) and gamma (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e171" xlink:type="simple"/></inline-formula>) ranges and these two frequencies are coupled (theta phase and gamma amplitude) <xref ref-type="bibr" rid="pcbi.1000570-Kendrick1">[13]</xref>. We have previously shown that learning increases theta amplitude, the ratio of theta to gamma, theta/gamma coherence and the tightness of theta phase <xref ref-type="bibr" rid="pcbi.1000570-Kendrick1">[13]</xref>.</p>
<p>With these experimental data, we can directly use our EGCM to detect the global network for all electrodes in both brain hemispheres. However, due to the size of the network, there are at least a few thousand free parameters to fit. To avoid this issue, we adopt another approach here. For each session we randomly select 3 time series from each region respectively and apply our model to detect the network structure for the six electrodes. This procedure is repeated for 100 times for each session (see <xref ref-type="fig" rid="pcbi-1000570-g003">Fig. 3</xref> for such an example). The visual stimulus to the IT (including feedforward and feedback signals) is impossible to know in the experiment. However, as we have shown above, we are able to make the assumption that the effect of the stimulus can be regarded as a constant input to each electrode. The inclusion of the stimulus signal will certainly make the model more reasonable.</p>
<fig id="pcbi-1000570-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000570.g003</object-id><label>Figure 3</label><caption>
<title>An example of the application of EGCM.</title>
<p>The network detected by EGCM (top-panel) and the corresponding frequency decomposition (bottom-panel) for six randomly selected electrodes. In the frequency decomposition, significant causal influences are marked by red.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.g003" xlink:type="simple"/></fig>
<p>A further problem here is that if we intend to reconstruct the connections for each six electrodes (left and right) before and after the stimulus respectively, we could end up with two different structures for the time series (not shown). This is certainly not the case since the duration of the stimuli is quite short (1–3 seconds) and the connections will not change in such a short time. To recover a reasonable structure of the connection in these areas in the brain, we therefore assume here that the connections in each trial don't change and the time series before and after the stimulus are generated from a unified structure. With the application of our EGCM approach, we can include the intermittent stimulus and obtain a comparatively reliable structure. <xref ref-type="fig" rid="pcbi-1000570-g003">Fig. 3</xref> (top-panel) shows such an example where three electrodes in the left and three in the right are randomly selected and that inter- and intra-hemisphere interactions are detected. <xref ref-type="fig" rid="pcbi-1000570-g003">Fig. 3</xref> (bottom-panel) is the corresponding frequency decomposition of the top panel.</p>
<p>In <xref ref-type="fig" rid="pcbi-1000570-g004">Fig. 4</xref> we show the mean connections within and between the left and right IT calculated using EGCM and as a function of learning. The results clearly demonstrate an asymmetry between the hemispheres. The top-panel is an illustration of the bottom panel which summarizes the results of all experimental data for the two sheep. The most noticeable change is a decrease in the number of connections from the left to the right and an increase in connections within the right but not in the left IT. Indeed there was a strong negative correlation between the number of left to right connections and the number within the right IT for both animals (Sheep B, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e172" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e173" xlink:type="simple"/></inline-formula>); Sheep C, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e174" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e175" xlink:type="simple"/></inline-formula>)). These changes occurred as soon as the learning criterion was successfully achieved in successive blocks of trials (i.e. in as little as 5–10 minutes in the case sheep B where learning was successfully achieved within a specific recording session) and were maintained after 1 month or more post-learning. They were not simply the results of stimulus repetition because in Sheep B where recordings were made in repeated sessions of up to 120 trials but where the learning criterion was not achieved for one of the face pairs there were no connectivity changes observed.</p>
<fig id="pcbi-1000570-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000570.g004</object-id><label>Figure 4</label><caption>
<title>Asymmetry between left and right hemisphere in the time domain.</title>
<p>A. A summary of the results in B, but locations in inferotemporal cortex are not precise, only for illustrative purposes. B. The mean connections from left hemisphere to right hemisphere, right hemisphere to left hemisphere and within both regions with the three bars corresponding to the results before learning (blue bar), after learning (green bar), and one month after learning (purple bar) in Sheep B. Significant changes after t-test are marked by arrows (right to left, all pairs are not significant, as indicated by “none”; within the right hemisphere, all pairs are significant, marked by “all”) . For Sheep C, an additional bar (one week after learning) is added (the third bar). Only significant changes from left to right and within the right hemisphere are indicated by arrows. C. Statistic summaries of results in B.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.g004" xlink:type="simple"/></fig>
<p>One of the advantages of the extended approach is that we have a frequency domain decomposition. Brain rhythms, not surprisingly, have also been intensively investigated in the literature <xref ref-type="bibr" rid="pcbi.1000570-Buzsaki1">[35]</xref>. Here we concentrate on the two main frequency bands: theta band (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e176" xlink:type="simple"/></inline-formula>) and gamma band (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e177" xlink:type="simple"/></inline-formula>) present in our IT recording data and which have been extensively linked to mechanisms of learning and memory <xref ref-type="bibr" rid="pcbi.1000570-Kendrick1">[13]</xref>,<xref ref-type="bibr" rid="pcbi.1000570-Buzsaki1">[35]</xref>. Using the frequency decomposition of our extended model discussed in <xref ref-type="sec" rid="s2">Methods</xref> section, we looked at the following two quantities:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e178" xlink:type="simple"/></disp-formula><xref ref-type="fig" rid="pcbi-1000570-g005">Fig. 5A</xref> shows the mean and maximum ratio integrating the data from all the three sheep in the experiment and at different stages of learning. From this it can be seen that both the mean and maximum ratios in the right hemisphere IT are about double those in the left hemisphere. This clearly indicates that for the right hemisphere, the theta band interaction is more dominant, i.e., the right hemisphere deals more with signals of lower frequency.</p>
<fig id="pcbi-1000570-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000570.g005</object-id><label>Figure 5</label><caption>
<title>Asymmetry in the frequency domain interactions.</title>
<p>A. Mean and maximum ratio using all the three sheep before and after learning. B. Upper panel: Mean and maximum ratio of sheep B (see Experiment subsection in <xref ref-type="sec" rid="s2">Methods</xref> section) before learning, after learning and one month after learning (see <xref ref-type="fig" rid="pcbi-1000570-g004">Fig. 4</xref>). Bottom panel: Mean and maximum ratio of sheep C before learning (the first bar), immediately after learning (the second bar), one week after learning and one month after learning (the third and the fourth bar). C. Summaries of results in B.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.g005" xlink:type="simple"/></fig>
<p>In order to provide a deeper insight into this frequency story, we compute the ratios at different stages of learning. <xref ref-type="fig" rid="pcbi-1000570-g005">Fig. 5B</xref> shows the mean and maximum ratio in two sheep before learning, after learning and a month after learning. An additional set of data is one week after learning for sheep C only. The most noticeable change is the reduction in the interactions in the theta band (low frequencies) in the right IT which occurs after learning and is maintained subsequently. Combining <xref ref-type="fig" rid="pcbi-1000570-g004">Fig. 4C</xref> (right) with <xref ref-type="fig" rid="pcbi-1000570-g005">Fig. 5C</xref>, we see that learning in general changes the connections in the right hemisphere (increasing), however, the increasing interactions are mainly due to the enhancement of the interaction at the high frequency domain.</p>
</sec></sec><sec id="s4">
<title>Discussion</title>
<sec id="s4a">
<title/>
<sec id="s4a1">
<title>Comparing EGCM, GCM and DCM</title>
<p>EGCM has a strong connection with DCM as well as GCM. We consider the Dynamical Causal model:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e179" xlink:type="simple"/><label>(17)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e180" xlink:type="simple"/></inline-formula> are state variables and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e181" xlink:type="simple"/></inline-formula> is the transpose of a vector, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e182" xlink:type="simple"/></inline-formula> is a known deterministic input corresponding to designed experimental effects, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e183" xlink:type="simple"/></inline-formula> is the set of parameters to estimate, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e184" xlink:type="simple"/></inline-formula> is the diffusion matrix (could depend on time) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e185" xlink:type="simple"/></inline-formula> is the Brownian motion (or in general, it could be a martingale). The state variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e186" xlink:type="simple"/></inline-formula> enter a specific model to produce the outputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e187" xlink:type="simple"/></inline-formula> with the observation noise <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e188" xlink:type="simple"/></inline-formula>.</p>
<p>Here we focus on the bilinear approximation of the Dynamical Causal model which is the most parsimonious but useful form <xref ref-type="bibr" rid="pcbi.1000570-Friston1">[10]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e189" xlink:type="simple"/><label>(18)</label></disp-formula>where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e190" xlink:type="simple"/></disp-formula>are parameters that mediate the intrinsic coupling among states, allow the inputs to modulate the coupling, and elicit the influence of extrinsic inputs on the states respectively. Here, for simplicity, we have expanded the state equation around <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e191" xlink:type="simple"/></inline-formula> and assumed that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e192" xlink:type="simple"/></inline-formula>.</p>
<p>The bilinear approximation of DCM is represented in terms of nonlinear differential equations while the GCM (see Eq. (1)) is formulated in discrete time and the dependencies among state variables are approximated by a linear mapping over time-lags which seems to be quite different. However, we can find the difference is that the bilinear form includes deterministic inputs and observation variables and equations which are not considered in GCM. The formulation (18) comes from the Volterra series and is certainly a more accurate and biophysical constraint representation of a biological system. On the other hand, the GCM with autoregressive representation always takes the past information into consideration while the bilinear approximation of DCM has no time-lags included in the differential equations although the general form of DCM may have <xref ref-type="bibr" rid="pcbi.1000570-David3">[36]</xref>. So, if we alter the DCM to the form:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e193" xlink:type="simple"/><label>(19)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e194" xlink:type="simple"/></inline-formula> is a kernel function, then the DCM shares the feature of the GCM. On the other hand, our EGCM includes both deterministic inputs and observation variables thus takes the advantages of both DCM and GCM. In the general form of EGCM (Eq. (2)), we can find that when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e195" xlink:type="simple"/></inline-formula>, this is the discrete form of the bilinear form of DCM, and when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000570.e196" xlink:type="simple"/></inline-formula>, it is the GCM with additional inputs.</p>
</sec><sec id="s4a2">
<title>Advantage of extended approach</title>
<p>In contrast to all previous methods in estimating Granger causality in the literature where essentially a regression method is employed, in EGCM we incorporate noise observation variables and apply the extended Kalman filter to recover the state variables. Additional inputs are also included in EGCM on the basis of an autoregressive model. The advantage of such an approach over the previous methods is obvious. The EGCM is more reasonable when we are faced with experimental data affected by a particular stimulus and applicable to cases where we cannot track the state variables respectively but just a function of them, or where the observation noise is considerable. Comparing to the traditional VAR models, all the coefficients in EGCM correspond to intrinsic or latent dynamic coupling and changes induced by each input which endow the model with interpretability power. Furthermore, all the previous methods in estimating Granger causality are batch learning: they require collection of all data before an estimation can be made. The extended Kalman filter, on the other hand, is an online learning: we can now update Granger causality instantaneously. One may argue that this is a common feature of online learning vs. batch learning. However, it is novel in the context of Granger causality. When we are faced with biological data, this feature becomes particularly significant. As we know, adaptation, or learning in animals, is very important but this makes it difficult to analyze since adaptation introduces dynamic change into the system. The classical way of estimating Granger causality can cope with this difficulty by introducing sliding windows in analyzing data. Of course, to select an optimal window size is always an issue in such an approach. However, in Kalman filtering, we can have the advantage of the connection of the state variables from the connection matrix and such an issue is automatically resolved.</p>
<p>In comparison with the bilinear approximation of DCM, the advantages of EGCM are the following: First, it allows time delay in the model more naturally and easier to deal with. Time delay is ubiquitous in a biological system, no matter whether we are considering gene, protein, metabolic and neuronal networks. Secondly, using Granger causality we are able to summarize the causal effect into a single number which is more transparent and easy to understand, particularly in a system with a time delay. Thirdly, it allows a frequency domain decomposition. We know that when we are dealing with a dynamic system it is sometimes much informative to view it in the frequency domain rather than in the time domain, as we have partly demonstrated here. Of course, since Eq. (19) is a continuous time version of Eq. (2), the results in the frequency domain obtained for Eq. (2) is essentially for the DCM model as well. We summarize our comparisons in <xref ref-type="table" rid="pcbi-1000570-t001">Table 1</xref> (see <xref ref-type="bibr" rid="pcbi.1000570-Friston1">[10]</xref>).</p>
<table-wrap id="pcbi-1000570-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000570.t001</object-id><label>Table 1</label><caption>
<title>Comparing DCM, GCM and EGCM.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000570-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000570.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Commonalities</td>
<td align="left" colspan="1" rowspan="1">DCM</td>
<td align="left" colspan="1" rowspan="1">GCM</td>
<td align="left" colspan="1" rowspan="1">EGCM</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Multivariate analysis of time-series data</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Models directed coupling</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Inference on models</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Frequency decomposition</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
</tr>
</tbody>
</table><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Differences</td>
<td align="left" colspan="1" rowspan="1">DCM</td>
<td align="left" colspan="1" rowspan="1">GCM</td>
<td align="left" colspan="1" rowspan="1">EGCM</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Causality based on temporal precedence</td>
<td align="left" colspan="1" rowspan="1">No</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">EGCM is more general</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Causality based on control theory</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">No</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Requires known inputs</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">No</td>
<td align="left" colspan="1" rowspan="1">In general yes, but see example 2</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Requires orthogonal innovations</td>
<td align="left" colspan="1" rowspan="1">No</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">Not necessary</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Requires stationary processes</td>
<td align="left" colspan="1" rowspan="1">No</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">Could use sliding window</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Requires a specific biophysical model</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">No</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Models non-linear coupling</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">No</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Inference on model parameters</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
<td align="left" colspan="1" rowspan="1">No</td>
<td align="left" colspan="1" rowspan="1">Yes</td>
</tr>
</tbody>
</table></alternatives></table-wrap></sec><sec id="s4a3">
<title>Other types of data</title>
<p>In the current paper, we have only applied EGCM to LFP data although it is clearly applicable to many other types of biological data. For example, in gene microarray data, we can have a readout of transcriptional changes in several thousand genes at different times over a period of many hours <xref ref-type="bibr" rid="pcbi.1000570-Feng1">[37]</xref>. The same is the case with multiple protein measurements over time in biological systems or in metabolic changes. In all these situations estimation of altered causal connections in both time and frequency domains will provide invaluable information about changes occurring in the relationships between different components in the systems being studied.</p>
</sec><sec id="s4a4">
<title>IT hemispheric differences and learning</title>
<p>The results of the EGCM analysis of our IT LFP data provide the first evidence for connectivity changes between and within left and right ITs as a result of face recognition learning. It is clear that learning is a dynamic and complex process <xref ref-type="bibr" rid="pcbi.1000570-Sedwick1">[38]</xref>,<xref ref-type="bibr" rid="pcbi.1000570-Robinson1">[39]</xref>. In both sheep during learning there were more causal connections from the left to the right IT than vice-versa during learning trials. However, immediately after learning had occurred the number of left to right connections diminished to the same low level as seen from right to left. Within the hemispheres connectivity increased progressively over time in the right IT after learning but remained the same or decreased in the left IT. There was a strong negative correlation between the number of connections from left to right and the number within the right IT. This suggests that the left to right IT connections may exert some form of inhibitory control over the number within the right IT and that this therefore needs to be weakened for new face discriminations to be learned. The EGCM frequency analysis using theta and gamma oscillation data in the two hemispheres showed that during learning of new face pairs there was significantly more information being processed in the low frequency (theta) in the right IT than in the left. After learning however this declined and it appeared that the higher frequency information (gamma) became more dominant in both hemispheres. Lower frequency oscillations are more associated with global encoding over widespread areas of brain whereas higher frequencies are more associated with more localized encoding. This may suggest that during the course of learning new faces the right IT uses a more global mode of encoding to promote more rapid learning and that once learning has successfully occurred the right IT shifts to a more localized encoding strategy for maintaining learning. In the left IT on the other hand this more local encoding strategy predominates both during and after learning. This is in broad agreement with recent proposals that the left hemisphere is more involved in local encoding and the right in global encoding <xref ref-type="bibr" rid="pcbi.1000570-MacNeilage1">[18]</xref>,<xref ref-type="bibr" rid="pcbi.1000570-Turgeon1">[19]</xref> although in the case of face recognition it would appear that the right hemisphere shifts from a global to local encoding strategy once faces have been learned. Clearly more analyses of this kind are required before these differences in left and right brain hemispheres processing and interactions can be fully understood but combining multiple LFP recordings and EGCM will be a powerful future approach.</p>
</sec><sec id="s4a5">
<title>Field-type model</title>
<p>In the current paper, we have not explicitly introduced the spatio-correlation between each variables (electrodes). In other words, we have ignored the geometric relationship of electrodes in the array. This is certainly an over-simplification of the real situation due to the following reasons. First of all, despite the long history of multi-electrode array recordings, <italic>in vivo</italic> recording in, for example, IT is still very rare and difficult. Even we have a reliable recording session, the obtained data set is hard to fully analyze: for example, to reliably sort the spikes <xref ref-type="bibr" rid="pcbi.1000570-Horton1">[40]</xref>. Secondly, assuming we could work out the spatio-temporal model for one animal, it is almost no sense to map the results about the detailed geometrical relationship (electrodes) to another animal. Also, in our experiments, we often face the situation that we have to discard the data from quite a few electrodes. A spatio-temporal (random field) approach as developed in fMRI <xref ref-type="bibr" rid="pcbi.1000570-ValdesSosa1">[41]</xref> is not considered here. However, with the improvement of experimental techniques and the introduction of new techniques (see for example, functional multineuron calcium imaging <xref ref-type="bibr" rid="pcbi.1000570-Namiki1">[42]</xref>), a spatio-temporal model (a random field or a random point field) is cried for and is one of our future research topics. The well developed approach: dynamic expectation maximization <xref ref-type="bibr" rid="pcbi.1000570-Friston3">[43]</xref>, could play a vital role here.</p>
</sec></sec></sec></body>
<back>
<ack>
<p>We thank Prof. Karl Friston for his helpful discussions and advice which helped us improve our paper considerably.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1000570-Cantone1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cantone</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Marucci</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Iorio</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Ricci</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Belcastro</surname><given-names>V</given-names></name>
<etal/></person-group>             <year>2009</year>             <article-title>A yeast synthetic network for in vivo assessment of reverse-engineering and modeling approaches.</article-title>             <source>Cell</source>             <volume>137</volume>             <fpage>172</fpage>             <lpage>181</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Camacho1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Camacho</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Collins</surname><given-names>J</given-names></name>
</person-group>             <year>2009</year>             <article-title>Systems biology strikes gold.</article-title>             <source>Cell</source>             <volume>137</volume>             <fpage>24</fpage>             <lpage>26</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Zou1"><label>3</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zou</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Kendrick</surname><given-names>KM</given-names></name>
<name name-style="western"><surname>Feng</surname><given-names>J</given-names></name>
</person-group>             <year>2009</year>             <article-title>The fourth way: Granger causality is better than the three other reverse-engineering approaches.</article-title>             <comment>Cell <ext-link ext-link-type="uri" xlink:href="http://www.cell.com/comments/S0092-8674(09)00156-1" xlink:type="simple">http://www.cell.com/comments/S0092-8674(09)00156-1</ext-link></comment>          </element-citation></ref>
<ref id="pcbi.1000570-Bressler1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bressler</surname><given-names>SL</given-names></name>
<name name-style="western"><surname>Tang</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Sylvester</surname><given-names>CM</given-names></name>
<name name-style="western"><surname>Shulman</surname><given-names>GL</given-names></name>
<name name-style="western"><surname>Corbetta</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Top-Down Control of Human Visual Cortex by Frontal and Parietal Cortex in Anticipatory Visual Spatial Attention.</article-title>             <source>J Neurosci</source>             <volume>28</volume>             <fpage>10056</fpage>             <lpage>10061</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Seth1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seth</surname><given-names>A</given-names></name>
</person-group>             <year>2008</year>             <article-title>Causal networks in simulated neural systems.</article-title>             <source>Cogn Neurodyn</source>             <volume>2</volume>             <fpage>49</fpage>             <lpage>64</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Seth2"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name>
<name name-style="western"><surname>Edelman</surname><given-names>GM</given-names></name>
</person-group>             <year>2007</year>             <article-title>Distinguishing causal interactions in neural populations.</article-title>             <source>Neural Computation</source>             <volume>19</volume>             <fpage>910</fpage>             <lpage>933</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Simpson1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Simpson</surname><given-names>EH</given-names></name>
</person-group>             <year>1951</year>             <article-title>The interpretation of interaction in contingency tables.</article-title>             <source>Journal of the Royal Statistical Society Series B (Methodological)</source>             <volume>13</volume>             <fpage>238</fpage>             <lpage>241</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Pearl1"><label>8</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pearl</surname><given-names>J</given-names></name>
</person-group>             <year>1998</year>             <source>Causality: Models, Reasoning, and Inference</source>             <publisher-loc>Cambridge, UK</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000570-Zou2"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zou</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Feng</surname><given-names>J</given-names></name>
</person-group>             <year>2009</year>             <article-title>Granger causality vs. dynamic bayesian network inference: a comparative study.</article-title>             <source>BMC Bioinformatics</source>             <volume>10</volume>             <fpage>122</fpage>          </element-citation></ref>
<ref id="pcbi.1000570-Friston1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
</person-group>             <year>2009</year>             <article-title>Causal modelling and brain connectivity in functional magnetic resonance imaging.</article-title>             <source>PLoS Biol</source>             <volume>7</volume>             <fpage>e1000033</fpage>          </element-citation></ref>
<ref id="pcbi.1000570-David1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>David</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Guillemain</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Saillet</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Reyt</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Deransart</surname><given-names>C</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>Identifying neural drivers with functional mri: An electrophysiological validation.</article-title>             <source>PLoS Biol</source>             <volume>6</volume>             <fpage>e315</fpage>          </element-citation></ref>
<ref id="pcbi.1000570-Friston2"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Harrison</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>
</person-group>             <year>2003</year>             <article-title>Dynamic causal modelling.</article-title>             <source>NeuroImage</source>             <volume>19</volume>             <fpage>1273</fpage>             <lpage>1302</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Kendrick1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kendrick</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Zhan</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Fischer</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Nicol</surname><given-names>AU</given-names></name>
<name name-style="western"><surname>Zhang</surname><given-names>X</given-names></name>
<etal/></person-group>             <year>2009</year>             <article-title>Learning alters theta-nested gamma oscillations in inferotemporal cortex.</article-title>             <source>Nature Precedings</source>             <comment>hdl: 10101/ npre. 2009.3151.1</comment>          </element-citation></ref>
<ref id="pcbi.1000570-Peirce1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Peirce</surname><given-names>JW</given-names></name>
<name name-style="western"><surname>Kendrick</surname><given-names>KM</given-names></name>
</person-group>             <year>2002</year>             <article-title>Functional asymmetry in sheep temporal cortex.</article-title>             <source>NeuroReport</source>             <volume>13</volume>             <fpage>2395</fpage>             <lpage>2399</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Kendrick2"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kendrick</surname><given-names>KM</given-names></name>
</person-group>             <year>2006</year>             <article-title>Brain asymmetries for face recognition and emotion control in sheep.</article-title>             <source>Cortex</source>             <volume>42</volume>             <fpage>96</fpage>             <lpage>98</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Tate1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tate</surname><given-names>AJ</given-names></name>
<name name-style="western"><surname>Fischer</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Leigh</surname><given-names>AE</given-names></name>
<name name-style="western"><surname>Kendrick</surname><given-names>KM</given-names></name>
</person-group>             <year>2006</year>             <article-title>Behavioural and neurophysiological evidence for face identity and face emotion processing in animals.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>361</volume>             <fpage>2155</fpage>             <lpage>2172</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Shinohara1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Shinohara</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Hirase</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Watanabe</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Itakura</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Takahashi</surname><given-names>M</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>Left-right asymmetry of the hippocampal synapses with differential subunit allocation of glutamate receptors.</article-title>             <source>Proceedings of the National Academy of Sciences</source>             <volume>105</volume>             <fpage>19498</fpage>             <lpage>19503</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-MacNeilage1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>MacNeilage</surname><given-names>PF</given-names></name>
<name name-style="western"><surname>Rogers</surname><given-names>LJ</given-names></name>
<name name-style="western"><surname>Vallortigara</surname><given-names>G</given-names></name>
</person-group>             <year>2009</year>             <article-title>Evolutionary origins of your right and left brain.</article-title>             <source>Scientific American Magazine</source>             <comment>6/24/09</comment>          </element-citation></ref>
<ref id="pcbi.1000570-Turgeon1"><label>19</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Turgeon</surname><given-names>M</given-names></name>
</person-group>             <year>1994</year>             <source>Right-brain left-brain reflexology: a self-help approach to balancing life energies with color, sound, and pressure point techniques</source>             <publisher-loc>Rochester, VT</publisher-loc>             <publisher-name>Healing Arts Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000570-Granger1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Granger</surname><given-names>CWJ</given-names></name>
</person-group>             <year>1969</year>             <article-title>Investigating causal relations by econometric models and cross-spectral methods.</article-title>             <source>Econometrica</source>             <volume>37</volume>             <fpage>414</fpage>             <lpage>428</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Sun1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sun</surname><given-names>X</given-names></name>
<name name-style="western"><surname>Jin</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Xiong</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Extended kalman filter for estimation of parameters in nonlinear state-space models of biochemical networks.</article-title>             <source>PLoS ONE</source>             <volume>3</volume>             <fpage>e3758</fpage>          </element-citation></ref>
<ref id="pcbi.1000570-Li1"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Li</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Goodall</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Kadirkamanathan</surname><given-names>V</given-names></name>
</person-group>             <year>2004</year>             <article-title>Estimation of parameters in a linear state space model using a rao-blackwellised particle filter.</article-title>             <source>Control Theory and Applications, IEE Proceedings</source>             <volume>151</volume>             <fpage>727</fpage>             <lpage>738</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Guo1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Guo</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Wu</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Ding</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Feng</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>Uncovering interactions in the frequency domain.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <fpage>e1000087</fpage>          </element-citation></ref>
<ref id="pcbi.1000570-Schelter1"><label>24</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schelter</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Winterhalder</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Timmer</surname><given-names>J</given-names></name>
</person-group>             <year>2006</year>             <source>Handbook of time series analysis: recent theoretical developments and applications</source>             <publisher-loc>Weinheim</publisher-loc>             <publisher-name>Wiley-VCH</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000570-Geweke1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Geweke</surname><given-names>J</given-names></name>
</person-group>             <year>1982</year>             <article-title>Measurement of linear dependence and feedback between multiple time series.</article-title>             <source>Journal of the American Statistical Association</source>             <volume>77</volume>             <fpage>304</fpage>             <lpage>313</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Geweke2"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Geweke</surname><given-names>J</given-names></name>
</person-group>             <year>1984</year>             <article-title>Measures of conditional linear dependence and feedback between time series.</article-title>             <source>Journal of the American Statistical Association</source>             <volume>79</volume>             <fpage>907</fpage>             <lpage>915</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Guo2"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Guo</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name>
<name name-style="western"><surname>Kendrick</surname><given-names>KM</given-names></name>
<name name-style="western"><surname>Zhou</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Feng</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>Partial granger causality–eliminating exogenous inputs and latent variables.</article-title>             <source>Journal of Neuroscience Methods</source>             <volume>172</volume>             <fpage>79</fpage>             <lpage>93</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Chen1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chen</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Rangarajan</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Feng</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Ding</surname><given-names>M</given-names></name>
</person-group>             <year>2004</year>             <article-title>Analyzing multiple nonlinear time series with extended granger causality.</article-title>             <source>Physics Letters A</source>             <volume>324</volume>             <fpage>26</fpage>             <lpage>35</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Ladroue1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ladroue</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Guo</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Kendrick</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Feng</surname><given-names>J</given-names></name>
</person-group>             <year>2009</year>             <article-title>Beyond element-wise interactions: Identifying complex interactions in biological processes.</article-title>             <source>PLoS ONE</source>             <volume>4</volume>             <fpage>e6899+</fpage>          </element-citation></ref>
<ref id="pcbi.1000570-Wu1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wu</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Liu</surname><given-names>X</given-names></name>
<name name-style="western"><surname>Feng</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>Detecting causality between different frequencies.</article-title>             <source>Journal of Neuroscience Methods</source>             <volume>167</volume>             <fpage>367</fpage>             <lpage>375</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Gourvitch1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gourvitch</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Bouquin-Jeanns</surname><given-names>RL</given-names></name>
<name name-style="western"><surname>Faucon</surname><given-names>G</given-names></name>
</person-group>             <year>2006</year>             <article-title>Linear and nonlinear causality between signals: methods, examples and neurophysiological applications.</article-title>             <source>Biol Cybern</source>             <volume>95</volume>             <fpage>349</fpage>             <lpage>369</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-David2"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>David</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Guillemain</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Saillet</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Reyt</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Deransart</surname><given-names>C</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>Identifying neural drivers with functional mri: An electrophysiological validation.</article-title>             <source>PLoS Biol</source>             <volume>6</volume>             <fpage>e315</fpage>          </element-citation></ref>
<ref id="pcbi.1000570-Arulampalam1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Arulampalam</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Maskell</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Gordon</surname><given-names>N</given-names></name>
</person-group>             <year>2002</year>             <article-title>A tutorial on particle filters for online nonlinear/non-gaussian bayesian tracking.</article-title>             <source>IEEE Transactions on Signal Processing</source>             <volume>50</volume>             <fpage>174</fpage>             <lpage>188</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Nelson1"><label>34</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nelson</surname><given-names>A</given-names></name>
</person-group>             <year>2007</year>             <article-title>Nonlinear estimation and modeling of noisy time-series by dual Kalman filtering methods.</article-title>             <comment>Ph.D. thesis</comment>          </element-citation></ref>
<ref id="pcbi.1000570-Buzsaki1"><label>35</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Buzsaki</surname><given-names>G</given-names></name>
</person-group>             <year>2006</year>             <source>Rhythms of the Brain</source>             <publisher-loc>USA</publisher-loc>             <publisher-name>Oxford University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000570-David3"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>David</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Kiebel</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Harrison</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Mattout</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Kilner</surname><given-names>J</given-names></name>
<etal/></person-group>             <year>2006</year>             <article-title>Dynamic causal modelling of evoked responses in EEG and MEG.</article-title>             <source>NeuroImage</source>             <volume>30</volume>             <fpage>1255</fpage>             <lpage>1272</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Feng1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Feng</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Yi</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Krishna</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Guo</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Buchanan-Wollaston</surname><given-names>V</given-names></name>
</person-group>             <year>2009</year>             <article-title>Listen to genes: Dealing with microarray data in the frequency domain.</article-title>             <source>PLoS ONE</source>             <volume>4</volume>             <fpage>e5098+</fpage>          </element-citation></ref>
<ref id="pcbi.1000570-Sedwick1"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sedwick</surname><given-names>C</given-names></name>
</person-group>             <year>2009</year>             <article-title>Practice makes perfect: Learning mind control of prosthetics.</article-title>             <source>PLoS Biol</source>             <volume>7</volume>             <fpage>e1000152</fpage>          </element-citation></ref>
<ref id="pcbi.1000570-Robinson1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Robinson</surname><given-names>R</given-names></name>
</person-group>             <year>2009</year>             <article-title>From child to young adult, the brain changes its connections.</article-title>             <source>PLoS Biol</source>             <volume>7</volume>             <fpage>e1000158</fpage>          </element-citation></ref>
<ref id="pcbi.1000570-Horton1"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Horton</surname><given-names>PM</given-names></name>
<name name-style="western"><surname>Nicol</surname><given-names>AU</given-names></name>
<name name-style="western"><surname>Kendrick</surname><given-names>KM</given-names></name>
<name name-style="western"><surname>Feng</surname><given-names>JF</given-names></name>
</person-group>             <year>2007</year>             <article-title>Spike sorting based upon machine learning algorithms (soma).</article-title>             <source>Journal of Neuroscience Methods</source>             <volume>160</volume>             <fpage>52</fpage>             <lpage>68</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-ValdesSosa1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Valdes-Sosa</surname><given-names>PA</given-names></name>
</person-group>             <year>2004</year>             <article-title>Spatio-temporal autoregressive models defined over brain manifolds.</article-title>             <source>Neuroinformatics</source>             <volume>2</volume>             <fpage>239</fpage>             <lpage>250</lpage>          </element-citation></ref>
<ref id="pcbi.1000570-Namiki1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Namiki</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Matsuki</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Ikegaya</surname><given-names>Y</given-names></name>
</person-group>             <year>2009</year>             <article-title>Large-scale imaging of brain network activity from &gt;10,000 neocortical cells.</article-title>             <source>Nature Precedings</source>             <comment>2009.2893.1</comment>          </element-citation></ref>
<ref id="pcbi.1000570-Friston3"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>
<name name-style="western"><surname>Trujillo-Barreto</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>Dem: A variational treatment of dynamic systems.</article-title>             <source>NeuroImage</source>             <volume>41</volume>             <fpage>849</fpage>             <lpage>885</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>