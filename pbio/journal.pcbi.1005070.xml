<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-02051</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005070</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neuronal plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Optimization</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Electronics</subject><subj-group><subject>Rectifiers</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Single neuron function</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Single neuron function</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Developmental neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Nonlinear Hebbian Learning as a Unifying Principle in Receptive Field Formation</article-title>
<alt-title alt-title-type="running-head">Nonlinear Hebbian Learning Unifies Receptive Field Formation</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6694-8457</contrib-id>
<name name-style="western">
<surname>Brito</surname> <given-names>Carlos S. N.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Gerstner</surname> <given-names>Wulfram</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>School of Computer and Communication Sciences and School of Life Science, Brain Mind Institute, Ecole Polytechnique Federale de Lausanne, Lausanne EPFL, Switzerland</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Beck</surname> <given-names>Jeff</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Duke University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"><list-item><p><bold>Conceived and designed the experiments:</bold> CSNB WG.</p></list-item>
<list-item><p><bold>Performed the experiments:</bold> CSNB.</p></list-item>
<list-item><p><bold>Analyzed the data:</bold> CSNB.</p></list-item>
<list-item><p><bold>Wrote the paper:</bold> CSNB WG.</p></list-item></list></p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">c.brito@ucl.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>9</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>30</day>
<month>9</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>9</issue>
<elocation-id>e1005070</elocation-id>
<history>
<date date-type="received">
<day>7</day>
<month>12</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>19</day>
<month>7</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Brito, Gerstner</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005070"/>
<abstract>
<p>The development of sensory receptive fields has been modeled in the past by a variety of models including normative models such as sparse coding or independent component analysis and bottom-up models such as spike-timing dependent plasticity or the Bienenstock-Cooper-Munro model of synaptic plasticity. Here we show that the above variety of approaches can all be unified into a single common principle, namely nonlinear Hebbian learning. When nonlinear Hebbian learning is applied to natural images, receptive field shapes were strongly constrained by the input statistics and preprocessing, but exhibited only modest variation across different choices of nonlinearities in neuron models or synaptic plasticity rules. Neither overcompleteness nor sparse network activity are necessary for the development of localized receptive fields. The analysis of alternative sensory modalities such as auditory models or V2 development lead to the same conclusions. In all examples, receptive fields can be predicted a priori by reformulating an abstract model as nonlinear Hebbian learning. Thus nonlinear Hebbian learning and natural statistics can account for many aspects of receptive field formation across models and sensory modalities.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>The question of how the brain self-organizes to develop precisely tuned neurons has puzzled neuroscientists at least since the discoveries of Hubel and Wiesel. In the past decades, a variety of theories and models have been proposed to describe receptive field formation, notably V1 simple cells, from natural inputs. We cut through the jungle of candidate explanations by demonstrating that in fact a single principle is sufficient to explain receptive field development. Our results follow from two major insights. First, we show that many representative models of sensory development are in fact implementing variations of a common principle: nonlinear Hebbian learning. Second, we reveal that nonlinear Hebbian learning is sufficient for receptive field formation through sensory inputs. The surprising result is that our findings are robust of specific details of a model, and allows for robust predictions on the learned receptive fields. Nonlinear Hebbian learning is therefore general in two senses: it applies to many models developed by theoreticians, and to many sensory modalities studied by experimental neuroscientists.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000781</institution-id>
<institution>European Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>268689</award-id>
</award-group>
<funding-statement>CSNB was supported by the European Community’s Seventh Framework Program (<ext-link ext-link-type="uri" xlink:href="https://ec.europa.eu/research/fp7/" xlink:type="simple">https://ec.europa.eu/research/fp7/</ext-link>) under grant agreement no. 237955 (FACETS-ITN). CSNB and WG were supported by the European Research Council (<ext-link ext-link-type="uri" xlink:href="https://erc.europa.eu/" xlink:type="simple">https://erc.europa.eu/</ext-link>) under grant agreement no. 268689 (MultiRules). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="1"/>
<page-count count="24"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Neurons in sensory areas of the cortex are optimally driven by stimuli with characteristic features that define the receptive field of the cell. While receptive fields of simple cells in primary visual cortex (V1) are localized in visual space and sensitive to the orientation of light contrast [<xref ref-type="bibr" rid="pcbi.1005070.ref001">1</xref>], those of auditory neurons are sensitive to specific time-frequency patterns in sounds [<xref ref-type="bibr" rid="pcbi.1005070.ref002">2</xref>]. The concept of a receptive field is also useful when studying higher-order sensory areas, for instance when analyzing the degree of selectivity and invariance of neurons to stimulus properties [<xref ref-type="bibr" rid="pcbi.1005070.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref004">4</xref>].</p>
<p>The characteristic receptive fields of simple cells in V1 have been related to statistical properties of natural images [<xref ref-type="bibr" rid="pcbi.1005070.ref005">5</xref>]. These findings inspired various models, based on principles as diverse as sparse sensory representations [<xref ref-type="bibr" rid="pcbi.1005070.ref006">6</xref>], optimal information transmission [<xref ref-type="bibr" rid="pcbi.1005070.ref007">7</xref>], or synaptic plasticity [<xref ref-type="bibr" rid="pcbi.1005070.ref008">8</xref>]. Several studies highlighted possible connections between biological and normative justifications of sensory receptive fields [<xref ref-type="bibr" rid="pcbi.1005070.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref012">12</xref>], not only in V1, but also in other sensory areas [<xref ref-type="bibr" rid="pcbi.1005070.ref013">13</xref>], such as auditory [<xref ref-type="bibr" rid="pcbi.1005070.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref015">15</xref>] and secondary visual cortex (V2) [<xref ref-type="bibr" rid="pcbi.1005070.ref016">16</xref>].</p>
<p>Since disparate models appear to achieve similar results, the question arises whether there exists a general underlying concept in unsupervised learning models [<xref ref-type="bibr" rid="pcbi.1005070.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref017">17</xref>]. Here we show that the principle of nonlinear Hebbian learning is sufficient for receptive field development under rather general conditions. The nonlinearity is defined by the neuron’s f-I curve combined with the nonlinearity of the plasticity function. The outcome of such nonlinear learning is equivalent to projection pursuit [<xref ref-type="bibr" rid="pcbi.1005070.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref020">20</xref>], which focuses on features with non-trivial statistical structure, and therefore links receptive field development to optimality principles.</p>
<p>Here we unify and broaden the above concepts and show that plastic neural networks, sparse coding models and independent component analysis can all be reformulated as nonlinear Hebbian learning. For natural images as sensory input, we find that a broad class of nonlinear Hebbian rules lead to orientation selective receptive fields, and explain how seemingly disparate approaches may lead to similar receptive fields. The theory predicts the diversity of receptive field shapes obtained in simulations for several different families of nonlinearities. The robustness to model assumptions also applies to alternative sensory modalities, implying that the statistical properties of the input strongly constrain the type of receptive fields that can be learned. Since the conclusions are robust to specific properties of neurons and plasticity mechanisms, our results support the idea that synaptic plasticity can be interpreted as nonlinear Hebbian learning, implementing a statistical optimization of the neuron’s receptive field properties.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>The effective Hebbian nonlinearity</title>
<p>In classic rate models of sensory development [<xref ref-type="bibr" rid="pcbi.1005070.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref006">6</xref>], a first layer of neurons, representing the sensory input <bold>x</bold>, is connected to a downstream neuron with activity <italic>y</italic>, through synaptic connections with weights <bold>w</bold> (<xref ref-type="fig" rid="pcbi.1005070.g001">Fig 1a</xref>). The response to a specific input is <italic>y</italic> = <italic>g</italic>(<bold>w</bold><sup><italic>T</italic></sup> <bold>x</bold>), where <italic>g</italic> is the frequency-current (f-I) curve. In most models of Hebbian plasticity [<xref ref-type="bibr" rid="pcbi.1005070.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref023">23</xref>], synaptic changes Δ<bold>w</bold> of the connection weights depend on pre- and post-synaptic activity, with a linear dependence on the pre-synaptic and a nonlinear dependence on the post-synaptic activity, Δ<bold>w</bold> ∝ <bold>x</bold> <italic>h</italic>(<italic>y</italic>), in accordance with models of pairing experiments [<xref ref-type="bibr" rid="pcbi.1005070.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref010">10</xref>]. The learning dynamics arise from a combination of the neuronal f-I curve <italic>y</italic> = <italic>g</italic>(<bold>w</bold><sup><bold>T</bold></sup><bold>x</bold>) and the Hebbian plasticity function Δ<bold>w</bold> ∝ <bold>x</bold> <italic>h</italic>(<italic>y</italic>):
<disp-formula id="pcbi.1005070.e001"><alternatives><graphic id="pcbi.1005070.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mrow><mml:mo>Δ</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo>∝</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mspace width="4pt"/><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo> <mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mi mathvariant="bold">T</mml:mi></mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi mathvariant="bold">x</mml:mi><mml:mspace width="4pt"/><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mi mathvariant="bold">T</mml:mi></mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(1)</label></disp-formula>
where we define the <italic>effective Hebbian nonlinearity</italic> <italic>f</italic> ≔ <italic>h</italic> ∘ <italic>g</italic> as the composition of the nonlinearity in the plasticity rule and the neuron’s f-I curve. In an experimental setting, the pre-synaptic activity <italic>x</italic> is determined by the set of sensory stimuli (influenced by, e.g., the rearing conditions during sensory development [<xref ref-type="bibr" rid="pcbi.1005070.ref025">25</xref>]). Therefore, the evolution of synaptic strength, <xref ref-type="disp-formula" rid="pcbi.1005070.e001">Eq 1</xref>, is determined by the effective nonlinearity <italic>f</italic> and the statistics of the input <bold>x</bold>.</p>
<fig id="pcbi.1005070.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005070.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The effective Hebbian nonlinearity of plastic cortical networks.</title>
<p>(<bold>a</bold>) Receptive field development between an input layer of neurons with activities <italic>x</italic><sub><italic>i</italic></sub>, connected by synaptic projections <italic>w</italic><sub><italic>i</italic></sub> to a neuron with firing rate <italic>y</italic>, given by an f-I curve <italic>y</italic> = <italic>g</italic>(<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold>)). Synaptic connections change according to a Hebbian rule Δ<italic>w</italic><sub><italic>i</italic></sub> ∝ <italic>x</italic><sub><italic>i</italic></sub> <italic>h</italic>(<italic>y</italic>). (<bold>b</bold>) f-I curve (blue) of a GIF model [<xref ref-type="bibr" rid="pcbi.1005070.ref026">26</xref>] of a pyramidal neurons in response to step currents of 500 ms duration (dashed line: piece-wise linear fit, with slope <italic>a</italic> = 143 Hz/nA and threshold <italic>θ</italic> = 0.08 nA). (<bold>c</bold>) Plasticity function of the triplet STDP model [<xref ref-type="bibr" rid="pcbi.1005070.ref024">24</xref>] (blue), fitted to visual cortex plasticity data [<xref ref-type="bibr" rid="pcbi.1005070.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref024">24</xref>], showing the weight change Δ<italic>w</italic><sub><italic>i</italic></sub> as a function of the post-synaptic rate <italic>y</italic>, under a constant pre-synaptic stimulation <italic>x</italic><sub><italic>i</italic></sub> (dashed line: fit by quadratic function, with LTD factor <italic>b</italic> = 22.1 Hz). (<bold>d</bold>) The combination of the f-I curve and plasticity function generates the effective Hebbian nonlinearity (dashed line: quadratic nonlinearity with LTD threshold <italic>θ</italic><sub>1</sub> = 0.08 nA, LTP threshold <italic>θ</italic><sub>2</sub> = 0.23 nA).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005070.g001" xlink:type="simple"/>
</fig>
<p>Many existing models can be formulated in the framework of <xref ref-type="disp-formula" rid="pcbi.1005070.e001">Eq 1</xref>. For instance, in a classic study of simple-cell formation [<xref ref-type="bibr" rid="pcbi.1005070.ref008">8</xref>], the Bienenstock-Cooper-Munro (BCM) model [<xref ref-type="bibr" rid="pcbi.1005070.ref022">22</xref>] has a quadratic plasticity nonlinearity, <italic>h</italic><sub><italic>θ</italic></sub>(<italic>y</italic>) = <italic>y</italic>(<italic>y</italic> − <italic>θ</italic>), with a variable plasticity threshold <italic>θ</italic> = 〈<italic>y</italic><sup>2</sup>〉, and a sigmoidal f-I curve, <italic>y</italic> = <italic>σ</italic>(<bold>w</bold><sup><italic>T</italic></sup> <bold>x</bold>). Since the threshold <italic>θ</italic> is adapted on a time scale sufficiently slow to sample the statistics of 〈<italic>y</italic><sup>2</sup>〉 [<xref ref-type="bibr" rid="pcbi.1005070.ref028">28</xref>], and on a time scale faster than the learning dynamics [<xref ref-type="bibr" rid="pcbi.1005070.ref029">29</xref>], we may consider it as fixed, and the dynamics are well described by nonlinear Hebbian learning, Δ<bold>w</bold> ∝ <bold>x</bold> <italic>h</italic><sub><italic>θ</italic></sub>(<italic>σ</italic>(<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold>)), with a nonlinearity modulated by <italic>θ</italic>.</p>
<p>More realistic cortical networks have dynamical properties which are not accounted for by rate models. By analyzing state-of-the-art models of cortical neurons and synaptic plasticity, we inspected whether plastic spiking networks can be reduced to nonlinear Hebbian learning. We considered a generalized leaky integrate-and-fire model (GIF), which includes adaptation, stochastic firing and predicts experimental spikes with high accuracy [<xref ref-type="bibr" rid="pcbi.1005070.ref026">26</xref>], and we approximate its f-I curve by a linear rectifier, <italic>g</italic>(<italic>u</italic>) = <italic>a</italic>(<italic>u</italic> − <italic>θ</italic>)<sub>+</sub>, with slope <italic>a</italic> and threshold <italic>θ</italic> (<xref ref-type="fig" rid="pcbi.1005070.g001">Fig 1b</xref>).</p>
<p>As a phenomenological model of synaptic plasticity grounded on experimental data [<xref ref-type="bibr" rid="pcbi.1005070.ref027">27</xref>], we implemented triplet spike-timing dependent plasticity (STDP) [<xref ref-type="bibr" rid="pcbi.1005070.ref024">24</xref>]. In this STDP model, the dependence of long-term potentiation (LTP) upon two post-synaptic spikes induces in the corresponding rate model a quadratic dependence on the post-synaptic rate, while long-term depression (LTD) is linear. The resulting rate plasticity [<xref ref-type="bibr" rid="pcbi.1005070.ref024">24</xref>] is <italic>h</italic>(<italic>y</italic>) = <italic>y</italic><sup>2</sup> − <italic>by</italic>, with an LTD factor <italic>b</italic> (post-synaptic activity threshold separating LTD from LTP, <xref ref-type="fig" rid="pcbi.1005070.g001">Fig 1c</xref>), similar to the classic BCM model [<xref ref-type="bibr" rid="pcbi.1005070.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref008">8</xref>].</p>
<p>Composing the f-I curve of the GIF with the <italic>h</italic>(<italic>y</italic>) for the triplet plasticity model, we have an approximation of the effective learning nonlinearity <italic>f</italic> = <italic>h</italic> ∘ <italic>g</italic> in cortical spiking neurons (<xref ref-type="fig" rid="pcbi.1005070.g001">Fig 1d</xref>), that can be described as a quadratic rectifier, with LTD threshold given by <italic>θ</italic><sub>1</sub> = <italic>θ</italic> and LTP threshold given by <italic>θ</italic><sub>2</sub> = <italic>θ</italic>+<italic>b</italic>/<italic>a</italic>. Interestingly, the f-I slope <italic>a</italic> and LTD factor <italic>b</italic> are redundant parameters of the learning dynamics: only their ratio counts in nonlinear Hebbian plasticity. Metaplasticity can control the LTD factor [<xref ref-type="bibr" rid="pcbi.1005070.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref030">30</xref>], thus regulating the LTP threshold.</p>
<p>If one considers a linear STDP model [<xref ref-type="bibr" rid="pcbi.1005070.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref032">32</xref>] instead of the triplet STDP [<xref ref-type="bibr" rid="pcbi.1005070.ref024">24</xref>], the plasticity curve is linear [<xref ref-type="bibr" rid="pcbi.1005070.ref023">23</xref>], as in standard Hebbian learning, and the effective nonlinearity is shaped by the properties of the f-I curve (<xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2a</xref>).</p>
<fig id="pcbi.1005070.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005070.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Simple cell development from natural images regardless of specific effective Hebbian nonlinearity.</title>
<p>(<bold>a</bold>) Effective nonlinearity of five common models (arbitrary units): quadratic rectifier (green, as in cortical and BCM models, <italic>θ</italic><sub>1</sub> = 1., <italic>θ</italic><sub>2</sub> = 2.), linear rectifier (dark blue, as in <italic>L</italic><sub>1</sub> sparse coding or networks with linear STDP, <italic>θ</italic> = 3.), Cauchy sparse coding nonlinearity (light blue, <italic>λ</italic> = 3.), <italic>L</italic><sub>0</sub> sparse coding nonlinearity (orange, <italic>λ</italic> = 3.), and negative sigmoid (purple, as in ICA models). (<bold>b</bold>) Relative optimization value 〈<italic>F</italic>(<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold>)〉 for each of the five models in <bold>a</bold>, for different preselected features <bold>w</bold>, averaged over natural image patches <bold><bold>x</bold></bold>. Candidate features are represented as two-dimensional receptive fields. For all models, the optimum is achieved at the localized oriented receptive field. Inset: Example of natural image and image patch (red square) used as sensory input. (<bold>c</bold>) Receptive fields learned in four trials for ten effective Hebbian functions <italic>f</italic> (from top: the five functions considered above, <italic>u</italic><sup>3</sup>, − <italic>sin</italic>(<italic>u</italic>), <italic>u</italic>, (|<italic>u</italic>| − 2)<sub>+</sub>, − <italic>cos</italic>(<italic>u</italic>)) (<bold>left</bold> <bold>column</bold>), and their opposites − <italic>f</italic> (<bold>right column</bold>). The first seven functions (above the dashed line) lead to localized oriented filters, while a sign-flip leads to random patterns. Linear or symmetric functions are exceptions and do not develop oriented filters (<bold>bottom</bold> <bold>rows</bold>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005070.g002" xlink:type="simple"/>
</fig>
<p>In the following we consider these rate approximations of STDP and analyze the developmental properties of spiking neurons through their effective nonlinearities.</p>
</sec>
<sec id="sec004">
<title>Sparse coding as nonlinear Hebbian learning</title>
<p>Beyond phenomenological modeling, normative principles that explain receptive fields development have been one of the goals of theoretical neuroscience [<xref ref-type="bibr" rid="pcbi.1005070.ref033">33</xref>]. Sparse coding [<xref ref-type="bibr" rid="pcbi.1005070.ref006">6</xref>] starts from the assumptions that V1 aims at maximizing the sparseness of the activity in the sensory representation, and became a well-known normative model to develop orientation selective receptive fields [<xref ref-type="bibr" rid="pcbi.1005070.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref013">13</xref>]. We demonstrate that the algorithm implemented in the sparse coding model is in fact a particular example of nonlinear Hebbian learning.</p>
<p>The sparse coding model aims at minimizing an input reconstruction error <inline-formula id="pcbi.1005070.e002"><alternatives><graphic id="pcbi.1005070.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:mi>E</mml:mi> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">x</mml:mi></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext> <mml:mi>λ</mml:mi> <mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, under a sparsity constraint <italic>S</italic> with relative importance <italic>λ</italic> &gt; 0. For <italic>K</italic> hidden neurons <italic>y</italic><sub><italic>j</italic></sub>, such a model implicitly assumes that the vector <bold>w</bold><sub><bold>j</bold></sub> of feed-forward weights onto neuron <italic>j</italic> are mirrored by hypothetical “reconstruction weights”, <bold>W</bold> = [<bold>w</bold><sub>1</sub> … <bold>w</bold><sub><italic>K</italic></sub>]. The resulting encoding algorithm can be recast as a neural model [<xref ref-type="bibr" rid="pcbi.1005070.ref034">34</xref>], if neurons are embedded in a feedforward model with lateral inhibition, <bold>y</bold> = <italic>g</italic>(<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold> − <bold>v</bold><sup><italic>T</italic></sup><bold>y</bold>), where <italic>v</italic> are inhibitory recurrent synaptic connections (see <xref ref-type="sec" rid="sec014">Methods</xref>). In the case of a single output neuron, its firing rate is simply <italic>y</italic> = <italic>g</italic>(<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold>). The nonlinearity <italic>g</italic> of the f-I curve is threshold-like, and determined by the choice of the sparsity constraint [<xref ref-type="bibr" rid="pcbi.1005070.ref034">34</xref>], such as the Cauchy, <italic>L</italic><sub>0</sub>, or <italic>L</italic><sub>1</sub> constraints (<xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2a</xref>, see <xref ref-type="sec" rid="sec014">Methods</xref>).</p>
<p>If weights are updated through gradient descent so as to minimize <italic>E</italic>, the resulting plasticity rule is Oja’s learning rule [<xref ref-type="bibr" rid="pcbi.1005070.ref035">35</xref>], Δ<bold>w</bold> ∝ <bold>x</bold> <italic>y</italic> − <bold>w</bold> <italic>y</italic><sup>2</sup>. The second term −<bold>w</bold> <italic>y</italic><sup>2</sup> has a multiplicative effect on the strength of synapses projecting onto the same neuron (weight rescaling), but does not affect the receptive field shape, whereas the first term <bold>x</bold> <italic>y</italic> drives feature selectivity and receptive field formation.</p>
<p>Together, these derivations imply that the one-unit sparse coding algorithm can be implemented by an effective nonlinear Hebbian rule combined with weight normalization. Although the plasticity mechanism is linear, Δ<bold>w</bold> ∝ <bold>x</bold> <italic>y</italic>, a nonlinearity arises from the f-I curve, <italic>y</italic> = <italic>g</italic>(<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold>), so that the effective plasticity is
<disp-formula id="pcbi.1005070.e003"><alternatives><graphic id="pcbi.1005070.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>∝</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mspace width="4pt"/><mml:mi>g</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula></p>
<p>This analysis reveals an equivalence between sparse coding models and neural networks with linear plasticity mechanisms, where the sparsity constraint is determined by the f-I curve <italic>g</italic>.</p>
<p>While Oja’s rule is commonly associated with principal component analysis (PCA), developing connections that project the input in the direction of largest variance [<xref ref-type="bibr" rid="pcbi.1005070.ref035">35</xref>], this relation is only valid for linear neurons. When nonlinear neurons are considered, Oja’s rule is also sensitive to higher-order statistics, as analyzed below.</p>
<p>Similarly, algorithms performing independent component analysis (ICA), a model class closely related to sparse coding, also perform effective nonlinear Hebbian learning, albeit inversely, with linear neurons and a nonlinear plasticity rule [<xref ref-type="bibr" rid="pcbi.1005070.ref036">36</xref>]. For variants of ICA based on information maximization [<xref ref-type="bibr" rid="pcbi.1005070.ref007">7</xref>] or kurtosis [<xref ref-type="bibr" rid="pcbi.1005070.ref036">36</xref>] different nonlinearities arise (<xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2a</xref>), but <xref ref-type="disp-formula" rid="pcbi.1005070.e003">Eq 2</xref> applies equally well. Hence, various instantiations of sparse coding and ICA models not only relate to each other in their normative assumptions [<xref ref-type="bibr" rid="pcbi.1005070.ref037">37</xref>], but when implemented as iterative gradient update rules, they all employ nonlinear Hebbian learning.</p>
</sec>
<sec id="sec005">
<title>Simple cell development for a large class of nonlinearities</title>
<p>Since the models described above can be implemented by similar plasticity rules, we hypothesized nonlinear Hebbian learning to be a general principle that explains the development of receptive field selectivity. Nonlinear Hebbian learning with an effective nonlinearity <italic>f</italic> is linked to an optimization principle with a function <inline-formula id="pcbi.1005070.e004"><alternatives><graphic id="pcbi.1005070.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mn>0</mml:mn></mml:mrow> <mml:mi>u</mml:mi></mml:msubsup> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>u</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:msup><mml:mi>u</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> [<xref ref-type="bibr" rid="pcbi.1005070.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref020">20</xref>]. For an input ensemble <bold>x</bold>, optimality is achieved by weights <inline-formula id="pcbi.1005070.e005"><alternatives><graphic id="pcbi.1005070.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> that maximize <inline-formula id="pcbi.1005070.e006"><alternatives><graphic id="pcbi.1005070.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:mo>〈</mml:mo><mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:msup><mml:mrow/><mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>〉</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where angular brackets denote the average over the input statistics. Nonlinear Hebbian learning is a stochastic gradient ascent implementation of this optimization process, called projection pursuit [<xref ref-type="bibr" rid="pcbi.1005070.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref020">20</xref>]:
<disp-formula id="pcbi.1005070.e007"><alternatives><graphic id="pcbi.1005070.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:mi mathvariant="bold">w</mml:mi></mml:msub> <mml:mrow><mml:mo>〈</mml:mo> <mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>〉</mml:mo></mml:mrow> <mml:mo>⇒</mml:mo> <mml:mtext> </mml:mtext><mml:mo>Δ</mml:mo><mml:mtext> </mml:mtext> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>∝</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mspace width="4pt"/><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula></p>
<p>Motivated by results from ICA theory [<xref ref-type="bibr" rid="pcbi.1005070.ref038">38</xref>] and statistical properties of whitened natural images [<xref ref-type="bibr" rid="pcbi.1005070.ref005">5</xref>], we selected diverse Hebbian nonlinearities <italic>f</italic> (<xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2a</xref>) and calculated the corresponding optimization value 〈<italic>F</italic>(<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold>)〉 for different features of interest that we consider as candidate RF shapes, with a whitened ensemble of patches extracted from natural images as input (see <xref ref-type="sec" rid="sec014">Methods</xref>). These include a random connectivity pattern, a non-local oriented edge (as in principal components of natural images) and localized oriented edges (as in cat and monkey simple cells in the visual cortex), shown in <xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2b</xref>. The relative value of 〈<italic>F</italic>(<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold>)〉 between one feature and another was remarkably consistent across various choices of the nonlinearity <italic>f</italic>, with localized orientation-selective receptive fields as maxima (<xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2b</xref>). Furthermore, we also searched for the maxima through gradient ascent, so as to confirm that the maxima are orientation selective (<xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2c</xref>, <bold>left</bold>). Our results indicate that receptive field development of simple cells is mainly governed by the statistical properties of natural images, while robust to specific model assumptions.</p>
<p>The relevant property of natural image statistics is that the distribution of a feature derived from typical localized oriented patterns has high kurtosis [<xref ref-type="bibr" rid="pcbi.1005070.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref039">39</xref>]. Thus to establish a quantitative measure whether a nonlinearity is suitable for feature learning, we define a <italic>selectivity index</italic> (<italic>SI</italic>), which measures the relative value of 〈<italic>F</italic>(.)〉 between a variable <italic>l</italic> with a Laplacian distribution and a variable <italic>g</italic> with Gaussian distribution [<xref ref-type="bibr" rid="pcbi.1005070.ref038">38</xref>]: <italic>SI</italic> = (〈<italic>F</italic>(<italic>l</italic>)〉 − 〈<italic>F</italic>(<italic>g</italic>)〉)/<italic>σ</italic><sub><italic>F</italic></sub> (see <xref ref-type="sec" rid="sec014">Methods</xref>). The Laplacian variable has higher kurtosis than the Gaussian variable, serving as a prototype of a kurtotic distribution. Since values obtained by filtering natural images with localized oriented patterns have a distribution with longer tails than other patterns [<xref ref-type="bibr" rid="pcbi.1005070.ref005">5</xref>], as does the Laplacian variable compared to the Gaussian, positive values <italic>SI</italic> &gt; 0 indicate good candidate functions for learning simple cell-like receptive fields from natural images. We find that each model has an appropriate parameter range where <italic>SI</italic> &gt; 0 (<xref ref-type="fig" rid="pcbi.1005070.g003">Fig 3</xref>). For example the quadratic rectifier nonlinearity needs an LTP threshold <italic>θ</italic><sub>2</sub> below some critical level, so as to be useful for feature learning (<xref ref-type="fig" rid="pcbi.1005070.g003">Fig 3a</xref>).</p>
<fig id="pcbi.1005070.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005070.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Selectivity index for different nonlinearities <italic>f</italic>.</title>
<p>(<bold>a</bold>) Quadratic rectifier (small graphic, three examples with different LTP thresholds) with LTD threshold at <italic>θ</italic><sub>1</sub> = 1: LTP threshold must be below 3.5 to secure positive selectivity index (green region, main Fig) and learn localized oriented receptive fields (inset). A negative selectivity index (red region) leads to a random connectivity pattern (inset) (<bold>b</bold>) Linear rectifier: activation threshold must be above zero. (<bold>c</bold>) Sigmoid: center must be below <italic>a</italic> = − 1.2 or, for a stronger effect, above <italic>a</italic> = +1.2. The opposite conditions apply to the negative sigmoid. (<bold>d</bold>) Cauchy sparse coding nonlinearity: positive but weak feature selectivity for any sparseness penalty <italic>λ</italic> &gt; 0. Insets show the nonlinearities for different choices of parameters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005070.g003" xlink:type="simple"/>
</fig>
<p>A sigmoidal function with threshold at zero has <italic>negative SI</italic>, but a <italic>negative</italic> sigmoid, as used in ICA studies [<xref ref-type="bibr" rid="pcbi.1005070.ref007">7</xref>], has <italic>SI</italic> &gt; 0. More generally, whenever an effective nonlinearity <italic>f</italic> is not suited for feature learning, its opposite − <italic>f</italic> should be, since its <italic>SI</italic> will have the opposite sign (<xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2c</xref>). This implies that, in general, half of the function space could be suitable for feature learning [<xref ref-type="bibr" rid="pcbi.1005070.ref038">38</xref>], i.e. it finds weights <italic>w</italic> such that the distribution of the feature <bold>w</bold><sup><italic>T</italic></sup><bold>x</bold> has a long tail, indicating high kurtosis (“kurtotic feature”). The other half of the function space learns the least kurtotic features (e.g. random connectivity patterns for natural images, <xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2b and 2c</xref>).</p>
<p>This universality strongly constrains the possible shape of receptive fields that may arise during development for a given input dataset. For whitened natural images, a learnable receptive field is in general either a localized edge detector or a non-localized random connectivity pattern.</p>
<p>While there is no simple description for the class of suitable functions, we may gain some intuition by considering the class of rectified power functions, <inline-formula id="pcbi.1005070.e008"><alternatives><graphic id="pcbi.1005070.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>u</mml:mi> <mml:mrow><mml:mo>+</mml:mo></mml:mrow> <mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, <italic>r</italic> ∈ ℜ<sup>+</sup>. In the case of powers <italic>r</italic> &gt; 2, the selectivity index is positive. As a consequence, any supra-linear nonlinearity <inline-formula id="pcbi.1005070.e009"><alternatives><graphic id="pcbi.1005070.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>u</mml:mi> <mml:msubsup><mml:mrow/><mml:mrow><mml:mo>+</mml:mo></mml:mrow> <mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> with <italic>p</italic> &gt; 1 should be suitable for feature learning. In <xref ref-type="table" rid="pcbi.1005070.t001">Table 1</xref>, we include the appropriate parameter range for various effective nonlinearities.</p>
<table-wrap id="pcbi.1005070.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005070.t001</object-id>
<label>Table 1</label>
<caption>
<title>Parameter ranges for suitable effective nonlinearities and corresponding optimization functions.</title>
</caption>
<alternatives>
<graphic id="pcbi.1005070.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005070.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center"/>
<th align="center">Effective nonlinearity <italic>f</italic>(<italic>u</italic>)</th>
<th align="center">Optimization function <italic>F</italic>(<italic>u</italic>)</th>
<th align="center">Parameter range</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Linear rectifier</td>
<td align="center">(<italic>u</italic> − <italic>θ</italic>)<sub>+</sub></td>
<td align="center">
<inline-formula id="pcbi.1005070.e010">
<alternatives>
<graphic id="pcbi.1005070.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e010" xlink:type="simple"/>
<mml:math display="inline" id="M10">
<mml:msubsup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mo>-</mml:mo>
<mml:mi>θ</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>+</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="center"><italic>θ</italic> &gt; 0</td>
</tr>
<tr>
<td align="center">Quadratic rectifier</td>
<td align="center">
<inline-formula id="pcbi.1005070.e011">
<alternatives>
<graphic id="pcbi.1005070.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e011" xlink:type="simple"/>
<mml:math display="inline" id="M11">
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mi>b</mml:mi>
<mml:mtext> </mml:mtext>
<mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mo>−</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mn>1</mml:mn>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
</mml:msub>
<mml:mtext> </mml:mtext>
<mml:mo>+</mml:mo>
<mml:mtext> </mml:mtext>
<mml:msubsup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mo>−</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mn>1</mml:mn>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>+</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="center">
<inline-formula id="pcbi.1005070.e012">
<alternatives>
<graphic id="pcbi.1005070.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e012" xlink:type="simple"/>
<mml:math display="inline" id="M12">
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mfrac>
<mml:mi>b</mml:mi>
<mml:mn>2</mml:mn>
</mml:mfrac>
<mml:msubsup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mo>−</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mn>1</mml:mn>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>+</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mtext> </mml:mtext>
<mml:mo>+</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mn>3</mml:mn>
</mml:mfrac>
<mml:msubsup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mo>−</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mn>1</mml:mn>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>+</mml:mo>
</mml:mrow>
<mml:mn>3</mml:mn>
</mml:msubsup>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="center"><italic>b</italic> &lt; 3.5</td>
</tr>
<tr>
<td align="center">Sigmoid</td>
<td align="center">(1 + <italic>e</italic><sup> − 2(<italic>u</italic> − <italic>a</italic>)</sup>)<sup> − 1</sup></td>
<td align="center">
<inline-formula id="pcbi.1005070.e013">
<alternatives>
<graphic id="pcbi.1005070.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e013" xlink:type="simple"/>
<mml:math display="inline" id="M13">
<mml:mrow>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mn>2</mml:mn>
</mml:mfrac>
<mml:mtext> </mml:mtext>
<mml:mi>l</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>g</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mtext> </mml:mtext>
<mml:mo>+</mml:mo>
<mml:mtext> </mml:mtext>
<mml:msup>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mn>2</mml:mn>
<mml:mo>(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mo>-</mml:mo>
<mml:mi>a</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="center">|<italic>a</italic>| &gt; − 1.2</td>
</tr>
<tr>
<td align="center">Negative sigmoid</td>
<td align="center">−(1 + <italic>e</italic><sup>−2(<italic>u</italic>−<italic>a</italic>)</sup>)<sup>−1</sup></td>
<td align="center">
<inline-formula id="pcbi.1005070.e014">
<alternatives>
<graphic id="pcbi.1005070.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e014" xlink:type="simple"/>
<mml:math display="inline" id="M14">
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mn>2</mml:mn>
</mml:mfrac>
<mml:mtext> </mml:mtext>
<mml:mi>l</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>g</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mtext> </mml:mtext>
<mml:mo>+</mml:mo>
<mml:mtext> </mml:mtext>
<mml:msup>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mn>2</mml:mn>
<mml:mo>(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mo>-</mml:mo>
<mml:mi>a</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="center">|<italic>a</italic>| &lt; −1.2</td>
</tr>
<tr>
<td align="center">Power</td>
<td align="center">
<inline-formula id="pcbi.1005070.e015">
<alternatives>
<graphic id="pcbi.1005070.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e015" xlink:type="simple"/>
<mml:math display="inline" id="M15">
<mml:mrow>
<mml:mi>u</mml:mi>
<mml:msubsup>
<mml:mrow/>
<mml:mrow>
<mml:mo>+</mml:mo>
</mml:mrow>
<mml:mi>p</mml:mi>
</mml:msubsup>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="center">
<inline-formula id="pcbi.1005070.e016">
<alternatives>
<graphic id="pcbi.1005070.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e016" xlink:type="simple"/>
<mml:math display="inline" id="M16">
<mml:mrow>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mrow>
<mml:mi>p</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:mfrac>
<mml:mi>u</mml:mi>
<mml:msubsup>
<mml:mrow/>
<mml:mrow>
<mml:mo>+</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi>p</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="center"><italic>p</italic> &gt; 1</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>An important special case is an effective linear curve, <italic>f</italic>(<italic>u</italic>) = <italic>u</italic>, which arises when both f-I and plasticity curves are linear [<xref ref-type="bibr" rid="pcbi.1005070.ref021">21</xref>]. Because the linear model maximizes variance 〈(<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold>)<sup>2</sup>〉, it can perform principal component analysis [<xref ref-type="bibr" rid="pcbi.1005070.ref035">35</xref>], but does not have any feature selectivity on whitened input datasets, where variance is constant (<xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2c</xref>).</p>
<p>Symmetric effective nonlinearities, <italic>f</italic>(<italic>u</italic>) = <italic>f</italic>(−<italic>u</italic>), are also exceptions, since their corresponding optimization functions are asymmetric, <italic>F</italic>(<italic>u</italic>) = −<italic>F</italic>(−<italic>u</italic>), so that for datasets with symmetric statistical distributions, <italic>P</italic>(<bold>x</bold>) = <italic>P</italic>(−<bold>x</bold>), the optimization value will be zero, 〈<italic>F</italic><sub><italic>asym</italic>.</sub>(<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold><sub><italic>sym</italic>.</sub>)〉 = 0. As natural images are not completely symmetric, localized receptive fields do develop, though without orientation selectivity, as illustrated by a cosine function and a symmetric piece-wise linear function as effective nonlinearities (<xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2c</xref>, bottom rows).</p>
</sec>
<sec id="sec006">
<title>Predicting receptive field diversity</title>
<p>Sensory neurons display a variety of receptive field shapes [<xref ref-type="bibr" rid="pcbi.1005070.ref040">40</xref>], and modeling efforts [<xref ref-type="bibr" rid="pcbi.1005070.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref012">12</xref>] have attempted to understand the properties that give rise to the specific receptive fields seen in experiments. We show here that the shape diversity of a model can be predicted by our projection pursuit analysis, and is primarily determined by the statistics of input representation, while relatively robust to the specific effective nonlinearity.</p>
<p>We studied a model with multiple neurons in the second layer, which compete with each other for the representation of specific features of the input. Each neuron had a piece-wise linear f-I curve and a quadratic rectifier plasticity function (see <xref ref-type="sec" rid="sec014">Methods</xref>) and projected inhibitory connections <italic>v</italic> onto all others. These inhibitory connections are learned by anti-Hebbian plasticity and enforce decorrelation of neurons, so that receptive fields represent different positions, orientations and shapes [<xref ref-type="bibr" rid="pcbi.1005070.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref044">44</xref>]. For 50 neurons, the resulting receptive fields became diversified (<xref ref-type="fig" rid="pcbi.1005070.g004">Fig 4a–4c</xref>, colored dots). In an overcomplete network of 1000 neurons, the diversity further increased (<xref ref-type="fig" rid="pcbi.1005070.g004">Fig 4d–4f</xref>, colored dots).</p>
<fig id="pcbi.1005070.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005070.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Optimal receptive field shapes in model networks induce diversity.</title>
<p>(<bold>a-f</bold>) Gray level indicates the optimization value for different lengths and widths (see inset in <bold>a</bold>) of oriented receptive fields for natural images, for the quadratic rectifier (left, see <xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2a</xref>), linear rectifier (center) and <italic>L</italic><sub>0</sub> sparse coding (right). Optima marked with a black cross. (<bold>a-c</bold>) Colored circles indicate the receptive fields of different shapes developed in a network of 50 neurons with lateral inhibitory connections. Insets on the right show example receptive fields developed during simulation. (<bold>d-f</bold>) Same for a network of 1000 neurons.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005070.g004" xlink:type="simple"/>
</fig>
<p>For the analysis of the simulation results, we refined our inspection of optimal oriented receptive fields for natural images by numerical evaluation of the optimality criterion 〈<italic>F</italic>(<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold>)〉 for receptive fields <bold>w</bold> = <bold>w</bold><sub><italic>Gabor</italic></sub>, described as Gabor functions of variable length, width and spatial frequency. For all tested nonlinearities, the optimization function for single-neuron receptive fields varies smoothly with these parameters (<xref ref-type="fig" rid="pcbi.1005070.g004">Fig 4</xref>, grey-shaded background). The single-neuron optimality landscape was then used to analyze the multi-neuron simulation results. We found that receptive fields are located in the area where the single-neuron optimality criterion is near its maximum, but spread out so as to represent different features of the input (<xref ref-type="fig" rid="pcbi.1005070.g004">Fig 4</xref>). Thus the map of optimization values, calculated from the theory of effective nonlinearity, enables us to qualitatively predict the shape diversity of receptive fields.</p>
<p>Although qualitatively similar, there are differences in the receptive fields developed for each model, such as smaller lengths for the <italic>L</italic><sub>0</sub> sparse coding model (<xref ref-type="fig" rid="pcbi.1005070.g004">Fig 4c</xref>). While potentially significant, these differences across models may be overwhelmed by differences due to other model properties, such as different network sizes or input representations.This is illustrated by observing that receptive field diversity for a given model differ substantially across network sizes (<xref ref-type="fig" rid="pcbi.1005070.g004">Fig 4</xref>).</p>
<p>We also studied the variation of receptive field position and orientation. For all five nonlinearities considered, the optimization value is equal for different positions of the receptive field centers, confirming the translation invariance in the image statistics, as long as the receptive field is not too close to the border of the anatomically allowed fan-in of synaptic connections (<xref ref-type="fig" rid="pcbi.1005070.g005">Fig 5b</xref>). Also, all nonlinearities reveal the same bias towards the horizontal and vertical orientations (<xref ref-type="fig" rid="pcbi.1005070.g005">Fig 5c</xref>). These optimality predictions are confirmed in single neuron simulations, which lead mostly to either horizontal or vertical orientations, at random positions (<xref ref-type="fig" rid="pcbi.1005070.g005">Fig 5d</xref>). When the network is expanded to 50 neurons, recurrent inhibition forces receptive fields to cover different positions, though excluding border positions, and some neurons have non-cardinal orientations (<xref ref-type="fig" rid="pcbi.1005070.g005">Fig 5e</xref>). With 1000 neurons, receptive fields diversify to many possible combinations of position, orientation and length (<xref ref-type="fig" rid="pcbi.1005070.g005">Fig 5f</xref>).</p>
<fig id="pcbi.1005070.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005070.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Diversity of receptive field size, position and orientation.</title>
<p>(<bold>a</bold>) The optimization value of localized oriented receptive fields, within a 16x16 pixel patch of sensors, as a function of size (see <xref ref-type="sec" rid="sec014">Methods</xref>), for five nonlinearities (colors as in <xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2a</xref>). Optimal size is a receptive field of width around 3 to 4 pixels (filled triangles). (<bold>b</bold>) The optimization value as a function of position of the receptive field center, for a receptive field width of 4 pixels, indicates invariance to position within the 16x16 patch, except near the borders. (<bold>c</bold>) The optimization value as a function of orientation shows preference toward horizontal and vertical directions, for all five nonlinearities. (<bold>d</bold>) Receptive field position, orientation and length (colored bars) learned for 50 single-neuron trials. The color code indicates different orientations. (<bold>e</bold>) Receptive field positions and orientations learned in a 50 neuron network reveal diversification of positions, except at the borders. (<bold>f</bold>) With 1000 neurons, positions and orientations cover the full range of combinations (top). Selecting 50 randomly chosen receptive fields highlights the diversification of position, orientation and size (bottom). Receptive fields were learned through the quadratic rectifier nonlinearity (<italic>θ</italic><sub>1</sub> = 1., <italic>θ</italic><sub>2</sub> = 2.).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005070.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>High sensitivity to input correlations</title>
<p>Natural images have non-uniform spectral properties, with higher variance at low spatial frequencies [<xref ref-type="bibr" rid="pcbi.1005070.ref039">39</xref>]. Since Hebbian learning is sensitive to second-order correlations, in order to learn receptive fields driven by higher-order statistics, most studies pre-whiten the input, making the variance uninformative [<xref ref-type="bibr" rid="pcbi.1005070.ref036">36</xref>]. While there is evidence that the early sensory pathway induces decorrelation across neurons [<xref ref-type="bibr" rid="pcbi.1005070.ref045">45</xref>], it is unlikely for the input to the visual cortex to be perfectly white.</p>
<p>To analyze the impact of residual second-order correlations, we simulated nonlinear Hebbian learning with natural image patches that have been only approximately whitened. Instead of estimating the whitening filter from input correlation matrix, we used the preprocessing filter from the original sparse coding studies [<xref ref-type="bibr" rid="pcbi.1005070.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref037">37</xref>], which assumes that natural images possess an ideal power-law energy spectra (see <xref ref-type="sec" rid="sec014">Methods</xref>).</p>
<p>In <xref ref-type="fig" rid="pcbi.1005070.g006">Fig 6</xref>, we show the receptive fields learned for non-white inputs through nonlinear Hebbian learning. For networks with few neurons (<xref ref-type="fig" rid="pcbi.1005070.g006">Fig 6a and 6b</xref>), nonlocal receptive fields develop, with shapes similar to the principal components of natural images [<xref ref-type="bibr" rid="pcbi.1005070.ref006">6</xref>]. It reflects that when second-order correlations are present, these dominate over higher-order statistics, in which case the models we have considered will not reproduce the development of localized oriented filters. However, when considering an overcomplete network with 1000 neurons, smaller receptive fields are learned (<xref ref-type="fig" rid="pcbi.1005070.g006">Fig 6c</xref>). Our optimization framework provides a new perspective on this phenomena. For non-white inputs, second-order correlation dominate the optimization values, making principal components optima. However, when more neurons are added, competition drives the diversification of receptive fields away from the optima, and localized filters with optimization values driven by higher-order statistics can be learned.</p>
<fig id="pcbi.1005070.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005070.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Receptive fields for non-whitened natural images.</title>
<p>(<bold>a-i</bold>) Receptive field obtained for network simulations with the quadratic rectifier (top), linear rectifier (center) and <italic>L</italic><sub>0</sub> sparse coding (bottom). For few neurons (left and center), the principal components dominate the optimization and receptive fields are nonlocal, since they extend over most of the image patch. For an overcomplete network with 1000 neurons (right), lateral inhibition promotes diversity of receptive fields, including more localized ones. (<bold>insets</bold>) Sample receptive fields developed for each simulation.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005070.g006" xlink:type="simple"/>
</fig>
<p>We also compare the receptive fields developed for different nonlinearities (<xref ref-type="fig" rid="pcbi.1005070.g006">Fig 6d–6i</xref>). Particularly, the quadratic rectifier appears to develop more elongated filters compared to the linear rectifier network, while the <italic>L</italic><sub>0</sub> sparse coding network develops shorter ones. However, these differences across nonlinearities are minor compared to the difference to the receptive fields for white inputs (<xref ref-type="fig" rid="pcbi.1005070.g004">Fig 4</xref>) or compared to the differences observed across different network sizes. Thus, our results suggest that efforts to model receptive field shapes observed experimentally [<xref ref-type="bibr" rid="pcbi.1005070.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref012">12</xref>] should pay particular attention to network size and input preprocessing, which may have a greater effect than the properties of the particular model.</p>
</sec>
<sec id="sec008">
<title>Beyond V1 simple cells</title>
<p>Nonlinear Hebbian learning is not limited to explaining simple cells in V1. We investigated if the same learning principles apply to receptive field development in other visual or auditory areas or under different rearing conditions.</p>
<p>For auditory neurons [<xref ref-type="bibr" rid="pcbi.1005070.ref014">14</xref>], we used segments of speech as input (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7a</xref>) and observed the development of spectrotemporal receptive fields localized in both frequency and time [<xref ref-type="bibr" rid="pcbi.1005070.ref002">2</xref>] (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7d</xref>). The statistical distribution of input patterns aligned with the learned receptive fields had longer tails than for random or non-local receptive fields, indicating temporal sparsity of responses (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7d</xref>). Similar to our simple cell results, the learned receptive fields show higher optimization value for all five effective nonlinearities (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7g</xref>).</p>
<fig id="pcbi.1005070.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005070.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Nonlinear Hebbian learning across sensory modalities.</title>
<p>(<bold>a</bold>) The auditory input is modeled as segments over time and frequency (red) of the spectrotemporal representation of speech signals. (<bold>b</bold>) The V2 input is assembled from the output of modeled V1 complex cells at different positions and orientations. Receptive fields are represented by bars with size proportional to the connection strength to the complex cell with the respective position and orientation. (<bold>c</bold>) Strabismic rearing is modeled as binocular stimuli with non-overlapping left and right eye input patches (red). (<bold>d-f</bold>) Statistical distribution (log scale) of the input projected onto three different features for speech (<bold>d</bold>), V2 (<bold>e</bold>) and strabismus (<bold>f</bold>). In all three cases, the learned receptive field (blue, inset) is characterized by a longer tailed distribution (arrows) than the random (red) and comparative (green) features. (<bold>g-i</bold>) Relative optimization value for five nonlinearities (same as in <xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2</xref>), for the three selected patterns (<bold>insets</bold>). The receptive fields learned with the quadratic rectifier nonlinearity (<italic>θ</italic><sub>1</sub> = 1., <italic>θ</italic><sub>2</sub> = 2.) are the maxima among the three patterns, for all five nonlinearities, for all three datasets.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005070.g007" xlink:type="simple"/>
</fig>
<p>For a study of receptive field development in the secondary visual cortex (V2) [<xref ref-type="bibr" rid="pcbi.1005070.ref016">16</xref>], we used natural images and the standard energy model [<xref ref-type="bibr" rid="pcbi.1005070.ref046">46</xref>] of V1 complex cells to generate input to V2 (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7b</xref>). The learned receptive field was selective to a single orientation over neighboring positions, indicating a higher level of translation invariance. When inputs were processed with this receptive field, we found longer tails in the feature distribution than with random features or receptive fields without orientation coherence (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7e</xref>), and the learned receptive field had a higher optimization value for all choices of nonlinearity (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7h</xref>).</p>
<p>Another important constraint for developmental models are characteristic deviations, such as strabismus, caused by abnormal sensory rearing. Under normal binocular rearing conditions, the fan-in of synaptic input from the left and right eyes overlap in visual space (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7c</xref>). In this case, binocular receptive fields with similar features for left and right eyes develop. In the strabismic condition, the left and right eyes are not aligned, modeled as binocular rearing with non-overlapping input from each eye (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7c</xref>). In this scenario, a monocular simple cell-like receptive field developed (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7f</xref>), as observed in experiments and earlier models [<xref ref-type="bibr" rid="pcbi.1005070.ref028">28</xref>]. The statistical distributions confirm that for disparate inputs the monocular receptive field is more kurtotic than a binocular one, explaining its formation in diverse models [<xref ref-type="bibr" rid="pcbi.1005070.ref047">47</xref>] (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7f and 7i</xref>).</p>
<p>Our results demonstrate the generality of the theory across multiple cortical areas. Selecting a relevant feature space for an extensive analysis, as we have done with simple cells and natural images, may not be possible in general. Nonetheless, nonlinear Hebbian learning helps to explain why some features (and not others) are learnable in network models [<xref ref-type="bibr" rid="pcbi.1005070.ref015">15</xref>].</p>
</sec>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Discussion</title>
<p>Historically, a variety of models have been proposed to explain the development and distribution of receptive fields. We have shown that nonlinear Hebbian learning is a parsimonious principle which is implicitly or explicitly present in many developmental models [<xref ref-type="bibr" rid="pcbi.1005070.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref047">47</xref>]. The fact that receptive field development is robust to the specific nonlinearity highlights a functional relation between different models. It also unifies feature learning across sensory modalities: receptive fields form around features with a long-tailed distribution.</p>
<sec id="sec010">
<title>Relation to previous studies</title>
<p>Earlier studies have already placed developmental models side by side, comparing their normative assumptions, algorithmic implementation or receptive fields developed. Though consistent with their findings, our results lead to revised interpretations and predictions.</p>
<p>The similarities between sparse coding and ICA are clear from their normative correspondence [<xref ref-type="bibr" rid="pcbi.1005070.ref037">37</xref>]. Nevertheless, the additional constraint in ICA, of having at most as many features as inputs, makes it an easier problem to solve, allowing for a range of suitable algorithms [<xref ref-type="bibr" rid="pcbi.1005070.ref036">36</xref>]. These differ from algorithms derived for sparse coding, in which the inference step is difficult due to overcompleteness. We have shown that regardless of the specific normative assumptions, it is the common implementation of nonlinear Hebbian learning that explains similarities in their learning properties. Since a given normative model may have very different algorithms, as exemplified by the family of ICA algorithms [<xref ref-type="bibr" rid="pcbi.1005070.ref036">36</xref>], this result is not trivial, and it has previously not been clear how sparse coding and ICA models related to each other at the algorithmic level.</p>
<p>In contrast to the idea that in sparse coding algorithms overcompleteness is required for development of localized oriented edges [<xref ref-type="bibr" rid="pcbi.1005070.ref037">37</xref>], we have demonstrated that a sparse coding model with a single neuron is mathematically equivalent to nonlinear Hebbian learning and learns localized filters in a setting that is clearly “undercomplete”. Thus differences observed in receptive field shapes between sparse coding and ICA models [<xref ref-type="bibr" rid="pcbi.1005070.ref040">40</xref>] are likely due to differences in network size and input preprocessing. For instance, the original sparse coding model [<xref ref-type="bibr" rid="pcbi.1005070.ref037">37</xref>] applied a preprocessing filter that did not completely whiten the input, leading to larger receptive fields (<xref ref-type="fig" rid="pcbi.1005070.g006">Fig 6</xref>).</p>
<p>Studies that derive spiking models from normative theories often interpret the development of oriented receptive fields as a consequence of its normative assumptions [<xref ref-type="bibr" rid="pcbi.1005070.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref012">12</xref>]. In a recent example, a spiking network has been related to the sparse coding model [<xref ref-type="bibr" rid="pcbi.1005070.ref012">12</xref>], using neural properties defined ad hoc. Our results suggest that many other choices of neural activations would have given qualitatively similar receptive fields, independent of the sparse coding assumption. While in sparse coding the effective nonlinearity derives from a linear plasticity rule combined with a nonlinear f-I curve, our results indicate that a nonlinear plasticity rule combined with a linear neuron model would give the same outcome.</p>
<p>In order to distinguish between different normative assumptions, or particular neural implementations, the observation of “oriented filters” is not sufficient and additional constraints are needed. Similarly receptive shape diversity, another important experimental constraint, should also be considered with care, since it cannot easily distinguish between models either. Studies that confront the receptive field diversity of a model to experimental data [<xref ref-type="bibr" rid="pcbi.1005070.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref012">12</xref>] should also take into account input preprocessing choices and how the shape changes with an increasing network size, since we have observed that these aspects may have a larger effect on receptive field shape than the particulars of the learning model.</p>
<p>Empirical studies of alternative datasets, including abnormal visual rearing [<xref ref-type="bibr" rid="pcbi.1005070.ref047">47</xref>], tactile and auditory stimuli [<xref ref-type="bibr" rid="pcbi.1005070.ref015">15</xref>], have also observed that different unsupervised learning algorithms lead to comparable receptive fields shapes. Our results offer a plausible theoretical explanation for these findings.</p>
<p>Past investigations on nonlinear Hebbian learning [<xref ref-type="bibr" rid="pcbi.1005070.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref038">38</xref>] demonstrated that many nonlinearities were capable of solving the cocktail party problem. Since it is a specific toy model, that asks for the unmixing of linearly mixed independent features, it is not clear a priori whether the same conclusions would hold in other settings. We have shown that the results of [<xref ref-type="bibr" rid="pcbi.1005070.ref020">20</xref>] and [<xref ref-type="bibr" rid="pcbi.1005070.ref038">38</xref>] generalize in two directions. First, the effective nonlinear Hebbian learning mechanism is also behind other models beyond ICA, such as sparse coding models and plastic spiking networks. Second, the robustness to the choice of nonlinearity is not limited to a toy example, but also holds in multiple real world data. Our approach of identifying generic principles enables us to transfer results from one model, such as orientation selectivity or optimization of higher-order statistics to other models within the general framework. Therefore our insights may contribute to predict the outcome of a variety of developmental models in diverse applications.</p>
</sec>
<sec id="sec011">
<title>Robustness to normative assumptions</title>
<p>Many theoretical studies start from normative assumptions [<xref ref-type="bibr" rid="pcbi.1005070.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref037">37</xref>], such as a statistical model of the sensory input or a functional objective, and derive neural and synaptic dynamics from them. Our claim of universality of feature learning indicates that details of normative assumptions may be of lower importance.</p>
<p>For instance, in sparse coding one assumes features with a specific statistical prior [<xref ref-type="bibr" rid="pcbi.1005070.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref037">37</xref>]. After learning, this prior is expected to match the posterior distribution of the neuron’s firing activity [<xref ref-type="bibr" rid="pcbi.1005070.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref037">37</xref>]. Nevertheless, we have shown that receptive field learning is largely unaffected by the choice of prior. Thus, one cannot claim that the features were learned because they match the assumed prior distribution, and indeed in general they do not. For a coherent statistical interpretation, one could search for a prior that would match the feature statistics. However, since the outcome of learning is largely unaffected by the choice of prior, such a statistical approach would have limited predictive power. Generally, kurtotic prior assumptions enable feature learning, but the specific priors are not as decisive as one might expect. Because normative approaches have assumptions, such as independence of hidden features, that are not generally satisfied by the data they are applied to, the actual algorithm that is used for optimization becomes more critical than the formal statistical model.</p>
<p>The concept of sparseness of neural activity is used with two distinct meanings. The first one is a single-neuron concept and specifically refers to the long-tailed distribution statistics of neural activity, indicating a “kurtotic” distribution. The second notion of sparseness is an ensemble concept and refers to the very low firing rate of neurons, observed in cortical activity [<xref ref-type="bibr" rid="pcbi.1005070.ref048">48</xref>], which may arise from lateral competition in overcomplete representations. Overcompleteness of ensembles makes sparse coding different from ICA [<xref ref-type="bibr" rid="pcbi.1005070.ref037">37</xref>]. We have shown here that competition between multiple neurons is fundamental for receptive field diversity, whereas it is not required for simple cell formation per se. Kurtotic features can be learned even by a single neuron with nonlinear Hebbian learning, and with no restrictions on the sparseness of its firing activity.</p>
<p>Recent studies have also questioned normative explanations for V1 receptive fields by highlighting that these models do not accurately capture the statistics of natural images [<xref ref-type="bibr" rid="pcbi.1005070.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref050">50</xref>]. The generative models learned for sparse coding or ICA do not generate qualitatively good samples of natural image patches [<xref ref-type="bibr" rid="pcbi.1005070.ref050">50</xref>]. In particular, the performance in the quantitative criteria that these models are designed to optimize, such as likelihood of the data [<xref ref-type="bibr" rid="pcbi.1005070.ref050">50</xref>] or higher-order redundancy [<xref ref-type="bibr" rid="pcbi.1005070.ref049">49</xref>], is sometimes only marginally better than that of simpler models. Further studies are necessary to elucidate more complex models going beyond the two-layer model considered here.</p>
<p>For instance, models of spiking networks learning spatio-temporal patterns have been proposed based on diverse principles such as reward-modulated plasticity [<xref ref-type="bibr" rid="pcbi.1005070.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref052">52</xref>], novelty-like global factors [<xref ref-type="bibr" rid="pcbi.1005070.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref054">54</xref>] and temporal correlations [<xref ref-type="bibr" rid="pcbi.1005070.ref055">55</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref056">56</xref>]. It would be interesting to investigate if generality principles can also shed light on such models. Furthermore, top-down inputs form a substantial part of the incoming signal to sensory areas [<xref ref-type="bibr" rid="pcbi.1005070.ref057">57</xref>] and it is unclear how they might affect learning and representation in sensory networks. Multi-layered models of probabilistic inference may provide ways to integrate these aspects under a coherent framework for sensory development [<xref ref-type="bibr" rid="pcbi.1005070.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref059">59</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref060">60</xref>].</p>
<p>Our arguments can be formulated using Marr’s three levels of analysis: the computational level, the algorithmic level and the implementational level [<xref ref-type="bibr" rid="pcbi.1005070.ref061">61</xref>]. We have argued that the algorithmic level, through nonlinear Hebbian learning, is fundamental in understanding many current models of sensory development, while being consistent with multiple biological implementations and computational goals. Our results show that the models and experimental evidence considered were not sufficient to conclusively discriminate between normative assumptions, indicating indeterminacy at the computational level. Since ultimately one also wants a normative understanding of sensory networks, our results argue for more experimental evidence to be taken into account, requiring more complex models, which in turn shall be described by, or derived from, precise computational objectives, such as probabilistic inference or efficient coding.</p>
</sec>
<sec id="sec012">
<title>Interaction with input preprocessing and homeostatic mechanisms</title>
<p>The concept of nonlinear Hebbian learning also clarifies the interaction of feature selectivity with input preprocessing. Most studies of receptive field development consider pre-whitened inputs, which may be justified by the evidence that the early sensory pathway decorrelates neural activity [<xref ref-type="bibr" rid="pcbi.1005070.ref062">62</xref>]. However, we have shown that developmental models are highly sensitive to second-order statistics, and even residual correlations will substantially alter receptive field development. When correlations at low spatial frequencies were present in the input images, nonlinear Hebbian learning models learned nonlocal receptive fields.</p>
<p>In this case, additional mechanisms become necessary to reproduce the development of localized receptive fields as observed in the visual cortex. One possibility is that the competition in overcomplete networks drives the diversify of receptive fields away from principal components, so that neurons become sensitive to higher-order statistics [<xref ref-type="bibr" rid="pcbi.1005070.ref006">6</xref>]. Another explanation is that the restriction on the arborization of input connections is responsible for local properties of V1 receptive fields [<xref ref-type="bibr" rid="pcbi.1005070.ref063">63</xref>], in which case localization is not related to higher-order statistics. These considerations demonstrate how alternative input preprocessing can radically change the interpretation of developmental studies, and suggests that more attention should be paid to the preprocessing steps performed in modeling studies. Importantly, it highlights the necessity of more investigations on learning models with robustness to second-order correlations.</p>
<p>In studies of spiking networks, the input is restricted to positive rates, possibly through an on/off representation, as observed in the LGN [<xref ref-type="bibr" rid="pcbi.1005070.ref063">63</xref>]. In such alternative representations, trivial receptive fields may develop, such as a single non-zero synapse, and additional mechanisms, such as hard bounds on each synaptic strength, <italic>a</italic> ≤ <italic>w</italic><sub><italic>j</italic></sub> ≤ <italic>b</italic>, may be necessary to restrict the optimization space to desirable features [<xref ref-type="bibr" rid="pcbi.1005070.ref010">10</xref>].</p>
<p>Instead of constraining the synaptic weights, one may implement a synaptic decay as in Oja’s plasticity rule [<xref ref-type="bibr" rid="pcbi.1005070.ref035">35</xref>], Δ<bold>w</bold> ∝ <bold>x</bold> <italic>y</italic> − <bold>w</bold> <italic>y</italic><sup>2</sup> (see also [<xref ref-type="bibr" rid="pcbi.1005070.ref064">64</xref>]). Because of its multiplicative effect, the decay term does not alter the receptive field, but only scales its strength. Thus, it is equivalent to rescaling the input in the f-I curve, so as to shift it to the appropriate range (<xref ref-type="fig" rid="pcbi.1005070.g003">Fig 3</xref>). Similar scaling effects arise from f-I changes due to intrinsic plasticity [<xref ref-type="bibr" rid="pcbi.1005070.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref065">65</xref>] or due to the sliding threshold in BCM-like models, where the effective nonlinearity is modulated by the current weights. Since we have shown that receptive field development is robust to the specific nonlinearity, we expect our results in general to remain valid in the presence of such homeostatic mechanisms. The precise relation between nonlinear Hebbian learning, spiking representations and homeostasis in the cortex is an important topic for further studies.</p>
</sec>
<sec id="sec013">
<title>Universality supports biological instantiation</title>
<p>The principle of nonlinear Hebbian learning has a direct correspondence to biological neurons and is compatible with a large variety of plasticity mechanisms. It is not uncommon for biological systems to have diverse implementations with comparable functional properties [<xref ref-type="bibr" rid="pcbi.1005070.ref066">66</xref>]. Different species, or brain areas, could have different neural and plasticity characteristics, and still have similar feature learning properties [<xref ref-type="bibr" rid="pcbi.1005070.ref067">67</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref068">68</xref>]. The generality of the results discussed in this paper reveals learning simple cell-like receptive fields from natural images to be much easier than previously thought. It implies that a biological interpretation of models is possible even if some aspects of a model appear simplified or even wrong in some biological aspects. Universality also implies that the study of receptive field development is not sufficient to distinguish between different models.</p>
<p>The relation of nonlinear Hebbian learning to projection pursuit endorses the interpretation of cortical plasticity as an optimization process. Under the rate coding assumptions considered here, the crucial property is an effective synaptic change linear in the pre-synaptic rate, and nonlinear in the post-synaptic input. Pairing experiments with random firing and independently varying pre- and post-synaptic rates would be valuable to investigate these properties [<xref ref-type="bibr" rid="pcbi.1005070.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref069">69</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref070">70</xref>]. Altogether, the robustness to details in both input modality and neural implementation suggests nonlinear Hebbian learning as a fundamental principle underlying the development of sensory representations.</p>
</sec>
</sec>
<sec id="sec014" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec015">
<title>Spiking model</title>
<p>A generalized leaky integrate-and-fire neuron [<xref ref-type="bibr" rid="pcbi.1005070.ref026">26</xref>] was used as spiking model, which includes power-law spike-triggered adaptation and stochastic firing, with parameters [<xref ref-type="bibr" rid="pcbi.1005070.ref026">26</xref>] fitted to pyramidal neurons. The f-I curve <italic>g</italic>(<italic>I</italic>) was estimated by injecting step currents and calculating the trial average of the spike count over the first 500 ms. The minimal triplet-STDP model [<xref ref-type="bibr" rid="pcbi.1005070.ref024">24</xref>] was implemented, in which synaptic changes follow
<disp-formula id="pcbi.1005070.e017"><alternatives><graphic id="pcbi.1005070.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mi>w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:msup><mml:mi>A</mml:mi> <mml:mo>+</mml:mo></mml:msup> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>+</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>+</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:msup><mml:mi>A</mml:mi> <mml:mo>-</mml:mo></mml:msup> <mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>-</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>y</italic>(<italic>t</italic>) and <italic>x</italic>(<italic>t</italic>) are the post- and pre-synaptic spike trains, respectively: <italic>y</italic>(<italic>t</italic>) = ∑<sub><italic>f</italic></sub> <italic>δ</italic>(<italic>t</italic> − <italic>t</italic><sup><italic>f</italic></sup>), where <italic>t</italic><sup><italic>f</italic></sup> are the firing times and <italic>δ</italic> denotes the Dirac <italic>δ</italic>-function; <italic>x</italic>(<italic>t</italic>) is a vector with components <inline-formula id="pcbi.1005070.e018"><alternatives><graphic id="pcbi.1005070.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>f</mml:mi></mml:msub> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:msubsup><mml:mi>t</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1005070.e019"><alternatives><graphic id="pcbi.1005070.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:msubsup><mml:mi>t</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> are the firing times of pre-synaptic neuron <italic>i</italic>; <italic>w</italic> is a vector comprising the synaptic weights <italic>w</italic><sub><italic>i</italic></sub> connecting a pre-synaptic neuron <italic>i</italic> to a post-synaptic cell. <italic>A</italic><sup>+</sup> = 6.5 ⋅ 10<sup>−3</sup> and <italic>A</italic><sup>−</sup> = 5.3 ⋅ 10<sup>−3</sup> are constants, and <inline-formula id="pcbi.1005070.e020"><alternatives><graphic id="pcbi.1005070.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>+</mml:mo></mml:msup></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1005070.e021"><alternatives><graphic id="pcbi.1005070.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>+</mml:mo></mml:msup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005070.e022"><alternatives><graphic id="pcbi.1005070.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>-</mml:mo></mml:msup></mml:math></alternatives></inline-formula> are moving averages, implemented by integration (e.g. <inline-formula id="pcbi.1005070.e023"><alternatives><graphic id="pcbi.1005070.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mi>τ</mml:mi> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mo>−</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>+</mml:mo> <mml:mi>y</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>), with time scales 114.0 ms, 16.8 ms and 33.7 ms, respectively [<xref ref-type="bibr" rid="pcbi.1005070.ref024">24</xref>]. For estimating the nonlinearity <italic>h</italic>(<italic>y</italic>) of the plasticity, pre- and post-synaptic spike trains were generated as Poisson processes, with the pre-synaptic rate set to 20 Hz.</p>
<p>A linear rectifier <italic>g</italic>(<italic>x</italic>) = <italic>a</italic>(<italic>x</italic> − <italic>b</italic>)<sub>+</sub> was fitted to the f-I curve of the spiking neuron model by squared error optimization. Similarly, a quadratic function <italic>h</italic>(<italic>x</italic>) = <italic>a</italic>(<italic>x</italic><sup>2</sup> − <italic>bx</italic>) was fitted to the nonlinearity of the triplet STDP model. The combination of these two fitted functions was plotted as fit for the effective nonlinearity <italic>f</italic>(<italic>x</italic>) = <italic>h</italic>(<italic>g</italic>(<italic>x</italic>)).</p>
</sec>
<sec id="sec016">
<title>Sparse coding analysis</title>
<p>A sparse coding model, with <italic>K</italic> neurons <italic>y</italic><sub>1</sub>, …, <italic>y</italic><sub><italic>K</italic></sub>, has a nonlinear Hebbian learning formulation. The sparse coding model minimizes a least square reconstruction error between the vector of inputs <bold>x</bold> and the reconstruction vector <bold>W</bold><bold>y</bold>, where <bold>W</bold> = [<bold>w</bold><sub>1</sub> …<bold>w</bold><sub><italic>K</italic></sub>], and <bold>y</bold> = (<italic>y</italic><sub>1</sub>, …, <italic>y</italic><sub><italic>K</italic></sub>) is the vector of neuronal activities, with <italic>y</italic><sub><italic>j</italic></sub> ≥ 0 for 1 ≤ <italic>j</italic> ≤ <italic>K</italic>. The total error <italic>E</italic> combines a sparsity constraint <italic>S</italic> with weight <italic>λ</italic> and the reconstruction error, <inline-formula id="pcbi.1005070.e024"><alternatives><graphic id="pcbi.1005070.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:mi>E</mml:mi> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">x</mml:mi></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext> <mml:mi>λ</mml:mi> <mml:mo>∑</mml:mo> <mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. <italic>E</italic> has to be minimal, averaged across all input samples, under the constraint <italic>y</italic><sub><italic>j</italic></sub> ≥ 0 for all <italic>j</italic>.</p>
<p>The minimization problem is solved by a two-step procedure. In the first step, for each input sample, one minimizes <italic>E</italic> with respect to all hidden units <italic>y</italic><sub><italic>j</italic></sub> <disp-formula id="pcbi.1005070.e025"><alternatives><graphic id="pcbi.1005070.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mi>E</mml:mi> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mn>0</mml:mn></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>⇔</mml:mo><mml:mtext> </mml:mtext> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:mi mathvariant="bold">W</mml:mi> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:mi>λ</mml:mi> <mml:msup><mml:mi>S</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>⇔</mml:mo><mml:mtext> </mml:mtext> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi mathvariant="bold">x</mml:mi> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>y</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:mi>λ</mml:mi> <mml:msup><mml:mi>S</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>⇔</mml:mo><mml:mtext> </mml:mtext> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext> <mml:mi>λ</mml:mi> <mml:msup><mml:mi>S</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi mathvariant="bold">x</mml:mi> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>y</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>⇔</mml:mo><mml:mtext> </mml:mtext> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi mathvariant="bold">x</mml:mi> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>y</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
where we constrained the vector <bold>w</bold><sub><italic>j</italic></sub> of synapses projecting onto unit <italic>y</italic><sub><italic>j</italic></sub> by ||<bold>w</bold><sub><italic>j</italic></sub>||<sup>2</sup> = 1, defined the activation function <italic>g</italic>(.) = <italic>T</italic><sup>−1</sup>(.), the inverse of <italic>T</italic>(<italic>y</italic>) = (<italic>y</italic>+<italic>λS</italic>′(<italic>y</italic>)), and defined recurrent synaptic weights <inline-formula id="pcbi.1005070.e026"><alternatives><graphic id="pcbi.1005070.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. For each input sample <bold>x</bold>, this equation shall be iterated until convergence. The equation can be interpreted as a recurrent neural network, where each neuron has an activation function <italic>g</italic>, and the input is given by the sum of the feedforward drive <inline-formula id="pcbi.1005070.e027"><alternatives><graphic id="pcbi.1005070.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and a recurrent inhibition term −∑<sub><italic>k</italic> ≠ <italic>j</italic></sub> <italic>v</italic><sub><italic>jk</italic></sub> <italic>y</italic><sub><italic>k</italic></sub>. To avoid instability, we implement a smooth membrane potential <italic>u</italic><sub><italic>j</italic></sub>, which has the same convergence point [<xref ref-type="bibr" rid="pcbi.1005070.ref034">34</xref>]
<disp-formula id="pcbi.1005070.e028"><alternatives><graphic id="pcbi.1005070.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mi>u</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mo>-</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi mathvariant="bold">x</mml:mi> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>y</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
initialized with <italic>u</italic><sub><italic>j</italic></sub>(<italic>t</italic>) = 0.</p>
<p>In the second step, we optimize the weights <bold>w</bold><sub><italic>j</italic></sub>, considering the activations <italic>y</italic><sub><italic>j</italic></sub> obtained in the previous step. Our derivation follows the approach of the original sparse coding study [<xref ref-type="bibr" rid="pcbi.1005070.ref006">6</xref>], which is related to the Expectation-Maximization (EM) algorithm, in which at this stage the latent variables (here the activations <italic>y</italic>) are treated as constants, so that <inline-formula id="pcbi.1005070.e029"><alternatives><graphic id="pcbi.1005070.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>y</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, and, in particular, <inline-formula id="pcbi.1005070.e030"><alternatives><graphic id="pcbi.1005070.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. We obtain a standard gradient descent implementation of the least square regression optimization, leading to a learning rule
<disp-formula id="pcbi.1005070.e031"><alternatives><graphic id="pcbi.1005070.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:mo>∝</mml:mo> <mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mi>E</mml:mi> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:msup><mml:mi mathvariant="bold">W</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mi mathvariant="bold">x</mml:mi> <mml:mtext> </mml:mtext><mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:msubsup><mml:mi>y</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:msub><mml:mi>y</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></disp-formula></p>
<p>The decay term <inline-formula id="pcbi.1005070.e032"><alternatives><graphic id="pcbi.1005070.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:msubsup><mml:mi>y</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> has no effect, since the norm is constrained to ||<bold>w</bold><sub><italic>j</italic></sub>|| = 1 at each step. For a single unit <italic>y</italic>, the model simplifies to a nonlinear Hebbian formulation, <inline-formula id="pcbi.1005070.e033"><alternatives><graphic id="pcbi.1005070.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mtext> </mml:mtext><mml:mo>∝</mml:mo><mml:mtext> </mml:mtext> <mml:mi mathvariant="bold">x</mml:mi> <mml:mtext> </mml:mtext><mml:mi>g</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. For multiple units, it can be interpreted as projection pursuit on an effective input, not yet represented by other neurons, <inline-formula id="pcbi.1005070.e034"><alternatives><graphic id="pcbi.1005070.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>˜</mml:mo></mml:mover> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mi mathvariant="bold">x</mml:mi> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:msub><mml:mi>y</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, which simplifies to <inline-formula id="pcbi.1005070.e035"><alternatives><graphic id="pcbi.1005070.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:mo>∝</mml:mo><mml:mtext> </mml:mtext> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mspace width="4pt"/><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>There are two non-local terms that need to be implemented by local mechanisms so as to be biologically plausible. First, the recurrent weights depend on the overlap between receptive fields, <inline-formula id="pcbi.1005070.e036"><alternatives><graphic id="pcbi.1005070.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, which is non-local. The sparse coding model assumes independent hidden neurons, which implies that after learning neurons should be pair-wise uncorrelated, <italic>cov</italic>(<italic>y</italic><sub><italic>j</italic></sub>, <italic>y</italic><sub><italic>k</italic></sub>) = 0. As an aside we note that the choice <inline-formula id="pcbi.1005070.e037"><alternatives><graphic id="pcbi.1005070.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> does not automatically guarantee decorrelation. Decorrelation may be enforced through plastic lateral connections, following an anti-Hebbian rule [<xref ref-type="bibr" rid="pcbi.1005070.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref012">12</xref>], Δ<italic>v</italic><sub><italic>jk</italic></sub> ∝ (<italic>y</italic><sub><italic>j</italic></sub>−〈<italic>y</italic><sub><italic>j</italic></sub>〉) ⋅ <italic>y</italic><sub><italic>k</italic></sub>, where 〈<italic>y</italic><sub><italic>j</italic></sub>〉 is a moving average (we use <italic>τ</italic> = 1000 input samples). Thus by substituting fixed recurrent connections by anti-Hebbian plasticity, convergence Δ<italic>v</italic><sub><italic>jk</italic></sub> = 0 implies <italic>cov</italic>(<italic>y</italic><sub><italic>j</italic></sub>, <italic>y</italic><sub><italic>k</italic></sub>) = 0. While this implementation does not guarantee <inline-formula id="pcbi.1005070.e038"><alternatives><graphic id="pcbi.1005070.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> after convergence, neither does <inline-formula id="pcbi.1005070.e039"><alternatives><graphic id="pcbi.1005070.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> guarantee decorrelation <italic>cov</italic>(<italic>y</italic><sub><italic>j</italic></sub>, <italic>y</italic><sub><italic>k</italic></sub>) = 0, it does lead to optimal decorrelation, which is the basis of the normative assumption. Additionally we constrain <italic>v</italic><sub><italic>jk</italic></sub> ≥ 0 to satisfy Dale’s law. Although some weights would converge to negative values otherwise, most neuron pairs have correlated receptive fields, and thus positive recurrent weights.</p>
<p>Second, we ignore the non-local term ∑<sub><italic>k</italic> ≠ <italic>j</italic></sub> <bold>w</bold><sub><italic>k</italic></sub> <italic>y</italic><sub><italic>k</italic></sub> <italic>y</italic><sub><italic>j</italic></sub> in the update rule. Although this approximation is not theoretically justified, we observed in simulations that receptive fields do not qualitatively differ when this term is removed.</p>
<p>The resulting Hebbian formulation can be summarized as
<disp-formula id="pcbi.1005070.e040"><alternatives><graphic id="pcbi.1005070.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e040" xlink:type="simple"/><mml:math display="block" id="M40"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi mathvariant="bold">x</mml:mi> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>y</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:mo>∝</mml:mo><mml:mtext> </mml:mtext> <mml:mi mathvariant="bold">x</mml:mi> <mml:mspace width="4pt"/><mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mtext> </mml:mtext><mml:mo>∝</mml:mo><mml:mtext> </mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:mrow><mml:mo>〈</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>〉</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula></p>
<p>This derivation unifies previous results on the biological implementation of sparse coding: the relation of the sparseness constraint to a specific activation function [<xref ref-type="bibr" rid="pcbi.1005070.ref034">34</xref>], the derivation of a Hebbian learning rule from quadratic error minimization [<xref ref-type="bibr" rid="pcbi.1005070.ref035">35</xref>], and the possibility of approximating lateral interaction terms by learned lateral inhibition [<xref ref-type="bibr" rid="pcbi.1005070.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref012">12</xref>].</p>
</sec>
<sec id="sec017">
<title>Nonlinearities and optimization value</title>
<p>The optimization value for a given effective nonlinearity <italic>f</italic>, synaptic weights <italic>w</italic>, and input samples <italic>x</italic>, is given by <italic>R</italic> = 〈<italic>F</italic>(<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold>)〉, where <inline-formula id="pcbi.1005070.e041"><alternatives><graphic id="pcbi.1005070.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mrow><mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mn>0</mml:mn></mml:mrow> <mml:mi>u</mml:mi></mml:msubsup> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>u</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:msup><mml:mi>u</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and angular brackets indicate the ensemble average over <italic>x</italic>. Relative optimization values in Figs <xref ref-type="fig" rid="pcbi.1005070.g002">2b</xref> and <xref ref-type="fig" rid="pcbi.1005070.g005">5</xref> were normalized to [0, 1], relative to the minimum and maximum values among the considered choice of features <italic>w</italic>, <italic>R</italic>* = (<italic>R</italic> − <italic>R</italic><sub><italic>min</italic></sub>)/(<italic>R</italic><sub><italic>max</italic></sub> − <italic>R</italic><sub><italic>min</italic></sub>). The selectivity index of a nonlinearity <italic>f</italic> is defined as <italic>SI</italic> = (〈<italic>F</italic>(<italic>l</italic>)〉 − 〈<italic>F</italic>(<italic>g</italic>)〉)/<italic>σ</italic><sub><italic>F</italic></sub>, where <italic>l</italic> and <italic>g</italic> are Laplacian and Gaussian variables respectively, normalized to unit variance. <inline-formula id="pcbi.1005070.e042"><alternatives><graphic id="pcbi.1005070.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mi>F</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:msqrt><mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>F</mml:mi> <mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>F</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> is a normalization factor, with <inline-formula id="pcbi.1005070.e043"><alternatives><graphic id="pcbi.1005070.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>F</mml:mi> <mml:mo>(</mml:mo> <mml:mo>.</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:msqrt><mml:mrow><mml:mo>〈</mml:mo> <mml:mi>F</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mo>.</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>〉</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>. The selectivity of an effective nonlinearity <italic>f</italic> is not altered by multiplicative scaling, <inline-formula id="pcbi.1005070.e044"><alternatives><graphic id="pcbi.1005070.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mi>α</mml:mi> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, neither by additive constants when the input distribution is symmetric, <inline-formula id="pcbi.1005070.e045"><alternatives><graphic id="pcbi.1005070.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mi>α</mml:mi> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>β</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. The effective nonlinearities in <xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2</xref> included the linear rectifier <inline-formula id="pcbi.1005070.e046"><alternatives><graphic id="pcbi.1005070.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mtext> </mml:mtext><mml:mi>u</mml:mi> <mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>u</mml:mi> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mtext> </mml:mtext><mml:mi>u</mml:mi> <mml:mtext> </mml:mtext><mml:mo>≥</mml:mo><mml:mtext> </mml:mtext> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:math></alternatives></inline-formula>, the quadratic rectifier <inline-formula id="pcbi.1005070.e047"><alternatives><graphic id="pcbi.1005070.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mtext> </mml:mtext><mml:mi>u</mml:mi> <mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mtext> − </mml:mtext> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mtext> − </mml:mtext> <mml:mi>θ</mml:mi> <mml:mtext> − </mml:mtext> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mtext> </mml:mtext><mml:mi>u</mml:mi> <mml:mtext> </mml:mtext><mml:mo>≥</mml:mo><mml:mtext> </mml:mtext> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:math></alternatives></inline-formula>, the <italic>L</italic><sub>0</sub> sparse coding nonlinearity <inline-formula id="pcbi.1005070.e048"><alternatives><graphic id="pcbi.1005070.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mtext> </mml:mtext><mml:mi>u</mml:mi> <mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext> <mml:mi>λ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>u</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mtext> </mml:mtext><mml:mi>u</mml:mi> <mml:mtext> </mml:mtext><mml:mo>≥</mml:mo><mml:mtext> </mml:mtext> <mml:mi>λ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:math></alternatives></inline-formula>, the Cauchy sparse coding nonlinearity <italic>f</italic> = <italic>T</italic><sup> − 1</sup>, where <inline-formula id="pcbi.1005070.e049"><alternatives><graphic id="pcbi.1005070.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mrow><mml:mi>T</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mtext> </mml:mtext><mml:mi>y</mml:mi> <mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>y</mml:mi> <mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext> <mml:mn>2</mml:mn> <mml:mi>λ</mml:mi> <mml:mi>y</mml:mi> <mml:mo>/</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext> <mml:msup><mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mtext> </mml:mtext><mml:mi>y</mml:mi> <mml:mtext> </mml:mtext><mml:mo>≥</mml:mo><mml:mtext> </mml:mtext> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:math></alternatives></inline-formula>, the negative sigmoid <italic>f</italic>(<italic>u</italic>) = 1 − 2/(1 + <italic>e</italic><sup> − 2<italic>u</italic></sup>), a polynomial function <italic>f</italic>(<italic>u</italic>) = <italic>u</italic><sup>3</sup>, trigonometric functions <italic>sin</italic>(<italic>u</italic>) and <italic>cos</italic>(<italic>u</italic>), a symmetric piece-wise linear function <inline-formula id="pcbi.1005070.e050"><alternatives><graphic id="pcbi.1005070.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mtext> </mml:mtext><mml:mo>|</mml:mo> <mml:mi>u</mml:mi> <mml:mo>|</mml:mo> <mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>|</mml:mo> <mml:mi>u</mml:mi> <mml:mo>|</mml:mo> <mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mi>θ</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mtext> </mml:mtext><mml:mo>|</mml:mo> <mml:mi>u</mml:mi> <mml:mo>|</mml:mo> <mml:mtext> </mml:mtext><mml:mo>≥</mml:mo><mml:mtext> </mml:mtext> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:math></alternatives></inline-formula>, as well as, for comparison, a linear function <italic>f</italic>(<italic>u</italic>) = <italic>u</italic>.</p>
</sec>
<sec id="sec018">
<title>Receptive field learning</title>
<p>Natural image patches (16 by 16 pixel windows) were sampled from a standard dataset [<xref ref-type="bibr" rid="pcbi.1005070.ref006">6</xref>] (10<sup>6</sup> patches). Patches were randomly rotated by ±90° degrees to avoid biases in orientation. The dataset was whitened by mean subtraction and a standard linear transformation <bold>x</bold>* = <bold>M</bold><bold>x</bold>, where <bold>M</bold> = <bold>R</bold><bold>D</bold><sup> − 1/2</sup> <bold>R</bold><sup><italic>T</italic></sup> and 〈<bold>x</bold> <bold>x</bold><sup><italic>T</italic></sup>〉 = <bold>RDR</bold><sup><italic>T</italic></sup> is the eigenvalue decomposition of the input correlation matrix. In <xref ref-type="fig" rid="pcbi.1005070.g006">Fig 6</xref>, we used images preprocessed as in [<xref ref-type="bibr" rid="pcbi.1005070.ref006">6</xref>], filtered in the spatial frequency domain by <inline-formula id="pcbi.1005070.e052"><alternatives><graphic id="pcbi.1005070.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. The exponential factor is a low-pass filter that attenuates high-frequency spatial noise, with <italic>f</italic><sub>0</sub> = 200 cycles per image. The linear factor <italic>f</italic> was designed to whiten the images by canceling the approximately 1/<italic>f</italic> power law spatial correlation observed in natural images [<xref ref-type="bibr" rid="pcbi.1005070.ref039">39</xref>]. But since the exponent of the power law for this particular dataset has an exponent closer to 1.2, the preprocessed images exhibit higher variance at lower spatial frequencies.</p>
<p>Synaptic weights were initialized randomly (normal distribution with zero mean) and, for an effective nonlinearity <italic>f</italic>, evolved through <inline-formula id="pcbi.1005070.e051"><alternatives><graphic id="pcbi.1005070.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005070.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext> <mml:mi>η</mml:mi> <mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>k</mml:mi></mml:msub> <mml:mspace width="4pt"/><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>k</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, for each input sample <bold>x</bold><sub><italic>k</italic></sub>, with a small learning rate <italic>η</italic>. We enforced normalized weights at each time step, ||<bold>w</bold>||<sub>2</sub> = 1, through multiplicative normalization, implicitly assuming rapid homeostatic mechanisms [<xref ref-type="bibr" rid="pcbi.1005070.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1005070.ref029">29</xref>]. For multiple neurons, the neural version of the sparse coding model described in <xref ref-type="disp-formula" rid="pcbi.1005070.e040">Eq 7</xref> was implemented. In Figs <xref ref-type="fig" rid="pcbi.1005070.g004">4</xref> and <xref ref-type="fig" rid="pcbi.1005070.g006">6</xref>, the learned receptive fields were fitted to Gabor filters by least square optimization. Receptive fields with less than 0.6 variance explained were rejected (less than 5% of all receptive fields).</p>
</sec>
<sec id="sec019">
<title>Receptive field selection</title>
<p>In <xref ref-type="fig" rid="pcbi.1005070.g002">Fig 2b</xref>, the five selected candidate patterns are: random connectivity filter (weights sampled independently from the normal distribution with zero mean), high-frequency Fourier filter (with equal horizontal and vertical spatial periods, <italic>T</italic><sub><italic>x</italic></sub> = <italic>T</italic><sub><italic>y</italic></sub> = 8 pixels), difference of Gaussians filter (<italic>σ</italic><sub>1</sub> = 3., <italic>σ</italic><sub>2</sub> = 4.), low-frequency Fourier filter (<italic>T</italic><sub><italic>x</italic></sub> = 16, <italic>T</italic><sub><italic>y</italic></sub> = 32), and centered localized Gabor filter (<italic>σ</italic><sub><italic>x</italic></sub> = 1.5, <italic>σ</italic><sub><italic>y</italic></sub> = 2.0, <italic>f</italic> = 0.2, <italic>θ</italic> = <italic>π</italic>/3, <italic>ϕ</italic> = <italic>π</italic>/2). Fourier filters were modeled as <italic>w</italic><sub><italic>ab</italic></sub> = <italic>sin</italic>(2<italic>πa</italic>/<italic>T</italic><sub><italic>x</italic></sub>) ⋅ <italic>cos</italic>(2<italic>πb</italic>/<italic>T</italic><sub><italic>y</italic></sub>); difference of Gaussians filters as the difference between two centered 2D Gaussians with same amplitude and standard deviations <italic>σ</italic><sub>1</sub> and <italic>σ</italic><sub>2</sub>; and we considered standard Gabor filters, with center (<italic>x</italic><sub><italic>c</italic></sub>, <italic>y</italic><sub><italic>c</italic></sub>), spatial frequency <italic>f</italic>, width <italic>σ</italic><sub><italic>x</italic></sub>, length <italic>σ</italic><sub><italic>y</italic></sub>, phase <italic>ϕ</italic> and angle <italic>θ</italic>. In Figs <xref ref-type="fig" rid="pcbi.1005070.g004">4</xref> and <xref ref-type="fig" rid="pcbi.1005070.g006">6</xref> we define the Gabor width and length in pixels as 2.5 times the standard deviation of the respective Gaussian envelopes, <italic>σ</italic><sub><italic>x</italic></sub> and <italic>σ</italic><sub><italic>y</italic></sub>. In <xref ref-type="fig" rid="pcbi.1005070.g005">Fig 5a</xref>, a Gabor filter of size <italic>s</italic> had parameters <italic>σ</italic><sub><italic>x</italic></sub> = 0.3 ⋅ <italic>s</italic>, <italic>σ</italic><sub><italic>y</italic></sub> = 0.6 ⋅ <italic>s</italic>, <italic>f</italic> = 1/<italic>s</italic> and <italic>θ</italic> = <italic>π</italic>/3. In <xref ref-type="fig" rid="pcbi.1005070.g005">Fig 5b and 5c</xref>, the Gabor filter parameters were <italic>σ</italic><sub><italic>x</italic></sub> = 1.2, <italic>σ</italic><sub><italic>y</italic></sub> = 2.4, <italic>f</italic> = 0.25. All receptive fields were normalized to ||<bold>w</bold>||<sub>2</sub> = 1. In Figs <xref ref-type="fig" rid="pcbi.1005070.g004">4</xref> and <xref ref-type="fig" rid="pcbi.1005070.g006">6</xref>, the background optimization value was calculated for Gabor filters of different widths, lengths, frequencies, phases <italic>ϕ</italic> = 0 and <italic>ϕ</italic> = <italic>π</italic>/2. For each width and length, the maximum value among frequencies and phases was plotted.</p>
</sec>
<sec id="sec020">
<title>Additional datasets</title>
<p>For the strabismus model, two independent natural image patches were concatenated, representing non-overlapping left and right eye inputs, forming a dataset with 16 by 32 patches [<xref ref-type="bibr" rid="pcbi.1005070.ref028">28</xref>]. For the binocular receptive field in the strabismus statistical analysis (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7a</xref>), a receptive field was learned with a binocular input with same input from left and right eyes. As V2 input, V1 complex cell responses were obtained from natural images as in standard energy models [<xref ref-type="bibr" rid="pcbi.1005070.ref046">46</xref>], modeled as the sum of the squared responses of simple cells with alternated phases. These simple cells were modeled as linear neurons with Gabor receptive fields (<italic>σ</italic><sub><italic>x</italic></sub> = 1.2, <italic>σ</italic><sub><italic>y</italic></sub> = 2.4, <italic>f</italic> = 0.3), with centers placed on a 8 by 8 grid (3.1 pixels spacing), with 8 different orientations at each position (total of 512 input dimensions). For the non-orientation selective receptive field in the V2 statistical analysis (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7d</xref>), the orientations of the input complex cells for the learned receptive field were randomized. As auditory input, spectrotemporal segments were sampled from utterances spoken by a US English male speaker (CMU US BDL ARCTIC database [<xref ref-type="bibr" rid="pcbi.1005070.ref071">71</xref>]). For the frequency decomposition [<xref ref-type="bibr" rid="pcbi.1005070.ref014">14</xref>], each audio segment was filtered by gammatone kernels, absolute and log value taken and downsampled to 50 Hz. Each sample was 20 time points long (400 ms segment) and 20 frequency points wide (equally spaced between 0.2 kHz and 4.0 kHz). For the non-local receptive field in the auditory statistical analysis (<xref ref-type="fig" rid="pcbi.1005070.g007">Fig 7g</xref>), a Fourier filter was used (<italic>T</italic><sub><italic>t</italic></sub> = <italic>T</italic><sub><italic>f</italic></sub> = 10). For all datasets, the input ensemble was whitened after the preprocessing steps, by the same linear transformation described above for natural images, and all receptive fields were normalized to ||<bold>w</bold>||<sub>2</sub> = 1.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>We thank C. Pozzorini and J. Brea for valuable comments, and D.S. Corneil for critical reading of the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005070.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hubel</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Wiesel</surname> <given-names>TN</given-names></name>. <article-title>Receptive fields of single neurones in the cat’s striate cortex</article-title>. <source>The Journal of Physiology</source>. <year>1959</year>;<volume>148</volume>(<issue>3</issue>):<fpage>574</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1113/jphysiol.1959.sp006308" xlink:type="simple">10.1113/jphysiol.1959.sp006308</ext-link></comment> <object-id pub-id-type="pmid">14403679</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Escabi</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Read</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name>. <article-title>Spectrotemporal receptive fields in the lemniscal auditory thalamus and cortex</article-title>. <source>Journal of Neurophysiology</source>. <year>2002</year>;<volume>87</volume>(<issue>1</issue>):<fpage>516</fpage>–<lpage>527</lpage>. <object-id pub-id-type="pmid">11784767</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>DiCarlo</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zoccolan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rust</surname> <given-names>N</given-names></name>. <article-title>How Does the Brain Solve Visual Object Recognition?</article-title> <source>Neuron</source>. <year>2012</year> <month>Feb</month>;<volume>73</volume>(<issue>3</issue>):<fpage>415</fpage>–<lpage>434</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.01.010" xlink:type="simple">10.1016/j.neuron.2012.01.010</ext-link></comment> <object-id pub-id-type="pmid">22325196</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Freeman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Metamers of the ventral stream</article-title>. <source>Nature Neuroscience</source>. <year>2011</year>;<volume>14</volume>(<issue>9</issue>):<fpage>1195</fpage>–<lpage>1201</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2889" xlink:type="simple">10.1038/nn.2889</ext-link></comment> <object-id pub-id-type="pmid">21841776</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Field</surname> <given-names>D</given-names></name>. <article-title>What is the goal of sensory coding?</article-title> <source>Neural Computation</source>. <year>1994</year>;<volume>6</volume>(<issue>4</issue>):<fpage>559</fpage>–<lpage>601</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1994.6.4.559" xlink:type="simple">10.1162/neco.1994.6.4.559</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source>. <year>1996</year> <month>Jun</month>;<volume>381</volume>(<issue>6583</issue>):<fpage>607</fpage>–<lpage>609</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/381607a0" xlink:type="simple">10.1038/381607a0</ext-link></comment> <object-id pub-id-type="pmid">8637596</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bell</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>The “independent components” of natural scenes are edge filters</article-title>. <source>Vision Research</source>. <year>1997</year> <month>Dec</month>;<volume>37</volume>(<issue>23</issue>):<fpage>3327</fpage>–<lpage>3338</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0042-6989(97)00121-1" xlink:type="simple">10.1016/S0042-6989(97)00121-1</ext-link></comment> <object-id pub-id-type="pmid">9425547</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Law</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Cooper</surname> <given-names>LN</given-names></name>. <article-title>Formation of Receptive Fields in Realistic Visual Environments According to the Bienenstock, Cooper, and Munro (BCM) Theory</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1994</year> <month>Aug</month>;<volume>91</volume>(<issue>16</issue>):<fpage>7797</fpage>–<lpage>7801</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.91.16.7797" xlink:type="simple">10.1073/pnas.91.16.7797</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rehn</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>FT</given-names></name>. <article-title>A network that uses few active neurones to code visual input predicts the diverse shapes of cortical receptive fields</article-title>. <source>Journal of Computational Neuroscience</source>. <year>2007</year>;<volume>22</volume>(<issue>2</issue>):<fpage>135</fpage>–<lpage>146</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10827-006-0003-9" xlink:type="simple">10.1007/s10827-006-0003-9</ext-link></comment> <object-id pub-id-type="pmid">17053994</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Clopath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Busing</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Vasilaki</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>Connectivity reflects coding: a model of voltage-based STDP with homeostasis</article-title>. <source>Nature Neuroscience</source>. <year>2010</year>;<volume>13</volume>(<issue>3</issue>):<fpage>344</fpage>–<lpage>352</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2479" xlink:type="simple">10.1038/nn.2479</ext-link></comment> <object-id pub-id-type="pmid">20098420</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Savin</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Joshi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Triesch</surname> <given-names>J</given-names></name>. <article-title>Independent component analysis in spiking neurons</article-title>. <source>PLoS Computational Biology</source>. <year>2010</year>;<volume>6</volume>(<issue>4</issue>):<fpage>e1000757</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000757" xlink:type="simple">10.1371/journal.pcbi.1000757</ext-link></comment> <object-id pub-id-type="pmid">20421937</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Murphy</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>DeWeese</surname> <given-names>MR</given-names></name>. <article-title>A Sparse Coding Model with Synaptically Local Plasticity and Spiking Neurons Can Account for the Diverse Shapes of V1 Simple Cell Receptive Fields</article-title>. <source>PLoS Comput Biol</source>. <year>2011</year> <month>Oct</month>;<volume>7</volume>(<issue>10</issue>):<fpage>e1002250</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002250" xlink:type="simple">10.1371/journal.pcbi.1002250</ext-link></comment> <object-id pub-id-type="pmid">22046123</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Sparse coding of sensory inputs</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2004</year> <month>Aug</month>;<volume>14</volume>(<issue>4</issue>):<fpage>481</fpage>–<lpage>487</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2004.07.007" xlink:type="simple">10.1016/j.conb.2004.07.007</ext-link></comment> <object-id pub-id-type="pmid">15321069</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Smith</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>. <article-title>Efficient auditory coding</article-title>. <source>Nature</source>. <year>2006</year> <month>Feb</month>;<volume>439</volume>(<issue>7079</issue>):<fpage>978</fpage>–<lpage>982</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature04485" xlink:type="simple">10.1038/nature04485</ext-link></comment> <object-id pub-id-type="pmid">16495999</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Saxe</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bhand</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Mudur</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Suresh</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Ng</surname> <given-names>AY</given-names></name>. <article-title>Unsupervised learning models of primary cortical receptive fields and receptive field plasticity</article-title>. <source>Advances in neural information processing systems</source>. <year>2011</year>;p. <fpage>1971</fpage>–<lpage>1979</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref016">
<label>16</label>
<mixed-citation publication-type="other" xlink:type="simple">
Lee H, Ekanadham C, Ng A. Sparse deep belief net model for visual area V2. Advances in neural information processing systems. 2007;20.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yamins</surname> <given-names>DLK</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Seibert</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year> <month>May</month>;p. 201403112.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friedman</surname> <given-names>JH</given-names></name>. <article-title>Exploratory Projection Pursuit</article-title>. <source>Journal of the American Statistical Association</source>. <year>1987</year> <month>Mar</month>;<volume>82</volume>(<issue>397</issue>):<fpage>249</fpage>–<lpage>66</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/01621459.1987.10478427" xlink:type="simple">10.1080/01621459.1987.10478427</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref019">
<label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Oja E, Ogawa H, Wangviwattana J. Learning in nonlinear constrained Hebbian networks. Artificial Neural Networks. 1991;.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fyfe</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Baddeley</surname> <given-names>R</given-names></name>. <article-title>Non-linear data structure extraction using simple Hebbian networks</article-title>. <source>Biological Cybernetics</source>. <year>1995</year>;<volume>72</volume>(<issue>6</issue>):<fpage>533</fpage>–<lpage>541</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00199896" xlink:type="simple">10.1007/BF00199896</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Keller</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Stryker</surname> <given-names>MP</given-names></name>. <article-title>Ocular dominance column development: analysis and simulation</article-title>. <source>Science</source>. <year>1989</year> <month>Aug</month>;<volume>245</volume>(<issue>4918</issue>):<fpage>605</fpage>–<lpage>615</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.2762813" xlink:type="simple">10.1126/science.2762813</ext-link></comment> <object-id pub-id-type="pmid">2762813</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bienenstock</surname> <given-names>EL</given-names></name>, <name name-style="western"><surname>Cooper</surname> <given-names>LN</given-names></name>, <name name-style="western"><surname>Munro</surname> <given-names>PW</given-names></name>. <article-title>Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</article-title>. <source>The Journal of Neuroscience</source>. <year>1982</year> <month>Jan</month>;<volume>2</volume>(<issue>1</issue>):<fpage>32</fpage>–<lpage>48</lpage>. <object-id pub-id-type="pmid">7054394</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref023">
<label>23</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Kistler</surname> <given-names>WM</given-names></name>, <name name-style="western"><surname>Naud</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <source>Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition</source>. <publisher-name>Cambridge University Press</publisher-name>; <year>2014</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pfister</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>Triplets of Spikes in a Model of Spike Timing-Dependent Plasticity</article-title>. <source>The Journal of Neuroscience</source>. <year>2006</year>;<volume>26</volume>(<issue>38</issue>):<fpage>9673</fpage>–<lpage>9682</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1425-06.2006" xlink:type="simple">10.1523/JNEUROSCI.1425-06.2006</ext-link></comment> <object-id pub-id-type="pmid">16988038</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wiesel</surname> <given-names>TN</given-names></name>, <name name-style="western"><surname>Hubel</surname> <given-names>DH</given-names></name>, <etal>et al</etal>. <article-title>Single-cell responses in striate cortex of kittens deprived of vision in one eye</article-title>. <source>Journal of Neurophysiology</source>. <year>1963</year>;<volume>26</volume>(<issue>6</issue>):<fpage>1003</fpage>–<lpage>1017</lpage>. <object-id pub-id-type="pmid">14084161</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pozzorini</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Naud</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Mensi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>Temporal whitening by power-law adaptation in neocortical neurons</article-title>. <source>Nature Neuroscience</source>. <year>2013</year> <month>Jul</month>;<volume>16</volume>(<issue>7</issue>):<fpage>942</fpage>–<lpage>948</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3431" xlink:type="simple">10.1038/nn.3431</ext-link></comment> <object-id pub-id-type="pmid">23749146</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sjostrom</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Turrigiano</surname> <given-names>GG</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>SB</given-names></name>. <article-title>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity</article-title>. <source>Neuron</source>. <year>2001</year>;<volume>32</volume>(<issue>6</issue>):<fpage>1149</fpage>–<lpage>1164</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(01)00542-6" xlink:type="simple">10.1016/S0896-6273(01)00542-6</ext-link></comment> <object-id pub-id-type="pmid">11754844</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref028">
<label>28</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Cooper</surname> <given-names>LN</given-names></name>, <name name-style="western"><surname>Intrator</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Blais</surname> <given-names>BS</given-names></name>, <name name-style="western"><surname>Shouval</surname> <given-names>HZ</given-names></name>. <source>Theory of Cortical Plasticity</source>. <publisher-name>World Scientific Pub Co Inc</publisher-name>; <year>2004</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zenke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Hennequin</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>Synaptic plasticity in neural networks needs homeostasis with a fast rate detector</article-title>. <source>PLoS Computational Biology</source>. <year>2013</year>;<volume>9</volume>(<issue>11</issue>):<fpage>e1003330</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003330" xlink:type="simple">10.1371/journal.pcbi.1003330</ext-link></comment> <object-id pub-id-type="pmid">24244138</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Turrigiano</surname> <given-names>G</given-names></name>. <article-title>Too many cooks? Intrinsic and synaptic homeostatic mechanisms in cortical circuit refinement</article-title>. <source>Annual review of neuroscience</source>. <year>2011</year>;<volume>34</volume>:<fpage>89</fpage>–<lpage>103</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-neuro-060909-153238" xlink:type="simple">10.1146/annurev-neuro-060909-153238</ext-link></comment> <object-id pub-id-type="pmid">21438687</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Song</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Competitive Hebbian learning through spike-timing-dependent synaptic plasticity</article-title>. <source>Nature Neuroscience</source>. <year>2000</year>;<volume>3</volume>(<issue>9</issue>):<fpage>919</fpage>–<lpage>926</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/78829" xlink:type="simple">10.1038/78829</ext-link></comment> <object-id pub-id-type="pmid">10966623</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Kempter</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Van Hemmen</surname> <given-names>JL</given-names></name>. <article-title>A neuronal learning rule for sub-millisecond temporal coding</article-title>. <source>Nature</source>. <year>1996</year>;<volume>383</volume>(<issue>6595</issue>):<fpage>76</fpage>–<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/383076a0" xlink:type="simple">10.1038/383076a0</ext-link></comment> <object-id pub-id-type="pmid">8779718</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref033">
<label>33</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <source>Theoretical neuroscience</source>. <volume>vol. 31</volume>. <publisher-name>MIT press</publisher-name> <publisher-loc>Cambridge, MA</publisher-loc>; <year>2001</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rozell</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Baraniuk</surname> <given-names>RG</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>. <article-title>Sparse Coding via Thresholding and Local Competition in Neural Circuits</article-title>. <source>Neural Computation</source>. <year>2008</year>;<volume>20</volume>(<issue>10</issue>):<fpage>2526</fpage>–<lpage>2563</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2008.03-07-486" xlink:type="simple">10.1162/neco.2008.03-07-486</ext-link></comment> <object-id pub-id-type="pmid">18439138</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Oja</surname> <given-names>E</given-names></name>. <article-title>Simplified neuron model as a principal component analyzer</article-title>. <source>Journal of mathematical biology</source>. <year>1982</year>;<volume>15</volume>(<issue>3</issue>):<fpage>267</fpage>–<lpage>273</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00275687" xlink:type="simple">10.1007/BF00275687</ext-link></comment> <object-id pub-id-type="pmid">7153672</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hyvarinen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Oja</surname> <given-names>E</given-names></name>. <article-title>Independent component analysis: algorithms and applications</article-title>. <source>Neural Networks</source>. <year>2000</year>;<volume>13</volume>(<issue>4-5</issue>):<fpage>411</fpage>–<lpage>430</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0893-6080(00)00026-5" xlink:type="simple">10.1016/S0893-6080(00)00026-5</ext-link></comment> <object-id pub-id-type="pmid">10946390</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Sparse coding with an overcomplete basis set: A strategy employed by V1?</article-title> <source>Vision research</source>. <year>1997</year>;<volume>37</volume>(<issue>23</issue>):<fpage>3311</fpage>–<lpage>3325</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0042-6989(97)00169-7" xlink:type="simple">10.1016/S0042-6989(97)00169-7</ext-link></comment> <object-id pub-id-type="pmid">9425546</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hyvarinen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Oja</surname> <given-names>E</given-names></name>. <article-title>Independent component analysis by general nonlinear Hebbian-like learning rules</article-title>. <source>Signal Processing</source>. <year>1998</year> <month>Feb</month>;<volume>64</volume>(<issue>3</issue>):<fpage>301</fpage>–<lpage>313</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0165-1684(97)00197-7" xlink:type="simple">10.1016/S0165-1684(97)00197-7</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ruderman</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Statistics of natural images: Scaling in the woods</article-title>. <source>Physical Review Letters</source>. <year>1994</year> <month>Aug</month>;<volume>73</volume>(<issue>6</issue>):<fpage>814</fpage>–<lpage>817</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.73.814" xlink:type="simple">10.1103/PhysRevLett.73.814</ext-link></comment> <object-id pub-id-type="pmid">10057546</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ringach</surname> <given-names>DL</given-names></name>. <article-title>Spatial Structure and Symmetry of Simple-Cell Receptive Fields in Macaque Primary Visual Cortex</article-title>. <source>Journal of Neurophysiology</source>. <year>2002</year> <month>Jul</month>;<volume>88</volume>(<issue>1</issue>):<fpage>455</fpage>–<lpage>463</lpage>. <object-id pub-id-type="pmid">12091567</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hateren</surname> <given-names>JHv</given-names></name>, <name name-style="western"><surname>Schaaf</surname> <given-names>Avd</given-names></name>. <article-title>Independent component filters of natural images compared with simple cells in primary visual cortex</article-title>. <source>Proceedings of the Royal Society of London Series B: Biological Sciences</source>. <year>1998</year> <month>Mar</month>;<volume>265</volume>(<issue>1394</issue>):<fpage>359</fpage>–<lpage>366</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rspb.1998.0303" xlink:type="simple">10.1098/rspb.1998.0303</ext-link></comment> <object-id pub-id-type="pmid">9523437</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Foldiak</surname> <given-names>P</given-names></name>. <article-title>Forming sparse representations by local anti-Hebbian learning</article-title>. <source>Biological cybernetics</source>. <year>1990</year>;<volume>64</volume>(<issue>2</issue>):<fpage>165</fpage>–<lpage>170</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF02331346" xlink:type="simple">10.1007/BF02331346</ext-link></comment> <object-id pub-id-type="pmid">2291903</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vogels</surname> <given-names>TP</given-names></name>, <name name-style="western"><surname>Sprekeler</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Zenke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Clopath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks</article-title>. <source>Science</source>. <year>2011</year>;<volume>334</volume>(<issue>6062</issue>):<fpage>1569</fpage>–<lpage>1573</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1211095" xlink:type="simple">10.1126/science.1211095</ext-link></comment> <object-id pub-id-type="pmid">22075724</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>King</surname> <given-names>PD</given-names></name>, <name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>DeWeese</surname> <given-names>MR</given-names></name>. <article-title>Inhibitory Interneurons Decorrelate Excitatory Cells to Drive Sparse Code Formation in a Spiking Model of V1</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year> <month>Mar</month>;<volume>33</volume>(<issue>13</issue>):<fpage>5475</fpage>–<lpage>5485</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4188-12.2013" xlink:type="simple">10.1523/JNEUROSCI.4188-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23536063</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Atick</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Redlich</surname> <given-names>AN</given-names></name>. <article-title>What Does the Retina Know about Natural Scenes?</article-title> <source>Neural Computation</source>. <year>1992</year> <month>Mar</month>;<volume>4</volume>(<issue>2</issue>):<fpage>196</fpage>–<lpage>210</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref046">
<label>46</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Hyvarinen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hurri</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hoyer</surname> <given-names>PO</given-names></name>. <source>Natural Image Statistics: A Probabilistic Approach to Early Computational Vision</source>. <volume>vol. 39</volume>. <publisher-name>Springer</publisher-name>; <year>2009</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hunt</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Goodhill</surname> <given-names>GJ</given-names></name>. <article-title>Sparse Coding Can Predict Primary Visual Cortex Receptive Field Changes Induced by Abnormal Visual Input</article-title>. <source>PLoS Computational Biology</source>. <year>2013</year> <month>May</month>;<volume>9</volume>(<issue>5</issue>):<fpage>e1003005</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003005" xlink:type="simple">10.1371/journal.pcbi.1003005</ext-link></comment> <object-id pub-id-type="pmid">23675290</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barth</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Poulet</surname> <given-names>JFA</given-names></name>. <article-title>Experimental evidence for sparse firing in the neocortex</article-title>. <source>Trends in Neurosciences</source>. <year>2012</year> <month>Jun</month>;<volume>35</volume>(<issue>6</issue>):<fpage>345</fpage>–<lpage>355</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2012.03.008" xlink:type="simple">10.1016/j.tins.2012.03.008</ext-link></comment> <object-id pub-id-type="pmid">22579264</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eichhorn</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sinz</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>. <article-title>Natural image coding in v1: How much use is orientation selectivity?</article-title> <source>PLoS Computational Biology</source>. <year>2009</year>;<volume>5</volume>(<issue>4</issue>):<fpage>e1000336</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000336" xlink:type="simple">10.1371/journal.pcbi.1000336</ext-link></comment> <object-id pub-id-type="pmid">19343216</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zoran</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Weiss</surname> <given-names>Y</given-names></name>. <article-title>Natural images, gaussian mixtures and dead leaves</article-title>. In: <source>Advances in Neural Information Processing Systems</source>; <year>2012</year>. p. <fpage>1736</fpage>–<lpage>1744</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Legenstein</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Pecevski</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>A learning theory for reward-modulated spike-timing-dependent-plasticity with application to biofeedback</article-title>. <source>PLoS Computational Biology</source>. <year>2008</year>;<volume>4</volume>(<issue>10</issue>):<fpage>e1000180</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000180" xlink:type="simple">10.1371/journal.pcbi.1000180</ext-link></comment> <object-id pub-id-type="pmid">18846203</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fremaux</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Sprekeler</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>Functional Requirements for Reward-Modulated Spike-Timing-Dependent Plasticity</article-title>. <source>The Journal of Neuroscience</source>. <year>2010</year> <month>Oct</month>;<volume>30</volume>(<issue>40</issue>):<fpage>13326</fpage>–<lpage>13337</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.6249-09.2010" xlink:type="simple">10.1523/JNEUROSCI.6249-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20926659</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brea</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Pfister</surname> <given-names>JP</given-names></name>. <article-title>Matching Recall and Storage in Sequence Learning with Spiking Neural Networks</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year> <month>Jun</month>;<volume>33</volume>(<issue>23</issue>):<fpage>9565</fpage>–<lpage>9575</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4098-12.2013" xlink:type="simple">10.1523/JNEUROSCI.4098-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23739954</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rezende</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>Stochastic variational learning in recurrent spiking networks</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2014</year>;<volume>8</volume>:<fpage>38</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Clopath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Longtin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>An online Hebbian learning rule that performs Independent Component Analysis</article-title>. <source>BMC Neuroscience</source>. <year>2008</year>;<volume>9</volume>(<issue>Suppl 1</issue>):<fpage>O13</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1471-2202-9-S1-O13" xlink:type="simple">10.1186/1471-2202-9-S1-O13</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gjorgjieva</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Clopath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Audet</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Pfister</surname> <given-names>JP</given-names></name>. <article-title>A triplet spike-timing-dependent plasticity model generalizes the Bienenstock-Cooper-Munro rule to higher-order spatiotemporal correlations</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year> <month>Nov</month>;<volume>108</volume>(<issue>48</issue>):<fpage>19383</fpage>–<lpage>19388</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1105933108" xlink:type="simple">10.1073/pnas.1105933108</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gilbert</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Sigman</surname> <given-names>M</given-names></name>. <article-title>Brain States: Top-Down Influences in Sensory Processing</article-title>. <source>Neuron</source>. <year>2007</year> <month>Jun</month>;<volume>54</volume>(<issue>5</issue>):<fpage>677</fpage>–<lpage>696</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.05.019" xlink:type="simple">10.1016/j.neuron.2007.05.019</ext-link></comment> <object-id pub-id-type="pmid">17553419</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lee</surname> <given-names>TS</given-names></name>, <name name-style="western"><surname>Mumford</surname> <given-names>D</given-names></name>. <article-title>Hierarchical Bayesian inference in the visual cortex</article-title>. <source>JOSA A</source>. <year>2003</year>;<volume>20</volume>(<issue>7</issue>):<fpage>1434</fpage>–<lpage>1448</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1364/JOSAA.20.001434" xlink:type="simple">10.1364/JOSAA.20.001434</ext-link></comment> <object-id pub-id-type="pmid">12868647</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref059">
<label>59</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Salakhutdinov</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <chapter-title>Deep Boltzmann machines</chapter-title>. In: <source>Proceedings of the International Conference on Artificial Intelligence and Statistics</source>. <volume>vol. 5</volume>; <year>2009</year>. p. <fpage>448</fpage>–<lpage>455</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref060">
<label>60</label>
<mixed-citation publication-type="other" xlink:type="simple">Series P, Reichert DP, Storkey AJ. Hallucinations in Charles Bonnet syndrome induced by homeostasis: a deep Boltzmann machine model. In: Advances in Neural Information Processing Systems; 2010. p. 2020–2028.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Marr</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>From understanding computation to understanding neural circuitry</article-title>. <source>Neurosciences Research Program Bulletin</source>. <year>1977</year>;<volume>15</volume>(<issue>3</issue>):<fpage>470</fpage>–<lpage>488</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Atick</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Redlich</surname> <given-names>AN</given-names></name>. <article-title>Towards a theory of early visual processing</article-title>. <source>Neural Computation</source>. <year>1990</year>;<volume>2</volume>(<issue>3</issue>):<fpage>308</fpage>–<lpage>320</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1990.2.3.308" xlink:type="simple">10.1162/neco.1990.2.3.308</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>KD</given-names></name>. <article-title>A model for the development of simple cell receptive fields and the ordered arrangement of orientation columns through activity-dependent competition between ON-and OFF-center inputs</article-title>. <source>Journal of Neuroscience</source>. <year>1994</year>;<volume>14</volume>:<fpage>409</fpage>–<lpage>409</lpage>. <object-id pub-id-type="pmid">8283248</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chen</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Lonjers</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Chistiakova</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Volgushev</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bazhenov</surname> <given-names>M</given-names></name>. <article-title>Heterosynaptic Plasticity Prevents Runaway Synaptic Dynamics</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year> <month>Oct</month>;<volume>33</volume>(<issue>40</issue>):<fpage>15915</fpage>–<lpage>15929</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5088-12.2013" xlink:type="simple">10.1523/JNEUROSCI.5088-12.2013</ext-link></comment> <object-id pub-id-type="pmid">24089497</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Elliott</surname> <given-names>T</given-names></name>. <article-title>Sparseness, Antisparseness and Anything in Between: The Operating Point of a Neuron Determines Its Computational Repertoire</article-title>. <source>Neural Computation</source>. <year>2014</year>;p. <fpage>1</fpage>–<lpage>49</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005070.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Prinz</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Bucher</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Marder</surname> <given-names>E</given-names></name>. <article-title>Similar network activity from disparate circuit parameters</article-title>. <source>Nature Neuroscience</source>. <year>2004</year> <month>Dec</month>;<volume>7</volume>(<issue>12</issue>):<fpage>1345</fpage>–<lpage>1352</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1352" xlink:type="simple">10.1038/nn1352</ext-link></comment> <object-id pub-id-type="pmid">15558066</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref067">
<label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sharma</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Angelucci</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sur</surname> <given-names>M</given-names></name>. <article-title>Induction of visual orientation modules in auditory cortex</article-title>. <source>Nature</source>. <year>2000</year> <month>Apr</month>;<volume>404</volume>(<issue>6780</issue>):<fpage>841</fpage>–<lpage>847</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/35009043" xlink:type="simple">10.1038/35009043</ext-link></comment> <object-id pub-id-type="pmid">10786784</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref068">
<label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kaschube</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schnabel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lowel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Coppola</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>White</surname> <given-names>LE</given-names></name>, <name name-style="western"><surname>Wolf</surname> <given-names>F</given-names></name>. <article-title>Universality in the Evolution of Orientation Columns in the Visual Cortex</article-title>. <source>Science</source>. <year>2010</year> <month>Nov</month>;<volume>330</volume>(<issue>6007</issue>):<fpage>1113</fpage>–<lpage>1116</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1194869" xlink:type="simple">10.1126/science.1194869</ext-link></comment> <object-id pub-id-type="pmid">21051599</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref069">
<label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sjostrom</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Rancz</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Roth</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hausser</surname> <given-names>M</given-names></name>. <article-title>Dendritic Excitability and Synaptic Plasticity</article-title>. <source>Physiol Rev</source>. <year>2008</year> <month>Apr</month>;<volume>88</volume>(<issue>2</issue>):<fpage>769</fpage>–<lpage>840</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/physrev.00016.2007" xlink:type="simple">10.1152/physrev.00016.2007</ext-link></comment> <object-id pub-id-type="pmid">18391179</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Graupner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>Calcium-based plasticity model explains sensitivity of synaptic changes to spike pattern, rate, and dendritic location</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2012</year> <month>Mar</month>;<volume>109</volume>(<issue>10</issue>):<fpage>3991</fpage>–<lpage>3996</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1109359109" xlink:type="simple">10.1073/pnas.1109359109</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005070.ref071">
<label>71</label>
<mixed-citation publication-type="other" xlink:type="simple">Kominek J, Black AW. The CMU Arctic speech databases. In: Fifth ISCA Workshop on Speech Synthesis; 2004.</mixed-citation>
</ref>
</ref-list>
</back>
</article>