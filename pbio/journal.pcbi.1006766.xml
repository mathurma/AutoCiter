<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006766</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00680</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Grammar</subject><subj-group><subject>Phonology</subject><subj-group><subject>Phonemes</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Inferior colliculus</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Inferior colliculus</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Auditory pathway</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Auditory pathway</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Auditory pathway</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Grammar</subject><subj-group><subject>Phonology</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A hierarchical sparse coding model predicts acoustic feature encoding in both auditory midbrain and cortex</article-title>
<alt-title alt-title-type="running-head">A hierarchical sparse coding model for the auditory pathway</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Zhang</surname>
<given-names>Qingtian</given-names>
</name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="fn" rid="currentaff001"><sup>¤</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4907-7354</contrib-id>
<name name-style="western">
<surname>Hu</surname>
<given-names>Xiaolin</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Hong</surname>
<given-names>Bo</given-names>
</name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Zhang</surname>
<given-names>Bo</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Computer Science and Technology, Tsinghua University, Beijing, China</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Center for Brain-Inspired Computing Research (CBICR), Tsinghua University, Beijing, China</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>School of Medicine, Tsinghua University, Beijing, China</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Theunissen</surname>
<given-names>Frédéric E.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of California at Berkeley, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<label>¤</label>
<p>Current address: Institute of Microelectronics, Tsinghua University, Beijing, China.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">xlhu@tsinghua.edu.cn</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>11</day>
<month>2</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>2</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>2</issue>
<elocation-id>e1006766</elocation-id>
<history>
<date date-type="received">
<day>29</day>
<month>4</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>21</day>
<month>12</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Zhang et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006766"/>
<abstract>
<p>The auditory pathway consists of multiple stages, from the cochlear nucleus to the auditory cortex. Neurons acting at different stages have different functions and exhibit different response properties. It is unclear whether these stages share a common encoding mechanism. We trained an unsupervised deep learning model consisting of alternating sparse coding and max pooling layers on cochleogram-filtered human speech. Evaluation of the response properties revealed that computing units in lower layers exhibited spectro-temporal receptive fields (STRFs) similar to those of inferior colliculus neurons measured in physiological experiments, including properties such as sound onset and termination, checkerboard pattern, and spectral motion. Units in upper layers tended to be tuned to phonetic features such as plosivity and nasality, resembling the results of field recording in human auditory cortex. Variation of the sparseness level of the units in each higher layer revealed a positive correlation between the sparseness level and the strength of phonetic feature encoding. The activities of the units in the top layer, but not other layers, correlated with the dynamics of the first two formants (F1, F2) of all phonemes, indicating the encoding of phoneme dynamics in these units. These results suggest that the principles of sparse coding and max pooling may be universal in the human auditory pathway.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>When speech enters the ear, it is subjected to a series of processing stages prior to arriving at the auditory cortex. Neurons acting at different processing stages have different response properties. For example, at the auditory midbrain, a neuron may specifically detect the onsets of a frequency component in the speech, whereas in the auditory cortex, a neuron may specifically detect phonetic features. The encoding mechanisms underlying these neuronal functions remain unclear. To address this issue, we designed a hierarchical sparse coding model, inspired by the sparse activity of neurons in the sensory system, to learn features in speech signals. We found that the computing units in different layers exhibited hierarchical extraction of speech sound features, similar to those of neurons in the auditory midbrain and auditory cortex, although the computational principles in these layers were the same. The results suggest that sparse coding and max pooling represent universal computational principles throughout the auditory pathway.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001809</institution-id>
<institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>61332007</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4907-7354</contrib-id>
<name name-style="western">
<surname>Hu</surname>
<given-names>Xiaolin</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001809</institution-id>
<institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>61621136008</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4907-7354</contrib-id>
<name name-style="western">
<surname>Hu</surname>
<given-names>Xiaolin</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>XH was supported in part by the National Natural Science Foundation of China under Grant Nos. 61621136008, 61332007 and 61836014. Website of the funder: <ext-link ext-link-type="uri" xlink:href="http://www.nsfc.gov.cn/" xlink:type="simple">http://www.nsfc.gov.cn/</ext-link>. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="2"/>
<page-count count="23"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-02-22</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Hearing is supported by a series of interconnected brain areas, collectively called the central auditory system or auditory pathway [<xref ref-type="bibr" rid="pcbi.1006766.ref001">1</xref>]. This pathway is thought to function as a series of hierarchical processing stages that encode features ranging from simple acoustic features and elementary time–frequency representations in the cochlea and inferior colliculus to complex phonetic features, phonemes, syllables, words, and grammatical features in the auditory cortex [<xref ref-type="bibr" rid="pcbi.1006766.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1006766.ref008">8</xref>]. However, it remains unclear how neurons encode these distinctive features, especially at higher stages.</p>
<p>The encoding mechanisms of neurons in the auditory pathway have been addressed in many studies, and various encoding mechanisms have been proposed. These include spatial coding in the cochlea [<xref ref-type="bibr" rid="pcbi.1006766.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1006766.ref011">11</xref>], spatial coding and temporal coding in the inferior colliculus [<xref ref-type="bibr" rid="pcbi.1006766.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1006766.ref016">16</xref>], and spatial coding and periodicity coding in the auditory cortex [<xref ref-type="bibr" rid="pcbi.1006766.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1006766.ref019">19</xref>]. However, these studies only describe the experimental data, and do not explain why the experiments yield specific outcomes. A notable exception is the sparse coding model, which assumes that neurons encode external stimuli using sparse codes. The model was originally proposed to explain the properties of simple cells in the primary visual cortex [<xref ref-type="bibr" rid="pcbi.1006766.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref021">21</xref>], but has been extended to explain the emergence of the response properties of auditory nerve fibers [<xref ref-type="bibr" rid="pcbi.1006766.ref022">22</xref>] and inferior colliculus neurons [<xref ref-type="bibr" rid="pcbi.1006766.ref023">23</xref>]. Another exception is a deep learning model that has been used to explain the emergence of response properties of neurons to speech in the auditory cortex [<xref ref-type="bibr" rid="pcbi.1006766.ref024">24</xref>]. Because the model was trained to recognize 40 English phonemes in a supervised fashion, high discrimination ability is critical for its success. These two models effectively explained certain neural response properties at different stages along the auditory pathway; however, their underlying computational principles are different. A fundamental question is whether the auditory system uses the same or different principles at different stages. We explored the former possibility in this study.</p>
<p>Our initial assumption was that sparse coding plays an important role in shaping neural response properties along the auditory pathway. Support for this is provided by the ubiquitous sparse firing of neurons in the auditory pathway [<xref ref-type="bibr" rid="pcbi.1006766.ref025">25</xref>–<xref ref-type="bibr" rid="pcbi.1006766.ref027">27</xref>], and sparse coding computational models have effectively interpreted experimental data recorded at certain stages of the auditory system [<xref ref-type="bibr" rid="pcbi.1006766.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref023">23</xref>]. We extended this model to multiple layers by introducing spatial pooling after each layer, resulting in an unsupervised deep learning model, which we named sparse HMAX (SHMAX) [<xref ref-type="bibr" rid="pcbi.1006766.ref028">28</xref>]. We studied the response properties of the computing units, called <italic>artificial neurons</italic>, in the model. After training the model on the cochleogram of speech, the spectro-temporal receptive fields (STRFs) of artificial neurons in lower layers in the model exhibited patterns similar to those of neurons in the inferior colliculus, whereas the responses of artificial neurons in the upper layers resembled the results of field recordings in human auditory cortex. The agreement between the model and neural data suggest that, although the features encoded at different levels of the auditory pathway differ, the encoding mechanisms are similar.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>A hierarchical sparse coding model, SHMAX [<xref ref-type="bibr" rid="pcbi.1006766.ref028">28</xref>], was trained on the cochleogram of English speech, which consisted of six S layers and six C layers in alternation (i.e., S1-C1-S2-C2…; <xref ref-type="fig" rid="pcbi.1006766.g001">Fig 1C</xref>). The S layers perform sparse coding and the C layers perform max pooling to integrate feature specificity and invariance, respectively, both of which are important for recognition. Each layer has multiple feature maps (<xref ref-type="fig" rid="pcbi.1006766.g001">Fig 1D–1I</xref>), and each feature map consists of the responses of artificial neurons with the same receptive field shape but different displacements in the input space. In the model, the number of feature maps increases from 100 in layers S1 and C1 to 500 in layers S6 and C6. This setting does not reflect biological facts, but is intended simply to balance the computing resource consumption in different layers because the feature maps are larger in lower layers and smaller in higher layers. The results obtained with other settings would be similar to those presented here if the number of feature maps in each layer was large enough.</p>
<fig id="pcbi.1006766.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006766.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Stimuli and experimental protocol.</title>
<p>(a) Example stimulus. (b) Cochleogram of the example stimulus. (c) Structure of SHMAX, which consists of alternate sparse coding layers (S layers) and max pooling layers (C layers). To avoid clutter, only S layers are displayed. The height of the feature maps in each S layer is indicated on the left, and the number of feature maps in each S layer is indicated at the top. The width of the feature maps (the temporal dimension) is not indicated because it varies according to the length of the input sentence. (d, e) Two example feature maps (activations of two features in response to the example stimulus) in layer S1. (f, g) Two example feature maps in layer S2. (h, i) Two example feature maps in layer S3.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.g001" xlink:type="simple"/>
</fig>
<p>We hypothesized that lower layers correspond to lower-level stages in the auditory pathway such as the inferior colliculus, whereas higher layers correspond to higher-level stages such as the auditory cortex. We compared the response properties of the artificial neurons in different layers to those of real neurons in the inferior colliculus and auditory cortex.</p>
<sec id="sec003">
<title>Response properties of the lower-level units resembled those of the auditory midbrain neurons</title>
<p>We first confirmed that the computing units in lower layers of the model could capture the firing properties of the inferior colliculus neurons in the auditory midbrain. This capability has been proven in a single-layer sparse coding model [<xref ref-type="bibr" rid="pcbi.1006766.ref023">23</xref>]; however, the time window of the bases in that model was too large (216 ms), and it was unclear whether a much smaller time window such as 20 ms, which is more compatible with physiological data [<xref ref-type="bibr" rid="pcbi.1006766.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref030">30</xref>], would yield similar results. In addition, an overly large time window in the first layer causes difficulty in constructing deep models so that the time windows of higher-layer units agree with those of cortical neurons. Our model started with a time window of 10 ms in layer S1 and ended with a time window of 194 ms in layer C6. In the following, we will show that, with these settings, several lower layers of the model, rather than only the lowest layer, can generate results qualitatively similar to those obtained in the previous study [<xref ref-type="bibr" rid="pcbi.1006766.ref023">23</xref>].</p>
<p>The response properties of an inferior colliculus neuron are usually delineated by STRFs, which are obtained by averaging the spectro-temporal structure of acoustic stimuli before a spike is fired [<xref ref-type="bibr" rid="pcbi.1006766.ref031">31</xref>]. The STRFs of the S1 units can be approximated by the bases used to reconstruct the stimuli [<xref ref-type="bibr" rid="pcbi.1006766.ref023">23</xref>]. To visualize the STRFs of higher-level units, we linearly combined the bases of units in previous layers (see <xref ref-type="sec" rid="sec013">Materials and Methods</xref>; <xref ref-type="fig" rid="pcbi.1006766.g002">Fig 2A</xref>; <xref ref-type="supplementary-material" rid="pcbi.1006766.s001">S1 Fig</xref>). Several example STRFs of units in the first three S layers are shown in <xref ref-type="fig" rid="pcbi.1006766.g002">Fig 2B–2D</xref>, and the full results are shown in <xref ref-type="supplementary-material" rid="pcbi.1006766.s002">S2 Fig</xref>. This visualization method is simple, fast, and applicable to lower-layer units. In fact, the results are similar to the STRFs obtained by the normalized reverse-correlation method [<xref ref-type="bibr" rid="pcbi.1006766.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref033">33</xref>] (see <xref ref-type="sec" rid="sec013">Materials and Methods</xref>; <xref ref-type="supplementary-material" rid="pcbi.1006766.s003">S3 Fig</xref>). However, in deeper layers, the STRFs become larger and more complex, and exhibit stronger nonlinearity, and are therefore difficult to capture with a linear method.</p>
<fig id="pcbi.1006766.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006766.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Calculation of STRFs and example STRFs.</title>
<p>(a) Illustration of the visualization of an S2 unit whose basis has size 2 × 2 × <italic>u</italic><sup>1</sup>, where <italic>u</italic><sup>1</sup> denotes the total number of S1 bases. The size of each S1 basis is 3 × 3. Suppose that there is a down-sampling operation with ratio 2 between layer S1 and layer S2, which could be a convolution with stride 2 in layer S2 (the case in this study) or a max pooling with ratio 2 and stride 2. In that case, we first need to expand each slice of the S2 unit, a 2 × 2 matrix, to a 4 × 4 matrix. Because there is a max pooling layer with pooling ratio 2 and stride 1 between layers S1 and S2, the first two dimensions of the S2 feature maps are 1 smaller than those of the S1 feature maps. To account for this effect, we pad zeros around the 4 × 4 matrices to obtain 5 × 5 matrices. Each 5×5 slice can be viewed as learned on the feature map, which is obtained by convolving an S1 basis on its previous layer, the input image. Then, the effect of this 5 × 5 slice in layer S1 is roughly equivalent to that of a 7 × 7 matrix (shown on the right in the dashed box) formed by summing the same S1 basis centered at 25 locations and weighted by the corresponding elements in the slice. For illustration, on the left in the dashed box, the sum of the S1 basis weighted by two elements (red and green) in each slice is shown. The STRF of the example S2 unit is the sum of all <italic>u</italic><sup>1</sup> 7 × 7 matrices. (b) Example STRFs in layer S1. (c) Example STRFs in layer S2. (d) Example STRFs in layer S3.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.g002" xlink:type="simple"/>
</fig>
<p>By visual inspection, all of the first three S layers exhibited a certain degree of agreement with physiological data collected from the inferior colliculus, with layer S2 providing the best match in terms of STRF size and pattern (a quantitative comparison will be provided later). Representative bases of layer S2 units are visualized in <xref ref-type="fig" rid="pcbi.1006766.g003">Fig 3</xref>. Most layer S2 bases had both excitatory regions and inhibitory regions. Specifically, some units favored excitation first, followed by inhibition at the same frequency (<xref ref-type="fig" rid="pcbi.1006766.g003">Fig 3A</xref>). This pattern has been observed in STRFs of inferior colliculus neurons in cats [<xref ref-type="bibr" rid="pcbi.1006766.ref034">34</xref>]. By contrast, some layer S2 units favored inhibition first, followed by excitation at the same frequency (<xref ref-type="fig" rid="pcbi.1006766.g003">Fig 3B</xref>). This pattern has been observed in STRFs of inferior colliculus neurons in gerbils [<xref ref-type="bibr" rid="pcbi.1006766.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref035">35</xref>]. Some layer S2 bases had localized checkerboard patterns in their STRFs (<xref ref-type="fig" rid="pcbi.1006766.g003">Fig 3C</xref>), also as in inferior colliculus neurons in gerbils [<xref ref-type="bibr" rid="pcbi.1006766.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref035">35</xref>]. Some layer S2 units were selective to spectral motion (<xref ref-type="fig" rid="pcbi.1006766.g003">Fig 3D</xref>), as in certain inferior colliculus neurons in Mexican free-tailed bat that are tuned to motion cues present in conspecific vocalizations [<xref ref-type="bibr" rid="pcbi.1006766.ref029">29</xref>]. In those studies, the favored frequencies of layer S2 bases differed from those of inferior colliculus neurons because the experiments were carried out on gerbils and cats, whereas our deep learning model was trained on human speech data with a lower range of frequency content.</p>
<fig id="pcbi.1006766.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006766.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Visualization of the representative bases in layer S2 along with typical STRFs of the inferior colliculus neurons in animals.</title>
<p>(a–d) STRFs of several typical layer S2 units. Curves denote the spectral and temporal profiles obtained by SVD. (a) Two ON-type units. (b) Two OFF-type units. (c) Two localized checkerboard units. (d) Two spectral motion units. Similar STRFs of typical inferior colliculus neurons have been observed in physiological experiments. One can compare (a) with Fig 3E in [<xref ref-type="bibr" rid="pcbi.1006766.ref034">34</xref>], (b) with Fig 6A in [<xref ref-type="bibr" rid="pcbi.1006766.ref023">23</xref>], (c) with Fig 7A in [<xref ref-type="bibr" rid="pcbi.1006766.ref023">23</xref>] and (d) with Fig 6C in [<xref ref-type="bibr" rid="pcbi.1006766.ref029">29</xref>].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.g003" xlink:type="simple"/>
</fig>
<p>We calculated the distributions of four parameters that characterized a STRF over all layer S1, S2, and S3 units, separately: best temporal modulation frequency (Best T), response duration (Duration), center frequency (Center F), and spectral bandwidth (Bandwidth). <xref ref-type="fig" rid="pcbi.1006766.g004">Fig 4A–4D</xref> shows the results of layer S2 units. The shapes of the distributions of layer S2 units were most similar to those of inferior colliculus neurons in cats [<xref ref-type="bibr" rid="pcbi.1006766.ref030">30</xref>] (<xref ref-type="fig" rid="pcbi.1006766.g004">Fig 4F–4I</xref>). The scatterplot of Best T and spectral modulation (<xref ref-type="fig" rid="pcbi.1006766.g004">Fig 4E</xref>) indicated a tradeoff between the temporal modulation and spectral modulation among layer S2 units; i.e., units with both high temporal modulation (Best T) and spectral modulation were scarce. This result agrees with observations made in inferior colliculus neurons of cats [<xref ref-type="bibr" rid="pcbi.1006766.ref034">34</xref>].</p>
<fig id="pcbi.1006766.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006766.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Distributions of STRF parameters of layer S2 units.</title>
<p>(a) Best temporal modulation frequency. (b) Response duration. (c) Center frequencies. (d) Spectral bandwidth. These four parameters respectively correspond to the peak and bandwidth with 90% power of the temporal and spectral profiles shown in <xref ref-type="fig" rid="pcbi.1006766.g003">Fig 3</xref>. (e) Tradeoff between temporal modulation (Best T) and spectral modulation. (f–i) Probability distribution of STRF parameters normalized from the corresponding histograms. For comparison, the normalized probability distributions in layers S1 and S3 and the reference distributions of inferior colliculus neurons in cats [<xref ref-type="bibr" rid="pcbi.1006766.ref030">30</xref>] are also plotted. The horizontal axis in each panel is normalized to [0, 1] by dividing all values by the maximum value.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec004">
<title>Response properties of higher-layer units resembled those of field recordings in human auditory cortex</title>
<p>We hypothesized that the higher layers in the network would correspond to the auditory cortex. A previous study using cortical surface recordings in humans reported selectivity for distinct English phonetic features at single electrodes [<xref ref-type="bibr" rid="pcbi.1006766.ref006">6</xref>]. Hence, we investigated whether similar results could be obtained in higher layers of our network. Following that study [<xref ref-type="bibr" rid="pcbi.1006766.ref006">6</xref>], we separately calculated the phoneme selectivity index (PSI) vectors of the units in layers S1 to C6 (see <xref ref-type="sec" rid="sec013">Materials and Methods</xref>). Each element in the PSI vector of a unit indicates the selectivity of the unit to a phoneme; the larger the element, the more selective to the corresponding phoneme. Units in lower layers, such as S1 to S4, did not exhibit distinctive phoneme selectivity (<xref ref-type="supplementary-material" rid="pcbi.1006766.s004">S4 Fig</xref>), whereas those in higher layers did (<xref ref-type="supplementary-material" rid="pcbi.1006766.s005">S5 Fig</xref>; <xref ref-type="fig" rid="pcbi.1006766.g005">Fig 5</xref>). As an example, <xref ref-type="fig" rid="pcbi.1006766.g005">Fig 5A</xref> shows the PSI vectors of 173 active units in layer C6 whose responses to randomly selected time frames were statistically larger than their response to silence (p&lt;0.001). Each of these units exhibits strong selectivity for a subset of phonemes, and the unit–phoneme map exhibits a strong clustering effect. We used hierarchical agglomerative clustering analyses [<xref ref-type="bibr" rid="pcbi.1006766.ref036">36</xref>] with Euclidean distance to determine selectivity patterns across phonemes (<xref ref-type="fig" rid="pcbi.1006766.g005">Fig 5B</xref>) and active units (<xref ref-type="fig" rid="pcbi.1006766.g005">Fig 5C</xref>). Phonemes were clustered into six groups according to the place and manner of articulation: plosive, fricative, low back, low front, high front, and nasal. The active units were also clustered into six groups, each selective for one of the six types of phonemes. The PSI vectors of active units sharing a particular phonetic feature were averaged to quantify the feature selectivity of these units. Six groups of units exhibited distinctive roles in characterizing these features (<xref ref-type="fig" rid="pcbi.1006766.g005">Fig 5D</xref>).</p>
<fig id="pcbi.1006766.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006766.g005</object-id>
<label>Fig 5</label>
<caption>
<title>PSI vectors of 173 active units in layer C6.</title>
<p>(a) PSI vectors of phonemes. Each column corresponds to a unit. (b) Hierarchical clustering across phonemes. (c) Hierarchical clustering across units. (d) PSI vectors of six phonetic features.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.g005" xlink:type="simple"/>
</fig>
<p>Similar clustering results were obtained in layers S5, C5, and S6 (<xref ref-type="supplementary-material" rid="pcbi.1006766.s005">S5 Fig</xref>). However, the dark area in the unit–phoneme plane becomes increasingly prominent from layer S5 to layer C6 (<xref ref-type="supplementary-material" rid="pcbi.1006766.s005">S5 Fig</xref>; <xref ref-type="fig" rid="pcbi.1006766.g005">Fig 5</xref>), suggesting increasing selectivity of these layers for phonetic features. As in previous studies [<xref ref-type="bibr" rid="pcbi.1006766.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref037">37</xref>], we defined an index (F-ratio) that measures the overall selectivity of each hidden layer to phonetic features (Materials and Methods). We found that the deeper the layer, the higher the F-ratio (<xref ref-type="table" rid="pcbi.1006766.t001">Table 1</xref>). Specifically, the active units in layer C6 exhibited the highest overall selectivity.</p>
<table-wrap id="pcbi.1006766.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006766.t001</object-id>
<label>Table 1</label> <caption><title>F-ratios of the last eight layers.</title></caption>
<alternatives>
<graphic id="pcbi.1006766.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="justify">Layer</th>
<th align="justify">S3</th>
<th align="justify">C3</th>
<th align="justify">S4</th>
<th align="justify">C4</th>
<th align="justify">S5</th>
<th align="justify">C5</th>
<th align="justify">S6</th>
<th align="justify">C6</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">F-ratio</td>
<td align="left">3.28</td>
<td align="left">3.31</td>
<td align="left">6.16</td>
<td align="left">8.82</td>
<td align="left">13.80</td>
<td align="left">18.74</td>
<td align="left">61.35</td>
<td align="left">73.41</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Phonetic feature categories are discrete acoustic parameters. We next investigated the encoding of continuous acoustic features that specify phonemes, including the fundamental frequency (F0), formant frequencies (F1, F2), voice-onset time (VOT), and spectral peak. We used linear regression to decode these features from the response amplitudes of model units (Materials and Methods). Because F0, F1, and F2 vary significantly across vowels, whereas VOT and spectral peak vary significantly across consonants, we separately decoded F0, F1, and F2 of vowels (<xref ref-type="fig" rid="pcbi.1006766.g006">Fig 6A</xref>) and the VOT and spectral peak of consonants (<xref ref-type="fig" rid="pcbi.1006766.g006">Fig 6B</xref>) from the responses of all active units in layer C6. A 20-fold validation scheme was used to predict each parameter. The prediction accuracies on the test sets (1-fold) were defined based on the regression errors on the corresponding training sets (19-fold). The prediction accuracies for each parameter were significantly higher than those of a random decoder (p&lt;10<sup>−5</sup>; Materials and Methods). These observations suggest that the variability of these acoustic features is well represented in the responses of the units. Similar prediction accuracies were obtained based on the neural population responses in human superior temporal gyrus [<xref ref-type="bibr" rid="pcbi.1006766.ref006">6</xref>].</p>
<fig id="pcbi.1006766.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006766.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Encoding of the acoustic parameters F0, F1, F2, VOT, and spectral peak in higher layers.</title>
<p>The mean and standard deviation of decoding accuracies in 20-fold training and testing experiments are shown. (a) Decoding accuracies of F0, F1, and F2 based on the response amplitudes of all active layer C6 units. These accuracies are significantly higher than that of a random decoder (p&lt;10<sup>−5</sup>). (b) Decoding accuracies of VOT and spectral peak based on the response amplitudes of all active layer C6 units. These accuracies are significantly higher than that of a random decoder (p&lt;10<sup>−5</sup>). (c) Decoding accuracies of acoustic parameters based on the response amplitudes of active layer C6 units in six different groups. These accuracies are significantly higher than that of a random decoder (p&lt;10<sup>−5</sup>). (d) Average decoding accuracies of the acoustic parameters in layers S4, C4, S5, C5, S6, and C6. The six groups of units are presented in the same order as in (c) (from left to right: plosive, fricative, nasal, low back, low front, and high front). In all panels, error bars indicate standard deviation over 20 accuracies. To avoid clutter, error bars in (d) are not shown.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.g006" xlink:type="simple"/>
</fig>
<p>The same linear regression method was applied to decode the acoustic parameters F0, F1, F2, VOT, and spectral peak from the cochleogram. We cut a length of 170 ms cochleogram for each phoneme instance and used it as the feature of this instance. All decoding accuracies were about 10%, much lower than those obtained from the responses of layer C6 units (<xref ref-type="fig" rid="pcbi.1006766.g006">Fig 6A and 6B</xref>).</p>
<p>In higher layers of the model, the units were always clustered into six phoneme groups according to PSI (<xref ref-type="fig" rid="pcbi.1006766.g005">Fig 5</xref>; <xref ref-type="supplementary-material" rid="pcbi.1006766.s005">S5 Fig</xref>). Therefore, we could calculate the decoding accuracies for each group of units and compare the results in different layers. In layer C6, we obtained significantly higher accuracies than did a random decoder (<xref ref-type="fig" rid="pcbi.1006766.g006">Fig 6C</xref>) (p&lt;10<sup>−5</sup>). Similar results were obtained in layer <xref ref-type="supplementary-material" rid="pcbi.1006766.s006">S6</xref> (<xref ref-type="fig" rid="pcbi.1006766.g006">Fig 6D</xref>). However, the decoding accuracies were very low in layers S5 and C5, and even lower in the earlier layers S4 and C4. These poor accuracies were partly due to the small STRFs of units in these layers, which contain less information about the acoustic features of a phoneme. <xref ref-type="fig" rid="pcbi.1006766.g006">Fig 6D</xref> shows a small improvement from layer S(<italic>l</italic>) to layer C(<italic>l</italic>), but a large improvement from layer C(<italic>l</italic>) to layer S(<italic>l</italic>+1). This is because STRF sizes were similar between units in layer S(<italic>l</italic>) and layer C(<italic>l</italic>), but very different between units in layer C(<italic>l</italic>) and layer S(<italic>l</italic>+1) (<xref ref-type="table" rid="pcbi.1006766.t002">Table 2</xref>).</p>
<table-wrap id="pcbi.1006766.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006766.t002</object-id>
<label>Table 2</label> <caption><title>STRF sizes of the units in different layers.</title></caption>
<alternatives>
<graphic id="pcbi.1006766.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="justify">Layer</th>
<th align="justify">S1</th>
<th align="justify">C1</th>
<th align="justify">S2</th>
<th align="justify">C2</th>
<th align="justify">S3</th>
<th align="justify">C3</th>
<th align="justify">S4</th>
<th align="justify">C4</th>
<th align="justify">S5</th>
<th align="justify">C5</th>
<th align="justify">S6</th>
<th align="justify">C6</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">STRF Size</td>
<td align="left">10</td>
<td align="left">12</td>
<td align="left">30</td>
<td align="left">34</td>
<td align="left">70</td>
<td align="left">74</td>
<td align="left">110</td>
<td align="left">114</td>
<td align="left">150</td>
<td align="left">154</td>
<td align="left">190</td>
<td align="left">194</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Thus far, we have described the encoding of static acoustic features of phonemes in higher layers of the model. However, phonemes are not static. In fact, the first two formants (F1 and F2) of the phonemes, especially the consonants, exhibited large variation over time (<xref ref-type="fig" rid="pcbi.1006766.g007">Fig 7</xref>). We investigated the encoding of the dynamic formants using the responses of the units. We defined the temporal variation index (TVI) of individual phonemes as the projections of their F1 or F2 contours (time course of formant frequencies averaged over all instances of a phoneme) onto their respective principal components over all phonemes (Materials and Methods). Therefore, TVI measures the matching degree of an individual formant contour to the principal component of all formant contours. For each unit, we calculated the correlations between its responses to phonemes and the TVIs of the phonemes. A high correlation indicated that the unit was sensitive to the TVI of the phonemes—it “liked” phonemes whose F1 or F2 contour were congruent to the principal component but “disliked” phonemes whose F1 or F2 contour were incongruent to the principal component. Some units in layer C6 were sensitive to either F1 or F2 TVI, whereas others were sensitive to both F1 and F2 TVI (<xref ref-type="fig" rid="pcbi.1006766.g007">Fig 7C</xref>). However, such units were scarce in layers S5, C5, and S6; in fact, for most of the units in these layers, the correlations between their responses and both F1 and F2 TVI were smaller than 0.5.</p>
<fig id="pcbi.1006766.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006766.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Encoding of dynamic properties of phonemes in higher layers.</title>
<p>(a) Spectrograms of two phonemes. Two instances of each phoneme are shown. The first two formant (F1 and F2) contours of these instances are denoted by red and yellow curves, respectively. The formant contours of a phoneme was defined as the averaged contours of different instances of the phoneme. (b) Principal components (PCs) of the F1 and F2 contours calculated over 33 phonemes. F1 or F2 TVI of a phoneme is defined as the projection of the phoneme’s F1 or F2 contour onto the F1 or F2 PC. (c) Encoding of the dynamic properties of phonemes in different layers. Each dot indicates the correlations between the responses of a unit to the phonemes and their F1 (horizontal axis) and F2 (vertical axis) TVIs. In each layer, 200 units were randomly selected.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Influence of sparseness to response properties of higher-layer units</title>
<p>The hallmark of the sparse coding model [<xref ref-type="bibr" rid="pcbi.1006766.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref038">38</xref>] is the sparse activity of the hidden units, which has been proven to be essential in reproducing the tuning properties of auditory nerve fibers [<xref ref-type="bibr" rid="pcbi.1006766.ref022">22</xref>] and neurons in the inferior colliculus [<xref ref-type="bibr" rid="pcbi.1006766.ref023">23</xref>]. However, it remains unclear whether sparse activity in higher layers of SHMAX also plays a significant role in producing the phoneme encodings that we observed. Hence, we investigated how the sparseness level influenced the results in layer S5 to layer C6.</p>
<p>First, we adjusted the parameter <italic>λ</italic> in the sparse coding model, which controls the sparseness of the responses of the units of a particular S layer, while keeping <italic>λ</italic> in other layers at the default value of 1. We found that sparseness and F-ratio were positively correlated in each layer (<xref ref-type="fig" rid="pcbi.1006766.g008">Fig 8A</xref>), i.e., the sparser the activity, the more selective the units. However, increasing the sparseness level in lower layers such as S5 and C5 did not lead to phoneme tuning as strong as that in C6. This suggests that sparseness was not the only factor that led to the strong phoneme-encoding property in the top layer, and that the hierarchical structure also played a significant role. Although we used the lifetime sparseness measure [<xref ref-type="bibr" rid="pcbi.1006766.ref039">39</xref>] (Materials and Methods) here, the conclusion did not change when the population sparseness measure [<xref ref-type="bibr" rid="pcbi.1006766.ref040">40</xref>] was applied. Second, we modulated the neural encoding of acoustic parameters with different sparseness levels. The results revealed that sparseness also played a key role in producing the neural population coding of acoustic parameters (<xref ref-type="fig" rid="pcbi.1006766.g008">Fig 8B</xref>).</p>
<fig id="pcbi.1006766.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006766.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Influence of the response sparseness of the units in the model.</title>
<p>To obtain the curve for one layer, a total of 16 values were chosen non-uniformly between 0.001 and 100 for λ in that layer, while keeping λ = 1 in lower layers. (a) Relationship between F-ratio and lifetime sparseness in layers S5, C5, S6, and C6. (b) Relationship between the decoding accuracy of different acoustic parameters and lifetime sparseness in layer C6. SP, spectral peak. (c) PSI vectors of phonemes in C6 with λ = 0.01. The order of rows is the same as in <xref ref-type="fig" rid="pcbi.1006766.g005">Fig 5A</xref>. (d) PSI vectors of phonemes in C6 with λ = 1 (exactly the same as <xref ref-type="fig" rid="pcbi.1006766.g005">Fig 5A</xref>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.g008" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec006">
<title>Influence of pooling on response properties</title>
<p>The other critical element in the model is max pooling. Without this element, the model would be almost linear because the only nonlinear operation is the down-sampling between layer S1 and layer S2, which is implemented by a convolution with stride 2; one would not expect such a model to produce striking results. To examine the influence of pooling, first, we removed the max pooling layers and trained the model as before. Under these conditions, the STRFs of layer S2 and layer S3 units exhibited much simpler patterns (<xref ref-type="fig" rid="pcbi.1006766.g009">Fig 9A and 9B</xref>), and the complex patterns obtained with max pooling (<xref ref-type="fig" rid="pcbi.1006766.g003">Fig 3</xref>) were scarce. More importantly, the higher layers did not exhibit similar results to those of field recordings in human auditory cortex (e.g., compare <xref ref-type="fig" rid="pcbi.1006766.g009">Fig 9C</xref> with <xref ref-type="supplementary-material" rid="pcbi.1006766.s005">S5C Fig</xref>). Second, we replaced all max pooling in the model with average pooling, another widely used operation in the deep learning field. The only difference is that, in average pooling, we take the average of values, instead of the maximum value, in a region. After training the model, we obtained poor results (e.g., compare <xref ref-type="fig" rid="pcbi.1006766.g009">Fig 9D</xref> and <xref ref-type="fig" rid="pcbi.1006766.g005">Fig 5A</xref>). These results suggest that max pooling played an important role in modeling the function of the auditory pathway.</p>
<fig id="pcbi.1006766.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006766.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Influence of the pooling method used in the model.</title>
<p>(a, b) STRFs of all units in layers S2 and S3 without pooling. (c) PSI vectors of 77 active units in layer S6 without pooling. (d) PSI vectors of 96 active units in layer C6 with average pooling.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.g009" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Influences of the number of layers and STRF size</title>
<p>In addition to sparse response and max pooling, two other factors are also important for the emergence of phoneme selectivity: the number of layers and STRF size of the units. This is because enough nonlinearity must be accumulated along the hierarchy through repeated max pooling and down-sampling, and the STRF must be large enough to cover the length of the phonemes. To better understand the roles of these two factors, we explored models with different architectures. First, we tried a different layer S5 with larger kernel size (20×20), whose STRF size was the same as in the original layer S6. The selectivity of the new layer S5 was weaker than that of the original layer S6 (<xref ref-type="supplementary-material" rid="pcbi.1006766.s006">S6A Fig</xref>; <xref ref-type="fig" rid="pcbi.1006766.g010">Fig 10</xref>). Because the nonlinearity of the model grows with ascending layers, this result indicates that the emergence of phoneme selectivity needs a certain degree of nonlinearity. Second, we replaced the original layer S6 (kernel size 10×10) with two new layers, S6 and S7, with kernel size 5×5. In comparison with the original layer S6, the phoneme selectivity of the new layer S6 was weaker, and that of the new layer S7 was similar (<xref ref-type="supplementary-material" rid="pcbi.1006766.s006">S6B and S6C Fig</xref>; <xref ref-type="fig" rid="pcbi.1006766.g010">Fig 10</xref>). The results indicate that STRF size is also important for the emergence of phoneme selectivity. Notice that there is an abrupt increase in the decoding accuracy of acoustic parameters from original layer S5 to layer S6, as observed in <xref ref-type="fig" rid="pcbi.1006766.g006">Fig 6D</xref>. This is mainly because the STRFs of layer S5 units were not large enough to capture the selectivity information, while those of layer S6 units happened to be large enough. <xref ref-type="fig" rid="pcbi.1006766.g010">Fig 10</xref> shows that, if the STRF size in original layer S5 is increased (green), or the STRF size in original layer S6 is decreased (red), the increase in decoding accuracy from the original layer S5 to layer S6 would not be as large as observed in <xref ref-type="fig" rid="pcbi.1006766.g006">Fig 6D</xref>.</p>
<fig id="pcbi.1006766.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006766.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Average decoding accuracies of the acoustic parameters in layers S5, S6, and S7 with different kernel sizes.</title>
<p>Note that S5 with kernel size 10×10 and S6 with kernel size 10×10 are layers in the original network. Layer S5 with kernel size 20×20 was obtained by fixing layers S1 to C4 of the original network; layer S6 with size 5×5 and layer S7 with kernel size 5×5 were obtained by fixing layers S1 to C5 of the original network. The STRF sizes in these layers are indicated in parentheses.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.g010" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec008" sec-type="conclusions">
<title>Discussion</title>
<p>In this study, we demonstrated the capability of a computational model to predict the neural response properties along the auditory pathway. With alternating sparse coding and max pooling operations, the model learned important features of speech, from simple spectro-temporal patterns to complex phonetic features, which are encoded at different stages of the auditory pathway. Specifically, the response properties of lower-layer units were similar to those of inferior colliculus neurons in the auditory midbrain, whereas the response properties of higher layers were similar to field recordings in the auditory cortex. It is worth emphasizing that the agreement between the model output and neural physiological data is not a result of neural data fitting, but rather of unsupervised learning of the deep network. Because the auditory system deals with dynamic sound stimulus and has more processing stages before the auditory cortex, its coding strategy was thought to be distinct from that of the visual system [<xref ref-type="bibr" rid="pcbi.1006766.ref041">41</xref>]. Our model demonstrated for the first time that the sparse and hierarchical coding strategies widely observed in the visual system can also be generalized to the auditory system.</p>
<sec id="sec009">
<title>Sparse coding and max pooling in auditory computation</title>
<p>Merely reconstructing the stimuli without sparseness regularization would not lead to biologically reasonable results. Indeed, we observed that, upon replacing the L1 regularization term in Eq (<xref ref-type="disp-formula" rid="pcbi.1006766.e006">1</xref>), which encourages sparse activities in the units, with the L2 regularization term <inline-formula id="pcbi.1006766.e001"><alternatives><graphic id="pcbi.1006766.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mi>λ</mml:mi><mml:mrow><mml:msub><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, the STRFs of lower-layer units had fewer semantic features, and the phonetic feature selectivity was poor even at layer C6. This result agrees with a recent study [<xref ref-type="bibr" rid="pcbi.1006766.ref037">37</xref>] in which two unsupervised deep neural networks trained on natural speech without this regularization exhibited no clear selectivity for phonetic features.</p>
<p>A previous study [<xref ref-type="bibr" rid="pcbi.1006766.ref023">23</xref>] using a single-layer sparse coding model reported qualitatively similar results to those obtained from the lower layers of SHMAX. The work described here extends that study in two ways: first, by setting a more biologically reasonable temporal window for learning the response properties of inferior colliculus neurons in auditory midbrain; and second, by introducing hierarchical structure for learning the results of field recording in the auditory cortex. By using control experiments, we have found that max pooling in the hierarchical structure also plays an important role in producing the results.</p>
<p>Because the same principles, sparse coding and max pooling, are employed in different layers of the model, the results suggest that these principles might underlie computation at multiple stages of the auditory system. This is in agreement with the assumption that the auditory system evolved to optimize the representation of natural sounds [<xref ref-type="bibr" rid="pcbi.1006766.ref042">42</xref>], of which human speech is an example. Natural sounds contain many forms of higher-order and nonlinear statistical regularities [<xref ref-type="bibr" rid="pcbi.1006766.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref043">43</xref>], and sparse coding and max pooling are capable of extracting such regularities from natural images [<xref ref-type="bibr" rid="pcbi.1006766.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref045">45</xref>]. In this work, we demonstrate that these approaches can also extract them from human speech.</p>
</sec>
<sec id="sec010">
<title>Unsupervised learning for phoneme representation</title>
<p>Psychological studies have reported that infants are capable of distinguishing phonemes in their native language, even though they are not explicitly trained to accomplish this task [<xref ref-type="bibr" rid="pcbi.1006766.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref047">47</xref>]. Inspired by this finding, many computational models have been proposed to investigate how phoneme categories can be formed from continuous speech through unsupervised learning (e.g., [<xref ref-type="bibr" rid="pcbi.1006766.ref048">48</xref>–<xref ref-type="bibr" rid="pcbi.1006766.ref050">50</xref>]). However, these single-layer models cannot reveal how phoneme information is encoded gradually along the auditory pathway. Aided by some side information, such as lexical information and the identity of the speaker, deep neural networks can learn phonetic features from speech [<xref ref-type="bibr" rid="pcbi.1006766.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref052">52</xref>]. A critical step in these systems is time alignment of frames, which complicates the learning process and makes it hard to interpret the learning process of infants. Recently, two hierarchical models, the deep belief network and auto-encoder network, were trained on unlabeled speech corpus, but failed to obtain increasing phoneme selectivity in ascending layers [<xref ref-type="bibr" rid="pcbi.1006766.ref037">37</xref>]. By contrast, after supervised training with phoneme labels, the multilayer perceptron exhibited increasing selectivity to phoneme classes in higher layers [<xref ref-type="bibr" rid="pcbi.1006766.ref037">37</xref>], and its top layer exhibited a feature organization pattern similar to that of human auditory cortex [<xref ref-type="bibr" rid="pcbi.1006766.ref024">24</xref>]. Nevertheless, this supervised learning model cannot explain how phoneme representation in the human brain is developed during infancy. In this study, we showed that phonetic feature representation can emerge in an unsupervised learning model trained on continuous speech data without relying on any side information. Our results oppose the view that phoneme learning occurs after [<xref ref-type="bibr" rid="pcbi.1006766.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref052">52</xref>] or concurrent with lexical learning and that the two processes cannot be addressed in isolation [<xref ref-type="bibr" rid="pcbi.1006766.ref053">53</xref>–<xref ref-type="bibr" rid="pcbi.1006766.ref055">55</xref>].</p>
</sec>
<sec id="sec011">
<title>Model predictions</title>
<p>The model makes two predictions that could be explored in future experiments. The first is that the emergence of selectivity for phoneme features along the auditory pathway is not abrupt; instead, it should be a continuous process. In our model, the selectivity emerged in several layers other than the top layer, although it was weaker in lower layers such as S5. Therefore, one may find neurons selective for phoneme features in subcortical areas, but their selectivity should not be as strong as in the auditory cortex. The second prediction is that there exist neurons in the auditory cortex that encode the formant dynamics of phonemes. As shown in <xref ref-type="fig" rid="pcbi.1006766.g007">Fig 7</xref>, some layer C6 units in the model were sensitive to variation in the first formant of the phonemes, the second formant, or both. This is parallel to the findings in the higher-level cortical areas in the visual pathway, in which neurons encode abstract features such as contours [<xref ref-type="bibr" rid="pcbi.1006766.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref057">57</xref>]. The fact that such units were scarce in layers S5 to C5 suggests that neurons whose responses are correlated to formant dynamics are not common in subcortical areas.</p>
</sec>
<sec id="sec012">
<title>Limitations</title>
<p>This study has some limitations. First, because SHMAX is not a biologically detailed model, it is unclear how it could be implemented in a biological system. Some neural circuits have been proposed to individually realize the two essential components of the model, i.e., sparse coding [<xref ref-type="bibr" rid="pcbi.1006766.ref058">58</xref>] and max pooling [<xref ref-type="bibr" rid="pcbi.1006766.ref059">59</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref060">60</xref>], but an approach for integrating them as a whole is still lacking. Second, this model differs from biological systems in many aspects. For example, real neurons can be excitatory or inhibitory, and they obey Dale’s law [<xref ref-type="bibr" rid="pcbi.1006766.ref061">61</xref>], but these features are not considered in SHMAX. In addition, the auditory system contains abundant feedback and recurrent connections, whereas SHMAX is a feedforward model. Due to these differences, the results in this paper should be interpreted cautiously. Although such discrepancies may not affect the abstract computational principles of the auditory system revealed by the model, a more biologically plausible model would make the results more convincing.</p>
</sec>
</sec>
<sec id="sec013" sec-type="materials|methods">
<title>Materials and methods</title>
<p>The model and analyses were implemented in MATLAB. The source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/QingtianZhang/SHMAX_AuditoryPathway" xlink:type="simple">https://github.com/QingtianZhang/SHMAX_AuditoryPathway</ext-link>.</p>
<sec id="sec014">
<title>Stimuli</title>
<p>Speech stimuli from the Texas Instruments/Massachusetts Institute of Technology (TIMIT) database [<xref ref-type="bibr" rid="pcbi.1006766.ref062">62</xref>], which includes 6,300 sentences spoken by 630 speakers (10 sentences per speaker) from eight major dialect regions of the United States, were used. The sample rate is 16 kHz. The sentences were first converted into cochleogram [<xref ref-type="bibr" rid="pcbi.1006766.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref063">63</xref>], which is similar to a spectrogram but better reflects the effects of the cochlea. A total of 194 frequency filters were generated from a cochlear model [<xref ref-type="bibr" rid="pcbi.1006766.ref009">9</xref>] whose center frequencies were between 73 and 7,630 Hz. The sample step was 1 ms. The cochleogram was used as the input for the SHMAX model.</p>
</sec>
<sec id="sec015">
<title>SHMAX</title>
<p>SHMAX [<xref ref-type="bibr" rid="pcbi.1006766.ref028">28</xref>] is an unsupervised deep learning model that integrates sparse coding into a well-known cortex-inspired visual recognition model, HMAX [<xref ref-type="bibr" rid="pcbi.1006766.ref064">64</xref>]. The structure used in this study (<xref ref-type="fig" rid="pcbi.1006766.g001">Fig 1</xref>) consists of six S layers and six C layers, which respectively perform sparse coding and max pooling, in alternation. We implemented the feedforward calculation of the model in the same manner as in a convolutional neural network [<xref ref-type="bibr" rid="pcbi.1006766.ref065">65</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref066">66</xref>]. The major difference is that the convolutional kernels were not learned by supervised learning but by unsupervised learning, specifically sparse coding (see below). Another difference is that we did not use nonlinear activation functions as in standard convolutional neural networks. The details are as follows.</p>
<p>Sparse coding is an unsupervised learning technique inspired by sparse firing of V1 simple cells [<xref ref-type="bibr" rid="pcbi.1006766.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref038">38</xref>]. Given a set of input signals <bold><italic>x</italic></bold><sup><italic>k</italic></sup> ∈ <italic>R</italic><sup><italic>n</italic></sup>, where <italic>k</italic> indexes the input, the objective of sparse coding is to find a set of bases <bold><italic>b</italic></bold><sub><italic>j</italic></sub> ∈ <italic>R</italic><sup><italic>n</italic></sup> such that <inline-formula id="pcbi.1006766.e002"><alternatives><graphic id="pcbi.1006766.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">σ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1006766.e003"><alternatives><graphic id="pcbi.1006766.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is the weighting factor of <bold><italic>b</italic></bold><sub><italic>j</italic></sub> and <bold><italic>σ</italic></bold><sup><italic>k</italic></sup> ∈ <italic>R</italic><sup><italic>n</italic></sup> is noise. The factor <inline-formula id="pcbi.1006766.e004"><alternatives><graphic id="pcbi.1006766.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is called the “response” of neuron <italic>j</italic> to the <italic>k</italic>-th input, whose receptive field is delineated by <bold><italic>b</italic></bold><sub><italic>j</italic></sub>. The critical requirement of this technique is that <inline-formula id="pcbi.1006766.e005"><alternatives><graphic id="pcbi.1006766.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> values are sparse (i.e., only a few of them are nonzero). A standard formulation of sparse coding is
<disp-formula id="pcbi.1006766.e006">
<alternatives>
<graphic id="pcbi.1006766.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">minimize</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mspace width="1em"/><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
<disp-formula id="pcbi.1006766.e007">
<alternatives>
<graphic id="pcbi.1006766.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mspace width="1em"/><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>≤</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.50em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
where <bold><italic>B</italic></bold> ∈ <italic>R</italic><sup><italic>n</italic>×<italic>m</italic></sup> is the collection of <bold><italic>b</italic></bold><sub><italic>j</italic></sub>, <bold><italic>r</italic></bold><sup><italic>k</italic></sup> ∈ <italic>R</italic><sup><italic>m</italic></sup> is the collection of <inline-formula id="pcbi.1006766.e008"><alternatives><graphic id="pcbi.1006766.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, and <italic>λ</italic> is a constant controlling the tradeoff between the reconstruction error (the first term) and the sparseness (the second term). ‖⋅‖<sub>2</sub> and ‖⋅‖<sub>1</sub> stand for the L2-norm and L1-norm of vectors, respectively. All results reported in this paper, except in <xref ref-type="fig" rid="pcbi.1006766.g008">Fig 8</xref>, were obtained with <italic>λ</italic> = 1. Without loss of generality, it was assumed that the STRFs of all units in the model were square. The bases of sparse coding in each S layer were learned from a number of 10 ×10 × <italic>u</italic> patches extracted randomly from the input of that layer (therefore, the size of each basis was also 10 ×10 × <italic>u</italic>), where <italic>u</italic> denotes the number of the input channels. In layer S1, <italic>u</italic> was equal to 1, and in other S layers it was equal to the number of bases in the preceding C layer. The online dictionary learning algorithm was used to learn <bold><italic>B</italic></bold> [<xref ref-type="bibr" rid="pcbi.1006766.ref067">67</xref>].</p>
<p>The response of each S layer could be obtained by solving Eq (<xref ref-type="disp-formula" rid="pcbi.1006766.e006">1</xref>) with learned <bold><italic>B</italic></bold> by inputting a sliding 10 ×10 × <italic>u</italic> patch in the previous layer as input [<xref ref-type="bibr" rid="pcbi.1006766.ref028">28</xref>]. This will result in a total of <italic>m</italic> feature maps consisting of responses of <italic>m</italic> bases <bold><italic>b</italic></bold><sub><italic>j</italic></sub> at every location in the previous layer. In this study, we used another approach: each basis <bold><italic>b</italic></bold><sub><italic>j</italic></sub> was convolved with the input to obtain the corresponding feature map. In doing so, <bold><italic>b</italic></bold><sub><italic>j</italic></sub> was reshaped to 10 ×10 × <italic>u</italic> and convolved with the previous layer whose size was <italic>h</italic> × <italic>t</italic> × <italic>u</italic>, where <italic>h</italic> and <italic>t</italic> denote the height and width of the feature maps, then the <italic>j</italic>-th feature map (a 2D matrix) in the current layer was obtained as follows:
<disp-formula id="pcbi.1006766.e009">
<alternatives>
<graphic id="pcbi.1006766.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where <italic>s</italic><sub><italic>conv</italic></sub> is the convolution stride. This approach yielded similar results but was much faster. Note that the difference between the outputs of the two approaches is unimportant because the responses obtained by the convolution approach are also sparse (<xref ref-type="supplementary-material" rid="pcbi.1006766.s007">S7 Fig</xref>), and sparseness is the focus of this study. In addition, in the convolution approach it is natural to define <bold><italic>b</italic></bold><sub><italic>j</italic></sub> as the receptive field of an artificial neuron because <bold><italic>b</italic></bold><sub><italic>j</italic></sub> is used to explain the output, whereas in the optimization approach the correspondence between <bold><italic>b</italic></bold><sub><italic>j</italic></sub> and receptive field is indirect because <bold><italic>b</italic></bold><sub><italic>j</italic></sub> is used to explain the input.</p>
<p>The convolution stride <italic>s</italic><sub><italic>conv</italic></sub> was 2 in the first two S layers (equivalent to vanilla convolution followed by down-sampling with ratio 2) and 1 in the other four S layers (vanilla convolution). The value <italic>u</italic> in each S layer is indicated at the top of <xref ref-type="fig" rid="pcbi.1006766.g001">Fig 1C</xref>.</p>
<p>The C layers take the responses of S layers as input and perform the max pooling operation. We slide on the feature maps in an S layer and at each location take the maximum value in a region of size <italic>s</italic><sub><italic>pool</italic></sub> × <italic>s</italic><sub><italic>pool</italic></sub> centered at that location (for simplicity, square shapes of the regions are assumed). All maximum values taken in a feature map in the S layer then constitute a feature map in the subsequent C layer. Therefore, the number of feature maps in a C layer is equal to that in the preceding S layer. Clearly, the output (or response) of a C unit is the maximum response of some S units in a local region. The pooling results were input to the next S layer.</p>
<p>In this study, we used overlapping pooling (stride of 1), and the feature maps in a C layer had one fewer column and one fewer row than those in the preceding S layer. Specifically, the calculation is as follows:
<disp-formula id="pcbi.1006766.e010">
<alternatives>
<graphic id="pcbi.1006766.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e010" xlink:type="simple"/>
<mml:math display="block" id="M10">
<mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>:</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>:</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where <italic>s</italic><sub><italic>pool</italic></sub> = 2.</p>
</sec>
<sec id="sec016">
<title>Spectro-temporal receptive field (STRF)</title>
<p>The STRFs of the units in layer S1 were approximated by the corresponding bases. The STRFs of the units in higher layers were obtained by linearly combining the bases of the units in previous layers [<xref ref-type="bibr" rid="pcbi.1006766.ref028">28</xref>]. The logic is that a unit in the current layer is locally connected to the units in the previous layer, and the favorite input pattern (spectro-temporal receptive field, STRF) of this unit depends on the favorite input patterns of the units in the previous layer.</p>
<p>The bottom-up computation process is described as follows. First, it should be noted that the units in a given feature map share the same STRF. For a unit in the <italic>j</italic>-th feature map in layer S(<italic>l</italic>), where <italic>l</italic>&gt;1, its STRF is defined as the weighted sum of the STRFs of the units in layer S(<italic>l</italic> − 1) with their centers aligned according to the locations of the weights in the basis <bold><italic>b</italic></bold><sub><italic>j</italic></sub>. If there is no down-sampling between layers S(<italic>l</italic>) and S(<italic>l</italic> − 1), this can be implemented by convolving <bold><italic>b</italic></bold><sub><italic>j</italic></sub> with all of the <italic>u</italic><sup><italic>l</italic>−1</sup> STRFs in layer S(<italic>l</italic> − 1). Note that the third dimension of <bold><italic>b</italic></bold><sub><italic>j</italic></sub> is <italic>u</italic><sup><italic>l</italic>−1</sup>, the number of feature maps in layer S(<italic>l</italic> − 1), and the result is a 2D matrix, which is the STRF of the units in the <italic>j</italic>-th feature map in layer S(<italic>l</italic>). If there is a down-sampling operation with ratio <italic>d</italic> between layers S(<italic>l</italic>) and S(<italic>l</italic> − 1), one needs to first expand <bold><italic>b</italic></bold><sub><italic>j</italic></sub> to 10<italic>d</italic> × 10<italic>d</italic> × <italic>u</italic><sup><italic>l</italic>−1</sup>, and then perform the convolution as described above. The nearest-neighbor interpolation was used for matrix expansion. See <xref ref-type="fig" rid="pcbi.1006766.g002">Fig 2A</xref> for an illustration of visualizing an S2 basis (STRF of an S2 unit). For a better illustration, in this example the first two dimensions of the S1 and S2 bases are assumed to be 3 and 2, respectively, instead of 10. Note that the weighted summation step in the figure is equivalent to 2D convolution (“full” mode) of the two input matrices. After obtaining STRFs of all S2 units, one can calculate STRFs of S3 units in the same way, and so forth.</p>
<p>One can also calculate STRFs of units in any S layer directly, without precomputing the STRFs of units in lower S layers (<xref ref-type="supplementary-material" rid="pcbi.1006766.s001">S1 Fig</xref>). This is equivalent to the bottom-up method (<xref ref-type="fig" rid="pcbi.1006766.g002">Fig 2A</xref>), except for some differences in the boundaries of STRFs. The latter method was used to visualize the STRFs of S units in this study.</p>
<p>Since a C unit takes the maximum response of four neighboring units in the preceding S layer (the max pooling ratio was 2) whose STRFs are the same, its STRF is very similar to the STRFs of the preceding S units except that it is a bit larger. Its size is jointly determined by the size of the STRFs of the four preceding S units and the shifts between them.</p>
<p>The STRF sizes of the units in each layer are shown in <xref ref-type="table" rid="pcbi.1006766.t002">Table 2</xref>. Because all STRFs are assumed to be square, only the side lengths are shown in the table.</p>
<p>To test the validity of the visualization method described above, the STRFPak toolbox with the normalized reverse-correlation method [<xref ref-type="bibr" rid="pcbi.1006766.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref033">33</xref>] was also used to calculate the STRFs of the units in layers S1, S2, and S3 (<xref ref-type="supplementary-material" rid="pcbi.1006766.s003">S3 Fig</xref>); this took much longer time than the linear combination method described above. The time lag used for calculating the stimulus auto-correlation was 200 ms, the tolerance value was 0.01, and the sparse parameter was 0. The overall mean firing rate was removed from the neuronal response, and the space-time non-separable algorithm was used.</p>
<p>To compare the model and experimental results [<xref ref-type="bibr" rid="pcbi.1006766.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref034">34</xref>], singular value decomposition (SVD) was performed on the obtained STRFs, and the two unitary vectors corresponding to the first singular value were used to quantify the spectral and temporal response characteristics, namely the spectral and temporal profiles (<xref ref-type="fig" rid="pcbi.1006766.g003">Fig 3</xref>) [<xref ref-type="bibr" rid="pcbi.1006766.ref030">30</xref>]. The peaks of the spectral and temporal profiles determine the center frequency (Center F) and best temporal modulation frequency (Best T) respectively. The widths of the spectral and temporal profile that account for 90% of the total energy determine the bandwidth and the duration respectively.</p>
</sec>
<sec id="sec017">
<title>Phoneme selectivity index (PSI)</title>
<p>To calculate the responses of a computing unit to a particular phoneme in speech, the TIMIT phonetic transcriptions were used to align responses to the onsets of all instances of the phoneme. Phoneme length was not normalized. The maximum absolute value of the response of a unit along the phoneme duration was defined as that unit’s response amplitude. The PSI vectors [<xref ref-type="bibr" rid="pcbi.1006766.ref006">6</xref>] were employed to characterize the selectivity of the units to phonemes. The method is briefly outlined as follows, and more details can be found in Ref. [<xref ref-type="bibr" rid="pcbi.1006766.ref006">6</xref>]. For every unit, the distribution of its response amplitudes across all samples of each phoneme was estimated first. To calculate a unit’s PSI for a particular phoneme, the non-parametric Wilcox rank-sum test was used to determine whether the response amplitude distribution of the phoneme had a larger median than those of other phonemes (p&lt;0.01). The number of phonemes whose median response amplitudes were statistically smaller than the median response amplitude of a particular phoneme was defined as the unit’s PSI for that phoneme. Because 33 phonemes were selected from the dataset, PSI ranges between 0 and 32, where 0 means no selectivity and 32 means extreme selectivity. The PSIs for all 33 phonemes form a 33-dimensional PSI vector for the unit.</p>
</sec>
<sec id="sec018">
<title>F-ratio</title>
<p>Active units whose responses to randomly selected time frames were statistically larger than their response to silence (p&lt;0.001) were selected, and the F-ratio [<xref ref-type="bibr" rid="pcbi.1006766.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1006766.ref037">37</xref>] was used to quantify the overall phoneme selectivity of all active units in a layer (<xref ref-type="table" rid="pcbi.1006766.t001">Table 1</xref>). The units were grouped based on the clustering of PSI vectors (see <xref ref-type="fig" rid="pcbi.1006766.g005">Fig 5C</xref> for an example). Suppose there are <italic>m</italic> active units in total, which form <italic>n</italic> groups in a certain layer. Let Ω<sub><italic>j</italic></sub> denote the set of indices of active units in group <italic>j</italic>, and |Ω<sub><italic>j</italic></sub>| = <italic>m</italic><sub><italic>j</italic></sub>. The F-ratio for that layer is defined as the ratio of between-group variability to within-group variability:
<disp-formula id="pcbi.1006766.e011">
<alternatives>
<graphic id="pcbi.1006766.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msub><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
where <bold><italic>p</italic></bold><sub><bold><italic>i</italic></bold></sub> denotes the PSI vector for unit <italic>i</italic>, <inline-formula id="pcbi.1006766.e012"><alternatives><graphic id="pcbi.1006766.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:mover></mml:math></alternatives></inline-formula> denotes the average of PSI vectors over all units, and <inline-formula id="pcbi.1006766.e013"><alternatives><graphic id="pcbi.1006766.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> denotes the average of PSI vectors over group <italic>j</italic>. The larger the F-ratio, the better the clustering effect.</p>
</sec>
<sec id="sec019">
<title>Phoneme to feature transformation</title>
<p>Six distinctive features were used to describe the acoustic properties of each phoneme [<xref ref-type="bibr" rid="pcbi.1006766.ref068">68</xref>]. Each phoneme has only one of the six features. The PSI vectors of all phonemes that shared a particular feature were averaged to describe the feature selectivity of the units (<xref ref-type="fig" rid="pcbi.1006766.g005">Fig 5D</xref>).</p>
</sec>
<sec id="sec020">
<title>Phoneme acoustic parameter estimation</title>
<p>A series of static acoustic parameters were estimated for phonemes that play a perceptually important role in speech perception, including F0, F1, F2, VOT, and spectral peak. The values of the first three parameters were calculated as the median value of transcribed boundaries over the duration of the phoneme [<xref ref-type="bibr" rid="pcbi.1006766.ref006">6</xref>]. The VOT was extracted as the phoneme transcription boundary. The spectral peak was defined as the maximum energy along the frequency axis in the cochleogram. Acoustic parameters differ among individual instances of a phoneme; therefore, each acoustic parameter for each phoneme in the dataset is expressed as a distribution.</p>
</sec>
<sec id="sec021">
<title>Linear regression analysis</title>
<p>Similar to [<xref ref-type="bibr" rid="pcbi.1006766.ref006">6</xref>], a linear model <italic>y</italic> = <bold><italic>wx</italic></bold> + <italic>b</italic> was used to regress the static acoustic parameters <italic>y</italic> such as the fundamental frequency (F0), formant frequencies (F1, F2), voice-onset time (VOT), and spectral peak, based on the response amplitudes of the computing units <bold><italic>x</italic></bold>, where <bold><italic>w</italic></bold> and <italic>b</italic> are parameters to be learned. The least-square error between the prediction <italic>y</italic> and the ground truth <italic>y</italic>* was minimized on a training set, and the trained model predicted the parameter values on a test set. The root-mean-squared error was calculated on the training set. A prediction for a test sample was regarded as “correct” if the prediction error was smaller than the root-mean-squared error. The percent of correct prediction on the test set was defined as the testing or decoding accuracy.</p>
<p>The acoustic parameters regressed were F0, F1, and F2 of vowels and VOT and spectral peak of consonants. <xref ref-type="fig" rid="pcbi.1006766.g006">Fig 6A and 6B</xref> presents the decoding results using the responses of all active units in layer C6. <xref ref-type="fig" rid="pcbi.1006766.g006">Fig 6C and 6D</xref> presents the decoding results using the responses of active units, in each of the six phoneme groups separately, in different layers. For each task, a 20-fold cross-validation scheme was adopted, and therefore 20 decoding accuracies were obtained. To conduct a significance test, a random decoder was constructed. Given a test sample, the decoder output a value between the minimum and maximum values of the ground truth <italic>y</italic>* on the training samples, with a uniform probability. This random prediction was evaluated as correct or not based on the same criterion used for linear prediction. The 20-fold cross-validation scheme resulted in 20 chance-level accuracies. Student’s t-test was performed to compare the two sets of accuracies.</p>
</sec>
<sec id="sec022">
<title>Temporal variation index of phoneme formants</title>
<p>The time course of the first formant frequency of a phoneme instance was called the F1 contour of that instance (<xref ref-type="fig" rid="pcbi.1006766.g007">Fig 7A</xref>). The averaged F1 contours over all instances of a phoneme was defined as the F1 contour of the phoneme. Since 33 phonemes were selected in the dataset, we obtained 33 F1 contours. Principal component analysis (PCA) was performed on these contours. The first principal component, which had the same length as the F1 contour, was calculated (<xref ref-type="fig" rid="pcbi.1006766.g007">Fig 7B</xref>). The projection of the F1 contour of each phoneme onto the first principal component (a scalar) was defined as the F1 temporal variation index (TVI) of that phoneme. A unit’s average response to all instances of a phoneme (a scalar) was defined as its response to that phoneme. The encoding of F1 dynamics in a unit was measured as the correlation between the unit’s responses to all of the 33 phonemes and the F1 TVIs of those phonemes. The same procedure was applied to measure the encoding of F2 dynamics in a unit.</p>
</sec>
<sec id="sec023">
<title>Calculation of lifetime sparseness</title>
<p>The definition of lifetime sparseness of a unit is as follows [<xref ref-type="bibr" rid="pcbi.1006766.ref039">39</xref>]:
<disp-formula id="pcbi.1006766.e014">
<alternatives>
<graphic id="pcbi.1006766.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e014" xlink:type="simple"/>
<mml:math display="block" id="M14">
<mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
where <italic>r</italic> denotes the response of the unit and the expectation is taken across all test data.</p>
</sec>
</sec>
<sec id="sec024">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006766.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Calculation of the STRFs of S units using the linear combination method.</title>
<p>A top–down method was used to calculate the STRF of any basis in layer S(<italic>l</italic>). Suppose that the third dimension of the basis is <italic>u</italic><sup><italic>l</italic>−1</sup>. The basis was first expanded and padded, which is the pseudo inverse operation of down-sampling and pooling. Then, the basis was convolved with all bases in layer S(<italic>l</italic> − 1) to obtain a 3D matrix whose third dimension is <italic>u</italic><sup><italic>l</italic>−2</sup>. By repeating this process until <italic>l</italic> = 1, we get the STRF of the specific basis in layer S(<italic>l</italic>). In the figure, <italic>p</italic> denotes the height and weight of the patch, <italic>s</italic><sub><italic>conv</italic></sub> is the convolution stride, and <italic>l</italic> ≥ 2 denotes the layer index. Note that, when <italic>l</italic> = 2, <italic>u</italic><sup><italic>l</italic>−2</sup> = 1, and the process is the same as in <xref ref-type="fig" rid="pcbi.1006766.g002">Fig 2A</xref>. When <italic>l</italic> &gt; 2, <italic>u</italic><sup><italic>l</italic>−2</sup> &gt; 1, and after 2D convolution and summation, one obtains a 3D matrix instead of a 2D matrix.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006766.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title/>
<p>STRFs of all units in layers S1 (a), S2 (b), and S3 (c) of the SHMAX model.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006766.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title/>
<p><bold>STRFs of all units in layers S2 (left) and S3 (right) using the reverse-correlation method.</bold> The results are similar to those obtained using the linear combination method (<xref ref-type="supplementary-material" rid="pcbi.1006766.s002">S2 Fig</xref>).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006766.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title/>
<p>PSI vectors of active units in Layer S3 (a) and Layer S4 (b).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006766.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title/>
<p>PSI vectors of active units in Layer S5 (a), Layer C5 (b) and Layer S6 (c).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006766.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Influence of the number of layers and STRF size on PSI vectors.</title>
<p>(a) PSI vectors of the new layer S5 with larger kernel size 20×20, obtained by fixing layers S1 to C4 of the original network. (b) PSI vectors of the new layer S6 with smaller kernel size 5×5, obtained by fixing layers S1 to C5 of the original network. (c) PSI vectors of the new layer S7 with kernel size 5×5, obtained by fixing layers S1 to C5 of the original network and use the layer S6 with smaller kernel size 5×5.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006766.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006766.s007" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Response statistics obtained by convolving different kinds of bases on the feature maps in layer C4 across ten randomly selected sentences.</title>
<p>(a) Response distribution of a layer S5 basis learned by sparse coding. The Kolmogorov–Smirnov (KS) test showed that the response followed a Laplacian distribution (p&lt;0.05), which is sparse. The dashed lines show the fitted results with a zero mean Laplacian distribution whose probability density function is <inline-formula id="pcbi.1006766.e015"><alternatives><graphic id="pcbi.1006766.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where σ is the fitting parameter. Note that the vertical axis is in log-scale. (b) Response distribution of all layer S5 bases learned by sparse coding. The distribution is also very sparse. (c) Distribution of the element value in all layer S5 bases. (d) Response distribution of a basis whose elements were randomly sampled from the distribution in (c). The KS test showed that the response followed a Gaussian distribution (p&lt;0.05), which is dense. The dashed lines show the fitted results with a zero mean Gaussian distribution whose probability density function is <inline-formula id="pcbi.1006766.e016"><alternatives><graphic id="pcbi.1006766.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006766.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where σ is the fitting parameter.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1006766.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saur</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kreher</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Schnell</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kümmerer</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kellmeyer</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Vry</surname> <given-names>M-S</given-names></name>, <etal>et al</etal>. <article-title>Ventral and dorsal pathways for language</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2008</year>;<volume>105</volume>(<issue>46</issue>):<fpage>18035</fpage>–<lpage>40</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Young</surname> <given-names>ED</given-names></name>. <article-title>Neural representation of spectral and temporal information in speech</article-title>. <source>Philosophical Transactions of the Royal Society of London B: Biological Sciences</source>. <year>2008</year>;<volume>363</volume>(<issue>1493</issue>):<fpage>923</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2007.2151" xlink:type="simple">10.1098/rstb.2007.2151</ext-link></comment> <object-id pub-id-type="pmid">17827107</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Joris</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schreiner</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Rees</surname> <given-names>A</given-names></name>. <article-title>Neural processing of amplitude-modulated sounds</article-title>. <source>Physiological Reviews</source>. <year>2004</year>;<volume>84</volume>(<issue>2</issue>):<fpage>541</fpage>–<lpage>77</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/physrev.00029.2003" xlink:type="simple">10.1152/physrev.00029.2003</ext-link></comment> <object-id pub-id-type="pmid">15044682</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rauschecker</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Scott</surname> <given-names>SK</given-names></name>. <article-title>Maps and streams in the auditory cortex: nonhuman primates illuminate human speech processing</article-title>. <source>Nat Neurosci</source>. <year>2009</year>;<volume>12</volume>(<issue>6</issue>):<fpage>718</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2331" xlink:type="simple">10.1038/nn.2331</ext-link></comment> <object-id pub-id-type="pmid">19471271</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hickok</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>. <article-title>The cortical organization of speech processing</article-title>. <source>Nat Rev Neurosci</source>. <year>2007</year>;<volume>8</volume>(<issue>5</issue>):<fpage>393</fpage>–<lpage>402</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn2113" xlink:type="simple">10.1038/nrn2113</ext-link></comment> <object-id pub-id-type="pmid">17431404</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mesgarani</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Cheung</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>EF</given-names></name>. <article-title>Phonetic feature encoding in human superior temporal gyrus</article-title>. <source>Science</source>. <year>2014</year>;<volume>343</volume>(<issue>6174</issue>):<fpage>1006</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1245994" xlink:type="simple">10.1126/science.1245994</ext-link></comment> <object-id pub-id-type="pmid">24482117</object-id>; PubMed Central PMCID: PMCPMC4350233.</mixed-citation></ref>
<ref id="pcbi.1006766.ref007"><label>7</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Froemke</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Atencio</surname> <given-names>CA</given-names></name>. <chapter-title>Spectral processing in auditory cortex</chapter-title>. <source>The Auditory Cortex</source>: <publisher-name>Springer</publisher-name>; <year>2011</year>. p. <fpage>275</fpage>–<lpage>308</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref008"><label>8</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Eggermont</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X</given-names></name>. <chapter-title>Temporal coding in auditory cortex</chapter-title>. <source>The Auditory Cortex</source>: <publisher-name>Springer</publisher-name>; <year>2011</year>. p. <fpage>309</fpage>–<lpage>28</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref009"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Lyon R. A computational model of filtering, detection, and compression in the cochlea. IEEE International Conference on Acoustics, Speech, and Signal Processing1982. p. 1282–5.</mixed-citation></ref>
<ref id="pcbi.1006766.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neely</surname> <given-names>ST</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>D</given-names></name>. <article-title>A model for active elements in cochlear biomechanics</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1986</year>;<volume>79</volume>(<issue>5</issue>):<fpage>1472</fpage>–<lpage>80</lpage>. <object-id pub-id-type="pmid">3711446</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Davis</surname> <given-names>H.</given-names></name> <article-title>An active process in cochlear mechanics</article-title>. <source>Hearing Research</source>. <year>1983</year>;<volume>9</volume>(<issue>1</issue>):<fpage>79</fpage>–<lpage>90</lpage>. <object-id pub-id-type="pmid">6826470</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Langner</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name>. <article-title>Periodicity coding in the inferior colliculus of the cat. I. Neuronal mechanisms</article-title>. <source>Journal of Neurophysiology</source>. <year>1988</year>;<volume>60</volume>(<issue>6</issue>):<fpage>1799</fpage>–<lpage>822</lpage>. <object-id pub-id-type="pmid">3236052</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Casseday</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ehrlich</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Covey</surname> <given-names>E</given-names></name>. <article-title>Neural tuning for sound duration: role of inhibitory mechanisms in the inferior colliculus</article-title>. <source>Science</source>. <year>1994</year>;<volume>264</volume>(<issue>5160</issue>):<fpage>847</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">8171341</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jastreboff</surname> <given-names>PJ</given-names></name>. <article-title>Salicylate-induced abnormal activity in the inferior colliculus of rats</article-title>. <source>Hearing Research</source>. <year>1995</year>;<volume>82</volume>(<issue>2</issue>):<fpage>158</fpage>–<lpage>78</lpage>. <object-id pub-id-type="pmid">7775282</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Langner</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Albert</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Briede</surname> <given-names>T</given-names></name>. <article-title>Temporal and spatial coding of periodicity information in the inferior colliculus of awake chinchilla (Chinchilla laniger)</article-title>. <source>Hearing Research</source>. <year>2002</year>;<volume>168</volume>(<issue>1</issue>):<fpage>110</fpage>–<lpage>30</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Malmierca</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Cristaudo</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pérez-González</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Covey</surname> <given-names>E</given-names></name>. <article-title>Stimulus-specific adaptation in the inferior colliculus of the anesthetized rat</article-title>. <source>Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>17</issue>):<fpage>5483</fpage>–<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4153-08.2009" xlink:type="simple">10.1523/JNEUROSCI.4153-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19403816</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Langner</surname> <given-names>G.</given-names></name> <article-title>Periodicity coding in the auditory system</article-title>. <source>Hearing Research</source>. <year>1992</year>;<volume>60</volume>(<issue>2</issue>):<fpage>115</fpage>–<lpage>42</lpage>. <object-id pub-id-type="pmid">1639723</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gaese</surname> <given-names>BH</given-names></name>, <name name-style="western"><surname>Ostwald</surname> <given-names>J</given-names></name>. <article-title>Temporal coding of amplitude and frequency modulation in the rat auditory cortex</article-title>. <source>European Journal of Neuroscience</source>. <year>1995</year>;<volume>7</volume>(<issue>3</issue>):<fpage>438</fpage>–<lpage>50</lpage>. <object-id pub-id-type="pmid">7773441</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liégeois-Chauvel</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>de Graaf</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Laguitton</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Chauvel</surname> <given-names>P</given-names></name>. <article-title>Specialization of left auditory cortex for speech perception in man depends on temporal coding</article-title>. <source>Cerebral Cortex</source>. <year>1999</year>;<volume>9</volume>(<issue>5</issue>):<fpage>484</fpage>–<lpage>96</lpage>. <object-id pub-id-type="pmid">10450893</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>(<issue>6583</issue>):<fpage>607</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/381607a0" xlink:type="simple">10.1038/381607a0</ext-link></comment> <object-id pub-id-type="pmid">8637596</object-id>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bell</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>The "independent components" of natural scenes are edge filters</article-title>. <source>Vision Res</source>. <year>1997</year>;<volume>37</volume>(<issue>23</issue>):<fpage>3327</fpage>–<lpage>38</lpage>. <object-id pub-id-type="pmid">9425547</object-id>; PubMed Central PMCID: PMCPMC2882863.</mixed-citation></ref>
<ref id="pcbi.1006766.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>. <article-title>Efficient coding of natural sounds</article-title>. <source>Nat Neurosci</source>. <year>2002</year>;<volume>5</volume>(<issue>4</issue>):<fpage>356</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn831" xlink:type="simple">10.1038/nn831</ext-link></comment> <object-id pub-id-type="pmid">11896400</object-id>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carlson</surname> <given-names>NL</given-names></name>, <name name-style="western"><surname>Ming</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>DeWeese</surname> <given-names>MR</given-names></name>. <article-title>Sparse codes for speech predict spectrotemporal receptive fields in the inferior colliculus</article-title>. <source>PLoS Comput Biol</source>. <year>2012</year>;<volume>8</volume>(<issue>7</issue>):<fpage>e1002594</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002594" xlink:type="simple">10.1371/journal.pcbi.1002594</ext-link></comment> <object-id pub-id-type="pmid">22807665</object-id>; PubMed Central PMCID: PMCPMC3395612.</mixed-citation></ref>
<ref id="pcbi.1006766.ref024"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Nagamine T, Seltzer ML, Mesgarani N. Exploring how deep neural networks form phonemic categories. INTERSPEECH; Dresden, Germany,2015. p. 1912–6.</mixed-citation></ref>
<ref id="pcbi.1006766.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hromádka</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>DeWeese</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Zador</surname> <given-names>AM</given-names></name>. <article-title>Sparse representation of sounds in the unanesthetized auditory cortex</article-title>. <source>PLoS Biol</source>. <year>2008</year>;<volume>6</volume>(<issue>1</issue>):<fpage>e16</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.0060016" xlink:type="simple">10.1371/journal.pbio.0060016</ext-link></comment> <object-id pub-id-type="pmid">18232737</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schneider</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Woolley</surname> <given-names>SM</given-names></name>. <article-title>Sparse and background-invariant coding of vocalizations in auditory scenes</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>79</volume>(<issue>1</issue>):<fpage>141</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.04.038" xlink:type="simple">10.1016/j.neuron.2013.04.038</ext-link></comment> <object-id pub-id-type="pmid">23849201</object-id>; PubMed Central PMCID: PMCPMC3713513.</mixed-citation></ref>
<ref id="pcbi.1006766.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barth</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Poulet</surname> <given-names>JF</given-names></name>. <article-title>Experimental evidence for sparse firing in the neocortex</article-title>. <source>Trends Neurosci</source>. <year>2012</year>;<volume>35</volume>(<issue>6</issue>):<fpage>345</fpage>–<lpage>55</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2012.03.008" xlink:type="simple">10.1016/j.tins.2012.03.008</ext-link></comment> <object-id pub-id-type="pmid">22579264</object-id>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>B</given-names></name>. <source>Sparsity-regularized HMAX for visual recognition</source>. Plos One. <year>2014</year>;<volume>9</volume>(<issue>1</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0081813" xlink:type="simple">10.1371/journal.pone.0081813</ext-link></comment> WOS:000329460100002. <object-id pub-id-type="pmid">24392078</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Andoni</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Pollak</surname> <given-names>GD</given-names></name>. <article-title>Spectrotemporal receptive fields in the inferior colliculus revealing selectivity for spectral motion in conspecific vocalizations</article-title>. <source>J Neurosci</source>. <year>2007</year>;<volume>27</volume>(<issue>18</issue>):<fpage>4882</fpage>–<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4342-06.2007" xlink:type="simple">10.1523/JNEUROSCI.4342-06.2007</ext-link></comment> <object-id pub-id-type="pmid">17475796</object-id>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Qiu</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Escabí</surname> <given-names>MA</given-names></name>. <article-title>Gabor analysis of auditory midbrain receptive fields: spectro-temporal and binaural composition</article-title>. <source>Journal of Neurophysiology</source>. <year>2003</year>;<volume>90</volume>(<issue>1</issue>):<fpage>456</fpage>–<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00851.2002" xlink:type="simple">10.1152/jn.00851.2002</ext-link></comment> <object-id pub-id-type="pmid">12660353</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>, <name name-style="western"><surname>Woolley</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Hsu</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fremouw</surname> <given-names>T</given-names></name>. <article-title>Methods for the analysis of auditory processing in the brain</article-title>. <source>Ann N Y Acad Sci</source>. <year>2004</year>;<volume>1016</volume>:<fpage>187</fpage>–<lpage>207</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1196/annals.1298.020" xlink:type="simple">10.1196/annals.1298.020</ext-link></comment> <object-id pub-id-type="pmid">15313776</object-id>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>, <name name-style="western"><surname>Sen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Doupe</surname> <given-names>AJ</given-names></name>. <article-title>Spectral-temporal receptive fields of nonlinear auditory neurons obtained using natural sounds</article-title>. <source>Journal of Neuroscience</source>. <year>2000</year>;<volume>20</volume>(<issue>6</issue>):<fpage>2315</fpage>–<lpage>31</lpage>. WOS:000085724200031. <object-id pub-id-type="pmid">10704507</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>, <name name-style="western"><surname>David</surname> <given-names>SV</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Hsu</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vinje</surname> <given-names>WE</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to natural stimuli</article-title>. <source>Network-Comp Neural</source>. <year>2001</year>;<volume>12</volume>(<issue>3</issue>):<fpage>289</fpage>–<lpage>316</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/0954-898x/12/3/304" xlink:type="simple">10.1088/0954-898x/12/3/304</ext-link></comment> WOS:000170945800006.</mixed-citation></ref>
<ref id="pcbi.1006766.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rodriguez</surname> <given-names>FA</given-names></name>, <name name-style="western"><surname>Read</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Escabi</surname> <given-names>MA</given-names></name>. <article-title>Spectral and temporal modulation tradeoff in the inferior colliculus</article-title>. <source>J Neurophysiol</source>. <year>2010</year>;<volume>103</volume>(<issue>2</issue>):<fpage>887</fpage>–<lpage>903</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00813.2009" xlink:type="simple">10.1152/jn.00813.2009</ext-link></comment> <object-id pub-id-type="pmid">20018831</object-id>; PubMed Central PMCID: PMCPMC2822687.</mixed-citation></ref>
<ref id="pcbi.1006766.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lesica</surname> <given-names>NA</given-names></name>, <name name-style="western"><surname>Grothe</surname> <given-names>B</given-names></name>. <article-title>Dynamic spectrotemporal feature selectivity in the auditory midbrain</article-title>. <source>Journal of Neuroscience</source>. <year>2008</year>;<volume>28</volume>(<issue>21</issue>):<fpage>5412</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0073-08.2008" xlink:type="simple">10.1523/JNEUROSCI.0073-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18495875</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref036"><label>36</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Rokach</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Maimon</surname> <given-names>O</given-names></name>. <chapter-title>Clustering methods</chapter-title>. <source>Data mining and knowledge discovery handbook</source>: <publisher-name>Springer</publisher-name>; <year>2005</year>. p. <fpage>321</fpage>–<lpage>52</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref037"><label>37</label><mixed-citation publication-type="other" xlink:type="simple">Räsänen O, Nagamine T, Mesgarani N, Papafragou A, Grodner D, Mirman D, et al. Analyzing distributional learning of phonemic categories in unsupervised deep neural networks. Annual Conference of the Cognitive Science Society2016.</mixed-citation></ref>
<ref id="pcbi.1006766.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Sparse coding with an overcomplete basis set: a strategy employed by V1?</article-title> <source>Vision Res</source>. <year>1997</year>;<volume>37</volume>(<issue>23</issue>):<fpage>3311</fpage>–<lpage>25</lpage>. <object-id pub-id-type="pmid">9425546</object-id>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willmore</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Mazer</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>J</given-names></name>. <article-title>Sparse coding in striate and extrastriate visual cortex</article-title>. <source>J Neurophysiol</source>. <year>2011</year>;<volume>105</volume>(<issue>6</issue>):<fpage>2907</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00594.2010" xlink:type="simple">10.1152/jn.00594.2010</ext-link></comment> <object-id pub-id-type="pmid">21471391</object-id>; PubMed Central PMCID: PMCPMC3118756.</mixed-citation></ref>
<ref id="pcbi.1006766.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willmore</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Tolhurst</surname> <given-names>D</given-names></name>. <article-title>Characterizing the sparseness of neural codes</article-title>. <source>Network</source>. <year>2001</year>;<volume>12</volume>(<issue>3</issue>):<fpage>255</fpage>–<lpage>70</lpage>. <object-id pub-id-type="pmid">11563529</object-id>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Unraveling the principles of auditory cortical processing: can we learn from the visual system?</article-title> <source>Nat Neurosci</source>. <year>2009</year>;<volume>12</volume>(<issue>6</issue>):<fpage>698</fpage>–<lpage>701</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2308" xlink:type="simple">10.1038/nn.2308</ext-link></comment> <object-id pub-id-type="pmid">19471268</object-id>; PubMed Central PMCID: PMCPMC3657701.</mixed-citation></ref>
<ref id="pcbi.1006766.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>, <name name-style="western"><surname>Elie</surname> <given-names>JE</given-names></name>. <article-title>Neural processing of natural sounds</article-title>. <source>Nat Rev Neurosci</source>. <year>2014</year>;<volume>15</volume>(<issue>6</issue>):<fpage>355</fpage>–<lpage>66</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3731" xlink:type="simple">10.1038/nrn3731</ext-link></comment> <object-id pub-id-type="pmid">24840800</object-id>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bell</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Learning the higher-order structure of a natural sound</article-title>. <source>Network: Computation in Neural Systems</source>. <year>1996</year>;<volume>7</volume>(<issue>2</issue>):<fpage>261</fpage>–<lpage>6</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref044"><label>44</label><mixed-citation publication-type="other" xlink:type="simple">Zeiler MD, Taylor GW, Fergus R. Adaptive deconvolutional networks for mid and high level feature learning. IEEE International Conference on Computer Vision (ICCV) 2011. p. 2018–25.</mixed-citation></ref>
<ref id="pcbi.1006766.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhuang</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Yamins</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>X</given-names></name>. <article-title>Deep learning predicts correlation between a functional signature of higher visual areas and sparse firing of neurons</article-title>. <source>Front Comput Neurosc</source>. <year>2017</year>;<volume>11</volume>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Polka</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Werker</surname> <given-names>JF</given-names></name>. <article-title>Developmental changes in perception of nonnative vowel contrasts</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>1994</year>;<volume>20</volume>(<issue>2</issue>):<fpage>421</fpage>. <object-id pub-id-type="pmid">8189202</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maye</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Werker</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Gerken</surname> <given-names>L</given-names></name>. <article-title>Infant sensitivity to distributional information can affect phonetic discrimination</article-title>. <source>Cognition</source>. <year>2002</year>;<volume>82</volume>(<issue>3</issue>):<fpage>B101</fpage>–<lpage>B11</lpage>. <object-id pub-id-type="pmid">11747867</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vallabha</surname> <given-names>GK</given-names></name>, <name name-style="western"><surname>McClelland</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Pons</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Werker</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Amano</surname> <given-names>S</given-names></name>. <article-title>Unsupervised learning of vowel categories from infant-directed speech</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2007</year>;<volume>104</volume>(<issue>33</issue>):<fpage>13273</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref049"><label>49</label><mixed-citation publication-type="other" xlink:type="simple">Miyazawa K, Kikuchi H, Mazuka R. Unsupervised learning of vowels from continuous speech based on self-organized phoneme acquisition model. Eleventh Annual Conference of the International Speech Communication Association2010.</mixed-citation></ref>
<ref id="pcbi.1006766.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peperkamp</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Le Calvez</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Nadal</surname> <given-names>J-P</given-names></name>, <name name-style="western"><surname>Dupoux</surname> <given-names>E</given-names></name>. <article-title>The acquisition of allophonic rules: Statistical learning with linguistic constraints</article-title>. <source>Cognition</source>. <year>2006</year>;<volume>101</volume>(<issue>3</issue>):<fpage>B31</fpage>–<lpage>B41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cognition.2005.10.006" xlink:type="simple">10.1016/j.cognition.2005.10.006</ext-link></comment> <object-id pub-id-type="pmid">16364279</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref051"><label>51</label><mixed-citation publication-type="other" xlink:type="simple">Synnaeve G, Schatz T, Dupoux E, editors. Phonetics embedding learning with side information. IEEE Spoken Language Technology Workshop (SLT); 2014: IEEE.</mixed-citation></ref>
<ref id="pcbi.1006766.ref052"><label>52</label><mixed-citation publication-type="other" xlink:type="simple">Thiolliere R, Dunbar E, Synnaeve G, Versteegh M, Dupoux E. A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling. Sixteenth Annual Conference of the International Speech Communication Association2015.</mixed-citation></ref>
<ref id="pcbi.1006766.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Feldman</surname> <given-names>NH</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Goldwater</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Morgan</surname> <given-names>JL</given-names></name>. <article-title>A role for the developing lexicon in phonetic category acquisition</article-title>. <source>Psychological Review</source>. <year>2013</year>;<volume>120</volume>(<issue>4</issue>):<fpage>751</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0034245" xlink:type="simple">10.1037/a0034245</ext-link></comment> <object-id pub-id-type="pmid">24219848</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref054"><label>54</label><mixed-citation publication-type="other" xlink:type="simple">Elsner M, Goldwater S, Eisenstein J. Bootstrapping a unified model of lexical and phonetic acquisition. Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1: Association for Computational Linguistics; 2012. p. 184–93.</mixed-citation></ref>
<ref id="pcbi.1006766.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Werker</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Curtin</surname> <given-names>S</given-names></name>. <article-title>PRIMIR: A developmental framework of infant speech processing</article-title>. <source>Language Learning and Development</source>. <year>2005</year>;<volume>1</volume>(<issue>2</issue>):<fpage>197</fpage>–<lpage>234</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pasupathy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Connor</surname> <given-names>CE</given-names></name>. <article-title>Responses to contour features in macaque area V4</article-title>. <source>J Neurophysiol</source>. <year>1999</year>;<volume>82</volume>(<issue>5</issue>):<fpage>2490</fpage>–<lpage>502</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1999.82.5.2490" xlink:type="simple">10.1152/jn.1999.82.5.2490</ext-link></comment> <object-id pub-id-type="pmid">10561421</object-id>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>El-Shamayleh</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Pasupathy</surname> <given-names>A</given-names></name>. <article-title>Contour curvature as an invariant code for objects in visual area V4</article-title>. <source>Journal of Neuroscience</source>. <year>2016</year>;<volume>36</volume>(<issue>20</issue>):<fpage>5532</fpage>–<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4139-15.2016" xlink:type="simple">10.1523/JNEUROSCI.4139-15.2016</ext-link></comment> WOS:000378329700011. <object-id pub-id-type="pmid">27194333</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>King</surname> <given-names>PD</given-names></name>, <name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>DeWeese</surname> <given-names>MR</given-names></name>. <article-title>Inhibitory interneurons decorrelate excitatory cells to drive sparse code formation in a spiking model of V1</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>(<issue>13</issue>):<fpage>5475</fpage>–<lpage>85</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4188-12.2013" xlink:type="simple">10.1523/JNEUROSCI.4188-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23536063</object-id>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kouh</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>A canonical neural circuit for cortical nonlinear operations</article-title>. <source>Neural Comput</source>. <year>2008</year>;<volume>20</volume>(<issue>6</issue>):<fpage>1427</fpage>–<lpage>51</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2008.02-07-466" xlink:type="simple">10.1162/neco.2008.02-07-466</ext-link></comment> <object-id pub-id-type="pmid">18254695</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref060"><label>60</label><mixed-citation publication-type="other" xlink:type="simple">Knoblich U, Bouvrie J, Poggio T, editors. Biophysical models of neural computation: Max and tuning circuits. International Workshop on Web Intelligence Meets Brain Informatics; 2006: Springer.</mixed-citation></ref>
<ref id="pcbi.1006766.ref061"><label>61</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <source>Theoretical Neuroscience: Cambridge</source>, <publisher-loc>MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>2001</year>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Garofalo</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Lamel</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Fisher</surname> <given-names>WM</given-names></name>, <name name-style="western"><surname>Fiscus</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Pallett</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Dahlgren</surname> <given-names>NL</given-names></name>. <article-title>The DARPA TIMIT acoustic-phonetic continuous speech corpus cdrom</article-title>. <source>Linguistic Data Consortium</source>. <year>1993</year>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Slaney</surname> <given-names>M.</given-names></name> <article-title>Auditory toolbox. Interval Research Corporation</article-title>, Tech Rep. <year>1998</year>;<volume>10</volume>:<fpage>1998</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nat Neurosci</source>. <year>1999</year>;<volume>2</volume>(<issue>11</issue>):<fpage>1019</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/14819" xlink:type="simple">10.1038/14819</ext-link></comment> <object-id pub-id-type="pmid">10526343</object-id>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Boser</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Denker</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Henderson</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Howard</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Hubbard</surname> <given-names>W</given-names></name>, <etal>et al</etal>. <article-title>Backpropagation applied to handwritten zip code recognition</article-title>. <source>Neural Comput</source>. <year>1989</year>;<volume>1</volume>(<issue>4</issue>):<fpage>541</fpage>–<lpage>51</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006766.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>. <article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>–<lpage>44</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14539" xlink:type="simple">10.1038/nature14539</ext-link></comment> WOS:000355286600030. <object-id pub-id-type="pmid">26017442</object-id></mixed-citation></ref>
<ref id="pcbi.1006766.ref067"><label>67</label><mixed-citation publication-type="other" xlink:type="simple">Mairal J, Bach F, Ponce J, Sapiro G. Online dictionary learning for sparse coding. International Conference on Machine Learning; Montreal, Quebec, Canada2009. p. 689–96.</mixed-citation></ref>
<ref id="pcbi.1006766.ref068"><label>68</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Chomsky</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Halle</surname> <given-names>M</given-names></name>. <source>The Sound Pattern of English</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Harper and Row</publisher-name>; <year>1968</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>