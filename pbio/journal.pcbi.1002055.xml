<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">10-PLCB-RA-2833</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002055</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Behavioral neuroscience</subject>
              <subject>Computational neuroscience</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Computer science</subject>
          <subj-group>
            <subject>Control engineering</subject>
            <subj-group>
              <subject>Control systems</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
          <subject>Computer Science</subject>
        </subj-group>
      </article-categories><title-group><article-title>Speed/Accuracy Trade-Off between the Habitual and the Goal-Directed Processes</article-title><alt-title alt-title-type="running-head">Habitual and Goal-directed Systems in Competition</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
          <name name-style="western">
            <surname>Keramati</surname>
            <given-names>Mehdi</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
          <name name-style="western">
            <surname>Dezfouli</surname>
            <given-names>Amir</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Piray</surname>
            <given-names>Payam</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>School of Management and Economics, Sharif University of Technology, Tehran, Iran</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Control and Intelligent Processing Center Of Excellence, School of Electrical and Computer Engineering, University of Tehran, Tehran, Iran</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Behrens</surname>
            <given-names>Tim</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">University of Oxford, United Kingdom</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">mohammadmahdi.keramati@ens.fr</email></corresp>
        <fn fn-type="con">
          <p>Wrote the paper: MK. Designed the model: MK AD. Simulated the model: AD PP. Analyzed simulation results: MK AD.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>5</month>
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>26</day>
        <month>5</month>
        <year>2011</year>
      </pub-date><volume>7</volume><issue>5</issue><elocation-id>e1002055</elocation-id><history>
        <date date-type="received">
          <day>14</day>
          <month>9</month>
          <year>2010</year>
        </date>
        <date date-type="accepted">
          <day>31</day>
          <month>3</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Keramati et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>Instrumental responses are hypothesized to be of two kinds: habitual and goal-directed, mediated by the sensorimotor and the associative cortico-basal ganglia circuits, respectively. The existence of the two heterogeneous associative learning mechanisms can be hypothesized to arise from the comparative advantages that they have at different stages of learning. In this paper, we assume that the goal-directed system is behaviourally flexible, but slow in choice selection. The habitual system, in contrast, is fast in responding, but inflexible in adapting its behavioural strategy to new conditions. Based on these assumptions and using the computational theory of reinforcement learning, we propose a normative model for arbitration between the two processes that makes an approximately optimal balance between search-time and accuracy in decision making. Behaviourally, the model can explain experimental evidence on behavioural sensitivity to outcome at the early stages of learning, but insensitivity at the later stages. It also explains that when two choices with equal incentive values are available concurrently, the behaviour remains outcome-sensitive, even after extensive training. Moreover, the model can explain choice reaction time variations during the course of learning, as well as the experimental observation that as the number of choices increases, the reaction time also increases. Neurobiologically, by assuming that phasic and tonic activities of midbrain dopamine neurons carry the reward prediction error and the average reward signals used by the model, respectively, the model predicts that whereas phasic dopamine indirectly affects behaviour through reinforcing stimulus-response associations, tonic dopamine can directly affect behaviour through manipulating the competition between the habitual and the goal-directed systems and thus, affect reaction time.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>When confronted with different alternatives, animals can respond either based on their pre-established habits, or by considering the short- and long-term consequences of each option. Whereas habitual decision making is fast, goal-directed thinking is a time-consuming task. Instead, habits are inflexible after being consolidated, but goal-directed decision making can rapidly adapt the animal's strategy after a change in environmental conditions. Based on these features of the two decision making systems, we suggest a computational model using the reinforcement learning framework, that makes a balance between the speed of decision making and behavioural flexibility. The behaviour of the model is consistent with the observation that at the early stages of learning, animals behave in a goal-directed way (flexible, but slow), but after extensive learning, their responses become habitual (inflexible, but fast). Moreover, the model explains that the animal's reaction time must decrease through the course of learning, as the habitual system takes control over behaviour. The model also attributes a functional role to the tonic activity of dopamine neurons in balancing the competition between the habitual and the goal-directed systems.</p>
      </abstract><funding-group><funding-statement>The authors received no specific funding for this work.</funding-statement></funding-group><counts>
        <page-count count="21"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>A very basic assumption in theories of animal decision making is that animals possess a complicated learning machinery that aims for maximizing rewards and minimizing threats to homeostasis <xref ref-type="bibr" rid="pcbi.1002055-Rangel1">[1]</xref>. The primary question within this framework is then how the brain, constrained by computational limitations, uses past experiences to predict rewarding and punishing consequences of possible responses.</p>
      <p>The dual-process theory of decision making proposes that two distinct brain mechanisms are involved in instrumental responding: the “habitual”, and the “goal-directed” systems <xref ref-type="bibr" rid="pcbi.1002055-Dickinson1">[2]</xref>. The habitual system is behaviourally defined as being insensitive to outcome-devaluation, as well as contingency-degradation. For example, in the experimental paradigm of outcome-devaluation, the animal is first trained for an extensive period to perform a sequence of actions for gaining access to a particular outcome. The outcome is then devaluated by being paired with an aversive stimuli (conditioned taste-aversion), or by over-consumption of that outcome (sensory-specific satiety). The critical observation is that in the test phase, which is performed in extinction, the animal continues responding for the outcome, even though it is devaluated. The goal-directed process, on the other hand, is defined as being sensitive to outcome-devaluation and contingency-degradation. This behavioural sensitivity is shown to emerge when the pre-devaluation training phase is limited, rather than extensive Adams <xref ref-type="bibr" rid="pcbi.1002055-Adams1">[3]</xref>.</p>
      <p>Based on these behavioural patterns, two different types of associative memory structures are proposed for the two systems. The behavioural autonomy demonstrated by the habitual system is hypothesized to be based on the establishment of associations between contextual stimuli and responses (S-R), whereas representational flexibility of the goal-directed system is suggested to rely on associations between actions and outcomes (A-O).</p>
      <p>A wide range of electrophysiological, brain imaging, and lesion studies indicate that different, and topographically segregated cortico-striato-pallido-thalamo-cortical loops underlie the two learning mechanisms discussed above (see <xref ref-type="bibr" rid="pcbi.1002055-Balleine1">[4]</xref> for review). The sensorimotor loop, comprising of glutamatergic projections from infralimbic cortices to dorsolateral striatum, is shown to be involved in habitual responding. In addition, phasic activity of dopamine (DA) neurons, originating from midbrain and projecting to different areas of the striatum is hypothesized to carry a reinforcement signal, that is shown to play an essential role in the formation of S-R associations. The associative loop, on the other hand, is proposed to underlie goal-directed responding. Some critical components of this loop include dorsomedial striatum and paralimbic cortex.</p>
      <p>The existence of two parallel neuronal circuits involved in decision making arises the question of how the two systems compete for taking control over behaviour. Daw and colleagues, proposed a reinforcement learning model in which, the competition between the two systems is based on the relative uncertainty of the systems in estimating the value of different actions <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>. Their model can explain some behavioural aspects of interaction between the two systems. A critical analysis of their model is provided in the <xref ref-type="sec" rid="s4">discussion</xref> section.</p>
      <p>In this paper, based on the model proposed in <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>, and using the idea that reward maximization is the performance measure of the decision making system of animals, we propose a novel, normative arbitration mechanism between the two systems that can explain a wider range of behavioural data. The basic assumption of the model is that the habitual system is fast in responding, but inflexible in adapting its behavioural strategy to new conditions. The goal-directed system, in contrast, can rapidly adapt its instrumental knowledge, but is considerably slower than the habitual system in making decisions. In the proposed model, not only the two systems seek to maximize the accrual of reward -by different algorithms-, but the arbitration mechanism between them is also designed in a way to exploit the comparative advantages of the two systems in value estimation.</p>
      <p>As a direct experimental observation for supporting the assumptions of the model, it has been reported classically that when rats traverse a T-maze to obtain access to an outcome, at the choice points, they pause and vicariously sample the alternative choices before committing to a decision <xref ref-type="bibr" rid="pcbi.1002055-Tolman1">[6]</xref>–<xref ref-type="bibr" rid="pcbi.1002055-Muenzinger1">[8]</xref>. This behaviour, called “vicarious trial-and-error” (VTE), is defined by head movements from one stimulus to another at a choice point, during simultaneous discrimination learning <xref ref-type="bibr" rid="pcbi.1002055-Brown1">[9]</xref>. This hesitation- and conflict-like behaviour is suggested to be indicative of deliberation or active processing by a planning system <xref ref-type="bibr" rid="pcbi.1002055-Tolman1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Tolman2">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Buckner1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Redish1">[11]</xref>. Important for our discussion, it has been shown that after extensive learning, VTE frequency declines significantly <xref ref-type="bibr" rid="pcbi.1002055-Tolman1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Hu1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Munn1">[13]</xref>. This observation is interpreted as a transition of behavioural control from the planning system to the habitual one, and shows difference in the decision-time between habitual and goal-directed responding <xref ref-type="bibr" rid="pcbi.1002055-Redish2">[14]</xref>.</p>
      <p>Beside being supported by the VTE behaviour, the assumption about the relative speed and flexibility of the two systems allows the model to explain some behavioural data on choice reaction time. The model also predicts that whereas phasic activity of DA neurons indirectly affects the arbitration through intervening in habit formation, tonic activity of DA neurons can directly influence the competition by modulating the cost of goal-directed deliberation.</p>
    </sec>
    <sec id="s2">
      <title>Model</title>
      <sec id="s2a">
        <title>The Preliminaries</title>
        <p>Reinforcement learning (RL) is learning how to establish different types if instrumental associations for the purpose of maximizing the accrual of rewards <xref ref-type="bibr" rid="pcbi.1002055-Sutton1">[15]</xref>. In the RL framework, stimuli and responses are referred to as states and actions, respectively. An RL agent perceives its surrounding environment in the form of a finite set of states, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e001" xlink:type="simple"/></inline-formula>, in each of which, one action among a finite set of actions, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e002" xlink:type="simple"/></inline-formula>, can be taken. The dynamics of the environment can be formulated by a transition function and a reward function. The transition function, denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e003" xlink:type="simple"/></inline-formula>, represents the probability of reaching state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e004" xlink:type="simple"/></inline-formula> after taking action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e005" xlink:type="simple"/></inline-formula> at state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e006" xlink:type="simple"/></inline-formula>. The reward function, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e007" xlink:type="simple"/></inline-formula>, indicates the probability of receiving reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e008" xlink:type="simple"/></inline-formula>, by executing action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e009" xlink:type="simple"/></inline-formula> at state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e010" xlink:type="simple"/></inline-formula>. This structure, known as the Markov Decision Process (MDP), can be demonstrated by a 4-tuple, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e011" xlink:type="simple"/></inline-formula>. At each time-step, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e012" xlink:type="simple"/></inline-formula>, the agent is in a certain state, say <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e013" xlink:type="simple"/></inline-formula>, and makes a choice, say <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e014" xlink:type="simple"/></inline-formula>, from several alternatives on the basis of subjective values that it has assigned to them through its past experiences in the environment. This value, denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e015" xlink:type="simple"/></inline-formula>, is aimed to be proportional to the sum of discounted rewards that are expected to be received after taking action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e016" xlink:type="simple"/></inline-formula> onward:</p>
        <p>
          <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e017" xlink:type="simple"/><label>(1)</label></disp-formula>
        </p>
        <p><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e018" xlink:type="simple"/></inline-formula> is the discount factor, which indicates the relative incentive value of delayed rewards compared to immediate ones.</p>
        <p>Model-free and model-based RL, are two variants of reinforcement learning with behavioural characteristics similar to the habitual and goal-directed systems, respectively <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>. These two variants are in fact two different mechanisms for estimating the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e019" xlink:type="simple"/></inline-formula>-function of equation 1 , based on the feedbacks, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e020" xlink:type="simple"/></inline-formula>, that the animal receives from the environment through learning.</p>
        <p>In temporal difference RL (TDRL), which is an implementation of model-free RL, a prediction error signal, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e021" xlink:type="simple"/></inline-formula>, is calculated each time the agent takes an action and receives a reward from the environment. This prediction error is calculated by comparing the prior expected value of taking that action, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e022" xlink:type="simple"/></inline-formula>, with its realized value after receiving reward, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e023" xlink:type="simple"/></inline-formula>:</p>
        <p>
          <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e024" xlink:type="simple"/><label>(2)</label></disp-formula>
        </p>
        <p><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e025" xlink:type="simple"/></inline-formula> is the maximum value of all feasible actions available at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e026" xlink:type="simple"/></inline-formula>. The prediction error signal is hypothesized to be carried by the burst firing of midbrain dopamine neurons. This signal can be used to update the estimated value of actions:</p>
        <p>
          <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e027" xlink:type="simple"/><label>(3)</label></disp-formula>
        </p>
        <p><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e028" xlink:type="simple"/></inline-formula> is the learning rate, representing the degree to which the prediction error adjusts the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e029" xlink:type="simple"/></inline-formula>-values of the habitual system. Assuming that the reward and transition functions of the environment are stationary, equations 2 and 3 will lead the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e030" xlink:type="simple"/></inline-formula>-values to eventually converge through learning to the expected sum of discounted rewards. Therefore, after a sufficiently long learning period, the habitual system will be equipped with the instrumental knowledge required for taking the optimal behavioural strategy. This optimal decision making is achievable without the agent knowing the dynamics of the environment. This is why this mechanism is known as model-free reinforcement learning. The gradual convergence of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e031" xlink:type="simple"/></inline-formula>-values to their steady levels, leads the habitual system toward being insensitive to sudden changes in the environment's dynamics, such as outcome-devaluation and contingency degradation. Instead, as all the information required for making a choice between several alternatives is cached in S-R associations through the course of learning, the habitual responses can be made within a short interval after the stimulus is presented.</p>
        <p>Instead of keeping and updating point estimations, by using Kalman reinforcement learning <xref ref-type="bibr" rid="pcbi.1002055-Geist1">[16]</xref>, the habitual system in our model keeps probability distributions for the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e032" xlink:type="simple"/></inline-formula>-values of each state-action pair (See Methods for mathematical details). These probability distributions contain substantial information that will be later used for arbitration between the habitual and the goal-directed systems.</p>
        <p>In contrast to the habitual process, the value estimation mechanism in a model-based RL is based on the transition and reward functions that the agent has learned through past experiences <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Sutton1">[15]</xref>. In fact, through the course of learning, the animal is hypothesized to learn the causal relationship between various actions and their outcomes, as well as the incentive value of different outcomes. Based on the former component of the environment's dynamics, the goal-directed system can deliberate the short-term and long-term consequences of each sequence of actions. Then by using the learned reward function, calculating the expected value for each action sequence will be possible.</p>
        <p>Letting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e033" xlink:type="simple"/></inline-formula> denote the value of each action calculated by this method, the recursive value-iteration algorithm below can compute it (See Methods for algorithmic details):</p>
        <p>
          <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e034" xlink:type="simple"/><label>(4)</label></disp-formula>
        </p>
        <p>Due to employing the estimated model of the environment for value estimation, the goal-directed system can rapidly revise the estimated values after an environmental change, as soon as the transition and reward functions are adapted to the new conditions. This can explain why the goal-directed system is sensitive to outcome-devaluation and contingency-degradation <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>. But according to this computational mechanism, one would expect the value estimation by the goal-directed system to take a considerable amount of time, as compared to the habit-based decision time. The difference in speed and accuracy of value estimation by the habitual and goal-directed processes is the core assumption of the arbitration mechanism proposed in this paper, that allows the model to explain a set of behavioural and neurobiological data.</p>
      </sec>
      <sec id="s2b">
        <title>Speed/Accuracy Trade-off</title>
        <p>If we assume for simplicity that the goal-directed system is always perfectly aware of the environment's dynamics, then it can be concluded that this system has perfect information about the value of different choices at each state. This is a valid assumption in most of the experimental paradigms considered in this paper. For example, in outcome-devaluation experiments, due to the existence of a re-exposure phase between training and test phases, the subjects have the opportunity to learn new incentive values for the outcomes. Although the goal-directed system, due to its flexible nature, will always have “more accurate” value estimations compared to the habitual system, the assumption of having “perfect” information might be violated under some conditions (like reversal learning tasks). This violation will naturally lead to some irrational arbitrations between the systems.</p>
        <p>Thus, the advantage of using the goad-directed system can be approximated by the advantage of having perfect information about the value of actions. But this perfect information can be extracted from transition and reward functions at the cost of losing time; a time which could be instead used for taking rapid habitual actions and thus, receiving less rewards in magnitude, but more in frequency. This trade-off is the essence of the arbitration rule between the two systems that we propose here. In other words, we hypothesize that animals balance the benefits of deliberations against their cost. Its benefit is proportional to the value of having perfect information, and its cost is equal to the potential reward that could be acquired during the time that the organism is waiting for the goal-directed system to deliberate.</p>
        <p>As illustrated schematically in <xref ref-type="fig" rid="pcbi-1002055-g001">Figure 1</xref> , at each time-step, the habitual system has an imperfect estimate for the value of each action in the form of a distribution function. Using these distribution functions, the expected benefit of estimating the value of each action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e035" xlink:type="simple"/></inline-formula> by the goal-directed system is computed (see below). This benefit, called “value of perfect information”, can be denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e036" xlink:type="simple"/></inline-formula>. The cost of deliberation, denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e037" xlink:type="simple"/></inline-formula>, is also computed separately (See below). Having the cost and benefit of deliberation for each action, if the benefit is greater than the cost, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e038" xlink:type="simple"/></inline-formula>, the arbitrator will decide to run the goal-directed system for estimating the value of action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e039" xlink:type="simple"/></inline-formula>; otherwise, the value of action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e040" xlink:type="simple"/></inline-formula> that will be used for action selection will be equal to the mean of the distribution function cached in the habitual system for that action. Finally, based on the estimated values of different actions that have been derived from either of the two instrumental systems, a softmax action selection rule, in which the probability of choosing each action increases exponentially with its estimated value, can be used (See Methods). Upon executing the selected action and consequently receiving a reward and entering a new state, both the habitual and goal-directed systems will update their instrumental knowledge for future exploitations.</p>
        <fig id="pcbi-1002055-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002055.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>An example for showing the proposed arbitration mechanism between the two processes.</title>
            <p>(A) The agent is at state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e041" xlink:type="simple"/></inline-formula> and three choices are available: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e042" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e043" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e044" xlink:type="simple"/></inline-formula>. The habitual system, as shown, has an estimate for the value of each action in the form of probability distribution functions, based on its previous experiences. These uncertain estimated values are then compared to each other in order to calculate the expected gain of having the exact value of each action (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e045" xlink:type="simple"/></inline-formula>). In the case of this example, action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e046" xlink:type="simple"/></inline-formula> has the highest mean value, according to the uncertain knowledge in the habitual system. However, it is probable that the exact value of this action be less than the mean value of action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e047" xlink:type="simple"/></inline-formula>. In that case, the best strategy would be to choose action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e048" xlink:type="simple"/></inline-formula>, rather that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e049" xlink:type="simple"/></inline-formula>. Thus, it is worth knowing the exact value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e050" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e051" xlink:type="simple"/></inline-formula> has a high value). (B) The exact value of actions is supposed to be attainable if a tree search is performed in the decision tree, by the goal-directed system. However, the benefit of search must be higher than its cost. The benefit of deliberation for each action is equal to its <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e052" xlink:type="simple"/></inline-formula> signal, whereas the cost of deliberation is equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e053" xlink:type="simple"/></inline-formula>, which is the total reward that could be potentially acquired during the deliberation time, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e054" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e055" xlink:type="simple"/></inline-formula> is the average over acquired rewards during some past actions). Since for action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e056" xlink:type="simple"/></inline-formula>, the benefit of deliberation has exceeded its cost, the goal-directed system is engaged in value estimation. (C) Finally, action selection is carried out based on the estimated values for actions, which have come from either the habitual (for actions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e057" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e058" xlink:type="simple"/></inline-formula>) or the goal-directed (for action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e059" xlink:type="simple"/></inline-formula>) system. For those actions that are not deliberated, the mean value of their distribution function is used for action selection.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.g001" xlink:type="simple"/>
        </fig>
        <p>Based on the decision theoretic ideas of “value of information” <xref ref-type="bibr" rid="pcbi.1002055-Howard1">[17]</xref>, a measure has been proposed in <xref ref-type="bibr" rid="pcbi.1002055-Dearden1">[18]</xref> for information value in the form of expected gains in performance, resulted from improved policies if perfect information was available. This measure, which is computed from probability distributions over the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e060" xlink:type="simple"/></inline-formula>-value of choices, is used in the original paper for proposing an optimal solution for the exploration/exploitation trade-off. Here, we use the same measure for estimating the benefit of goal-directed search.</p>
        <p>To see how this measure can be computed, assume that the animal is in the state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e061" xlink:type="simple"/></inline-formula>, and one of the available actions is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e062" xlink:type="simple"/></inline-formula>, with the estimated value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e063" xlink:type="simple"/></inline-formula> assigned to it by the habitual system. At this stage, we are interested to know how much the animal will benefit if it understands that the true value of actions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e064" xlink:type="simple"/></inline-formula> is equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e065" xlink:type="simple"/></inline-formula>, rather than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e066" xlink:type="simple"/></inline-formula>. Obviously, any new information about the exact value of an action is valuable only if it improves the previous policy of the animal that was based on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e067" xlink:type="simple"/></inline-formula>. This can happen in two scenarios: (a) when knowing the exact value signifies that an action previously considered to be sub-optimal is revealed to be the best choice, and (b) when the new knowledge shows that the action which was considered to be the best, is actually inferior to some other actions. Therefore, the gain of knowing that the true value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e068" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e069" xlink:type="simple"/></inline-formula> can be defined as <xref ref-type="bibr" rid="pcbi.1002055-Dearden1">[18]</xref>:</p>
        <p>
          <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e070" xlink:type="simple"/><label>(5)</label></disp-formula>
        </p>
        <p><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e071" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e072" xlink:type="simple"/></inline-formula> are the actions with the best and second best expected values, respectively. In the definition of the gain function, the first and the second rules correspond to the second and the first scenarios discussed above, respectively.</p>
        <p>According to this definition, calculating the gain function for each choice requires knowing the true value of that state-action pair, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e073" xlink:type="simple"/></inline-formula>, which is unavailable. But, as the habitual system is assumed to keep a probability distribution function for the value of actions, the agent has access to the probability of possible values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e074" xlink:type="simple"/></inline-formula>. Using this probability distribution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e075" xlink:type="simple"/></inline-formula>, the animal can take expectation over the gain function to estimate the value of perfect information (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e076" xlink:type="simple"/></inline-formula>):</p>
        <p>
          <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e077" xlink:type="simple"/><label>(6)</label></disp-formula>
        </p>
        <p>Intuitively, and crudely speaking, the value of perfect information for an action is somehow proportional to the overlap between the distribution function of that action and the distribution function of the expectedly best action. Exceptionally, for the case of the expectedly best action, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e078" xlink:type="simple"/></inline-formula> signal is proportional to the overlap between its distribution function and the distribution function of the expectedly second best action. It is worth to emphasize that for the calculation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e079" xlink:type="simple"/></inline-formula> signals, the goal-directed system has in no way been involved and instead, all the necessary information has been provided by the habitual process. The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e080" xlink:type="simple"/></inline-formula> signal for an action expresses the degree to which having perfect information about that action, i.e. knowing its true value, results in policy improvement and thus, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e081" xlink:type="simple"/></inline-formula> is indicative of the benefit of deliberation.</p>
        <p>It is worth mentioning that computing the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e082" xlink:type="simple"/></inline-formula> integral proposed in equation 6 is shown to have a closed form equation <xref ref-type="bibr" rid="pcbi.1002055-Dearden1">[18]</xref> and thus, the integral doesn't need to be actually taken. Therefore, assuming that the time needed for evaluating <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e083" xlink:type="simple"/></inline-formula> is considerably less than that of running the goal-directed system is plausible.</p>
        <p>For computing the cost of deliberation, on the other hand, assuming that deliberation about the value of each action takes a fixed time, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e084" xlink:type="simple"/></inline-formula>, the cost of deliberation can be quantified as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e085" xlink:type="simple"/></inline-formula>; where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e086" xlink:type="simple"/></inline-formula> is the average rate of reward per time unit. Average reward can be interpreted as the opportunity cost of latency in responding to the environmental stimuli <xref ref-type="bibr" rid="pcbi.1002055-Niv1">[19]</xref>. It means that when the average reward has a high value, every second in which a reward is not obtained is costly. Average reward can be computed as an exponentially-weighted moving average of obtained rewards:</p>
        <p>
          <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e087" xlink:type="simple"/><label>(7)</label></disp-formula>
        </p>
        <p>The arbitration mechanism proposed above, is an approximately optimal trade-off between speed and accuracy of responding. This means that given that the assumptions are true, the arbitration mechanism calls or doesn't call the goal-directed system, based on the criterion that sum of discounted rewards, as defined in equation 1 , should be maximized [See Methods for optimality proof]. The most challenging assumption, as mentioned before, is that the goal-directed system is assumed to have perfect information on the value of choices. As some cases that challenge the validity of this assumption one could mention the cases where only the goal-directed system is affected (for example after receiving some verbal instructions by the subject). Clearly, the cached values in the habitual system and thus the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e088" xlink:type="simple"/></inline-formula> signal will not be affected under such treatments, though the real accuracy that the goal-directed system has in estimating values has changed.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Results</title>
      <sec id="s3a">
        <title>Outcome-Sensitivity after Moderate vs. Extensive Training</title>
        <p>First discovered by Adams <xref ref-type="bibr" rid="pcbi.1002055-Adams1">[3]</xref> and later replicated in a lengthy series of studies <xref ref-type="bibr" rid="pcbi.1002055-Dickinson2">[20]</xref>–<xref ref-type="bibr" rid="pcbi.1002055-Yin1">[23]</xref>, it has been shown that the effect that the devaluation of outcome exerts on the animal's responses depends upon the extent of pre-devaluation training; i.e. responses are sensitive to outcome devaluation after moderate training, whereas overtraining makes responding insensitive to devaluation.</p>
        <p>To check the validity of the proposed model, the model has been simulated in a schedule analogous to those used in the above mentioned experiments. The formal representation of the task, which was first suggested in <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>, is illustrated in <xref ref-type="fig" rid="pcbi-1002055-g002">Figure 2</xref> . As the figure shows, the procedure is composed of 3 phases. The agent is first placed in an environment where pressing the lever (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e089" xlink:type="simple"/></inline-formula>) followed by entering the food magazine (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e090" xlink:type="simple"/></inline-formula>) results in obtaining a reward with the magnitude of one; but magazine entry before lever press, or pressing the lever and not entering the magazine leads to no reward. As the task is supposed to be cyclic, after performing each chain of actions, the agent goes to the initial state and will start afresh (<xref ref-type="fig" rid="pcbi-1002055-g002">Figure 2:A</xref>). After a certain amount of training in this phase, the food outcome is devalued by being paired with poison, which is aversive with magnitude of one (equivalently, its reward is equal to -1) (<xref ref-type="fig" rid="pcbi-1002055-g002">Figure 2:B</xref>). Finally, to assess the effect of devaluation, the performance of the agent is measured in extinction, i.e. in the absence of any outcome (neither appetitive, nor aversive), in order to avoid the instrumental associations acquired during training from being affected in the test phase (<xref ref-type="fig" rid="pcbi-1002055-g002">Figure 2:C</xref>).</p>
        <fig id="pcbi-1002055-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002055.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Formal representation of the devaluation experiment with one lever and one outcome, and behavioural results.</title>
            <p>(A) In the training phase, the animal is put in a Skinner box where pressing the lever <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e091" xlink:type="simple"/></inline-formula> followed by a nose-poke entry in the food magazine (enter-magazine: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e092" xlink:type="simple"/></inline-formula>) leads to obtaining the food reward. Other action sequences, like entering the magazine before pressing the lever (i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e093" xlink:type="simple"/></inline-formula>) result in no reward. As the task is supposed to be cyclic, the agent will return back to the initial state, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e094" xlink:type="simple"/></inline-formula>, after taking each sequence of responses. (B) In the second phase, the devaluation phase, the food outcome which used to be acquired during the training period is devalued by being paired with illness. (C) The animal's behaviour is then tested in the same Skinner box used for training, with the difference that no outcome is delivered to the animal anymore, in order to avoid changes in behaviour due to new reinforcement. (D) Behavioural results (adopted from ref <xref ref-type="bibr" rid="pcbi.1002055-Killcross1">[22]</xref>) show that the rate of pressing the lever decreases significantly after devaluation for the case of moderate pre-devaluation training. In contrast, it doesn't show a significant change, when the training period has been extensive. Error bars represent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e095" xlink:type="simple"/></inline-formula> (standard error of the mean).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.g002" xlink:type="simple"/>
        </fig>
        <p>The behavioural results, as illustrated in <xref ref-type="fig" rid="pcbi-1002055-g002">Figure2:D</xref>, show that behavioural sensitivity to goal-devaluation depends on the extent of pre-devaluation training. In the moderate training case, the rate of responding has significantly decreased after devaluation, which is an indicator of goal-directed responding. However, after extensive training, no significant sensitivity to devaluation of the outcome is observed, implying that responding has become habitual.</p>
        <p>Through numerical simulation, homogeneous agents, i.e. agents with equal free parameters of the model, have carried out the experimental procedure under two scenarios: moderate vs. extensive pre-devaluation training. The only difference between the two scenarios is in the number of training trials in the first phase of the schedule: 40 trials for the moderate, and 240 trials for the extensive training scenario. The results are illustrated separately for these two scenarios in <xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3</xref> . It must be noted that since neither the “lever-press” nor the “enter-magazine” actions are performed by the animal during the devaluation phase, the habitual knowledge remains intact in this period; i.e. the habitual system is not simulated during the devaluation period. Devaluation is assumed to only affect the reward function, used by the goal-directed system.</p>
        <fig id="pcbi-1002055-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002055.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Simulation results of the model in the schedule depicted in <xref ref-type="fig" rid="pcbi-1002055-g002"><bold>Figure 2</bold></xref>.</title>
            <p>The model is simulated under two scenarios: moderate training (left column), and extensive training (right column). In the moderate training scenario, the agent has experienced the environment for 40 trials before devaluation treatment, whereas in the extensive training scenario, 240 pre-devaluation training trials have been provided. In sum, the figure shows that after extensive training, but not moderate training, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e096" xlink:type="simple"/></inline-formula> signal is below <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e097" xlink:type="simple"/></inline-formula> at the time of devaluation (Plot <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e098" xlink:type="simple"/></inline-formula> against <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e099" xlink:type="simple"/></inline-formula>). Thus, the behaviour in the second scenario, but not the first, doesn't change right after devaluation (Plot <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e100" xlink:type="simple"/></inline-formula> against <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e101" xlink:type="simple"/></inline-formula>. Also, plot <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e102" xlink:type="simple"/></inline-formula> against <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e103" xlink:type="simple"/></inline-formula>). The low value of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e104" xlink:type="simple"/></inline-formula> signal at the time of devaluation for the second scenario is because there is little overlap between the distribution functions of the values of the two available choices (Plots <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e105" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e106" xlink:type="simple"/></inline-formula>). The opposite is true for the first scenario (Plots <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e107" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e108" xlink:type="simple"/></inline-formula>). Numbers along the horizontal axis in plots <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e109" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e110" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e111" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e112" xlink:type="simple"/></inline-formula>, represent trial numbers. Each “trial” ends when the simulated agent receives a reward; e.g. in the schedule of <xref ref-type="fig" rid="pcbi-1002055-g002">Figure 2</xref> , each time the agent chooses <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e113" xlink:type="simple"/></inline-formula> at state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e114" xlink:type="simple"/></inline-formula>, the trial number is counted up. Plots <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e115" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e116" xlink:type="simple"/></inline-formula> show the distribution functions of the habitual system over its estimated <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e117" xlink:type="simple"/></inline-formula>-values, at one trial before devaluation. Bar charts <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e118" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e119" xlink:type="simple"/></inline-formula> show the average probability of performing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e120" xlink:type="simple"/></inline-formula> at 10 trials before (filled bars) and 10 trials after (empty bars) devaluation. All data reported are means over 3000 runs. The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e121" xlink:type="simple"/></inline-formula> for all bar charts is close to zero and thus, not illustrated.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.g003" xlink:type="simple"/>
        </fig>
        <p><xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:A</xref> and G show that at the early stages of learning, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e122" xlink:type="simple"/></inline-formula> signal has a high value for both of the actions, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e123" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e124" xlink:type="simple"/></inline-formula>, at the initial state, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e125" xlink:type="simple"/></inline-formula>. This indicates that due to initial ignorance of the habitual system, knowing the exact value of both of the actions will greatly improve the agent's behavioural strategy. Hence, the benefit of deliberation is more than its cost, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e126" xlink:type="simple"/></inline-formula>. By obtaining a reward, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e127" xlink:type="simple"/></inline-formula> signal elevates gradually. Concurrently, as the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e128" xlink:type="simple"/></inline-formula>-values estimated by the habitual process for the two actions converge to their real values through learning, the difference between them increases (<xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:D</xref> and J). This increase leads to the overlap between the distribution functions over the two actions becoming less and less (<xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:E and K</xref>) and consequently, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e129" xlink:type="simple"/></inline-formula> signal decreasing gradually.</p>
        <p>Now by focusing on the moderate training scenario, it is clear that when devaluation has occurred at the trial number 40, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e130" xlink:type="simple"/></inline-formula> signals have not yet become less than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e131" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:A</xref>). Thus, the actions have been goal-directed at the time of devaluation and hence, the agent's responses have shown a great sensitivity to devaluation at the very early stages after devaluation; i.e. the probability of choosing action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e132" xlink:type="simple"/></inline-formula> has sharply decreased to 50%, which is equal to that of action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e133" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:B and F</xref>). <xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:C</xref> also shows that in the moderate training scenario, deliberation time has always been high; indicating that actions have always been deliberated using the goal-directed system.</p>
        <p>In contrast to the moderate training scenario, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e134" xlink:type="simple"/></inline-formula> signal is below <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e135" xlink:type="simple"/></inline-formula> at the time of devaluation in the extensive training scenario (<xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:G</xref>). This means that at this point of time, the cost of devaluation has exceeded its benefit and hence, actions are chosen habitually. This can be seen in <xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:I</xref>, where deliberation time has reached zero after almost 100 training trials. As a consequence, the agent's responses have not sharply changed after devaluation (<xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:H and L</xref>). Because the test has been performed in extinction, the average reward signal has gradually decreased to zero after devaluation and concurrently, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e136" xlink:type="simple"/></inline-formula> signal has slowly raised again, due to the reduction of the difference between the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e137" xlink:type="simple"/></inline-formula>-values of the two choices (<xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:J</xref>) and so, the augmentation of the overlap between their distribution functions. At the point that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e138" xlink:type="simple"/></inline-formula> has exceeded <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e139" xlink:type="simple"/></inline-formula>, the agent's responses have become goal-directed again and so, deliberation time has boosted (<xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:I</xref>). Consistently, the rate of selection of each of the two choices has been adapted to the post-devaluation conditions (<xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:H</xref>).</p>
        <p>In a nutshell, the simulation of the model in these two scenarios is consistent with the behavioural observation that moderately trained behaviours are sensitive to outcome devaluation, but extensively trained behaviours are not. Moreover, the model predicts that after extensive training, deliberation time declines; a prediction that is consistent with the VTE behaviour observed in rats <xref ref-type="bibr" rid="pcbi.1002055-Tolman1">[6]</xref>. Furthermore, the model predicts that deliberation time increases with a lag after devaluation in the extensive training scenario, whereas it remains unchanged before and after devaluation in the moderate training scenario.</p>
        <p>Just for the sake of more clarification, the reason that the mean value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e140" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1002055-g003">Figures 3:E</xref> and K is above zero is because of the cyclic nature of the task, i.e. by taking action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e141" xlink:type="simple"/></inline-formula> at state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e142" xlink:type="simple"/></inline-formula>, the agent goes back to the same state, which might have a positive value.</p>
      </sec>
      <sec id="s3b">
        <title>Outcome-Sensitivity in a Concurrent Schedule</title>
        <p>The focus of the previous section was on simple tasks with only one response for each outcome. In another class of experiments, the development of behavioural autonomy has been assessed in more complex tasks where two different responses produce two different outcomes <xref ref-type="bibr" rid="pcbi.1002055-Holland1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Colwill1">[24]</xref>–<xref ref-type="bibr" rid="pcbi.1002055-Kosaki1">[26]</xref>. Among those experiments, to the best of our knowledge, it is only in the experiment in <xref ref-type="bibr" rid="pcbi.1002055-Kosaki1">[26]</xref> that the two different choices (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e143" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e144" xlink:type="simple"/></inline-formula>) are concurrently available and hence, the animal is given a choice between the two responses (<xref ref-type="fig" rid="pcbi-1002055-g004">Figure 4:A</xref>). In the others, the two different responses are trained and also tested in separate sessions and so, their schedules are not compatible with the requirements of the reinforcement learning framework that is used in our model.</p>
        <fig id="pcbi-1002055-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002055.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Tree representation of the devaluation experiment with two levers available concurrently.</title>
            <p>(A) In the training phase, either pressing lever one <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e145" xlink:type="simple"/></inline-formula> or pressing lever two <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e146" xlink:type="simple"/></inline-formula>, if followed by entering the magazine <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e147" xlink:type="simple"/></inline-formula>, results in acquiring one unit of either of the two rewards, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e148" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e149" xlink:type="simple"/></inline-formula>, respectively. The reinforcing value of the two rewards is equal to one. Other action sequences lead to no reward. As in the task of <xref ref-type="fig" rid="pcbi-1002055-g002">Figure 2</xref> , this task is also assumed to be cyclic. (B) In the devaluation phase, the outcome of one of the responses (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e150" xlink:type="simple"/></inline-formula>) is devalued (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e151" xlink:type="simple"/></inline-formula>), whereas the rewarding value of the outcome of the other response (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e152" xlink:type="simple"/></inline-formula>) has remained unchanged. After the devaluation phase, the animal's behaviour is tested in extinction (for space consideration, this phase is not illustrated). Similar to the task of <xref ref-type="fig" rid="pcbi-1002055-g002">Figure 2</xref> , neither <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e153" xlink:type="simple"/></inline-formula> nor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e154" xlink:type="simple"/></inline-formula> is delivered to the animal in the test phase.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.g004" xlink:type="simple"/>
        </fig>
        <p>In <xref ref-type="bibr" rid="pcbi.1002055-Kosaki1">[26]</xref>, rats received extensive concurrent instrumental training in a task where pressing the two different levers produces different types of outcomes: food pellets and sucrose solution. Although the outcomes are different, they have equal reinforcing strength, in terms of the response rates supported by them. A task similar to that used in their experiment is formally depicted in <xref ref-type="fig" rid="pcbi-1002055-g004">Figure 4</xref>.</p>
        <p>After extensively reinforcing the two responses, one of the outcomes was devalued by flavour aversion conditioning, as illustrated in <xref ref-type="fig" rid="pcbi-1002055-g004">Figure 4:B.</xref> Subsequently, given a choice between the two responses, the sensitivity of instrumental performance to this devaluation was assessed in extinction tests. The results of their experiment showed that devaluation reduced the relative performance of the response associated with the devalued outcome at the very early stage of the test phase, even after extensive training. Thus, it can be concluded that whatever the amount of instrumental training, S-R habits do not overcome goal-directed decision making when two responses with equal affective values are concurrently available.</p>
        <p>Simulating the proposed model in the task of <xref ref-type="fig" rid="pcbi-1002055-g004">Figure 4</xref> has replicated this behavioural observation. As illustrated in <xref ref-type="fig" rid="pcbi-1002055-g005">Figure 5:A</xref>, initially, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e155" xlink:type="simple"/></inline-formula> signal for the two responses has a high value which gradually decreases over time as the variance of the distribution functions over the estimated values of the two responses decreases; meaning that the habitual process becomes more and more certain about the estimated values. However, due to the forgetting effect, i.e. the habitual system forgets very old samples and does not use them in approximating the distribution function, the variance of the distribution functions over the values of actions doesn't converge to zero, but instead, converges to a level higher than zero. Moreover, because the strength of the two reinforcers is equal, as revealed in <xref ref-type="fig" rid="pcbi-1002055-g005">Figure 5:D</xref> ,the distribution functions do not get divorced (<xref ref-type="fig" rid="pcbi-1002055-g005">Figure 5:E</xref>). As a result of these two facts, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e156" xlink:type="simple"/></inline-formula> signal has converged at a level higher than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e157" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1002055-g005">Figure 5:A</xref>). This has led to the performance remaining goal-directed (<xref ref-type="fig" rid="pcbi-1002055-g005">Figure 5:C</xref>) and sensitive to devaluation of one of the outcomes; i.e. after devaluing the outcome of the action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e158" xlink:type="simple"/></inline-formula>, its rate of selection has sharply decreased and instead, the probability of selecting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e159" xlink:type="simple"/></inline-formula> has increased (<xref ref-type="fig" rid="pcbi-1002055-g005">Figure 5:B and F</xref>).</p>
        <fig id="pcbi-1002055-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002055.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Simulation results for the task of <xref ref-type="fig" rid="pcbi-1002055-g004"><bold>Figure 4</bold></xref>.</title>
            <p>The results show that since the reinforcing value of the two outcomes is equal, there is a huge overlap between the distribution functions over the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e160" xlink:type="simple"/></inline-formula>-values of actions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e161" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e162" xlink:type="simple"/></inline-formula>, at state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e163" xlink:type="simple"/></inline-formula>, even after extensive training (240 trials) (Plots <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e164" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e165" xlink:type="simple"/></inline-formula>). Accordingly, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e166" xlink:type="simple"/></inline-formula> signals (benefit of goal-directed deliberation) for these two actions remain higher than the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e167" xlink:type="simple"/></inline-formula> signal (cost of deliberation) (Plot <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e168" xlink:type="simple"/></inline-formula>) and thus, the goal-directed system is always engaged in value-estimation for these two choices. The behaviourally observable result is that responding remains sensitive to revaluation of outcomes, even though devaluation has happened after a prolonged training period (Plots <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e169" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e170" xlink:type="simple"/></inline-formula>).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.g005" xlink:type="simple"/>
        </fig>
        <p>As it is clear from the above discussion, the relative strength of the reinforcers critically affects the arbitration mechanism in our model. In fact, the model predicts that when the affective values of the two outcomes are close enough to each other, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e171" xlink:type="simple"/></inline-formula> signal will not decline and hence, the behaviour will remain goal-directed and sensitive to devaluation, even after extensive training. But if the two outcomes have different reinforcing strength, then their corresponding distribution functions will gradually get divorced and thus, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e172" xlink:type="simple"/></inline-formula> signal will converge to zero. This leads to the habitual process taking control of behaviour and the performance becoming insensitive to outcome devaluation. This prediction is in contrast to the model proposed in <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>, in which the arbitration between the two systems is independent of the relative incentive values of the two outcomes. In fact, in that model, whether the value of an action comes from the habitual or the goal-directed system, only depends on the uncertainty of the two systems about their estimated values and thus, the arbitration between the two systems is independent of the estimated value for other actions.</p>
      </sec>
      <sec id="s3c">
        <title>Reaction-Time in a Reversal Learning Task</title>
        <p>Using a classical reversal learning task, Pessiglione and colleagues have measured human subjects' reaction time by temporal decoupling of deliberation and execution processes <xref ref-type="bibr" rid="pcbi.1002055-Pessiglione1">[27]</xref>. Reaction time, in their experiment, is defined as the interval between stimulus presentation and the subsequent response initiation. Subjects are required to choose between two alternative responses (“go” and “no-go”), as soon as one of the two stimuli (“<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e173" xlink:type="simple"/></inline-formula>” and “<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e174" xlink:type="simple"/></inline-formula>”) appear on the screen. As shown in <xref ref-type="fig" rid="pcbi-1002055-g006">Figure 6:A</xref>, at each trial, one of the two stimuli <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e175" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e176" xlink:type="simple"/></inline-formula> will appear in random, and after the presentation of each stimuli, only one of the two actions results in a gain, whereas the other action results in a loss (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e177" xlink:type="simple"/></inline-formula>). The rule governing the appropriate response must be learned by the subject through trial and error. After several learning trials, the reward function changes without warning (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e178" xlink:type="simple"/></inline-formula>). This second phase is called the reversal phase. Finally, during the extinction phase, the “go” action never leads to a gain, and the appropriate action is to always choose the “no-go” response (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e179" xlink:type="simple"/></inline-formula>).</p>
        <fig id="pcbi-1002055-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002055.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Tree representation of the reversal learning task, used in <xref ref-type="bibr" rid="pcbi.1002055-Pessiglione1">[<bold>27</bold>]</xref>, and the behavioural results.</title>
            <p>(A) When each trial begins, one of the two stimuli, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e180" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e181" xlink:type="simple"/></inline-formula>, is presented in random on a screen. The subject can then choose whether to touch the screen (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e182" xlink:type="simple"/></inline-formula> action) or not (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e183" xlink:type="simple"/></inline-formula> action). The task is performed in three phases: training, reversal, and extinction. During the training phase, the subject will receive a reward if the stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e184" xlink:type="simple"/></inline-formula> is presented and the action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e185" xlink:type="simple"/></inline-formula> is performed by the subject, or if the stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e186" xlink:type="simple"/></inline-formula> is presented and the action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e187" xlink:type="simple"/></inline-formula> is selected (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e188" xlink:type="simple"/></inline-formula>). During the reversal phase, the reward function is reversed, meaning that the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e189" xlink:type="simple"/></inline-formula> action must be chosen when the stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e190" xlink:type="simple"/></inline-formula> is presented, and vice versa (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e191" xlink:type="simple"/></inline-formula>). Finally, during the extinction phase, regardless of the presented stimulus, only the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e192" xlink:type="simple"/></inline-formula> action leads to a reward (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e193" xlink:type="simple"/></inline-formula>). (B) During both the training and reversal phases, subjects' reaction time is high at the early stages when they don't have enough experience with the new conditions yet. However, after some trials, the reaction time declines significantly. Error bars represent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e194" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.g006" xlink:type="simple"/>
        </fig>
        <p>To analyse the results of the experiments, the authors have divided each phase into two sequential periods: a “searching” period during which the subjects learn the reward function by trial and error, and an “applying” period during which the learned rule is applied. The results show that in the searching period of each phase, the subjects might choose either the right or the wrong choice, whereas during the applying period, they almost always choose the appropriate action. Moreover, as shown in <xref ref-type="fig" rid="pcbi-1002055-g006">Figure 6:B</xref>, the subjects' reaction time is significantly lower during the applying period, compared to the searching period.</p>
        <p><xref ref-type="fig" rid="pcbi-1002055-g007">Figure 7</xref> shows that our model captures the essence of experimental results reported in <xref ref-type="bibr" rid="pcbi.1002055-Pessiglione1">[27]</xref>. In fact, the model predicts that during the searching period, the goal-directed process is involved in decision making, whereas during the applying period, the arbitration mechanism doesn't ask for its help in value estimation. It should be noticed that the reaction time reported in <xref ref-type="bibr" rid="pcbi.1002055-Pessiglione1">[27]</xref>, is presumably the sum of stimulus-recognition time, deliberation time, etc. Thus, a fixed value, which is the sum of all the other processes involved in choice selection, must be added to the deliberation time computed by our model.</p>
        <fig id="pcbi-1002055-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002055.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Simulation results of the model in the reversal learning task depicted in <xref ref-type="fig" rid="pcbi-1002055-g006"><bold>Figure 6</bold></xref>.</title>
            <p>Since the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e195" xlink:type="simple"/></inline-formula> signals have high values at the early stages of learning (plot <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e196" xlink:type="simple"/></inline-formula>), the goal-directed system is active and thus, the deliberation time is relatively high (plot <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e197" xlink:type="simple"/></inline-formula>). After further training, the habitual system takes control over behaviour (plot <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e198" xlink:type="simple"/></inline-formula>) and as a result, the model's reaction time decreases (plot <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e199" xlink:type="simple"/></inline-formula>). After reversal, it takes some trials for the habitual system to realize that the cached <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e200" xlink:type="simple"/></inline-formula>-values are not precise anymore (equivalent to an increase in the variance of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e201" xlink:type="simple"/></inline-formula>). Thus, after some trials after reversal, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e202" xlink:type="simple"/></inline-formula> signal increases again (plot <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e203" xlink:type="simple"/></inline-formula>), which results in re-activation of the goal-directed system. As a result, the model's reaction time increases again (plot <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e204" xlink:type="simple"/></inline-formula>). A similar explanation holds for the rest of the trials. In sum, consistent with the experimental data, the reaction time is higher during the searching period, than the applying period.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.g007" xlink:type="simple"/>
        </fig>
        <p>One might argue that variations in reaction time in the mentioned experiment could also be explained by a single habitual system, by assuming that lack of sufficient learning induces a hesitation-like behaviour. For example, high uncertainty in the habitual system at the early stages of learning a task, or after a change is recognized, can result in a higher-than-normal rate of exploration <xref ref-type="bibr" rid="pcbi.1002055-Dearden1">[18]</xref>. Thus, assuming that exploration takes more time than exploitation, reaction time will be higher when the uncertainty of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e205" xlink:type="simple"/></inline-formula>-values is high. However, as emphasized by the authors in <xref ref-type="bibr" rid="pcbi.1002055-Pessiglione1">[27]</xref>, uncertainty doesn't have any effect on the subject's movement time, but only on the reaction time. In fact, movement time remains constant through the course of the experiment. Movement time is defined as the interval between response initiation and submission of the choice. Since movement time is unaffected by the extent of learning, it is unlikely that variations in reaction time be due to a hesitation-like effect and thus, as an alternative, it can be attributed to involvement of deliberative processes. Moreover, such an explanation lacks a normative rationale for the assumption that exploration takes more time than exploitation.</p>
      </sec>
      <sec id="s3d">
        <title>Reaction-Time as a Function of the Number of Choices</title>
        <p>According to a classical literature in behavioural psychology, choice reaction time (CRT) is fastest when only one possible response is available, and as the number of alternatives increases, so does the response latency. Originally, Hick <xref ref-type="bibr" rid="pcbi.1002055-Hick1">[28]</xref> found that in choice reaction time experiments, CRT increases in proportion to the logarithm of the number of alternatives. Later on, a wealth of evidence validated his finding (e.g., <xref ref-type="bibr" rid="pcbi.1002055-Hyman1">[29]</xref>–<xref ref-type="bibr" rid="pcbi.1002055-Vickrey1">[35]</xref>), such that it became known as “Hick's law”.</p>
        <p>Other researchers <xref ref-type="bibr" rid="pcbi.1002055-Mowbray1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Welford1">[37]</xref> found that Hick's law holds only for unpracticed subjects, and that training shortens CRT. They also found that in well-trained subjects, there is no difference in CRT as the number of choices varies.</p>
        <p>In a typical CRT experiments, a certain number of stimuli and the same number of responses are used in each session of the experiment. <xref ref-type="fig" rid="pcbi-1002055-g008">Figure 8</xref> shows the tree representation of an example task with four stimuli and four alternatives. In each trial, one of the four alternatives appears at random, and only one of the four responses results in a reward. As in the CRT experiments the subjects are provided with a prior knowledge about the appropriate response after the presentation of each stimuli, we assume that this declarative knowledge can be fed into and used by the goal-directed system in the form of transition and reward functions. Furthermore, subjects are asked to make true responses, and at the same time as fast as possible. Hence, since subjects know the structure of the task in advance, they show very high performance (as defined by the rate of correct responses) in the task.</p>
        <fig id="pcbi-1002055-g008" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002055.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>The tree representation of the task for testing the Hick's law.</title>
            <p>In this example, at each trial, one of the four stimuli is presented with equal probabilities. After observing the stimulus, only one of four available choices lead to a reward (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e206" xlink:type="simple"/></inline-formula>). The task structure is verbally instructed to the subjects before they start performing the task. The interval between the appearance of the stimulus and the initiation of a response is measured as “reaction time”. The experiment is performed under different numbers of stimulus-response pairs; e.g. some subjects perform the task when only one stimulus-response pair is available (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e207" xlink:type="simple"/></inline-formula>), whereas for other subjects the number of stimulus-response pairs might be different.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.g008" xlink:type="simple"/>
        </fig>
        <p>As demonstrated in <xref ref-type="fig" rid="pcbi-1002055-g009">Figure 9</xref> , the behaviour of the model has replicated the results of CRT experiments: at the early stages of learning, the deliberation time increases as the number of choices increases, whereas after sufficient training, no difference in deliberation time can be seen. It must be mentioned that in contrast to behavioural data, our model predicts a linear correlation between the CRT and the number of alternatives, rather than a logarithmic function. Again, a fixed value characterizing stimulus-identification time must be added to the deliberation time computed by our model in order to reach the reaction time reported in the CRT literature.</p>
        <fig id="pcbi-1002055-g009" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002055.g009</object-id>
          <label>Figure 9</label>
          <caption>
            <title>Simulation results for the task of <xref ref-type="fig" rid="pcbi-1002055-g008"><bold>Figure 8</bold></xref>.</title>
            <p>Consistent with the behavioural data, the results show that as the number of stimulus-response pairs increase, the reaction time also increases. Moreover, if extensive training is provided to the subjects, the reaction time decreases and becomes independent from the number of choices.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.g009" xlink:type="simple"/>
        </fig>
        <p>Since in CRT experiments a declarative knowledge about appropriate responses is provided to the subjects, they have a relatively high performance from the very beginning of the experiment. The proposed model can explain this behavioural characteristic due to the fact that at the early stages of the experiment, when the habitual system is totally ignorant about the task structure, the goal-directed system controls the behaviour and exploits the prior knowledge fed into it. Thus, a single habitual system cannot explain the performance profile of subjects, even though it might be able to replicate the reaction-time profile. For example, a habitual system that uses a winner-take-all neural mechanism for the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e208" xlink:type="simple"/></inline-formula>-values of different choices to compete <xref ref-type="bibr" rid="pcbi.1002055-Yuille1">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Ellias1">[39]</xref> also predicts that at the early stages of learning where the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e209" xlink:type="simple"/></inline-formula>-values are close to each other, reaching a state that one action overcomes the others takes longer, compared to the later stages where the best choice has a markedly higher <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e210" xlink:type="simple"/></inline-formula>-value than other actions. Such a mechanism also predicts that at the early stages, if the number of choices increases, the reaction time will also increase. However, since feeding the subject's declarative knowledge into the habitual system is not consistent with the nature of this system, a single habitual system cannot explain the performance of subjects in Hick's experiment.</p>
      </sec>
    </sec>
    <sec id="s4">
      <title>Discussion</title>
      <sec id="s4a">
        <title>Neural Implications</title>
        <p>As mentioned, training-induced neuroplasticity in cortico-basal ganglia circuits is suggested to be mediated by dopamine (DA), a key neuromodulater in the brain reward circuitry. Whereas phasic activity of midbrain DA neurons is hypothesized to carry the prediction error signal <xref ref-type="bibr" rid="pcbi.1002055-Montague1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Schultz1">[41]</xref>, and thus imposes an indirect effect on behaviour through its role in learning the value of actions, the tonic activity of DA has shown to have a direct effect on behaviour. For example, DA agonists have been demonstrated to have an invigorating effect on a range of behaviours <xref ref-type="bibr" rid="pcbi.1002055-Lyons1">[42]</xref>–<xref ref-type="bibr" rid="pcbi.1002055-Ljungberg1">[46]</xref>. It is also shown that higher levels of intrastriatal DA concentration is correlated with higher rates of responding <xref ref-type="bibr" rid="pcbi.1002055-Jackson1">[47]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Carr1">[48]</xref>, whereas DA antagonist or DA depletion results in reduced responsivity <xref ref-type="bibr" rid="pcbi.1002055-Sokolowski1">[49]</xref>–<xref ref-type="bibr" rid="pcbi.1002055-Mingote1">[53]</xref>.</p>
        <p>Based on these evidence, it has been suggested in previous RL models that tonic DA might report the average reward signal (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e211" xlink:type="simple"/></inline-formula>) <xref ref-type="bibr" rid="pcbi.1002055-Niv1">[19]</xref>. By adopting the same assumption, our model also provides a normative explanation for those mentioned experimental results, in terms of tonic DA-based variations in deliberation time.</p>
      </sec>
      <sec id="s4b">
        <title>Rationality of Type II</title>
        <p>In the economic literature of decision theory, rational individuals make optimal choices based on their desires and goals <xref ref-type="bibr" rid="pcbi.1002055-MasColell1">[54]</xref>, without taking into account the time needed to find the optimal action. In contrast, models of bounded rationality are concerned with information and computational limitations imposed on individuals when they are encountered with alternative choices. Normative models of rational choice that take into account the time and effort required for decision making are known as rationality of type II. This notion emphasizes that computing the optimal answer is feasible, but not economical in complex domains.</p>
        <p>First introduced by Herbert Simon, it was argued that agents have limited computational power and that they must react within a reasonable amount of time <xref ref-type="bibr" rid="pcbi.1002055-Simon1">[55]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Simon2">[56]</xref>. To capture this concept, <xref ref-type="bibr" rid="pcbi.1002055-Simon3">[57]</xref> used the Scottish word “satisficing” which means satisfying, to refer to a decision making mechanism that searches until an alternative that meets the agent's aspiration level criterion is found. In other words, the search process is continued until a satisfactory solution is found. Borrowed from psychology, aspiration level denotes a solution evaluation criterion that can be either static or context-dependent and acquired by experience. A similar idea has been taken by neuroscientists to explain the speed/accuracy trade-off, using signal detection theory (see <xref ref-type="bibr" rid="pcbi.1002055-Gold1">[58]</xref> for review). In this framework, the accumulated information gathered from a sequence of observations from a noisy evidence must reach a certain threshold, in order for the animals to convert the accumulated information into a categorical choice. If the threshold goes up, the accuracy increases. As in this case more information must be gathered to satisfy that increased level of accuracy, response latency will decrease.</p>
        <p>Simon's initial proposal has launched much attempt in both social science and computer science to develop models that sacrifice optimality in favor of fast-responding. The focus has been on complex uncertain environments, where the agent must respond in a limited amount of time. The answer given to this dilemma in social science is often based on a variety of domain-specific heuristic methods <xref ref-type="bibr" rid="pcbi.1002055-Gigerenzer1">[59]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Gigerenzer2">[60]</xref> in which, rather than employing a general-purpose optimizer, animals use a set of simple and hard-coded rules to make their decisions in each particular situation. In the artificial intelligence literature, on the other hand, the answer is often based on approximate reasoning. In this approach, details of a complex problem are ignored in order to build a simpler representation of the original problem. Finding the optimal solution of this simple problem will be feasible in an admissible amount of time <xref ref-type="bibr" rid="pcbi.1002055-Zilberstein1">[61]</xref>.</p>
        <p>To capture the concept of time limitation and to incorporate it into models of decision making, we have used the dual-process theory of decision making. The model we have proposed is based on the assumption that the habitual process is fast in responding to environmental stimuli, but is slow in adapting its behavioural strategies, particularly in environments with low stability. The goal-directed system, in contrast, needs time for deliberating the value of different alternatives by tracing down the decision tree, but, is flexible in behavioural adaptation. The rule for arbitrating between these two systems assumes that animals balance decision quality against the computational requirements of decision-making.</p>
        <p>However, the optimality of the arbitration rule is based on the strong assumption that the goal-directed decision process has perfectly learned the environmental contingencies. This assumption might be violated at some points, particularly at the very early stages of learning a new task. When both systems are totally ignorant of the task structure, although the habitual system is in desperate need of having perfect information (high <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e212" xlink:type="simple"/></inline-formula> signal), the goal-directed system doesn't have any information to provide. Thus, deliberation not only doesn't improve animal's strategy, but leads to a waste of the time that could be used for blind exploration. Though, since the goal-directed system is very efficient in terms of exploiting the experienced contingencies, this sub-optimal behaviour of the model doesn't last long. More importantly, in real world situations, the goal-directed process seems to always have considerably more accurate information than the habitual system, even in environments that have never been explored before. This is because many environmental contingencies can be discovered by mere visual observation (e.g. searching for food in an open field) or verbal instruction (as in the Hick's task discussed before), without any experience being required.</p>
      </sec>
      <sec id="s4c">
        <title>State of the Art</title>
        <p>Our model is in fact based on the previous computational model of the dual-process theory, proposed by Daw and colleagues <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>. After assigning model-free and model-based RL models to habitual and goal-directed systems, respectively, they suggest an uncertainty-based arbitration mechanism between the two systems. In their model, each of the two systems not only separately estimate a value for each certain action, but their uncertainties about that value-estimations are also computed. As in our model, lack of enough experiences in the environment results in uncertainty in the habitual system. The source of uncertainty in the goal-directed system, on the other hand, is (1) uncertainty in transition and reward functions, due to the lack of enough experiences and (2) “pruning”, which refers to incomplete consideration of the all parts of the decision tree when considering the consequences of alternative choices. The latter source of uncertainty is not explicitly modeled and instead, is captured by adding a noise to the estimated values.</p>
        <p>At any given point of time, both systems get involved in value and uncertainty estimation for all the available choices and when they have both finished, the system that is more certain about its estimation of the value of each action will determine the value of that action for action-selection. As a result of this arbitration rule, the goal-directed system is dominant at the early stages of learning; but after extensive learning, the habitual process will take control over behaviour. This happens because uncertainty of the habitual system decreases through the course of learning, whereas the goal-directed process remains uncertain due to the incomplete search of the decision tree (the added noise). Thus, their model can explain the canonical observation in the experimental paradigm of outcome-devaluation (Outcome-sensitivity after moderate, but not extensive training).</p>
        <p>The added noise to the goal-directed system in that model actually characterizes, in an adhoc way, all the computational constraints that the goal-directed system is confronted with; e.g. time constraint, working memory constraint, caloric needs, etc. It has also been pointed out in <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>, that the trade-off between behavioural flexibility and computational costs can be captured in a cost-benefit fashion. In this respect, the arbitration mechanism we have proposed in this paper is a variant of the model proposed in <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>, where only one of the computational constraint, i.e. deliberation time, is modeled in an explicit, cost-benefit account.</p>
        <p>Beside this noticeable behavioural harmony of that model with the current dual-process literature, it suffers from some deficiencies. These deficiencies arise from the fact that in that model, the goal-directed system ceaselessly searches for the optimal policy, regardless of the system that is controlling the behaviour. In contrast to this assumption, overtraining of a behaviour is shown to causes a transition in neural activity from the associative to the sensorimotor network; i.e., whereas PFC and caudate nucleus are activated at the early stages of learning a new motor response, this activity shifts to motor cortices and putamen as the response becomes well-trained <xref ref-type="bibr" rid="pcbi.1002055-Jueptner1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Jueptner2">[63]</xref>. As a result, response latency in that model doesn't vary through learning. Of course, it should be mentioned that by adding the noise to the goal-directed system in order to model pruning, time-limitations have been implicitly incorporated into the model; but as this noise level remains fixed through learning, the involvement of the goal-directed system, and so the deliberation time, doesn't change even after extensive training.</p>
        <p>As mentioned before, the core idea that we have proposed here for arbitration between the two systems is that there should be a balance between speed and accuracy in responding. A similar idea has been previously used by Shah and Barto <xref ref-type="bibr" rid="pcbi.1002055-Shah1">[64]</xref>, but in an evolving sensory representation framework. In the task that they have simulated, subjects must choose among the potential goals in each trial. However, the sensory representation of the true goal of each trial is weak at the beginning of the trial, and resolves gradually during the course of the trial <xref ref-type="bibr" rid="pcbi.1002055-Tassinari1">[65]</xref>. The basic assumption of their model is that the planning system can select actions only when goal representation is fully resolved, but the habitual system can also use “uncertain” accumulated sensory information. At the early trials of learning the task, since the value of different choices is not learned by the habitual system yet, this system cannot choose among the choices within a considerable period of time. This is due to using a winner-take-all competition mechanism for action selection <xref ref-type="bibr" rid="pcbi.1002055-Yuille1">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Ellias1">[39]</xref>. Thus, at the early trials, the sensory representation has enough time to be fully resolved and as a consequence of this, the planning system controls behaviour. However, after extended training, the habitual system can make a decision before the goal is fully identified, based on uncertain sensory information.</p>
        <p>Although both the model we proposed here and the model proposed in <xref ref-type="bibr" rid="pcbi.1002055-Shah1">[64]</xref> use speed-accuracy trade-off for arbitration between the two systems, there is fundamental differences between them. Whereas the extra time needed by the planning system in is used for state recognition <xref ref-type="bibr" rid="pcbi.1002055-Shah1">[64]</xref>, this time is used for deliberating the consequences of choices in our model. In fact, it is the process of state recognition that is time-consuming in their model, and not the process of deliberation. Due to this difference, the model of <xref ref-type="bibr" rid="pcbi.1002055-Shah1">[64]</xref> can only be applied in cases where stimulus identification takes non-negligible time, which doesn't seem to be the case of the experiments addressed by our model.</p>
        <p>Changes in the animals' response rate has been previously explained in the reinforcement learning literature <xref ref-type="bibr" rid="pcbi.1002055-Niv1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-McClure1">[66]</xref>. Importantly, in the model proposed by Niv et al. <xref ref-type="bibr" rid="pcbi.1002055-Niv1">[19]</xref>, as in our model, animals make a balance between the cost and benefit of acting quickly. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e213" xlink:type="simple"/></inline-formula> is the cost of responding after an interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e214" xlink:type="simple"/></inline-formula>. Thus, in their model, as in our model, the animal benefits from responding fast, because it loses less potential rewards. But as they do not model the goal-directed system, the cost of acting quickly in their model is due to an extra fatigue-like cost induced by responding fast, whereas this cost in our model is due to inaccurate and inflexible value estimations. We believe that both factors, influence the animals' response rate.</p>
        <p>But as a result of this fundamental difference, the two models have different behavioural predictions. In fact, the term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e215" xlink:type="simple"/></inline-formula> in the model proposed in <xref ref-type="bibr" rid="pcbi.1002055-Niv1">[19]</xref> refers to “execution time”, whereas in our model it refers to “reaction time”. Notice that reaction time is, by definition, the interval between stimulus presentation and performance initiation, whereas execution time (movement time) refers to the interval between response initiation and its finalization. Due to this difference, their model cannot explain any of the three experiments on reaction time that our model can: (1) VTE behaviour, (2) increase in reaction time as the number of choices increases, (3) decrease in reaction time after reversals, in the go/no-go task. Interestingly, by temporal decopulation of deliberation and execution, it has been shown in <xref ref-type="bibr" rid="pcbi.1002055-Pessiglione1">[27]</xref> that whereas reaction time has significantly decreased after reversal in a go/no-go task, the execution time has remained intact.</p>
      </sec>
      <sec id="s4d">
        <title>Untested Behavioural Predictions of the <xref ref-type="sec" rid="s2">Model</xref></title>
        <p>As mentioned previously, one prediction of the competition mechanism proposed in this paper is that outcome sensitivity is dependent on the relative value of the choices that are concurrently available. That is, if the value of choices are sufficiently close together, the habitual system will remain uncertain about what the best choice is (equivalent to high <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e216" xlink:type="simple"/></inline-formula>), even after extensive training. This will result in the informational gain of knowing the exact value of choices remaining high and thus, the goal-directed system staying dominant. Such a mechanism can explain the behavioural data reported in <xref ref-type="bibr" rid="pcbi.1002055-Kosaki1">[26]</xref>.</p>
        <p>By contrast, the model predicts that in a concurrent schedule where the value of the two choices are sufficiently different, responding will eventually become habitual. This is because after extensive training, the habitual system will have sufficient information for choosing the better choice among the two, without needing the exact value of them; i.e., without needing the goal-directed system. To our knowledge, this prediction is not tested yet. In this respect, the model has a different prediction from what the model proposed in <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref> predicts. According to that model, the goal-directedness of responding doesn't depend on the relative value of choices and thus, it predicts that responding will remain goal-directed in concurrent schedules, whether the values of choices are equal or not.</p>
        <p>Another prediction of our model is that if the two choices in a concurrent schedule lead to a unique outcome, responses will remain sensitive to devaluation, regardless of the amount of instrumental training. This is because when the outcomes are identical, the values of the two choices that lead to it will be exactly the same. In fact, when the values of the two choices are equal, our model predicts that responding will remain goal-directed, whether the identity of the outcomes of choices are the same or not. However, in the model proposed in <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>, if the two outcomes are identical, it can be said that since fewer outcome values must be learned, the asymptotic uncertainties of the habitual system will decrease. Thus, according to that model, responding might become habitual or remain goal-directed after extensive training, depending on the parameters of the model.</p>
        <p>It should be mentioned that in an experiment by Holland <xref ref-type="bibr" rid="pcbi.1002055-Holland1">[21]</xref>, sensitivity to devaluation is tested where two different choices result in an identical outcome. However, since in that experiment responding for the two choices is trained and tested in separate sessions, rather than the choices being available concurrently, the reinforcement learning framework cannot see it as if the values of the choices could be compared together. Therefore, in order to test the above prediction of our model, it is necessary to use a concurrent schedule.</p>
        <p>Another theoretical account for competition between the S-R and the A-O systems proposed by Dickinson <xref ref-type="bibr" rid="pcbi.1002055-Dickinson3">[67]</xref> predicts that competition between the systems depends on the relative value of choices. In this account, responding is goal-directed if, and only if, the animal experiences instrumental contingency between responses and outcomes. Experienced contingency is defined as the correlation between a change in response rate and a change in reward rate. Consistent with behavioural data, this theory predicts that in one-choice tasks where a ratio schedule is used, the response rate and thus the reward rate increase during the initial acquisition period. Hence, due to the positive experienced correlation between the changes in these two variables, responding will be goal-directed. However, after extended training, response rate, as well as reward rate, converge to a high rate. This will remove any experienced contingency perceived by the animal and thus, the habitual system becomes dominant.</p>
        <p>For the case of concurrent schedules where the two outcomes are different but have equal values, this account predicts that even after extensive training, the animal might choose either of the two responses from time to time. Thus, every time that the animal performs one of the two responses, it experiences a loss of the outcome that could be acquired by performing the other response. In this respect, the animal always experiences a local correlation between response and outcome rates and thus, remains goal-directed even after extensive training. This prediction is also consistent with behavioural data <xref ref-type="bibr" rid="pcbi.1002055-Kosaki1">[26]</xref>.</p>
        <p>However, if the identity of the two outcomes are the same, this theory will have a different prediction. In such a case, since the outcomes are identical, the rate of outcome will be fixed after extensive training regardless of which of the two responses is performed. Thus, in this case, the local A-O rate correlation dies out and responding becomes habitual. Moreover, this account predicts that if the two choices result in different outcomes that have markedly different values, responding will become habitual after extensive training. This is because after extensive training, the high-value choice will become stereotyped and the other response will be chosen rarely. Thus, since only one of the two outcomes is often experienced with a consistently high rate, the locally experienced A-O rate correlation decreases. In fact, the experienced A-O rate correlation is negatively correlated with the difference between the values of the two outcomes: the higher the difference between the values, the lower the experienced instrumental contingency. As a result, if the values of the two outcomes are sufficiently different, responding will become habitual eventually. In this respect, both the theoretical account of <xref ref-type="bibr" rid="pcbi.1002055-Dickinson3">[67]</xref> and our model predict that arbitration depends on the relative value of the two choices.</p>
        <p>A summary of the predictions of the reviewed dual-process accounts are provided in <xref ref-type="table" rid="pcbi-1002055-t001">Table 1</xref>. The experimental schedules of the first and the third rows of the table, as discussed before, are used in <xref ref-type="bibr" rid="pcbi.1002055-Adams1">[3]</xref> and <xref ref-type="bibr" rid="pcbi.1002055-Kosaki1">[26]</xref>, respectively. As shown, the prediction of all three arbitration mechanisms for these two cases are the same, and supported by behavioural data. However, the theories have differential predictions in the other two cases that are not tested yet.</p>
        <table-wrap id="pcbi-1002055-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002055.t001</object-id><label>Table 1</label><caption>
            <title>Prediction of different dual-process accounts about the dominant process after extensive training.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002055-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.t001" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">Dickinson <xref ref-type="bibr" rid="pcbi.1002055-Dickinson3">[67]</xref></td>
                <td align="left" colspan="1" rowspan="1">Daw et al. <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref></td>
                <td align="left" colspan="1" rowspan="1">Our model</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1">Single choice</td>
                <td align="left" colspan="1" rowspan="1">S-R</td>
                <td align="left" colspan="1" rowspan="1">S-R</td>
                <td align="left" colspan="1" rowspan="1">S-R</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Two concurrent choices with identical outcomes</td>
                <td align="left" colspan="1" rowspan="1">S-R</td>
                <td align="left" colspan="1" rowspan="1">S-R or A-O</td>
                <td align="left" colspan="1" rowspan="1">A-O</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Two concurrent choices with different outcomes, but equal values</td>
                <td align="left" colspan="1" rowspan="1">A-O</td>
                <td align="left" colspan="1" rowspan="1">A-O</td>
                <td align="left" colspan="1" rowspan="1">A-O</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Two concurrent choices with different outcomes and sufficiently different values</td>
                <td align="left" colspan="1" rowspan="1">S-R</td>
                <td align="left" colspan="1" rowspan="1">A-O</td>
                <td align="left" colspan="1" rowspan="1">S-R</td>
              </tr>
            </tbody>
          </table></alternatives></table-wrap>
        <p>One critical assumption of our model that is worth being tested is the assumption that arbitration between the systems is independent of any knowledge that is acquired by the goal-directed system. This assumption is in contrast to the model proposed in <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>, where the uncertainty of the goal-directed system also plays role in competition among the systems. One way to test this assumption of our model is to manipulate the knowledge of the goal-directed system, while other variables are remained intact, and to test the impact on the goal-directedness of animal's behaviour. For this purpose, a place/response task similar to what is suggested in <xref ref-type="fig" rid="pcbi-1002055-g010">Figure 10</xref> can be used.</p>
        <fig id="pcbi-1002055-g010" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002055.g010</object-id>
          <label>Figure 10</label>
          <caption>
            <title>An experiment for testing the validity of the model.</title>
            <p>The proposed model predicts that manipulating the knowledge acquired by the goal-directed system should not affect the goal-directedness of behaviours. To test this prediction, a place/response task can be used. (A) In the first phase, the animal is moderately trained to acquire food reward in a T-maze. Since this training is moderate, the goal-directed system is expected to control behaviour during this phase. (B) In the second phase, the uncertainty of the goal-directed system is increased by putting the animal inside the right arm for some few trials, while the food reward comes at random or is totally removed. (C) Since the second phase doesn't have any effect on the habitual system, our model predicts that the arbitration between the system must have remained intact and thus, responding should still be goal-directed in the third phase. For that, the animal should still chose turning toward the window, even though its starting point is at the opposite end of the maze.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.g010" xlink:type="simple"/>
        </fig>
        <p>In the first phase, the animal is moderately trained to retrieve food from one arm of a T-maze. Since the training period is moderate, we expect that at the end of this phase, the animal will use a place strategy (goal-directed system) at the choice point, rather than a response strategy (habitual system). Thus, if the animal is then directly tested in the third phase, e.g., the starting arm is placed at the opposite end of the maze, it is expected to still turn toward the window. Now, the critical prediction of our model is that if any manipulation is applied only to the goal-directed system during a new phase between training and test, it should not change the animal's strategy. In fact, our model will be falsified if after such manipulations, the animal chooses the “turn right” response at the choice point (going in the opposite direction of the window), which indicates that it is using the response strategy, rather than the place strategy.</p>
        <p>One manipulation is to put the animal inside the right arm for some very few trials, while the food reward comes at random or is totally removed. This will increase the uncertainty of the goal-directed system about the outcome of the strategy “running toward the window”. Note that the number of trials should be sufficiently small such that the animal is not able to learn the new conditions, but only to increase its uncertainty. Among the variables of our model that influence arbitration (i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e217" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e218" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e219" xlink:type="simple"/></inline-formula>), the only variable that is affected due to this manipulation is the average reward variable (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e220" xlink:type="simple"/></inline-formula>). However, since this variable is decreased, the model predicts that such a manipulation will make responding even more goal-directed than before. As the animal has not experienced being at the choice point during the second phase, the habitual system will remain intact in this phase. In sum, our model predicts that whatever the number of trials in the second phase is, the animal must still respond goal-directedly (turn toward the window) in the test phase, even though the second phase has increased the uncertainty of the goal-directed system.</p>
        <p>The above experiment is in fact a way to test the hypothesis of the model that outcome-sensitivity after re-exposure (in devaluation experiments) is not the result of shift in control from the habitual to the goal-directed system (through manipulating the goal-directed knowledge during the incentive learning period, as suggested in <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>), but instead, it is because the goal-directed system has been dominant even before devaluation, and the only effect of the re-exposure phase is learning the new incentive value of outcomes (updating the reward function of the goal-directed system). This explanation is the dominant explanation for incentive learning <xref ref-type="bibr" rid="pcbi.1002055-Balleine2">[68]</xref>. However, if the rats in the above experiment show response strategy in the third phase (in contrast to what our model predicts), it will support the hypothesis that manipulating the goal-directed system can affect arbitration, and that outcome-sensitivity after devaluation might be due to such a manipulation <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref>.</p>
        <p>Another assumption of our model is that when the animal is at the choice point, the time needed for computing the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e221" xlink:type="simple"/></inline-formula>, which is in fact the time needed for arbitration, is trivial, compared to the time needed for goal-directed search. As mentioned before, this is a plausible assumption since the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e222" xlink:type="simple"/></inline-formula> signal can be computed by a closed form equation <xref ref-type="bibr" rid="pcbi.1002055-Dearden1">[18]</xref>. However, it might be argued that goal-directed responding can also be achieved within a trivial period of time. This is possible, for example, by assuming that the goal-directed system is capable of evaluating the value of choices in an off-line mode (when the animal is not necessarily performing the task) and caching them for future exploitations. Similarly, the goal-directed system might be argued to be neurally implemented by an attractor equation for value iteration (e.g. <xref ref-type="bibr" rid="pcbi.1002055-Suri1">[69]</xref>). Fortunately, the assumption of our model that goal-directed search requires a considerable time is experimentally testable by measuring the animal's reaction time at the choice points, and comparing them when responding is habitual vs. when it is goal-directed (see <xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:I</xref>).</p>
      </sec>
      <sec id="s4e">
        <title>Future Directions</title>
        <p>One limitation of the proposed model is that the computation of the average reward signal, which is assumed to be encoded by tonic dopamine, requires the simulated task to be cyclic and highly repetitive. For example, since shifts in the animal's motivational states don't have an immediate impact on the average reward signal, they cannot have a direct effect on the arbitration mechanism. This is despite the fact that motivational states, like hunger and thirst, are demonstrated to modulate the tonic firing activity of dopamine neurons <xref ref-type="bibr" rid="pcbi.1002055-Robbins1">[70]</xref>, even before new training under the new motivational state being provided to the animal. It is also analytically more reasonable that the opportunity cost be a function of motivational states; e.g. a hungry animal has a higher opportunity cost, compared to a sated one. One way to resolve this limitation is to develop a more realistic formulation for opportunity cost, rather than the simple average reward formulation.</p>
        <p>A similar limitation of the model concerns the necessity of experiencing rule changes by the subject, for the arbitration mechanism to be affected. In fact, the model is silent about how an unexperienced, but verbally communicated, environmental change can affect the competition between the two systems. At least in some cases for humans, it seems that a communicated change in the context makes the goal-directed system able to override the habitual response. Modeling such a phenomenon requires a normative way for the arbitration mechanism to be directly influenced by verbal instructions. Although in our model verbal instructions are supposed to affect the subjects' goal-directed knowledge, they don't contribute to the arbitration mechanism.</p>
        <p>A critical question that must be answered in any dual-process account of decision making is why animals need two systems. In fact, if the goal-directed system makes more rational decisions, then why the habitual system should have survived? One raw answer to this question could be that animals' brains were not redesigned anew through the course of evolution, but new capabilities were added to the underlying, evolutionarily old brain structures. A more sophisticated answer is that deliberation is subject to some constraints in a way that making habitual responses is more optimal at many choice points. The constraint that our model relies on is the slowness of deliberation. But it can be argued that an increase in response latency is only one of the costs that the animals' decision making machinery must pay for flexibility in sensorimotor coordination; and some other advantages can be counted for the habitual process, each of which is potentially the basis of another normative computational model.</p>
        <p>Working memory limitations is another constraint imposed on the goal-directed process. The previously acquired information that the goal-directed system requires for its analysis must first be loaded to working memory. Hence, subject to working memory limitations, the goal-directed system might not be provided with enough materials for an accurate deliberation and so, its response might be less optimal than the corresponding habitual response.</p>
        <p>One more comparative advantage of the habitual system is that it seems impossible, or at least very costly to deliberate about more than one issue at a time, whereas the habitual responses involve massively parallel processing <xref ref-type="bibr" rid="pcbi.1002055-Shiv1">[71]</xref>. For example, so many habitual responses are made by a taxi driver while he/she is driving, but the deliberative system is involved in only one issue, e.g. finding the shortest path to reach the destination. Another influential factor that seems to favour habitual decisions despite their non-optimality is that goal-directed deliberation consumes more energy than habitual action selection. For example, low availability of blood glucose, which is the main fuel supporting brain function, results in impairments in cognitive tasks <xref ref-type="bibr" rid="pcbi.1002055-Evans1">[72]</xref>. This factor can be captured by adding an energy cost term, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e223" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e224" xlink:type="simple"/></inline-formula>), to the cost of deliberation, and hence, for arbitration between the two processes, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e225" xlink:type="simple"/></inline-formula> signal must be compared with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e226" xlink:type="simple"/></inline-formula>.</p>
        <p>In both dual-process models proposed in <xref ref-type="bibr" rid="pcbi.1002055-Daw1">[5]</xref> and in this paper, the only type of interaction between the two systems is “competition”. However, collaborative interaction between different associative structures can also facilitate optimal action selection. Among different anatomy-based proposals offered for how segregated cortico-basal ganglia loops might be integrated, the spiral organization of DA neurons have proved compatible with the RL framework. Through these spiral connections between the striatum and the Ventral Tegmental Area/Sabstantia Nigra, the output of more ventral areas of the striatum can affect the functioning of more dorsal regions <xref ref-type="bibr" rid="pcbi.1002055-Haber1">[73]</xref>, <xref ref-type="bibr" rid="pcbi.1002055-Haber2">[74]</xref>. Accordingly, it has been hypothesized that by propagating the teaching signal from associative to motor areas of the basal ganglia, more abstract policy representations can facilitate learning habitual motor-level actions <xref ref-type="bibr" rid="pcbi.1002055-Belin1">[75]</xref>–<xref ref-type="bibr" rid="pcbi.1002055-Joel1">[77]</xref>. Based on these evidence, the goal-directed system can be assumed to affect the computation of the prediction error signal, in order to accelerate consolidating the optimal responses in the habitual system. This can substantially resolve the curse of dimensionality in model-free RL, which refers to the exponential growth of learning required for the habitual system when the complexity of the environment increases <xref ref-type="bibr" rid="pcbi.1002055-Barto1">[78]</xref>.</p>
      </sec>
      <sec id="s4f">
        <title>Mathematical Methods</title>
        <sec id="s4f1">
          <title>Value estimation by the habitual process</title>
          <p>The role of the habitual system is to store and update the value of state-action pairs in a cached form, from which high-speed retrieval is possible. If enough experience in provided, the value of each state-action pair, denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e227" xlink:type="simple"/></inline-formula>, converges to the total discounted rewards expected to be obtained by taking action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e228" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e229" xlink:type="simple"/></inline-formula> and then following the optimal policy in subsequent states. Regarding that probability distribution functions over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e230" xlink:type="simple"/></inline-formula>-values are required for calculating the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e231" xlink:type="simple"/></inline-formula> signal, the habitual system also stores and updates an estimation of the accuracy of the learned <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e232" xlink:type="simple"/></inline-formula>-values.</p>
          <p>For storing state-action values a look-up table representation is used, which is a special case of the linear parametrization of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e233" xlink:type="simple"/></inline-formula>-values. For learning <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e234" xlink:type="simple"/></inline-formula>-values, we used the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e235" xlink:type="simple"/></inline-formula>-learning version of the Kalman Temporal Differences (KTD) framework proposed in <xref ref-type="bibr" rid="pcbi.1002055-Geist1">[16]</xref>. In addition to learning state-action values, this method provides a measure of accuracy of learned values, which corresponds to the certainty of estimations.</p>
          <p>In this framework, the state-space of the problem is formulated as follows:</p>
          <p>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e236" xlink:type="simple"/><label>(8)</label></disp-formula>
          </p>
          <p>The first equation implies that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e237" xlink:type="simple"/></inline-formula>-values follow a random walk process. This means that the value of a state-action is composed of its past value plus an evolution noise, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e238" xlink:type="simple"/></inline-formula> (a Gaussian white noise). The assumption of a process noise for the evolution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e239" xlink:type="simple"/></inline-formula>-values is necessary because we utilize this framework for the learning of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e240" xlink:type="simple"/></inline-formula>-values in a non-stationary MDP, i.e., the reward function of the environment might change over time. The second equation is based on the Bellman equation. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e241" xlink:type="simple"/></inline-formula> is the observation noise and is supposed to be a Gaussian white noise.</p>
          <p>As in the KTD framework where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e242" xlink:type="simple"/></inline-formula>-values have distribution functions rather than point estimations, the algorithm keeps track of two matrices: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e243" xlink:type="simple"/></inline-formula>, which stores the mean of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e244" xlink:type="simple"/></inline-formula>-values for different state-action pairs, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e245" xlink:type="simple"/></inline-formula>, which is the covariance matrix of the former matrix. The diagonal elements of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e246" xlink:type="simple"/></inline-formula> contain the variance of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e247" xlink:type="simple"/></inline-formula>-values. The distribution functions over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e248" xlink:type="simple"/></inline-formula>-values are assumed to be Gaussian.</p>
          <p>Based on this formulation, after taking action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e249" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e250" xlink:type="simple"/></inline-formula> and transiting to a new state, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e251" xlink:type="simple"/></inline-formula>, the matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e252" xlink:type="simple"/></inline-formula> can be updated using the following learning rule:</p>
          <p>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e253" xlink:type="simple"/><label>(9)</label></disp-formula>
          </p>
          <p>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e254" xlink:type="simple"/></inline-formula> is the temporal difference error, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e255" xlink:type="simple"/></inline-formula> is the Kalman Gain, which determines the direction in which the current representation of values must be corrected. Moreover, after each transition, the covariance matrix is updated using the following equation:</p>
          <p>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e256" xlink:type="simple"/><label>(10)</label></disp-formula>
          </p>
          <p>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e257" xlink:type="simple"/></inline-formula> is the estimated variance of the observation equation. The Kalman Gain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e258" xlink:type="simple"/></inline-formula> is computed by:</p>
          <p>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e259" xlink:type="simple"/><label>(11)</label></disp-formula>
          </p>
          <p><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e260" xlink:type="simple"/></inline-formula> is the covariance between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e261" xlink:type="simple"/></inline-formula>-values and the observation equation. Regarding that the observation equation is nonlinear -because of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e262" xlink:type="simple"/></inline-formula> operator-, the values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e263" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e264" xlink:type="simple"/></inline-formula> cannot be directly computed from the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e265" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e266" xlink:type="simple"/></inline-formula> matrices. To address this issue, an unscented transform <xref ref-type="bibr" rid="pcbi.1002055-Julier1">[79]</xref> is used to approximate the statistics of interest <xref ref-type="bibr" rid="pcbi.1002055-Geist1">[16]</xref>. For more details of the KTD algorithm see <xref ref-type="bibr" rid="pcbi.1002055-Geist1">[16]</xref> (Algorithm 5).</p>
          <p>Finally, in equation 8 , the covariance matrix of the process noise is chosen in an adaptive way, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e267" xlink:type="simple"/></inline-formula>.</p>
          <p>Since the KTD algorithm used for estimating the mean and the variance of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e268" xlink:type="simple"/></inline-formula>-values is computationally expensive (e.g. it involves matrix inversions), one might think that it practically takes the same time that is sometimes withdrawn from goal-directed search. That is, the time necessary for doing the heavy computations of the KTD algorithm must also be taken into account when choosing whether to deliberate or not. However, it must be noticed that at the time that the model is confronted with some choices, all the knowledge required for computing the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e269" xlink:type="simple"/></inline-formula> signals (mean and variance of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e270" xlink:type="simple"/></inline-formula>-values) is already available in the KTD (habitual) system, without any new computation being required. In fact, all the heavy computations of the KTD algorithm are performed only after a decision is made and the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e271" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e272" xlink:type="simple"/></inline-formula> matrices should be updated. Thus, the time required for these computations doesn't influence reaction time.</p>
          <p>Moreover, it must be mentioned that the central contribution of the model is in the new arbitration mechanism proposed, and in how the mean and the variance of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e273" xlink:type="simple"/></inline-formula>-values can be used to make the arbitration rule approximately optimal. In this respect, any algorithm that can give an estimate of the mean and the variance of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e274" xlink:type="simple"/></inline-formula>-values can be substituted with the KTD algorithm, without affecting the arbitration rule. However, to our knowledge, the KTD algorithm is the most appropriate algorithm, among the currently available algorithms, for the case of the model presented here. The bayesian Q-learning algorithm <xref ref-type="bibr" rid="pcbi.1002055-Dearden1">[18]</xref>, for instance, updates the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e275" xlink:type="simple"/></inline-formula>-values without using a prediction-error signal and thus, it loses relevance to the dopamine theory.</p>
        </sec>
        <sec id="s4f2">
          <title>Value estimation by the goal-directed process</title>
          <p>Assuming that the goal-directed system has access to an estimation of the reward function, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e276" xlink:type="simple"/></inline-formula>, and the transition function, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e277" xlink:type="simple"/></inline-formula>, of the environment, then the value of each state-action pair can be calculated using the following recursive equation:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e278" xlink:type="simple"/><label>(12)</label></disp-formula></p>
          <p>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e279" xlink:type="simple"/></inline-formula> is the discount factor. As the transition graph is cyclic, we impose a maximum limit on the depth of the search. This maximum limit is assumed to be three levels in simulations. After this limit is reached, the recursive process stops and uses the estimated <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e280" xlink:type="simple"/></inline-formula> from the habitual system as an estimation of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e281" xlink:type="simple"/></inline-formula> afterward.</p>
          <p>The transition function is initialized to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e282" xlink:type="simple"/></inline-formula>, for all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e283" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e284" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e285" xlink:type="simple"/></inline-formula> is the total number of states. Assuming that after taking action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e286" xlink:type="simple"/></inline-formula> at state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e287" xlink:type="simple"/></inline-formula>, the animal goes to the new state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e288" xlink:type="simple"/></inline-formula>, the transition function can be updated using the following rule:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e289" xlink:type="simple"/><label>(13)</label></disp-formula></p>
          <p>Where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e290" xlink:type="simple"/></inline-formula> is the update rate of the transition function. This redistribution rule ensures <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e291" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e292" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e293" xlink:type="simple"/></inline-formula>.</p>
          <p>The estimation of an immediate reward, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e294" xlink:type="simple"/></inline-formula>, is calculated by taking an exponential moving average over the rewards gained after execution of action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e295" xlink:type="simple"/></inline-formula> at state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e296" xlink:type="simple"/></inline-formula> by the agent:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e297" xlink:type="simple"/><label>(14)</label></disp-formula>Where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e298" xlink:type="simple"/></inline-formula> is the update rate of the reward function. For modeling the devaluation of the outcome in the first two simulations, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e299" xlink:type="simple"/></inline-formula> is set to -1.</p>
        </sec>
        <sec id="s4f3">
          <title>Arbitration between the two processes</title>
          <p>When the agent is in state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e300" xlink:type="simple"/></inline-formula>, for the purpose of selecting an action among the feasible choices for performance, it needs to have an estimate of the value of each choice. The estimated value of each action can come from either the habitual or the goal-directed process. Thus, for having the final estimated value of each action, the agent has two options: to use values stored in the habitual system or to follow action-outcome contingencies to gain perfect information about state-action values.</p>
          <p>If the habitual system is used for acquiring the value of action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e301" xlink:type="simple"/></inline-formula> at state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e302" xlink:type="simple"/></inline-formula>, then the animal predicts that it will gain a future reward equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e303" xlink:type="simple"/></inline-formula>, by taking that action. In contrast, if the agent chooses to use the goal-directed system, then the expected sum of discounted rewards will increase by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e304" xlink:type="simple"/></inline-formula> units, due to the policy improvement effect resulted from deliberation. But as it takes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e305" xlink:type="simple"/></inline-formula> time units for goal-directed value estimation, that extra amount of reward (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e306" xlink:type="simple"/></inline-formula>) will come after a delay and thus, will be discounted. In fact, by using the goal-directed system, the agent predicts to gain a future reward equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e307" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e308" xlink:type="simple"/></inline-formula> is the discount factor. To act optimally, the agent chooses to deliberate only if it predicts that deliberation will bring it more rewards in future, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e309" xlink:type="simple"/></inline-formula>. This argument leads to the following decision rule:</p>
          <p>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e310" xlink:type="simple"/><label>(15)</label></disp-formula>
          </p>
          <p>We are interested in finding a more intuitive equivalent for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e311" xlink:type="simple"/></inline-formula>. To do so, as proposed in <xref ref-type="bibr" rid="pcbi.1002055-Daw2">[80]</xref>, equation 1 can be rewritten as follows:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e312" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e313" xlink:type="simple"/></inline-formula> is the average reward calculated over non-exploratory actions, which means that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e314" xlink:type="simple"/></inline-formula> is updated by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e315" xlink:type="simple"/></inline-formula>, only if the action with the highest expected value has been executed.</p>
          <p>In equation 16 , as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e316" xlink:type="simple"/></inline-formula>, the first term of the above equation tends to the average adjusted value of the state-action pair, which remains finite under some conditions that hold when linear parametrization of values is used and the environment is cyclic <xref ref-type="bibr" rid="pcbi.1002055-Tsitsiklis1">[81]</xref>. Hence, we will have:</p>
          <p>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e317" xlink:type="simple"/><label>(17)</label></disp-formula>
          </p>
          <p>Using the above equation and assuming that the discount factor has a value close to one, the decision rule noted in equation 15 , can be rewritten as follows:</p>
          <p>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e318" xlink:type="simple"/><label>(18)</label></disp-formula>
          </p>
          <p>It is straightforward to show that if rather than the sum of discounted rewards, the goal of the agent was to maximize the average reward signal during its life, then equation 18 would still be an optimal decision rule. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e319" xlink:type="simple"/></inline-formula> is computed according to equation 7 over non-exploratory actions. For calculation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e320" xlink:type="simple"/></inline-formula>, we assume that the time spent for one value-iteration is proportional to the number of edges of the graph traversed during the value iteration process. Also, the time needed to traverse an edge of the graph is assumed to be 0.08 of a time-step. Under these assumptions, we compute the agent's expectation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e321" xlink:type="simple"/></inline-formula> by averaging over the amount of time spent on previous deliberations.</p>
          <p>Based on the above discussion, we can define <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e322" xlink:type="simple"/></inline-formula>, the final estimated value assigned to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e323" xlink:type="simple"/></inline-formula> for the purpose of action selection, as follows:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e324" xlink:type="simple"/><label>(19)</label></disp-formula></p>
          <p>As illustrated, this value has come from the habitual or the goal-directed process, depending on the result of arbitration. According to this valuation, action selection will be carried out using the softmax action selection rule:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e325" xlink:type="simple"/><label>(20)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e326" xlink:type="simple"/></inline-formula> is inverse temperature and determines the rate of exploration.</p>
          <p>Finally, assuming that each state-action value has a normal distribution as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e327" xlink:type="simple"/></inline-formula>, then based on equation 6, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e328" xlink:type="simple"/></inline-formula> can be calculated as follows <xref ref-type="bibr" rid="pcbi.1002055-Dearden1">[18]</xref>:</p>
          <p>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e329" xlink:type="simple"/><label>(21)</label></disp-formula>
          </p>
          <p>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e330" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e331" xlink:type="simple"/></inline-formula> are the best and the second best actions at state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e332" xlink:type="simple"/></inline-formula>, respectively.</p>
          <p><bold>Simulation details.</bold> <xref ref-type="table" rid="pcbi-1002055-t002">Table 2</xref> shows the free parameters of the model and their assigned values in simulations.</p>
          <table-wrap id="pcbi-1002055-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002055.t002</object-id><label>Table 2</label><caption>
              <title>Free parameters of the model and their assigned values.</title>
            </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002055-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002055.t002" xlink:type="simple"/><table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Value</td>
                  <td align="left" colspan="1" rowspan="1">Range</td>
                  <td align="left" colspan="1" rowspan="1">Symbol</td>
                  <td align="left" colspan="1" rowspan="1">Free Parameter</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" colspan="1" rowspan="1">0.02</td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e333" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e334" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">Updating Rate of the Average Reward</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">0.0001</td>
                  <td align="left" colspan="1" rowspan="1">-</td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e335" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">Used to Determine Process Noise</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">0.05</td>
                  <td align="left" colspan="1" rowspan="1">-</td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e336" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">Variance of Observation Noise</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">1</td>
                  <td align="left" colspan="1" rowspan="1">-</td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e337" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">Rate of Exploration</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">0.1</td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e338" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e339" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">Update Rate of the Reward Function</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">0.95</td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e340" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e341" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">Discount Factor</td>
                </tr>
              </tbody>
            </table></alternatives></table-wrap>
          <p>We showed before that one requirement for the proposed switching mechanism between the two systems to be statistically optimal is that the discount factor, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e342" xlink:type="simple"/></inline-formula>, should be sufficiently close to one. However, as the MDPs of the simulated tasks are cyclic, setting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e343" xlink:type="simple"/></inline-formula> equal to one is nonsense (it will result in non-converging, infinitely large <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e344" xlink:type="simple"/></inline-formula>-values). Thus, in simulations, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e345" xlink:type="simple"/></inline-formula> is set very close to one (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e346" xlink:type="simple"/></inline-formula>).</p>
          <p>Since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e347" xlink:type="simple"/></inline-formula> is close to one, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e348" xlink:type="simple"/></inline-formula>-variables converge to relatively high values. However, as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e349" xlink:type="simple"/></inline-formula> is only affected by the relative value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e350" xlink:type="simple"/></inline-formula>-variables, and not their absolute values, the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e351" xlink:type="simple"/></inline-formula> does not affect <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e352" xlink:type="simple"/></inline-formula> and thus, does not affect the temporal dynamics of arbitration directly.</p>
          <p>On the other hand, since a softmax action selection rule is used, the absolute value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e353" xlink:type="simple"/></inline-formula>-variables also becomes important. In fact, high values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e354" xlink:type="simple"/></inline-formula>-variables caused by the high value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e355" xlink:type="simple"/></inline-formula> decreases the probability of better actions to be chosen at the action selection phase. This is why the model has chosen at best 60% in <xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:H</xref>, although the difference between the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e356" xlink:type="simple"/></inline-formula>-values of the two actions is remarkable (<xref ref-type="fig" rid="pcbi-1002055-g003">Figure 3:J</xref>). Of course, this effect can be easily controlled by adjusting the exploration rate, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e357" xlink:type="simple"/></inline-formula>. Higher values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e358" xlink:type="simple"/></inline-formula> will result in relatively higher probability of selecting the best action.</p>
          <p>In sum, although the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e359" xlink:type="simple"/></inline-formula> does not affect the arbitration mechanism directly, since it changes action selection probabilities, it influences the convergence speed of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e360" xlink:type="simple"/></inline-formula>-values and thus, affect the arbitration mechanism indirectly. However, it is shown through some simulations that different values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e361" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002055.e362" xlink:type="simple"/></inline-formula> do not change the essence of the behaviour of the model, but only affect the exact time at which switching from one system to the other happens.</p>
        </sec>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <p>The authors would like to thank Quentin Huys, Yael Niv and Arash Khodadadi for their valuable comments on the manuscript. We are also very grateful to Nathaniel Daw for helpful discussions.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002055-Rangel1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rangel</surname><given-names>A</given-names></name><name name-style="western"><surname>Camerer</surname><given-names>C</given-names></name><name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name></person-group>             <year>2008</year>             <article-title>A framework for studying the neurobiology of valuebased decision making.</article-title>             <source>Nat Rev Neurosci</source>             <volume>9</volume>             <fpage>545</fpage>             <lpage>556</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Dickinson1">
        <label>2</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dickinson</surname><given-names>A</given-names></name><name name-style="western"><surname>Balleine</surname><given-names>BW</given-names></name></person-group>             <year>2002</year>             <article-title>The role of learning in motivation.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Gallistel</surname><given-names>CR</given-names></name></person-group>             <source>Steven's Handbook of Experimental Psychology: Learning, Motivation, and Emotion</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Wiley</publisher-name>             <fpage>497</fpage>             <lpage>533</lpage>             <comment>Volume 3</comment>             <comment>3rd edition</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Adams1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Adams</surname><given-names>CD</given-names></name></person-group>             <year>1982</year>             <article-title>Variations in the sensitivity of instrumental responding to reinforcer devaluation.</article-title>             <source>Q J Exp Psychol</source>             <volume>34</volume>             <fpage>77</fpage>             <lpage>98</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Balleine1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Balleine</surname><given-names>BW</given-names></name><name name-style="western"><surname>O'Doherty</surname><given-names>JP</given-names></name></person-group>             <year>2010</year>             <article-title>Human and rodent homologies in action control: corticostriatal determinants of goal-directed and habitual action.</article-title>             <source>Neuropsychopharmacol</source>             <volume>35</volume>             <fpage>48</fpage>             <lpage>69</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Daw1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name><name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name></person-group>             <year>2005</year>             <article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control.</article-title>             <source>Nat Neurosci</source>             <volume>8</volume>             <fpage>1704</fpage>             <lpage>11</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Tolman1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tolman</surname><given-names>E</given-names></name></person-group>             <year>1938</year>             <article-title>The determiners of behavior at a choice point.</article-title>             <source>Psychol Rev</source>             <volume>45</volume>             <fpage>1</fpage>             <lpage>41</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Tolman2">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tolman</surname><given-names>E</given-names></name></person-group>             <year>1939</year>             <article-title>Prediction of vicarious trial and error by means of the schematic sowbug.</article-title>             <source>Psychol Rev</source>             <volume>46</volume>             <fpage>318</fpage>             <lpage>336</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Muenzinger1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Muenzinger</surname><given-names>K</given-names></name></person-group>             <year>1938</year>             <article-title>Vicarious trial and error at a point of choice. i. a general survey of its relation to learning efficacy.</article-title>             <source>J Genet Psychol</source>             <volume>53</volume>             <fpage>75</fpage>             <lpage>86</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Brown1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Brown</surname><given-names>MF</given-names></name></person-group>             <year>1992</year>             <article-title>Does a cognitive map guide choices in the radial-arm maze?</article-title>             <source>J Exp Psychol Anim Behav Process</source>             <volume>18</volume>             <fpage>55</fpage>             <lpage>66</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Buckner1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Buckner</surname><given-names>RL</given-names></name><name name-style="western"><surname>Carroll</surname><given-names>DC</given-names></name></person-group>             <year>2007</year>             <article-title>Self-projection and the brain.</article-title>             <source>Trends Cogn Sci</source>             <volume>11</volume>             <fpage>49</fpage>             <lpage>57</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Redish1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Redish</surname><given-names>AD</given-names></name><name name-style="western"><surname>Jensen</surname><given-names>S</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>A</given-names></name><name name-style="western"><surname>Kurth-Nelson</surname><given-names>Z</given-names></name></person-group>             <year>2007</year>             <article-title>Reconciling reinforcement learning models with behavioral extinction and renewal: implications for addiction, relapse, and problem gambling.</article-title>             <source>Psychol Rev</source>             <volume>114</volume>             <fpage>784</fpage>             <lpage>805</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Hu1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>D</given-names></name><name name-style="western"><surname>Xu</surname><given-names>X</given-names></name><name name-style="western"><surname>Gonzalez-Lima</surname><given-names>F</given-names></name></person-group>             <year>2006</year>             <article-title>Vicarious trial-and-error behavior and hippocampal cytochrome oxidase activity during y-maze discrimination learning in the rat.</article-title>             <source>Int J Neurosci</source>             <volume>116</volume>             <fpage>265</fpage>             <lpage>280</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Munn1">
        <label>13</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Munn</surname><given-names>NL</given-names></name></person-group>             <year>1950</year>             <article-title>Handbook of psychological research on the rat.</article-title>             <publisher-loc>Boston</publisher-loc>             <publisher-name>Houghton Mifflin</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Redish2">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Redish</surname><given-names>AD</given-names></name><name name-style="western"><surname>Jensen</surname><given-names>S</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>A</given-names></name></person-group>             <year>2008</year>             <article-title>A unified framework for addiction: vulnerabilities in the decision process.</article-title>             <source>Behav Brain Sci</source>             <volume>31</volume>             <fpage>415</fpage>             <lpage>437</lpage>             <page-range>415</page-range>             <comment>discussion 437–487</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Sutton1">
        <label>15</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name><name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name></person-group>             <year>1998</year>             <article-title>Reinforcement Learning: An Introduction.</article-title>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Geist1">
        <label>16</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Geist</surname><given-names>M</given-names></name><name name-style="western"><surname>Pietquin</surname><given-names>O</given-names></name><name name-style="western"><surname>Fricout</surname><given-names>G</given-names></name></person-group>             <year>2009</year>             <article-title>Kalman temporal differences: the deterministic case.</article-title>             <publisher-loc>USA</publisher-loc>             <publisher-name>Nashville</publisher-name>             <fpage>185</fpage>             <lpage>192</lpage>             <comment>In:Proceedings of the 2009 IEEE International Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL-09)</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Howard1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Howard</surname><given-names>R</given-names></name></person-group>             <year>1996</year>             <article-title>Information value theory.</article-title>             <source>IEEE T Syst Sci Cyb</source>             <volume>2</volume>             <fpage>22</fpage>             <lpage>26</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Dearden1">
        <label>18</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dearden</surname><given-names>R</given-names></name><name name-style="western"><surname>Friedman</surname><given-names>N</given-names></name><name name-style="western"><surname>Russell</surname><given-names>S</given-names></name></person-group>             <year>1998</year>             <article-title>Bayesian Q-Learning.</article-title>             <fpage>761</fpage>             <lpage>768</lpage>             <comment>In:Proceedings of the 15th National Conference on Artificial Intelligence (AAAI)</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Niv1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name><name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name><name name-style="western"><surname>Joel</surname><given-names>D</given-names></name><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name></person-group>             <year>2007</year>             <article-title>Tonic dopamine: opportunity costs and the control of response vigor.</article-title>             <source>Psychopharmacology (Berl)</source>             <volume>191</volume>             <fpage>507</fpage>             <lpage>520</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Dickinson2">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dickinson</surname><given-names>A</given-names></name><name name-style="western"><surname>Balleine</surname><given-names>B</given-names></name></person-group>             <year>1995</year>             <article-title>Motivational control of instrumental action.</article-title>             <source>Curr Dir Psychol Sci</source>             <volume>4</volume>             <fpage>162</fpage>             <lpage>167</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Holland1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Holland</surname><given-names>PC</given-names></name></person-group>             <year>2004</year>             <article-title>Relations between pavlovian-instrumental transfer and reinforcer devaluation.</article-title>             <source>J Exp Psychol Anim Behav Process</source>             <volume>30</volume>             <fpage>104</fpage>             <lpage>117</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Killcross1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Killcross</surname><given-names>S</given-names></name><name name-style="western"><surname>Coutureau</surname><given-names>E</given-names></name></person-group>             <year>2003</year>             <article-title>Coordination of actions and habits in the medial prefrontal cortex of rats.</article-title>             <source>Cereb Cortex</source>             <volume>13</volume>             <fpage>400</fpage>             <lpage>408</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Yin1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>HH</given-names></name><name name-style="western"><surname>Knowlton</surname><given-names>BJ</given-names></name><name name-style="western"><surname>Balleine</surname><given-names>BW</given-names></name></person-group>             <year>2004</year>             <article-title>Lesions of dorsolateral striatum preserve outcome expectancy but disrupt habit formation in instrumental learning.</article-title>             <source>Eur J Neurosci</source>             <volume>19</volume>             <fpage>181</fpage>             <lpage>189</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Colwill1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Colwill</surname><given-names>RM</given-names></name><name name-style="western"><surname>Rescorla</surname><given-names>RA</given-names></name></person-group>             <year>1985</year>             <article-title>Instrumental responding remains sensitive to reinforcer devaluation after extensive training.</article-title>             <source>J Exp Psychol Anim Behav Process</source>             <volume>11</volume>             <fpage>520</fpage>             <lpage>536</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Colwill2">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Colwill</surname><given-names>RM</given-names></name><name name-style="western"><surname>Rescorla</surname><given-names>RA</given-names></name></person-group>             <year>1988</year>             <article-title>The role of response-reinforcer associations increases throughout extended instrumental training.</article-title>             <source>Anim Learn Behav</source>             <volume>16</volume>             <fpage>105</fpage>             <lpage>111</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Kosaki1">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kosaki</surname><given-names>Y</given-names></name><name name-style="western"><surname>Dickinson</surname><given-names>A</given-names></name></person-group>             <year>2010</year>             <article-title>Choice and contingency in the development of behavioral autonomy during instrumental conditioning.</article-title>             <source>J Exp Psychol Anim Behav Process</source>             <volume>36</volume>             <fpage>334</fpage>             <lpage>342</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Pessiglione1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pessiglione</surname><given-names>M</given-names></name><name name-style="western"><surname>Czernecki</surname><given-names>V</given-names></name><name name-style="western"><surname>Pillon</surname><given-names>B</given-names></name><name name-style="western"><surname>Dubois</surname><given-names>B</given-names></name><name name-style="western"><surname>Schüpbach</surname><given-names>M</given-names></name><etal/></person-group>             <year>2005</year>             <article-title>An effect of dopamine depletion on decision-making: the temporal coupling of deliberation and execution.</article-title>             <source>J Cog Neurosci</source>             <volume>17</volume>             <fpage>1886</fpage>             <lpage>1896</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Hick1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hick</surname><given-names>WE</given-names></name></person-group>             <year>1952</year>             <article-title>On the rate of gain of information.</article-title>             <source>Q J Exp Psychol</source>             <volume>4</volume>             <fpage>11</fpage>             <lpage>26</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Hyman1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hyman</surname><given-names>R</given-names></name></person-group>             <year>1953</year>             <article-title>Stimulus information as a determinant of reaction time.</article-title>             <source>J Exp Psychol</source>             <volume>45</volume>             <fpage>188</fpage>             <lpage>196</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Alluisi1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Alluisi</surname><given-names>E</given-names></name></person-group>             <year>1965</year>             <article-title>Interaction of S-R compatibility and the rate of gain of information.</article-title>             <source>Percept Mot Skills</source>             <volume>20</volume>             <fpage>815</fpage>             <lpage>816</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Alluisi2">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Alluisi</surname><given-names>E</given-names></name><name name-style="western"><surname>Strain</surname><given-names>G</given-names></name><name name-style="western"><surname>Thursmond</surname><given-names>J</given-names></name></person-group>             <year>1964</year>             <article-title>Stimulus-response compatibility and the rate of gain of information.</article-title>             <source>Psychon Sci</source>             <volume>1</volume>             <fpage>111</fpage>             <lpage>112</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Broadbent1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Broadbent</surname><given-names>DE</given-names></name><name name-style="western"><surname>Gregory</surname><given-names>M</given-names></name></person-group>             <year>1965</year>             <article-title>On the interaction of S-R compatibility with other variables affecting reaction time.</article-title>             <source>Brit J Psychol</source>             <volume>56</volume>             <fpage>61</fpage>             <lpage>67</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Spigel1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Spigel</surname><given-names>IM</given-names></name></person-group>             <year>1965</year>             <article-title>Lift reaction time and topographic compatibility of the S-R field.</article-title>             <source>J Gen Psychol</source>             <volume>72</volume>             <fpage>165</fpage>             <lpage>172</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Mahurin1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mahurin</surname><given-names>RK</given-names></name><name name-style="western"><surname>Pirozzolo</surname><given-names>FJ</given-names></name></person-group>             <year>1993</year>             <article-title>Application of hick's law of response speed in alzheimer and parkinson diseases.</article-title>             <source>Percept Mot Skills</source>             <volume>77</volume>             <fpage>107</fpage>             <lpage>113</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Vickrey1">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vickrey</surname><given-names>C</given-names></name><name name-style="western"><surname>Neuringer</surname><given-names>A</given-names></name></person-group>             <year>2000</year>             <article-title>Pigeon reaction time, hick's law, and intelligence.</article-title>             <source>Psychon Bull Rev</source>             <volume>7</volume>             <fpage>284</fpage>             <lpage>291</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Mowbray1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mowbray</surname><given-names>GH</given-names></name><name name-style="western"><surname>Rhoades</surname><given-names>MV</given-names></name></person-group>             <year>1959</year>             <article-title>On the reduction of choice reaction-times with practice.</article-title>             <source>Q J Exp Psychol</source>             <volume>11</volume>             <fpage>16</fpage>             <lpage>23</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Welford1">
        <label>37</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Welford</surname><given-names>AT</given-names></name></person-group>             <year>1980</year>             <article-title>Choice reaction time: basic concepts.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Welford</surname><given-names>AT</given-names></name></person-group>             <source>Reaction Times</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Academic Press</publisher-name>             <fpage>73</fpage>             <lpage>128</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Yuille1">
        <label>38</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yuille</surname><given-names>A</given-names></name><name name-style="western"><surname>Geiger</surname><given-names>D</given-names></name></person-group>             <year>1995</year>             <article-title>Winner-Take-All mechanisms.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Arbib</surname><given-names>M</given-names></name></person-group>             <source>Handbook of Brain Theory and Neural Networks</source>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Ellias1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ellias</surname><given-names>SA</given-names></name><name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name></person-group>             <year>1975</year>             <article-title>Pattern formation, contrast control, and oscillations in the short term memory of shunting on-center off-surround networks.</article-title>             <source>Biol Cybernetics</source>             <volume>20</volume>             <fpage>69</fpage>             <lpage>98</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Montague1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>             <year>1996</year>             <article-title>A framework for mesencephalic dopamine systems based on predictive hebbian learning.</article-title>             <source>J Neurosci</source>             <volume>16</volume>             <fpage>1936</fpage>             <lpage>1947</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Schultz1">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name></person-group>             <year>1998</year>             <article-title>Predictive reward signal of dopamine neurons.</article-title>             <source>J Neurophysiol</source>             <volume>80</volume>             <fpage>1</fpage>             <lpage>27</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Lyons1">
        <label>42</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lyons</surname><given-names>M</given-names></name><name name-style="western"><surname>Robbins</surname><given-names>TW</given-names></name></person-group>             <year>1975</year>             <article-title>The action of central nervous system stimulant drugs: a general theory concerning amphetamine effects.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Essman</surname><given-names>W</given-names></name><name name-style="western"><surname>Valzelli</surname><given-names>L</given-names></name></person-group>             <source>Current developments in psychopharmacology</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Spectrum</publisher-name>             <fpage>80</fpage>             <lpage>163</lpage>             <comment>Volume 2</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Evenden1">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Evenden</surname><given-names>JL</given-names></name><name name-style="western"><surname>Robbins</surname><given-names>TW</given-names></name></person-group>             <year>1983</year>             <article-title>Increased response switching, perseveration and perseverative switching following d-amphetamine in the rat.</article-title>             <source>Psychopharmacology (Berl)</source>             <volume>80</volume>             <fpage>67</fpage>             <lpage>73</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Taylor1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Taylor</surname><given-names>JR</given-names></name><name name-style="western"><surname>Robbins</surname><given-names>TW</given-names></name></person-group>             <year>1984</year>             <article-title>Enhanced behavioural control by conditioned reinforcers following microinjections of d-amphetamine into the nucleus accumbens.</article-title>             <source>Psychopharmacology (Berl)</source>             <volume>84</volume>             <fpage>405</fpage>             <lpage>412</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Taylor2">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Taylor</surname><given-names>JR</given-names></name><name name-style="western"><surname>Robbins</surname><given-names>TW</given-names></name></person-group>             <year>1986</year>             <article-title>6-Hydroxydopamine lesions of the nucleus accumbens, but not of the caudate nucleus, attenuate enhanced responding with reward-related stimuli produced by intra-accumbens d-amphetamine.</article-title>             <source>Psychopharmacology (Berl)</source>             <volume>90</volume>             <fpage>390</fpage>             <lpage>397</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Ljungberg1">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ljungberg</surname><given-names>T</given-names></name><name name-style="western"><surname>Enquist</surname><given-names>M</given-names></name></person-group>             <year>1987</year>             <article-title>Disruptive effects of low doses of d-amphetamine on the ability of rats to organize behaviour into functional sequences.</article-title>             <source>Psychopharmacology (Berl)</source>             <volume>93</volume>             <fpage>146</fpage>             <lpage>151</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Jackson1">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jackson</surname><given-names>DM</given-names></name><name name-style="western"><surname>Andén</surname><given-names>NE</given-names></name><name name-style="western"><surname>Dahlström</surname><given-names>A</given-names></name></person-group>             <year>1975</year>             <article-title>A functional effect of dopamine in the nucleus accumbens and in some other dopamine-rich parts of the rat brain.</article-title>             <source>Psychopharmacologia</source>             <volume>45</volume>             <fpage>139</fpage>             <lpage>149</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Carr1">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Carr</surname><given-names>GD</given-names></name><name name-style="western"><surname>White</surname><given-names>NM</given-names></name></person-group>             <year>1987</year>             <article-title>Effects of systemic and intracranial amphetamine injections on behavior in the open field: a detailed analysis.</article-title>             <source>Pharmacol Biochem Behav</source>             <volume>27</volume>             <fpage>113</fpage>             <lpage>122</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Sokolowski1">
        <label>49</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sokolowski</surname><given-names>JD</given-names></name><name name-style="western"><surname>Salamone</surname><given-names>JD</given-names></name></person-group>             <year>1998</year>             <article-title>The role of accumbens dopamine in lever pressing and response allocation: effects of 6-OHDA injected into core and dorsomedial shell.</article-title>             <source>Pharmacol Biochem Behav</source>             <volume>59</volume>             <fpage>557</fpage>             <lpage>566</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Aberman1">
        <label>50</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Aberman</surname><given-names>JE</given-names></name><name name-style="western"><surname>Salamone</surname><given-names>JD</given-names></name></person-group>             <year>1999</year>             <article-title>Nucleus accumbens dopamine depletions make rats more sensitive to high ratio requirements but do not impair primary food reinforcement.</article-title>             <source>Neuroscience</source>             <volume>92</volume>             <fpage>545</fpage>             <lpage>552</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Salamone1">
        <label>51</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Salamone</surname><given-names>JD</given-names></name><name name-style="western"><surname>Wisniecki</surname><given-names>A</given-names></name><name name-style="western"><surname>Carlson</surname><given-names>BB</given-names></name><name name-style="western"><surname>Correa</surname><given-names>M</given-names></name></person-group>             <year>2001</year>             <article-title>Nucleus accumbens dopamine depletions make animals highly sensitive to high fixed ratio requirements but do not impair primary food reinforcement.</article-title>             <source>Neuroscience</source>             <volume>105</volume>             <fpage>863</fpage>             <lpage>870</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Correa1">
        <label>52</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Correa</surname><given-names>M</given-names></name><name name-style="western"><surname>Carlson</surname><given-names>BB</given-names></name><name name-style="western"><surname>Wisniecki</surname><given-names>A</given-names></name><name name-style="western"><surname>Salamone</surname><given-names>JD</given-names></name></person-group>             <year>2002</year>             <article-title>Nucleus accumbens dopamine and work requirements on interval schedules.</article-title>             <source>Behav Brain Res</source>             <volume>137</volume>             <fpage>179</fpage>             <lpage>187</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Mingote1">
        <label>53</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mingote</surname><given-names>S</given-names></name><name name-style="western"><surname>Weber</surname><given-names>SM</given-names></name><name name-style="western"><surname>Ishiwari</surname><given-names>K</given-names></name><name name-style="western"><surname>Correa</surname><given-names>M</given-names></name><name name-style="western"><surname>Salamone</surname><given-names>JD</given-names></name></person-group>             <year>2005</year>             <article-title>Ratio and time requirements on operant schedules: effort-related effects of nucleus accumbens dopamine depletions.</article-title>             <source>Eur J Neurosci</source>             <volume>21</volume>             <fpage>1749</fpage>             <lpage>1757</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-MasColell1">
        <label>54</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mas-Colell</surname><given-names>A</given-names></name><name name-style="western"><surname>Whinston</surname><given-names>MD</given-names></name><name name-style="western"><surname>Green</surname><given-names>JR</given-names></name></person-group>             <year>1995</year>             <article-title>Microeconomic Theory.</article-title>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Cambridge Univ. Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Simon1">
        <label>55</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Simon</surname><given-names>H</given-names></name></person-group>             <year>1947</year>             <article-title>Administrative behavior.</article-title>             <publisher-loc>New York</publisher-loc>             <publisher-name>Macmillan</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Simon2">
        <label>56</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Simon</surname><given-names>H</given-names></name></person-group>             <year>1982</year>             <article-title>Volume 2, Models of bounded rationality.</article-title>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Simon3">
        <label>57</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Simon</surname><given-names>H</given-names></name></person-group>             <year>1955</year>             <article-title>A behavioral model of rational choice.</article-title>             <source>Q J Econ</source>             <volume>69</volume>             <fpage>99</fpage>             <lpage>118</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Gold1">
        <label>58</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gold</surname><given-names>JI</given-names></name><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name></person-group>             <year>2007</year>             <article-title>The neural basis of decision making.</article-title>             <source>Annu Rev Neurosci</source>             <volume>30</volume>             <fpage>535</fpage>             <lpage>574</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Gigerenzer1">
        <label>59</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gigerenzer</surname><given-names>G</given-names></name></person-group>             <year>2000</year>             <article-title>Adaptive thinking: Rationality in the real world.</article-title>             <publisher-loc>Evolution and New York</publisher-loc>             <publisher-name>Oxford University Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Gigerenzer2">
        <label>60</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gigerenzer</surname><given-names>G</given-names></name><name name-style="western"><surname>Todd</surname><given-names>PM</given-names></name></person-group>             <collab xlink:type="simple">Group AR</collab>             <year>1999</year>             <article-title>Simple heuristics that make us smart.</article-title>             <publisher-loc>New York</publisher-loc>             <publisher-name>Oxford University Press</publisher-name>             <comment>1st edition</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Zilberstein1">
        <label>61</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zilberstein</surname><given-names>S</given-names></name><name name-style="western"><surname>Russell</surname><given-names>S</given-names></name></person-group>             <year>1995</year>             <article-title>Approximate reasoning using anytime algorithms.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Natarajan</surname><given-names>S</given-names></name></person-group>             <source>Imprecise and Approximate Computation</source>             <publisher-name>Springer</publisher-name>             <fpage>43</fpage>             <lpage>62</lpage>             <comment>Volume 318 The Kluwer International Series in Engineering and Computer Science</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Jueptner1">
        <label>62</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jueptner</surname><given-names>M</given-names></name><name name-style="western"><surname>Frith</surname><given-names>CD</given-names></name><name name-style="western"><surname>Brooks</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Frackowiak</surname><given-names>RS</given-names></name><name name-style="western"><surname>Passingham</surname><given-names>RE</given-names></name></person-group>             <year>1997</year>             <article-title>Anatomy of motor learning. II. subcortical structures and learning by trial and error.</article-title>             <source>J Neurophysiol</source>             <volume>77</volume>             <fpage>1325</fpage>             <lpage>1337</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Jueptner2">
        <label>63</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jueptner</surname><given-names>M</given-names></name><name name-style="western"><surname>Stephan</surname><given-names>KM</given-names></name><name name-style="western"><surname>Frith</surname><given-names>CD</given-names></name><name name-style="western"><surname>Brooks</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Frackowiak</surname><given-names>RS</given-names></name><etal/></person-group>             <year>1997</year>             <article-title>Anatomy of motor learning. i. frontal cortex and attention to action.</article-title>             <source>J Neurophysiol</source>             <volume>77</volume>             <fpage>1313</fpage>             <lpage>1324</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Shah1">
        <label>64</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shah</surname><given-names>A</given-names></name><name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name></person-group>             <year>2009</year>             <article-title>Effect on movement selection of an evolving sensory representation: a multiple controller model of skill acquisition.</article-title>             <source>Brain Res</source>             <volume>1299</volume>             <fpage>55</fpage>             <lpage>73</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Tassinari1">
        <label>65</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tassinari</surname><given-names>H</given-names></name><name name-style="western"><surname>Hudson</surname><given-names>TE</given-names></name><name name-style="western"><surname>Landy</surname><given-names>MS</given-names></name></person-group>             <year>2006</year>             <article-title>Combining priors and noisy visual cues in a rapid pointing task.</article-title>             <source>J Neurosci</source>             <volume>26</volume>             <fpage>10154</fpage>             <lpage>10163</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-McClure1">
        <label>66</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>McClure</surname><given-names>SM</given-names></name><name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name><name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name></person-group>             <year>2003</year>             <article-title>A computational substrate for incentive salience.</article-title>             <source>Trends Neurosci</source>             <volume>26</volume>             <fpage>423</fpage>             <lpage>428</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Dickinson3">
        <label>67</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dickinson</surname><given-names>A</given-names></name></person-group>             <year>1985</year>             <article-title>Actions and habits: The development of behavioural autonomy.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>308</volume>             <fpage>78</fpage>             <lpage>67</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Balleine2">
        <label>68</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Balleine</surname><given-names>B</given-names></name><name name-style="western"><surname>Garner</surname><given-names>C</given-names></name><name name-style="western"><surname>Gonzalez</surname><given-names>F</given-names></name><name name-style="western"><surname>Dickinson</surname><given-names>A</given-names></name></person-group>             <year>1995</year>             <article-title>Motivational control of heterogeneous instrumental chains.</article-title>             <source>J Exp Psychol Anim Behav Process</source>             <volume>21</volume>             <fpage>203</fpage>             <lpage>217</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Suri1">
        <label>69</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Suri</surname><given-names>RE</given-names></name></person-group>             <year>2001</year>             <article-title>Anticipatory responses of dopamine neurons and cortical neurons reproduced by internal model.</article-title>             <source>Exp Brain Res</source>             <volume>140</volume>             <fpage>234</fpage>             <lpage>240</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Robbins1">
        <label>70</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Robbins</surname><given-names>TW</given-names></name><name name-style="western"><surname>Everitt</surname><given-names>BJ</given-names></name></person-group>             <year>1992</year>             <article-title>Functions of dopamine in the dorsal and ventral striatum.</article-title>             <source>Semin Neurosci</source>             <volume>4</volume>             <fpage>119</fpage>             <lpage>127</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Shiv1">
        <label>71</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shiv</surname><given-names>B</given-names></name><name name-style="western"><surname>Fedorikhin</surname><given-names>A</given-names></name></person-group>             <year>1999</year>             <article-title>Heart and mind in conflict: The interplay of affect and cognition in consumer decision making.</article-title>             <source>J Cons Res</source>             <volume>26</volume>             <fpage>278</fpage>             <lpage>92</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Evans1">
        <label>72</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Evans</surname><given-names>ML</given-names></name><name name-style="western"><surname>Sherwin</surname><given-names>RS</given-names></name></person-group>             <year>2002</year>             <article-title>Blood glucose and the brain in diabetes: between a rock and a hard place?</article-title>             <source>Curr Diab Rep</source>             <volume>2</volume>             <fpage>101</fpage>             <lpage>102</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Haber1">
        <label>73</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Haber</surname><given-names>SN</given-names></name><name name-style="western"><surname>Fudge</surname><given-names>JL</given-names></name><name name-style="western"><surname>McFarland</surname><given-names>NR</given-names></name></person-group>             <year>2000</year>             <article-title>Striatonigrostriatal pathways in primates form an ascending spiral from the shell to the dorsolateral striatum.</article-title>             <source>J Neurosci</source>             <volume>20</volume>             <fpage>2369</fpage>             <lpage>2382</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Haber2">
        <label>74</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Haber</surname><given-names>SN</given-names></name></person-group>             <year>2003</year>             <article-title>The primate basal ganglia: parallel and integrative networks.</article-title>             <source>J Chem Neuroanat</source>             <volume>26</volume>             <fpage>317</fpage>             <lpage>330</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Belin1">
        <label>75</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Belin</surname><given-names>D</given-names></name><name name-style="western"><surname>Jonkman</surname><given-names>S</given-names></name><name name-style="western"><surname>Dickinson</surname><given-names>A</given-names></name><name name-style="western"><surname>Robbins</surname><given-names>TW</given-names></name><name name-style="western"><surname>Everitt</surname><given-names>BJ</given-names></name></person-group>             <year>2009</year>             <article-title>Parallel and interactive learning processes within the basal ganglia: relevance for the understanding of addiction.</article-title>             <source>Behav Brain Res</source>             <volume>199</volume>             <fpage>89</fpage>             <lpage>102</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Haruno1">
        <label>76</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Haruno</surname><given-names>M</given-names></name><name name-style="western"><surname>Kawato</surname><given-names>M</given-names></name></person-group>             <year>2006</year>             <article-title>Heterarchical reinforcement-learning model for integration of multiple cortico-striatal loops: fMRI examination in stimulus-action-reward association learning.</article-title>             <source>Neural Netw</source>             <volume>19</volume>             <fpage>1242</fpage>             <lpage>1254</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Joel1">
        <label>77</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Joel</surname><given-names>D</given-names></name><name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name><name name-style="western"><surname>Ruppin</surname><given-names>E</given-names></name></person-group>             <year>2002</year>             <article-title>Actor-critic models of the basal ganglia: new anatomical and computational perspectives.</article-title>             <source>Neural Netw</source>             <volume>15</volume>             <fpage>535</fpage>             <lpage>547</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Barto1">
        <label>78</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name><name name-style="western"><surname>Mahadevan</surname><given-names>S</given-names></name></person-group>             <year>2003</year>             <article-title>Recent advances in hierarchical reinforcement learning.</article-title>             <source>Discrete Event Dyn S</source>             <volume>13</volume>             <fpage>341</fpage>             <lpage>379</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Julier1">
        <label>79</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Julier</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Uhlmann</surname><given-names>J</given-names></name></person-group>             <year>2004</year>             <article-title>Unscented filtering and nonlinear estimation.</article-title>             <source>Proc IEEE</source>             <volume>92</volume>             <fpage>401</fpage>             <lpage>422</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Daw2">
        <label>80</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name><name name-style="western"><surname>Touretzky</surname><given-names>DS</given-names></name></person-group>             <year>2002</year>             <article-title>Long-term reward prediction in TD models of the dopamine system.</article-title>             <source>Neural Comput</source>             <volume>14</volume>             <fpage>2567</fpage>             <lpage>2583</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002055-Tsitsiklis1">
        <label>81</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tsitsiklis</surname><given-names>JN</given-names></name><name name-style="western"><surname>Roy</surname><given-names>BV</given-names></name></person-group>             <year>1997</year>             <article-title>Average cost temporal-difference learning.</article-title>             <source>Automatica</source>             <volume>35</volume>             <fpage>1799</fpage>             <lpage>1808</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>