<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN"><front><journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="publisher">pcbi</journal-id><journal-id journal-id-type="flc">plcb</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1371/journal.pcbi.0020144</article-id><article-id pub-id-type="publisher-id">06-PLCB-RA-0327R2</article-id><article-id pub-id-type="sici">plcb-02-10-14</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology</subject><subject>Neuroscience</subject><subject>Computational Biology/Systems Biology</subject></subj-group><subj-group subj-group-type="System Taxonomy"><subject>Nervous system (visual)</subject></subj-group></article-categories><title-group><article-title>Mapping Information Flow in Sensorimotor Networks</article-title><alt-title alt-title-type="running-head">Mapping Information Flow in Sensorimotor Networks</alt-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Lungarella</surname><given-names>Max</given-names></name><xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref><xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>Olaf</given-names></name><xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref></contrib></contrib-group><aff id="aff1">
				<label>1</label><addr-line> Department of Mechano-Informatics, The University of Tokyo, Tokyo, Japan
			</addr-line></aff><aff id="aff2">
				<label>2</label><addr-line> Department of Psychological and Brain Sciences, Indiana University, Bloomington, Indiana, United States of America
			</addr-line></aff><contrib-group><contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes><fn fn-type="con" id="ack1"><p>ML and OS conceived and designed the experiments, performed the experiments, analyzed the data, contributed reagents/materials/analysis tools, and wrote the paper.</p></fn><corresp id="cor1">* To whom correspondence should be addressed. E-mail: <email xlink:type="simple">maxl@isi.imi.i.u-tokyo.ac.jp</email></corresp><fn fn-type="conflict" id="ack3"><p> The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="ppub"><month>10</month><year>2006</year></pub-date><pub-date pub-type="epub"><day>27</day><month>10</month><year>2006</year></pub-date><volume>2</volume><issue>10</issue><elocation-id>e144</elocation-id><history><date date-type="received"><day>9</day><month>8</month><year>2006</year></date><date date-type="accepted"><day>20</day><month>9</month><year>2006</year></date></history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2006</copyright-year><copyright-holder>Lungarella and Sporns</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract><p>Biological organisms continuously select and sample information used by their neural structures for perception and action, and for creating coherent cognitive states guiding their autonomous behavior. Information processing, however, is not solely an internal function of the nervous system. Here we show, instead, how sensorimotor interaction and body morphology can induce statistical regularities and information structure in sensory inputs and within the neural control architecture, and how the flow of information between sensors, neural units, and effectors is actively shaped by the interaction with the environment. We analyze sensory and motor data collected from real and simulated robots and reveal the presence of information structure and directed information flow induced by dynamically coupled sensorimotor activity, including effects of motor outputs on sensory inputs. We find that information structure and information flow in sensorimotor networks (a) is spatially and temporally specific; (b) can be affected by learning, and (c) can be affected by changes in body morphology. Our results suggest a fundamental link between physical embeddedness and information, highlighting the effects of embodied interactions on internal (neural) information processing, and illuminating the role of various system components on the generation of behavior.</p></abstract><abstract abstract-type="synopsis"><title>Synopsis</title><p>How neurons encode and process information is a key problem in computational biology and neuroscience. In this paper, Lungarella and Sporns present a novel application of computational methods to the integration of neural and sensorimotor processes at the systems-level scale. The central result of their study is that sensorimotor interaction and body morphology can induce statistical regularities and information structure in sensory inputs and within the neural control architecture. The informational content of inputs is thus not independent of output, and the authors suggest that neural coding needs to be considered in the context of the “embeddedness” of the organism within its eco-niche. Using robots and nonlinear time-series analysis techniques, they investigate how the flow of information between sensors, neural units, and effectors is actively shaped by interaction with the environment. This study represents a first step towards the development of an explicit quantitative framework that unifies neural and behavioral processes. Such a framework could also shed significant new light on key constraints shaping the evolution and development of nervous systems and their behavioral and cognitive capacities. In addition, it could provide an important design principle to guide the construction of more efficient artificial cognitive systems.</p></abstract><funding-group><funding-statement>ML was supported by research grant 04413 of the Japanese Society for the Promotion of Science. OS was partly supported by the James S. McDonnell Foundation.</funding-statement></funding-group><counts><page-count count="12"/></counts><!--===== Restructure custom-meta-wrap to custom-meta-group =====--><custom-meta-group><custom-meta><meta-name>citation</meta-name><meta-value>Lungarella M, Sporns O (2006) Mapping information flow in sensorimotor networks. PLoS Comput Biol 2(10): e144. DOI: <ext-link ext-link-type="doi" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.0020144" xlink:type="simple">10.1371/journal.pcbi.0020144</ext-link></meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><title>Introduction</title><p>All organisms with nervous systems are physically embedded (embodied) within their respective ecological niche. Their neural structures have evolved to sample and process sensory inputs to create adaptive neural representations, and to select and control motor outputs to position their bodies or to impose changes on the environment. Such sensorimotor activity involves dynamic reciprocal coupling between organism and environment, and a continuous flow of information between sensors, neural units, and effectors. The pattern of information flow defines complex sensorimotor networks, consisting of structured relations and dependencies between sensor, neural, and motor variables. Information structure, such as correlations, redundancies, and invariances in sensory and motor patterns, is critical for adaptivity, robustness, and learning, as well as for enabling action selection, perceptual categorization, and developmental processes [<xref ref-type="bibr" rid="pcbi-0020144-b001">1</xref>–<xref ref-type="bibr" rid="pcbi-0020144-b003">3</xref>]. Natural stimuli, e.g., visual scenes, contain statistical regularities that are often reflected in the response properties of sensory neurons [<xref ref-type="bibr" rid="pcbi-0020144-b004">4</xref>]. The observed match between the structure of sensory inputs and neural responses supports theoretical frameworks suggesting a biological trend towards the development and evolution of optimal neural coding [<xref ref-type="bibr" rid="pcbi-0020144-b005">5</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b006">6</xref>]. In this paper we examine the hypothesis that statistical regularities in sensory inputs and optimal coding in natural environments are not only the result of the physical properties and statistics of the environment, but can also be induced by the combined action of sensory and motor systems and by body morphology. Building on research in direct and active perception [<xref ref-type="bibr" rid="pcbi-0020144-b007">7</xref>–<xref ref-type="bibr" rid="pcbi-0020144-b009">9</xref>], and in animate, interactive, and enactive vision [<xref ref-type="bibr" rid="pcbi-0020144-b010">10</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b011">11</xref>], we adopt the notion that embodied systems actively seek information (stimuli) while engaging in behavior. We employ physical and simulated robots that serve as models of embodied organisms, sharing their embeddedness and dynamical coupling, while being significantly easier to manipulate and monitor [<xref ref-type="bibr" rid="pcbi-0020144-b012">12</xref>]. In previous work, we found that coordinated and dynamically coupled sensorimotor activity induces quantifiable changes in sensory information, including decreased entropy, increased mutual information, integration, and complexity within specific regions of sensory space [<xref ref-type="bibr" rid="pcbi-0020144-b013">13</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b014">14</xref>]. In this paper, we demonstrate the existence of networks of directed information flow between specific sensory, neural, and motor variables. These networks are dependent on the degree of sensorimotor coupling between the embodied organism and its environment, on experience-dependent plasticity and learning, and on morphological features of the body. Our adoption of a quantitative framework based on information theory allows, in principle, for an investigation of these effects across a broad range of living systems, and may provide a novel link between neural coding, behavioral dynamics, and the evolution of morphology.</p><p>In what follows, we identify a set of fundamental mechanisms (involving sensorimotor interaction and body morphology) that support and complement biological information processing carried out by nervous systems. We introduce a set of measures designed to detect information structure and directed information flow between coupled systems, such as brain, body, and environment. In particular, we opt for a “model-free” approach to data analysis, and use mutual information and transfer entropy [<xref ref-type="bibr" rid="pcbi-0020144-b015">15</xref>] to discriminate nondirected and directed components of sensorimotor coupling. We find that information structure and information flow can be mapped between a variety of sensory and motor variables recorded from three morphologically different robotic platforms (a humanoid robot, a mobile quadruped, and a mobile wheeled robot), each of which reveals a specific aspect of information flow in embodied systems (<xref ref-type="fig" rid="pcbi-0020144-g001">Figure 1</xref>). First, using the humanoid robot controlled by a saliency-based visual system, we show that the degree of sensorimotor coupling is reflected in the information structure and the strength of information flow between sensory, neural, and motor variables. Second, we illustrate how experience-dependent learning and plasticity can affect the directed transfer of information in sensorimotor networks. Specifically, changes in the neural system of the mobile quadruped robot, which depend on reward and aversiveness for particular types of objects, give rise to changing patterns of information flow between sensory, neural, and motor variables. Third, we demonstrate the effect of changes in body morphology on information flow, by varying the arrangement of photoreceptors in the simulated retina of the one-eyed mobile wheeled robot. In the final section, we make predictions and develop further hypotheses.</p><fig id="pcbi-0020144-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020144.g001</object-id><label>Figure 1</label><caption><title>Robots, Sensorimotor Interactions, and Neural Control Architecture</title><p>(A1) <italic>Roboto</italic> has a total of 14 DOF, five of which are used in the current set of experiments. Note the head-mounted CCD camera, the pan-tilt head system (2 DOF), and the moveable left arm with shoulder, elbow, and wrist joints (3 DOF). The object is a red ball (1.25 inches diameter) attached to the tip of the last joint.</p><p>(A2) <italic>Strider</italic> has a total of 14 DOF, with four legs of 3 DOF each and 2 DOF in the pan-tilt head system. Objects are red and blue blocks (1 inch cubes). <italic>Strider</italic> is situated in an environmental enclosure with black walls.</p><p>(A3) <italic>Madame</italic> has 4 DOF, with 2 DOF in the pan-tilt system and 2 DOF for the wheels, which are both located on an axis vertical to the main body axis. The environment is a square arena bounded by blue walls containing 20 red-colored floating spheres.</p><p>(B1) <italic>Roboto</italic> engages in sensorimotor interactions via the head system and arm movements; sensory → motor (dotted arrows), motor → sensory (dashed arrows).</p><p>(B2) <italic>Strider</italic> engages in sensorimotor interactions via the head system, as well as via steering signals generated by the head and transmitted to the four legs.</p><p>(B3) <italic>Madame</italic>'s behavior consists of a series of approaches to colored objects and ovations. Fixations to the objects are maintained by independent action of head and body.</p><p>(C) Neural control architecture. The architecture common to all robots is composed of color image arrays <italic>I<sub>R</sub>, I<sub>G</sub>, I<sub>B</sub>,</italic> color- intensity map<bold> <italic>Col</italic></bold><italic><sub>RGBY</sub></italic>, and saliency map <italic>Sal</italic> (see text for details). The peak of the saliency map (blue cross) determines the pan-tilt camera motion and body steering. In addition, <italic>Strider</italic>'s neural system contains a value system with taste sensory inputs relayed via a virtual taste sensor (blue square in visual image) to taste neurons (<italic>T<sub>AP,AV</sub></italic>), which in turn generates reward and aversiveness signals (rew, ave). These signals are used to modulate the strengths of the saliency factors <italic>η<sub>RGBY</sub></italic> (see text for details).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020144.g001" xlink:type="simple"/></fig></sec><sec id="s2"><title>Results</title><p>We analyze several sensory and motor variables collected from three different robotic platforms and reveal the presence of causal structure induced by dynamically coupled sensorimotor activity. Causal linkages between sensory and motor states are spatially and temporally specific, and are sensitive to changing environments and movement strategies.</p><sec id="s2a"><title>Effects of Sensorimotor Coupling on Information Structure and Flow</title><p>We evaluated the contribution of sensorimotor coupling to all informational measures by comparing two experimental conditions, one in which sensorimotor coupling was undisturbed and one in which sensorimotor coupling was disrupted—we refer to these two conditions as “fov” and “rnd,” respectively. In condition fov, all sensory, neural, and motor dynamics unfolded without intervention in real time. In condition rnd, a previously recorded motor signal was substituted, resulting in motor activity that was not driven by actual real-time sensory inputs. This enforced dissociation between sensory and motor data was designed to disrupt sensorimotor coupling, while leaving intact the statistical patterns within both sensory and motor domains. Differences in informational measures between these two conditions can be attributed to the presence or absence of coupling between sensory and motor streams. Thus, condition rnd represents an interventional or perturbational approach designed to discern patterns of information flow caused by sensorimotor coupling.</p><p><xref ref-type="fig" rid="pcbi-0020144-g002">Figure 2</xref> shows maps of entropy, mutual information, integration, and complexity for data collected from array <italic>I<sub>R</sub></italic> in <italic>Roboto</italic>. If the sensorimotor interaction was undisturbed (condition fov, <xref ref-type="fig" rid="pcbi-0020144-g002">Figure 2</xref>A), we observed increased cumulative intensity of the color red near the center of the visual field (unpublished data), as well as decreased entropy and increased mutual information, integration, and complexity. All measures exhibited significantly weaker effects if sensorimotor interaction was disrupted (condition rnd, <xref ref-type="fig" rid="pcbi-0020144-g002">Figure 2</xref>B). Very similar informational patterns were observed for neural activation states of <italic>Sal,</italic> as well as for <italic>I<sub>R</sub>, I<sub>G</sub>, I<sub>B</sub>,</italic> and <italic>Sal</italic> in <italic>Strider</italic> (unpublished data). These results were entirely consistent with those reported earlier for a different robotic pan-tilt platform tracking salient stimuli in color movies [<xref ref-type="bibr" rid="pcbi-0020144-b013">13</xref>]. While both platforms shared similar active vision control architectures, their body morphologies were significantly different, as was the nature of their visual stimulation.</p><fig id="pcbi-0020144-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020144.g002</object-id><label>Figure 2</label><caption><title>Information Structure in Sensorimotor Data Obtained from <italic>Roboto</italic></title><p>Data represents average profiles obtained from five runs per condition (1,000 time steps each). Resulting maps show (from left to right) entropy, mutual information, integration, and complexity for “fov” (A) and “rnd” (B) conditions. Gray scale ranges (at right) are [<xref ref-type="bibr" rid="pcbi-0020144-b002">2</xref> <xref ref-type="bibr" rid="pcbi-0020144-b004">4</xref>] bits (entropy), [0.35 0.95] bits (mutual information), [<xref ref-type="bibr" rid="pcbi-0020144-b023">23</xref> <xref ref-type="bibr" rid="pcbi-0020144-b048">48</xref>] bits (integration), and [0.4 0.65] bits (complexity).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020144.g002" xlink:type="simple"/></fig><p><xref ref-type="fig" rid="pcbi-0020144-g003">Figure 3</xref> summarizes results obtained from an analysis of directed information flow using transfer entropy, performed on the same datasets used for the noncausal analyses shown in <xref ref-type="fig" rid="pcbi-0020144-g002">Figure 2</xref>. The introduction of variable time offsets between sensory (S) and motor (M) time-series data allowed us to plot causal relations between these variables across all time delays (<xref ref-type="fig" rid="pcbi-0020144-g003">Figure 3</xref>A1 and <xref ref-type="fig" rid="pcbi-0020144-g003">3</xref>B1). When examining the relation between visual inputs (S, array <italic>I<sub>R</sub>,</italic> <xref ref-type="fig" rid="pcbi-0020144-g003">Figure 3</xref>A1) and the amplitude of pan-tilt head movements (M) for condition fov, we found positive transfer entropy in the direction S → M at offsets of +1 and (with decreasing magnitude) for offsets bigger than +1. No transfer entropy was found for offsets less than or equal to zero. In the reverse direction, we found transfer entropy in the direction M → S when M preceded S by at least one time step (time offset = −1) with a falloff towards more negative offsets. For condition rnd, transfer entropy was diminished if not eliminated, in accordance with the experimentally introduced disruption of causal interactions between sensory and motor time series. Residual transfer entropy in the direction M → S persisted in condition rnd, as pan-tilt head movements continued to cause displacements of the visual scene, albeit decoupled from those of the red object. The analysis also revealed the presence (for all time offsets) of elevated transfer entropy near the edges of the stimulus object (“ring-like” structures in <xref ref-type="fig" rid="pcbi-0020144-g003">Figure 3</xref>A and <xref ref-type="fig" rid="pcbi-0020144-g003">3</xref>B), indicating that discontinuities in the visual image (e.g., at object boundaries) produce state transitions that are more effective in driving changes in motor variables. <xref ref-type="fig" rid="pcbi-0020144-g003">Figure 3</xref>A2 shows plots of transfer entropy across all offsets for the center of array <italic>I<sub>R</sub></italic>. By comparing the transfer entropy at or near zero time offset to baseline values (z-score maps for transfer entropy; <xref ref-type="fig" rid="pcbi-0020144-g003">Figure 3</xref>A3), we also provided a statistical estimate of image regions that either exerted significant causal effects on motor states (S → M) or were causally affected by motor states (M → S). We found that only the surface representation of the red object caused head displacement, which in turn caused displacement of the entire visual scene (including background). <xref ref-type="fig" rid="pcbi-0020144-g003">Figure 3</xref>B utilized the activity pattern of the saliency map, a neural variable, as the sensory (S) time series. Similar patterns of causality were revealed, with peak transfer entropies that were equal to, if not greater in magnitude than, those obtained analyzing data from <italic>I<sub>R</sub></italic>.</p><fig id="pcbi-0020144-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020144.g003</object-id><label>Figure 3</label><caption><title>Information Flow (Transfer Entropy) between Sensory Input, Neural Representation of Saliency, and Motor Variables in <italic>Roboto</italic></title><p>(A1) Transfer entropy between array <italic>I<sub>R</sub></italic> (variable S) and pan-tilt amplitude (variable M). Series of plots show maps of transfer entropy from S to M (S → M) and from M to S (M → S) over visual space (55 × 77 pixels), calculated for offsets between −7 (“M leading S”) and +7 (“S leading M”) time steps. Plots show data for conditions “fov” and “rnd.” The gray scale ranges from 0.0 to 0.5 bits (for all plots in panels A1 and B1).</p><p>(A2) Curves show transfer entropy for five individual runs (thin lines) as well as the average over five runs (thick lines) between the single central pixel of array <italic>I<sub>R</sub></italic> (S) and pan-tilt amplitude (M), for directions M → S (black) and S → M (gray).</p><p>(A3) z-Score maps of significant image regions (plotted between z = 0 and z = 6). The z-scores are expressed as number of standard deviations above background at time offset +1 (S → M) and −1 (M → S). Mean and standard deviation of background is calculated from transfer entropy values at maximal time delays (−7,+7 time steps).</p><p>(B) All three panels have the same format as (A), but the neural activations of the saliency map <italic>Sal</italic> are substituted as variable S (11 × 11 neural units).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020144.g003" xlink:type="simple"/></fig><p>Peak transfer entropies depended on a variety of factors, including some key parameters in the behavioral pattern. We varied the “jump frequency” i.e., the frequency with which the object in <italic>Roboto</italic>'s hand was translated to a new randomly chosen position. This “jump” could not be predicted by the head system and acted like a large environmentally driven perturbation that elicited corrective action by the pan-tilt unit to maintain foveation. Peak transfer entropies declined as these jumps became less frequent; peak <italic>T</italic>(<italic>S → M</italic>) was 0.29, 0.28, 0.23, 0.12, 0.05 bits, for jump frequencies of 5, 10, 20, 50, and 100 time steps (n = 5 runs). No significant transfer entropy was measured if the object was not moved at all and always remained foveated. Transfer entropy was zero if no changes occurred. This result indicates that transfer entropy increases with the amount of environmental changes causing behavioral responses—even if the perception–action loop is unperturbed (as for condition fov).</p><p>Estimates of information structure (entropy, mutual information, integration, and complexity) as well as transfer entropy maps for sensory and motor variables were obtained from a second morphologically and behaviorally different robotic platform, the quadruped <italic>Strider</italic>. Distributions of information across the visual field were comparable to those obtained with <italic>Roboto</italic> (unpublished data), indicating increased levels of information structure near the visual fovea and for behaviorally salient feature maps. Summaries of spatial profiles (z-score maps) for transfer entropy obtained from <italic>Strider</italic> are displayed in <xref ref-type="fig" rid="pcbi-0020144-g004">Figure 4</xref>. While considerably more noisy than maps obtained from <italic>Roboto,</italic> profiles for <italic>T</italic>(<italic>S → M</italic>) and <italic>T</italic>(<italic>M → S</italic>) reveal similar causal relations between <italic>I<sub>R</sub>, Sal,</italic> and pan-tilt amplitude, with peaks for transfer entropy near the center of the visual field. Plots of <italic>T</italic>(<italic>S → M</italic>) and <italic>T</italic>(<italic>M → S</italic>) between sensory maps and leg movement amplitudes display peaks that are laterally displaced, reflecting the efficacy of steering input relayed from laterally located visual targets via the head system to the legs. For example, visual targets on the right side resulted in decreased movement amplitudes of the legs on the right side of the body axis, a causal connection that is retrieved via estimation of transfer entropy between the appropriate sensorimotor variables.</p><fig id="pcbi-0020144-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020144.g004</object-id><label>Figure 4</label><caption><title>Information Flow (Transfer Entropy) in <italic>Strider</italic></title><p>Data is shown as z-score maps, plotted between z = 0 and z = 6, for a variety of sensory and motor variables. The z-scores are expressed as number of standard deviations above background, calculated as for <xref ref-type="fig" rid="pcbi-0020144-g003">Figure 3</xref>.</p><p>(A) Transfer entropy between S = red intensity map <italic>I<sub>R</sub></italic> (55 × 77), and M = eye (pan-tilt) amplitude (top), M = right-leg amplitude (middle), and M = left-leg amplitude (bottom).</p><p>(B) Transfer entropy for S = downsampled saliency map <italic>Sal</italic> (11 × 11). M variables correspond to those in (A).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020144.g004" xlink:type="simple"/></fig></sec><sec id="s2b"><title>Effects of Learning on Information Flow</title><p>To illustrate the effects of learning on patterns of information flow, we included a value system as part of the neural architecture of <italic>Strider</italic> (<xref ref-type="fig" rid="pcbi-0020144-g001">Figure 1</xref>C). Based on changes in reward and aversiveness, the value system was capable of modulating saliency factors that, in turn, were used to compute the activation profile of the saliency map controlling head movements. Close encounters with objects resulted in the deployment of a virtual taste sensor. Positive changes in this sensor's activation triggered reward (appetitive taste) and aversive (aversive taste) signals (<xref ref-type="fig" rid="pcbi-0020144-g001">Figure 1</xref>C). Reward and aversive signals, in turn, modulated saliency factors such that encounters of rewarding objects tended to increase the saliency factor for that object's color, while encounters of aversive objects had the opposite effect. If the saliency distribution of objects in the environment changed over time (e.g., previously rewarding objects became aversive, and vice versa), the system adapted through changes in the saliency factors. As saliency factors changed, different objects became capable of “capturing attention” and causing approach or foveation behavior. These changes in saliency and attention could be monitored by recording behavioral and neural data. <xref ref-type="fig" rid="pcbi-0020144-g005">Figure 5</xref>A shows sample traces of average activities of the color maps and the value system collected in the course of a representative experiment lasting 8,000 time steps (approximately 13 minutes of real time). Activations of color maps increased as objects were approached and peaked around the time of object encounter. Sensing of taste triggered a value signal (either rewarding or aversive) coupled with visual inputs relaying the object's color. The comparison of two time segments, before and after a switch in color-taste contingency was made, documents a switch in behavior from approach of red objects to approach of blue objects, as well as changed reward/aversiveness profiles. <xref ref-type="fig" rid="pcbi-0020144-g005">Figure 5</xref>B displays the time course of saliency factors during the same experiment as shown in <xref ref-type="fig" rid="pcbi-0020144-g005">Figure 5</xref>A. The reversal of reward/aversiveness at time step t = 3,000 is accompanied by a reversal of saliency factors for red and blue, due to the modulatory actions of the value system.</p><fig id="pcbi-0020144-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020144.g005</object-id><label>Figure 5</label><caption><title>Changes in Behaviour, Saliency Factors, and Information Flow (Transfer Entropy) in <italic>Strider</italic></title><p>Data are from a single representative experiment, collected over 8,000 time steps. <italic>Strider</italic> navigated though its enclosure and approaching and “tasting” salient objects. Saliency (contingency) was under experimental control, and was switched at t = 3,000 from red = rewarding and blue = aversive to red = aversive and blue = rewarding.</p><p>(A) Traces of average activation levels in color-selective maps <bold><italic>Col</italic></bold><italic><sub>RGBY</sub></italic>, for red and blue (green and yellow are not shown for clarity), sampled between t = 2,001 and 3,000 (left plot) and t = 7,001 and 8,000 (right plot). Raster plot at the bottom gives corresponding rewarding (rew, top trace) and aversive (ave, bottom trace) events, with the color of the plot corresponding to the dominant color present in the center of the visual field at the time of the reward/aversive signal.</p><p>(B) Traces of saliency factors <italic>η<sub>RGBY</sub></italic> over time. Note reversal of red/blue at the time of contingency switch (t = 3,000).</p><p>(C) Transfer entropy maps for sensory variables S = <italic>I<sub>R</sub></italic> (top rows), S = <italic>I<sub>B</sub></italic> (middle rows), and S = <italic>Sal</italic> (bottom rows). M = eye (pan-tilt) amplitude throughout. Gray scale ranges from 0.1 to 0.4 bits.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020144.g005" xlink:type="simple"/></fig><p>Changes also occurred in sensorimotor information flow, i.e., transfer entropy. Transfer entropy maps obtained from a representative experiment (matching <xref ref-type="fig" rid="pcbi-0020144-g005">Figure 5</xref>A and <xref ref-type="fig" rid="pcbi-0020144-g005">5</xref>B) are shown in <xref ref-type="fig" rid="pcbi-0020144-g005">Figure 5</xref>C. Initially (from t = 1 to t = 3,000), red objects were rewarding, leading to an increase in the corresponding saliency factor <italic>η<sub>R</sub>,</italic> accompanied by a strong flow of information from red visual sensors near the fovea to the pan-tilt head system. Peak transfer entropy from sensory to motor variables was <italic>T</italic>(<italic>S → M</italic>) = 0.449 bits, while the peak value in the reverse direction was <italic>T</italic>(<italic>M → S</italic>) = 0.447 bits. As reward was switched from red to blue objects (at t = 3,000), the saliency factors adjusted, with <italic>η<sub>B</sub></italic> becoming dominant around t = 4,000. This adaptive change was reflected in a decrease of information flow emanating from red sensors and an increase of information flow from blue sensors to pan-tilt motors with a peak value of <italic>T</italic>(<italic>S → M</italic>) = 0.412 bits. Peak values for transfer entropy from the neural saliency map to motors remained high throughout the experiment, as the map continually drove foveation behavior, even while its afferent connections adapted to changes in the environment.</p></sec><sec id="s2c"><title>Effects of Morphology on Information Flow</title><p>Body shape, limb articulation, as well as the position and density of sensors on the body surface have a large impact on sensory and motor capabilities of an organism. For instance, theoretical and experimental studies demonstrate that the distribution of retinal cells (e.g., cones, rods, ganglion cells) impacts the coding and transmission of the retinal image to higher levels in the visual pathway [<xref ref-type="bibr" rid="pcbi-0020144-b016">16</xref>–<xref ref-type="bibr" rid="pcbi-0020144-b018">18</xref>]. To take a specific case, can the morphology of visual sensors affect visuo–motor information flow? We recall that the retina of most biological eyes is a variable resolution (space–variant) sensor: the mosaic formed by the photoreceptors (cones and rods) across the retinal surface is inhomogeneous, yielding a spatial resolution that varies across the visual field [<xref ref-type="bibr" rid="pcbi-0020144-b019">19</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b020">20</xref>]. In primates, the density of cones (used for high acuity vision) is typically greatest in the center (fovea) and falls off with retinal eccentricity (angular distance from the center of gaze). This morphological arrangement simultaneously enables high acuity of some parts of the visual field (achieved by appropriate head/eye movements) and a wide field of view, without requiring an enormous number of sensing elements and processing resources.</p><p>Body morphology is shaped in the course of evolution and development and is hard to manipulate systematically in either animals or physical robots. Our approach was to bypass this experimental difficulty by using a simulated mobile robot (<italic>Madame</italic>). Of all possible implementations of visual sensors, in <italic>Madame</italic> we implemented variants of retinal morphologies with a “log-polar” distribution of photoreceptors (here, only cones) [<xref ref-type="bibr" rid="pcbi-0020144-b021">21</xref>–<xref ref-type="bibr" rid="pcbi-0020144-b023">23</xref>]. The log-polar geometry has been shown to model accurately the topographical (retino–cortical) mapping of retinal cells (cones or ganglion cells) to the geniculate body and the striate cortex (area V1) [<xref ref-type="bibr" rid="pcbi-0020144-b016">16</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b024">24</xref>]. We define the mapping from the “Cartesian retina” (x,y) onto the “cortical” plane (u,v) as the following coordinate change: <italic>u</italic>(<italic>r</italic>,<italic>θ</italic>) = <italic>k</italic>log(<italic>r</italic>/<italic>a</italic> + 1), <italic>v</italic>(<italic>r</italic>, <italic>θ</italic>) = <italic>θ</italic> where <italic>k</italic> is a normalization constant, the parameter <italic>a</italic> determines the density distribution of the retinal cells, and polar coordinates (<italic>r</italic>,<italic>θ</italic>) are used to replace Cartesian ones (<italic>x</italic>,<italic>y</italic>) in the retina: <italic>r</italic> = <inline-formula id="pcbi-0020144-ex001"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020144.ex001" xlink:type="simple"/></inline-formula>
					 <italic>θ</italic> = tan<sup>−1</sup>(<italic>y</italic>/<italic>x</italic>). A possible implementation of this arrangement is shown in <xref ref-type="fig" rid="pcbi-0020144-g006">Figure 6</xref>A, where a constant number of photoreceptors (represented by crosses) is arranged so as to give rise to an increase of the cells' spacings with respect to the distance from the central point of the structure. The mapping “template” is composed of nonoverlapping ring sectors (receptive fields) formed by the intersection of rays originating at the center of the retina. The photoreceptors' activities are calculated as the average of the intensities of the photoreceptors within their receptive fields. The larger number of receptors in the foveal part of the retina leads to a visual magnification in the cortical plane (<xref ref-type="fig" rid="pcbi-0020144-g006">Figure 6</xref>B). Magnified areas correspond to receptors that are proportionally more important than others requiring more accurate information processing [<xref ref-type="bibr" rid="pcbi-0020144-b025">25</xref>]. The visual magnification factor is defined as the derivative of the mapping <italic>k</italic>/(<italic>r</italic> + <italic>a</italic>), and has a magnitude inversely proportional to distance from the center of the retina. Note that the derivative is radially symmetric, and roughly inversely proportional to the retinal eccentricity, approximating the retinotopic structure of the cat, owl monkey, rhesus monkey, and human visual cortex [<xref ref-type="bibr" rid="pcbi-0020144-b016">16</xref>].
				</p><fig id="pcbi-0020144-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020144.g006</object-id><label>Figure 6</label><caption><title>Illustration of the Retino-Cortical Mapping Process</title><p>(A) Iso-density contours of photoreceptors (red crosses) for four selected values of the parameter <italic>a</italic>. The photoreceptors are arranged as 18 concentric rings. The mapping “template” is composed of nonoverlapping 32-ring sectors (receptive fields) formed by the intersection of rays originating at the center of the retina. The value of the photoreceptor is the average of the intensities of the photoreceptors (pixels) within the receptive field's boundary.</p><p>(B) Inverse mapping of “cortical” images back to the retinal (input) domain. The cortical magnification effect is due to inhomogeneous distribution of photoreceptors. A “Cartesian” visual image (here, a checkerboard; 512 × 512 pixels) is mapped onto a “cortical” plane. Mapping is dependent on the linear parameter <italic>a</italic>. The foveal part of visual field is magnified, that is, a larger piece of cortical area is devoted to processing.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020144.g006" xlink:type="simple"/></fig><p>Here, we not only show that morphology shapes information flow but we also provide a quantitative measure for the amount of flow (the behavior of the robot was qualitatively the same throughout all experiments). <xref ref-type="fig" rid="pcbi-0020144-g007">Figure 7</xref> displays transfer entropy for values of the parameter <italic>a</italic> = 2<italic><sup>k</sup></italic>(<italic>k</italic> ∈ [−5,8]) evaluated as the average over both a region of the visual field and multiple experimental runs. The transfer entropy was calculated by averaging the transfer entropy between every pixel of a central (and noncentral) 6 × 6 pixel patch (variable S) and the difference between angular speed of left and right wheel (variable M). The error bars in <xref ref-type="fig" rid="pcbi-0020144-g007">Figure 7 </xref>denote standard deviations calculated for five runs of 4,096 samples each. Invariably, <italic>T</italic>(<italic>M → S</italic>) was larger than <italic>T</italic>(<italic>S → M</italic>) (for both central and noncentral visual patches), i.e., motor variables (e.g., difference between left and right angular speed) were more effective in driving sensor variables (e.g., red color intensity map) than vice versa. A striking result is that the information flow <italic>T</italic>(<italic>M → S</italic>) and <italic>T</italic>(<italic>S → M</italic>) for <italic>a</italic> &lt; 0.25 is larger than for <italic>a</italic> &gt; 2, with a transition between the two regions characterized by a clear inflection in the profile. This inflection is most likely a function of the size of the object on the robot's retina. Such “apparent” (or relative) size is used to regulate the distance of the robot from the object: the closer the artificial creature is to the object, the larger the object looms in front of it, and the more the creature slows down. The “causal” effect is more evident for lower values of <italic>a</italic> because the visual magnification factor is larger. Clear numerical differences can be also seen in the standard deviation. As a result of the visual magnification effect and in accord with our intuitions, the standard deviation for larger values of <italic>a</italic> is lower. Notably, the standard deviation for <italic>T</italic>(<italic>M → S</italic>) for <italic>a</italic> = {0.125,0.25,0.5,1.0} is significantly larger than for <italic>T</italic>(<italic>S → M</italic>). By contrast, for <italic>a</italic> &gt; 2 and <italic>a</italic> &lt; 0.125, the standard deviations for the two conditions are of comparable magnitude. By increasing the visual area over which the transfer entropy was calculated (up to patches of 12 × 12 central pixels), we observed no significant change in the resulting plots (unpublished data). In the case of noncentral visual patches, the graph flattens and the inflection is less pronounced; also the standard deviations are smaller. Given that the visual magnification is inversely proportional to the distance from the center, peripheral areas of the retina are less effective in driving the motor variables, and, vice versa, the effect of movements is less pronounced in the periphery.</p><fig id="pcbi-0020144-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020144.g007</object-id><label>Figure 7</label><caption><title>Effect of Retinal Morphology on Information Flow</title><p>Transfer entropy between sensor (S) and motor variables (M) as a function of parameter <italic>a</italic> in <italic>Madame</italic>. The linear parameter <italic>a</italic> determines the distribution of the photoreceptors in the retina and thus the eye morphology. Transfer entropy is calculated for every pixel of a visual region (S; 6 × 6 pixel patch) and the difference between left and right wheel speed (M; angular velocity). Squares show information flow from M to S; triangles indicate information flow from S to M. Pixels were selected from a central visual region (continuous lines) and a peripheral region (dashed lines). Data in all graphs are averages of five representative experiments consisting of 4,096 samples each, error bars show standard deviations.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020144.g007" xlink:type="simple"/></fig><p>These results indicate that eye morphology can affect information structure (here, mutual information) and information flow (here, transfer entropy), and how such effects of morphology can be quantified. Our findings are consistent with the hypothesis that morphology has an effect on information measures capturing statistical interactions and dynamic dependencies between variables.</p></sec></sec><sec id="s3"><title>Discussion</title><p>As organisms interact with their environment, their sensory inputs are transformed into motor outputs and their motor outputs determine what is sensed next. The continuous and dynamic coupling between sensory, neural, and motor variables defines sensorimotor networks that describe the informational embedding of organisms within their ecological niches at multiple time scales. The comparison of the relative influence such variables exert on each other helps extract (functional and structural) patterns of interaction between the networks' elements that may support biological information processing. In this paper we provide a quantitative framework for how to map these sensorimotor networks, which by using mutual information and transfer entropy allows capture of undirected and directed exchanges of information (information flow) between sensory, neural, and motor variables in three physically embedded (embodied) systems. Our central hypothesis is that sensorimotor interaction and morphological structure induce information structure in the sensory input and neural system, promoting information processing and flow between sensory input and motor output. We find that information flow in sensorimotor networks is (a) <italic>quantifiable and variable in magnitude;</italic> (b) <italic>temporally specific,</italic> i.e., restricted to short temporal delays between sensory and motor time series; (c) <italic>spatially specific,</italic> i.e., restricted to specific portions of the visual input capable of driving motor responses; (d) <italic>modifiable with experience,</italic> e.g., in the course of value-dependent learning of stimulus–response contingencies; and (e) <italic>dependent upon morphology,</italic> e.g., the density and distribution of visual sensors. Our results are robust with respect to the details of the sensory and motor systems employed, and hold across several different robotic platforms (stationary and mobile, simulated and real) and a range of sensory and motor variables.</p><p>Information structure created by sensorimotor interactions is evident from a variety of informational measures including basic functionals such as entropy and mutual information, which have been discussed in detail elsewhere [<xref ref-type="bibr" rid="pcbi-0020144-b013">13</xref>]. In this paper, we placed special emphasis on directed information transfer (information flow) between sensory, neural, and motor variables, and not on static correlations or undirected (shared) information. A variety of measures of directed information transfer (and “causal dependency”) are available; they rely on the use of univariate and multivariate time-series analysis and embedding techniques [<xref ref-type="bibr" rid="pcbi-0020144-b026">26</xref>–<xref ref-type="bibr" rid="pcbi-0020144-b029">29</xref>], probabilistic graphical models (e.g., Bayesian networks [<xref ref-type="bibr" rid="pcbi-0020144-b030">30</xref>]), and perturbation analysis [<xref ref-type="bibr" rid="pcbi-0020144-b031">31</xref>]. Based on the results of a comparison study [<xref ref-type="bibr" rid="pcbi-0020144-b026">26</xref>], we chose transfer entropy [<xref ref-type="bibr" rid="pcbi-0020144-b015">15</xref>] as our measure of information flow because it makes minimal assumptions about the dynamics of the time series, captures linear and nonlinear effects, and is numerically stable even for reasonably small sample sizes (1,000 samples). We stress that to infer “causal dependency” from mere time-series data is problematic, due to the often nonlinear, transient, and noisy quality of the data and due to uncertainty introduced by the potential existence of unobserved variables or hidden common sources. In general, approaches based on observational quantities alone are not able to disclose a full causal picture of the system, and interventional (or perturbational) techniques (e.g., [<xref ref-type="bibr" rid="pcbi-0020144-b031">31</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b032">32</xref>]) will ultimately be needed to provide a truly causal description of sensorimotor and neural networks. This inherent weakness of time-series−based measures does not undermine their “heuristic” usefulness in detecting directed information transfer and mapping sensorimotor networks if care is taken in the design of the experimental setup and the selection of the observables and the state space. For example, our comparison between unperturbed and perturbed experimental conditions (fov and rnd) has allowed the identification of directed relationships in sensorimotor networks caused by sensorimotor coupling. In other approaches, measures based on time-series analysis have been applied to real and simulated neural datasets [<xref ref-type="bibr" rid="pcbi-0020144-b033">33</xref>–<xref ref-type="bibr" rid="pcbi-0020144-b035">35</xref>], revealing patterns of information flow (and causal dependencies) within extended neural systems in the course of behavioral or cognitive tasks. The link between causal networks and behavior has been addressed in [<xref ref-type="bibr" rid="pcbi-0020144-b036">36</xref>], demonstrating that rich adaptive behavior displays a higher density of causal interactions in neural networks, as well as a stronger flow of information from sensory input to motor output.</p><p>Various approaches linking information structure and neural processing have been suggested on the basis of information theory considerations. These include modelling frameworks for effective information transmission [<xref ref-type="bibr" rid="pcbi-0020144-b037">37</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b038">38</xref>], efficient and sparse coding [<xref ref-type="bibr" rid="pcbi-0020144-b005">5</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b006">6</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b039">39</xref>], visual attention [<xref ref-type="bibr" rid="pcbi-0020144-b040">40</xref>], extraction of behaviorally relevant stimulus properties [<xref ref-type="bibr" rid="pcbi-0020144-b041">41</xref>], and information processing in sensorimotor systems [<xref ref-type="bibr" rid="pcbi-0020144-b042">42</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b043">43</xref>]. Each of the proposed approaches predicts specific transformations of stimulus representations along the processing hierarchy. Our work complements these studies in several ways: (1) The extension of the information theory approach to sensorimotor networks in embodied systems naturally captures the effects of motor outputs on sensory inputs, an aspect often neglected in work focusing only on information processing in neural systems. In this paper we identified ways in which sensorimotor coupling can generate additional information that may promote more efficient neural coding. (2) Techniques that map directed information flow can simultaneously be applied to sensory, motor, and neural variables. Although we focused mostly on sensory and motor variables while mapping sensorimotor networks, such networks readily extend across all hierarchical levels of neural processing. (3) The morphology of an embodied system can have significant effects on its information processing capacity. We tested the hypothesis that sensor morphology (here, the arrangement of photoreceptors in a simulated retina) influences the flow of information in a sensorimotor system.</p><p>The last point in the previous paragraph supports the notion of a quantitative link between the morphology of the retina and a computational principle of “optimal flow of information.” Given a fixed number of photosensitive elements, their space-variant arrangement maximizes the information gathered, even more so in a system engaged in a sensorimotor interaction, e.g., foveation behavior. If the photoreceptors were uniformly distributed in the retina, those in the periphery would be underutilized; also, fewer photoreceptors would be in the fovea, yielding (on average) lower spatial resolution, and resulting in less accurate estimates of object locations. Such non-uniformity at the receptor level is mirrored by non-uniformity at the cortical level in a topology-preserving fashion, that is, nearby parts of the sensory world are processed in nearby locations in the cortex. There has been some work on deriving such topology-preserving maps through the principles of uniform cortical information density [<xref ref-type="bibr" rid="pcbi-0020144-b025">25</xref>] and entropy maximization [<xref ref-type="bibr" rid="pcbi-0020144-b044">44</xref>]. We argue here that in a sensorimotor system, the rate of information transfer is maximized at the receptor stage if the probability distribution of target objects on the retina is adapted to the local photoreceptor density (a morphological property), and that this can be achieved through appropriate system–environment interaction, e.g., foveation, saccades, or adequate hand movements [<xref ref-type="bibr" rid="pcbi-0020144-b045">45</xref>]. A further implication of our findings relates to the possible role of early visual processing for the learning of causal relationships between stimuli. It has been shown, for instance, that the receptive fields of retinal ganglion cells produce efficient (predictive) coding of the average visual scene [<xref ref-type="bibr" rid="pcbi-0020144-b017">17</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b046">46</xref>]. We propose that such coding also depends on the local arrangement of the receptors and on the spatial frequencies encountered during the organism's lifetime.</p><p>In conclusion, our results highlight the fundamental importance of embodied interactions and body morphology in biological information processing, supporting a conceptual view of cognition that is based on the interplay between physical and information processes. In line with this view, most theories of embodied cognition are built around the notion that intelligent behavior and cognitive processes are the result of the continuous interaction and the reciprocal causal influence of brain, body, and environment [<xref ref-type="bibr" rid="pcbi-0020144-b047">47</xref>–<xref ref-type="bibr" rid="pcbi-0020144-b051">51</xref>]. According to these theories, it is the complex and dynamic interaction of neural processing, bodily action, and environmental forces that forms the basis of real-time adaptive response. Our work represents a step towards the development of an explicit quantitative framework that restores the unity of body and brain on the basis of their informational dependencies. Such a framework could also shed significant new light on key constraints shaping the evolution and development of nervous systems and their behavioral and cognitive capacities. In addition, a quantitative framework for information flow in embodied systems could provide an important design principle [<xref ref-type="bibr" rid="pcbi-0020144-b014">14</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b052">52</xref>] to guide the construction of more efficient artificial cognitive systems.</p></sec><sec id="s4"><title>Materials and Methods</title><sec id="s4a"><title>Robots.</title><p>We used three morphologically and behaviorally different robotic platforms, a fixed miniature humanoid named <italic>Roboto</italic> (<xref ref-type="fig" rid="pcbi-0020144-g001">Figure 1</xref>A1), a mobile quadruped named <italic>Strider</italic> (<xref ref-type="fig" rid="pcbi-0020144-g001">Figure 1</xref>A2), and a simulated mobile robot with wheels named <italic>Madame</italic> (<xref ref-type="fig" rid="pcbi-0020144-g001">Figure 1</xref>A3).</p><p><italic>Roboto.</italic> For the present experiments we used five of <italic>Roboto</italic>'s 14 kinematic degrees of freedom (DOF), three in the left arm (shoulder, elbow, and wrist), and two in the head system (pan and tilt), which was equipped with a centrally mounted CCD camera. A red object (visual target) was connected to the tip of the arm's most distal link and the arm was moved in a preprogrammed pattern. Initially the arm and object were positioned directly in front of <italic>Roboto,</italic> in view of the CCD camera. Every ten time steps, the arm was abruptly moved to a randomly chosen new position (a “jump”), selected within a range of the workspace of the individual joints, resulting in a displacement of the object relative to the head. Motor actions of the head were under visual control (see below), and displacement of the visual target resulted in foveation, with the robot tracking the position of the object as the arm was moved (<xref ref-type="fig" rid="pcbi-0020144-g001">Figure 1</xref>B1).</p><p><italic>Strider.</italic> While <italic>Roboto</italic> was mounted on a pedestal, <italic>Strider</italic> (<xref ref-type="fig" rid="pcbi-0020144-g001">Figure 1</xref>A2) was fully mobile and situated within a tub-like environmental enclosure (~1 m diameter) containing a number of stationary colored cubes. Two front-mounted infrared sensors were used for wall avoidance. Locomotion was generated by rhythmic movement of the four legs (12 DOF), using ipsilateral and contralateral phase coupling between the legs [<xref ref-type="bibr" rid="pcbi-0020144-b053">53</xref>]. The head system contained two DOF (pan, tilt) and was similar in construction and identical in terms of neural control to that of <italic>Roboto</italic> (see below). Motor actions of the head system were under visual control, resulting in foveation of colored objects. The position of the head system was relayed to the locomotion controller to steer <italic>Strider</italic> by modulating the movement amplitudes of two of the legs. This amplitude modulation resulted in gradual orientation of the body axis towards the object, while fixation was maintained by the independent action of the head system (<xref ref-type="fig" rid="pcbi-0020144-g001">Figure 1</xref>B2). The resulting behavior was a series of approaches to colored objects, each lasting for about 20 time steps, with intermittent periods of searching for new targets while navigating through the environment. All experiments were carried out with 12 red and 12 blue objects, initially positioned at random throughout the environment.</p><p><italic>Madame.</italic> The third robot was implemented in simulation. It consisted of a mobile two-wheeled platform (of length L) equipped with seven proximity sensors for obstacle avoidance and a pan-tilt camera unit. The pan and the tilt angles were constrained to vary in an angular interval of 60° relative to the robot's midline. The environment was a square arena bounded by blue walls (of length 40L) containing 20 red-colored floating spheres (of diameter 0.3L) placed in random locations. The elevation of the spheres was L and was affected by a small amount of Gaussian noise. The spheres were relocated to a new position (not too far from the previous position) every 300 time steps. Neural control of the head system was identical to that used in <italic>Roboto</italic> and <italic>Strider</italic> (see below). Similar to <italic>Strider</italic>, <italic>Madame</italic>'s behavior consisted of a series of approaches to colored objects and foveations with intermittent periods of search while moving through the environment. Fixation to the objects was maintained by independent action of head and body (<xref ref-type="fig" rid="pcbi-0020144-g001">Figure 1</xref>B3 illustrates the sensorimotor links).</p></sec><sec id="s4b"><title>Neural control architecture.</title><p>All three robots used an active vision system (<xref ref-type="fig" rid="pcbi-0020144-g001">Figure 1</xref>C) designed to direct attention—and thus processing resources—to particular locations in space according to their behavioral relevance or saliency, here encoded exclusively by color feature maps [<xref ref-type="bibr" rid="pcbi-0020144-b013">13</xref>]. The design of the color and saliency system closely followed the model of Itti et al. [<xref ref-type="bibr" rid="pcbi-0020144-b054">54</xref>]. The sampled raw visual images were first luminance-scaled according to standard formulae, and then used to compute color-opponent maps for red, green, blue, and yellow (<italic>R, G, B,</italic> and <italic>Y</italic>). Subsequently, an opponent threshold was applied, followed by a “winner-take-all” mechanism resulting in four color-intensity maps <italic>Col<sub>RGBY</sub>(R), Col<sub>RGBY</sub>(G), Col<sub>RGBY</sub>(B), and Col<sub>RGBY</sub>(Y),</italic> which recorded the pixel-wise thresholded intensity of the dominant colors <italic>R</italic>, <italic>G</italic>, <italic>B,</italic> and <italic>Y</italic>. A color-saliency map normalized to [0, 1] was created by linear summation of the individual intensity maps as <italic>Sal</italic> = <italic><underline><bold>η</bold></underline><sub>RGBY</sub> · </italic><bold><italic>Col</italic></bold><italic><sub>RGBY</sub></italic>, where <italic><bold>η</bold><sub>RGBY</sub></italic> = [<italic>η<sub>R</sub></italic>,<italic>η<sub>G</sub></italic>,<italic>η<sub>B</sub></italic>,<italic>η<sub>Y</sub></italic>] and <bold><italic>Col</italic></bold><italic><sub>RGBY</sub></italic> = [<italic>Col<sub>RGBY</sub></italic>(<italic>R</italic>),<italic>Col<sub>RGBY</sub></italic>(<italic>G</italic>),<italic>Col<sub>RGBY</sub></italic>(<italic>B</italic>),<italic>Col<sub>RGBY</sub></italic>(<italic>Y</italic>)], and by scaling to the global maximum. The saliency factors <italic>η<sub>R</sub>, η<sub>G</sub>, η<sub>B</sub>,</italic> and <italic>η<sub>Y</sub></italic> encoded the relative saliency of each of the four color components. In the experiments with <italic>Roboto</italic> and <italic>Madame</italic>, <italic>η<sub>R</sub></italic> was set to one, and all other saliency factors were set to zero, which resulted in a strong preference of the active vision system for the color red. In the experiments with <italic>Strider,</italic> saliency factors were modified dependent upon experience (see below). Once derived, the color saliency map was “block-averaged” to yield a map <italic>Sal</italic> with lower spatial resolution, whose global maximum determined the spatial location to which eye/camera movements were directed. The spatial coordinates of the maximum were then transformed into servo motor commands relayed to the pan-tilt motors moving the camera and to the servo motors driving the legs (<italic>Strider</italic>) and wheels (<italic>Madame</italic>). Camera motion resulted in lateral image shifts and a repositioning of the saliency profile. For stationary objects, camera motion stabilized quickly to direct gaze toward the maximum of the saliency map (foveation).</p><p>In the experiments with <italic>Strider,</italic> the saliency factors were subject to experience-dependent plasticity. Plastic changes were under the control of a value system [<xref ref-type="bibr" rid="pcbi-0020144-b055">55</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b056">56</xref>] capable of influencing the coupling between <bold><italic>Col</italic></bold><italic><sub>RGBY</sub></italic> and <italic>Sal</italic> in the presence of changes in innately salient sensory stimulation, analogous to biological neuromodulatory systems (e.g., [<xref ref-type="bibr" rid="pcbi-0020144-b057">57</xref>]). Innately salient sensory inputs were modelled as “virtual taste,” sampled through a small virtual tastepad attached below the camera. Whenever the camera pointed downward, indicative of close approach and foveation of a target object, the virtual tastepad was activated. Taste inputs were either appetitive (<italic>T<sub>AP</sub></italic>) or aversive (<italic>T<sub>AV</sub></italic>), depending on the color of the object. Color-taste associations were under experimental control and varied between red-appetitive/blue-aversive and red-aversive/blue-appetitive. The level of appetitive and aversive taste input was transformed into a rewarding [rew(t)] or aversive neural signal [ave(t)], respectively, according to:
					<disp-formula id="pcbi-0020144-e001"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020144.e001" xlink:type="simple"/><!-- <mml:math display='block'><mml:mtable columnalign='left'><mml:mtr><mml:mtd><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&sdot;</mml:mo><mml:mi>&Phi;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&sdot;</mml:mo><mml:mi>&Phi;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math> --></disp-formula>
					<disp-formula id="pcbi-0020144-e002"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020144.e002" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mrow><mml:mo>&lcub;</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> </mml:mrow></mml:mrow></mml:math> --></disp-formula>with Φ(·) denoting a standard sigmoidal function used to scale <italic>T<sub>AP</sub></italic> and <italic>T<sub>AV</sub></italic> to the interval [0, 1]. The saliency factors <italic>η<sub>RGBY</sub></italic> were adjusted by means of the following equation:
					<disp-formula id="pcbi-0020144-e003"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020144.e003" xlink:type="simple"/><!-- <mml:math display='block'><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:munder accentunder='true'><mml:mi>&eta;</mml:mi><mml:mo stretchy='true'>&macr;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>G</mml:mi><mml:mi>B</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msubsup><mml:mrow><mml:munder accentunder='true'><mml:mi>&eta;</mml:mi><mml:mo stretchy='true'>&macr;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>G</mml:mi><mml:mi>B</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow></mml:mrow></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>&alpha;</mml:mi><mml:mo>&sdot;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>&minus;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mn>2</mml:mn><mml:mo>&sdot;</mml:mo><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&sdot;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mrow><mml:munder accentunder='true'><mml:mi>P</mml:mi><mml:mo stretchy='true'>&macr;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>G</mml:mi><mml:mi>B</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>&minus;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mi>&delta;</mml:mi><mml:mo>&sdot;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mrow><mml:munder accentunder='true'><mml:mi>&eta;</mml:mi><mml:mo stretchy='true'>&macr;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>G</mml:mi><mml:mi>B</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mrow><mml:munder accentunder='true'><mml:mi>&eta;</mml:mi><mml:mo stretchy='true'>&macr;</mml:mo></mml:munder></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math> --></disp-formula>
				</p><p><bold><italic>P</italic></bold><italic><sub>RGBY</sub></italic> corresponded to a binary representation of the activation of the color intensity map <bold><italic>Col</italic></bold><italic><sub>RGBY</sub></italic> in the center of the visual field (e.g., a red object generated <bold><italic>P</italic></bold><italic><sub>RGBY</sub></italic> = [1 0 0 0]). The incremental learning rate <italic>α</italic> was set to 0.2, and the decay rate <italic>δ</italic> = 0.0005, with <bold><italic>η</italic></bold><italic><sub>0</sub></italic> = [0.1, 0.1, 0.1, 0.1] and |<bold><italic>η</italic></bold><italic><sub>RGBY</sub></italic> (t)| ≥ 0 at all times. At the beginning of every run, the saliency factors were initialized as <bold><italic>η</italic></bold><italic><sub>RGBY</sub></italic>(0) = [0.25,0.25,0.25,0.25]. During experience, positive changes in the taste input (i.e., the onset of rewarding or aversive sensation) generated phasic and graded reward/aversive signals that were used to increase (in the case of reward) or decrease (in the case of aversiveness) the saliency factors.</p></sec><sec id="s4c"><title>Informational measures.</title><p>We note that the meaning of the term information differs depending on the particular context. Here, information is used in the Shannon sense, that is, to quantify statistical patterns in observed variables. We applied a set of five informational measures, all of them fundamentally based on Shannon entropy [<xref ref-type="bibr" rid="pcbi-0020144-b058">58</xref>,<xref ref-type="bibr" rid="pcbi-0020144-b059">59</xref>]. Four of these measures (entropy, mutual information, integration, and complexity) capture statistical regularities between random variables without taking into account temporal precedence. These measures as well as some of the details of their computational derivation are discussed in more detail in [<xref ref-type="bibr" rid="pcbi-0020144-b013">13</xref>]. To estimate directed information flow, we used a measure developed for time-series analysis, transfer entropy [<xref ref-type="bibr" rid="pcbi-0020144-b015">15</xref>].</p><p><italic>Shannon entropy.</italic> Given a time series <italic>x<sub>t</sub></italic> that can assume N states, entropy provides a measure of the average uncertainty, or information, calculated from the state probability distribution according to:
					<disp-formula id="pcbi-0020144-e004"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020144.e004" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mo>&minus;</mml:mo><mml:mstyle displaystyle='true'><mml:msubsup><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mi>log</mml:mi><mml:mo></mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>where <italic>P<sub>X</sub></italic>(<italic>i</italic>) is the probability of <italic>x<sub>t</sub></italic> being in the i<sup>th</sup> state. Entropy is maximal if all states occur with equal probability (maximal disorder or uncertainty), while deviations from equal probability result in lowered entropy (increased order and decreased uncertainty).
				</p><p><italic>Mutual information.</italic> Mutual information is a general measure of association between two or more random variables, naturally encompassing both linear and nonlinear dependencies. The formal definition of mutual information in terms of single and joint state probability distributions is
					<disp-formula id="pcbi-0020144-e005"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020144.e005" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mo>&minus;</mml:mo><mml:mstyle displaystyle='true'><mml:msub><mml:mo>&sum;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mstyle displaystyle='true'><mml:msub><mml:mo>&sum;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mi>log</mml:mi><mml:mo></mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>If <italic>X</italic> and <italic>Y</italic> are two statistically independent random variables, <italic>P<sub>XY</sub></italic>(<italic>i</italic>, <italic>j</italic>) = <italic>P<sub>x</sub></italic>(<italic>i</italic>)<italic>P<sub>Y</sub></italic>(<italic>j</italic>) and <italic>MI</italic>(<italic>X</italic>,<italic>Y</italic>) = 0. In a sense, mutual information quantifies the error we make in assuming <italic>X</italic> and <italic>Y</italic> as independent variables, i.e., any statistical dependence between <italic>X</italic> and <italic>Y</italic> yields <italic>MI</italic>(<italic>X</italic>,<italic>Y</italic>) &gt; 0. In general, statistical dependency as measured by mutual information is insufficient to disclose directed interactions (e.g., causal relationships) between <italic>X</italic> and <italic>Y,</italic> or between <italic>Y</italic> and <italic>X,</italic> thus requiring the use of special techniques (see below).
				</p><p><italic>Integration.</italic> Integration (or multi-information; [<xref ref-type="bibr" rid="pcbi-0020144-b060">60</xref>]) is the multivariate generalization of mutual information and captures the total amount of statistical dependency among a set of random variables <italic>X<sub>i</sub></italic> forming elements of a system <bold><italic>X</italic> =</bold> {<italic>X<sub>i</sub></italic> }. Integration [<xref ref-type="bibr" rid="pcbi-0020144-b061">61</xref>] is defined as the difference between the individual entropies of the elements and their joint entropy:
					<disp-formula id="pcbi-0020144-e006"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020144.e006" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi mathvariant='bolditalic'>X</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:msub><mml:mo>&sum;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>&minus;</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>As for mutual information, if all elements <italic>X<sub>i</sub></italic> are statistically independent, <italic>I(</italic><bold><italic>X</italic></bold><italic>)</italic> = 0. Any amount of statistical dependence leads to <italic>I(</italic><bold><italic>X</italic></bold><italic>)</italic> &gt; 0.
				</p><p><italic>Complexity.</italic> If a system <bold><italic>X</italic></bold> has positive integration, i.e., some amount of statistical dependence, we may ask how such statistical dependence is distributed within the system. If the system consists of locally segregated and globally integrated components, we would expect to find statistical dependence among units at specific spatial scales. A system combining local and global structure has high complexity:
					<disp-formula id="pcbi-0020144-e007"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020144.e007" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi mathvariant="bolditalic">X</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&minus;</mml:mo><mml:mstyle displaystyle='true'><mml:msub><mml:mo>&sum;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="bolditalic">X</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>where <italic>H</italic>(<italic>X<sub>i</sub></italic>|<bold><italic>X</italic></bold> – <italic>X<sub>i</sub></italic>) is the conditional entropy of one element <italic>X<sub>i</sub></italic> given the complement <bold><italic>X</italic></bold> – <italic>X<sub>i</sub></italic> composing the rest of the system. Previous studies (e.g., [<xref ref-type="bibr" rid="pcbi-0020144-b062">62</xref>]) have shown that complexity is high for systems that effectively combine local and global order, e.g., systems that are both functionally segregated and functionally integrated. On the other hand, complexity is low for systems that are entirely random or entirely uniform.
				</p><p><italic>Transfer entropy.</italic> In addition to these noncausal informational measures, we used a measure that aims at extracting directed flow or transfer of information (also referred to as “causal dependency”) between time series, called transfer entropy [<xref ref-type="bibr" rid="pcbi-0020144-b015">15</xref>]. Given two time series <italic>x<sub>t</sub></italic> and <italic>y<sub>t</sub>,</italic> transfer entropy essentially quantifies the deviation from the generalized Markov property: <italic>p</italic>(<italic>x<sub>t</sub></italic><sub>+1</sub>|<italic>x<sub>t</sub></italic>) = <italic>p</italic>(<italic>x<sub>t</sub></italic><sub>+1</sub>|<italic>x<sub>t</sub></italic>,<italic>y<sub>t</sub></italic>), where <italic>p</italic> denotes the transition probability. If the deviation from a generalized Markov process is small, then the state of <bold><italic>Y</italic></bold> can be assumed to have no (or little) relevance on the transition probabilities of system <bold><italic>X</italic></bold>. If the deviation is large, however, then the assumption of a Markov process is not valid. The incorrectness of the assumption can be expressed by the transfer entropy, formulated as a specific version of the Kullback-Leibler entropy [<xref ref-type="bibr" rid="pcbi-0020144-b015">15</xref>]:
					<disp-formula id="pcbi-0020144-e008"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020144.e008" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>&rarr;</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mstyle displaystyle='true'><mml:munder><mml:mo>&sum;</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&plus;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mstyle displaystyle='true'><mml:munder><mml:mo>&sum;</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mstyle displaystyle='true'><mml:munder><mml:mo>&sum;</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow></mml:mrow></mml:mstyle><mml:mi>p</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&plus;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mi>log</mml:mi><mml:mo></mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&plus;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&plus;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math> --></disp-formula>where the sums are over all amplitude states, and the index <italic>T</italic>(<italic>Y → X</italic>) indicates the influence of <bold><italic>Y</italic></bold> on <bold><italic>X</italic></bold>. The transfer entropy is explicitly nonsymmetric under the exchange of <bold><italic>X</italic></bold> and <bold><italic>Y</italic></bold>—a similar expression exists for <italic>T</italic>(<italic>X → Y</italic>)—and can thus be used to detect the directed exchange of information (e.g., information flow, or causal influence) between two systems. As a special case of the conditional Kullback-Leibler entropy, transfer entropy is non-negative, any information flow between the two systems resulting in <italic>T</italic> &gt; 0. In the absence of information flow, i.e., if the state of system <bold><italic>Y</italic></bold> has no influence on the transition probabilities of system <bold><italic>X</italic></bold><italic>,</italic> or if <bold><italic>X</italic></bold> and <bold><italic>Y</italic></bold> are completely synchronized, <italic>T</italic>(<italic>Y → X</italic>) = 0 bit.
				</p><p>In summary, applied to sensory, neural, and motor datasets, entropy quantifies the average uncertainty (or self-information) about the state of individual elements, while mutual information measures the statistical dependency between two elements. Integration (or multi-information) serves as the multivariate extension of mutual information, capturing the degree to which two or more elements share information. The degree to which individual elements are specialized (representing statistical independence) while also sharing information (through global interdependence) is captured by complexity. Transfer entropy is designed to detect “directed” information exchange or coupling between two elements or parts of a system.</p></sec><sec id="s4d"><title>Data collection.</title><p>In <italic>Roboto</italic> and <italic>Strider,</italic> visual sensory data was collected at a frame rate of approximately 10 Hz by head-mounted CCD video cameras and separated into red, green, and blue components. The raw visual images were sampled at a resolution of 240 × 320 pixels, and downsampled to 55 × 77 pixels (arrays <italic>I<sub>R</sub>, I<sub>G</sub>, I<sub>B</sub></italic>). Motor data was collected as motor commands were issued to the robot's servos and saved as angular positions for each separate servo motor. Servo positions were issued and recorded at a resolution of 256 steps per approximately 100° rotation. Time series of individual motor positions were transformed into movement amplitudes and directions by temporal differencing. For <italic>Roboto</italic> and <italic>Strider,</italic> we collected data from five runs per experimental condition. For <italic>Roboto,</italic> all runs had a length of 1,000 time steps (approximately 100 s). For <italic>Strider</italic>, runs involving learning had a length of 8,000 time steps (approximately 800 s). For <italic>Madame</italic>, the raw visual images were sampled at a resolution of 80 × 80 pixels and downsampled to 40 × 40 pixels. As for both physical robots, sensor and motor data (speed of left and right wheel and angular displacement of pan and tilt motor) were collected as motor commands were issued to the agent's actuators. We collected data from five runs with a length of 4,096 simulation steps each (note that although the sampling frequency cannot be meaningfully expressed as updates/s (in Hz), it can be taken to be equivalent to the sampling frequencies of the two physical robots, 10 Hz).</p><p>On a more general note, the simulation time step (sampling period) needs to “match” the behavioral/neural time scale. Here, we chose the minimal possible sampling period, which is one time step. There is no possible sampling period below one time step (as time steps cannot be subdivided). Having designed the systems, we know that this time scale matters, because it is the time scale at which sensors sample the environment and at which motors change position.</p></sec><sec id="s4e"><title>Data analysis.</title><p>All numerical computations for data analysis were carried out in Matlab (Mathworks, <ext-link ext-link-type="uri" xlink:href="http://www.mathworks.com" xlink:type="simple">http://www.mathworks.com</ext-link>). To calculate entropy and mutual information, datasamples were discretized (16 states, 4 bits) to allow robust estimates of probability distributions. Mutual information, integration, and complexity were calculated from differenced datasets, i.e., the original time series <italic>x<sub>t</sub></italic> was replaced with its first-order temporal derivative, <italic>y<sub>t</sub></italic> = <italic>x<sub>t</sub></italic> – <italic>x<sub>t</sub></italic><sub>−1</sub>. Differenced datasets remove trends while exhibiting improved stationarity. In addition, the use of the first temporal derivative mimics the sensitivity of visual neurons to spatial and temporal changes in visual inputs, resulting in more stable representations of object properties especially in the presence of object motion. To estimate integration and complexity, we used statistical formulae that allow the calculation of entropies from the covariance matrix, under the assumption that these covariances were generated by a stationary Gaussian random process with zero mean and unit variance [<xref ref-type="bibr" rid="pcbi-0020144-b058">58</xref>]. All differenced datasamples were examined for Gaussian state distributions (by fitting state histograms) as well as stationarity (by ensuring stable means and standard deviations across time). Nonstationary datasets were excluded from analysis. To calculate transfer entropy, time series were discretized to eight states (3 bits) and joint probabilities and conditional probabilities were approximated by means of kernel density estimation. As in [<xref ref-type="bibr" rid="pcbi-0020144-b013">13</xref>], we chose a step kernel. Temporal delays across time series were introduced by shifting one time series relative to the other, thus allowing the evaluation of directed relationships across variable time offsets (or delays). Such delays could potentially be introduced due to the discrete nature of the updating of the control architecture and due to the temporal persistence of sensory and motor states. All results reported in this paper were qualitatively robust with respect to the specific choice of state space over a broad range of discretization (32 to four states).</p></sec></sec></body><back><ack><p>We thank T. Pegors and D. Bulwinkle for constructing <italic>Roboto</italic> and <italic>Strider,</italic> and Christopher Honey as well as three anonymous reviewers for valuable comments.</p></ack><glossary><title>Abbreviations</title><def-list><def-item><term>DOF</term><def><p>degrees of freedom</p></def></def-item></def-list></glossary><ref-list><title>References</title><ref id="pcbi-0020144-b001"><label>1</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Körding</surname><given-names>KP</given-names></name><name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name></person-group>
					<year>2006</year>
					<article-title>Bayesian decision theory in sensorimotor control.</article-title>
					<source>Trends Cogn Sci</source>
					<volume>10</volume>
					<fpage>319</fpage>
					<lpage>326</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b002"><label>2</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>O'Regan</surname><given-names>JK</given-names></name><name name-style="western"><surname>Noë</surname><given-names>A</given-names></name></person-group>
					<year>2001</year>
					<article-title>A sensorimotor account of vision and visual consciousness.</article-title>
					<source>Behav Brain Sci</source>
					<volume>24</volume>
					<fpage>939</fpage>
					<lpage>1031</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b003"><label>3</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Thelen</surname><given-names>E</given-names></name><name name-style="western"><surname>Smith</surname><given-names>L</given-names></name></person-group>
					<year>1994</year>
					<source>A dynamic systems approach to the development of cognition and action</source>
					<publisher-loc>Cambridge, MA</publisher-loc>
					<publisher-name>MIT Press/Bradford</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">408</size>
				</element-citation></ref><ref id="pcbi-0020144-b004"><label>4</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Simoncelli</surname><given-names>E</given-names></name><name name-style="western"><surname>Olshausen</surname><given-names>B</given-names></name></person-group>
					<year>2001</year>
					<article-title>Natural image statistics and neural representation.</article-title>
					<source>Ann Rev Neurosci</source>
					<volume>24</volume>
					<fpage>1193</fpage>
					<lpage>1216</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b005"><label>5</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name></person-group>
					<year>2001</year>
					<article-title>Redundancy reduction revisited.</article-title>
					<source>Network: Comput Neural Sys</source>
					<volume>12</volume>
					<fpage>241</fpage>
					<lpage>253</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b006"><label>6</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name><name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name></person-group>
					<year>2004</year>
					<article-title>Sparse coding of sensory inputs.</article-title>
					<source>Curr Op Neurobiol</source>
					<volume>14</volume>
					<fpage>481</fpage>
					<lpage>487</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b007"><label>7</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bajcsy</surname><given-names>R</given-names></name></person-group>
					<year>1988</year>
					<article-title>Active perception.</article-title>
					<source>Proc IEEE</source>
					<volume>76</volume>
					<fpage>996</fpage>
					<lpage>1005</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b008"><label>8</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gibson</surname><given-names>JJ</given-names></name></person-group>
					<year>1979</year>
					<source>The ecological approach to visual perception</source>
					<publisher-loc>Boston</publisher-loc>
					<publisher-name>Houghton Mifflin, 311 p</publisher-name>
				</element-citation></ref><ref id="pcbi-0020144-b009"><label>9</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Noë</surname><given-names>A</given-names></name></person-group>
					<year>2004</year>
					<source>Action in perception</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">392</size>
				</element-citation></ref><ref id="pcbi-0020144-b010"><label>10</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Ballard</surname><given-names>D</given-names></name></person-group>
					<year>1991</year>
					<article-title>Animate vision.</article-title>
					<source>Artificial Intell</source>
					<volume>48</volume>
					<fpage>57</fpage>
					<lpage>86</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b011"><label>11</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Churchland</surname><given-names>PS</given-names></name><name name-style="western"><surname>Ramachandran</surname><given-names>V</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name></person-group>
					<year>1994</year>
					<article-title>A critique of pure vision.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name><name name-style="western"><surname>Davis</surname><given-names>J</given-names></name></person-group>
					<source>Large-scale neuronal theories of the brain</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<comment>355 p.</comment>
				</element-citation></ref><ref id="pcbi-0020144-b012"><label>12</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Webb</surname><given-names>B</given-names></name></person-group>
					<year>2001</year>
					<article-title>Can robots make good models of biological behaviour?</article-title>
					<source>Behav Brain Sci</source>
					<volume>24</volume>
					<fpage>1033</fpage>
					<lpage>1050</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b013"><label>13</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Lungarella</surname><given-names>M</given-names></name><name name-style="western"><surname>Pegors</surname><given-names>T</given-names></name><name name-style="western"><surname>Bulwinkle</surname><given-names>D</given-names></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name></person-group>
					<year>2005</year>
					<article-title>Methods for quantifying the information structure of sensory and motor data.</article-title>
					<source>Neuroinformatics</source>
					<volume>3</volume>
					<fpage>243</fpage>
					<lpage>262</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b014"><label>14</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Lungarella</surname><given-names>M</given-names></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name></person-group>
					<year>2005</year>
					<article-title>Information self-structuring: Key principle for learning and development.</article-title>
					<source>Proceedings of the 4th IEEE International Conference on Development and Learning</source>
					<conf-date>July 2005;</conf-date>
					<conf-loc>Osaka, Japan.</conf-loc>
					<fpage>25</fpage>
					<lpage>30</lpage>
					<comment>Available: <ext-link ext-link-type="uri" xlink:href="http://www.indiana.edu/~cortex/ICDL05_paper.pdf" xlink:type="simple">http://www.indiana.edu/~cortex/ICDL05_paper.pdf</ext-link>. Accessed 26 September 2006.</comment>
				</element-citation></ref><ref id="pcbi-0020144-b015"><label>15</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Schreiber</surname><given-names>T</given-names></name></person-group>
					<year>2000</year>
					<article-title>Measuring information transfer.</article-title>
					<source>Phys Rev Lett</source>
					<volume>85</volume>
					<fpage>461</fpage>
					<lpage>464</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b016"><label>16</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Schwartz</surname><given-names>EL</given-names></name></person-group>
					<year>1980</year>
					<article-title>Computational anatomy and functional architecture of striate cortex: A spatial mapping approach to perceptual coding.</article-title>
					<source>Vision Res</source>
					<volume>20</volume>
					<fpage>645</fpage>
					<lpage>669</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b017"><label>17</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Srinivasan</surname><given-names>MV</given-names></name><name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name><name name-style="western"><surname>Dubs</surname><given-names>A</given-names></name></person-group>
					<year>1982</year>
					<article-title>Predictive coding: A fresh view of inhibition in the retina.</article-title>
					<source>Proc R Soc Lond B</source>
					<volume>216</volume>
					<fpage>427</fpage>
					<lpage>459</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b018"><label>18</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>van Essen</surname><given-names>DC</given-names></name><name name-style="western"><surname>Anderson</surname><given-names>CH</given-names></name></person-group>
					<year>1994</year>
					<article-title>Information processing strategies and pathways in the primate visual system.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Zornetzer</surname><given-names>SF</given-names></name><name name-style="western"><surname>Davis</surname><given-names>JL</given-names></name><name name-style="western"><surname>Lau</surname><given-names>C</given-names></name><name name-style="western"><surname>McKenna</surname><given-names>T</given-names></name></person-group>
					<source>An introduction to neural and electronic networks</source>
					<publisher-loc>Orlando (Florida)</publisher-loc>
					<publisher-name>Academic Press</publisher-name>
					<fpage>45</fpage>
					<lpage>76</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b019"><label>19</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Curcio</surname><given-names>AC</given-names></name><name name-style="western"><surname>Kenneth</surname><given-names>RS</given-names></name><name name-style="western"><surname>Robert</surname><given-names>EK</given-names></name><name name-style="western"><surname>Hendrickson</surname><given-names>AE</given-names></name></person-group>
					<year>1990</year>
					<article-title>Human receptor topography.</article-title>
					<source>J Comp Neurol</source>
					<volume>292</volume>
					<fpage>497</fpage>
					<lpage>523</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b020"><label>20</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Land</surname><given-names>MF</given-names></name><name name-style="western"><surname>Fernald</surname><given-names>RD</given-names></name></person-group>
					<year>1992</year>
					<article-title>The evolution of eyes.</article-title>
					<source>Ann Rev Neurosci</source>
					<volume>15</volume>
					<fpage>1</fpage>
					<lpage>29</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b021"><label>21</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bolduc</surname><given-names>M</given-names></name><name name-style="western"><surname>Levine</surname><given-names>MD</given-names></name></person-group>
					<year>1998</year>
					<article-title>A review of biologically motivated space-variant data reduction models for robotic vision.</article-title>
					<source>Comput Vis Image Underst</source>
					<volume>69</volume>
					<fpage>170</fpage>
					<lpage>184</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b022"><label>22</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sandini</surname><given-names>G</given-names></name><name name-style="western"><surname>Metta</surname><given-names>G</given-names></name></person-group>
					<year>2002</year>
					<article-title>Retina-like sensors: Motivations, technology and applications.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Secomb</surname><given-names>TW</given-names></name><name name-style="western"><surname>Barth</surname><given-names>F</given-names></name><name name-style="western"><surname>Humphrey</surname><given-names>P</given-names></name></person-group>
					<source>Sensors and sensing in biology and engineering</source>
					<publisher-loc>Berlin</publisher-loc>
					<publisher-name>Springer-Verlag</publisher-name>
					<fpage>379</fpage>
					<lpage>392</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b023"><label>23</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wallace</surname><given-names>RS</given-names></name><name name-style="western"><surname>Ong</surname><given-names>PW</given-names></name><name name-style="western"><surname>Bederson</surname><given-names>BB</given-names></name><name name-style="western"><surname>Schwartz</surname><given-names>EL</given-names></name></person-group>
					<year>1994</year>
					<article-title>Space variant image processing.</article-title>
					<source>Int J Comput Vis</source>
					<volume>13</volume>
					<fpage>71</fpage>
					<lpage>90</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b024"><label>24</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Schwartz</surname><given-names>EL</given-names></name></person-group>
					<year>1977</year>
					<article-title>Spatial mapping in the primate sensory projection: Analytic structure and relevance to perception.</article-title>
					<source>Biol Cybern</source>
					<volume>25</volume>
					<fpage>181</fpage>
					<lpage>194</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b025"><label>25</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Plumbley</surname><given-names>MD</given-names></name></person-group>
					<year>1999</year>
					<article-title>Do cortical maps adapt to optimize information density?</article-title>
					<source>Network: Comput Neural Sys</source>
					<volume>10</volume>
					<fpage>41</fpage>
					<lpage>58</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b026"><label>26</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Lungarella</surname><given-names>M</given-names></name><name name-style="western"><surname>Ishiguro</surname><given-names>K</given-names></name><name name-style="western"><surname>Kuniyoshi</surname><given-names>Y</given-names></name><name name-style="western"><surname>Otsu</surname><given-names>N</given-names></name></person-group>
					<year>2007</year>
					<article-title>Methods for quantifying the causal structure of bivariate time series.</article-title>
					<source>Int J Bifurcation Chaos</source>
					<comment>In press.</comment>
				</element-citation></ref><ref id="pcbi-0020144-b027"><label>27</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Pereda</surname><given-names>E</given-names></name><name name-style="western"><surname>Quian Quiroga</surname><given-names>R</given-names></name><name name-style="western"><surname>Bhattacharya</surname><given-names>J</given-names></name></person-group>
					<year>2005</year>
					<article-title>Nonlinear multivariate analysis of neurophysiological signals.</article-title>
					<source>Progress Neurobiol</source>
					<volume>77</volume>
					<fpage>1</fpage>
					<lpage>37</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b028"><label>28</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Vastano</surname><given-names>JA</given-names></name><name name-style="western"><surname>Swinney</surname><given-names>HL</given-names></name></person-group>
					<year>1988</year>
					<article-title>Information transport in spatiotemporal systems.</article-title>
					<source>Phys Rev Lett</source>
					<volume>60</volume>
					<fpage>1173</fpage>
					<lpage>1176</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b029"><label>29</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Verdes</surname><given-names>PF</given-names></name></person-group>
					<year>2005</year>
					<article-title>Assessing causality from multivariate time series.</article-title>
					<source>Phys Rev E</source>
					<volume>72</volume>
					<fpage>026222</fpage>
				</element-citation></ref><ref id="pcbi-0020144-b030"><label>30</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Pearl</surname><given-names>J</given-names></name></person-group>
					<year>2000</year>
					<source>Causality: Models, reasoning, and inference</source>
					<publisher-loc>Cambridge (United Kingdom)</publisher-loc>
					<publisher-name>Cambridge University Press</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">384</size>
				</element-citation></ref><ref id="pcbi-0020144-b031"><label>31</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name></person-group>
					<year>2003</year>
					<article-title>Measuring information integration.</article-title>
					<source>BMC Neurosci</source>
					<volume>4</volume>
					<fpage>31</fpage>
				</element-citation></ref><ref id="pcbi-0020144-b032"><label>32</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Ay</surname><given-names>N</given-names></name><name name-style="western"><surname>Polani</surname><given-names>D</given-names></name></person-group>
					<year>2007</year>
					<article-title>Information flows in causal networks.</article-title>
					<source>Adv Compl Syst. In press. Santa Fe Institute Working Paper 06-05-014</source>
					<comment>Available: <ext-link ext-link-type="uri" xlink:href="http://www.santafe.edu/research/publications/workingpapers/06-05-014.pdf" xlink:type="simple">http://www.santafe.edu/research/publications/workingpapers/06-05-014.pdf</ext-link>. Accessed 26 September 2006.</comment>
				</element-citation></ref><ref id="pcbi-0020144-b033"><label>33</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bhattacharya</surname><given-names>J</given-names></name><name name-style="western"><surname>Petsche</surname><given-names>H</given-names></name><name name-style="western"><surname>Pereda</surname><given-names>E</given-names></name></person-group>
					<year>2001</year>
					<article-title>Interdependencies in the spontaneous EEG while listening to music.</article-title>
					<source>Int J Psychophysiol</source>
					<volume>42</volume>
					<fpage>287</fpage>
					<lpage>301</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b034"><label>34</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Brovelli</surname><given-names>A</given-names></name><name name-style="western"><surname>Ding</surname><given-names>M</given-names></name><name name-style="western"><surname>Ledberg</surname><given-names>A</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y</given-names></name><name name-style="western"><surname>Nakamura</surname><given-names>R</given-names></name><etal/></person-group>
					<year>2004</year>
					<article-title>Beta oscillations in a large-scale sensorimotor cortical network: Directional influences revealed by Granger causality.</article-title>
					<source>Proc Natl Acad Sci U S A</source>
					<volume>101</volume>
					<fpage>9849</fpage>
					<lpage>9854</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b035"><label>35</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Salazar</surname><given-names>RF</given-names></name><name name-style="western"><surname>König</surname><given-names>P</given-names></name><name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name></person-group>
					<year>2004</year>
					<article-title>Directed interactions between visual areas and their role in processing image structure and expectancy.</article-title>
					<source>Eur J Neurosci</source>
					<volume>20</volume>
					<fpage>1391</fpage>
					<lpage>1401</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b036"><label>36</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name></person-group>
					<year>2005</year>
					<article-title>Causal connectivity analysis of evolved neural networks during behavior.</article-title>
					<source>Network: Comput Neural Sys</source>
					<volume>16</volume>
					<fpage>35</fpage>
					<lpage>54</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b037"><label>37</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Becker</surname><given-names>S</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name></person-group>
					<year>1992</year>
					<article-title>Self-organizing neural network that discovers surfaces in random-dot stereograms.</article-title>
					<source>Nature</source>
					<volume>355</volume>
					<fpage>161</fpage>
					<lpage>163</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b038"><label>38</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Linsker</surname><given-names>R</given-names></name></person-group>
					<year>1988</year>
					<article-title>Self-organization in a perceptual network.</article-title>
					<source>IEEE Computer</source>
					<volume>21</volume>
					<fpage>105</fpage>
					<lpage>117</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b039"><label>39</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name></person-group>
					<year>1961</year>
					<article-title>Possible principles underlying the transformation of sensory messages.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Rosenblith</surname><given-names>IW</given-names></name></person-group>
					<source>Sensory communication</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<fpage>217</fpage>
					<lpage>234</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b040"><label>40</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Anderson</surname><given-names>CH</given-names></name><name name-style="western"><surname>van Essen</surname><given-names>DC</given-names></name><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name></person-group>
					<year>2005</year>
					<article-title>Directed visual attention and the dynamic control of information flow.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Itti</surname><given-names>L</given-names></name><name name-style="western"><surname>Rees</surname><given-names>G</given-names></name><name name-style="western"><surname>Tsotsos</surname><given-names>J</given-names></name></person-group>
					<source>Neurobiology of attention</source>
					<publisher-loc>San Diego (California)</publisher-loc>
					<publisher-name>Elsevier</publisher-name>
					<fpage>11</fpage>
					<lpage>17</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b041"><label>41</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Rieke</surname><given-names>F</given-names></name><name name-style="western"><surname>Bodnar</surname><given-names>DA</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>
					<year>1995</year>
					<article-title>Naturalistic stimuli increase the rate and efficiency of information transmission by primary auditory afferents.</article-title>
					<source>Proc R Soc Lond B</source>
					<volume>262</volume>
					<fpage>259</fpage>
					<lpage>265</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b042"><label>42</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Klyubin</surname><given-names>AS</given-names></name><name name-style="western"><surname>Polani</surname><given-names>D</given-names></name><name name-style="western"><surname>Nehaniv</surname><given-names>C</given-names></name></person-group>
					<year>2005</year>
					<article-title>Empowerment: A universal agent-centric measure of control.</article-title>
					<source>Proceedings of the IEEE Congress on Evolutionary Computation</source>
					<conf-date>2–5 September 2005;</conf-date>
					<conf-loc>Edinburgh, Scotland.</conf-loc>
					<comment>Volume 1. pp.</comment>
					<fpage>128</fpage>
					<lpage>135</lpage>
					<comment>ISBN: 0-7803-9363–5.</comment>
				</element-citation></ref><ref id="pcbi-0020144-b043"><label>43</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Porr</surname><given-names>B</given-names></name><name name-style="western"><surname>Egerton</surname><given-names>A</given-names></name><name name-style="western"><surname>Wörgotter</surname><given-names>F</given-names></name></person-group>
					<year>2006</year>
					<article-title>Towards closed loop information: Predictive information.</article-title>
					<source>Constructivist Foundations</source>
					<volume>1</volume>
					<fpage>83</fpage>
					<lpage>90</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b044"><label>44</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>van Hulle</surname><given-names>MM</given-names></name></person-group>
					<year>1997</year>
					<article-title>Topology-preserving map formation achieved by a purely local unsupervised competitive learning rule.</article-title>
					<source>Neural Netw</source>
					<volume>10</volume>
					<fpage>431</fpage>
					<lpage>446</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b045"><label>45</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Lewis</surname><given-names>A</given-names></name><name name-style="western"><surname>Garcia</surname><given-names>R</given-names></name><name name-style="western"><surname>Zhaoping</surname><given-names>L</given-names></name></person-group>
					<year>2003</year>
					<article-title>The distribution of visual objects on the retina: Connecting eye movements and cone distributions.</article-title>
					<source>J Vision</source>
					<volume>3</volume>
					<fpage>893</fpage>
					<lpage>905</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b046"><label>46</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hosoya</surname><given-names>T</given-names></name><name name-style="western"><surname>Baccus</surname><given-names>SA</given-names></name><name name-style="western"><surname>Meister</surname><given-names>M</given-names></name></person-group>
					<year>2005</year>
					<article-title>Dynamic predictive coding by the retina.</article-title>
					<source>Nature</source>
					<volume>436</volume>
					<fpage>71</fpage>
					<lpage>77</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b047"><label>47</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Chiel</surname><given-names>HJ</given-names></name><name name-style="western"><surname>Beer</surname><given-names>RD</given-names></name></person-group>
					<year>1997</year>
					<article-title>The brain has a body: Adaptive behaviour emerges from interactions of nervous system, body, and environment.</article-title>
					<source>Trends Neurosci</source>
					<volume>20</volume>
					<fpage>553</fpage>
					<lpage>557</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b048"><label>48</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Clark</surname><given-names>A</given-names></name></person-group>
					<year>1999</year>
					<article-title>An embodied cognitive science?</article-title>
					<source>Trends Cogn Sci</source>
					<volume>3</volume>
					<fpage>345</fpage>
					<lpage>351</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b049"><label>49</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="editor"><name name-style="western"><surname>Iida</surname><given-names>F</given-names></name><name name-style="western"><surname>Pfeifer</surname><given-names>R</given-names></name><name name-style="western"><surname>Steels</surname><given-names>L</given-names></name><name name-style="western"><surname>Kuniyoshi</surname><given-names>Y</given-names></name></person-group>
					<year>2004</year>
					<source>Embodied artificial intelligence</source>
					<publisher-loc>Berlin/Heidelberg</publisher-loc>
					<publisher-name>Springer-Verlag</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">331</size>
				</element-citation></ref><ref id="pcbi-0020144-b050"><label>50</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Pfeifer</surname><given-names>R</given-names></name><name name-style="western"><surname>Bongard</surname><given-names>JC</given-names></name></person-group>
					<year>2006</year>
					<source>How the body shapes the way we think—A new view of intelligence</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<comment>In press.</comment>
				</element-citation></ref><ref id="pcbi-0020144-b051"><label>51</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name></person-group>
					<year>2003</year>
					<article-title>Embodied cognition.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Arbib</surname><given-names>M</given-names></name></person-group>
					<source>Handbook of brain theory and neural networks</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<fpage>395</fpage>
					<lpage>398</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b052"><label>52</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name><name name-style="western"><surname>Lungarella</surname><given-names>M</given-names></name></person-group>
					<year>2006</year>
					<article-title>Evolving coordinated behavior by maximizing information structure.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Rocha</surname><given-names>L</given-names></name><name name-style="western"><surname>Yaeger</surname><given-names>LS</given-names></name><name name-style="western"><surname>Bedau</surname><given-names>M</given-names></name><name name-style="western"><surname>Floreano</surname><given-names>D</given-names></name><name name-style="western"><surname>Goldstone</surname><given-names>RL</given-names></name><etal/></person-group>
					<source>Proceedings of Artificial Life X</source>
					<conf-date>3–7 June 2006;</conf-date>
					<conf-loc>Bloomington, Indiana, United States. </conf-loc>
					<conf-name>Tenth International Congress on Simulation and Synthesis of Living Systems.</conf-name>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<fpage>323</fpage>
					<lpage>329</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b053"><label>53</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Delcomyn</surname><given-names>F</given-names></name></person-group>
					<year>2004</year>
					<article-title>Insect walking and robotics.</article-title>
					<source>Annu Rev Entomol</source>
					<volume>49</volume>
					<fpage>51</fpage>
					<lpage>70</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b054"><label>54</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Itti</surname><given-names>L</given-names></name><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name><name name-style="western"><surname>Niebur</surname><given-names>E</given-names></name></person-group>
					<year>1998</year>
					<article-title>A model of saliency-based visual attention for rapid scene analysis.</article-title>
					<source>Trans Patt Anal Mach Intell</source>
					<volume>20</volume>
					<fpage>1254</fpage>
					<lpage>1259</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b055"><label>55</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name><name name-style="western"><surname>Reeke</surname><given-names>GN</given-names><suffix>Jr</suffix></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name><name name-style="western"><surname>Edelman</surname><given-names>GM</given-names></name></person-group>
					<year>1994</year>
					<article-title>Value-dependent selection in the brain: Simulation in a synthetic neural model.</article-title>
					<source>J Neurosci</source>
					<volume>59</volume>
					<fpage>229</fpage>
					<lpage>243</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b056"><label>56</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name><name name-style="western"><surname>Alexander</surname><given-names>WH</given-names></name></person-group>
					<year>2002</year>
					<article-title>Neuromodulation and plasticity in an autonomous robot.</article-title>
					<source>Neural Netw</source>
					<volume>15</volume>
					<fpage>761</fpage>
					<lpage>774</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b057"><label>57</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name></person-group>
					<year>2002</year>
					<article-title>Metalearning and neuromodulation.</article-title>
					<source>Neural Netw</source>
					<volume>15</volume>
					<fpage>495</fpage>
					<lpage>506</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b058"><label>58</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Cover</surname><given-names>TM</given-names></name><name name-style="western"><surname>Thomas</surname><given-names>JA</given-names></name></person-group>
					<year>1991</year>
					<source>Elements of information theory</source>
					<publisher-loc>New York</publisher-loc>
					<publisher-name>Wiley</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">542</size>
				</element-citation></ref><ref id="pcbi-0020144-b059"><label>59</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Shannon</surname><given-names>C</given-names></name></person-group>
					<year>1948</year>
					<article-title>A mathematical theory of communication.</article-title>
					<source>Bell Sys Tech J</source>
					<volume>27</volume>
					<fpage>379</fpage>
					<lpage>423</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b060"><label>60</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>McGill</surname><given-names>WJ</given-names></name></person-group>
					<year>1954</year>
					<article-title>Multivariate information transmission.</article-title>
					<source>IEEE Trans Inform Theory</source>
					<volume>4</volume>
					<fpage>93</fpage>
					<lpage>111</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b061"><label>61</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name><name name-style="western"><surname>Edelman</surname><given-names>GM</given-names></name></person-group>
					<year>1994</year>
					<article-title>A measure for brain complexity: Relating functional segregation and integration in the nervous system.</article-title>
					<source>Proc Natl Acad Sci U S A</source>
					<volume>91</volume>
					<fpage>5033</fpage>
					<lpage>5037</lpage>
				</element-citation></ref><ref id="pcbi-0020144-b062"><label>62</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name><name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name><name name-style="western"><surname>Edelman</surname><given-names>GM</given-names></name></person-group>
					<year>2000</year>
					<article-title>Theoretical neuroanatomy: Relating anatomical and functional connectivity in graphs and cortical connection matrices.</article-title>
					<source>Cereb Cortex</source>
					<volume>10</volume>
					<fpage>127</fpage>
					<lpage>141</lpage>
				</element-citation></ref></ref-list></back></article>