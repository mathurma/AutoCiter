<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
   <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
         <publisher-name>Public Library of Science</publisher-name>
         <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
   <article-meta><article-id pub-id-type="publisher-id">10-PLCB-RA-2700R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1001111</article-id><article-categories>
         <subj-group subj-group-type="heading">
            <subject>Research Article</subject>
         </subj-group>
         <subj-group subj-group-type="Discipline">
<subject>Computational Biology</subject>
<subject>Computational Biology/Computational Neuroscience</subject>
<subject>Neuroscience/Sensory Systems</subject>
<subject>Neuroscience/Theoretical Neuroscience</subject>
<subject>Physics</subject>

         </subj-group>
      </article-categories><title-group><article-title>Minimal Models of Multidimensional Computations</article-title><alt-title alt-title-type="running-head">Maximum Noise Entropy</alt-title></title-group><contrib-group>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Fitzgerald</surname><given-names>Jeffrey D.</given-names></name>
            <xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
            <xref ref-type="aff" rid="aff2"><sup>2</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Sincich</surname><given-names>Lawrence C.</given-names></name>
            <xref ref-type="aff" rid="aff3"><sup>3</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Sharpee</surname><given-names>Tatyana O.</given-names></name>
            <xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
            <xref ref-type="aff" rid="aff2"><sup>2</sup></xref>
            <xref ref-type="corresp" rid="cor1"><sup>*</sup></xref>
         </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Computational Neurobiology Laboratory, The Salk Institute for Biological Studies, La Jolla, California, United States of America</addr-line>
      </aff><aff id="aff2"><label>2</label><addr-line>Center for Theoretical Biological Physics and Department of Physics, University of California, San Diego, La Jolla, California, United States of America</addr-line>
      </aff><aff id="aff3"><label>3</label><addr-line>Beckman Vision Center, University of California, San Francisco, San Francisco, California, United States of America</addr-line>
      </aff><contrib-group>
         <contrib contrib-type="editor" xlink:type="simple">
            <name name-style="western"><surname>Friston</surname><given-names>Karl J.</given-names></name>
            <role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib>
      </contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes>
         <corresp id="cor1">* E-mail: <email xlink:type="simple">sharpee@salk.edu</email></corresp>
         <fn fn-type="con">
            <p>Conceived and designed the experiments: JDF LCS TOS. Performed the experiments: LCS. Analyzed the data: JDF TOS. Wrote the paper: JDF LCS TOS.</p>
         </fn>
      <fn fn-type="conflict">
         <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
         <month>3</month>
         <year>2011</year>
      </pub-date><pub-date pub-type="epub">
         <day>24</day>
         <month>3</month>
         <year>2011</year>
      </pub-date><volume>7</volume><issue>3</issue><elocation-id>e1001111</elocation-id><history>
         <date date-type="received">
            <day>12</day>
            <month>8</month>
            <year>2010</year>
         </date>
         <date date-type="accepted">
            <day>17</day>
            <month>2</month>
            <year>2011</year>
         </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Fitzgerald et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
         <p>The multidimensional computations performed by many biological systems are often characterized with limited information about the correlations between inputs and outputs. Given this limitation, our approach is to construct the maximum noise entropy response function of the system, leading to a closed-form and minimally biased model consistent with a given set of constraints on the input/output moments; the result is equivalent to conditional random field models from machine learning. For systems with binary outputs, such as neurons encoding sensory stimuli, the maximum noise entropy models are logistic functions whose arguments depend on the constraints. A constraint on the average output turns the binary maximum noise entropy models into minimum mutual information models, allowing for the calculation of the information content of the constraints and an information theoretic characterization of the system's computations. We use this approach to analyze the nonlinear input/output functions in macaque retina and thalamus; although these systems have been previously shown to be responsive to two input dimensions, the functional form of the response function in this reduced space had not been unambiguously identified. A second order model based on the logistic function is found to be both necessary and sufficient to accurately describe the neural responses to naturalistic stimuli, accounting for an average of 93% of the mutual information with a small number of parameters. Thus, despite the fact that the stimulus is highly non-Gaussian, the vast majority of the information in the neural responses is related to first and second order correlations. Our results suggest a principled and unbiased way to model multidimensional computations and determine the statistics of the inputs that are being encoded in the outputs.</p>
      </abstract><abstract abstract-type="summary">
         <title>Author Summary</title>

         <p>Biological systems across many scales, from molecules to ecosystems, can all be considered information processors, detecting important events in their environment and transforming them into actions. Detecting events of interest in the presence of noise and other overlapping events often necessitates the use of nonlinear transformations of inputs. The nonlinear nature of the relationships between inputs and outputs makes it difficult to characterize them experimentally given the limitations imposed by data collection. Here we discuss how minimal models of the nonlinear input/output relationships of information processing systems can be constructed by maximizing a quantity called the noise entropy. The proposed approach can be used to “focus” the available data by determining which input/output correlations are important and creating the least-biased model consistent with those correlations. We hope that this method will aid the exploration of the computations carried out by complex biological systems and expand our understanding of basic phenomena in the biological world.</p>
      </abstract><funding-group><funding-statement>This work was funded by NIH Grant EY019493; NSF Grants IIS-0712852 and PHY-0822283 and the Searle Funds; the Alfred P. Sloan Fellowship; the McKnight Scholarship; W.M. Keck Research Excellence Award; and the Ray Thomas Edwards Career Development Award in Biomedical Sciences. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
<page-count count="8"/>
</counts></article-meta>
</front>
<body>
   <sec id="s1">
      <title>Introduction</title>

      <p>There is an emerging view that the primary function of many biological systems, from the molecular level to ecosystems, is to process information <xref ref-type="bibr" rid="pcbi.1001111-Haken1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1001111-Sanfey1">[4]</xref>. The nature of the computations these systems perform can be quite complex <xref ref-type="bibr" rid="pcbi.1001111-Grenfell1">[5]</xref>, often due to large numbers of components interacting over wide spatial and temporal scales, and to the amount of data necessary to fully characterize those interactions. Constructing a model of the system using limited knowledge of the correlations between inputs and outputs can impose implicit assumptions and biases leading to a mischaracterization of the computations. To minimize this type of bias, we maximize the noise entropy of the system subject to constraints on the input/output moments, resulting in the response function that agrees with our limited knowledge and is maximally uncommitted toward everything else. An equivalent approach in machine learning is known as conditional random fields <xref ref-type="bibr" rid="pcbi.1001111-Lafferty1">[6]</xref>. We apply this idea to study neural coding, showing that logistic functions not only maximize the noise entropy for binary outputs, but are also special closed-form cases of the minimum mutual information (MinMI) solutions <xref ref-type="bibr" rid="pcbi.1001111-Globerson1">[7]</xref> when the average firing rate of a neuron is fixed. Recently, MinMI was used to assess the information content in constraints on the interactions between neurons in a network <xref ref-type="bibr" rid="pcbi.1001111-Globerson2">[8]</xref>. We use this idea to study single neuron coding to discover what statistics of the inputs are encoded in the outputs. In macaque retina and lateral geniculate nucleus, we find that the single neuron responses to naturalistic stimuli are well described with only first and second order moments constrained. Thus, the vast majority of the information encoded in the spiking of these cells is related only to the first and second order statistics of the inputs.</p>
      <p>To begin, consider a system which at each moment in time receives a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e001" xlink:type="simple"/></inline-formula>-dimensional input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e002" xlink:type="simple"/></inline-formula> from a known distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e003" xlink:type="simple"/></inline-formula>, such as a neuron receiving a sensory stimulus or post-synaptic potentials. The system then performs some computation to determine the output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e004" xlink:type="simple"/></inline-formula> according to its response function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e005" xlink:type="simple"/></inline-formula>. The complete input/output correlation structure, i.e. all moments involving <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e006" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e007" xlink:type="simple"/></inline-formula>, can be calculated from this function through the joint distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e008" xlink:type="simple"/></inline-formula>, e.g. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e009" xlink:type="simple"/></inline-formula>. Alternatively, the full list of such moments contains the same information about the computation as the response function itself, although such a list is infinite and experimentally impossible to obtain. However, a partial list is usually obtainable, and as a first step we can force the input/output correlations from the model to match those which are known from the data. The problem is then choosing from the infinite number of models that agree with those constraints. Following the argument of Jaynes <xref ref-type="bibr" rid="pcbi.1001111-Jaynes1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1001111-Jaynes2">[10]</xref>, we seek the model which avails the most uncertainty about how the system will respond.</p>
      <p>Information about the identity of the input can be obtained by observing the output, or vice versa, quantified by the mutual information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e010" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1001111-Shannon1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1001111-Cover1">[12]</xref>. The first term is the response entropy, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e011" xlink:type="simple"/></inline-formula>, which captures the overall uncertainty in the output. The second term is the so-called noise entropy <xref ref-type="bibr" rid="pcbi.1001111-Rieke1">[13]</xref>,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e012" xlink:type="simple"/><label>(1)</label></disp-formula>representing the uncertainty in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e013" xlink:type="simple"/></inline-formula> that remains if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e014" xlink:type="simple"/></inline-formula> is known. If the inputs completely determine the outputs, there is no noise and the mutual information reaches its highest possible value, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e015" xlink:type="simple"/></inline-formula>. In many realistic situations however, repeated presentations of the same inputs produce variable outputs producing a nonzero noise entropy <xref ref-type="bibr" rid="pcbi.1001111-Strong1">[14]</xref> and lowering the information transmitted.</p>
      <p>By maximizing the noise entropy, the model is forced to be consistent with the known stimulus/response relationships but is as uncertain as possible with respect to everything else. We show that this maximum noise entropy (MNE) response function for binary output systems with fixed average outputs is also a minimally informative one. This approach is a special closed-form case of the mutual information minimization technique <xref ref-type="bibr" rid="pcbi.1001111-Globerson2">[8]</xref>, which has been used to address the information content of constraints on the interactions between neurons. Here we use the minimization of the mutual information to characterize the computations of single neurons and discover what about the stimulus is being encoded in their spiking behavior.</p>
   </sec>
   <sec id="s2">
      <title>Results</title>

      <sec id="s2a">
         <title>Maximum noise entropy models</title>

         <p>The starting point for constructing any maximum noise entropy model is the specification of a set of constraints <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e016" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e017" xlink:type="simple"/></inline-formula> indicates an average over the joint distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e018" xlink:type="simple"/></inline-formula>. These constraints reflect what is known about the system from experimental measurements, or a hypothesis about what is relevant for the information processing of the system. For neural coding, the constraints could be quantities such as the spike-triggered average <xref ref-type="bibr" rid="pcbi.1001111-deBoer1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1001111-Chichilnisky1">[18]</xref> or covariance <xref ref-type="bibr" rid="pcbi.1001111-deRuytervanSteveninck1">[19]</xref>–<xref ref-type="bibr" rid="pcbi.1001111-Schwartz1">[22]</xref>, equivalent to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e019" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e020" xlink:type="simple"/></inline-formula>, respectively. With each additional constraint, our knowledge of the true input/output relationship increases and the correlation structure of the model becomes more similar to that of the actual system.</p>
         <p>Given the constraints, the general MNE response function is given by (see <xref ref-type="sec" rid="s4">Methods</xref>)<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e021" xlink:type="simple"/><label>(2)</label></disp-formula>where the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e022" xlink:type="simple"/></inline-formula>-dependent partition function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e023" xlink:type="simple"/></inline-formula> ensures that the MNE response function is consistent with normalization, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e024" xlink:type="simple"/></inline-formula>. The MNE response function in Eq. (2) has the form of a Boltzmann distribution <xref ref-type="bibr" rid="pcbi.1001111-Landau1">[23]</xref> with a Lagrange multiplier <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e025" xlink:type="simple"/></inline-formula> for each constraint. The values of these parameters are found by matching the experimentally observed averages with the analytical averages obtained by from derivatives of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e026" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1001111-Landau1">[23]</xref>.</p>
      </sec>
      <sec id="s2b">
         <title>Binary responses and minimum mutual information</title>

         <p>Many systems in biological settings produce binary outputs. For instance, the neural state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e027" xlink:type="simple"/></inline-formula> can be thought of as binary, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e028" xlink:type="simple"/></inline-formula> for the silent state and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e029" xlink:type="simple"/></inline-formula> for the “spiking” state, during which an action potential is fired <xref ref-type="bibr" rid="pcbi.1001111-Rieke1">[13]</xref>. The inputs themselves could be a sensory stimulus or all of the synaptic activity impinging upon a neuron, both of which are typically high-dimensional <xref ref-type="bibr" rid="pcbi.1001111-Dayan1">[24]</xref>. Another example is gene regulation <xref ref-type="bibr" rid="pcbi.1001111-Kauffman1">[25]</xref>, where the inputs could be the concentrations of transcription factors and the binary output represents an on/off transcription state of the gene. For these systems, the constraints of interest are proportional to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e030" xlink:type="simple"/></inline-formula>. This is because any moments independent of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e031" xlink:type="simple"/></inline-formula> will cancel due to the partition function and any moments with higher powers are redundant, e.g. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e032" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e033" xlink:type="simple"/></inline-formula> or 1. In this case, the set of constraints may be written more specifically as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e034" xlink:type="simple"/></inline-formula> and the MNE response function becomes the well-known logistic function<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e035" xlink:type="simple"/><label>(3)</label></disp-formula>with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e036" xlink:type="simple"/></inline-formula>. Thus for all binary MNE models, the effect of the constraints is to perform a nonlinear transformation of the input variables, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e037" xlink:type="simple"/></inline-formula>, to a space where the spike probability is given by the logistic function (inset, <xref ref-type="fig" rid="pcbi-1001111-g001">Fig. 1</xref>).</p>
         <fig id="pcbi-1001111-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001111.g001</object-id><label>Figure 1</label>
            <caption>
               <title>The maximum noise entropy (MNE) limit.</title>
               <p>This cartoon illustrates the consequences of a minimally informative, MNE response function. As knowledge of the correlation structure increases (which amounts to constraining more moments of the conditional output distribution), the least possible amount of information consistent with that knowledge increases along the solid line. Below the MNE limit is a forbidden region where a response function cannot be consistent with the given set of constraints. All models are bounded from above by the response entropy, corresponding to a noiseless system. Any response function above the MNE limit thus involves unknown and unconstrained moments which carry information. The information associated with the MNE response function increases toward the true value as the knowledge of the distribution tends to infinity. For a binary system, the response function is a logistic function (inset) in the transformed input space defined by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e038" xlink:type="simple"/></inline-formula>, cf. Eq. (3).</p>
            </caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001111.g001" xlink:type="simple"/></fig>
         <p>For neural coding, one of the most fundamental and easily measured quantities is the total number of spikes produced by a neuron over the course of an experiment, equivalent to the mean firing rate. By constraining this quantity, or more specifically its normalized version <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e039" xlink:type="simple"/></inline-formula>, the MNE model is turned into a minimum information model. This holds because the response entropy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e040" xlink:type="simple"/></inline-formula> is completely determined by the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e041" xlink:type="simple"/></inline-formula>, which is in turn constrained by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e042" xlink:type="simple"/></inline-formula> if the response is binary. With the response entropy constrained to match the experimentally observed system, maximizing the noise entropy is equivalent to minimizing information. Therefore, as was proposed in <xref ref-type="bibr" rid="pcbi.1001111-Globerson1">[7]</xref>, any model that satisfies a given set of constraints will convey the information that is due only to those constraints. With each additional constraint our knowledge of the correlation structure increases along with the minimum possible information given that knowledge, which approaches the true value as illustrated schematically in <xref ref-type="fig" rid="pcbi-1001111-g001">Fig. 1</xref>.</p>
         <p>The simplest choice is a first order model (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e043" xlink:type="simple"/></inline-formula>) where the spikes are correlated with each input dimension separately. This model requires knowledge of the set of moments <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e044" xlink:type="simple"/></inline-formula>, the spike-triggered average stimulus. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e045" xlink:type="simple"/></inline-formula>, the transformation on the inputs is linear, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e046" xlink:type="simple"/></inline-formula>, where the constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e047" xlink:type="simple"/></inline-formula> is the Lagrange multiplier for the spike probability constraint. With knowledge of only first order correlations, we see that the model neuron is effectively one-dimensional, choosing a single dimension in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e048" xlink:type="simple"/></inline-formula>-dimensional input space <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e049" xlink:type="simple"/></inline-formula> and disregarding all information about any other directions.</p>
         <p>With higher order constraints, the transformation is nonlinear and the model neuron is truly multidimensional. For instance, the next level of complexity is a second order model (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e050" xlink:type="simple"/></inline-formula>), in which spikes may also interact with pairs of inputs. This model is obtained by constraining <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e051" xlink:type="simple"/></inline-formula>, equivalent to knowing the spike-triggered covariance of the stimulus, resulting in the input transformation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e052" xlink:type="simple"/></inline-formula>. Any other MNE model can be constructed in the same fashion by choosing a different set of constraints, reflecting different amounts of knowledge.</p>
         <p>The mutual information of the MNE model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e053" xlink:type="simple"/></inline-formula> is the information content of the constraints. The ratio of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e054" xlink:type="simple"/></inline-formula> to the empirical estimate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e055" xlink:type="simple"/></inline-formula> of the true mutual information of the system is the percent of the information captured by the constraints. This quantity is always less than or equal to one, with equality being reached if and only if all of the relevant moments have been constrained. This suggests a procedure to identify the relevant constraints, described in <xref ref-type="fig" rid="pcbi-1001111-g002">Fig. 2A</xref>. First, a hypothesis is made about which constraints are important. Then the corresponding MNE model is constructed and the information calculated. If the information captured is too small, the constraints are modified until a sufficiently large percentage is reached. Any constraints beyond that are relatively unimportant for describing the computation of the neuron.</p>
         <fig id="pcbi-1001111-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001111.g002</object-id><label>Figure 2</label>
            <caption>
               <title>Using the MNE method for response functions to binary inputs.</title>
               <p><bold>A</bold>) Flowchart representing how to determine the relevant constraints. The hypothesis that a minimal set of constraints is sufficient is tested by constructing the corresponding MNE model and calculating the information captured by the model. If the percent information is insufficient, the set of constraints is augmented. <bold>B</bold>) Response functions and MNE models for two binary inputs; the true system is shown in black, and first and second order MNE models (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e056" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e057" xlink:type="simple"/></inline-formula>) in yellow and orange, respectively. The AND and OR gates use only first order interactions; both MNE models explain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e058" xlink:type="simple"/></inline-formula> of the information. <bold>C</bold>) The XOR gate (left) uses only second order interactions; <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e059" xlink:type="simple"/></inline-formula> explains 0% while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e060" xlink:type="simple"/></inline-formula> explains 100% of the information. An example of a mixed response function (right), for which both first and second order interactions are used (10% and 100% respectively). <bold>D</bold>) Two examples of response functions with three binary inputs, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e061" xlink:type="simple"/></inline-formula> shown in red. Only second order interactions are necessary for the top gate, with 48%, 100% and 100% of the information captured by the first, second and third order MNE models. For the bottom gate, the models capture 39%, 71% and 100% of the information, indicating that third order constraints are necessary. In all cases, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e062" xlink:type="simple"/></inline-formula> was calculated assuming a uniform input distribution.</p>
            </caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001111.g002" xlink:type="simple"/></fig>
         <p>As an illustrative example of the MNE method, consider a binary neuron which itself receives binary inputs (i.e. a logic gate). If the neuron in question receives <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e063" xlink:type="simple"/></inline-formula> binary inputs, we are guaranteed to capture 100% of the information with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e064" xlink:type="simple"/></inline-formula>-order statistics because all moments involving powers greater than one of either <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e065" xlink:type="simple"/></inline-formula> or any <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e066" xlink:type="simple"/></inline-formula> are redundant. However, different coding schemes may encode different statistics of the inputs. For instance, if the neuron receives only two inputs (<xref ref-type="fig" rid="pcbi-1001111-g002">Fig. 2B</xref>), the well-known AND and OR logic gate behaviors are completely described with only first order moments <xref ref-type="bibr" rid="pcbi.1001111-Schneidman1">[26]</xref>. Correspondingly, the first order model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e067" xlink:type="simple"/></inline-formula> captures 100% of the information. Such a neuron can be said to encode only first order statistics of the inputs, and the spike-triggered average stimulus contains all of the information necessary to fully understand the computation. On the other hand, the XOR gate (<xref ref-type="fig" rid="pcbi-1001111-g002">Fig. 2C</xref>, left) requires second order interactions. This is reflected by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e068" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e069" xlink:type="simple"/></inline-formula> accounting for 0% and 100% of the information, respectively. More complicated coding schemes may involve both first and second order interactions, such as for the gate shown in the right panel of <xref ref-type="fig" rid="pcbi-1001111-g002">Fig. 2C</xref>. Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e070" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e071" xlink:type="simple"/></inline-formula> account for 10% and 100% of the information, respectively, and correctly quantify the degree to which each order of interaction is relevant to this neuron.</p>
         <p>Similar situations show up for neurons that receive three binary inputs. The top panel of <xref ref-type="fig" rid="pcbi-1001111-g002">Fig. 2D</xref> shows an example of a neuron which only requires second order interactions. The parameters of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e072" xlink:type="simple"/></inline-formula> are exactly the same as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e073" xlink:type="simple"/></inline-formula>, with the third order coefficient <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e074" xlink:type="simple"/></inline-formula>. The bottom panel shows an example of a situation in which third order interactions are necessary. Correspondingly, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e075" xlink:type="simple"/></inline-formula> increases the information explained over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e076" xlink:type="simple"/></inline-formula> from 71% to 100%. These simulations demonstrate that despite the different coding schemes used by neurons, the information content of each order of interaction can be correctly identified using logistic MNE models.</p>
      </sec>
      <sec id="s2c">
         <title>Neural coding of naturalistic inputs</title>

         <p>In their natural environment, neurons commonly encode high-dimensional analog inputs, such as a visual or auditory stimulus as a function of time. It is important to note that the non-binary nature of the inputs means that the ability to capture 100% of the information between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e077" xlink:type="simple"/></inline-formula> and the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e078" xlink:type="simple"/></inline-formula> inputs with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e079" xlink:type="simple"/></inline-formula>-order statistics is not guaranteed anymore. Often, the dimensionality of the inputs may be reduced because the neurons are driven by a smaller subspace of relevant dimensions (e.g. <xref ref-type="bibr" rid="pcbi.1001111-Fairhall1">[27]</xref>–<xref ref-type="bibr" rid="pcbi.1001111-Hong1">[33]</xref>). However, even in those cases we are often forced to use qualitative terms such as ‘ring’ or ‘crescent’ to describe the experimentally observed response functions. With no principled way of fitting empirical response functions, the details of the interactions between neural responses and reduced inputs have been difficult to quantify.</p>
         <p>The MNE method provides a quantitative framework for characterizing neural response functions, which we now apply to 9 retinal ganglion cells (RGCs) and 9 cells in the lateral geniculate nucleus (LGN) of macaque monkeys, recorded <italic>in vivo</italic> (see <xref ref-type="sec" rid="s4">Methods</xref>). The visual input was a time dependent sequence of luminance values synthesized to mimic the non-Gaussian statistics of light intensity fluctuations in the natural visual environment <xref ref-type="bibr" rid="pcbi.1001111-Ruderman1">[34]</xref>–<xref ref-type="bibr" rid="pcbi.1001111-Simoncelli1">[36]</xref>.</p>
         <p>A 1s segment of the normalized light intensity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e080" xlink:type="simple"/></inline-formula> is shown in <xref ref-type="fig" rid="pcbi-1001111-g003">Fig. 3A</xref>. A previous study has shown that the responses of these neurons are correlated with the stimulus over an approximately 200 ms window preceding the response. When binned at 4 ms resolution, which ensures binary responses, the input is a vector in a 50 dimensional space. However, spikes are well predicted by using a 2 dimensional subspace <xref ref-type="bibr" rid="pcbi.1001111-Sincich1">[29]</xref> identified through the Maximally Informative Dimensions (MID) technique <xref ref-type="bibr" rid="pcbi.1001111-Sharpee1">[37]</xref>.</p>
         <fig id="pcbi-1001111-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001111.g003</object-id><label>Figure 3</label>
            <caption>
               <title>MNE models for a RGC.</title>
               <p><bold>A</bold>) The normalized luminance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e081" xlink:type="simple"/></inline-formula> of the visual input, along with the two most informative reduced inputs, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e082" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e083" xlink:type="simple"/></inline-formula>, shown for a section of the stimulus presented to neuron mn122R4_3_RGC. <bold>B</bold>) The two maximally informative dimensions (MID) for this neuron (error bars are standard error in the mean). Each dimension is a filter which spans 200 ms before the neural output. The convolution of these filters with the stimulus produce <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e084" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e085" xlink:type="simple"/></inline-formula>, which are normalized to lie in the range -1 to 1. In this 2-<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e086" xlink:type="simple"/></inline-formula> reduced input space, the input distribution, <bold>C</bold>), and observed response function, <bold>D</bold>), are shown, discretized into 14 bins along each dimension. White squares in the input distribution indicate unsampled inputs, while white squares in the response function indicate no spikes were recorded. The first order, <bold>E</bold>), and second order, <bold>F</bold>), MNE response functions for this cell explain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e087" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e088" xlink:type="simple"/></inline-formula> of the information, respectively.</p>
            </caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001111.g003" xlink:type="simple"/></fig>
         <p>These two relevant dimensions, shown for a RGC in <xref ref-type="fig" rid="pcbi-1001111-g003">Fig. 3B</xref>, form a two dimensional receptive field which preserves the most information about the spikes in going from 50 to 2 dimensions. The two linear filters are convolved with the stimulus to produce reduced inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e089" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e090" xlink:type="simple"/></inline-formula>, shown in <xref ref-type="fig" rid="pcbi-1001111-g003">Fig. 3A</xref>. The resulting input probability distribution in the reduced space is shown in <xref ref-type="fig" rid="pcbi-1001111-g003">Fig. 3C</xref>. The measured responses of the neuron then form a two-dimensional response function shown in <xref ref-type="fig" rid="pcbi-1001111-g003">Fig. 3D</xref>, where the color scale indicates the probability of a spike as a function of the two relevant input components.</p>
         <p>To gain insight into the nature of this neuron's computational function and find the important interactions, we apply the MNE method starting with the first order MNE model shown in <xref ref-type="fig" rid="pcbi-1001111-g003">Fig. 3E</xref>. The first order model produces a response function which bears little resemblance to the empirical one and accounts for only <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e091" xlink:type="simple"/></inline-formula> of the information. The next step is a second order MNE model (<xref ref-type="fig" rid="pcbi-1001111-g003">Fig. 3F</xref>), which produces a response function quite similar to the empirical one in both shape and amplitude, while accounting for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e092" xlink:type="simple"/></inline-formula> of the information. Thus, for this neuron, knowledge of second order moments is both necessary and sufficient to generate a highly accurate model of the neural responses.</p>
         <p>This result was typical across the population of cells, as illustrated in <xref ref-type="fig" rid="pcbi-1001111-g004">Fig. 4A</xref> by comparing the information captured by the first order versus second order models. The majority of the cells were well described by the second order model, accounting for over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e093" xlink:type="simple"/></inline-formula> of the information. When averaged across the population, the first order model captured <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e094" xlink:type="simple"/></inline-formula> and the second order model captured <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e095" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e096" xlink:type="simple"/></inline-formula>. These results suggest that the inclusion of second order interactions are both necessary and sufficient to describe the responses of these neurons to naturalistic stimuli.</p>
         <fig id="pcbi-1001111-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001111.g004</object-id><label>Figure 4</label>
            <caption>
               <title>Second order MNE models are sufficient across the population.</title>
               <p><bold>A</bold>) A direct comparison of the percent information captured by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e097" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e098" xlink:type="simple"/></inline-formula>. No cells are sufficiently modeled with a first order model, but most are with the second order model. The average information captured is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e099" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e100" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e101" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e102" xlink:type="simple"/></inline-formula>. <bold>B</bold>) Comparison of experimentally measured values to theoretical predictions for higher-order unconstrained moments. Predictions for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e103" xlink:type="simple"/></inline-formula>, left, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e104" xlink:type="simple"/></inline-formula>, right, show a dramatic improvement when second order interactions are included in the model.</p>
            </caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001111.g004" xlink:type="simple"/></fig>
         <p>Since the MNE response function is a distribution of outputs given inputs, another way to check the effectiveness of any MNE model is to compare its moments with those obtained from experiments. The moments constrained to obtain the model will be identical to the experimental values by construction; it is the higher order moments, left unconstrained, that should be compared. In <xref ref-type="fig" rid="pcbi-1001111-g004">Fig. 4B</xref> we show two such comparisons for the correlation functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e105" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e106" xlink:type="simple"/></inline-formula>, which involve moments unconstrained in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e107" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e108" xlink:type="simple"/></inline-formula> models. In both cases, the first order model predictions show more scatter than those of the second order model; the latter does a reasonable job of predicting the experimentally observed correlations. This result broadly demonstrates the sufficiency of second order interactions to model these neural responses, and shows that higher-order moments carry little to no additional information.</p>
         <p>The two-dimensional second order MNE response functions have contours of constant probability which are conic sections. The parameter which governs the interaction between the two input dimensions, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e109" xlink:type="simple"/></inline-formula>, is related to the degree to which the axes of symmetry of the conic sections are aligned with the two-dimensional basis. For example, if the contours are ellipses, then <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e110" xlink:type="simple"/></inline-formula> if the semi-major and semi-minor axes are parallel to the axes chosen to describe the input space, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e111" xlink:type="simple"/></inline-formula> otherwise (see inset, <xref ref-type="fig" rid="pcbi-1001111-g005">Fig. 5</xref>). To assess the importance of this cross term, we compared the performance of second order MNE models with and without <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e112" xlink:type="simple"/></inline-formula>. This additional term can only improve the performance of the model; however, as shown in <xref ref-type="fig" rid="pcbi-1001111-g005">Fig. 5</xref>, the improvements across the population are small. Thus, the dimensions found using the MID method are naturally parallel to the axes of symmetry of the response functions; however, this does not imply that the response function is separable due to the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e113" xlink:type="simple"/></inline-formula> dependence of the normalization term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e114" xlink:type="simple"/></inline-formula>.</p>
         <fig id="pcbi-1001111-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001111.g005</object-id><label>Figure 5</label>
            <caption>
               <title>Importance of the mixed second order moments.</title>
               <p>A comparison of the percent of the information captured by a second order model (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e115" xlink:type="simple"/></inline-formula>) that constrains <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e116" xlink:type="simple"/></inline-formula> (i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e117" xlink:type="simple"/></inline-formula>) and a second order model with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e118" xlink:type="simple"/></inline-formula>. For most cells, the information increases only slightly for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e119" xlink:type="simple"/></inline-formula>, indicating that little information is gained by constraining this moment. (Inset) The parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e120" xlink:type="simple"/></inline-formula> determines the angle between the axes of symmetry of the response function and the basis of the input space.</p>
            </caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001111.g005" xlink:type="simple"/></fig>
      </sec>
   </sec>
   <sec id="s3">
      <title>Discussion</title>

      <p>For neural coding of naturalistic visual stimuli in early visual processing, we see that the bulk of what is being encoded is first order stimulus statistics. While the information gained by measuring the spike-triggered average is substantial, it is insufficient to accurately describe the neural responses. A second order model, which takes into account the spike-triggered input covariance, adds a sufficient amount of information. Thus the firing rates of these neurons have encoded the first and second order statistics of the inputs. Due to the fact that the natural inputs are non-binary and non-Gaussian, there exists a potential for very high-order interactions to be represented in the neural firing rate. It is known that higher order parameters of textures are perceptually salient <xref ref-type="bibr" rid="pcbi.1001111-Chubb1">[38]</xref>–<xref ref-type="bibr" rid="pcbi.1001111-Tkaik1">[40]</xref>, but it is unknown whether high order temporal statistics are also perceptually salient. Our results suggest that such temporal statistics are not encoded in the time-dependent firing rate, although they could be represented through populations of neurons or specific temporal sequences of spikes <xref ref-type="bibr" rid="pcbi.1001111-Theunissen1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1001111-Brenner1">[42]</xref>.</p>
      <p>Jaynes' principle of maximum entropy <xref ref-type="bibr" rid="pcbi.1001111-Jaynes1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1001111-Jaynes2">[10]</xref> has a long and diverse history, with example applications in image restoration in astrophysics <xref ref-type="bibr" rid="pcbi.1001111-Narayan1">[43]</xref>, extension of Wiener analysis to nonlinear stochastic transducers <xref ref-type="bibr" rid="pcbi.1001111-Victor2">[44]</xref> and more recently in neuroscience <xref ref-type="bibr" rid="pcbi.1001111-Schneidman2">[45]</xref>–<xref ref-type="bibr" rid="pcbi.1001111-Tang1">[47]</xref>. In the latter studies, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e121" xlink:type="simple"/></inline-formula> was maximized subject to constraints on the first and second order moments of the neural states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e122" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e123" xlink:type="simple"/></inline-formula> for a set of neurons in a network. The resulting pairwise Ising model was shown to accurately describe the distribution of network states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e124" xlink:type="simple"/></inline-formula> of real neurons under various conditions. Since then the application of the Ising model to neuroscience has received much attention <xref ref-type="bibr" rid="pcbi.1001111-Roudi1">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1001111-Roudi2">[49]</xref>, and it is still a subject of debate if and how these results extrapolate to larger populations of neurons <xref ref-type="bibr" rid="pcbi.1001111-Roudi3">[50]</xref>. Temporal correlations have also been shown to be important in both cortical slices and networks of cultured neurons <xref ref-type="bibr" rid="pcbi.1001111-Tang1">[47]</xref>.</p>
      <p>In contrast to maximum entropy models that deal with stationary or averaged distributions of states, the goal of maximizing the noise entropy is to find unbiased response functions. This approach is equivalent to conditional random field (CRF) models <xref ref-type="bibr" rid="pcbi.1001111-Lafferty1">[6]</xref> in machine learning. The parameters of a CRF are fit by maximizing the likelihood using iterative or gradient ascent algorithms <xref ref-type="bibr" rid="pcbi.1001111-Malouf1">[51]</xref> and have been used, for example, in classification and segmentation tasks <xref ref-type="bibr" rid="pcbi.1001111-Berger1">[52]</xref>. The parameters of MNE models may also be found using maximum likelihood, or as was done here, by solving a set of simultaneous constraint equations numerically. Another example of a maximum noise entropy distribution is the Fermi-Dirac distribution <xref ref-type="bibr" rid="pcbi.1001111-Landau1">[23]</xref> from statistical physics, which is a logistic function governing the binary occupation of fermion energy levels. Thus, in the same way that the Boltzmann distribution was interpreted by Jaynes as the most random one consistent with measurements of the energy, the Fermi-Dirac distribution can be interpreted as the least biased binary response function consistent with an average energy. However, to our knowledge, this method has never been used in the context of neural coding to determine the input statistics which are being encoded by a neuron and create the corresponding unbiased models.</p>
      <p>Previous work has applied the principle of minimum mutual information (MinMI) <xref ref-type="bibr" rid="pcbi.1001111-Globerson1">[7]</xref> to neural coding, thus identifying the relevant interactions between neurons <xref ref-type="bibr" rid="pcbi.1001111-Globerson2">[8]</xref>. We have shown that the closed-form MNE solutions for binary neurons constitute a special case of MinMI, since the response entropy is fixed if the average firing rate is constrained. In general, the MinMI principle results in a self-consistent solution that must be solved iteratively to obtain the response function. The reason why MNE models are closed-form is that the constraints are formulated in terms of moments of the output distribution instead of the output distribution itself. In addition to the case of binary responses, MNE models can become closed-form MinMI models for any input/output systems where the response entropy can be fixed in terms of the moments of the output variable. Examples include Poisson processes with fixed average response rate or Gaussian processes with fixed mean and variance of the response rate. The framework for analyzing the interactions between inputs and outputs that we present here can thus be extended to a broad and diverse set of computational systems.</p>
      <p>Our approach can be compared to other optimization techniques commonly used to study information processing. For example, rate-distortion theory <xref ref-type="bibr" rid="pcbi.1001111-Shannon1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1001111-Cover1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1001111-Berger2">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1001111-Tishby1">[54]</xref> seeks minimum information transmission rate over a channel with a fixed level of signal distortion, e.g. lossy image or video compression. In that case, the best solution is the one which transmits minimal information because this determines the average length of the codewords. In our method, we also obtain minimally informative solutions, not because they are optimal for signal transmission, but because they are the most unbiased guess at a solution given limited knowledge of a complex system.</p>
      <p>At the other end of the optimization spectrum is maximization of information <xref ref-type="bibr" rid="pcbi.1001111-Haken1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1001111-Rieke1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1001111-Li1">[55]</xref>. The goal in that case is to study not how the neuron <italic>does</italic> compute, but how it <italic>should</italic> compute to get the most information, perhaps with limited resources. This strategy has been used to find neural response functions for single neurons <xref ref-type="bibr" rid="pcbi.1001111-Laughlin1">[56]</xref>, <xref ref-type="bibr" rid="pcbi.1001111-Sharpee2">[57]</xref>, as well as networks <xref ref-type="bibr" rid="pcbi.1001111-Fitzgerald1">[58]</xref>, <xref ref-type="bibr" rid="pcbi.1001111-Nikitin1">[59]</xref>. When confronted with incomplete knowledge of the correlation structure, a maximum information approach would choose the values of the unconstrained moments such that they convey the most information possible, whereas the minimum information approach provides a lower bound to the true mutual information, and allows us to investigate how this lower bound increases as more moments are included. If the goal is to study the limits of neural coding, then maximizing the information may be the best procedure. If, however, the goal is to dissect the computational function of an observed neuron, we argue that the more agnostic approaches of maximizing the noise entropy or minimizing the mutual information are better-suited.</p>
   </sec>
   <sec id="s4" sec-type="methods">
      <title>Methods</title>

      <sec id="s4a">
         <title>Ethics statement</title>

         <p>Experimental data were collected as part of the previous study using procedures approved by the UCSF Institutional Animal Care and Use Committee, and in accordance with National Institutes of Health guidelines.</p>
      </sec>
      <sec id="s4b">
         <title>Maximum noise entropy model</title>

         <p>A maximum noise entropy model is a response function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e125" xlink:type="simple"/></inline-formula> which agrees with a set of constraints and is maximally unbiased toward everything else. The constraints are experimentally observed moments involving the response <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e126" xlink:type="simple"/></inline-formula> and stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e127" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e128" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e129" xlink:type="simple"/></inline-formula>, which must be reproduced by the model. The set of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e130" xlink:type="simple"/></inline-formula> constraints, including the normalization of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e131" xlink:type="simple"/></inline-formula>, are then added to the noise entropy to form the functional<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e132" xlink:type="simple"/><label>(4)</label></disp-formula>with a Lagrange multiplier <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e133" xlink:type="simple"/></inline-formula> for each constraint. Setting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e134" xlink:type="simple"/></inline-formula> and enforcing normalization yields Eq. 1. For a binary system, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e135" xlink:type="simple"/></inline-formula> or 1, all the constraints take the form <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e136" xlink:type="simple"/></inline-formula>, and the partition function is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e137" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e138" xlink:type="simple"/></inline-formula></p>
         <p>The values of the Lagrange multipliers are found such that the set of equations<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e139" xlink:type="simple"/><label>(5)</label></disp-formula>is satisfied, with the analytical averages on the right-hand side obtained from derivatives of the free energy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e140" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1001111-Landau1">[23]</xref>. Simultaneously solving this set of equations has previously been shown to be equivalent to maximizing the log-likelihood <xref ref-type="bibr" rid="pcbi.1001111-Malouf1">[51]</xref>.</p>
      </sec>
      <sec id="s4c">
         <title>Physiology experiment</title>

         <p>The neural data analyzed here were collected in a previous study <xref ref-type="bibr" rid="pcbi.1001111-Sincich1">[29]</xref> and the details are found therein. Briefly, the stimulus was a spot of light covering a cell's receptive field center, flickering with non-Gaussian statistics that mimic those of light intensity fluctuations found in natural environments <xref ref-type="bibr" rid="pcbi.1001111-vanHateren1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1001111-Simoncelli1">[36]</xref>. The values of light intensities were updated every <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e141" xlink:type="simple"/></inline-formula> (update rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e142" xlink:type="simple"/></inline-formula>). The spikes were recorded extracellularly in the LGN with high signal-to-noise, allowing for excitatory post-synaptic potentials generated by the RGC inputs to be recorded. From such data, the complete spike trains of both RGCs and LGN neurons could be reconstructed <xref ref-type="bibr" rid="pcbi.1001111-Sincich2">[60]</xref>.</p>
      </sec>
      <sec id="s4d">
         <title>Dimensionality reduction</title>

         <p>The neural spike trains were binned at 4 ms resolution, ensuring that the response was binary. The stimulus was re-binned at 250 Hz to match the bin size of the spike analysis. The neurons were uncorrelated with light fluctuations beyond 200 ms before a spike, and the stimulus vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e143" xlink:type="simple"/></inline-formula> was taken to be the 200 ms window (50 time points) of the stimulus preceding <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e144" xlink:type="simple"/></inline-formula>. Just two projections of this 50-dimensional input are sufficient to capture a large fraction of the information between the light intensity fluctuations and the neural responses (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e145" xlink:type="simple"/></inline-formula> for the example neuron mn122R4_3_RGC, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e146" xlink:type="simple"/></inline-formula> on average across the population). The two most relevant features of each neuron were found by searching the space of all linear combinations of two input dimensions for those which accounted for maximal information in the measured neural responses <xref ref-type="bibr" rid="pcbi.1001111-Sharpee1">[37]</xref>, subject to cross-validation to avoid overfitting. Each of the two features, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e147" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e148" xlink:type="simple"/></inline-formula>, is a 50-dimensional vector which converts the input into a reduced input, calculated by taking the dot product, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001111.e149" xlink:type="simple"/></inline-formula>. The algorithm for searching for maximally informative dimensions is available online at <ext-link ext-link-type="uri" xlink:href="http://cnl-t.salk.edu" xlink:type="simple">http://cnl-t.salk.edu</ext-link>.</p>
      </sec>
   </sec>
</body>
<back>
   <ack>
      <p>We thank Jonathan C. Horton for sharing the data collected in his laboratory and the CNL-T group for helpful conversations.</p>
   </ack>
   <ref-list>
      <title>References</title>
      <ref id="pcbi.1001111-Haken1">
         <label>1</label>
         <element-citation publication-type="other" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Haken</surname><given-names>H</given-names></name>
            </person-group>
            <year>1988</year>
            <article-title>Information and Self-Organization.</article-title>
            <publisher-loc>Berlin</publisher-loc>
            <publisher-name>Springer</publisher-name>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Bray1">
         <label>2</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Bray</surname><given-names>D</given-names></name>
            </person-group>
            <year>1995</year>
            <article-title>Protein molecules as computational elements in living cells.</article-title>
            <source>Nature</source>
            <volume>376</volume>
            <fpage>307</fpage>
            <lpage>312</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Krding1">
         <label>3</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Körding</surname><given-names>K</given-names></name>
            </person-group>
            <year>2007</year>
            <article-title>Decision theory: What “should” the nervous system do?</article-title>
            <source>Science</source>
            <volume>318</volume>
            <fpage>606</fpage>
            <lpage>610</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Sanfey1">
         <label>4</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Sanfey</surname><given-names>AG</given-names></name>
            </person-group>
            <year>2007</year>
            <article-title>Social decision-making: Insights from game theory and neuroscience.</article-title>
            <source>Science</source>
            <volume>318</volume>
            <fpage>598</fpage>
            <lpage>602</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Grenfell1">
         <label>5</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Grenfell</surname><given-names>BT</given-names></name>
               <name name-style="western"><surname>Williams</surname><given-names>CS</given-names></name>
               <name name-style="western"><surname>Björnstad</surname><given-names>ON</given-names></name>
               <name name-style="western"><surname>Banavar</surname><given-names>JR</given-names></name>
            </person-group>
            <year>2006</year>
            <article-title>Simplifying biological complexity.</article-title>
            <source>Nat Phys</source>
            <volume>2</volume>
            <fpage>212</fpage>
            <lpage>214</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Lafferty1">
         <label>6</label>
         <element-citation publication-type="other" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Lafferty</surname><given-names>J</given-names></name>
               <name name-style="western"><surname>McCallum</surname><given-names>A</given-names></name>
               <name name-style="western"><surname>Pereira</surname><given-names>F</given-names></name>
            </person-group>
            <year>2001</year>
            <article-title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</article-title>
            <source>Proceedings of the Eighteenth International Conference on Machine Learning</source>
            <fpage>282</fpage>
            <lpage>289</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Globerson1">
         <label>7</label>
         <element-citation publication-type="other" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Globerson</surname><given-names>A</given-names></name>
               <name name-style="western"><surname>Tishby</surname><given-names>N</given-names></name>
            </person-group>
            <year>2004</year>
            <article-title>The minimum information principle for discriminative learning.</article-title>
            <source>Proceedings of the 20th conference on Uncertainty in artificial intelligence</source>
            <publisher-loc>Arlington, Virginia</publisher-loc>
            <publisher-name>AUAI Press, UAI '04, 193–200</publisher-name>
            <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://portal.acm.org/citation.cfm?id=1036843.1036867" xlink:type="simple">http://portal.acm.org/citation.cfm?id=1036843.1036867</ext-link></comment>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Globerson2">
         <label>8</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Globerson</surname><given-names>A</given-names></name>
               <name name-style="western"><surname>Stark</surname><given-names>E</given-names></name>
               <name name-style="western"><surname>Vaadia</surname><given-names>E</given-names></name>
               <name name-style="western"><surname>Tishby</surname><given-names>N</given-names></name>
            </person-group>
            <year>2009</year>
            <article-title>The minimum information principle and its application to neural code analysis.</article-title>
            <source>Proc Natl Acad Sci USA</source>
            <volume>106</volume>
            <fpage>3490</fpage>
            <lpage>3495</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Jaynes1">
         <label>9</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Jaynes</surname><given-names>ET</given-names></name>
            </person-group>
            <year>1957</year>
            <article-title>Information theory and statistical mechanics.</article-title>
            <source>Phys Rev</source>
            <volume>106</volume>
            <fpage>620</fpage>
            <lpage>630</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Jaynes2">
         <label>10</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Jaynes</surname><given-names>ET</given-names></name>
            </person-group>
            <year>1957</year>
            <article-title>Information theory and statistical mechanics ii.</article-title>
            <source>Phys Rev</source>
            <volume>108</volume>
            <fpage>171</fpage>
            <lpage>190</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Shannon1">
         <label>11</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Shannon</surname><given-names>C</given-names></name>
            </person-group>
            <year>1949</year>
            <article-title>Communication in the presence of noise.</article-title>
            <source>Proc of the IRE</source>
            <volume>37</volume>
            <fpage>10</fpage>
            <lpage>21</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Cover1">
         <label>12</label>
         <element-citation publication-type="other" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Cover</surname><given-names>TM</given-names></name>
               <name name-style="western"><surname>Thomas</surname><given-names>JA</given-names></name>
            </person-group>
            <year>1991</year>
            <article-title>Information theory.</article-title>
            <publisher-loc>New York</publisher-loc>
            <publisher-name>John Wiley &amp; Sons, Inc</publisher-name>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Rieke1">
         <label>13</label>
         <element-citation publication-type="other" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Rieke</surname><given-names>F</given-names></name>
               <name name-style="western"><surname>Warland</surname><given-names>D</given-names></name>
               <name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name>
               <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>
            </person-group>
            <year>1997</year>
            <article-title>Spikes: Exploring the neural code.</article-title>
            <publisher-loc>Cambridge</publisher-loc>
            <publisher-name>MIT Press</publisher-name>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Strong1">
         <label>14</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Strong</surname><given-names>SP</given-names></name>
               <name name-style="western"><surname>Koberle</surname><given-names>R</given-names></name>
               <name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name>
               <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>
            </person-group>
            <year>1998</year>
            <article-title>Entropy and information in neural spike trains.</article-title>
            <source>Phys Rev Lett</source>
            <volume>80</volume>
            <fpage>197</fpage>
            <lpage>200</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-deBoer1">
         <label>15</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>de Boer</surname><given-names>E</given-names></name>
               <name name-style="western"><surname>Kuyper</surname><given-names>P</given-names></name>
            </person-group>
            <year>1968</year>
            <article-title>Triggered correlation.</article-title>
            <source>IEEE Trans Biomed Eng</source>
            <volume>15</volume>
            <fpage>169</fpage>
            <lpage>179</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Victor1">
         <label>16</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Victor</surname><given-names>J</given-names></name>
               <name name-style="western"><surname>Shapley</surname><given-names>R</given-names></name>
            </person-group>
            <year>1980</year>
            <article-title>A method of nonlinear analysis in the frequency domain.</article-title>
            <source>Biophys J</source>
            <volume>29</volume>
            <fpage>459</fpage>
            <lpage>483</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Meister1">
         <label>17</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Meister</surname><given-names>M</given-names></name>
               <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names></name>
            </person-group>
            <year>1999</year>
            <article-title>The neural code of the retina.</article-title>
            <source>Neuron</source>
            <volume>22</volume>
            <fpage>435</fpage>
            <lpage>450</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Chichilnisky1">
         <label>18</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Chichilnisky</surname><given-names>EJ</given-names></name>
            </person-group>
            <year>2001</year>
            <article-title>A simple white noise analysis of neuronal light responses.</article-title>
            <source>Network: Comput Neural Syst</source>
            <volume>12</volume>
            <fpage>199</fpage>
            <lpage>213</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-deRuytervanSteveninck1">
         <label>19</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name>
               <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>
            </person-group>
            <year>1988</year>
            <article-title>Real-time performance of a movement-sensitive neuron in the blowfly visual system: coding and information transfer in short spike sequences.</article-title>
            <source>Proc R Soc Lond B</source>
            <volume>265</volume>
            <fpage>259</fpage>
            <lpage>265</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Bialek1">
         <label>20</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>
               <name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name>
            </person-group>
            <year>2005</year>
            <article-title>Features and dimensions: Motion estimation in fly vision.</article-title>
            <source>Q-bio/</source>
            <volume>0505003</volume>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Pillow1">
         <label>21</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Pillow</surname><given-names>J</given-names></name>
               <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name>
            </person-group>
            <year>2006</year>
            <article-title>Dimensionality reduction in neural models: An information-theoretic generalization of spike-triggered average and covariance analysis.</article-title>
            <source>J Vis</source>
            <volume>6</volume>
            <fpage>414</fpage>
            <lpage>428</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Schwartz1">
         <label>22</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name>
               <name name-style="western"><surname>Pillow</surname><given-names>J</given-names></name>
               <name name-style="western"><surname>Rust</surname><given-names>N</given-names></name>
               <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name>
            </person-group>
            <year>2006</year>
            <article-title>Spike-triggered neural characterization.</article-title>
            <source>J Vis</source>
            <volume>176</volume>
            <fpage>484</fpage>
            <lpage>507</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Landau1">
         <label>23</label>
         <element-citation publication-type="other" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Landau</surname><given-names>LD</given-names></name>
               <name name-style="western"><surname>Lifshitz</surname><given-names>EM</given-names></name>
            </person-group>
            <year>1959</year>
            <article-title>Statistical Physics.</article-title>
            <publisher-loc>Oxford</publisher-loc>
            <publisher-name>Pergamon Press</publisher-name>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Dayan1">
         <label>24</label>
         <element-citation publication-type="other" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
               <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name>
            </person-group>
            <year>2001</year>
            <article-title>Theoretical neuroscience: computational and mathematical modeling of neural systems.</article-title>
            <publisher-loc>Cambridge</publisher-loc>
            <publisher-name>MIT Press</publisher-name>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Kauffman1">
         <label>25</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Kauffman</surname><given-names>SA</given-names></name>
            </person-group>
            <year>1969</year>
            <article-title>Metabolic stability and epigenesis in randomly constructed genetic nets.</article-title>
            <source>J Theoret Biol</source>
            <volume>22</volume>
            <fpage>437</fpage>
            <lpage>467</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Schneidman1">
         <label>26</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name>
               <name name-style="western"><surname>Still</surname><given-names>S</given-names></name>
               <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names></name>
               <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>
            </person-group>
            <year>2003</year>
            <article-title>Network information and connected correlations.</article-title>
            <source>Phys Rev Lett</source>
            <volume>91</volume>
            <fpage>238701</fpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Fairhall1">
         <label>27</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Fairhall</surname><given-names>AL</given-names></name>
               <name name-style="western"><surname>Burlingame</surname><given-names>CA</given-names></name>
               <name name-style="western"><surname>Narasimhan</surname><given-names>R</given-names></name>
               <name name-style="western"><surname>Harris</surname><given-names>RA</given-names></name>
               <name name-style="western"><surname>Puchalla</surname><given-names>JL</given-names></name>
               <etal/>
            </person-group>
            <year>2006</year>
            <article-title>Selectivity for multiple stimulus features in retinal ganglion cells.</article-title>
            <source>J Neurophysiol</source>
            <volume>96</volume>
            <fpage>2724</fpage>
            <lpage>2738</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Rust1">
         <label>28</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name>
               <name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name>
               <name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name>
               <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name>
            </person-group>
            <year>2005</year>
            <article-title>Spatiotemporal elements of macaque v1 receptive fields.</article-title>
            <source>Neuron</source>
            <volume>46</volume>
            <fpage>945</fpage>
            <lpage>956</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Sincich1">
         <label>29</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Sincich</surname><given-names>LC</given-names></name>
               <name name-style="western"><surname>Horton</surname><given-names>JC</given-names></name>
               <name name-style="western"><surname>Sharpee</surname><given-names>TO</given-names></name>
            </person-group>
            <year>2009</year>
            <article-title>Preserving information in neural transmission.</article-title>
            <source>J Neurosci</source>
            <volume>29</volume>
            <fpage>6207</fpage>
            <lpage>6216</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Chen1">
         <label>30</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Chen</surname><given-names>X</given-names></name>
               <name name-style="western"><surname>Han</surname><given-names>F</given-names></name>
               <name name-style="western"><surname>Poo</surname><given-names>MM</given-names></name>
               <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name>
            </person-group>
            <year>2007</year>
            <article-title>Excitatory and suppressive receptive field subunits in awake monkey primary visual cortex (v1).</article-title>
            <source>Proc Natl Acad Sci USA</source>
            <volume>104</volume>
            <fpage>19120</fpage>
            <lpage>5</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Atencio1">
         <label>31</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Atencio</surname><given-names>CA</given-names></name>
               <name name-style="western"><surname>Sharpee</surname><given-names>TO</given-names></name>
               <name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name>
            </person-group>
            <year>2008</year>
            <article-title>Cooperative nonlinearities in auditory cortical neurons.</article-title>
            <source>Neuron</source>
            <volume>58</volume>
            <fpage>956</fpage>
            <lpage>966</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Maravall1">
         <label>32</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Maravall</surname><given-names>M</given-names></name>
               <name name-style="western"><surname>Petersen</surname><given-names>RS</given-names></name>
               <name name-style="western"><surname>Fairhall</surname><given-names>A</given-names></name>
               <name name-style="western"><surname>Arabzadeh</surname><given-names>E</given-names></name>
               <name name-style="western"><surname>Diamond</surname><given-names>M</given-names></name>
            </person-group>
            <year>2007</year>
            <article-title>Shifts in coding properties and maintenance of information transmission during adaptation in barrel cortex.</article-title>
            <source>PLoS Biol</source>
            <volume>5</volume>
            <fpage>e19</fpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Hong1">
         <label>33</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Hong</surname><given-names>S</given-names></name>
               <name name-style="western"><surname>Arcas</surname><given-names>BA</given-names></name>
               <name name-style="western"><surname>Fairhall</surname><given-names>AL</given-names></name>
            </person-group>
            <year>2007</year>
            <article-title>Single neuron computation: from dynamical system to feature detector.</article-title>
            <source>Neural Comput</source>
            <volume>112</volume>
            <fpage>3133</fpage>
            <lpage>3172</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Ruderman1">
         <label>34</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Ruderman</surname><given-names>DL</given-names></name>
               <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>
            </person-group>
            <year>1994</year>
            <article-title>Statistics of natural images: scaling in the woods.</article-title>
            <source>Phys Rev Lett</source>
            <volume>73</volume>
            <fpage>814</fpage>
            <lpage>817</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-vanHateren1">
         <label>35</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>van Hateren</surname><given-names>JH</given-names></name>
            </person-group>
            <year>1997</year>
            <article-title>Processing of natural time series of intensities by the visual system of the blowfly.</article-title>
            <source>Vision Res</source>
            <volume>37</volume>
            <fpage>3407</fpage>
            <lpage>3416</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Simoncelli1">
         <label>36</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name>
               <name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name>
            </person-group>
            <year>2001</year>
            <article-title>Natural image statistics and neural representation.</article-title>
            <source>Annu Rev Neurosci</source>
            <volume>24</volume>
            <fpage>1193</fpage>
            <lpage>1216</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Sharpee1">
         <label>37</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Sharpee</surname><given-names>T</given-names></name>
               <name name-style="western"><surname>Rust</surname><given-names>N</given-names></name>
               <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>
            </person-group>
            <year>2004</year>
            <article-title>Analyzing neural responses to natural signals: Maximally informative dimensions.</article-title>
            <source>Neural Computation</source>
            <volume>16</volume>
            <fpage>223</fpage>
            <lpage>250</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Chubb1">
         <label>38</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Chubb</surname><given-names>C</given-names></name>
               <name name-style="western"><surname>Econopouly</surname><given-names>J</given-names></name>
               <name name-style="western"><surname>Landy</surname><given-names>MS</given-names></name>
            </person-group>
            <year>1994</year>
            <article-title>Histogram contrast analysis and the visual segregation of iid textures.</article-title>
            <source>J Opt Soc Am A</source>
            <volume>11</volume>
            <fpage>2350</fpage>
            <lpage>2374</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Chubb2">
         <label>39</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Chubb</surname><given-names>C</given-names></name>
               <name name-style="western"><surname>Landy</surname><given-names>MS</given-names></name>
               <name name-style="western"><surname>Econopouly</surname><given-names>J</given-names></name>
            </person-group>
            <year>2004</year>
            <article-title>A visual mechanism tuned to black.</article-title>
            <source>Vision Research</source>
            <volume>44</volume>
            <fpage>3223</fpage>
            <lpage>3232</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Tkaik1">
         <label>40</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Tkačik</surname><given-names>G</given-names></name>
               <name name-style="western"><surname>Prentice</surname><given-names>JS</given-names></name>
               <name name-style="western"><surname>Victor</surname><given-names>JD</given-names></name>
               <name name-style="western"><surname>Balasubramanian</surname><given-names>V</given-names></name>
            </person-group>
            <year>2010</year>
            <article-title>Local statistics in natural scenes predict the saliency of synthetic textures.</article-title>
            <source>Proc Natl Acad Sci USA</source>
            <volume>107</volume>
            <fpage>18149</fpage>
            <lpage>18154</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Theunissen1">
         <label>41</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name>
               <name name-style="western"><surname>Miller</surname><given-names>JP</given-names></name>
            </person-group>
            <year>1995</year>
            <article-title>Temporal encoding in nervous systems: a rigorous definition.</article-title>
            <source>J Comput Neurosci</source>
            <volume>2</volume>
            <fpage>149</fpage>
            <lpage>162</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Brenner1">
         <label>42</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Brenner</surname><given-names>N</given-names></name>
               <name name-style="western"><surname>Strong</surname><given-names>SP</given-names></name>
               <name name-style="western"><surname>Koberle</surname><given-names>R</given-names></name>
               <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>
               <name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name>
            </person-group>
            <year>2000</year>
            <article-title>Synergy in a neural code.</article-title>
            <source>Neural Computation</source>
            <volume>12</volume>
            <fpage>1531</fpage>
            <lpage>1552</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Narayan1">
         <label>43</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Narayan</surname><given-names>R</given-names></name>
               <name name-style="western"><surname>Nityananda</surname><given-names>R</given-names></name>
            </person-group>
            <year>1986</year>
            <article-title>Maximum entropy image restoration in astronomy.</article-title>
            <source>Ann Rev Astron Astrophys</source>
            <volume>24</volume>
            <fpage>127</fpage>
            <lpage>170</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Victor2">
         <label>44</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Victor</surname><given-names>JD</given-names></name>
               <name name-style="western"><surname>Johannesma</surname><given-names>P</given-names></name>
            </person-group>
            <year>1986</year>
            <article-title>Maximum-entropy approximations of stochastic nonlinear transductions: an extension of the wiener theory.</article-title>
            <source>Biol Cybern</source>
            <volume>54</volume>
            <fpage>289</fpage>
            <lpage>300</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Schneidman2">
         <label>45</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name>
               <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names></name>
               <name name-style="western"><surname>Segev</surname><given-names>R</given-names></name>
               <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>
            </person-group>
            <year>2006</year>
            <article-title>Weak pairwise correlations imply strongly correlated network states in a neural population.</article-title>
            <source>Nature</source>
            <volume>440</volume>
            <fpage>1007</fpage>
            <lpage>1012</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Shlens1">
         <label>46</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name>
               <name name-style="western"><surname>Field</surname><given-names>GD</given-names></name>
               <name name-style="western"><surname>Gauthier</surname><given-names>JL</given-names></name>
               <name name-style="western"><surname>Grivich</surname><given-names>MI</given-names></name>
               <name name-style="western"><surname>Petrusca</surname><given-names>D</given-names></name>
               <etal/>
            </person-group>
            <year>2006</year>
            <article-title>The structure of multi-neuron firing patterns in primate retina.</article-title>
            <source>J Neurosci</source>
            <volume>26</volume>
            <fpage>8254</fpage>
            <lpage>8266</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Tang1">
         <label>47</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Tang</surname><given-names>A</given-names></name>
               <name name-style="western"><surname>Jackson</surname><given-names>D</given-names></name>
               <name name-style="western"><surname>Hobbs</surname><given-names>J</given-names></name>
               <name name-style="western"><surname>Chen</surname><given-names>W</given-names></name>
               <name name-style="western"><surname>Smith</surname><given-names>JL</given-names></name>
               <etal/>
            </person-group>
            <year>2008</year>
            <article-title>A maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro.</article-title>
            <source>J Neurosci</source>
            <volume>28</volume>
            <fpage>505</fpage>
            <lpage>518</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Roudi1">
         <label>48</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name>
               <name name-style="western"><surname>Tyrcha</surname><given-names>J</given-names></name>
               <name name-style="western"><surname>Hertz</surname><given-names>J</given-names></name>
            </person-group>
            <year>2009</year>
            <article-title>Ising model for neural data: model quality and approximate methods for extracting functional connectivity.</article-title>
            <source>Phys Rev E</source>
            <volume>79</volume>
            <fpage>051915</fpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Roudi2">
         <label>49</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name>
               <name name-style="western"><surname>Aurell</surname><given-names>E</given-names></name>
               <name name-style="western"><surname>Hertz</surname><given-names>J</given-names></name>
            </person-group>
            <year>2009</year>
            <article-title>Statistical physics of pairwise probability models.</article-title>
            <source>Front in Comp Neuro</source>
            <volume>3</volume>
            <fpage>22</fpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Roudi3">
         <label>50</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name>
               <name name-style="western"><surname>Nirenberg</surname><given-names>S</given-names></name>
               <name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name>
            </person-group>
            <year>2009</year>
            <article-title>Pairwise maximum entropy models for studying large biological systems: When they can work and when they can't.</article-title>
            <source>PLoS Comput Biol</source>
            <volume>5</volume>
            <fpage>e1000380</fpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Malouf1">
         <label>51</label>
         <element-citation publication-type="other" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Malouf</surname><given-names>R</given-names></name>
            </person-group>
            <year>2002</year>
            <article-title>A comparison of algorithms for maximum entropy parameter estimation.</article-title>
            <source>Proceedings of the Conference on Natural Language Learning</source>
            <fpage>49</fpage>
            <lpage>55</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Berger1">
         <label>52</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Berger</surname><given-names>AL</given-names></name>
               <name name-style="western"><surname>Pietra</surname><given-names>SAD</given-names></name>
               <name name-style="western"><surname>Pietra</surname><given-names>VJD</given-names></name>
            </person-group>
            <year>1996</year>
            <article-title>A maximum entropy approach to natural language processing.</article-title>
            <source>Computational Linguistics</source>
            <volume>22</volume>
            <fpage>39</fpage>
            <lpage>71</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Berger2">
         <label>53</label>
         <element-citation publication-type="other" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Berger</surname><given-names>T</given-names></name>
            </person-group>
            <year>1971</year>
            <article-title>Rate distortion theory: Mathematical basis for data compression.</article-title>
            <publisher-loc>New Jersey</publisher-loc>
            <publisher-name>Prentice Hall</publisher-name>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Tishby1">
         <label>54</label>
         <element-citation publication-type="other" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Tishby</surname><given-names>N</given-names></name>
               <name name-style="western"><surname>Pereira</surname><given-names>FC</given-names></name>
               <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>
            </person-group>
            <year>1999</year>
            <article-title>The information bottleneck method.</article-title>
            <source>Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing</source>
            <fpage>368</fpage>
            <lpage>377</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Li1">
         <label>55</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Li</surname><given-names>Z</given-names></name>
            </person-group>
            <year>2006</year>
            <article-title>Theoretical understanding of the early visual processes by data compression and data selection.</article-title>
            <source>Network: Computation in neural systems</source>
            <volume>17</volume>
            <fpage>301</fpage>
            <lpage>334</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Laughlin1">
         <label>56</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name>
            </person-group>
            <year>1981</year>
            <article-title>A simple coding procedure enhances a neuron's information capacity.</article-title>
            <source>Z Naturf</source>
            <volume>36c</volume>
            <fpage>910</fpage>
            <lpage>912</lpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Sharpee2">
         <label>57</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Sharpee</surname><given-names>TO</given-names></name>
               <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>
            </person-group>
            <year>2007</year>
            <article-title>Neural decision boundaries for maximal information transmission.</article-title>
            <source>PLoS One</source>
            <volume>2</volume>
            <fpage>e646</fpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Fitzgerald1">
         <label>58</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Fitzgerald</surname><given-names>JD</given-names></name>
               <name name-style="western"><surname>Sharpee</surname><given-names>TO</given-names></name>
            </person-group>
            <year>2009</year>
            <article-title>Maximally informative pairwise interactions in networks.</article-title>
            <source>Phys Rev E</source>
            <volume>80</volume>
            <fpage>031914</fpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Nikitin1">
         <label>59</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Nikitin</surname><given-names>AP</given-names></name>
               <name name-style="western"><surname>Stocks</surname><given-names>NG</given-names></name>
               <name name-style="western"><surname>Morse</surname><given-names>RP</given-names></name>
               <name name-style="western"><surname>McDonnell</surname><given-names>MD</given-names></name>
            </person-group>
            <year>2009</year>
            <article-title>Neural population coding is optimized by discrete tuning curves.</article-title>
            <source>Phys Rev Lett</source>
            <volume>103</volume>
            <fpage>138101</fpage>
         </element-citation>
      </ref>
      <ref id="pcbi.1001111-Sincich2">
         <label>60</label>
         <element-citation publication-type="journal" xlink:type="simple">
            <person-group person-group-type="author">
               <name name-style="western"><surname>Sincich</surname><given-names>LC</given-names></name>
               <name name-style="western"><surname>Adams</surname><given-names>DL</given-names></name>
               <name name-style="western"><surname>Economides</surname><given-names>JR</given-names></name>
               <name name-style="western"><surname>Horton</surname><given-names>JC</given-names></name>
            </person-group>
            <year>2007</year>
            <article-title>Transmission of spike trains at the retinogeniculate synapse.</article-title>
            <source>J Neurosci</source>
            <volume>27</volume>
            <fpage>2683</fpage>
            <lpage>2692</lpage>
         </element-citation>
      </ref>
   </ref-list>
   
</back></article>