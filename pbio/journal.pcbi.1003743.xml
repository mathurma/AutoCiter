<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-01778</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003743</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>No, There Is No 150 ms Lead of Visual Speech on Auditory Speech, but a Range of Audiovisual Asynchronies Varying from Small Audio Lead to Large Audio Lag</article-title>
<alt-title alt-title-type="running-head">Visual Speech Does Not Always Lead Auditory Speech</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname><given-names>Jean-Luc</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Savariaux</surname><given-names>Christophe</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
</contrib-group>
<aff id="aff1"><addr-line>GIPSA-Lab, Speech and Cognition Department, UMR 5216 CNRS Grenoble-Alps University, Grenoble, France</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Vatakis</surname><given-names>Argiro</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Cognitive Systems Research Institute, Greece</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">jean-luc.schwartz@gipsa-lab.grenoble-inp.fr</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: JLS CS. Performed the experiments: JLS CS. Analyzed the data: JLS CS. Contributed reagents/materials/analysis tools: JLS CS. Wrote the paper: JLS CS.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>7</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>31</day><month>7</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>7</issue>
<elocation-id>e1003743</elocation-id>
<history>
<date date-type="received"><day>9</day><month>10</month><year>2013</year></date>
<date date-type="accepted"><day>9</day><month>6</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Schwartz, Savariaux</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>An increasing number of neuroscience papers capitalize on the assumption published in this journal that visual speech would be typically 150 ms ahead of auditory speech. It happens that the estimation of audiovisual asynchrony in the reference paper is valid only in very specific cases, for isolated consonant-vowel syllables or at the beginning of a speech utterance, in what we call “preparatory gestures”. However, when syllables are chained in sequences, as they are typically in most parts of a natural speech utterance, asynchrony should be defined in a different way. This is what we call “comodulatory gestures” providing auditory and visual events more or less in synchrony. We provide audiovisual data on sequences of plosive-vowel syllables (pa, ta, ka, ba, da, ga, ma, na) showing that audiovisual synchrony is actually rather precise, varying between 20 ms audio lead and 70 ms audio lag. We show how more complex speech material should result in a range typically varying between 40 ms audio lead and 200 ms audio lag, and we discuss how this natural coordination is reflected in the so-called temporal integration window for audiovisual speech perception. Finally we present a toy model of auditory and audiovisual predictive coding, showing that visual lead is actually not necessary for visual prediction.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Since a paper was published in this journal, an increasing number of neuroscience papers capitalize on the assumption that visual speech would be typically 150 ms ahead of auditory speech. It happens that the estimation of audiovisual asynchrony in the mentioned paper is valid only in very specific cases, for isolated consonant-vowel syllables or at the beginning of a speech utterance. But the view that vision leads audition is globally oversimplified and often wrong. It should be replaced by the acknowledgement that the temporal relationship between auditory and visual cues is complex, including a range of configurations more or less reflected by the temporal integration window from 30 to 50 ms auditory lead to 170 to 200 ms visual lead. This has important consequences for computational models of audiovisual speech processing in the human brain.</p>
</abstract>
<funding-group><funding-statement>This work was funded by CNRS (<ext-link ext-link-type="uri" xlink:href="http://www.cnrs.fr/" xlink:type="simple">http://www.cnrs.fr/</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="10"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<sec id="s1a">
<title>Early audiovisual interactions in the human brain</title>
<p>Sensory processing has long been conceived as modular and hierarchic, beginning by monosensory cue extraction in the primary sensory cortices before higher level multisensory interactions took place in associative areas, preparing the route for final decision and adequate behavioral answer. However, it is now firmly established that low-level multisensory interactions are much more pervasive than classical views assumed they were and affect brain regions and neural responses traditionally considered as modality specific <xref ref-type="bibr" rid="pcbi.1003743-Ghazanfar1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Driver1">[2]</xref>.</p>
<p>Restricting to audiovisual interactions in speech perception, direct connections have been displayed between primary auditory cortex and primary visual cortex (e.g. <xref ref-type="bibr" rid="pcbi.1003743-Falchier1">[3]</xref> on macaques), and electrophysiological data on speech perception display early influence of the visual component of speech stimuli on auditory evoked response potentials (ERPs). Indeed, there appears a decrease in amplitude and latency of the first negative peak N1 and the second positive peak P2, 100 to 200 ms after the acoustic onset, when the visual component is present <xref ref-type="bibr" rid="pcbi.1003743-Besle1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-VanWassenhove1">[5]</xref>. It is still under debate to determine the specific role of direct connections between primary sensory cortices vs. the role of associative cortex and particularly the superior temporal sulcus in these early interactions <xref ref-type="bibr" rid="pcbi.1003743-Ghazanfar2">[6]</xref>–<xref ref-type="bibr" rid="pcbi.1003743-Arnal2">[8]</xref>.</p>
<p>The computational nature of audiovisual interactions is now the focus of a large number of recent papers. Capitalizing on the natural rhythmicity of the auditory speech input, it has been suggested <xref ref-type="bibr" rid="pcbi.1003743-Lakatos1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Schroeder1">[10]</xref> that the visual input could enhance neuronal oscillations thanks to a phase-resetting mechanism across sensory modalities. This has led to various experimental demonstrations that visual speech improves the tracking of audiovisual speech information in the auditory cortex by phase coupling of auditory and visual cortices <xref ref-type="bibr" rid="pcbi.1003743-Luo1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-ZionGolumbic1">[12]</xref>.</p>
<p>A number of these studies have proposed predictive coding as a possible unifying framework for dealing with audiovisual interactions. Predictive coding posits that neural processing exploits a differential coding between predicted and incoming signals, with decreased activity when a signal is correctly predicted <xref ref-type="bibr" rid="pcbi.1003743-Friston1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Friston2">[14]</xref>. Visual prediction would be responsible for early modifications in auditory ERPs evoked by visual speech decreasing latency and amplitude of N1 and P2 (e.g. <xref ref-type="bibr" rid="pcbi.1003743-VanWassenhove1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Arnal1">[7]</xref>). This has led to recent proposals about the role of specific components in neural oscillations respectively conveying top-down predictions and bottom-up prediction errors in audiovisual speech processing <xref ref-type="bibr" rid="pcbi.1003743-Arnal2">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Arnal3">[15]</xref>.</p>
</sec><sec id="s1b">
<title>The underlying audiovisual structure of speech stimuli</title>
<p>The previously mentioned studies capitalize on the underlying audiovisual structure of speech stimuli, that is the way sounds and sights provided by the speaker are comodulated in time (so that their phase can indeed be coupled) and more generally how one modality provides adequate information for partial prediction of the other modality.</p>
<p>It is actually known since long that the auditory and video streams are related by a high level of cross-predictability related to their common underlying motor cause. This is displayed in a number of studies about audio-visual correlations between various kinds of video (e.g. lip parameters, facial flesh points, video features extracted from the face) and audio (acoustic envelope, band-pass filter outputs, spectral features) parameters <xref ref-type="bibr" rid="pcbi.1003743-Yehia1">[16]</xref>–<xref ref-type="bibr" rid="pcbi.1003743-Berthommier1">[20]</xref>.</p>
<p>In a recent and influential paper published in this journal, Chandrasekaran et al. <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref> present a number of analyses about the “natural statistics of audiovisual speech”, based on various databases in different languages (British and American English, and French), with four major results: firstly, there is a robust correlation in time between variations of mouth opening and variations of the acoustic envelope; secondly, focusing the acoustic envelope to narrow regions in the acoustic spectrum, correlation is maximum in two regions, one around 300–800 Hz, typically where is situated the first vocal tract resonance (formant) F1, and the other around 3000 Hz interpreted by the authors as corresponding to the second and third resonances F2 and F3; thirdly, temporal comodulations of the mouth and acoustic envelope appear in the 2–7 Hz frequency range, typically corresponding to the syllabic rhythm; last but not least in the context of the present paper, “the timing of mouth movements relative to the onset of the voice is consistently between 100 and 300 ms” (penultimate sentence of the paper abstract).</p>
<p>Since the publication of this paper and systematically referring to it, an increasing number of neuroscience papers – including some of those cited previously – capitalize on the assumption that visual speech would be typically 150 ms ahead of auditory speech. Let us mention a few quotations from these papers: “In most ecological settings, auditory input lags visual input, i.e., mouth movements and speech associated gestures, by ∼150 ms” <xref ref-type="bibr" rid="pcbi.1003743-Arnal1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Arnal2">[8]</xref>; “there is a typical visual to auditory lag of 150–200 ms in face-to-face communication” <xref ref-type="bibr" rid="pcbi.1003743-Musacchia1">[22]</xref>; “articulatory facial movements are also correlated with the speech envelope and precede it by ∼150 ms” <xref ref-type="bibr" rid="pcbi.1003743-ZionGolumbic1">[12]</xref>.</p>
<p>The invoked natural audiovisual asynchrony is used in these papers in support to development on models and experiments assessing the predictive coding theory. The assumption that image leads sound plays two different roles in the above mentioned neuroscience papers. It is sometimes used as a trick to demonstrate that the visual stimulus plays a role in modulating the neural auditory response, rightly capitalizing on a situation where a consonant-vowel (CV) sequence (e.g. “pa” or “ta”) is produced after a pause. In this case, the preparatory movement of the mouth and lips is visible before any sound is produced, hence visual prediction can occur ahead of sound and results in visual modulation of auditory ERPs <xref ref-type="bibr" rid="pcbi.1003743-Besle1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-VanWassenhove1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Arnal1">[7]</xref>.</p>
<p>The second role is more problematic. Considering that there would be a systematic and more or less stable advance of vision on audition around 150 ms, it is proposed that this situation would play a role in the ability to use the visual input to predict the auditory one all along the time. Audiovisual asynchrony is implicitly incorporated in a number of models and proposals.</p>
<p>However, as we will see in the next section, the situation studied in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref> is very specific, characteristic of a CV sequence produced in isolation or at the beginning of an utterance after a pause. The objective of the present paper is to show that, while the method proposed by Chandrasekaran et al. to estimate audiovisual delays is adequate for the onset in preparatory sequences or the start of a speech utterance, in chained sequences which actually provide the most general case in speech communication, the method should be modified. Furthermore, if an appropriate method is used, delays actually vary in a different range from the one they propose – with the consequence that “there is no 150 ms lead of visual speech on auditory speech”.</p>
</sec><sec id="s1c">
<title>Preparatory gestures and comodulatory gestures: The hammer and the balloon</title>
<p>The rationale in the measure of asynchrony proposed by Chandrasekaran et al. is based on the notion of <italic>preparatory gestures</italic> (<xref ref-type="fig" rid="pcbi-1003743-g001">Figure 1</xref>). This is also the case of the N1-P2 studies mentioned previously (e.g. <xref ref-type="bibr" rid="pcbi.1003743-VanWassenhove1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Arnal2">[8]</xref>). This can be related to a rather classical analogy, namely the movement of a hammer towards a table (<xref ref-type="fig" rid="pcbi-1003743-g001">Figure 1a</xref>). To produce a sound with a hammer, one must previously realize a downward stroke and the onset of this downward stroke is visible much before the hammer touches the table and makes a sound. Notice that in this scene, one could define actually <italic>two</italic> visible events, one at the onset of the downward stroke and one at the instant when the hammer touches the table; and only <italic>one</italic> auditory event, the sound onset, which is actually perfectly synchronous with the second visual event. The downward stroke may be called a “preparatory gesture” in that it prepares the sound and hence predicts something about it (its time of arrival, and also its acoustic content since a subject looking at the hammer going towards the table knows the kind of sound which will be produced soon).</p>
<fig id="pcbi-1003743-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003743.g001</object-id><label>Figure 1</label><caption>
<title>Preparatory gestures are visible and not audible.</title>
<p>(a) A preparatory gesture for a hammer hitting a table. (b) A preparatory gesture for a labial burst after a pause.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003743.g001" position="float" xlink:type="simple"/></fig>
<p>It is exactly the same for preparatory lip gestures before “p” at the beginning of a speech utterance (<xref ref-type="fig" rid="pcbi-1003743-g001">Figure 1b</xref>): when the lips begin to close, a subject looking at the speaker knows that they will soon join together for a lip closure, and she/he can predict rather accurately when will sound occur and what will be its spectrum (the typical flat low-frequency spectrum of a bilabial burst <xref ref-type="bibr" rid="pcbi.1003743-Blumstein1">[23]</xref>). Here again, there are two visual events, namely the onset of the lip closing gesture and the further onset of the lip opening gesture, and only one auditory event, the burst onset, quite synchronous with the second visual event. Notice that the analogy between the preparatory gestures for the hammer and for speech is not perfect. Indeed, the sound is produced by the hammer at the end of the downward stroke, while for speech the lips must open again. There is actually a complex coordination between larynx, lungs and lips to achieve the adequate aerodynamic strategy <xref ref-type="bibr" rid="pcbi.1003743-Lfqvist1">[24]</xref>, which fixes rules about the duration of lip closure before lip opening. But the audiovisual asynchrony involved in preparatory gestures for both hammer and speech are similar: in both cases, audiovisual asynchrony is assessed by the duration between two <italic>different</italic> events, the onset of the preparatory gesture for the visual channel and its offset for the auditory channel.</p>
<p>Therefore it appears that the crucial aspect of preparatory gestures is that they are visible but produce no sound. This could be different, actually. Consider for example what happens if you replace the hammer by a whip or a flexible stick. Now the downward stroke produces a whistling sound (which also predicts the sound produced when the whip or stick touches the table). There are now <italic>two</italic> auditory events, just as there are two visual events, and for both pairs of audiovisual events (at the beginning and end of the visual stroke) the auditory and visual events are quite in synchrony.</p>
<p>This leads us towards another kind of gestures that we propose to call “comodulatory gestures” since these gestures produce both auditory and visual stimuli more or less in synchrony all along the time (<xref ref-type="fig" rid="pcbi-1003743-g002">Figure 2</xref>). Comodulatory gestures are actually by far the most common gestures in speech. Here we should move towards another analogy that is a balloon in which one adjusts the mouthpiece. When its size increases or decreases, shape, volume and pressure change leading to more or less synchronous auditory and visual events for both opening and closing phases (<xref ref-type="fig" rid="pcbi-1003743-g002">Figure 2a</xref>), just as opening and closing the lips while vocalizing produces auditory and visible events quite in synchrony (<xref ref-type="fig" rid="pcbi-1003743-g002">Figure 2b</xref>).</p>
<fig id="pcbi-1003743-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003743.g002</object-id><label>Figure 2</label><caption>
<title>Comodulatory gestures are visible and audible.</title>
<p>(a) A comodulatory closing/opening gesture for a balloon. (b) A comodulatory closing/opening gesture for lips in speech communication.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003743.g002" position="float" xlink:type="simple"/></fig></sec><sec id="s1d">
<title>Objectives of this paper</title>
<p>In the remaining of this paper we present simple audiovisual data on plosive-vowel syllables (pa, ta, ka, ba, da, ga, ma, na), produced either in isolation or in sequence. We show that when syllables are produced in isolation, preparatory gestures provide audiovisual asynchronies quite in line with those measured in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref>. However, when syllables are chained in sequences, they provide comodulatory gestures in which audiovisual synchrony is actually precise, contrary to the data provided on similar sequences in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref>, just because the measure of audiovisual asynchrony is different. In such cases, there are actually auditory events that were not taken into account in the original paper, and these need to be taken into account if one is talking about asynchrony.</p>
<p>After presenting Methodology and Results, we discuss how natural coordination between sound and image can actually produce both cases of lead and lag of the visual input. We relate the range of leads and lags to the so-called temporal integration window for audiovisual speech perception <xref ref-type="bibr" rid="pcbi.1003743-VanWassenhove2">[25]</xref>. We propose that the “visual lead” hypothesis, wrong in many cases, is actually not necessary to deal with audiovisual predictability, and we illustrate this by briefly introducing a simple audiovisual prediction model dealing with the speech sequences studied previously. We conclude by some methodological and theoretical remarks on neurophysiological developments about audiovisual predictability in the human brain.</p>
</sec></sec><sec id="s2" sec-type="methods">
<title>Methods</title>
<sec id="s2a">
<title>Data</title>
<p>In the experimental work we focus on audiovisual temporal relationships in CV sequences where C is a voiced, unvoiced or nasal stop consonant that is, for English or French (the two languages considered in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref>), one of the sounds /p t k b d g m n/, and V is the open vowel /a/. We consider both CV sequences produced in isolation and chained sequences VCVCVCV. This corpus is very simple though sufficient to illustrate the difference between preparatory gestures – for isolated syllables – and comodulatory gestures – for chained syllables. The /a/ context in which the plosives /p t k b d g m n/ are produced is selected because it provides a large gesture amplitude providing more salient trajectories both in the visual and auditory modality. We will consider more general phonetic material in the discussion.</p>
<p>We recorded a small database of 6 repetitions of 8 syllables /pa ta ka ba da ga ma na/ uttered by a French speaker either in isolation /Ca/ or in sequence /aCa/. The syllables were produced in a fixed order at a relatively slow rhythm (around 800 ms per syllable). In the “isolated syllables” condition, syllables were embedded in silence: /pa#ta#ka#ba#da#ga#ma#na/ where /#/ means a silence (typically 500 ms silence between two consecutive syllables). In the “chained syllables” condition, they were produced in the same order though with no silence between syllables: /apatakabadagamana/.</p>
<p>The recording was done with a PAL camera at 50 Hz. The recording set up was based on the classical paradigm we use in Grenoble since years <xref ref-type="bibr" rid="pcbi.1003743-Lallouache1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Noiray1">[27]</xref> with blue make up applied on the lips. For each image, we extracted automatically and precisely the lip contours by applying a Chroma Key process extracting blue areas on the face. The lips parameters were extracted every 20 ms, synchronously with the acoustic signal, which is sampled at 22.05 kHz.</p>
</sec><sec id="s2b">
<title>Analysis</title>
<sec id="s2b1">
<title>Detection of auditory and visual events</title>
<p>Then on each CV utterance of this database we labeled auditory and visual events.</p>
<p>The acoustic analysis was done with Praat <xref ref-type="bibr" rid="pcbi.1003743-Boersma1">[28]</xref>. The first formant was extracted after a Linear Predictive Coding (LPC) analysis. A typical display of the synchronized acoustic signal with its time-frequency analysis (including intensity and formants) and lip trajectory (one measure every 20 ms) is presented on <xref ref-type="fig" rid="pcbi-1003743-g003">Figure 3a</xref> for an isolated /pa/ and <xref ref-type="fig" rid="pcbi-1003743-g003">Figure 3b</xref> for a /pa/ chained in a sequence (with a zoom around the consonant /p/ in /apa/).</p>
<fig id="pcbi-1003743-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003743.g003</object-id><label>Figure 3</label><caption>
<title>Acoustic signal (top panel), intensity in green, lip height in blue and formants in red (bottom panel): For isolated /pa/ (a, left) and /apa/ (b, right).</title>
<p>Blue arrows: lip events. Green arrows: intensity events. Red arrows: formant events. CAF/OAF (red): Closing/Opening onset for Audio Formant. CAI/OAI (green): Closing/Opening onset for Audio Intensity. CVL/OVL (blue): Closing/Opening onset for Visible Lips.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003743.g003" position="float" xlink:type="simple"/></fig>
<p>On such kinds of displays we detected auditory and visual events corresponding to the onset of opening or closing gestures, with criteria based on given ranges of energy decrease/increase – 1 dB –, formant decrease/increase – 60 Hz – or lip height decrease/increase – 0.15 cm – from previous minimal or maximal values. For the detection of visual events, considering the rather low sampling frequency at 50 Hz and since lip opening may be rather quick, specifically for bilabials, we applied linear interpolation between lip height values at two consecutive images to refine event detection. We labelled the corresponding events:</p>
<list list-type="simple"><list-item>
<p>on the acoustic signal, in the case of chained sequences (<xref ref-type="fig" rid="pcbi-1003743-g003">Figure 3b</xref>): the beginning of the decrease of the first formant F1 in the portion from the previous “a” to the next plosive (Closing onset for Audio Formant: CAF); the corresponding beginning of intensity decrease (Closing onset for Audio Intensity: CAI). And in all cases, for chained as well as isolated sequences, the beginning of F1 increase in the portion from the plosive to the next “a” (Opening onset for Audio Formant: OAF) and the corresponding beginning of intensity increase, that is the burst onset (Opening onset for Audio Intensity: OAI).</p>
</list-item><list-item>
<p>on the lip trajectory, in all cases: the beginning of lip area decrease in the portion from the previous “a” or from silence to the next plosive (Closing onset for Visible Lips: CVL) and the beginning of lip area increase at the plosive release towards the next vowel (Opening onset for Visible Lips: OVL).</p>
</list-item></list>
</sec><sec id="s2b2">
<title>Estimation of audiovisual asynchrony</title>
<p>Estimation of audiovisual temporal relationship is done differently for preparatory gestures (isolated sequences) and comodulatory gestures (chained sequences).</p>
<p>For isolated syllables such as /pa/ (<xref ref-type="fig" rid="pcbi-1003743-g004">Figure 4a</xref>), lips first close to prepare the “p”. This involves a visible gesture described in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref> by two temporal events, the initiation of the closing gesture, and the velocity peak of the lips during the closure phase (down blue arrow in <xref ref-type="fig" rid="pcbi-1003743-g004">Figure 4a</xref>). Then comes the release, which corresponds to a third visible event that is an opening onset (up blue arrow in <xref ref-type="fig" rid="pcbi-1003743-g004">Figure 4a</xref>, not discussed by the authors) and to the first auditory event that is the acoustic burst for the plosive (up red arrow in <xref ref-type="fig" rid="pcbi-1003743-g004">Figure 4a</xref>). Of course, the first visible event (closure gesture initiation, down blue arrow) and the first auditory event (opening gesture initiation, up red arrow) are asynchronous, since closure must occur before opening. Asynchrony is described in this case between the first visible event and the first auditory event, providing the same measure in our study (AV asynchro (Sc) in <xref ref-type="fig" rid="pcbi-1003743-g004">Figure 4</xref>, with Sc for Schwartz and Savariaux) and in Chandrasekaran et al. (AV asynchro (Ch)). The temporal distance may reach 150 ms or even more: actually lips can close any time before they open (imagine you want to stop your interlocutor by uttering “please”, you prepare the “p” but don't succeed to interrupt him or her: you will stay with your lips closed for a while, and the temporal delay between visible lip closing and audible burst may reach very large values).</p>
<fig id="pcbi-1003743-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003743.g004</object-id><label>Figure 4</label><caption>
<title>Sequence of auditory and visual events and measure of audiovisual asynchrony in isolated “pa” (top) and “pa” chained in a sequence “apa” (bottom).</title>
<p>AV asynchro (Ch) refers to the AV asynchrony measure used in <xref ref-type="bibr" rid="pcbi.1003743-Ghazanfar1">[1]</xref>, AV asynchro (Sc) refers to the AV asynchrony measure used in the present paper.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003743.g004" position="float" xlink:type="simple"/></fig>
<p>For chained sequences such as “apa” (<xref ref-type="fig" rid="pcbi-1003743-g004">Figure 4b</xref>), lips closure is both visible (down blue arrow in <xref ref-type="fig" rid="pcbi-1003743-g004">Figure 4b</xref>) and audible since it changes the formants (acoustic resonances of the vocal tract) and the intensity of the sound (down red arrow in <xref ref-type="fig" rid="pcbi-1003743-g004">Figure 4b</xref>). At the end of the closing gesture the sound stops (or changes into intervocalic voicing in the case of “aba”). In such cases it is mistaken to characterize audiovisual coordination as the delay between closing gesture initiation for vision (down blue arrow) and opening gesture initiation for audition (up red arrow) – though this is what Chandrasekaran et al. do in their Figure 9 – because there is actually an audible and a visible event for both closure gesture (down blue and red arrows) and opening gesture initiation (up blue and red arrows). This provides therefore different measures of asynchrony in our study and in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref>.</p>
<p>Altogether this results in completely different estimations of audiovisual asynchrony for preparatory gestures (<xref ref-type="fig" rid="pcbi-1003743-g004">Figure 4a</xref>) and comodulatory gestures (<xref ref-type="fig" rid="pcbi-1003743-g004">Figure 4b</xref>). Of course one could argue that it is better to use the same measure for asynchrony in all situations, but the measure used in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref> in the case of chained sequences – actually corresponding to what happens in most of continuous speech – is inappropriate since it forgets audible events in the closing phase.</p>
</sec></sec></sec><sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Isolated syllables: Confirming <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref> for preparatory gestures</title>
<p>We display on <xref ref-type="fig" rid="pcbi-1003743-g005">Figure 5</xref> the data for isolated syllables. In this case, where there is no audible event for closure, we report the same measure as in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref> that is the delay between the first visible event, CVL, and the first audible event, OAI or OAF. There is a very large anticipation, which actually reaches values much larger than 150 ms here (and which may reach 400 ms in some cases). These values are compatible with the range 100-300 ms proposed in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref>, the more so considering that the measure used by the authors for detecting visual events (half open point in the lip closing trajectory, while we used the onset of the closing phase) would produce values lower than the ones in <xref ref-type="fig" rid="pcbi-1003743-g005">Figure 5</xref>.</p>
<fig id="pcbi-1003743-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003743.g005</object-id><label>Figure 5</label><caption>
<title>Delay between the first visual event (for the closing phase) and the first auditory event (for the opening phase) in isolated /Ca/.</title>
<p>Negative values mean that the acoustic event lags the visual one. In red: acoustic events for formants. In green: acoustic events for intensity. Signs point at mean values (over the 6 repetitions), and error bars correspond to the standard deviation.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003743.g005" position="float" xlink:type="simple"/></fig></sec><sec id="s3b">
<title>Chained syllables: Infirming <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref> for comodulatory gestures</title>
<p>We display on <xref ref-type="fig" rid="pcbi-1003743-g006">Figure 6</xref> typical audiovisual sequences for all types of chained syllables (with a zoom around the consonant). It clearly shows that there is comodulation of the auditory and visual information, with audible and visible events for both closing and opening phases. The event detection is sometimes not straightforward or not very precise in time (e.g. detection of CAI for /ata/ or /ada/), which is quite classical in this type of stimuli, and gross trends are more important that precise values in the following.</p>
<fig id="pcbi-1003743-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003743.g006</object-id><label>Figure 6</label><caption>
<title>Acoustic signal (top panel), intensity in green, lip height in blue and formants in red for the 8 chained sequences.</title>
<p>Blue arrows: lip events (CVL/OVL: Closing/Opening onset for Visible Lips). Green arrows: intensity events (CAI/OAI: Closing/Opening onset for Audio Intensity). Red arrows: formant events (CAF/OAF: Closing/Opening onset for Audio Formant). (a) /apa/; (b) /ata/; (c) /aka/; (d) /aba/; (e) /ada/; (f) /aga/; (g) /ama/; (h) /ana/.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003743.g006" position="float" xlink:type="simple"/></fig>
<p>We display on <xref ref-type="fig" rid="pcbi-1003743-g007">Figure 7</xref> the data about temporal coordination between audio and visual events for either closing (<xref ref-type="fig" rid="pcbi-1003743-g007">Figure 7a</xref>) or opening (<xref ref-type="fig" rid="pcbi-1003743-g007">Figure 7b</xref>) in the case of chained sequences. The mean delay between visual and acoustic events at the closure (in the /aC/ portion, <xref ref-type="fig" rid="pcbi-1003743-g007">Figure 7a</xref>) varies between −20 ms and −40 ms for intensity (CVL-CAI, in green) and reaches values from −40 to −80 ms for formants (CVL-CAF, in red). This means that there is a small lead of the visual channel compared to the audio channel (where information is available on intensity before formants). But this lead is much smaller than the 150 ms lead mentioned in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref>, and there are actually cases where audio and video information are available more or less in synchrony, e.g. for /ad/, /ag/ or /ak/ where the tongue gesture towards the voiced plosive decreases intensity or formants while jaw may stay rather stable, and hence lip area does not decrease much – which prevents early video detection.</p>
<fig id="pcbi-1003743-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003743.g007</object-id><label>Figure 7</label><caption>
<title>Delay between visual and auditory events: (a) in the closing phase, in /aC/ where C is a plosive in the set /p t k b d g m n/; (b) in the opening phase, in /Ca/ with the same plosives.</title>
<p>Positive values mean that the acoustic event leads the visual one. In red: acoustic events for formants. In green: acoustic events for intensity. Signs point at mean values (over the 6 repetitions), and error bars correspond to the standard deviation.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003743.g007" position="float" xlink:type="simple"/></fig>
<p>In the opening phase (<xref ref-type="fig" rid="pcbi-1003743-g007">Figure 7b</xref>) the synchrony is even larger. Concentrating on the delay between labial and intensity events (OVL-OAI, in green) we actually observe an almost perfect synchrony for labials (/p b m/). This is trivial: as soon as the lips begin to open, the sound drastically changes, from silence (for /p/) or prevoicing (for /b/) or nasal murmur (for /m/) to the plosive burst. For velars /k g/ there is actually a clear lead of the audio channel, since the first tongue movement producing the plosive release is done with no jaw movement at all and hence before any labial event is actually detectable: the audio lead may reach more than 20 ms (see examples in <xref ref-type="fig" rid="pcbi-1003743-g006">Figure 6</xref>). Notice that while the video sampling frequency at 50 Hz can make the detection of the opening event for bilabials a bit imprecise with a precision around 10 ms for very quick gestures, the variations of lip area for dentals or velars is smooth and hence imprecision in event detection cannot explain such an audio lead.</p>
<p>Therefore the discrepancy with <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref> is clear for chained syllables, just because this corresponds to what we called comodulatory gestures, for which we argue that a different measure of the audiovisual asynchrony should be used.</p>
</sec></sec><sec id="s4">
<title>Discussion</title>
<sec id="s4a">
<title>Summary of the experimental results</title>
<p>The experimental results presented previously show that for isolated syllables associated with preparatory gestures, our measure of audiovisual asynchrony provides quantitative estimates from 200 ms to 400 ms of visual lead (<xref ref-type="fig" rid="pcbi-1003743-g005">Figure 5</xref>). This is in line with the 100 to 300 ms visual lead proposed in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref>, the more so considering that the estimate of the visible onset for lip closure in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref> is done at the mid closing phase – while we prefer detecting the first visible event that is at the very beginning of the lip closure phase, typically 100 ms before. The coherence of both sets of measures was expected considering that the same definition of asynchrony for preparatory gestures is used in both papers, between the first visible event (onset of lip closing phase) and the first auditory event (plosive burst at labial release).</p>
<p>However the data are quite different for chained sequences associated with comodulatory gestures. In this case the range of asynchronies is much more restricted and more centered around 0, from 70 ms visual lead to 20 ms audio lead when auditory events are detected on intensity, auditory events detected on the formant trajectory being somewhat delayed in respect to intensity (<xref ref-type="fig" rid="pcbi-1003743-g007">Figure 7</xref>). Mean video lead amounts to 35 ms in the closing phase and 0 ms in the opening phase for intensity, 60 ms in the closing phase and less than 10 ms in the opening phase for formants. Therefore the departure between our data and those proposed in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref> is now important. This is not due to variability in the speech material, but to a difference in the measure proposed for assessing audiovisual asynchrony. As explained in <xref ref-type="fig" rid="pcbi-1003743-g004">Figure 4</xref>, the measures differ hence their results also differ. Speech gestures in chained sequences typically produce both auditory and visual events all along the time (see <xref ref-type="fig" rid="pcbi-1003743-g006">Figure 6</xref>) hence resulting in a rather precise audiovisual synchrony in most cases.</p>
</sec><sec id="s4b">
<title>The range of possible AV asynchronies in human speech</title>
<p>Preparatory gestures do exist in speech communication, and ERP studies rightly capitalized on this experimental situation in which the gap between the first visible and the first auditory event may be quite large and able to lead to significant influence of the visual input on the electrophysiological response in the auditory cortex, for both speech <xref ref-type="bibr" rid="pcbi.1003743-VanWassenhove1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Arnal2">[8]</xref> and non-speech stimuli <xref ref-type="bibr" rid="pcbi.1003743-Stekelenburg1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Vroomen1">[30]</xref>. Notice that this may actually depend on the prephonatory configuration: if somebody keeps the lips closed while listening to the interlocutor, there will actually be no preparatory gesture before an initial bilabial sound such as /b/ or /m/, and hence there will be no visual lead at all in this case. One could even imagine a reverse situation in which a speaker keeps the lips closed and systematically signals her/his turn taking by a backchannel signal “hmm” (which is not rare): in this case the preparatory gesture would be actually audible and not visible, leading to an auditory lead in the preparatory phase.</p>
<p>However, most of the speech material is made of comodulatory gestures. Of course, speech utterances involve a range of phonetic configurations much larger than the /Ca/ sequences that were studied in this paper. This variety of configurations leads to a variety of situations in terms of audiovisual asynchronies. This is where the analogy we proposed previously with the deflating balloon being both audible and visible reaches some limits: actually, not every action realized on the vocal tract is always either audible or visible, which may lead to delays between perceivable auditory or visible cues for a given speech gesture.</p>
<p>A first general property of speech concerns anticipatory coarticulation – much more relevant and general than preparatory movements discussed in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref>. This relates to articulatory gestures towards a given phonetic target, which can begin within a previous phoneme. Anticipatory coarticulation generally capitalizes on a property of the articulatory-to-acoustic transform, in which an articulatory gesture has sometimes no or weak effect on the sound and hence can be prepared in advance without audible consequences.</p>
<p>A typical example concerns the rounding gesture from /i/ to /y/ or /u/ in sequences such as /iC<sub>1</sub>C<sub>2</sub>…C<sub>n</sub>y/ or /iC<sub>1</sub>C<sub>2</sub>…C<sub>n</sub>u/ with a variable number of consonants C<sub>1</sub>…C<sub>n</sub> not involving a specific labial control (e.g. /s t k r/) between the unrounded /i/ and the rounded /y/ or /u/. In this case the rounding gesture from /i/ towards /y/ or /u/ can begin within the sequence of consonants /C<sub>1</sub>C<sub>2</sub>…C<sub>n</sub>/, and hence anticipate the vowel by 100 to 300 ms <xref ref-type="bibr" rid="pcbi.1003743-Abry1">[31]</xref>. Various sets of data and various theoretical models of this anticipatory coarticulation process have been proposed in the literature <xref ref-type="bibr" rid="pcbi.1003743-Benguerel1">[32]</xref>–<xref ref-type="bibr" rid="pcbi.1003743-Roy1">[36]</xref>. In such cases the rounding gesture can hence be visible well before it is audible.</p>
<p>So there are cases where visible information is available before auditory information (e.g. in /iC<sub>1</sub>…C<sub>n</sub>u/ sequences), others where vision and audition are quite synchronous (e.g. in /aCa/ sequences), and there are also cases where audition may actually lead vision as was shown e.g. in <xref ref-type="fig" rid="pcbi-1003743-g007">Figure 7</xref>. But the next question is to know if the auditory and visual systems are able to process the information efficiently as soon as it is available. This is actually not always the case, and in gating experiments on the visual vs. auditory identification of coarticulated sequences, Troille et al. <xref ref-type="bibr" rid="pcbi.1003743-Troille1">[37]</xref> display in some configurations a lead of audition on vision which can reach up to 40 ms, because of the poor visibility of some articulatory gestures. This leads the authors to claim that they have discovered a case where “speech can be heard before it is seen”.</p>
<p>In summary, there are actually a variety of situations from audio lead (estimated to 40 ms in <xref ref-type="bibr" rid="pcbi.1003743-Troille1">[37]</xref>) to visual lead (which can reach more than 200 ms). In their study of mutual information between audio and video parameters on speech sequences, Feldhoffer et al. <xref ref-type="bibr" rid="pcbi.1003743-Feldhoffer1">[38]</xref> show that mutual information is maximal for some audio and video parameters when it incorporates a video lead up to 100 ms. In audiovisual speech recognition experiments, Czap <xref ref-type="bibr" rid="pcbi.1003743-Czap1">[39]</xref> obtains a smaller value, recognition scores being higher with a small global video lead (20 ms). Altogether, these global estimations are concordant with the classical view that “in average, the visual stream may lead the auditory stream”, which is generally advocated by specialists of audiovisual speech perception (e.g. <xref ref-type="bibr" rid="pcbi.1003743-Bernstein1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Kim1">[41]</xref>). However, the “average” view hides a large range of variations, typically inside a window between 40 ms audio lead to 200 ms visual lead in the phonetic content of normal speech communication.</p>
</sec><sec id="s4c">
<title>Plausible consequences for the temporal integration window for AV speech in the human brain</title>
<p>A large number of recent studies have attempted to characterize the temporal integration window in various kinds of multisensory interactions. This typically involves two kinds of paradigms. Firstly, evaluation of intersensory synchrony may be based on either simultaneity or temporal order judgment tasks (see a recent review in <xref ref-type="bibr" rid="pcbi.1003743-Vroomen2">[42]</xref>). Secondly, the “multisensory temporal binding window” describes the range of asynchronies between two modalities in which a fused percept may emerge <xref ref-type="bibr" rid="pcbi.1003743-Stevenson1">[43]</xref>.</p>
<p>The “audiovisual temporal integration window” is well described for speech perception (e.g. <xref ref-type="bibr" rid="pcbi.1003743-Massaro1">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Munhall1">[45]</xref>). Van Wassenhove et al. <xref ref-type="bibr" rid="pcbi.1003743-VanWassenhove2">[25]</xref> compared estimates of audiovisual temporal integration window based on either simultaneity perceptual judgments or regions where the McGurk effect seems to stay at a maximal value. They show that these various estimates converge on an asymmetric window between about 30 ms audio lead and 170 ms audio lag.</p>
<p>This provides a set of values rather coherent with the range of possible asynchronies in the speech material itself. Small audio leads may occur because of the lack of visibility of certain audible gestures, as shown in <xref ref-type="fig" rid="pcbi-1003743-g007">Figure 7</xref> or in gating experiments <xref ref-type="bibr" rid="pcbi.1003743-Troille1">[37]</xref>. Large video leads are mostly due to labial anticipatory coarticulation and described in many studies <xref ref-type="bibr" rid="pcbi.1003743-Abry1">[31]</xref>–<xref ref-type="bibr" rid="pcbi.1003743-Roy1">[36]</xref>. A tentative interpretation is that the perceptual system has internalized this range through a learning process. This is in line with the so-called “unity assumption” <xref ref-type="bibr" rid="pcbi.1003743-Welch1">[46]</xref> according to which subjects would naturally bind together multisensory stimuli referring to a common cause, which would lead to both fused percepts and decreased ability to detect temporal asynchronies <xref ref-type="bibr" rid="pcbi.1003743-Vatakis1">[47]</xref>. We speculate that unity assumption is based on a statistical learning of the comodulation properties of the auditory and visual streams in the speech natural environment, naturally providing an asymmetrical window around the range [−30 ms, +170 ms].</p>
<p>The asymmetry of the temporal integration window has been the topic of much discussion – including assumptions about the difference between optic and acoustic wave speeds, which cannot however explain such a large asymmetry: a speaker 10 m apart from a listener would not provide more than 30 ms visual advance! We argue here that the psychophysical asymmetry just mirrors the natural phonetic asymmetry, according to which there are plenty of cases of large visual anticipation due to coarticulation – typically in the 100 to 200 ms range – and less cases of auditory anticipation, in a smaller range – typically less than 40 ms as displayed in our data in <xref ref-type="fig" rid="pcbi-1003743-g007">Figure 7</xref> or in gating data <xref ref-type="bibr" rid="pcbi.1003743-Vatakis1">[47]</xref>. But, once again, this does not mean that there is a constant visual lead, but rather a range of audiovisual asynchronies mirrored in the temporal integration window.</p>
<p>Recent data on the development of the audiovisual temporal integration window fit rather well with this proposal. Indeed, these data show that the window is initially quite large and then progressively refined by “perceptual narrowing” in the first months of life <xref ref-type="bibr" rid="pcbi.1003743-Lewkowicz1">[48]</xref>. The window actually appears to stay rather wide and symmetrical until at least 11 years of age <xref ref-type="bibr" rid="pcbi.1003743-Hillock1">[49]</xref>. It is only after this age that the left part of the window (for auditory lead) refines from 200 ms to 100 ms, which is proposed by the authors as the typical value for adults (the fact that these values are larger than in <xref ref-type="bibr" rid="pcbi.1003743-VanWassenhove2">[25]</xref> likely comes from the use of a different criterion to define binding windows from simultaneity curves). On the contrary, the right part of the window stays stable. The interpretation is that the large initial symmetric window [−200 ms, +200 ms] is progressively tuned to the window characteristic of the speech input, asymmetric in nature. The fact that learning the asymmetrical pattern occurs so late may appear surprising, but it is in fact compatible with data showing that the maturation of the McGurk effect is not complete before at least 8 years of age for native stimuli and even later for non-native stimuli <xref ref-type="bibr" rid="pcbi.1003743-Sekiyama1">[50]</xref>.</p>
<p>There is also a rather large deal of variations of audiovisual temporal integration window from one subject to another <xref ref-type="bibr" rid="pcbi.1003743-Stevenson1">[43]</xref>. These variations respect the asymmetry trend, though with large variations in quantitative values. The fact that these variations are correlated with the results of various fusion paradigms suggests that inter-individual differences could be related with specific weights attributed by subjects to one or the other modality <xref ref-type="bibr" rid="pcbi.1003743-Giard1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Schwartz1">[52]</xref>. Interestingly, it also appears a large ability to tune and decrease the integration window with auditory or visual experience <xref ref-type="bibr" rid="pcbi.1003743-Powers1">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Petrini1">[54]</xref>, including the possibility to decrease the asymmetry and specifically decrease the large visual-lead part of the window, which suggests that the integration window actually combines stimulus-driven content with individually-tuned perceptual experience.</p>
</sec><sec id="s4d">
<title>AV predictability without AV asynchrony</title>
<p>The data recalled in the previous section rule out over-simplistic claims about audiovisual predictability. Does it raise a problem for predictability in general? The answer is clearly <italic>no</italic>. The reason is that predictability does <italic>not</italic> require asynchrony. Actually, a pure auditory trajectory may provide predictions on its future stages, and the visual input may enhance these predictions, since it is naturally in advance on <italic>future</italic> auditory events, though not systematically in advance on <italic>present</italic> ones. This is illustrated on the toy model presented in <xref ref-type="bibr" rid="pcbi.1003743-Schwartz2">[55]</xref> and sketchily introduced here under (see a detailed presentation in the Supplementary <xref ref-type="supplementary-material" rid="pcbi.1003743.s004">Text S1</xref>).</p>
<p>The model was developed for dealing with a corpus of repetitions of sequences /aba/, /ada/ and /aga/ uttered by a male French speaker. A predictive coding model was developed to provide guesses about the closure point of the acoustic trajectory /aC/ (with C one of the plosives /b, d, g/) from a given point of the trajectory. We implemented such a model within a Bayesian probabilistic framework, comparing predictions provided by audio-alone inputs with predictions provided by audiovisual inputs.</p>
<p>Importantly, audiovisual inputs were shown to produce better predictions, providing values closer to the actual endpoint than with audio-only inputs. This shows that the visual component provides information able to improve predictions. This toy model is of course highly oversimplified in respect to what should be a reliable system dealing with the whole complexity of speech. However it presents the interest to show that the visual input may strongly improve predictions, in spite of the close synchrony of basic temporal events in the auditory and visual streams, according to the data presented in the Results section. In a word, there is no theoretical requirement for visual lead to argue that visual predictive coding could be at work in the sensory processing of speech in the human brain.</p>
</sec><sec id="s4e">
<title>Concluding remarks</title>
<p>The impressive advances of neurosciences on the processing of speech in the human brain, sometimes simplify the <italic>complexity of speech</italic>, and miss or forget a number of evidence and facts known from long by phoneticians – on the structure of phonetic information, on the auditory and visual cues, on some major principles of speech perception and production. In consequence, there is a serious risk that these advances oversimplify “much of the known complexity of speech as [it] is spoken and of speakers as they speak” <xref ref-type="bibr" rid="pcbi.1003743-Cummins1">[56]</xref>.</p>
<p>This paper attempts to make clear that the view that vision leads audition is globally oversimplified and often wrong. It should be replaced by the acknowledgement that the temporal relationship between auditory and visual cues is complex, including a range of configurations more or less reflected by the temporal integration window from 30 to 50 ms auditory lead to 170 to 200 ms visual lead.</p>
<p>It is important to recall that fortunately, this caveat does not put in question the experimental studies that capitalized on the presumed “150-ms video lead” to assess audiovisual interactions in EEG or MEG data. Indeed, all these studies (e.g. <xref ref-type="bibr" rid="pcbi.1003743-Besle1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-VanWassenhove1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003743-Arnal1">[7]</xref>) used isolated plosive-vowel syllables for which the preparatory visual movement is actually realized without any audio counterpart, hence producing a clear visual anticipation (see <xref ref-type="fig" rid="pcbi-1003743-g005">Figure 5</xref>).</p>
<p>But the pervasive message linking visual lead and visual prediction within a predictive coding stance needs some refinement. Actually, as shown in the last part of this paper, audiovisual predictability does not require audiovisual asynchrony. The development of realistic computational proposals for assessing auditory and audiovisual prediction coding models in speech perception is a challenge for future work in cognitive neuroscience. For this perspective, precise knowledge of the natural statistics of audiovisual speech is a pre-requisite. A number of useful and important data and principles were provided in <xref ref-type="bibr" rid="pcbi.1003743-Chandrasekaran1">[21]</xref>, though the last of its four conclusions needed some refinement. The present paper hopefully contributed to enhance the available knowledge about the complexity of human speech.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003743.s001" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003743.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p>Trajectories of /ab/, /ad/, /ag/ in the F2–F3 plane.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003743.s002" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003743.s002" position="float" xlink:type="simple"><label>Figure S2</label><caption>
<p>Variations of lip aperture for /ab/, /ad/, /ag/.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003743.s003" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003743.s003" position="float" xlink:type="simple"><label>Figure S3</label><caption>
<p>Variations of C<sub>efficiency</sub> for the 4 prediction models. Mean values in solid lines, maximum and minimum values in dotted lines, for each prediction model (see text).</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003743.s004" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pcbi.1003743.s004" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p>AV predictability without AV asynchrony: a toy model for audio and audiovisual predictive coding of /aCa/ trajectories.</p>
<p>(DOCX)</p>
</caption></supplementary-material></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1003743-Ghazanfar1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name>, <name name-style="western"><surname>Schroeder</surname><given-names>CE</given-names></name> (<year>2006</year>) <article-title>Is neocortex essentially multisensory?</article-title> <source>Trends Cogn Sci</source> <volume>10</volume>: <fpage>278</fpage>–<lpage>285</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Driver1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Driver</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Noesselt</surname><given-names>T</given-names></name> (<year>2008</year>) <article-title>Multisensory interplay reveals crossmodal influences on ‘sensory specific’ brain regions, neural responses, and judgments</article-title>. <source>Neuron</source> <volume>57</volume>: <fpage>11</fpage>–<lpage>23</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Falchier1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Falchier</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Clavagnier</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Barone</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Kennedy</surname><given-names>H</given-names></name> (<year>2002</year>) <article-title>Anatomical evidence of multimodal integration in primate striate cortex</article-title>. <source>J Neurosci</source> <volume>22</volume>: <fpage>5749</fpage>–<lpage>5759</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Besle1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Besle</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Fort</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Delpuech</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Giard</surname><given-names>M-H</given-names></name> (<year>2004</year>) <article-title>Bimodal Speech: Early Visual Effect in the Human Auditory Cortex</article-title>. <source>Eur J Neurosci</source> <volume>20</volume>: <fpage>2225</fpage>–<lpage>2234</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-VanWassenhove1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Wassenhove</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Grant</surname><given-names>KW</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2005</year>) <article-title>Visual speech speeds up the neural processing of auditory speech</article-title>. <source>PNAS</source> <volume>102</volume>: <fpage>1181</fpage>–<lpage>1186</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Ghazanfar2"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name>, <name name-style="western"><surname>Chandrasekaran</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name> (<year>2008</year>) <article-title>Interactions between the Superior Temporal Sulcus and Auditory Cortex Mediate Dynamic Face/ Voice Integration in Rhesus Monkeys</article-title>. <source>J Neurosci</source> <volume>28</volume>: <fpage>4457</fpage>–<lpage>4469</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Arnal1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arnal</surname><given-names>LH</given-names></name>, <name name-style="western"><surname>Morillon</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Kell</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Giraud</surname><given-names>A-L</given-names></name> (<year>2009</year>) <article-title>Dual neural routing of visual facilitation in speech processing</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>13445</fpage>–<lpage>53</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Arnal2"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arnal</surname><given-names>LH</given-names></name>, <name name-style="western"><surname>Wyart</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Giraud</surname><given-names>A-L</given-names></name> (<year>2011</year>) <article-title>Transitions in neural oscillations reflect prediction errors generated in audiovisual speech</article-title>. <source>Nat Neurosci</source> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2810" xlink:type="simple">10.1038/nn.2810</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003743-Lakatos1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>C-M</given-names></name>, <name name-style="western"><surname>O'Connell</surname><given-names>MN</given-names></name>, <name name-style="western"><surname>Mills</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Schroeder</surname><given-names>CE</given-names></name> (<year>2007</year>) <article-title>Neuronal Oscillations and Multisensory Interaction in Primary Auditory Cortex</article-title>. <source>Neuron</source> <volume>53</volume>: <fpage>279</fpage>–<lpage>292</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Schroeder1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schroeder</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Kajikawa</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Partan</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Puce</surname><given-names>A</given-names></name> (<year>2008</year>) <article-title>Neuronal oscillations and visual amplification of speech</article-title>. <source>Trends Cogn Sci</source> <volume>12</volume>: <fpage>106</fpage>–<lpage>113</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Luo1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luo</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2010</year>) <article-title>Auditory cortex tracks both auditory and visual stimulus dynamics using low-frequency neuronal phase modulation</article-title>. <source>PLoS Biol</source> <volume>8</volume>: <fpage>e1000445</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-ZionGolumbic1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zion Golumbic</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Cogan</surname><given-names>GB</given-names></name>, <name name-style="western"><surname>Schroeder</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2013</year>) <article-title>Visual input enhances selective speech envelope tracking in auditory cortex at a “cocktail party”</article-title>. <source>J Neurosci</source> <volume>33</volume>: <fpage>1417</fpage>–<lpage>26</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Friston1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2005</year>) <article-title>A theory of cortical responses</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source> <volume>360</volume>: <fpage>815</fpage>–<lpage>836</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Friston2"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name> (<year>2009</year>) <article-title>Cortical circuits for perceptual inference</article-title>. <source>Neural Networks</source> <volume>22</volume>: <fpage>1093</fpage>–<lpage>1104</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Arnal3"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arnal</surname><given-names>LH</given-names></name>, <name name-style="western"><surname>Giraud</surname><given-names>A-L</given-names></name> (<year>2012</year>) <article-title>Cortical oscillations and sensory predictions</article-title>. <source>Trends Cogn Sci</source> <volume>16</volume>: <fpage>390</fpage>–<lpage>398</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Yehia1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yehia</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Rubin</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Vatikiotis-Bateson</surname><given-names>E</given-names></name> (<year>1998</year>) <article-title>Quantitative association of vocal tract and facial behavior</article-title>. <source>Speech Comm</source> <volume>26</volume>: <fpage>23</fpage>–<lpage>43</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-BarkerJ1"><label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">Barker J P, Berthommier F (1999) Evidence of correlation between acoustic and visual features of speech. Proc ICPhS '<volume>99</volume>  (pp. 199–202). San Francisco: USA.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Grant1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grant</surname><given-names>KW</given-names></name>, <name name-style="western"><surname>Seitz</surname><given-names>P</given-names></name> (<year>2000</year>) <article-title>The use of visible speech cues for improving auditory detection of spoken sentences. J. Acoust. Soc. Am</article-title>. <volume>108</volume>: <fpage>1197</fpage>–<lpage>1208</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Jiang1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jiang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Alwan</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Keating</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Auer</surname><given-names>ET</given-names><suffix>Jr</suffix></name>, <name name-style="western"><surname>Bernstein</surname><given-names>LE</given-names></name> (<year>2002</year>) <article-title>On the Relationship between Face Movements, Tongue Movements, and Speech Acoustics</article-title>. <source>Eurasip J Adv Sig Proc</source> <volume>11</volume>: <fpage>1174</fpage>–<lpage>1188</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Berthommier1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berthommier</surname><given-names>F</given-names></name> (<year>2004</year>) <article-title>A phonetically neutral model of the low-level audiovisual interaction</article-title>. <source>Speech Comm</source> <volume>44</volume>: <fpage>31</fpage>–<lpage>41</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Chandrasekaran1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chandrasekaran</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Trubanova</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Stillittano</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Caplier</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name> (<year>2009</year>) <article-title>The natural statistics of audiovisual speech</article-title>. <source>PLoS Comput Biol</source> <volume>5</volume>: <fpage>e1000436</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Musacchia1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Musacchia</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Schroeder</surname><given-names>CE</given-names></name> (<year>2009</year>) <article-title>Neuronal mechanisms, response dynamics and perceptual functions of multisensory interactions in auditory cortex</article-title>. <source>Hear Res</source> <volume>258</volume>: <fpage>72</fpage>–<lpage>79</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Blumstein1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blumstein</surname><given-names>SE</given-names></name>, <name name-style="western"><surname>Stevens</surname><given-names>KN</given-names></name> (<year>1980</year>) <article-title>Perceptual invariance and onset spectra for stop consonants in different vowel environments. J. Acoust. Soc. Am</article-title>. <volume>67(2)</volume>: <fpage>648</fpage>–<lpage>662</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Lfqvist1"><label>24</label>
<mixed-citation publication-type="other" xlink:type="simple">Löfqvist A (1995) Laryngeal mechanisms and interarticulator timing in voiceless consonant production. In Bell-Berti F, Raphael L, Eds. Producing Speech: Contemporary Issues (pp. 99–116). NY: AIP Press Woodbury.</mixed-citation>
</ref>
<ref id="pcbi.1003743-VanWassenhove2"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Wassenhove</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Grant</surname><given-names>KW</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2007</year>) <article-title>Temporal window of integration in auditory-visual speech perception</article-title>. <source>Neuropsychologia</source> <volume>45</volume>: <fpage>598</fpage>–<lpage>607</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Lallouache1"><label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Lallouache TM (1990) Un poste ‘visage-parole’. Acquisition et traitement de contours labiaux (A “face-speech” workstation. Acquisition and processing of labial contours). In Proceedings XVIIIèmes Journées d'Études sur la Parole: 282–286.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Noiray1"><label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Noiray A, Cathiard M-A, Abry C, Ménard L, Savariaux C (2008) Emergence of a vowel gesture control: Attunement of the anticipatory rounding temporal pattern in French children. In Kern S, Gayraud F &amp; Marsico E (Eds.) Emergence of Language Abilities (pp. 100–117). Newcastle: Cambridge Scholars Pub.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Boersma1"><label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Boersma P, Weenink D (2012) Praat: doing phonetics by computer (Version 5.3.04) [Computer program]. Retrieved May 2012, from <ext-link ext-link-type="uri" xlink:href="http://www.praat.org" xlink:type="simple">http://www.praat.org</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Stekelenburg1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stekelenburg</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Vroomen</surname><given-names>J</given-names></name> (<year>2007</year>) <article-title>Neural correlates of multisensory integration of ecologically valid audiovisual events</article-title>. <source>J Cog Neurosci</source> <volume>19</volume>: <fpage>1964</fpage>–<lpage>1973</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Vroomen1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vroomen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Stekelenburg</surname><given-names>JJ</given-names></name> (<year>2010</year>) <article-title>Visual anticipatory information modulates multisensory interactions of artificial audiovisual stimuli</article-title>. <source>J Cog Neurosci</source> <volume>22</volume>: <fpage>1583</fpage>–<lpage>1596</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Abry1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abry</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Lallouache</surname><given-names>TM</given-names></name> (<year>1995</year>) <article-title>Modeling lip constriction anticipatory behaviour for rounding in French with the MEM. Proc</article-title>. <source>ICPhS'</source> <volume>95</volume>: <fpage>152</fpage>–<lpage>155</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Benguerel1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benguerel</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Cowan</surname><given-names>HA</given-names></name> (<year>1974</year>) <article-title>Coarticulation of upper lip protrusion in French</article-title>. <source>Phonetica</source> <volume>30</volume>: <fpage>41</fpage>–<lpage>55</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-BellBerti1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bell-Berti</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>KS</given-names></name> (<year>1982</year>) <article-title>Temporal patterns of coarticulation: Lip rounding</article-title>. <source>J. Acoust. Soc. Am</source> <volume>71</volume>: <fpage>449</fpage>–<lpage>459</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Perkell1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perkell</surname><given-names>JS</given-names></name>, <name name-style="western"><surname>Matthies</surname><given-names>LM</given-names></name> (<year>1992</year>) <article-title>Temporal measures of anticipatory labial coarticulation for the vowel /u/: Within- and cross-subject variability</article-title>. <source>J. Acoust. Soc. Am</source> <volume>91</volume>: <fpage>2911</fpage>–<lpage>2925</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Abry2"><label>35</label>
<mixed-citation publication-type="other" xlink:type="simple">Abry C, Lallouache TM, Cathiard M-A (1996) How can coarticulation models account for speech sensitivity to audio-visual desynchronization? In Stork D and Hennecke M (Eds.) Speechreading by Humans and Machines, NATO ASI Series F (vol. 150, pp. 247–255). Berlin: Springer-Verlag.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Roy1"><label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Roy J-R, Sock R, Vaxelaire B, Hirsch F (2003) Auditory effects of anticipatory and carryover coarticulation. In Proc. 6th Int. Sem. Speech Production, Macquarie Series for Cognitive Sciences: 243–248.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Troille1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Troille</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Cathiard</surname><given-names>M-A</given-names></name>, <name name-style="western"><surname>Abry</surname><given-names>C</given-names></name> (<year>2010</year>) <article-title>Speech face perception is locked to anticipation in speech production</article-title>. <source>Speech Comm</source> <volume>52</volume>: <fpage>513</fpage>–<lpage>524</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Feldhoffer1"><label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Feldhoffer G, Bárdi T, Takács G, Tihanyi A (2007) Temporal asymmetry in relations of acoustic and visual features of speech. Proc. 15th European Signal Processing Conf., Poznan.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Czap1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Czap</surname><given-names>L</given-names></name> (<year>2011</year>) <article-title>On the audiovisual asynchrony of speech. Proc</article-title>. <source>AVSP'</source> <volume>2011</volume>: <fpage>137</fpage>–<lpage>140</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Bernstein1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bernstein</surname><given-names>LE</given-names></name>, <name name-style="western"><surname>Takayanagi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Auer</surname><given-names>ET</given-names><suffix>Jr</suffix></name> (<year>2004</year>) <article-title>Auditory speech detection in noise enhanced by lipreading</article-title>. <source>Speech Comm</source> <volume>44</volume>: <fpage>5</fpage>–<lpage>18</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Kim1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Davis</surname><given-names>C</given-names></name> (<year>2004</year>) <article-title>Investigating the audio-visual speech detection advantage</article-title>. <source>Speech Comm</source> <volume>44</volume>: <fpage>19</fpage>–<lpage>30</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Vroomen2"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vroomen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Keetels</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Perception of intersensory synchrony: a tutorial review</article-title>. <source>Attention, Perception, &amp; Psychophysics</source> <volume>72</volume>: <fpage>871</fpage>–<lpage>884</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Stevenson1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stevenson</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Zemtsov</surname><given-names>RK</given-names></name>, <name name-style="western"><surname>Wallace</surname><given-names>MT</given-names></name> (<year>2012</year>) <article-title>Individual differences in the multisensory temporal binding window predict susceptibility to audiovisual illusions</article-title>. <source>J Exp Psychol Human</source> <volume>38</volume>: <fpage>1517</fpage>–<lpage>1529</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Massaro1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Massaro</surname><given-names>DW</given-names></name>, <name name-style="western"><surname>Cohen</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Smeele</surname><given-names>PM</given-names></name> (<year>1996</year>) <article-title>Perception of asynchronous and conflicting visual and auditory speech</article-title>. <source>J. Acoust. Soc. Am</source> <volume>100</volume>: <fpage>1777</fpage>–<lpage>1786</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Munhall1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Munhall</surname><given-names>KG</given-names></name>, <name name-style="western"><surname>Gribble</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Sacco</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Ward</surname><given-names>M</given-names></name> (<year>1996</year>) <article-title>Temporal constraints on the McGurk effect</article-title>. <source>Perception and Psychophysics</source> <volume>58</volume>: <fpage>351</fpage>–<lpage>362</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Welch1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Welch</surname><given-names>RB</given-names></name>, <name name-style="western"><surname>Warren</surname><given-names>DH</given-names></name> (<year>1980</year>) <article-title>Immediate perceptual response to intersensory discrepancy</article-title>. <source>Psychol Bull</source> <volume>88</volume>: <fpage>638</fpage>–<lpage>667</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Vatakis1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vatakis</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Spence</surname><given-names>C</given-names></name> (<year>2007</year>) <article-title>Crossmodal binding: Evaluating the ‘unity assumption’ using audiovisual speech stimuli</article-title>. <source>Perception &amp; Psychophysics</source> <volume>69</volume>: <fpage>744</fpage>–<lpage>756</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Lewkowicz1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewkowicz</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Flom</surname><given-names>R</given-names></name> (<year>2014</year>) <article-title>The audio-visual temporal binding window narrows in early childhood</article-title>. <source>Child Development</source> <volume>85</volume>: <fpage>685</fpage>–<lpage>694</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Hillock1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hillock</surname><given-names>AR</given-names></name>, <name name-style="western"><surname>Powers</surname><given-names>AR</given-names></name>, <name name-style="western"><surname>Wallace</surname><given-names>MT</given-names></name> (<year>2011</year>) <article-title>Binding of sights and sounds: age-related changes in multisensory temporal processing</article-title>. <source>Neuropsychologia</source> <volume>49</volume>: <fpage>461</fpage>–<lpage>467</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Sekiyama1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sekiyama</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Burnham</surname><given-names>D</given-names></name> (<year>2008</year>) <article-title>Impact of language on development of auditory-visual speech perception</article-title>. <source>Dev Sci</source> <volume>11</volume>: <fpage>303</fpage>–<lpage>317</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Giard1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Giard</surname><given-names>MH</given-names></name>, <name name-style="western"><surname>Peronnet</surname><given-names>F</given-names></name> (<year>1999</year>) <article-title>Auditory-visual integration during multimodal object recognition in humans: A behavioral and electrophysiological study</article-title>. <source>J Cog Neurosci</source> <volume>11</volume>: <fpage>473</fpage>–<lpage>490</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Schwartz1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname><given-names>JL</given-names></name> (<year>2010</year>) <article-title>A reanalysis of McGurk data suggests that audiovisual fusion in speech perception is subject-dependent</article-title>. <source>J. Acoust. Soc. Am</source> <volume>127</volume>: <fpage>1584</fpage>–<lpage>1594</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Powers1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Powers</surname><given-names>AR</given-names></name> (<year>2009</year>) <article-title>Perceptual training narrows the temporal window of multisensory binding</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>12265</fpage>–<lpage>12274</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Petrini1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Petrini</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Dahl</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Rocchesso</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Waadeland</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Avanzini</surname><given-names>F</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Multisensory integration of drumming actions: musical expertise affects perceived audiovisual asynchrony</article-title>. <source>Experimental Brain Research</source> <volume>198</volume>: <fpage>339</fpage>–<lpage>352</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Schwartz2"><label>55</label>
<mixed-citation publication-type="other" xlink:type="simple">Schwartz JL, Savariaux C (2013) Data and simulations about audiovisual asynchrony and predictability in speech perception. Proc. AVSP'2013, 147–152.</mixed-citation>
</ref>
<ref id="pcbi.1003743-Cummins1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cummins</surname><given-names>F</given-names></name> (<year>2012</year>) <article-title>Oscillators and syllables: a cautionary note</article-title>. <source>Front Psychol</source> <volume>3</volume>: <fpage>364</fpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>