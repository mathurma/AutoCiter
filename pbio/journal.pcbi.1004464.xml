<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-02203</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004464</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Efficient "Shotgun" Inference of Neural Connectivity from Highly Sub-sampled Activity Data</article-title>
<alt-title alt-title-type="running-head">Efficient "Shotgun" Inference of Neural Connectivity</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Soudry</surname> <given-names>Daniel</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Keshri</surname> <given-names>Suraj</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Stinson</surname> <given-names>Patrick</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Oh</surname> <given-names>Min-hwan</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Iyengar</surname> <given-names>Garud</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Paninski</surname> <given-names>Liam</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Statistics, Department of Neuroscience, the Center for Theoretical Neuroscience, the Grossman Center for the Statistics of Mind, the Kavli Institute for Brain Science, and the NeuroTechnology Center, Columbia University, New York, New York, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Industrial Engineering and Operations Research, Columbia University, New York, New York, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bethge</surname> <given-names>Matthias</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Tübingen and Max Planck Institute for Biologial Cybernetics, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: DS LP. Performed the experiments: DS SK PS MO. Analyzed the data: DS. Contributed reagents/materials/analysis tools: GI. Wrote the paper: DS SK PS MO GI LP. Derived mathematical results: DS LP.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">daniel.soudry@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>10</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>14</day>
<month>10</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>10</issue>
<elocation-id>e1004464</elocation-id>
<history>
<date date-type="received">
<day>8</day>
<month>12</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>9</day>
<month>7</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Soudry et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004464" xlink:type="simple"/>
<abstract>
<p>Inferring connectivity in neuronal networks remains a key challenge in statistical neuroscience. The “common input” problem presents a major roadblock: it is difficult to reliably distinguish causal connections between pairs of observed neurons versus correlations induced by common input from unobserved neurons. Available techniques allow us to simultaneously record, with sufficient temporal resolution, only a small fraction of the network. Consequently, naive connectivity estimators that neglect these common input effects are highly biased. This work proposes a “shotgun” experimental design, in which we observe multiple sub-networks briefly, in a serial manner. Thus, while the full network cannot be observed simultaneously at any given time, we may be able to observe much larger subsets of the network over the course of the entire experiment, thus ameliorating the common input problem. Using a generalized linear model for a spiking recurrent neural network, we develop a scalable approximate expected loglikelihood-based Bayesian method to perform network inference given this type of data, in which only a small fraction of the network is observed in each time bin. We demonstrate in simulation that the shotgun experimental design can eliminate the biases induced by common input effects. Networks with thousands of neurons, in which only a small fraction of the neurons is observed in each time bin, can be quickly and accurately estimated, achieving orders of magnitude speed up over previous approaches.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Optical imaging of the activity in a neuronal network is limited by the scanning speed of the imaging device. Therefore, typically, only a small fixed part of the network is observed during the entire experiment. However, in such an experiment, it can be hard to infer from the observed activity patterns whether (1) a neuron A directly affects neuron B, or (2) another, unobserved neuron C affects both A and B. To deal with this issue, we propose a “shotgun” observation scheme, in which, at each time point, we observe a small changing subset of the neurons from the network. Consequently, many fewer neurons remain completely unobserved during the entire experiment, enabling us to eventually distinguish between cases (1) and (2) given sufficiently long experiments. Since previous inference algorithms cannot efficiently handle so many missing observations, we develop a scalable algorithm for data acquired using the shotgun observation scheme, in which only a small fraction of the neurons are observed in each time bin. Using this kind of simulated data, we show the algorithm is able to quickly infer connectivity in spiking recurrent networks with thousands of neurons.</p>
</abstract>
<funding-group>
<funding-statement>The work of DS was supported by the Gruss Lipper Charitable Foundation (<ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.eglcf.org">http://www.eglcf.org</ext-link>). The work of LS was supported by 1) The army Research Office Multidisciplinary University Research Initiative W911NF-12-1-0594 (<ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.arl.army.mil/www/default.cfm?page=472">http://www.arl.army.mil/www/default.cfm?page=472</ext-link>), 2) The Defense Advanced Research Projects Agency W91NF-14-1-0269 and N66001-15-C-4032 (<ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.darpa.mil">http://www.darpa.mil</ext-link>) 3) The National Science Foundation Faculty Early Career Development Program IOS-0641912 (<ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.nsf.gov">http://www.nsf.gov</ext-link>) 4) The National Science Foundation IIS-1430239 (<ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.nsf.gov">http://www.nsf.gov</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="1"/>
<page-count count="30"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All code is available on <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="https://github.com/danielso/Shotgun">https://github.com/danielso/Shotgun</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<disp-quote>
<p>This is a <italic>PLOS Computational Biology Methods</italic> paper</p>
</disp-quote>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>It is now possible to image hundreds of neurons simultaneously at high spatiotemporal resolution [<xref ref-type="bibr" rid="pcbi.1004464.ref001">1</xref>] or tens of thousands of neurons at low spatiotemporal resolution [<xref ref-type="bibr" rid="pcbi.1004464.ref002">2</xref>]. The number of recorded neurons is expected to continue to grow exponentially [<xref ref-type="bibr" rid="pcbi.1004464.ref003">3</xref>]. This, in principle, provides the opportunity to infer the “functional” (or “effective”) connectivity of neuronal networks, <italic>i.e</italic>. a statistical estimate of how neurons are affected by each other, and by a stimulus. The ability to accurately estimate large, possibly time-varying, neural connectivity diagrams would open up an exciting new range of fundamental research questions in systems and computational neuroscience [<xref ref-type="bibr" rid="pcbi.1004464.ref004">4</xref>]. Therefore, the task of estimating connectivity from neural activity can be considered one of the central problems in statistical neuroscience.</p>
<p>Naturally, such a central problem has attracted much attention in recent years (see section 8). Perhaps the biggest challenge here involves the proper accounting for the activity of unobserved neurons. Despite rapid progress in simultaneously recording activity in massive populations of neurons, it is still beyond the reach of current technology to simultaneously monitor a complete large network of spiking neurons at high temporal resolution. Since connectivity estimation relies on the analysis of the the activity of neurons in relation to their inputs, the inability to monitor all of these inputs can result in persistent errors in the connectivity estimation due to model miss-specification. More specifically, “common input” errors, in which correlations due to shared inputs from unobserved neurons are mistaken for direct, causal connections, plague most naive approaches to connectivity estimation. Developing a robust approach for incorporating the latent effects of such unobserved neurons remains an area of active research in connectivity analysis (see section 8).</p>
<p>In this paper we propose an experimental design which can greatly ameliorate these common-input problems. The idea is simple: if we cannot observe all neurons in a network simultaneously, perhaps we can instead observe many overlapping sub-networks in a serial manner over the course of a long experiment. Then we can use statistical techniques to patch the full estimated network back together, analogous to “shotgun” genetic sequencing [<xref ref-type="bibr" rid="pcbi.1004464.ref005">5</xref>]. Obviously, it is not feasible to purposefully sample from many distinct sub-networks at many different overlapping locations using multi-electrode recording arrays, since multiple re-insertions of the array would lead to tissue damage. However, fluorescence-based imaging of neuronal calcium [<xref ref-type="bibr" rid="pcbi.1004464.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref007">7</xref>] (or, perhaps in the not-too-distant future, voltage [<xref ref-type="bibr" rid="pcbi.1004464.ref008">8</xref>]) makes this approach experimentally feasible.</p>
<p>For example, such a shotgun approach could be highly beneficial and relatively straightforward to implement using a 3D acousto-optical deflector microscope [<xref ref-type="bibr" rid="pcbi.1004464.ref001">1</xref>]. Using such a microscope, one can scan a volume of 400 × 400 × 500 <italic>μm</italic>, which contains approximately 8000 cells. In normal use, the microscope’s 50kHz sampling rate allows for a frame rate of about 6Hz when scanning the entire volume. Unfortunately, this frame rate is too low for obtaining reliable connectivity estimates, which requires a frame rate of at least 30Hz [<xref ref-type="bibr" rid="pcbi.1004464.ref009">9</xref>]. However, we can increase the effective frame rate to 30Hz by using a shotgun approach. We simply divide the experimental duration into segments, where in each segment we scan only 20% of the network. As a side benefit of this shotgun approach, photobleaching and phototoxicity (two of the most important limitations on the duration of these experiments [<xref ref-type="bibr" rid="pcbi.1004464.ref010">10</xref>]) are reduced, since only a subset of the network is illuminated and imaged at any given time.</p>
<p>Connectivity estimation with missing observations is particularly challenging (section 9). Fortunately, as we show here, given the shotgun sampling scheme, we do not have to infer the unobserved spikes. We considerably simplify the network model loglikelihood using the expected loglikelihood approximation [<xref ref-type="bibr" rid="pcbi.1004464.ref011">11</xref>–<xref ref-type="bibr" rid="pcbi.1004464.ref013">13</xref>], and a generalized Central Limit Theorem (CLT) [<xref ref-type="bibr" rid="pcbi.1004464.ref014">14</xref>] argument to approximate the neuronal input as a Gaussian variable when the size of the network is large. This approximate loglikelihood and its gradients depend only on the empiric second order statistics of the spiking process (mean spike rate and spike correlations). Importantly, these approximate sufficient statistics can be calculated, even with partial observations, by simply “ignoring” any unobserved activity (section 3.6).</p>
<p>In order to obtain an accurate estimation of the connectivity, posterior distributions involving this simplified loglikelihood (along with various types of prior information about network connectivity) can be efficiently maximized. Using a sparsity inducing prior on the weights, we demonstrate numerically the effectiveness of our approach on simulated recurrent networks of spiking neurons. First, we demonstrate that the shotgun experimental design can largely eliminate the biases induced by common input effects (section 4). Then, we show that we can quickly infer connectivity for large networks, with a low fraction of neurons observed in each time bin (section 5). For example, our algorithm can be used to infer the connectivity of a sparse network with <italic>O</italic>(10<sup>3</sup>) neurons and <italic>O</italic>(10<sup>5</sup>) connections, given <italic>O</italic>(10<sup>6</sup>) time bins of spike data in which only 10% − 20% of the neurons are observed in each time bin. On a standard laptop, simulating such a network takes about half an hour, while inference takes a few minutes. This is faster than previous approaches by orders of magnitude, even when all spikes are observed (section 6.2). Our parameter scans suggest that our method is robust, and could be used for arbitrarily low observation ratios and an arbitrarily large number of neurons, given long enough experiments. We will discuss the outlook for experimental realizations of the proposed approach below, after presenting the basic methodology and simulated results. The supplementary material <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref> contains the full details of the mathematical derivations and the numerical simulations.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec003">
<title>1 Preliminaries</title>
<sec id="sec004">
<title>1.1 General Notation</title>
<p>A boldfaced letter <bold>x</bold> denotes a vector with components <italic>x</italic><sub><italic>i</italic></sub>, a boldfaced capital letter <bold>X</bold> denotes a matrix with components <italic>X</italic><sub><italic>i</italic>, <italic>j</italic></sub>, <bold>X</bold><sup>(<italic>k</italic>)</sup> denotes the <italic>k</italic>-th matrix in a list, and <bold>X</bold><sub>⋅, <italic>k</italic></sub> (<bold>X</bold><sub><italic>k</italic>, ⋅</sub>) the <italic>k</italic>-th column (row) vector of matrix <bold>X</bold>. For <bold>X</bold> ∈ ℝ<sup><italic>N</italic>×<italic>T</italic></sup> we define the empiric average and variance
<disp-formula id="pcbi.1004464.e001"><alternatives><graphic id="pcbi.1004464.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e001"/><mml:math id="M1" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:mtd> <mml:mtd><mml:mo>≜</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mo>;</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>T</mml:mi></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mo>≜</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
Note the above expressions do not depend on <italic>t</italic>, despite the <italic>t</italic> index, which is maintained for notational convenience. For any condition <italic>A</italic>, we make use of 𝓘{<italic>A</italic>}, the indicator function (𝓘{<italic>A</italic>} = 1 if <italic>A</italic> holds, and zero otherwise). We define <italic>δ</italic><sub><italic>i</italic>, <italic>j</italic></sub> ≜ 𝓘{<italic>i</italic> = <italic>j</italic>}, Kronecker’s delta function. If <bold>x</bold> ∼ 𝓝(<bold>μ</bold>,<bold>Σ</bold>), then <bold>x</bold> is Gaussian random vector with mean <bold>μ</bold> and covariance matrix <bold>Σ</bold>, and we denote its density by 𝓝(<bold>x</bold>∣<bold>μ</bold>,<bold>Σ</bold>).</p>
</sec>
<sec id="sec005">
<title>1.2 Model</title>
<p>We use a discrete-time neural network. The neurons, indexed from <italic>i</italic> = 1 to <italic>N</italic>, produce spikes in time bins indexed from <italic>t</italic> = 1 to <italic>T</italic>. The spiking matrix <bold>S</bold> is composed of variables <italic>S</italic><sub><italic>i</italic>, <italic>t</italic></sub> indicating the number of spikes neuron <italic>i</italic> produces at time bin <italic>t</italic>. We assume each neuron <italic>i</italic> generates spikes <italic>S</italic><sub><italic>i</italic>, <italic>t</italic></sub> ∈ {0,1} according to a Generalized Linear neuron Model (GLM [<xref ref-type="bibr" rid="pcbi.1004464.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1004464.ref017">17</xref>]), with a logistic probability function
<disp-formula id="pcbi.1004464.e002"><alternatives><graphic id="pcbi.1004464.e002g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e002"/><mml:math id="M2" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mo>≜</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
depending on the the input <italic>U</italic><sub><italic>i</italic>, <italic>t</italic></sub> it receives from other neurons, as well as from some external stimulus. Such a logistic function is adequate if any time bin rarely contains more than one spike (this is approximately true if the time bin is much smaller than the average inter-spike interval). The input to all the neurons in the network is therefore
<disp-formula id="pcbi.1004464.e003"><alternatives><graphic id="pcbi.1004464.e003g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e003"/><mml:math id="M3" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">U</mml:mi> <mml:mrow><mml:mo>·</mml:mo> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="1pt"/><mml:mo>≜</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi mathvariant="bold">S</mml:mi> <mml:mrow><mml:mo>·</mml:mo> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">G</mml:mi> <mml:msub><mml:mi mathvariant="bold">X</mml:mi> <mml:mrow><mml:mo>·</mml:mo> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>b</italic><sub><italic>i</italic></sub> is the (unknown) bias of neuron <italic>i</italic>; <bold>X</bold> ∈ ℝ<sup><italic>D</italic>×<italic>T</italic></sup> are the external inputs (with <italic>D</italic> being the number of inputs); <bold>G</bold> ∈ ℝ<sup><italic>N</italic>×<italic>D</italic></sup> is the input gain; and <bold>W</bold> ∈ ℝ<sup><italic>N</italic>×<italic>N</italic></sup> is the (unknown) network connectivity matrix. The diagonal elements <italic>W</italic><sub><italic>i</italic>, <italic>i</italic></sub> of the connectivity matrix correspond to the post spike filter accounting for the cell’s own post-spike effects (<italic>e.g</italic>., refractory period), while the off-diagonal terms <italic>W</italic><sub><italic>i</italic>, <italic>j</italic></sub> represent the connection weights from neuron <italic>j</italic> to neuron <italic>i</italic>. The bias <italic>b</italic><sub><italic>i</italic></sub> controls the mean spike probability (firing rate) of neuron <italic>i</italic>. The external input <bold>X</bold> can represent a direct (<italic>e.g</italic>., light activated ion channels) or sensory stimulation of neurons in the network. The input gain <bold>G</bold> is a spatial filter that acts on the input <bold>X</bold>. We assume that the initial spiking pattern is drawn from some fixed distribution <italic>P</italic>(<bold>S</bold><sub>⋅,0</sub>).</p>
<p>To simplify notation, we have assumed in <xref ref-type="disp-formula" rid="pcbi.1004464.e003">Eq 2</xref> that <bold>U</bold><sub>⋅, <italic>t</italic></sub> is only affected by spiking activity from the previous time bin (<bold>W</bold> <bold>S</bold><sub>⋅, <italic>t</italic>−1</sub>). However, to include a longer history of the spiking activity, we can simply replace the vector <bold>S</bold><sub>⋅, <italic>t</italic>−1</sub> in <xref ref-type="disp-formula" rid="pcbi.1004464.e003">Eq 2</xref> with the concatenation of the vectors <bold>S</bold><sub>⋅, <italic>t</italic>−1</sub>, …,<bold>S</bold><sub>⋅, <italic>t</italic>−<italic>k</italic></sub> and obtain similar results.</p>
</sec>
<sec id="sec006">
<title>1.3 Task</title>
<p>Our goal is to infer the connectivity matrix <bold>W</bold>, biases <bold>b</bold> and the stimulus gain <bold>G</bold>. We assume that we have some prior information on the weights, and that we know <italic>N</italic>, and the external input <bold>X</bold>. We noiselessly observe a subset of the generated spikes. For simplicity we initially ignore the problem of inferring spikes from the experimental data, which requires spike sorting or deconvolution of fluorescence traces. Later, we will address this issue of spike inference numerically (see also [<xref ref-type="bibr" rid="pcbi.1004464.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref018">18</xref>] for a more systematic analysis of this issue). We use a binary matrix <bold>O</bold> to indicate which neurons were observed, so
<disp-formula id="pcbi.1004464.e004"><alternatives><graphic id="pcbi.1004464.e004g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e004"/><mml:math id="M4" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="1pt"/><mml:mo>≜</mml:mo><mml:mspace width="1pt"/><mml:mo>𝓘</mml:mo> <mml:mo>[</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.166667em"/><mml:mtext>was</mml:mtext> <mml:mspace width="0.166667em"/><mml:mtext>observed</mml:mtext> <mml:mo>]</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
Practically, if <italic>O</italic><sub><italic>i</italic>, <italic>t</italic></sub> = 1 neuron <italic>i</italic> was imaged for sufficiently long time and with a high enough frame rate around time bin <italic>t</italic> so that we can infer whether a spike occurred in time bin <italic>t</italic> with relative certainty.</p>
</sec>
</sec>
<sec id="sec007">
<title>2 Analytical results—Bayesian inference of the weights</title>
<p>We use a Bayesian approach to infer the unknown weights. Suppose initially, for simplicity, that all spikes are observed and that there is no external input (<bold>G</bold> = 0). In this case, the log-posterior of the weights, given the spiking activity, is
<disp-formula id="pcbi.1004464.e005"><alternatives><graphic id="pcbi.1004464.e005g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e005"/><mml:math id="M5" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo form="prefix">ln</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">S</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mo form="prefix">ln</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">S</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mo form="prefix">ln</mml:mo> <mml:msub><mml:mi>P</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>C</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where ln <italic>P</italic>(<bold>S</bold>∣<bold>W</bold>,<bold>b</bold>) is the loglikelihood, <italic>P</italic><sub>0</sub>(<bold>W</bold>) is some prior on the weights (we do not assume a prior on the biases <bold>b</bold>), and <italic>C</italic> is some unimportant constant which does not depend on <bold>W</bold> or <bold>b</bold>. Our aim is to find the Maximum A Posteriori (MAP) estimator for <bold>W</bold>, together with the Maximum Likelihood (ML) estimator for <bold>b</bold>, by solving
<disp-formula id="pcbi.1004464.e006"><alternatives><graphic id="pcbi.1004464.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e006"/><mml:math id="M6" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo> <mml:mrow><mml:mi mathvariant="bold">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:munder> <mml:mo form="prefix">ln</mml:mo> <mml:mi>P</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi mathvariant="bold">S</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
If <bold>S</bold> is fully observed, this problem can be straightforwardly optimized without requiring an approximation (though the optimization procedure can be slow). However, our goal is to provide an estimate when only a subset of <bold>S</bold> is observed. This cannot be easily done using standard method. To see this, we examine the likelihood of a GLM (recalling Eqs (<xref ref-type="disp-formula" rid="pcbi.1004464.e002">1</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1004464.e003">2</xref>)),
<disp-formula id="pcbi.1004464.e007"><alternatives><graphic id="pcbi.1004464.e007g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e007"/><mml:math id="M7" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo form="prefix">ln</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">S</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:mo form="prefix">ln</mml:mo> <mml:mo>[</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:msup></mml:mrow></mml:mfrac> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula> <disp-formula id="pcbi.1004464.e008"><alternatives><graphic id="pcbi.1004464.e008g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e008"/><mml:math id="M8" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:mo>[</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mo form="prefix">ln</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:msup> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
This likelihood (<xref ref-type="disp-formula" rid="pcbi.1004464.e008">Eq 7</xref>), and its gradients, both contain a sum over weighted spikes in <italic>U</italic><sub><italic>i</italic>, <italic>t</italic></sub> (the <bold>WS</bold><sub>⋅, <italic>t</italic>−1</sub> term in <xref ref-type="disp-formula" rid="pcbi.1004464.e003">Eq 2</xref>), that cannot be evaluated if some spikes are missing, unless the missing spikes are accurately inferred (section E in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>). However, methods for inferring these missing spikes are typically slow, and do not scale well.</p>
<p>To circumvent these issues, we will show the loglikelihood can be approximated with a simple form, under a few reasonable assumptions. Importantly, this simple form can be easily calculated even if there are missing observations (the full derivation is in section 2.1). Using an extension of the techniques in [<xref ref-type="bibr" rid="pcbi.1004464.ref011">11</xref>–<xref ref-type="bibr" rid="pcbi.1004464.ref013">13</xref>], we develop an approximation to the likelihood based on the law of large numbers (the “expected loglikelihood” approximation) together with a generalized Central Limit Theorem (CLT) argument [<xref ref-type="bibr" rid="pcbi.1004464.ref014">14</xref>], in which we approximate the neuronal input to be Gaussian near the limit <italic>N</italic> → ∞; then we calculate the “profile likelihood” max<sub><bold>b</bold></sub>ln <italic>P</italic>(<bold>S</bold>∣<bold>W</bold>,<bold>b</bold>), in which the bias term has been substituted for its maximizing value. The end result is
<disp-formula id="pcbi.1004464.e009"><alternatives><graphic id="pcbi.1004464.e009g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e009"/><mml:math id="M9" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo> <mml:mi mathvariant="bold">b</mml:mi></mml:munder> <mml:mo form="prefix">ln</mml:mo> <mml:mi>P</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi mathvariant="bold">S</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>≈</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>T</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mo>[</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mo>[</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mo>Σ</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>]</mml:mo> <mml:mo>-</mml:mo> <mml:mi>h</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:msqrt><mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi>π</mml:mi> <mml:mn>8</mml:mn></mml:mfrac> <mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mo>Σ</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt> <mml:mo>]</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
where we defined the mean spike probability, spike covariance, and the entropy function, respectively:
<disp-formula id="pcbi.1004464.e010"><alternatives><graphic id="pcbi.1004464.e010g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e010"/><mml:math id="M10" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub><mml:mspace width="1pt"/><mml:mo>≜</mml:mo><mml:mspace width="1pt"/><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula> <disp-formula id="pcbi.1004464.e011"><alternatives><graphic id="pcbi.1004464.e011g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e011"/><mml:math id="M11" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mo>Σ</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mspace width="1pt"/><mml:mo>≜</mml:mo><mml:mspace width="1pt"/><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>m</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula> <disp-formula id="pcbi.1004464.e012"><alternatives><graphic id="pcbi.1004464.e012g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e012"/><mml:math id="M12" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>h</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mo>≜</mml:mo><mml:mspace width="1pt"/><mml:mo>-</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo form="prefix">ln</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo form="prefix">ln</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
A few comments:
<list list-type="order"><list-item><p>Importantly, the profile loglikelihood (<xref ref-type="disp-formula" rid="pcbi.1004464.e009">Eq 8</xref>) depends only on the first and second order moments of the spikes <bold>m</bold> and <bold>Σ</bold><sup>(<italic>k</italic>)</sup> for <italic>k</italic> ∈ {0,1}. When all of the neurons in the network are observed, these moments can be computed directly, and therefore the empirical moments are approximate sufficient statistics, whose value contains all the information needed to compute any estimate of <bold><bold>W</bold></bold>. As we explain in section 3, these empirical moments can be estimated even if only a subset of the spikes is observed.</p></list-item> <list-item><p>As we show in section A in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>, the profile loglikelihood (<xref ref-type="disp-formula" rid="pcbi.1004464.e009">Eq 8</xref>) is concave, so it is easy to maximize the log-posterior and obtain the MAP estimate of <bold>W</bold>. This can be done orders of magnitude faster than in the standard MAP estimate (section 6.2), since <xref ref-type="disp-formula" rid="pcbi.1004464.e009">Eq 8</xref> does not contain a sum over time, as the original loglikelihood (<xref ref-type="disp-formula" rid="pcbi.1004464.e008">Eq 7</xref>). Moreover, the optimization problem of finding the MAP estimate can be parallelized over the rows of <bold>W</bold>.
<disp-formula id="pcbi.1004464.e013"><alternatives><graphic id="pcbi.1004464.e013g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e013"/><mml:math id="M13" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo> <mml:mi mathvariant="bold">b</mml:mi></mml:munder> <mml:mo form="prefix">ln</mml:mo> <mml:mi>P</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi mathvariant="bold">S</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo> <mml:mi mathvariant="bold">b</mml:mi></mml:munder> <mml:mo form="prefix">ln</mml:mo> <mml:mi>P</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mo>·</mml:mo></mml:mrow></mml:msub> <mml:mrow><mml:mo stretchy="false">|</mml:mo> <mml:mi mathvariant="bold">S</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">b</mml:mi></mml:mrow> <mml:mo stretchy="false">)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
because the profile loglikelihood (<xref ref-type="disp-formula" rid="pcbi.1004464.e009">Eq 8</xref>) decomposes over the rows of <bold>W</bold>, as does the L1 prior we will use here (Eq 46).</p></list-item> <list-item><p>As we show in section A in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>, we can straightforwardly differentiate <xref ref-type="disp-formula" rid="pcbi.1004464.e009">Eq 8</xref> to analytically obtain the gradient, Hessian, and even the maximizer of this profile loglikelihood, which is the maximum likelihood estimate of <bold>W</bold>. However, due to the nature of the integral approximation we make in <xref ref-type="disp-formula" rid="pcbi.1004464.e016">Eq 14</xref>, more accurate results are obtained if we first differentiate the original loglikelihood (<xref ref-type="disp-formula" rid="pcbi.1004464.e008">Eq 7</xref>), and then use the expectation approximation (together with the generalized CLT argument). This results in an adjustment of the loglikelihood gradient (section D in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>).</p></list-item> <list-item><p>A novel aspect of this work is that we apply the Expected LogLikelihood (ELL) approximation to a GLM with a bounded logistic rate function (<xref ref-type="disp-formula" rid="pcbi.1004464.e002">Eq 1</xref>), which allows us to infer connectivity in <italic>recurrent</italic> neural networks. In contrast, previous works that used the ELL approximation [<xref ref-type="bibr" rid="pcbi.1004464.ref011">11</xref>–<xref ref-type="bibr" rid="pcbi.1004464.ref013">13</xref>] focused on single neuron responses, with an emphasis on either a Poisson neuron model with an exponential rate function, or simpler linear Gaussian models. Such models are less suitable for recurrent neural networks. Exponential rate functions cause instability, as the activity tends to to diverge, unless both the weights and the time bins are small. Linear networks are not a very realistic model for a neural network, and do not perform well in inferring synaptic connectivity [<xref ref-type="bibr" rid="pcbi.1004464.ref019">19</xref>].</p></list-item> <list-item><p>Though we assumed a logistic neuron model (<xref ref-type="disp-formula" rid="pcbi.1004464.e002">Eq 1</xref>), similar results can be derived for any spiking neuron model for which 1−<italic>f</italic>(<italic>x</italic>) = <italic>f</italic>(−<italic>x</italic>). This is explained in section A.3 <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>.</p></list-item> <list-item><p>Though we assumed the network does not have a stimulus (<bold>G</bold> = 0), one can be incorporated into the inference procedure. To do so, we treat the stimulus <bold>X</bold><sub>⋅, <italic>t</italic></sub> simply as the activity of additional, fully observed, neurons (albeit <italic>X</italic><sub><italic>i</italic>, <italic>t</italic></sub> ∈ ℝ while <italic>S</italic><sub><italic>i</italic>, <italic>t</italic></sub> ∈ {0,1}). Specifically, we define a new “spikes” matrix <bold>S</bold><sup>new</sup> ≜ (<bold>S</bold><sup>⊤</sup>,<bold>X</bold><sup>⊤</sup>)<sup>⊤</sup>, a new connectivity matrix
<disp-formula id="pcbi.1004464.e014"><alternatives><graphic id="pcbi.1004464.e014g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e014"/><mml:math id="M14" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathvariant="bold">W</mml:mi> <mml:mtext>new</mml:mtext></mml:msup><mml:mspace width="1pt"/><mml:mo>≜</mml:mo><mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mi mathvariant="bold">W</mml:mi></mml:mtd> <mml:mtd><mml:mi mathvariant="bold">G</mml:mi></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msup><mml:mn>0</mml:mn> <mml:mrow><mml:mi>D</mml:mi> <mml:mo>×</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mtd> <mml:mtd><mml:msup><mml:mn>0</mml:mn> <mml:mrow><mml:mi>D</mml:mi> <mml:mo>×</mml:mo> <mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
and a new observation matrix <bold>O</bold><sup>new</sup> ≜ (<bold>O</bold><sup>⊤</sup>,1<sup><italic>T</italic>×<italic>D</italic></sup>)<sup>⊤</sup>. Repeating the derivations for <bold>S</bold><sup>new</sup>,<bold>W</bold><sup>new</sup> and <bold>O</bold><sup>new</sup>, we obtain the same profile loglikelihood. Once it is used to infer <bold>W</bold><sup>new</sup>, we extract the estimates of <bold>W</bold> and <bold>G</bold> from their corresponding blocks in <bold>W</bold><sup>new</sup>.</p></list-item> <list-item><p>For simplicity and efficiency, we chose to focus on MAP estimates. However, other types of estimators and Bayesian approaches (<italic>e.g</italic>., MCMC, variational Bayes) might be used with this approximate loglikelihood, and should be explored in future work.</p></list-item></list></p>
<sec id="sec008">
<title>2.1 Derivation of the simplified loglikelihood (<xref ref-type="disp-formula" rid="pcbi.1004464.e009">Eq 8</xref>)</title>
<p>Recall Eqs (<xref ref-type="disp-formula" rid="pcbi.1004464.e002">1</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1004464.e003">2</xref>) with <bold>G</bold> = 0. Combining both for times <italic>t</italic> = 1, ⋯, <italic>T</italic> and neurons <italic>i</italic> = 1, …, <italic>N</italic>, we obtain
<disp-formula id="pcbi.1004464.e015"><alternatives><graphic id="pcbi.1004464.e015g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e015"/><mml:math id="M15" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo form="prefix">ln</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">S</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:mo>[</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mo form="prefix">ln</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:msup> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mo>=</mml:mo> <mml:mi>T</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mo>[</mml:mo> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:mo form="prefix">ln</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:msup> <mml:mo>)</mml:mo> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mover><mml:mo>≈</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mover> <mml:mi>T</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mo>[</mml:mo> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mo>∫</mml:mo> <mml:mo form="prefix">ln</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mi>x</mml:mi></mml:msup> <mml:mo>)</mml:mo> <mml:mo>𝓝</mml:mo> <mml:mo>(</mml:mo> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>T</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mi>d</mml:mi> <mml:mi>x</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mover><mml:mo>≈</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mover> <mml:mi>T</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mo>[</mml:mo> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msqrt><mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>π</mml:mi> <mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>T</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo> <mml:mo>/</mml:mo> <mml:mn>8</mml:mn></mml:mrow></mml:msqrt> <mml:mo form="prefix">ln</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:msqrt><mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>π</mml:mi> <mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>T</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo> <mml:mo>/</mml:mo> <mml:mn>8</mml:mn></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mover><mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>3</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mover> <mml:mi>T</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mo>Σ</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>b</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mo>-</mml:mo> <mml:msqrt><mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>π</mml:mi> <mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mo>Σ</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>/</mml:mo> <mml:mn>8</mml:mn></mml:mrow></mml:msqrt> <mml:mo form="prefix">ln</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>m</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:msqrt><mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>π</mml:mi> <mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mo>Σ</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>/</mml:mo> <mml:mn>8</mml:mn></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
where we used the following:
<list list-type="order"><list-item><p>The neuronal input, as a sum of <italic>N</italic> variables, converges to a Gaussian distribution, in the limit of large <italic>N</italic>, under rather general conditions [<xref ref-type="bibr" rid="pcbi.1004464.ref014">14</xref>]. Formally, we need to make sure these are fulfilled for our approximate method to work, which can become even more challenging with the addition of (arbitrary) external inputs. However, such a generalized CLT-based approximation tends to work quite well even when the neuronal input is not strictly Gaussian [<xref ref-type="bibr" rid="pcbi.1004464.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1004464.ref022">22</xref>]. This robustness is demonstrated numerically in our simulations.</p></list-item> <list-item><p>The integral approximation
<disp-formula id="pcbi.1004464.e016"><alternatives><graphic id="pcbi.1004464.e016g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e016"/><mml:math id="M16" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>∞</mml:mi></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo form="prefix">log</mml:mo> <mml:mo stretchy="false">(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mi>x</mml:mi></mml:msup> <mml:mo stretchy="false">)</mml:mo> <mml:mo>𝓝</mml:mo> <mml:mo stretchy="false">(</mml:mo> <mml:mrow><mml:mi>x</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo></mml:mrow> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo stretchy="false">)</mml:mo> <mml:mi>d</mml:mi> <mml:mi>x</mml:mi> <mml:mo>≈</mml:mo> <mml:msqrt><mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>π</mml:mi> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>/</mml:mo> <mml:mn>8</mml:mn></mml:mrow></mml:msqrt> <mml:mo form="prefix">log</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:mi>μ</mml:mi> <mml:msqrt><mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>π</mml:mi> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>/</mml:mo> <mml:mn>8</mml:mn></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
from Eq 8 from [<xref ref-type="bibr" rid="pcbi.1004464.ref023">23</xref>]. This approximation is valid on a limited range, and is inaccurate for low <italic>μ</italic>. However, this can be corrected by adjusting by the gradient, as we explain in section D <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>.</p></list-item> <list-item><p><xref ref-type="disp-formula" rid="pcbi.1004464.e003">Eq 2</xref> for the neuronal input and Eqs (<xref ref-type="disp-formula" rid="pcbi.1004464.e010">9</xref>)–(<xref ref-type="disp-formula" rid="pcbi.1004464.e011">10</xref>) for the spike statistics, which yields
<disp-formula id="pcbi.1004464.e017"><alternatives><graphic id="pcbi.1004464.e017g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e017"/><mml:math id="M17" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>m</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>T</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mo>Σ</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p></list-item></list></p>
<p>Though the loglikelihood in <xref ref-type="disp-formula" rid="pcbi.1004464.e015">Eq 13</xref> has already become tractable (and depends only on the sufficient statistics from Eqs (<xref ref-type="disp-formula" rid="pcbi.1004464.e010">9</xref>)–(<xref ref-type="disp-formula" rid="pcbi.1004464.e011">10</xref>), we can simplify it further by maximizing it over <bold>b</bold>. To do so, we equate the derivative of the simplified loglikelihood (<xref ref-type="disp-formula" rid="pcbi.1004464.e015">Eq 13</xref>) to zero
<disp-formula id="pcbi.1004464.e018"><alternatives><graphic id="pcbi.1004464.e018g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e018"/><mml:math id="M18" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>b</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">ln</mml:mo> <mml:mi>P</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi mathvariant="bold">S</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
Solving this equation, we obtain
<disp-formula id="pcbi.1004464.e019"><alternatives><graphic id="pcbi.1004464.e019g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e019"/><mml:math id="M19" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>b</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msqrt><mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>π</mml:mi> <mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mo>Σ</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>/</mml:mo> <mml:mn>8</mml:mn></mml:mrow></mml:msqrt> <mml:mo form="prefix">ln</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>m</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
Substituting this maximizer into <xref ref-type="disp-formula" rid="pcbi.1004464.e015">Eq 13</xref>, we obtain <xref ref-type="disp-formula" rid="pcbi.1004464.e009">Eq 8</xref>.</p>
</sec>
</sec>
<sec id="sec009">
<title>3 Observation schemes</title>
<p>As we showed in section 2, in order to infer network connectivity, we just need to estimate the first and second empiric spike statistics, defined in Eqs (<xref ref-type="disp-formula" rid="pcbi.1004464.e010">9</xref>)–(<xref ref-type="disp-formula" rid="pcbi.1004464.e011">10</xref>). These statistics cannot be calculated exactly if some observations are missing; in this case they must be estimated, as we discuss in section 3.6 below. First, though, it is useful to discuss a few concrete examples of the partial network observation schemes we are considering (<xref ref-type="fig" rid="pcbi.1004464.g001">Fig 1</xref>). We discuss the pros and cons of each scheme in terms of both inferential and experimental constraints.</p>
<fig id="pcbi.1004464.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004464.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Observation scheme examples.</title>
<p>In each scheme (the different rows) we observe two out of ten neurons in each time bin: <bold>(A,B)</bold> Fixed subset <bold>(C,D)</bold> Serial <bold>(E,F)</bold> Fully Random, <bold>(G,H)</bold> Random Blocks, and <bold>(I,J)</bold> double serial. <italic>Left</italic> (A,C,E,G,I): A sample of the observations <bold>O</bold> demonstrating the scanning method (a zero-one matrix, <xref ref-type="disp-formula" rid="pcbi.1004464.e004">Eq 3</xref>). <italic>Right</italic> (B,D,F,H,J): empirical frequency of observed neuron pairs <inline-formula id="pcbi.1004464.e020"><alternatives><graphic id="pcbi.1004464.e020g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e020"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="true">⟨</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo stretchy="true">⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, with saturated colors to accentuate differences between methods (all values above 0.05 are shown in yellow). In the “fixed” scheme, some neurons are never observed. In the “serial” scheme some neuronal pairs are never observed. In all other schemes, all neuronal pairs are observed, so we can estimate the empirical moments using Eqs (<xref ref-type="disp-formula" rid="pcbi.1004464.e022">16</xref>)–(<xref ref-type="disp-formula" rid="pcbi.1004464.e025">17</xref>) and infer connectivity. In the two bottom schemes, observations are collected in persistent blocks, so neuron pairs which are close to the diagonal are observed more often.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004464.g001"/>
</fig>
<sec id="sec010">
<title>3.1 Fixed subset observations</title>
<p>The simplest and most commonly used observation scheme is simply to image a fixed subset of the network. To increase the size of the subset, we must image with a lower frame rate. However, since we observe only a subset of the neurons and neuronal pairs (<xref ref-type="fig" rid="pcbi.1004464.g001">Fig 1A, 1B</xref>), we can estimate only a subset of the mean spike probabilities <bold>m</bold> and spike covariances <bold>Σ</bold><sup>(<italic>k</italic>)</sup>. If we attempt to infer connectivity using only these incomplete empirical moments (<italic>i.e</italic>., by “pretending” the unobserved neurons do not exist), a persistent bias may be present in our estimate. This is due to the common input problem, as discussed in more depth in section 4 below.</p>
</sec>
<sec id="sec011">
<title>3.2 Serial subset observations</title>
<p>An alternative natural approach is to continuously shift the observed subset (<xref ref-type="fig" rid="pcbi.1004464.g001">Fig 1C, 1D</xref>): observe a given volume of tissue, then move the microscope (or specimen) to the left a bit, then repeat this procedure in a scanning fashion. However, under this approach some neuron pairs are still never observed. Specifically, as can be seen in <xref ref-type="fig" rid="pcbi.1004464.g001">Fig 1D</xref>, if the size of the scanning block is <italic>k</italic>, we do not observe neuron pairs for which ∣<italic>i</italic>−<italic>j</italic>∣ &gt; <italic>k</italic> (<italic>i.e</italic>., ⟨<italic>O</italic><sub><italic>i</italic>, <italic>t</italic></sub> <italic>O</italic><sub><italic>j</italic>, <italic>t</italic>−1</sub>⟩<sub><italic>T</italic></sub> = 0 for these pairs). Since we do not observe all pairs, it is not always possible to infer the spike covariances (<bold>Σ</bold><sup>(<italic>k</italic>)</sup>, <xref ref-type="disp-formula" rid="pcbi.1004464.e011">Eq 10</xref>), which may be required for inferring connectivity.</p>
</sec>
<sec id="sec012">
<title>3.3 Fully randomized subsets</title>
<p>In order to accurately infer spike covariances, we examine a different observation method. If we randomly generate our observations (<italic>O</italic><sub><italic>i</italic>, <italic>t</italic></sub> = 1 with probability <italic>p</italic><sub>obs</sub>, and otherwise <italic>O</italic><sub><italic>i</italic>, <italic>t</italic></sub> = 0), then all neuron pairs are uniformly observed (<italic>i.e</italic>., <inline-formula id="pcbi.1004464.e021"><alternatives><graphic id="pcbi.1004464.e021g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e021"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="true">⟨</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo stretchy="true">⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mtext mathvariant="normal">obs</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, ∀<italic>i</italic>, <italic>j</italic>); <xref ref-type="fig" rid="pcbi.1004464.g001">Fig 1E, 1F</xref>. In this case, it is easy to estimate both <bold>m</bold> and <bold>Σ</bold><sup>(<italic>k</italic>)</sup>, as explained in section 3.6 below. However, this observation scheme is experimentally infeasible, as we cannot infer spikes from fluorescence traces if neurons are observed for too short a time.</p>
</sec>
<sec id="sec013">
<title>3.4 Persistent block observations</title>
<p>To facilitate spike inference from fluorescence traces, we can randomly select a block of <italic>p</italic><sub>obs</sub> <italic>N</italic> neurons from the network, and observe this block for a sufficiently long time and with a sufficiently high frame rate, so that all spikes within the block can be inferred accurately (<xref ref-type="fig" rid="pcbi.1004464.g001">Fig 1G, 1H</xref>). Again, we can easily estimate both <bold>m</bold> and <bold>Σ</bold><sup>(<italic>k</italic>)</sup> (section 3.6).</p>
<p>Randomly selecting blocks can be technically challenging. Within the field of view, this can be done using an acousto-optical deflector [<xref ref-type="bibr" rid="pcbi.1004464.ref024">24</xref>] or a spatial light modulator [<xref ref-type="bibr" rid="pcbi.1004464.ref025">25</xref>]. Enlarging the field of view of these methods remains an open experimental challenge, however.</p>
<p>Of course, non-random block scanning approaches are also possible. An alternative approach could be to employ light sheet methods [<xref ref-type="bibr" rid="pcbi.1004464.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref026">26</xref>] and then slowly rotate the angle the light sheet forms as it passes through the specimen; as this angle changes, we will collect statistics involving groups of neurons in different planes. The estimation of <bold>m</bold> and <bold>Σ</bold><sup>(<italic>k</italic>)</sup> would remain straightforward in this case.</p>
</sec>
<sec id="sec014">
<title>3.5 Double serial scanning</title>
<p>Finally, we note that combinations of the above schemes are possible. One such approach uses two simultaneous serial scans. For example, we can use a lexicographic scheme with two scanners–we first observe a fixed subset with one scanner, and, at the same time, serially observe the remaining blocks of the network with another scanner. Then, we move the first scanner to a different area, and perform another full scan with the second. We continue this way until we have completed a full scan of the network with the first scanner. Alternatively, we do not have to wait until the second scanner has finished a complete scan of the network. Instead, we can continuously scan with both scanners. If the scanning periods are incommensurate (<italic>i.e</italic>., their ratio is an irrational number) then eventually, we will observe all neuron pairs, as illustrated in <xref ref-type="fig" rid="pcbi.1004464.g001">Fig 1I, 1J</xref>. Such a dual-scanning scheme would of course pose some significant engineering challenges, but recent progress in imaging technology provides some hope that these challenges will be surmountable [<xref ref-type="bibr" rid="pcbi.1004464.ref027">27</xref>].</p>
</sec>
<sec id="sec015">
<title>3.6 Moment estimation</title>
<p>Next, we explain how the empirical moments can be estimated when there are missing observations. Perhaps the simplest estimate of these mean spike probabilities ignores any missing observations and just re-normalizes the empirical sums accordingly:
<disp-formula id="pcbi.1004464.e022"><alternatives><graphic id="pcbi.1004464.e022g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e022"/><mml:math id="M22" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>m</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub><mml:mspace width="1pt"/><mml:mo>≜</mml:mo><mml:mspace width="1pt"/><mml:mfrac><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
This estimate is consistent (<italic>i.e</italic>., <inline-formula id="pcbi.1004464.e023"><alternatives><graphic id="pcbi.1004464.e023g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e023"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>m</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>→</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> when <italic>T</italic> → ∞), since
<disp-formula id="pcbi.1004464.e024"><alternatives><graphic id="pcbi.1004464.e024g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e024"/><mml:math id="M24" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub></mml:mfrac></mml:mtd> <mml:mtd><mml:mover><mml:mo>→</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mover></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub></mml:mrow> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub></mml:mfrac> <mml:mover><mml:mo>→</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mover> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where we have assumed that
<list list-type="simple"><list-item><p>1. The observation process is uncorrelated with the spikes.</p></list-item> <list-item><p>2. ⟨<italic>O</italic><sub><italic>i</italic>, <italic>t</italic></sub>⟩<sub><italic>T</italic></sub> converges to a strictly positive limit ∀<italic>i</italic>.</p></list-item></list></p>
<p>The first condition is typically the case in most experiments. The second condition implies we observe each neuron for a large number of time bins; importantly, this condition does <italic>not</italic> hold in the fixed subset observation scheme. Similarly,
<disp-formula id="pcbi.1004464.e025"><alternatives><graphic id="pcbi.1004464.e025g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e025"/><mml:math id="M25" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mover accent="true"><mml:mo>Σ</mml:mo> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mtd> <mml:mtd><mml:mspace width="1pt"/><mml:mo>≜</mml:mo><mml:mspace width="1pt"/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msub></mml:mfrac> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>m</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi>m</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>→</mml:mo> <mml:msup><mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
if we additionally assume that
<list list-type="simple"><list-item><p>3. ⟨<italic>O</italic><sub><italic>i</italic>, <italic>t</italic></sub> <italic>O</italic><sub><italic>j</italic>, <italic>t</italic>−<italic>k</italic></sub>⟩<sub><italic>T</italic></sub> converges to a strictly positive limit ∀<italic>i</italic>, <italic>j</italic> and ∀<italic>k</italic> ∈ {0,1}.</p></list-item></list></p>
<p>This third condition implies that we observe each neuron pair (either at the same time, or delayed) for a large number of time bins (recall that this condition does not hold in the serial observation scheme).</p>
<p>This direct approach is simple and computationally quite cheap. However, it seems to ignore potentially useful information: if we could “fill in” the activity in the unobserved bins of the <italic>S</italic><sub><italic>i</italic>, <italic>t</italic></sub> matrix, we would increase our effective sample size and therefore estimate the required moments more accurately. We have experimented with a couple approximate Bayesian methods for filling in this missing information (as detailed in more depth in the section E <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>), and somewhat surprisingly have found that they do not greatly improve the estimation accuracy, while imposing significant computational cost. See section 6.1 below for further details.</p>
</sec>
</sec>
</sec>
<sec id="sec016" sec-type="results">
<title>Results</title>
<p>Our goal in this section is to demonstrate numerically that connectivity can be inferred, efficiently and accurately, from highly sub-sampled spike data. Readers can find the basic notation used in this section in <xref ref-type="table" rid="pcbi.1004464.t001">Table 1</xref>. In section 4, we give a qualitative demonstration that the shotgun approach can be used to significantly decrease the usual persistent bias resulting from common inputs. In section 5, we perform quantitative tests to show that our estimation method is effective and robust; we perform parameter scans for various network sizes, observation probabilities, firing rates and connection sparsities. In section 6, we show that our Expected LogLikelihood (ELL) based estimation method is efficient, both statistically and computationally. Finally, in section 7, we infer connectivity from fluorescence measurements.</p>
<table-wrap id="pcbi.1004464.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004464.t001</object-id>
<label>Table 1</label>
<caption>
<title>Basic notation.</title>
</caption>
<alternatives>
<graphic id="pcbi.1004464.t001g" mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004464.t001"/>
<table frame="box" rules="all" border="0">
<colgroup span="1">
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
</colgroup>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>N</italic></td>
<td align="left" rowspan="1" colspan="1">Total number of neurons</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>T</italic></td>
<td align="left" rowspan="1" colspan="1">Total number of time bins</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>p</italic><sub>obs</sub></td>
<td align="left" rowspan="1" colspan="1">Empiric observation probability—the mean fraction of neurons observed at each
time bin</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>p</italic><sub>conn</sub></td>
<td align="left" rowspan="1" colspan="1">Network sparsity—the average probability that two neurons are directly
connected</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>S</bold></td>
<td align="left" rowspan="1" colspan="1"><italic>N</italic> × <italic>T</italic> matrix of spike activity</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>W</bold></td>
<td align="left" rowspan="1" colspan="1"><italic>N</italic> × <italic>N</italic> matrix of synaptic connection weights</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold><bold>U</bold></bold></td>
<td align="left" rowspan="1" colspan="1"><italic>N</italic> × <italic>T</italic> matrix of neuronal inputs</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>b</bold></td>
<td align="left" rowspan="1" colspan="1"><italic>N</italic>×1 vector of neuronal biases</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>O</bold></td>
<td align="left" rowspan="1" colspan="1"><italic>N</italic> × <italic>T</italic> binary matrix denoting when neurons are observed</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>m</bold></td>
<td align="left" rowspan="1" colspan="1"><italic>N</italic>×1 vector of mean spike probability (firing rates)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Σ</bold><sup>(<italic>k</italic>)</sup></td>
<td align="left" rowspan="1" colspan="1"><italic>N</italic> × <italic>N</italic> matrix of mean spike covariances with lag <italic>k</italic></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<sec id="sec017">
<title>4 The common input problem</title>
<p>In this section we use a toy network with <italic>N</italic> = 50 neurons to visualize the common input problem, and its suggested solution—the “shotgun” approach.</p>
<p>Errors caused by common inputs are particularly troublesome for connectivity estimation, since they can persist even as <italic>T</italic> → ∞. Therefore, for simplicity, we work in a regime where the experiment is long and data is abundant (<italic>T</italic> = 5⋅10<sup>8</sup> timebins). In this regime, any prior information we have on the connectivity becomes unimportant so we simply use the Maximum Likelihood (ML) estimator. We chose the weight matrix <bold>W</bold> to illustrate a “worst-case” common input condition (<xref ref-type="fig" rid="pcbi.1004464.g002">Fig 2A</xref>). Note that the upper-left third of <bold>W</bold> is diagonal (<xref ref-type="fig" rid="pcbi.1004464.g002">Fig 2B</xref>): <italic>i.e</italic>., neurons <italic>i</italic> = 1, …, 16 share no connections to each other, other than the self-connection terms <italic>W</italic><sub><italic>i</italic>, <italic>i</italic></sub>. However, we have seeded this <bold>W</bold> with many common-input motifs, in which neurons <italic>i</italic> and <italic>j</italic> (with <italic>i</italic>, <italic>j</italic> ≤ 16) both receive common input from neurons <italic>k</italic> with <italic>k</italic> ≥ 17.</p>
<fig id="pcbi.1004464.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004464.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Visualization of the persistence of the common input problem, despite a large amount of spiking data, and its suggested solution—the shotgun approach.</title>
<p><bold>(A)</bold> The true connectivity—the weight matrix <bold>W</bold> of a network with <italic>N</italic> = 50 neurons. <bold>(B)</bold> A zoomed-in view of the top 16 neurons in A (upper left white rectangle in A). <bold>(C)</bold> The same zoomed-in view of the top 16 neurons in the ML estimate of the weight matrix <bold>W</bold> (Eq 25), where we used the shotgun (random blocks) observation scheme on the whole network, with a random observation probability of <italic>p</italic><sub>obs</sub> = 16/50. <bold>(D)</bold> The ML estimator of the weight matrix <bold>W</bold> of the top 16 neurons if we observe only these neurons. Note the unobserved neurons cause false positives in connectivity estimation. These “spurious connections” do not vanish even when we have a large amount of spike data. In contrast, the shotgun approach (C), does not have these persistent errors, since it spreads the same number of observations evenly over the network. <italic>T</italic> = 5⋅10<sup>8</sup>, <italic>b</italic><sub><italic>i</italic></sub> ∼ 𝓝(−0.5, 0.1).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004464.g002"/>
</fig>
<p>If we use a “shotgun” approach and observe the whole network with <italic>p</italic><sub>obs</sub> = 16/50 with a fully random observation scheme, we obtain a good ML estimate of the network connectivity, including the 16 × 16 upper-left submatrix (<xref ref-type="fig" rid="pcbi.1004464.g002">Fig 2C</xref>). Now, suppose instead we concentrate all our observations on these 16 neurons, so that <italic>p</italic><sub>obs</sub> = 1 within that sub-network, but the other neurons are unobserved. If common input was not a problem, our estimation quality should improve on that submatrix (since we have more measurements per neuron). However, if common noise is problematic, then we will “hallucinate” many nonexistent connections (i.e., off-diagonal terms) in this submatrix. <xref ref-type="fig" rid="pcbi.1004464.g002">Fig 2D</xref> illustrates this phenomenon. In contrast to the shotgun case, the resulting estimates are significantly corrupted by the common input effects.</p>
</sec>
<sec id="sec018">
<title>5 Connectivity estimation—quantitative analysis</title>
<p>Next, we quantitatively test the performance of the Maximum A Posteriori (MAP) estimate of the network connectivity matrix <bold>W</bold> using a detailed network model with biologically plausible parameters from the mouse visual cortex. Details on the network parameters, simulation details and definitions of the quality measures are given in section B in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>. We use the inference method described in section 2, with a sparsity inducing prior (section C in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>) on a simulated network with GLM neurons (Eqs (<xref ref-type="disp-formula" rid="pcbi.1004464.e002">1</xref>)–(<xref ref-type="disp-formula" rid="pcbi.1004464.e003">2</xref>)).</p>
<p>First, in <xref ref-type="fig" rid="pcbi.1004464.g003">Fig 3</xref>, we examine a small GLM network with <italic>N</italic> = 50 observed neurons, with an experiment length of 5.5 hours. As can be seen, the weight matrix can be very accurately estimated for high values of observation probability <italic>p</italic><sub>obs</sub>, and reasonably well even for low value of <italic>p</italic><sub>obs</sub>. For example, even if <italic>p</italic><sub>obs</sub> = 0.04, and <italic>only two neurons</italic> are observed in each timestep, we get a correlation of <italic>C</italic> ≈ 0.84 between inferred weights and the true weights, and the signs of the non-zero weights are only wrong only for 4 weights (out of 448 non-zero weights). When <italic>p</italic><sub>obs</sub> is decreased, the variance of the estimation increases, more weak weights are inferred as zero weights (and vice versa), and we also see more “shrinkage” of the non-diagonal weights (a decreased magnitude of the non-zero weights) due to the <italic>L</italic>1 penalty imposed on them (Eq 47 in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>).</p>
<fig id="pcbi.1004464.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004464.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Network connectivity can be well estimated even with low observation ratios.</title>
<p>With <italic>N</italic> = 50 neurons, and an experiment length of 5.5 hours, we examine various observation probabilities: <italic>p</italic><sub>obs</sub> = 1,0.2,0.1,0.04. <italic>Left</italic> <bold>(A,B,F,J,N)</bold>: weight matrix (either true or estimated). <italic>Middle left</italic> <bold>(C,G,K,O)</bold>: non-zero weights histogram (blue—true, red—estimated). <italic>Middle right</italic> <bold>(D,H,L,P)</bold>: inferred weight vs. true weight. <italic>Right</italic> <bold>(E,I,M,Q)</bold>: quality of estimation—S = sign detection, Z = zero detection, C = correlation, <inline-formula id="pcbi.1004464.e026"><alternatives><graphic id="pcbi.1004464.e026g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e026"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:mtext mathvariant="normal">R</mml:mtext> <mml:mo>=</mml:mo> <mml:msqrt><mml:msup><mml:mtext mathvariant="normal">R</mml:mtext> <mml:mn>2</mml:mn></mml:msup></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> (for exact definitions see Eqs 40–43 in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>); higher values correspond to better estimates. In the first row, we have the true weight matrix <bold>W</bold>. In the other rows we have the inferred <bold>W</bold>—the MAP estimate of the weight matrix with the L1 prior (section C in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>), with <italic>λ</italic> chosen so that the sparsity of the inferred <bold>W</bold> matches that of <bold>W</bold>. Estimation is possible even with very low observation ratios; in the lowest row we observe only 2 neurons out of 50 in each time bin. The weights on the diagonal are estimated better because we observe them more often in double serial scanning scheme (<xref ref-type="fig" rid="pcbi.1004464.g001">Fig 1I, 1J</xref>).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004464.g003"/>
</fig>
<p>In <xref ref-type="fig" rid="pcbi.1004464.g004">Fig 4</xref> we demonstrate that our method works well even if the neuron model is not a GLM, as we assume, but a Leaky Integrate and Fire (LIF) neuron model (<xref ref-type="fig" rid="pcbi.1004464.g004">Fig 4</xref>). The model mismatch results in a weight mismatch by a global multiplicative constant, and in a worse estimate of the diagonal weights, due to the hard reset in the LIF model. Besides these issues, inference results are both qualitatively and quantitatively similar to results in the GLM network</p>
<fig id="pcbi.1004464.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004464.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Network connectivity can be reasonably estimated, even with model mismatch.</title>
<p>The panels (<bold>A-Q</bold>) are the same as <xref ref-type="fig" rid="pcbi.1004464.g003">Fig 3</xref>, where instead of a logistic GLM (<xref ref-type="disp-formula" rid="pcbi.1004464.e002">Eq 1</xref>), we used a stochastic leaky integrate and fire neuron model (in discrete time). In this model, <italic>V</italic><sub><italic>i</italic>, <italic>t</italic></sub> = (<italic>γV</italic><sub><italic>i</italic>, <italic>t</italic>−1</sub>+(1−<italic>γ</italic>)<italic>U</italic><sub><italic>i</italic>, <italic>t</italic></sub>+<italic>ε</italic><sub><italic>i</italic>, <italic>t</italic></sub>)𝓘[<italic>S</italic><sub><italic>i</italic>, <italic>t</italic>−1</sub> = 0] (<bold>U</bold> defined in <xref ref-type="disp-formula" rid="pcbi.1004464.e003">Eq 2</xref>), <italic>S</italic><sub><italic>i</italic>, <italic>t</italic>+1</sub> = 𝓘[<italic>V</italic><sub><italic>i</italic>, <italic>t</italic></sub> &gt; 0.5]. We used <italic>ε</italic><sub><italic>i</italic>, <italic>t</italic></sub> ∼ 𝓝(0,1) as a white noise source. Also, we set <italic>γ</italic> = 20ms<sup>−1</sup>, similar to the inverse of the membrane’s voltage average integration timescale [<xref ref-type="bibr" rid="pcbi.1004464.ref060">60</xref>]. The weights were estimated up to a global multiplicative constant (resulting from the model mismatch), which was adjusted for in the figure. We conclude that our estimation method is robust to modeling errors, except perhaps the diagonal weights—their magnitudes were somewhat over-estimated due to the reset mechanism (which effectively increases self inhibition).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004464.g004"/>
</fig>
<p>In <xref ref-type="fig" rid="pcbi.1004464.g005">Fig 5</xref>, we examine another GLM network with <italic>N</italic> = 1000 observed neurons, which is closer to the scale of the number of recorded neurons in current calcium imaging experiments (see activity simulation in Fig S1 <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>). The experiment duration is again 5.5 hours. Results are qualitatively the same as the case of <italic>N</italic> = 50, except performance is somewhat decreased (as we have more parameters to estimate). Additional information is available in <xref ref-type="fig" rid="pcbi.1004464.g006">Fig 6</xref>. On the left (A,D,G), we see that the algorithm converges properly to a single solution. In the middle panels (B,E,H), we see that for <italic>p</italic><sub>obs</sub> = 1 we have very good performance (in terms of area under the ROC), but this performance declines for the excitatory weights as <italic>p</italic><sub>obs</sub> decreases. The inhibitory weights are correctly detected much better than the excitatory weights. This is because most excitatory weights are much weaker, as can be seen on the right column (C,F,I). In that column, we observe that strong weights are more easily detected than weak weights. Specifically, around the median of the excitatory weight distribution (0.178), we detected 99.9%, 34.9% and 16.1% of all the weights, when <italic>p</italic><sub>obs</sub> = 1,0.2 and 0.1, respectively.</p>
<fig id="pcbi.1004464.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004464.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Network connectivity can be well estimated even with a large network.</title>
<p>The panels (<bold>A-M</bold>) are arranged in columns as in <xref ref-type="fig" rid="pcbi.1004464.g003">Fig 3</xref>, except now we have <italic>N</italic> = 1000 observed neurons and additional 200 unobserved (this is the same simulation as in Fig S1), and <italic>p</italic><sub>obs</sub> = 1, 0.2 and 0.1. In the left (A,B,F,J) and middle right columns (D,H,L) we show a random subset of 50 neurons out of 1000, to improve visibility. The other columns show statistics for all observed neurons. On a standard laptop, the network simulation takes about half an hour, and the connectivity estimate can be produced in minutes. Therefore, our algorithm is scalable, and much faster than standard GLM based approaches, as we explain in section 6.2.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004464.g005"/>
</fig>
<fig id="pcbi.1004464.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004464.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Statistical Analysis.</title>
<p>We use the same network as in <xref ref-type="fig" rid="pcbi.1004464.g005">Fig 5</xref> with <italic>N</italic> = 1000 and <italic>p</italic><sub>obs</sub> = 1, 0.2 and 0.1. <italic>Left</italic> <bold>(A,D,G)</bold>: convergence of performance. Recall that we use the FISTA algorithm (section C.1 in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>) inside an outer loop that sets the regularization parameter according to sparsity (section C.2 in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>). Therefore convergence is non-monotonic, and “jumps” each time the parameter is changed. Each time this happens, it takes about a thousand iterations until convergence. <italic>Middle</italic> <bold>(B,E,H)</bold>: Receiver Operating Characteristic (ROC) curve, showing the trade-off between the false positive rate and the true positive rate (FPR and TPR, Eqs 44 and 45 in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>, receptively) in detecting excitatory (blue) or inhibitory weights (red)—<italic>i.e</italic>., inferring a non zero weight, with the right sign. The curve illustrates the classification performance as the discrimination threshold is varied by changing <italic>λ</italic>, the L1 regularization parameter (Eq 46 in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>). The ‘×’ marks the performance for <italic>λ</italic> chosen by our algorithm. For each case (excitatory/inhibitory), the measures <italic>E</italic> and <italic>I</italic> are the area under the curve—values close to 1 (0.5) indicate good (bad) performance. Performance is significantly better for the inhibitory weights, since they are typically stronger, and we can more easily distinguish non-zero weights. We see this explicitly on the <italic>Right</italic> <bold>(C,F,I)</bold>: Magenta line—fraction of weights detected with the correct sign (-1,0, or 1) as a function of weight value. Line stops if less than 30 weight values exist in that range. For clarity, we added the non-zero weight distribution (shaded gray area, scaled to fit panel) and zoomed on the range [−1, 1]. Weights with magnitude larger then 1 were perfectly detected. Small weights are harder to detect at low <italic>p</italic><sub>obs</sub>.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004464.g006"/>
</fig>
<p>Next, in <xref ref-type="fig" rid="pcbi.1004464.g007">Fig 7</xref> we quantify how inference performance changes with parameters. We vary the number of neurons, <italic>N</italic>, observation probability <italic>p</italic><sub>obs</sub>, mean firing rate <italic>m</italic> and connection sparsity <italic>p</italic><sub>conn</sub>. For the given parameters <italic>N</italic>, <italic>p</italic><sub>obs</sub> and <italic>m</italic>, performance monotonically improves when <italic>T</italic> increases. These scans suggest we can maintain a good quality of connectivity estimation for arbitrarily large or small values of <italic>N</italic> or <italic>p</italic><sub>obs</sub>, respectively—as long as we sufficiently increase <italic>T</italic>. Note there is a lower bound on <italic>T</italic>, below which estimation does not work. Looking at <xref ref-type="fig" rid="pcbi.1004464.g007">Fig 7</xref>, we find that approximately, this lower bound scales as
<disp-formula id="pcbi.1004464.e027"><alternatives><graphic id="pcbi.1004464.e027g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e027"/><mml:math id="M27" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>T</mml:mi> <mml:mo>∝</mml:mo> <mml:mfrac><mml:mi>N</mml:mi> <mml:msubsup><mml:mi>p</mml:mi> <mml:mrow><mml:mtext>obs</mml:mtext></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula>
Above this lower bound, estimation quality gradually improves with <italic>T</italic>. Moreover, in order to maintain good estimation quality (up to some saturation level) above this bound, <italic>T</italic> should be scaled as
<disp-formula id="pcbi.1004464.e028"><alternatives><graphic id="pcbi.1004464.e028g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e028"/><mml:math id="M28" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>T</mml:mi> <mml:mo>∝</mml:mo> <mml:mfrac><mml:mi>N</mml:mi> <mml:mrow><mml:msubsup><mml:mi>p</mml:mi> <mml:mrow><mml:mtext>obs</mml:mtext></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:msup><mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula>
This scaling can be explained intuitively. Our main sufficient statistic is the partially observed spike covariance <inline-formula id="pcbi.1004464.e029"><alternatives><graphic id="pcbi.1004464.e029g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e029"/><mml:math id="M29" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mrow><mml:mtext mathvariant="bold">Σ</mml:mtext> <mml:mspace width="0.333em"/></mml:mrow> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mi>k</mml:mi> <mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1004464.e025">Eq 17</xref>). Each component (<italic>i</italic>, <italic>j</italic>) of <inline-formula id="pcbi.1004464.e030"><alternatives><graphic id="pcbi.1004464.e030g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e030"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mrow><mml:mtext mathvariant="bold">Σ</mml:mtext> <mml:mspace width="0.333em"/></mml:mrow> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mi>k</mml:mi> <mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> contains a sum of all the observed spike pairs (<italic>T</italic>⟨<italic>O</italic><sub><italic>i</italic>, <italic>t</italic></sub> <italic>O</italic><sub><italic>j</italic>, <italic>t</italic>−<italic>k</italic></sub> <italic>S</italic><sub><italic>i</italic>, <italic>t</italic></sub> <italic>S</italic><sub><italic>j</italic>, <italic>t</italic>−<italic>k</italic></sub>⟩<sub><italic>T</italic></sub>) divided by the number of observed neurons (<italic>T</italic>⟨<italic>O</italic><sub><italic>i</italic>, <italic>t</italic></sub> <italic>O</italic><sub><italic>j</italic>, <italic>t</italic>−<italic>k</italic></sub>⟩<sub><italic>T</italic></sub>). The total number of observed neuron pairs is approximately <inline-formula id="pcbi.1004464.e031"><alternatives><graphic id="pcbi.1004464.e031g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e031"/><mml:math id="M31" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi> <mml:mi>T</mml:mi> <mml:msubsup><mml:mi>p</mml:mi> <mml:mtext mathvariant="normal">obs</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> (ignoring observation correlations), and the total number of observed spike pairs is approximately <inline-formula id="pcbi.1004464.e032"><alternatives><graphic id="pcbi.1004464.e032g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e032"/><mml:math id="M32" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi> <mml:mi>T</mml:mi> <mml:msubsup><mml:mi>p</mml:mi> <mml:mtext mathvariant="normal">obs</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:msup><mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> (ignoring spike correlations, and assuming the firing rate is not very high), where <italic>T</italic> is measured in time bins. The total number of components in <inline-formula id="pcbi.1004464.e033"><alternatives><graphic id="pcbi.1004464.e033g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e033"/><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mrow><mml:mtext mathvariant="bold">Σ</mml:mtext> <mml:mspace width="0.333em"/></mml:mrow> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mi>k</mml:mi> <mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is <italic>N</italic><sup>2</sup>. Therefore, in each component of <inline-formula id="pcbi.1004464.e034"><alternatives><graphic id="pcbi.1004464.e034g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e034"/><mml:math id="M34" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mrow><mml:mtext mathvariant="bold">Σ</mml:mtext> <mml:mspace width="0.333em"/></mml:mrow> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mi>k</mml:mi> <mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, the average number of observed neuron pairs is <inline-formula id="pcbi.1004464.e035"><alternatives><graphic id="pcbi.1004464.e035g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e035"/><mml:math id="M35" display="inline" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi> <mml:msubsup><mml:mi>p</mml:mi> <mml:mtext mathvariant="normal">obs</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>/</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, while the average number of observed spike pairs is approximately <inline-formula id="pcbi.1004464.e036"><alternatives><graphic id="pcbi.1004464.e036g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e036"/><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi> <mml:msubsup><mml:mi>p</mml:mi> <mml:mtext mathvariant="normal">obs</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:msup><mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>/</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> (except on the diagonal of <bold>Σ</bold><sup>(0)</sup>, where we have <italic>Tp</italic><sub>obs</sub>/<italic>N</italic> neuron pairs and <italic>Tp</italic><sub>obs</sub> <italic>m</italic>/<italic>N</italic> spikes). We conclude that the number of both observed neuron pairs and spike pairs must be above a certain threshold so that inference will be able to work properly. Above these thresholds, performance improves further when <italic>p</italic><sub>conn</sub> is decreased (<xref ref-type="fig" rid="pcbi.1004464.g007">Fig 7</xref>), as this reduces the effective number of parameters we are required to estimate. For analytic results on this issue see [<xref ref-type="bibr" rid="pcbi.1004464.ref028">28</xref>].</p>
<fig id="pcbi.1004464.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004464.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Parameter scans show that <italic>C</italic>, the correlation between true and estimated connectivity, monotonically increases with <italic>T</italic> in various parameter regimes.</title>
<p>We scan over <bold>(A)</bold> observation probability <italic>p</italic><sub>obs</sub>, <bold>(B)</bold> network size <italic>N</italic>, <bold>(C)</bold> mean firing rate <italic>m</italic> (similar to the firing rate of the excitatory neurons—inhibitory neurons fire approximately twice as fast), and <bold>(D)</bold> connection sparsity parameter <italic>p</italic><sub>0</sub> (which is proportional to actual connection sparsity <italic>p</italic><sub>conn</sub>—see Eq 37 in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>) and experiment duration <italic>T</italic>. Other parameters (when these are not scanned): <italic>p</italic><sub>obs</sub> = 0.2, <italic>N</italic> = 500.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004464.g007"/>
</fig>
<p>If our goal is to infer all the input connections of a single neuron, then performance can be significantly improved if we always observe the output of that neuron. This is demonstrated in <xref ref-type="fig" rid="pcbi.1004464.g008">Fig 8</xref>. In this figure, we examine a single neuron with <italic>O</italic>(10<sup>4</sup>) observed inputs, <italic>O</italic>(10<sup>3</sup>) of which are non-zero (implementation details in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>, section B.2). The inputs are partially observed (with <italic>p</italic><sub>obs</sub> = 1,0.1,0.01), but we always observe the output neuron. Therefore, the average number of observed neuron pairs and spike pairs in <bold>Σ</bold><sup>(1)</sup> is increased to <italic>NTp</italic><sub>obs</sub> and <italic>NmTp</italic><sub>obs</sub>, respectively. This can improve the scaling relations in Eqs (<xref ref-type="disp-formula" rid="pcbi.1004464.e027">18</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1004464.e028">19</xref>) to <italic>T</italic>/<italic>p</italic><sub>obs</sub> and <italic>T</italic>/(<italic>p</italic><sub>obs</sub> <italic>m</italic><sup>2</sup>), respectively, if the off-diagonal terms of <bold>Σ</bold><sup>(0)</sup> are not too strong (since the number of observations for these components still scales with <inline-formula id="pcbi.1004464.e037"><alternatives><graphic id="pcbi.1004464.e037g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e037"/><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:mo>∝</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mtext mathvariant="normal">obs</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>). Thus, in <xref ref-type="fig" rid="pcbi.1004464.g008">Fig 8</xref>, we see that even when <italic>p</italic><sub>obs</sub> = 0.01 it is still possible to estimate strong weights with some accuracy, despite the large number of connections.</p>
<fig id="pcbi.1004464.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004464.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Inferring input connectivity to a single neuron with many inputs and low observation ratios.</title>
<p>The panels <bold>B-D</bold>,<bold>F-H</bold>, and <bold>J-L</bold> are arranged in columns as in <xref ref-type="fig" rid="pcbi.1004464.g003">Fig 3</xref>. In the left column <bold>(A,E,I)</bold> we show a sample of 1000 input weight values from the true (blue) and inferred weights (red), sorted according to the value of the true weights. In the other panels, we show all the weights. We have <italic>N</italic> = 10626 observed inputs, 968 of which have non-zero weights. The output neuron is always observed, while 83% of the input neurons are only partially observed with <italic>p</italic><sub>obs</sub> = 1,0.1,0.01. The rest are never observed. Other network parameters are the same as before (<italic>e.g</italic>., <italic>T</italic> = 5.5 hours), and the firing rate of the output neuron was 2.8Hz. More implementation details in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>, section B.2.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004464.g008"/>
</fig>
</sec>
<sec id="sec019">
<title>6 Expected LogLikelihood-based estimation: accuracy and speed</title>
<sec id="sec020">
<title>6.1 Statistical efficiency</title>
<p>The results of the previous section (mostly, <xref ref-type="fig" rid="pcbi.1004464.g003">Fig 3</xref>, for <italic>p</italic><sub>obs</sub> = 1, as well as our parameter scans) demonstrate numerically that our Expected LogLikelihood-based (ELL) estimation method is effective given sufficiently large observation times <italic>T</italic>. However, it is still not clear if our approximations hurt the statistical efficiency of our estimation. Specifically, can we get a significantly smaller error with the same <italic>T</italic>, if we did not use any approximations? We give numerical evidence in <xref ref-type="fig" rid="pcbi.1004464.g009">Fig 9</xref> that this is not the case. All the parameters are as described in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>, section B.1, except we used a random blocks observation scheme (see <xref ref-type="fig" rid="pcbi.1004464.g001">Fig 1G, 1H</xref>) and did not have any unobserved neurons.</p>
<fig id="pcbi.1004464.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004464.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Expected LogLikelihood (ELL) based estimation is statistically efficient.</title>
<p><italic>Top</italic> <bold>(A,B)</bold>: The ELL-based method (blue) compares favorably to the standard MAP estimate (red) when spikes are fully observed (using the same L1 prior). <italic>Bottom</italic> <bold>(C,D)</bold>: We compare the ELL based method (blue) to the Expectation Maximization (EM) approach, when only 10% of the spikes are observed. We show the results after one (cyan) and two (magenta) EM steps. The EM steps do not improve over the ELL-based method. Parameters: <italic>N</italic> = 50, <italic>T</italic> = 1.4 hours. For the Gibbs sampling we used a single sample after a burn-in period of 30 samples, as we used in our EM simulations without the ELL-based initialization (section E <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004464.g009"/>
</fig>
<p>First, we examine the case in which all the spikes are observed (<italic>p</italic><sub>obs</sub> = 1). We compare our ELL-based estimate with standard MAP optimization of the full likelihood (with the same L1 prior). We implement the latter by plugging the weight gradients of the loglikelihood (<xref ref-type="disp-formula" rid="pcbi.1004464.e008">Eq 7</xref>) in the same optimization algorithm we use for the ELL-based estimate (section C.1 in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>), together with the bias gradients. As can be seen in <xref ref-type="fig" rid="pcbi.1004464.g009">Fig 9A, 9B</xref>, the approximate ELL-based MAP estimate (blue) actually slightly outperforms the accurate MAP estimate (red, which exhibits more shrinkage). These results support the validity of our approximations (See [<xref ref-type="bibr" rid="pcbi.1004464.ref013">13</xref>] for further discussion of how the ELL approximation can in some cases improve the MAP estimate).</p>
<p>Next, we demonstrate numerically that we can safely ignore missing spikes without increasing estimation error, when <italic>p</italic><sub>obs</sub> &lt; 1. Specifically, we want to verify the efficiency of using the “partial” empirical moments (<inline-formula id="pcbi.1004464.e038"><alternatives><graphic id="pcbi.1004464.e038g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e038"/><mml:math id="M38" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold">m</mml:mtext> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004464.e039"><alternatives><graphic id="pcbi.1004464.e039g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e039"/><mml:math id="M39" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext mathvariant="bold">Σ</mml:mtext> <mml:mspace width="0.333em"/></mml:mrow> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>) instead of the full sufficient statistics (<bold>m</bold> and <bold>Σ</bold>), as detailed in section 3.6. These full sufficient statistics can be potentially inferred “correctly” using Bayesian inference techniques such as Markov chain Monte Carlo (MCMC) (section E in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>). In <xref ref-type="fig" rid="pcbi.1004464.g009">Fig 9C, 9D</xref>, we demonstrate that inferring these missing spikes does not improve our estimation. We compare the ELL-based estimate (blue), to the estimate produced by initializing with the ELL-based estimate and then performing one (red) and two (magenta) Expectation-Maximization (EM) steps [<xref ref-type="bibr" rid="pcbi.1004464.ref029">29</xref>]. In more detail, to perform the first EM step we first estimate <bold>W</bold> using the ELL-based method, then Gibbs sample the missing spikes (section E.1.1 in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>), assuming this <bold>W</bold> is the true connectivity, and then infer <bold>W</bold> again using the ELL-based method. For the second step, we initialize <bold>W</bold> with the first step, Gibbs sample the missing spikes, and infer <bold>W</bold>, again using the ELL-based method. This Monte Carlo EM procedure should converge to a local optimizer of the full log-posterior, assuming a sufficient number of MCMC samples are obtained in each iteration [<xref ref-type="bibr" rid="pcbi.1004464.ref029">29</xref>]. Nonetheless, empirically, we see that these EM steps do not help to improve estimation quality here. In addition, using the standard MAP estimate of <bold>W</bold> (instead of the ELL-based estimate) in the EM steps does not qualitatively change these results (data not shown).</p>
</sec>
<sec id="sec021">
<title>6.2 Computational efficiency</title>
<p>An important advantage of the ELL-based method is that it enables extremely fast MAP estimation of the weights (<xref ref-type="disp-formula" rid="pcbi.1004464.e006">Eq 5</xref>). In the standard MAP estimate, we need to calculate all the <italic>N</italic><sup>2</sup> components of the gradient of the original loglikelihood (<xref ref-type="disp-formula" rid="pcbi.1004464.e008">Eq 7</xref>). In total, this requires <italic>O</italic>(<italic>N</italic><sup>3</sup> <italic>T</italic>) operations in each iteration of the optimization procedure. We also find <italic>O</italic>(<italic>N</italic><sup>3</sup> <italic>T</italic>) operations-per-step in other standard estimation methods we tested for <bold><italic>W</italic></bold> (MCMC and variational Bayes, see section E.1.2 in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>). In contrast, in the ELL-based method, the first step is to calculate <inline-formula id="pcbi.1004464.e040"><alternatives><graphic id="pcbi.1004464.e040g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e040"/><mml:math id="M40" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold">m</mml:mtext> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004464.e041"><alternatives><graphic id="pcbi.1004464.e041g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004464.e041"/><mml:math id="M41" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext mathvariant="bold">Σ</mml:mtext> <mml:mspace width="0.333em"/></mml:mrow> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, which takes <italic>O</italic>(<italic>N</italic><sup>2</sup> <italic>T</italic>) operations, but we only need to do this once (this usually takes much less time than the simulation of the network activity, which also takes <italic>O</italic>(<italic>N</italic><sup>2</sup> <italic>T</italic>) operations). Once these are calculated, we need only <italic>O</italic>(<italic>N</italic><sup>3</sup>) operations to calculate the loglikelihood (or its gradient) in each iteration of the optimization algorithm. This results in orders of magnitude improvements in estimation speed over the standard MAP estimate from the original loglikelihood.</p>
<p>For example, in the simulation we show in <xref ref-type="fig" rid="pcbi.1004464.g009">Fig 9A, 9B</xref>, (where <italic>N</italic> = 50 and <italic>T</italic> = 5⋅10<sup>5</sup> time bins) it takes about 11 seconds to run the optimization algorithm using the ELL-based method: approximately one second to calculate the empirical moments, and 10 seconds for the optimization algorithm to converge. A single step of calculating the gradient and updating the weights (in the internal FISTA loop) took 0.002 sec. A similar step took 0.7 sec in the standard MAP estimation. In total, the algorithm took about 4 hours to converge (taking more iterations than the ELL-based method).</p>
<p>As another example, in [<xref ref-type="bibr" rid="pcbi.1004464.ref030">30</xref>], it takes <italic>O</italic>(10<sup>5</sup>) CPU hours using a computer cluster to estimate the connectivity of a network (where <italic>N</italic> = 1000 and <italic>T</italic> = 3.6 ⋅ 10<sup>7</sup> time bins). In our case (where <italic>N</italic> = 1000 and <italic>T</italic> = 2 ⋅ 10<sup>6</sup> time bins), a similar simulation on a standard laptop (<xref ref-type="fig" rid="pcbi.1004464.g005">Fig 5</xref>) takes about half an hour to generate the spikes, together with the sufficient statistics, and a few more minutes to perform the estimation for a given prior distribution. While our model is slightly simpler than that of [<xref ref-type="bibr" rid="pcbi.1004464.ref030">30</xref>], most of this massive improvement in speed is due to the differences in the inference methods used.</p>
<p>Lastly, we note that Gibbs sampling the spikes also requires <italic>O</italic>(<italic>N</italic><sup>3</sup> <italic>T</italic>) operations in each time step (Eq 78 in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>). For example, in the simulation behind <xref ref-type="fig" rid="pcbi.1004464.g009">Fig 9C, 9D</xref>, each Gibb steps for all the spikes took about <italic>2.5</italic> minutes. All the steps took in total about 80 minutes. Therefore, ignoring the missing spikes, instead of sampling them, greatly improves computational speed.</p>
</sec>
</sec>
<sec id="sec022">
<title>7 Fluorescence-based inference</title>
<p>In a real imaging experiment, we would not have direct access to spikes, as we have assumed for simplicity so far. Next, we test the estimation quality when we only have direct access to the fluorescence traces of activity (<xref ref-type="fig" rid="pcbi.1004464.g010">Fig 10A</xref>). The fluorescence traces were generated using a model of GCaMP6f calcium fluorescence indicator. Implementation details are described in section B.3 in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>. As can be seen in <xref ref-type="fig" rid="pcbi.1004464.g010">Fig 10A, 10B</xref>, our spike inference algorithm works reasonably well, both in high and low noise regimes. We then infer network connectivity both from the inferred spikes and the true spikes. As can be seen in <xref ref-type="fig" rid="pcbi.1004464.g010">Fig 10C-10H</xref>, using the inferred spikes usually somewhat reduces estimation performance. This is due to the temporal inaccuracy in the spike estimation. For example, in the inhibitory neurons, the higher firing rates result in more missing spikes in the inference. This causes shrinkage in the magnitude of the inferred weights, since the cross-correlation is weakened by these missing spikes. Combining this information into the inference algorithm (as in [<xref ref-type="bibr" rid="pcbi.1004464.ref009">9</xref>]), it may be possible to correct for this; we have not pursued this question further here. However, even at low observation probabilities (<italic>p</italic><sub>obs</sub> = 0.1), strong weights are inferred reasonably well, and the sign of synapse is usually inferred correctly for almost all nonzero weights. Therefore, weight inference is still possible at low firing rates, using current generation fluorescence imaging methods.</p>
<fig id="pcbi.1004464.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004464.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Inferring connectivity from fluorescence measurements from a network with <italic>N</italic> = 50 observed neurons—for different noise regimes: none (blue), low (yellow, snr = 0.2) and high (brown, snr = 0.4).</title>
<p><bold>(A)</bold> A short sample showing the fluorescence traces, for both noise regimes. On top we show the actual spikes (blue cross), and inferred spikes for low / high noise (yellow/brown triangles). <bold>(B)</bold> Correlation (population mean ± std) between actual and inferred spikes,for both low and high noise regimes. We bin the spikes (both actual and inferred) at various time bin sizes (x axis) and calculate the correlation using the definition of <italic>C</italic> (Eq 41, only for spikes instead of weights). Spikes are reasonably well estimated, given the noisy fluorescence traces. <bold>(C-E)</bold> Estimated weights vs. true weights for <italic>p</italic><sub>obs</sub> = 1, 0.2 and 0.1. <bold>(F-H)</bold> Quality of inference for the C-E, respectively. Blue—spikes are directly measured, Yellow / brown—spikes are inferred from the respective fluorescence traces. Inhibitory weights exhibit more “shrinkage” due to their higher firing rate, which makes it harder to infer spikes from fluorescence. The mean firing rate is 3Hz, and <italic>T</italic> = 5.5 hours.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004464.g010"/>
</fig>
</sec>
</sec>
<sec id="sec023" sec-type="conclusions">
<title>Discussion</title>
<sec id="sec024">
<title>8 Previous works</title>
<p>Neural connectivity inference has attracted much attention in recent years. One approach to this problem is direct anatomical tracing [<xref ref-type="bibr" rid="pcbi.1004464.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref032">32</xref>]. However, this method is computationally challenging [<xref ref-type="bibr" rid="pcbi.1004464.ref033">33</xref>]; moreover, the magnitudes of the synaptic connections (which also vary over time [<xref ref-type="bibr" rid="pcbi.1004464.ref034">34</xref>]) currently cannot be inferred this way. Another approach, on which we focus here, aims to infer the synaptic connectivity from neural activity. This activity can be either action potentials (“spikes”) [<xref ref-type="bibr" rid="pcbi.1004464.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref035">35</xref>–<xref ref-type="bibr" rid="pcbi.1004464.ref037">37</xref>] or calcium fluorescence traces [<xref ref-type="bibr" rid="pcbi.1004464.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref038">38</xref>–<xref ref-type="bibr" rid="pcbi.1004464.ref040">40</xref>] which are approximately a noisy and filtered version of the spikes.</p>
<p>Various inference procedures have been suggested for this purpose. Some works use model-free empirical scores [<xref ref-type="bibr" rid="pcbi.1004464.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref041">41</xref>]. Others assume an explicit generative model for the network activity [<xref ref-type="bibr" rid="pcbi.1004464.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref035">35</xref>–<xref ref-type="bibr" rid="pcbi.1004464.ref037">37</xref>], and then infer connectivity by estimating model parameters. So far, only few works have validated the connectivity estimate with some form of “ground truth”. Gerhard et al. [<xref ref-type="bibr" rid="pcbi.1004464.ref019">19</xref>] inferred small scale anatomical connectivity, comparing different methods. A Generalized Linear Model (GLM) approach was successful, while linear models and model-free approaches failed. Volgushev et al. [<xref ref-type="bibr" rid="pcbi.1004464.ref042">42</xref>] estimated the weights of fictitious synapses (injected current). Again, a GLM-based approach outperformed simple correlation-based approaches. Lastly, Latimer et al. [<xref ref-type="bibr" rid="pcbi.1004464.ref043">43</xref>] was able to infer the magnitude of intracellular synaptic conductances, using a modified GLM. These results indicate that a GLM-based approach should be the method of choice for estimating synaptic connectivity.</p>
<p>The task of inferring synaptic connectivity is severely hindered by technical limitations on the number of neurons that can be simultaneously observed with sufficient quality. Typically, the scanning speed of the imaging device is limited, so we cannot cover the entire network with a high enough frame rate and signal-to-noise ratio to infer spikes from the observed fluorescence traces. Previous studies indicate that at low frame rates (below 30Hz [<xref ref-type="bibr" rid="pcbi.1004464.ref009">9</xref>]), synaptic connectivity cannot be inferred. In such low frame rate regimes, one may use spike correlations or simple dynamical systems as a coarse measure of effective connectivity (<italic>e.g</italic>., [<xref ref-type="bibr" rid="pcbi.1004464.ref044">44</xref>]), but such measures are not claimed to predict synaptic connectivity, only provide a statistical description of the network dynamics.</p>
<p>Therefore, common approaches to infer connectivity of a neural network focus all the observations in one experiment on a small part of the network, in which all neurons are fully observed at a high frame rate. However, unobserved input into this sub-network can generate significant error in the estimation, and this error does not vanish with longer experiments. Various works aimed to deal with this persistent error: [<xref ref-type="bibr" rid="pcbi.1004464.ref017">17</xref>] inferred connectivity in a simulated two-neuron network in which one neuron was never observed; [<xref ref-type="bibr" rid="pcbi.1004464.ref045">45</xref>] inferred connectivity in a simulated network with two observed neurons and an unobserved common input; [<xref ref-type="bibr" rid="pcbi.1004464.ref046">46</xref>] inferred unobserved common input in an experimentally recorded network of 250 neurons using a GLM network with latent variables; [<xref ref-type="bibr" rid="pcbi.1004464.ref047">47</xref>] inferred connectivity in a simulated network with 100 neurons where 20−50 were never observed, with a varying degree of success.</p>
</sec>
<sec id="sec025">
<title>9 The shotgun approach</title>
<p>To help deal with the “common input” problem, we propose a “shotgun” approach, in which we reconstruct network connectivity by serially observing small parts of the network—where each part is observed at a high frame rate for a limited duration. Thus, despite the limited scanning speed of the imaging device, by using this method, we can extend the number of the neurons covered by the scanning device and effectively decrease the number (and therefore the effect) of unobserved common inputs. Additionally, as only a small part of the network is illuminated together, this method can potentially reduce phototoxicity and photobleaching, and allow long, possibly chronic [<xref ref-type="bibr" rid="pcbi.1004464.ref048">48</xref>], imaging experiments.</p>
<sec id="sec026">
<title>9.1 Inferring correlations</title>
<p>Though our goal is to infer synaptic connections, we first discuss the closely related goal of inferring correlations between neurons. It is straightforward to infer these correlations from sub-sampled shotgun data when all neuron pairs can be observed together for long enough durations. We simply have to “ignore” any unobserved activity (section 3.6). We therefore suggest several observation schemes that might be used to eventually observe a much larger fraction of neuron pairs in the network (section 3). For example, we show that this can be done using two serial scanners with incommensurate periods (<xref ref-type="fig" rid="pcbi.1004464.g001">Fig 1I, 1J</xref>). If two scanning systems are combined on the same microscope, it can increase the effective frame rate above the critical 30Hz level [<xref ref-type="bibr" rid="pcbi.1004464.ref009">9</xref>] and allow successful weight reconstruction given long enough experiments. Alternatively, if we can use two moving microscopes to implement this scheme [<xref ref-type="bibr" rid="pcbi.1004464.ref027">27</xref>], the “effective field of view”, could be expanded to any region that is not visually obstructed (such as deep regions in the tissue). This expansion can be arbitrarily large, again, as long as the experimental duration is long enough to compensate.</p>
<p>It may be also possible to infer correlations even if not all neuron pairs are observed (e.g., in a serial scanning scheme). For example, the methods discussed in [<xref ref-type="bibr" rid="pcbi.1004464.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref049">49</xref>] might be helpful, if the fraction of observed neurons is not too low. In [<xref ref-type="bibr" rid="pcbi.1004464.ref044">44</xref>], in which experimentally recorded spikes are divided into two minimally overlapping blocks, the covariance matrix could only be accurately completed if more than 60% of the neurons were observed in each block (so, in total, 68% of all neuron pairs). Another covariance matrix completion method loosely requires that the size of the overlapping regions between the blocks must be larger than the rank of the full matrix [<xref ref-type="bibr" rid="pcbi.1004464.ref049">49</xref>]. It is not yet clear when these conditions apply in a physiologically relevant regime. And so, it remains to be seen if such methods could be used when only small fraction of all neurons is observed in each block.</p>
</sec>
<sec id="sec027">
<title>9.2 Inferring connections</title>
<p>As we discussed in the previous section, it relatively straightforward to infer neuronal correlations, given enough observed neural pairs. It is also relatively easy to infer a linear-Gaussian model of the network activity with missing observations [<xref ref-type="bibr" rid="pcbi.1004464.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref050">50</xref>], since we can analytically integrate out any unknown observations. However, as mentioned earlier (section 8) when inferring actual synaptic connectivity from real data, correlation and linear-based methods are inferior to a GLM-based approach.</p>
<p>Connectivity estimation with missing observations in a GLM is particularly challenging. Standard inference methods (maximum likelihood or maximum a posteriori) cannot be used, since the GLM likelihood cannot be evaluated without first inferring the missing spikes. However, exact Bayesian inference of the unobserved spikes is generally intractable. Therefore, previous works approximated the unobserved spikes through sampling [<xref ref-type="bibr" rid="pcbi.1004464.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref051">51</xref>], using Markov Chain Monte-Carlo (MCMC) methods on a GLM. However, such methods typically do not scale well for large networks. In fact, even if all the spikes are observed, inferring network connectivity using GLMs is very slow—taking about 10<sup>5</sup> CPU hours for a network with a thousand neurons in the recent work of [<xref ref-type="bibr" rid="pcbi.1004464.ref030">30</xref>].</p>
<p>In order to infer connectivity from this type of sub-sampled data we developed an Expected LogLiklihood (ELL) based method, which approximates the loglikelihood and its gradients so they depend on the spikes only through easily estimated second order statistics. By ignoring missing spikes in these statistics, we can infer neural network connectivity even when the spike data is (heavily) sub-sampled. This way we avoid the task of inferring the unobserved spikes, which requires computationally expensive latent variable approaches (section E) in <xref ref-type="supplementary-material" rid="pcbi.1004464.s001">S1 Text</xref>, as in [<xref ref-type="bibr" rid="pcbi.1004464.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref051">51</xref>]. Even when all neurons are observed, the computational complexity drastically improves (section 6.2)—from <italic>O</italic>(<italic>N</italic><sup>3</sup> <italic>TK</italic>) in standard algorithms, to <italic>O</italic>(<italic>N</italic><sup>3</sup> <italic>K</italic>+<italic>N</italic><sup>2</sup> <italic>T</italic>) in the ELL-based method, where <italic>K</italic> is the number of iterations in the algorithm (<italic>K</italic> did not increase in the ELL-based estimation).</p>
</sec>
<sec id="sec028">
<title>9.3 Numerical results</title>
<p>We demonstrate numerically (section 3.6) that such a double serial scanning method can be used to estimate the synaptic connectivity of a spiking neural network with connectivity roughly similar to that of the mouse visual cortex. We show that the inference is possible even if the spike data is sub-sampled at arbitrarily low observation ratios (<italic>e.g</italic>., 10% in a network model with <italic>N</italic> = 1000 neurons, Figs <xref ref-type="fig" rid="pcbi.1004464.g005">5</xref> and <xref ref-type="fig" rid="pcbi.1004464.g006">6</xref>, or 1% in single neuron model with <italic>O</italic>(10<sup>4</sup>) inputs, <xref ref-type="fig" rid="pcbi.1004464.g008">Fig 8</xref>); if the actual neuron model is not a GLM (a LIF model, <xref ref-type="fig" rid="pcbi.1004464.g004">Fig 4</xref>); and if fluorescence traces are observed instead of spikes (<xref ref-type="fig" rid="pcbi.1004464.g010">Fig 10</xref>). We perform parameter scans to examine the robustness of our method, and find the amount of data required for accurate shotgun reconstruction (<xref ref-type="fig" rid="pcbi.1004464.g007">Fig 7</xref>). Additionally, we confirm the accuracy and efficiency of our ELL-based method, in comparison to existing methods (<xref ref-type="fig" rid="pcbi.1004464.g009">Fig 9</xref>).</p>
<p>These results indicate that by using the shotgun observation scheme, we can remove the persistent bias resulting from the common input problem (<xref ref-type="fig" rid="pcbi.1004464.g002">Fig 2</xref>). Therefore, the limited scanning speed of imaging devices is not a fundamental obstacle hindering connectivity estimation. A complete removal of the bias is possible only if all the neurons in the network are observed together with all inputs to the network for a sufficient length of experimental time. However, in most experimental setups, some neurons will never be observed. Therefore, some persistent bias may remain. We modelled such a small bias in all simulations by adding a small number of neurons which are never observed. As we demonstrate numerically, this did not have a strong effect on our results. Stronger common inputs may require the incorporation of latent variables in the model, as in [<xref ref-type="bibr" rid="pcbi.1004464.ref046">46</xref>]; this is conceptually straightforward, and is an important direction for future research.</p>
<p>Clearly, the most important test for a connectivity inference method is on experimental data. Typically, on real data, performance is almost never as good as in simulations. Moreover, our numerical results suggest that, though our method is clearly much faster and more scalable than previous approaches, it still requires a substantial amount of data (hours). For low amounts of data (e.g., due to low observation ratios) it is likely to capture only the strongest connections accurately (<xref ref-type="fig" rid="pcbi.1004464.g006">Fig 6</xref>). These limitations are not properties of the method, but rather properties of the problem at hand and the type and amount of data typically available. For example, it was previously demonstrated in [<xref ref-type="bibr" rid="pcbi.1004464.ref042">42</xref>] that a significant amount of data is required to infer weak weights. However, there are a few potential extensions to the inference method that may significantly improve performance, as we explain next.</p>
</sec>
</sec>
<sec id="sec029">
<title>10 Challenges and future directions</title>
<p>We showed here that the proposed method is capable of incorporating prior information about the sparsity of synaptic connections. More specific information could be included. An abundance of such prior information is available for both connection probabilities and synaptic weight distributions as a function of cell location and identity [<xref ref-type="bibr" rid="pcbi.1004464.ref052">52</xref>]. Cutting edge labeling and tissue preparation methods such as Brainbow [<xref ref-type="bibr" rid="pcbi.1004464.ref053">53</xref>] and CLARITY [<xref ref-type="bibr" rid="pcbi.1004464.ref054">54</xref>] are beginning to provide rich anatomical data about “potential connectivity” (<italic>e.g</italic>., the degree of coarse spatial overlap between a given set of dendrites and axons) that can be incorporated into these priors. Exploiting such prior information can significantly improve inference quality, as demonstrated in previous network inference papers [<xref ref-type="bibr" rid="pcbi.1004464.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref055">55</xref>]. For example, by adjusting the L1 regularization parameters, we can reflect such additional priors: that the probability of having a connection between two neurons typically decreases with the distance between two neurons, and that it is affected by the neuronal type.</p>
<p>Another way to improve connectivity estimates is to use stimulus information. For example, increasing the firing rate can improve quality (<xref ref-type="disp-formula" rid="pcbi.1004464.e028">Eq 19</xref> and <xref ref-type="fig" rid="pcbi.1004464.g007">Fig 7</xref>), up to a limit. If the firing rate is too high, it becomes harder to infer spikes from fluorescence. A more sophisticated spatio-temporal stimulus scheme can potentially lead to significant improvements in estimation quality [<xref ref-type="bibr" rid="pcbi.1004464.ref056">56</xref>]. The type of stimulus used can also affect performance. Sensory stimulation usually affects the measured network indirectly, potentially through many layers of neuronal processing. This may result in undesirable common input (“noise correlations”). Optogenetic stimulation does not have this problem, since it stimulates neurons directly by using light sensitive ion channels. However, this type of optical stimulation can potentially interfere with optical recording. Such cross-talk can be minimized by using persistent ion channels [<xref ref-type="bibr" rid="pcbi.1004464.ref057">57</xref>] (which require only a brief optical stimulus to be activated), or more sophisticated types of stimulation schemes [<xref ref-type="bibr" rid="pcbi.1004464.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1004464.ref059">59</xref>]. Such optogenetic approaches, coupled with the inference and experimental design methods described here, have the potential to lead to significantly improved connectivity estimates.</p>
<p>Even if all the neuronal inputs are eventually observed, if the observation probability <italic>p</italic><sub>obs</sub> is low then the variance due to the unobserved inputs may still be high, since, at any given time, most of the inputs to each neuron will be unobserved (see also [<xref ref-type="bibr" rid="pcbi.1004464.ref028">28</xref>]). As a result, the duration of the experiment required for accurate inference increases quadratically with the inverse of the observation probability (Eqs (<xref ref-type="disp-formula" rid="pcbi.1004464.e027">18</xref>)–(<xref ref-type="disp-formula" rid="pcbi.1004464.e028">19</xref>) and <xref ref-type="fig" rid="pcbi.1004464.g007">Fig 7</xref>), and weak weights become much harder to infer (<xref ref-type="fig" rid="pcbi.1004464.g006">Fig 6</xref>). Note this variance may be significantly reduced if we only aim to infer the input connections to only a few neurons (<xref ref-type="fig" rid="pcbi.1004464.g008">Fig 8</xref>). However, in many cases we wish to infer the entire network. In those cases the variance issue will persist, for any fixed observation strategy that does not take into account any prior information on the network connectivity.</p>
<p>However, there might be a significant improvement in performance if we can focus the observations on synaptic connections which are more probable. This way, we can effectively reduce input noise from unobserved neurons, and improve the signal to noise ratio. As a simple example, suppose we know the network is divided into several disconnected components. In this case, we should scan each sub-network separately, <italic>i.e</italic>., there is no point in interleaving spike observations from two disconnected sub-networks. How should one focus observations in the more general case, making use of past observations in an online manner? Again, we leave this “active learning” problem as an important direction for future research.</p>
</sec>
<sec id="sec030" sec-type="conclusions">
<title>11 Conclusions</title>
<p>In this work we suggest a “shotgun” experimental design, in which we infer the connectivity of a neural network from highly sub-sampled spike data. This is done in order to overcome experimental limitations stemming from the bounded scanning speed of any imaging device.</p>
<p>To do this, we develop a statistical expected loglikelihood-based Bayesian method. This method formally captures the intuitive notion that empiric spike correlations and mean spike rates are approximately the sufficient statistics for connectivity inference. Exploiting these sufficient statistics, our method has two major advantages over previous related approaches: (1) it is orders of magnitude faster (2) it can be used even when the spike data is massively sub-sampled.</p>
<p>We show that by using a double serial scanning scheme, all spike rates and correlations can be eventually inferred (and therefore neural connectivity). We demonstrate numerically that our method works efficiently in a simulated model with highly sub-sampled data and thousands of neurons. We conclude that the limited scanning speed of an imaging device recording neuronal activity is not a fundamental barrier which prevents consistent estimation of network connectivity.</p>
</sec>
</sec>
<sec id="sec031">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004464.s001" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004464.s001" mimetype="application/pdf" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Technical appendix with full mathematical derivations and algorithmic details.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors are thankful to Eftychios Pnevmatikakis, Ari Pakman and Ben Shababo for their help and support, and to Ran Rubin, Christophe Dupre, Lars Buesing and Yuriy Mishchenko for their helpful comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004464.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Katona</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Szalay</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Maák</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Kaszás</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Veress</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hillier</surname> <given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Fast two-photon in vivo imaging with three-dimensional random-access scanning in large tissue volumes</article-title>. <source>Nat Methods</source>. <year>2012</year>;<volume>9</volume>(<issue>2</issue>):<fpage>201</fpage>–<lpage>208</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.1851" xlink:type="simple">10.1038/nmeth.1851</ext-link></comment> <object-id pub-id-type="pmid">22231641</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ahrens</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Orger</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Robson</surname> <given-names>DN</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Keller</surname> <given-names>PJ</given-names></name>. <article-title>Whole-brain functional imaging at cellular resolution using light-sheet microscopy</article-title>. <source>Nat Methods</source>. <year>2013</year> <month>May</month>;<volume>10</volume>(<issue>5</issue>):<fpage>413</fpage>–<lpage>420</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.2434" xlink:type="simple">10.1038/nmeth.2434</ext-link></comment> <object-id pub-id-type="pmid">23524393</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Stevenson</surname> <given-names>IH</given-names></name>, <name name-style="western"><surname>Kording</surname> <given-names>KP</given-names></name>. <article-title>How advances in neural recording affect data analysis</article-title>. <source>Nat Neurosci</source>. <year>2011</year> <month>Feb</month>;<volume>14</volume>(<issue>2</issue>):<fpage>139</fpage>–<lpage>142</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2731" xlink:type="simple">10.1038/nn.2731</ext-link></comment> <object-id pub-id-type="pmid">21270781</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Alivisatos</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Chun</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Church</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Greenspan</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Roukes</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Yuste</surname> <given-names>R</given-names></name>. <article-title>The Brain Activity Map Project and the Challenge of Functional Connectomics</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>74</volume>(<issue>6</issue>):<fpage>970</fpage>–<lpage>974</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.06.006" xlink:type="simple">10.1016/j.neuron.2012.06.006</ext-link></comment> <object-id pub-id-type="pmid">22726828</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Venter</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Adams</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Sutton</surname> <given-names>GG</given-names></name>, <name name-style="western"><surname>Kerlavage</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>HO</given-names></name>, <name name-style="western"><surname>Hunkapiller</surname> <given-names>M</given-names></name>. <article-title>Shotgun sequencing of the human genome</article-title>. <source>Science</source>. <year>1998</year>;<volume>280</volume>:<fpage>1540</fpage>–<lpage>1542</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.280.5369.1540" xlink:type="simple">10.1126/science.280.5369.1540</ext-link></comment> <object-id pub-id-type="pmid">9644018</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Reddy</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Kelleher</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Fink</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Saggau</surname> <given-names>P</given-names></name>. <article-title>Three-dimensional random access multiphoton microscopy for functional imaging of neuronal activity</article-title>. <source>Nat Neurosci</source>. <year>2008</year>;<volume>11</volume>(<issue>6</issue>):<fpage>713</fpage>–<lpage>720</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2116" xlink:type="simple">10.1038/nn.2116</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Grewe</surname> <given-names>BF</given-names></name>, <name name-style="western"><surname>Langer</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kasper</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Kampa</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Helmchen</surname> <given-names>F</given-names></name>. <article-title>High-speed in vivo calcium imaging reveals neuronal network activity with near-millisecond precision</article-title>. <source>Nat Methods</source>. <year>2010</year> <month>May</month>;<volume>7</volume>(<issue>5</issue>):<fpage>399</fpage>–<lpage>405</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.1453" xlink:type="simple">10.1038/nmeth.1453</ext-link></comment> <object-id pub-id-type="pmid">20400966</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hochbaum</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Zhao</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Farhi</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Klapoetke</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Werley</surname> <given-names>Ca</given-names></name>, <name name-style="western"><surname>Kapoor</surname> <given-names>V</given-names></name>, <etal>et al</etal>. <article-title>All-optical electrophysiology in mammalian neurons using engineered microbial rhodopsins</article-title>. <source>Nat Methods</source>. <year>2014</year> <month>Jun</month>;<volume>11</volume>(<issue>8</issue>):<fpage>825</fpage>–<lpage>833</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.3000" xlink:type="simple">10.1038/nmeth.3000</ext-link></comment> <object-id pub-id-type="pmid">24952910</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mishchenko</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Vogelstein</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>A Bayesian approach for inferring neuronal connectivity from calcium fluorescent imaging data</article-title>. <source>Ann Appl Stat</source>. <year>2011</year>;<volume>5</volume>(<issue>2B</issue>):<fpage>1229</fpage>–<lpage>1261</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/09-AOAS303" xlink:type="simple">10.1214/09-AOAS303</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hoebe</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Van der Voort</surname> <given-names>HTM</given-names></name>, <name name-style="western"><surname>Stap</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Van Noorden</surname> <given-names>CJF</given-names></name>, <name name-style="western"><surname>Manders</surname> <given-names>EMM</given-names></name>. <article-title>Quantitative determination of the reduction of phototoxicity and photobleaching by controlled light exposure microscopy</article-title>. <source>J Microsc</source>. <year>2008</year> <month>Jul</month>;<volume>231</volume>(<issue>Pt 1</issue>):<fpage>9</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1365-2818.2008.02009.x" xlink:type="simple">10.1111/j.1365-2818.2008.02009.x</ext-link></comment> <object-id pub-id-type="pmid">18638185</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Park</surname> <given-names>IM</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>. <article-title>Bayesian Spike-Triggered Covariance Analysis</article-title>. In: <source>Neural Inf Process Syst</source>; <year>2011</year>. p. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sadeghi</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Gauthier</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Greschner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Agne</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>EJ</given-names></name>, <etal>et al</etal>. <article-title>Monte Carlo methods for localization of cones given multielectrode retinal ganglion cell recordings</article-title>. <source>Network</source>. <year>2013</year>;<volume>24</volume>(<issue>1</issue>):<fpage>27</fpage>–<lpage>51</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3109/0954898X.2012.740140" xlink:type="simple">10.3109/0954898X.2012.740140</ext-link></comment> <object-id pub-id-type="pmid">23194406</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ramirez</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Fast inference in generalized linear models via expected log-likelihoods</article-title>. <source>J Comput Neurosci</source>. <year>2014</year> <month>Apr</month>;<volume>36</volume>(<issue>2</issue>):<fpage>215</fpage>–<lpage>234</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10827-013-0466-4" xlink:type="simple">10.1007/s10827-013-0466-4</ext-link></comment> <object-id pub-id-type="pmid">23832289</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Diaconis</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Freedman</surname> <given-names>D</given-names></name>. <article-title>Asymptotics of graphical projection pursuit</article-title>. <source>Ann Stat</source>. <year>1984</year>;<volume>12</volume>(<issue>3</issue>):<fpage>793</fpage>–<lpage>815</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/aos/1176346703" xlink:type="simple">10.1214/aos/1176346703</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brillinger</surname> <given-names>D</given-names></name>. <article-title>Maximum likelihood analysis of spike trains of interacting nerve cells</article-title>. <source>Biol Cyberkinetics</source>. <year>1988</year>;<volume>59</volume>:<fpage>189</fpage>–<lpage>200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00318010" xlink:type="simple">10.1007/BF00318010</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rigat</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>de Gunst</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>van Pelt</surname> <given-names>J</given-names></name>. <article-title>Bayesian modelling and analysis of spatio-temporal neuronal networks</article-title>. <source>Bayesian Anal</source>. <year>2006</year>;<volume>1</volume>:<fpage>733</fpage>–<lpage>764</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/06-BA124" xlink:type="simple">10.1214/06-BA124</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>P</given-names></name>. <article-title>Neural characterization in partially observed populations of spiking neurons</article-title>. In: <source>Neural Inf Process Syst</source>; <year>2007</year>. p. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lütcke</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Gerhard</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Zenke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Helmchen</surname> <given-names>F</given-names></name>. <article-title>Inference of neuronal network spike dynamics and topology from calcium imaging data</article-title>. <source>Front Neural Circuits</source>. <year>2013</year>;<volume>7</volume>(<issue>Dec</issue>):<fpage>201</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncir.2013.00201" xlink:type="simple">10.3389/fncir.2013.00201</ext-link></comment> <object-id pub-id-type="pmid">24399936</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gerhard</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Kispersky</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Gutierrez</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Marder</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Kramer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Eden</surname> <given-names>U</given-names></name>. <article-title>Successful reconstruction of a physiological circuit with known connectivity from spiking activity alone</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year> <month>Jul</month>;<volume>9</volume>(<issue>7</issue>):<fpage>e1003138</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003138" xlink:type="simple">10.1371/journal.pcbi.1003138</ext-link></comment> <object-id pub-id-type="pmid">23874181</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Teh</surname> <given-names>YW</given-names></name>, <name name-style="western"><surname>Newman</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Welling</surname> <given-names>M</given-names></name>. <article-title>A collapsed variational Bayesian inference algorithm for latent Dirichlet allocation</article-title>. In: <source>Neural Inf Process Syst</source>; <year>2006</year>. p. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ribeiro</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Opper</surname> <given-names>M</given-names></name>. <article-title>Expectation propagation with factorizing distributions: a Gaussian approximation and performance results for simple models</article-title>. <source>Neural Comput</source>. <year>2011</year> <month>Apr</month>;<volume>23</volume>(<issue>4</issue>):<fpage>1047</fpage>–<lpage>1069</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00104" xlink:type="simple">10.1162/NECO_a_00104</ext-link></comment> <object-id pub-id-type="pmid">21222527</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Soudry</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hubara</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Meir</surname> <given-names>R</given-names></name>. <article-title>Expectation backpropagation: parameter-free training of multilayer neural networks with continuous or discrete weights</article-title>. In: <source>Neural Inf Process Syst. Montreal</source>; <year>2014</year>. p. <fpage>963</fpage>–<lpage>971</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wang</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Manning</surname> <given-names>CD</given-names></name>. <article-title>Fast dropout training</article-title>. <source>Int Conf Mach Learn</source>. <year>2013</year>;<volume>28</volume>:<fpage>118</fpage>–<lpage>126</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cotton</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Froudarakis</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Storer</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Saggau</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>AS</given-names></name>. <article-title>Three-dimensional mapping of microcircuit correlation structure</article-title>. <source>Front Neural Circuits</source>. <year>2013</year>;<volume>7</volume>:<fpage>151</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncir.2013.00151" xlink:type="simple">10.3389/fncir.2013.00151</ext-link></comment> <object-id pub-id-type="pmid">24133414</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nikolenko</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Watson</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Araya</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Woodruff</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Peterka</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Yuste</surname> <given-names>R</given-names></name>. <article-title>SLM microscopy: scanless two-photon imaging and photostimulation using spatial light modulators</article-title>. <source>Front Neural Circuits</source>. <year>2008</year>;<volume>2</volume>:<fpage>5</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/neuro.04.005.2008" xlink:type="simple">10.3389/neuro.04.005.2008</ext-link></comment> <object-id pub-id-type="pmid">19129923</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bouchard</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Voleti</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Mendes</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Lacefield</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Grueber</surname> <given-names>WB</given-names></name>, <name name-style="western"><surname>Mann</surname> <given-names>RS</given-names></name>, <etal>et al</etal>. <article-title>Swept confocally-aligned planar excitation (SCAPE) microscopy for high-speed volumetric imaging of behaving organisms</article-title>. <source>Nat Photonics</source>. <year>2015</year> <month>Jan</month>;<volume>9</volume>(<issue>2</issue>):<fpage>113</fpage>–<lpage>119</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nphoton.2014.323" xlink:type="simple">10.1038/nphoton.2014.323</ext-link></comment> <object-id pub-id-type="pmid">25663846</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lecoq</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Savall</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Vučinić</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Grewe</surname> <given-names>BF</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>JZ</given-names></name>, <etal>et al</etal>. <article-title>Visualizing mammalian brain area interactions by dual-axis two-photon calcium imaging</article-title>. <source>Nat Neurosci</source>. <year>2014</year> <month>Nov</month>;<volume>17</volume>(<issue>12</issue>):<fpage>1825</fpage>–<lpage>1829</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3867" xlink:type="simple">10.1038/nn.3867</ext-link></comment> <object-id pub-id-type="pmid">25402858</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="other">Mishchenko Y. Consistency of the complete neuronal population connectivity reconstructions using shotgun imaging; 2015. In prep.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>McLachlan</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Krishnan</surname> <given-names>T</given-names></name>. <source>The EM algorithm and extensions</source>. <publisher-name>Wiley-Interscience</publisher-name>; <year>2007</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="other">Zaytsev YV, Morrison A, Deger M. Reconstruction of recurrent synaptic connectivity of thousands of neurons from simulated spiking activity; 2015. <comment><ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://arxiv.org/abs/1502.04993">http://arxiv.org/abs/1502.04993</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Denk</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Briggman</surname> <given-names>KL</given-names></name>, <name name-style="western"><surname>Helmstaedter</surname> <given-names>M</given-names></name>. <article-title>Structural neurobiology: missing link to a mechanistic understanding of neural computation</article-title>. <source>Nat Rev Neurosci</source>. <year>2012</year> <month>May</month>;<volume>13</volume>(<issue>5</issue>):<fpage>351</fpage>–<lpage>358</lpage>. <object-id pub-id-type="pmid">22353782</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Takemura</surname> <given-names>Sy</given-names></name>, <name name-style="western"><surname>Bharioke</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Nern</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vitaladevuni</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rivlin</surname> <given-names>PK</given-names></name>, <etal>et al</etal>. <article-title>A visual motion detection circuit suggested by Drosophila connectomics</article-title>. <source>Nature</source>. <year>2013</year> <month>Aug</month>;<volume>500</volume>(<issue>7461</issue>):<fpage>175</fpage>–<lpage>181</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature12450" xlink:type="simple">10.1038/nature12450</ext-link></comment> <object-id pub-id-type="pmid">23925240</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Helmstaedter</surname> <given-names>M</given-names></name>. <article-title>Cellular-resolution connectomics: challenges of dense neural circuit reconstruction</article-title>. <source>Nat Methods</source>. <year>2013</year> <month>Jun</month>;<volume>10</volume>(<issue>6</issue>):<fpage>501</fpage>–<lpage>507</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.2476" xlink:type="simple">10.1038/nmeth.2476</ext-link></comment> <object-id pub-id-type="pmid">23722209</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Minerbi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kahana</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Goldfeld</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Kaufman</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Marom</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ziv</surname> <given-names>NE</given-names></name>. <article-title>Long-term relationships between synaptic tenacity, synaptic remodeling, and network activity</article-title>. <source>PLoS Biol</source>. <year>2009</year>;<volume>7</volume>(<issue>6</issue>):<fpage>e1000136</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.1000136" xlink:type="simple">10.1371/journal.pbio.1000136</ext-link></comment> <object-id pub-id-type="pmid">19554080</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nykamp</surname> <given-names>DQ</given-names></name>. <article-title>Reconstructing stimulus-driven neural networks from spike times</article-title>. In: <source>Neural Inf Process Syst</source>. vol. <volume>15</volume>; <year>2003</year>. p. <fpage>309</fpage>–<lpage>316</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Maximum likelihood estimation of cascade point-process neural encoding models</article-title>. <source>Netw Comput Neural Syst</source>. <year>2004</year> <month>Nov</month>;<volume>15</volume>(<issue>4</issue>):<fpage>243</fpage>–<lpage>262</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0954-898X/15/4/002" xlink:type="simple">10.1088/0954-898X/15/4/002</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Memmesheimer</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Olveczky</surname> <given-names>BP</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Learning precisely timed spikes</article-title>. <source>Neuron</source>. <year>2014</year> <month>May</month>;<volume>82</volume>(<issue>4</issue>):<fpage>925</fpage>–<lpage>938</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2014.03.026" xlink:type="simple">10.1016/j.neuron.2014.03.026</ext-link></comment> <object-id pub-id-type="pmid">24768299</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Stetter</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Battaglia</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Soriano</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Geisel</surname> <given-names>T</given-names></name>. <article-title>Model-free reconstruction of excitatory neuronal connectivity from calcium imaging signals</article-title>. <source>PLoS Comput Biol</source>. <year>2012</year> <month>Jan</month>;<volume>8</volume>(<issue>8</issue>):<fpage>e1002653</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002653" xlink:type="simple">10.1371/journal.pcbi.1002653</ext-link></comment> <object-id pub-id-type="pmid">22927808</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fletcher</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Rangan</surname> <given-names>S</given-names></name>. <article-title>Scalable inference for neuronal connectivity from calcium imaging</article-title>. In: <source>Neural Inf Process Syst</source>; <year>2014</year>. p. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mohler</surname> <given-names>G</given-names></name>. <article-title>Learning convolution filters for inverse covariance estimation of neural network connectivity</article-title>. In: <source>Neural Inf Process Syst</source>; <year>2014</year>. p. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kispersky</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Gutierrez</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Marder</surname> <given-names>E</given-names></name>. <article-title>Functional connectivity in a rhythmic inhibitory circuit using Granger causality</article-title>. <source>Neural Syst Circuits</source>. <year>2011</year>;<volume>1</volume>:<fpage>9</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/2042-1001-1-9" xlink:type="simple">10.1186/2042-1001-1-9</ext-link></comment> <object-id pub-id-type="pmid">22330428</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Volgushev</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ilin</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Stevenson</surname> <given-names>IH</given-names></name>. <article-title>Identifying and tracking simulated synaptic inputs from neuronal firing: insights from in vitro experiments</article-title>. <source>PLOS Comput Biol</source>. <year>2015</year>;<volume>11</volume>(<issue>3</issue>):<fpage>e1004167</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1004167" xlink:type="simple">10.1371/journal.pcbi.1004167</ext-link></comment> <object-id pub-id-type="pmid">25823000</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Latimer</surname> <given-names>KW</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>. <article-title>Inferring synaptic conductances from spike trains with a biophysically inspired point process model</article-title>. In: <source>Neural Inf Process Syst</source>; <year>2014</year>. p. <fpage>954</fpage>–<lpage>962</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Turaga</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Buesing</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Packer</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Dalgleish</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Pettit</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hausser</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Inferring neural population dynamics from multiple partial recordings of the same neural circuit</article-title>. In: <source>Neural Inf Process Syst</source>; <year>2013</year>. p. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nykamp</surname> <given-names>DQ</given-names></name>. <article-title>Pinpointing connectivity despite hidden nodes within stimulus-driven networks</article-title>. <source>Phys Rev E</source>. <year>2008</year> <month>Aug</month>;<volume>78</volume>(<issue>2</issue>):<fpage>021902</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevE.78.021902" xlink:type="simple">10.1103/PhysRevE.78.021902</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Vidne</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ahmadian</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Shlens</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Kulkarni</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Litke</surname> <given-names>AM</given-names></name>, <etal>et al</etal>. <article-title>Modeling the impact of common noise inputs on the network activity of retinal ganglion cells</article-title>. <source>J Comput Neurosci</source>. <year>2012</year>;<volume>33</volume>(<issue>1</issue>):<fpage>97</fpage>–<lpage>121</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10827-011-0376-2" xlink:type="simple">10.1007/s10827-011-0376-2</ext-link></comment> <object-id pub-id-type="pmid">22203465</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tyrcha</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hertz</surname> <given-names>J</given-names></name>. <article-title>Network inference with hidden units</article-title>. <source>Math Biosci Eng</source>. <year>2014</year> <month>Feb</month>;<volume>11</volume>(<issue>1</issue>):<fpage>149</fpage>–<lpage>165</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3934/mbe.2014.11.149" xlink:type="simple">10.3934/mbe.2014.11.149</ext-link></comment> <object-id pub-id-type="pmid">24245678</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Aramuni</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Griesbeck</surname> <given-names>O</given-names></name>. <article-title>Chronic calcium imaging in neuronal development and disease</article-title>. <source>Exp Neurol</source>. <year>2013</year> <month>Apr</month>;<volume>242</volume>:<fpage>50</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.expneurol.2012.02.008" xlink:type="simple">10.1016/j.expneurol.2012.02.008</ext-link></comment> <object-id pub-id-type="pmid">22374357</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bishop</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Byron</surname> <given-names>M</given-names></name>. <article-title>Deterministic Symmetric Positive Semidefinite Matrix Completion</article-title>. In: <source>Neural Inf Process Syst</source>; <year>2014</year>. p. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pakman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Huggins</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Fast penalized state-space methods for inferring dendritic synaptic connectivity</article-title>. <source>J Comput Neurosci</source>. <year>2014</year> <month>Sep</month>;<volume>36</volume>(<issue>3</issue>):<fpage>415</fpage>–<lpage>443</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10827-013-0478-0" xlink:type="simple">10.1007/s10827-013-0478-0</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref051">
<label>51</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mishchenko</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Efficient methods for sampling spike trains in networks of coupled neurons</article-title>. <source>Ann Appl Stat</source>. <year>2011</year>;<volume>5</volume>(<issue>3</issue>):<fpage>1893</fpage>–<lpage>1919</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/11-AOAS467" xlink:type="simple">10.1214/11-AOAS467</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref052">
<label>52</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Song</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sjöström</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Reigl</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>. <article-title>Highly nonrandom features of synaptic connectivity in local cortical circuits</article-title>. <source>PLoS Biol</source>. <year>2005</year> <month>Mar</month>;<volume>3</volume>(<issue>3</issue>):<fpage>507</fpage>–<lpage>519</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0030068" xlink:type="simple">10.1371/journal.pbio.0030068</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref053">
<label>53</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lichtman</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Livet</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sanes</surname> <given-names>JR</given-names></name>. <article-title>A technicolour approach to the connectome</article-title>. <source>Nat Rev Neurosci</source>. <year>2008</year> <month>Jun</month>;<volume>9</volume>(<issue>6</issue>):<fpage>417</fpage>–<lpage>422</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2391" xlink:type="simple">10.1038/nrn2391</ext-link></comment> <object-id pub-id-type="pmid">18446160</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref054">
<label>54</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chung</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Wallace</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kalyanasundaram</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Andalman</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Davidson</surname> <given-names>TJ</given-names></name>, <etal>et al</etal>. <article-title>Structural and molecular interrogation of intact biological systems</article-title>. <source>Nature</source>. <year>2013</year> <month>Apr</month>;<volume>497</volume>:<fpage>332</fpage>–<lpage>337</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature12107" xlink:type="simple">10.1038/nature12107</ext-link></comment> <object-id pub-id-type="pmid">23575631</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref055">
<label>55</label>
<mixed-citation xlink:type="simple" publication-type="other">Jonas E, Kording K. Automatic discovery of cell types and microcircuitry from neural connectomics; 2014. <comment><ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://arxiv.org/abs/1407.4137">http://arxiv.org/abs/1407.4137</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref056">
<label>56</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Shababo</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Brooks</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pakman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Bayesian inference and online experimental design for mapping neural microcircuits</article-title>. In: <source>Neural Inf Process Syst</source>; <year>2013</year>. p. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004464.ref057">
<label>57</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Berndt</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Yizhar</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Gunaydin</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Hegemann</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Deisseroth</surname> <given-names>K</given-names></name>. <article-title>Bi-stable neural state switches</article-title>. <source>Nat Neurosci</source>. <year>2009</year> <month>Feb</month>;<volume>12</volume>(<issue>2</issue>):<fpage>229</fpage>–<lpage>234</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2247" xlink:type="simple">10.1038/nn.2247</ext-link></comment> <object-id pub-id-type="pmid">19079251</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref058">
<label>58</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rickgauer</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Deisseroth</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Tank</surname> <given-names>DW</given-names></name>. <article-title>Simultaneous cellular-resolution optical perturbation and imaging of place cell firing fields</article-title>. <source>Nat Neurosci</source>. <year>2014</year> <month>Nov</month>;<volume>17</volume>(<issue>12</issue>):<fpage>1816</fpage>–<lpage>1824</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3866" xlink:type="simple">10.1038/nn.3866</ext-link></comment> <object-id pub-id-type="pmid">25402854</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref059">
<label>59</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Packer</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Russell</surname> <given-names>LE</given-names></name>, <name name-style="western"><surname>Dalgleish</surname> <given-names>HWP</given-names></name>, <name name-style="western"><surname>Häusser</surname> <given-names>M</given-names></name>. <article-title>Simultaneous all-optical manipulation and recording of neural circuit activity with cellular resolution in vivo</article-title>. <source>Nat Methods</source>. <year>2014</year>;<volume>12</volume>(<issue>2</issue>):<fpage>140</fpage>–<lpage>146</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.3217" xlink:type="simple">10.1038/nmeth.3217</ext-link></comment> <object-id pub-id-type="pmid">25532138</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004464.ref060">
<label>60</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tripathy</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Savitskaya</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Burton</surname> <given-names>SD</given-names></name>, <name name-style="western"><surname>Urban</surname> <given-names>NN</given-names></name>, <name name-style="western"><surname>Gerkin</surname> <given-names>RC</given-names></name>. <article-title>NeuroElectro: a window to the world’s neuron electrophysiology data</article-title>. <source>Front Neuroinform</source>. <year>2014</year> <month>Jan</month>;<volume>8</volume>:<fpage>40</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fninf.2014.00040" xlink:type="simple">10.3389/fninf.2014.00040</ext-link></comment> <object-id pub-id-type="pmid">24808858</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>