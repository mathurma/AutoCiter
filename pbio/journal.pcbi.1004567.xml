<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-00974</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004567</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A Unifying Probabilistic View of Associative Learning</article-title>
<alt-title alt-title-type="running-head">A Unifying Probabilistic View of Associative Learning</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Gershman</surname> <given-names>Samuel J.</given-names></name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>Department of Psychology and Center for Brain Science, Harvard University, Cambridge, Massachusetts, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Diedrichsen</surname> <given-names>Jörn</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University College London, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: SJG. Performed the experiments: SJG. Analyzed the data: SJG. Contributed reagents/materials/analysis tools: SJG. Wrote the paper: SJG.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">gershman@fas.harvard.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>11</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>4</day>
<month>11</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>11</issue>
<elocation-id>e1004567</elocation-id>
<history>
<date date-type="received">
<day>16</day>
<month>6</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>22</day>
<month>9</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Samuel J. Gershman</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004567" xlink:type="simple"/>
<abstract>
<p>Two important ideas about associative learning have emerged in recent decades: (1) Animals are Bayesian learners, tracking their uncertainty about associations; and (2) animals acquire long-term reward predictions through reinforcement learning. Both of these ideas are normative, in the sense that they are derived from rational design principles. They are also descriptive, capturing a wide range of empirical phenomena that troubled earlier theories. This article describes a unifying framework encompassing Bayesian and reinforcement learning theories of associative learning. Each perspective captures a different aspect of associative learning, and their synthesis offers insight into phenomena that neither perspective can explain on its own.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>How do we learn about associations between events? The seminal Rescorla-Wagner model provided a simple yet powerful foundation for understanding associative learning. However, much subsequent research has uncovered fundamental limitations of the Rescorla-Wagner model. One response to these limitations has been to rethink associative learning from a normative statistical perspective: How would an ideal agent learn about associations? First, an agent should track its uncertainty using Bayesian principles. Second, an agent should learn about long-term (not just immediate) reward, using reinforcement learning principles. This article brings together these principles into a single framework and shows how they synergistically account for a number of complex learning phenomena.</p>
</abstract>
<funding-group>
<funding-statement>This research was supported by startup funds from Harvard University. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="0"/>
<page-count count="20"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All code for reproducing the simulations are available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/sjgershm/KTD" xlink:type="simple">https://github.com/sjgershm/KTD</ext-link></meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Learning to predict rewards (or punishments) from the occurrence of other stimuli is fundamental to the survival of animals. When such learning occurs, it is commonly assumed that a stimulus-reward association is stored in memory [<xref ref-type="bibr" rid="pcbi.1004567.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref002">2</xref>]. Two ideas have, over the last few decades, altered our understanding of how such associations are formed, and the nature of their content. First, Bayesian theories of learning have suggested that animals estimate not only the strength of associations, but also their uncertainty in these estimates [<xref ref-type="bibr" rid="pcbi.1004567.ref003">3</xref>–<xref ref-type="bibr" rid="pcbi.1004567.ref008">8</xref>]. Second, reinforcement learning (RL) theories have suggested that animals estimate long-term cumulative future reward [<xref ref-type="bibr" rid="pcbi.1004567.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1004567.ref011">11</xref>].</p>
<p>Both Bayesian and RL theories can be viewed as generalizations of the seminal Rescorla-Wagner model [<xref ref-type="bibr" rid="pcbi.1004567.ref012">12</xref>] that address some of its limitations. The mathematical derivations of these generalizations and their empirical support will be reviewed in the following sections. Bayesian and RL theories are derived from different—but not mutually exclusive—assumptions about the nature of the learning task. The goal of this paper is to unify these perspectives and explore the implications of this unification.</p>
<p>One set of assumptions about the learning task concerns the target of learning. The Bayesian generalization of the Rescorla-Wagner model, embodied in the Kalman filter [<xref ref-type="bibr" rid="pcbi.1004567.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref006">6</xref>], assumes that this is the problem of predicting immediate reward, whereas RL theories, such as temporal difference (TD) learning, assume that the goal of learning is to predict the cumulative future reward. A second set of assumptions concerns the representation of uncertainty. The Kalman filter learns a Bayesian estimator (the posterior distribution) of expected immediate reward, whereas TD learns a point estimator (a single value rather than a distribution) of expected future reward. As shown below, the Rescorla-Wagner model can be construed as a point estimator of expected immediate reward.</p>
<p>After reviewing these different modeling assumptions (organized in <xref ref-type="fig" rid="pcbi.1004567.g001">Fig 1</xref>), I show how they can be naturally brought together in the form of the Kalman TD model. This model has been previously studied in the RL literature [<xref ref-type="bibr" rid="pcbi.1004567.ref013">13</xref>], but has received relatively little attention in neuroscience or psychology (see [<xref ref-type="bibr" rid="pcbi.1004567.ref014">14</xref>] for an exception). I explain how this model combines the strengths of Bayesian and TD models. I will demonstrate this point using several experimental examples that neither model can account for on its own.</p>
<fig id="pcbi.1004567.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004567.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Organizing Bayesian and reinforcement learning theories.</title>
<p>Point estimation algorithms learn the expected reward or value, while Bayesian algorithms learn a posterior distribution over reward or value. The columns show <italic>what</italic> is learned, and the rows show <italic>how</italic> it is learned.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004567.g001"/>
</fig>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Preliminaries</title>
<p>Let <bold>x</bold><sub><italic>n</italic></sub> denote the vector of conditioned stimulus (CS) intensities on trial <italic>n</italic> (all vectors are taken to be column vectors), <bold>w</bold><sub><italic>n</italic></sub> denote the associative strengths (or weights), and <italic>r</italic><sub><italic>n</italic></sub> denote the unconditioned stimulus (US; i.e., observed reward). Note that traditional associative learning theories interpret <italic>r</italic><sub><italic>n</italic></sub> as the asymptotic level of responding supported by the US on the current trial; however, in this article I interpret <italic>r</italic><sub><italic>n</italic></sub> as reward in order to facilitate the connection to RL.</p>
<p>To compactly describe experimental paradigms, I use uppercase letters (A, B, etc.) to denote conditioned stimuli, and combinations of letters (e.g., AB) to denote stimulus compounds. A stimulus (or compound) terminating in reward is denoted by A→+. Similarly, a stimulus terminating in no reward is denoted by A→-. A stimulus terminating with the onset of another stimulus is denoted A→B. The notation A→? indicates that conditioned responding to A is the dependent measure in a particular experiment. When multiple trial types are interleaved within a phase, forward slashes are used (e.g., A→+ / B→-), and contiguous phases are separated by semi-colons (e.g., A→+; B→-).</p>
<p>Making predictions about empirical phenomena is complicated by the fact that experimental paradigms use diverse stimuli, rewards, and behavioral measures. The simulations reported below are predicated on the assumption that we can abstract away from some of these experimental details and predict response rates simply on the basis of reward expectation, as acquired by trial-and-error learning. This assumption is certainly false: response rates depend on other factors, such as motivation and stimulus-specific properties (e.g., [<xref ref-type="bibr" rid="pcbi.1004567.ref015">15</xref>]). Nonetheless, this assumption enables the models considered below to make predictions about a wide range of experimental paradigms without getting bogged down in experimental minutiae. The same is true for many other computational models, and is helpful for making progress before more realistic theoretical assumptions can be refined.</p>
</sec>
<sec id="sec004">
<title>The Rescorla-Wagner model</title>
<p>The Rescorla-Wagner model is the cornerstone of modern associative learning theory. While it has a number of crucial shortcomings [<xref ref-type="bibr" rid="pcbi.1004567.ref016">16</xref>], the model stimulated decades of experimental research and served as the basis of more sophisticated models [<xref ref-type="bibr" rid="pcbi.1004567.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1004567.ref019">19</xref>]. Learning is governed by the following equation:
<disp-formula id="pcbi.1004567.e001"><alternatives><graphic id="pcbi.1004567.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e001"/><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula> <disp-formula id="pcbi.1004567.e002"><alternatives><graphic id="pcbi.1004567.e002g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e002"/><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>n</mml:mi> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>α</italic> ∈ [0, 1] is a learning rate parameter (also known as <italic>associability</italic>), <italic>δ</italic><sub><italic>n</italic></sub> = <italic>r</italic><sub><italic>n</italic></sub> − <italic>v</italic><sub><italic>n</italic></sub> is the <italic>prediction error</italic>, and <italic>v</italic><sub><italic>n</italic></sub> is the reward expectation, which is taken to be monotonically related to the conditioned response.</p>
<p>In the next section, I describe a probabilistic interpretation of this learning rule, which will play an important role in subsequent developments. I then discuss some empirical implications of the model.</p>
<sec id="sec005">
<title>Probabilistic interpretation</title>
<p>To derive a probabilistic interpretation, we need to impute to the animal a set of probabilistic assumptions about how its sensory data are generated—the animal’s internal model. Specifically, the internal model is defined by a <italic>prior</italic> on weights, <italic>p</italic>(<bold>w</bold><sub>0</sub>), a <italic>change process</italic> on the weights, <italic>p</italic>(<bold>w</bold><sub><italic>n</italic></sub>∣<bold>w</bold><sub><italic>n</italic>−1</sub>), and a reward distribution given stimuli and weights, <italic>p</italic>(<italic>r</italic><sub><italic>n</italic></sub>∣<bold>w</bold><sub><italic>n</italic></sub>, <bold>x</bold><sub><italic>n</italic></sub>). Following earlier work [<xref ref-type="bibr" rid="pcbi.1004567.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref006">6</xref>], I take this to be a linear-Gaussian dynamical system (LDS):
<disp-formula id="pcbi.1004567.e003"><alternatives><graphic id="pcbi.1004567.e003g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e003"/><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mi mathvariant="bold">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula> <disp-formula id="pcbi.1004567.e004"><alternatives><graphic id="pcbi.1004567.e004g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e004"/><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msup><mml:mi>τ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi mathvariant="bold">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula> <disp-formula id="pcbi.1004567.e005"><alternatives><graphic id="pcbi.1004567.e005g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e005"/><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
where <bold>I</bold> is the identity matrix. Intuitively, the LDS makes the following claims about the animal’s internal model. First, the prior on weights posits that weights tend to be close to 0 (i.e., associations tend to be weak); the strength of this prior is inversely proportional to <inline-formula id="pcbi.1004567.e006"><alternatives><graphic id="pcbi.1004567.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e006"/><mml:math id="M6" display="inline" overflow="scroll"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>. Second, the change process posits that weights tend to change slowly and independently over time; the volatility of this change process increases with <italic>τ</italic><sup>2</sup>. Third, the reward distribution posits that reward is a noisy linear combination of stimulus activations.</p>
<p>From the animal’s perspective, the goal of learning is to recover an estimate of the weights. The generative process serves as a set of soft constraints on the weight estimator. In other words, the generative process provides an inductive bias that makes some estimators better than others. In order to precisely define what makes an estimator “better,” we need to specify an objective function that is maximized by the optimal estimator. Let us first make the simplifying assumption that the weights do not change over time (i.e., <italic>τ</italic><sup>2</sup> = 0), in which case the weights are static parameters and we can drop the trial index. Under this assumption, it can be shown that the objective function maximized (asymptotically as <italic>t</italic> → ∞) by the Rescorla-Wagner model is the log-likelihood log <italic>p</italic>(<italic>r</italic><sub>1:<italic>n</italic></sub>∣<bold>w</bold>, <bold>x</bold><sub>1:<italic>t</italic></sub>), where the index 1:<italic>n</italic> denotes all trials from 1 to <italic>n</italic>.</p>
<p>To show this, I draw a connection between the Rescorla-Wagner model and the Robbins-Monro algorithm for stochastic approximation [<xref ref-type="bibr" rid="pcbi.1004567.ref020">20</xref>]. In the context of the LDS described above, the Robbins-Monro algorithm updates the weight estimate <inline-formula id="pcbi.1004567.e007"><alternatives><graphic id="pcbi.1004567.e007g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e007"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> according to:
<disp-formula id="pcbi.1004567.e008"><alternatives><graphic id="pcbi.1004567.e008g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e008"/><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:msup><mml:mi>σ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
where <italic>α</italic><sub><italic>n</italic></sub> is a dynamically decreasing learning rate satisfying
<disp-formula id="pcbi.1004567.e009"><alternatives><graphic id="pcbi.1004567.e009g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e009"/><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msub><mml:mi>α</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>∞</mml:mi> <mml:mo>,</mml:mo> <mml:mspace width="1.em"/><mml:mspace width="1.em"/><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msubsup><mml:mi>α</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>&lt;</mml:mo> <mml:mi>∞</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula>
One simple choice of learning rate that satisfies these conditions is <italic>α</italic><sub><italic>n</italic></sub> = 1/<italic>n</italic>. The Robbins-Monro algorithm converges asymptotically to the maximum likelihood estimate of <bold>w</bold>. Comparing Eqs <xref ref-type="disp-formula" rid="pcbi.1004567.e001">1</xref> and <xref ref-type="disp-formula" rid="pcbi.1004567.e008">6</xref> (and allowing <italic>σ</italic><sup>−2</sup> to be absorbed into the learning rate), it can be seen that the Rescorla-Wagner model with a dynamically decreasing learning rate is a maximum likelihood estimator (see also [<xref ref-type="bibr" rid="pcbi.1004567.ref021">21</xref>]). This analysis echoes the observation that the Rescorla-Wagner model is an instantiation of the “least mean squares” (aka Widrow-Hoff) learning rule [<xref ref-type="bibr" rid="pcbi.1004567.ref022">22</xref>]: under a Gaussian observation model, minimizing summed squared error is equivalent to maximizing likelihood. The main difference is that the least mean squares rule assumes a static learning rate, and imposes restrictions on the learning rate to ensure convergence.</p>
<p>While the Rescorla-Wagner model thus has a normative basis in statistical estimation, it is not a fully probabilistic estimator—it only maintains a single “point” hypothesis about the weights. As a consequence, the estimator ignores uncertainty about the weights. There is good evidence that the brain maintains representations of uncertainty [<xref ref-type="bibr" rid="pcbi.1004567.ref023">23</xref>], and updates these representations using Bayesian inference [<xref ref-type="bibr" rid="pcbi.1004567.ref024">24</xref>]. Below I discuss a Bayesian generalization of the Rescorla-Wagner model, following a brief consideration of the empirical phenomena that motivate this generalization.</p>
</sec>
<sec id="sec006">
<title>Empirical implications</title>
<p>The Rescorla-Wagner model formalizes two important principles: (1) learning is driven by reward prediction errors; and (2) simultaneously presented stimuli summate to predict reward. These principles will figure prominently in the subsequent discussion of the model’s limitations and possible remedies.</p>
<p>To see that learning is driven solely by reward prediction errors, notice that <bold>w</bold><sub><italic>n</italic></sub> is updated only when the prediction error is non-zero. One surprising consequence of this property is that associative strength can in some cases <italic>weaken</italic> as a consequence of reinforcement. For example, Rescorla [<xref ref-type="bibr" rid="pcbi.1004567.ref025">25</xref>] demonstrated that reinforcing a compound consisting of two previously reinforced stimuli caused a decrement in responding to the individual stimuli on a subsequent test. This effect is referred to as <italic>overexpectation</italic> because summing the associative strength of two individually reinforced stimuli should produce a larger reward prediction than either stimulus alone. Because the reinforcer magnitude is the same, the prediction error will be negative, and thus the associative strength for both stimuli will be decremented. This demonstrates that learning is driven not by reinforcement <italic>per se</italic>, but by <italic>unexpected</italic> reinforcement.</p>
<p>The same principles can give rise to negative (inhibitory) associative strength. In the <italic>conditioned inhibition</italic> paradigm [<xref ref-type="bibr" rid="pcbi.1004567.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref027">27</xref>], A→+ trials are interspersed with AB→- trials, resulting in negative associative strength accruing to stimulus B (as assessed, for example, by showing that pairing B with a previously reinforced stimulus C reduces responding relative to C alone). According to the Rescorla-Wagner model, the negative association is acquired because of the negative prediction error on AB→- trials; B must have a negative weight in order to counterbalance the excitatory weight of A.</p>
<p>The combination of error-driven learning with associative summation leads to stimulus competition. For example, in <italic>forward (Kamin) blocking</italic>[<xref ref-type="bibr" rid="pcbi.1004567.ref028">28</xref>], stimulus A is paired with reward and then in a second phase the compound AB is paired with reward. In a subsequent test of B alone, responding is lower compared to a condition in which the first phase is omitted. In terms of the Rescorla-Wagner model, stimulus A <italic>blocks</italic> acquisition of an association between B and reward because the reward is fully predicted by A and hence there is no prediction error to drive learning in the second phase. A similar argument accounts for the phenomenon of <italic>overshadowing</italic>[<xref ref-type="bibr" rid="pcbi.1004567.ref026">26</xref>], in which reinforcing the compound AB results in weaker responding to the individual stimulus elements compared to a condition in which each stimulus is reinforced separately.</p>
<p>Although considerable evidence supports the existence of error-driven learning and stimulus competition in associative learning, violations of these principles are well-documented [<xref ref-type="bibr" rid="pcbi.1004567.ref016">16</xref>]. For example, presenting a stimulus alone prior to pairing it with reward retards acquisition of the stimulus-reward association, a phenomena known as the <italic>CS pre-exposure effect</italic> or <italic>latent inhibition</italic>[<xref ref-type="bibr" rid="pcbi.1004567.ref029">29</xref>]. Because the associative strength is presumably initialized to 0, the prediction error is 0 during pre-exposure and hence no associative learning should occur according to the Rescorla-Wagner model. Another example of learning in the absence of prediction errors is second-order conditioning [<xref ref-type="bibr" rid="pcbi.1004567.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref030">30</xref>]: The serial compound A→B results in conditioning of A if B was previously paired with reward. Here again there is no prediction error during the A→B and hence no learning should have occurred (a more fundamental problem here, which I discuss further below, is that the Rescorla-Wagner model only makes trial-level predictions and hence is actually inapplicable to serial-compound conditioning).</p>
<p>The Rescorla-Wagner model also runs into trouble in situations where absent stimuli appear to compete with present stimuli. For example, in backward blocking [<xref ref-type="bibr" rid="pcbi.1004567.ref031">31</xref>–<xref ref-type="bibr" rid="pcbi.1004567.ref033">33</xref>], a compound AB is reinforced and then A is reinforced by itself, resulting in a reduction of responding to B alone. Conversely, stimulus competition can be reduced by post-training extinction of one element [<xref ref-type="bibr" rid="pcbi.1004567.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1004567.ref036">36</xref>].</p>
<p>These findings undercut some of the basic claims of the Rescorla-Wagner model, and have stimulated extensive work in animal learning theory [<xref ref-type="bibr" rid="pcbi.1004567.ref002">2</xref>]. The next two sections will focus on two normatively-motivated generalizations of the Rescorla-Wagner model that can accommodate these (and many other) findings, before proceeding to a unifying view of these generalizations.</p>
</sec>
</sec>
<sec id="sec007">
<title>Bayesian inference and the Kalman filter</title>
<p>The probabilistic interpretation of the Rescorla-Wagner model given above shows that it is a maximum likelihood estimator of the weight vector. This estimator neglects the learner’s uncertainty by only representing the single most likely weight vector. Given that humans and other animals are able to report their uncertainty, and that these reports are often well-calibrated with veridical confidence (i.e., the probability of being correct; see [<xref ref-type="bibr" rid="pcbi.1004567.ref037">37</xref>]), it appears necessary to consider models that explicitly represent uncertainty. Moreover, such models are an important step towards understanding how the brain represents uncertainty [<xref ref-type="bibr" rid="pcbi.1004567.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref024">24</xref>].</p>
<p>Bayesian models of learning posit that the learner represents uncertainty in the form of a posterior distribution over hypotheses given data. In the case of associative learning, the posterior distribution is stipulated by Bayes’ rule as follows:
<disp-formula id="pcbi.1004567.e010"><alternatives><graphic id="pcbi.1004567.e010g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e010"/><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>:</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∝</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>:</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula>
Under the LDS specified in Eqs <xref ref-type="disp-formula" rid="pcbi.1004567.e003">3</xref>–<xref ref-type="disp-formula" rid="pcbi.1004567.e005">5</xref>, the posterior is Gaussian with mean <inline-formula id="pcbi.1004567.e011"><alternatives><graphic id="pcbi.1004567.e011g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e011"/><mml:math id="M11" display="inline" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub></mml:math></alternatives></inline-formula> and covariance matrix Σ<sub><italic>n</italic></sub>, updated using the Kalman filter equations:
<disp-formula id="pcbi.1004567.e012"><alternatives><graphic id="pcbi.1004567.e012g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e012"/><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">k</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula> <disp-formula id="pcbi.1004567.e013"><alternatives><graphic id="pcbi.1004567.e013g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e013"/><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mo>Σ</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>τ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi mathvariant="bold">I</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold">k</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:msubsup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>n</mml:mi> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>τ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi mathvariant="bold">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula>
where <inline-formula id="pcbi.1004567.e014"><alternatives><graphic id="pcbi.1004567.e014g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e014"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004567.e015"><alternatives><graphic id="pcbi.1004567.e015g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e015"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>Σ</mml:mo> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, and <bold>k</bold><sub><italic>n</italic></sub> is the <italic>Kalman gain</italic>:
<disp-formula id="pcbi.1004567.e016"><alternatives><graphic id="pcbi.1004567.e016g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e016"/><mml:math id="M16" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">k</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>τ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi mathvariant="bold">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>n</mml:mi> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>τ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi mathvariant="bold">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
Here the Kalman gain has replaced the learning rate <italic>α</italic> in the Rescorla-Wagner model. Importantly, the Kalman gain is stimulus-specific, dynamic and grows monotonically with the uncertainty encoded in the diagonals of the posterior covariance matrix Σ<sub><italic>n</italic></sub>. This allows the Kalman filter model to explain some of the phenomena that are problematic for the Rescorla-Wagner model.</p>
<p>Two factors govern the covariance matrix update. First, uncertainty grows over time due to the random diffusion of the weights (<xref ref-type="disp-formula" rid="pcbi.1004567.e004">Eq 4</xref>); this is expressed by the <italic>τ</italic><sup>2</sup><bold>I</bold> term in <xref ref-type="disp-formula" rid="pcbi.1004567.e013">Eq 10</xref>. The growth of uncertainty over time increases with the diffusion variance <italic>τ</italic><sup>2</sup>, leading to higher learning rates in more “volatile” environments. The relationship between volatility and learning rate follows intuitively from the fact that high volatility means that older information is less relevant and can therefore be forgotten [<xref ref-type="bibr" rid="pcbi.1004567.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref039">39</xref>]. The second factor governing the covariance matrix update is the reduction of uncertainty due to observation of data, as expressed by the term <inline-formula id="pcbi.1004567.e017"><alternatives><graphic id="pcbi.1004567.e017g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e017"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">k</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:msubsup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>n</mml:mi> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>τ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi mathvariant="bold">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Whenever a cue is observed, its variance on the diagonal of the covariance matrix is reduced, as are the covariances (off-diagonals) for any correlated cues.</p>
<p>One implication of the Kalman filter is that repeated CS presentations will attenuate posterior uncertainty and therefore reduce the Kalman gain. As illustrated in <xref ref-type="fig" rid="pcbi.1004567.g002">Fig 2</xref>, this reduction in gain produces latent inhibition, capturing the intuition that CS pre-exposure reduces “attention” (associability or learning rate). The Kalman filter can also explain why interposing an interval between pre-exposure and conditioning attenuates latent inhibition [<xref ref-type="bibr" rid="pcbi.1004567.ref040">40</xref>]: The posterior variance grows over the interval (due to random diffusion of the weights), increasing the Kalman gain. Thus, the Kalman filter can model some changes in learning that occur in the absence of prediction error, unlike the Rescorla-Wagner model.</p>
<fig id="pcbi.1004567.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004567.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Kalman filter simulation of latent inhibition.</title>
<p>(<italic>A</italic>) Reward expectation following pre-exposure (Pre) and no pre-exposure (No-Pre) conditions. (<italic>B</italic>) The Kalman gain as a function of pre-exposure trial.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004567.g002"/>
</fig>
<p>The Kalman filter can also account for the effects of various post-training manipulations, such as backward blocking [<xref ref-type="bibr" rid="pcbi.1004567.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref006">6</xref>]. During the compound training phase, the model learns that the cue weights must sum to 1 (the reward value), and thus any weight configurations in which one weight is large necessitates that the other weight be small. Mathematically, this is encoded as negative covariance between the weights (i.e., the off-diagonals of Σ<sub><italic>n</italic></sub>). As a consequence, learning that <italic>A</italic> predicts reward leads to a reduction in the associative strength for <italic>B</italic>.</p>
<p>Beyond backward blocking, the Kalman filter can capture a wider range of recovery phenomena than has previously been simulated. Four examples are shown in <xref ref-type="fig" rid="pcbi.1004567.g003">Fig 3</xref> (see <xref ref-type="sec" rid="sec013">Methods</xref> for simulation details). As shown by Matzel and colleagues [<xref ref-type="bibr" rid="pcbi.1004567.ref034">34</xref>], overshadowing (AB→+ training leads to weaker responding to B compared to B→+ training) can be counteracted by extinguishing one of the stimulus elements prior to test (AB→+; A→-). Similarly, extinguishing the blocking stimulus in a forward blocking paradigm (A→+; AB→+; A→-; B→?) causes a recovery of responding to the blocked stimulus [<xref ref-type="bibr" rid="pcbi.1004567.ref035">35</xref>], and extinguishing one of the stimulus A in an overexpectation paradigm (A→+ / B→+; AB→+; A→-; B→?) causes a recovery of responding to the other stimulus B [<xref ref-type="bibr" rid="pcbi.1004567.ref036">36</xref>]. Finally, extinguishing the excitatory stimulus A in a conditioned inhibition paradigm (A→+ / AB→-; A→-) reduces the negative associative strength of the inhibitory stimulus B [<xref ref-type="bibr" rid="pcbi.1004567.ref041">41</xref>].</p>
<fig id="pcbi.1004567.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004567.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Kalman filter simulation of recovery phenomena.</title>
<p>(<italic>A</italic>) Overshadowing and unovershadowing by extinction of the overshadowing stimulus. (<italic>B</italic>) Forward blocking and unblocking by extinction of the blocking stimulus. (<italic>C</italic>) Overexpectation and unoverexpectation by extinction of one element. (<italic>D</italic>) Conditioned inhibition and uninhibition by extinction of the excitatory stimulus.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004567.g003"/>
</fig>
<p>All of these examples have a common structure shared with backward blocking, where compound training causes the acquisition of negative covariance between the stimulus elements. This negative covariance implies that post-training inflation or deflation of one stimulus will cause changes in beliefs about the other stimulus. Post-training recovery phenomena have inspired new theories that allow learning to occur for absent stimuli. For example, Van Hamme and Wasserman [<xref ref-type="bibr" rid="pcbi.1004567.ref018">18</xref>] developed an extension of the Rescorla-Wagner model in which the associative strengths for absent cues are modified just like present cues, but possibly with a smaller learning rate (see also [<xref ref-type="bibr" rid="pcbi.1004567.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref043">43</xref>]). The Kalman filter provides a normative explanation of recovery phenomena, while retaining close similarities with classical theories like the Rescorla-Wagner model.</p>
</sec>
<sec id="sec008">
<title>Temporal difference learning and long-term reward prediction</title>
<p>The Kalman filter fixes some of the problems vexing the Rescorla-Wagner model, but a fundamental limitation remains: The Rescorla-Wagner model is a <italic>trial-level</italic> model, which means that it only makes predictions at the granularity of a trial, remaining blind to intra-trial structure such as stimulus duration and the inter-stimulus interval. While one can finesse this by treating each time-step in the model as a sub-division of a trial, such a solution is inadequate because it fails to capture the fact that conditioned responses are anticipatory of long-term future events. For example, interposing a delay between CS offset and US onset means that the CS never co-occurs with the US and hence should not produce any conditioning according to this particular real-time extension of the Rescorla-Wagner model (contrary to the empirical data).</p>
<p>It is possible to augment the Rescorla-Wagner model with a time-varying stimulus trace evoked by the CS, allowing the trace to enter into association with the US. This idea goes back to the work of Pavlov [<xref ref-type="bibr" rid="pcbi.1004567.ref026">26</xref>] and Hull [<xref ref-type="bibr" rid="pcbi.1004567.ref044">44</xref>], who posited that the stimulus trace persists for several seconds following CS offset, decaying gradually over time. More complex stimulus traces have been explored by later researchers (e.g., [<xref ref-type="bibr" rid="pcbi.1004567.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref046">46</xref>]).</p>
<p>While a persistent trace enables the model to capture aspects of intra-trial temporal structure, there is an additional problem: the association between the trace and the US can only be reinforced following US presentation, but contrary to this assumption it has been demonstrated empirically that an association can be reinforced without any pairing between the CS and US. As mentioned above, an example is second-order conditioning [<xref ref-type="bibr" rid="pcbi.1004567.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref030">30</xref>], where A is paired with reward and subsequently B is paired with A, resulting in conditioned responding to B. An analogous phenomenon, known as <italic>conditioned reinforcement</italic>, has been studied in operant conditioning [<xref ref-type="bibr" rid="pcbi.1004567.ref047">47</xref>]. Somehow, a CS must be able to acquire the reinforcing properties of the US with which it has been paired.</p>
<p>The TD model [<xref ref-type="bibr" rid="pcbi.1004567.ref009">9</xref>] offers a solution to both of these problems, grounded in a different rational analysis of associative learning. The underlying assumption of the TD model is that the associative learning system is designed to learn a prediction of <italic>long-term future</italic> reward, rather than <italic>immediate</italic> reward (as was assumed in our rational analysis of the Rescorla-Wagner and Kalman filter models). Specifically, let us imagine an animal that traverses a “state space” defined by the configuration of stimuli, moving from <bold>x</bold><sub><italic>t</italic></sub> at time <italic>t</italic> to <bold>x</bold><sub><italic>t</italic>+1</sub> according to a transition distribution <italic>p</italic>(<bold>x</bold><sub><italic>t</italic>+1</sub>∣<bold>x</bold><sub><italic>t</italic></sub>). (Note that we now index by <italic>t</italic> to emphasize that we are in “real time”). The <italic>value</italic> of state <bold>x</bold><sub><italic>t</italic></sub> is defined as the expected discounted future return (cumulative reward):
<disp-formula id="pcbi.1004567.e018"><alternatives><graphic id="pcbi.1004567.e018g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e018"/><mml:math id="M18" display="block" overflow="scroll"><mml:mrow><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi><mml:mspace width="1pt"/><mml:mo>[</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>γ</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>]</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>
where <italic>γ</italic> ∈ [0, 1] is a <italic>discount factor</italic> that controls how heavily the near future is weighted relative to the distant future. Applications of the TD model to associative learning assume that conditioned responding is monotonically related to the animal’s value estimate. This means that two stimuli might have the same expected reward, but responding will be higher to the stimulus that predicts greater cumulative reward in the future.</p>
<p>The RL problem is to learn the value function. As is common in the RL literature [<xref ref-type="bibr" rid="pcbi.1004567.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref049">49</xref>], I will assume that the value function can be approximated as a linear combination of stimuli: <inline-formula id="pcbi.1004567.e019"><alternatives><graphic id="pcbi.1004567.e019g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e019"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>t</mml:mi> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. This reduces the RL problem to learning <bold>w</bold><sub><italic>t</italic></sub>. This can be accomplished using an update very similar to that of the Rescorla-Wagner model [<xref ref-type="bibr" rid="pcbi.1004567.ref049">49</xref>]:
<disp-formula id="pcbi.1004567.e020"><alternatives><graphic id="pcbi.1004567.e020g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e020"/><mml:math id="M20" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
where <italic>δ</italic><sub><italic>t</italic></sub> is now defined as the <italic>temporal difference prediction error</italic>:
<disp-formula id="pcbi.1004567.e021"><alternatives><graphic id="pcbi.1004567.e021g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e021"/><mml:math id="M21" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>δ</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi></mml:mrow> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi></mml:mrow> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
Except for the addition of the future reward expectation term <inline-formula id="pcbi.1004567.e022"><alternatives><graphic id="pcbi.1004567.e022g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e022"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:mi>γ</mml:mi> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi></mml:mrow> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, the TD prediction error is identical to the Rescorla-Wagner prediction error, and reduces to it when <italic>γ</italic> = 0.</p>
<p>In order to apply the TD model to associative learning tasks, it is necessary to specify a temporally extended stimulus representation. Sutton and Barto [<xref ref-type="bibr" rid="pcbi.1004567.ref009">9</xref>] adopted the <italic>complete serial compound</italic> (CSC) representation, which divides a stimulus into a sequence of non-overlapping bins. Thus, a stimulus lasting for two time steps would be represented by <bold>x</bold><sub>1</sub> = [1, 0] and <bold>x</bold><sub>2</sub> = [0, 1]. Although there are a number of problems with this representation [<xref ref-type="bibr" rid="pcbi.1004567.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref050">50</xref>–<xref ref-type="bibr" rid="pcbi.1004567.ref052">52</xref>], I use it here for continuity with previous work.</p>
<p>The TD model can account for a number of intra-trial phenomena, such as the effect of stimulus timing on acquisition and cue competition (see [<xref ref-type="bibr" rid="pcbi.1004567.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref011">11</xref>] for extensive simulations). It also provides a natural explanation for second-order conditioning: despite the immediate reward term <italic>r</italic><sub><italic>t</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1004567.e021">Eq 14</xref> being 0 for A→B trials, the future reward expectation term <inline-formula id="pcbi.1004567.e023"><alternatives><graphic id="pcbi.1004567.e023g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e023"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mi>γ</mml:mi> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>t</mml:mi></mml:mrow> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is positive (due to the B→+ trials) and hence the value of A is increased.</p>
<p>In summary, the TD model has proven to be a successful real-time generalization of the Rescorla-Wagner model, and also has the advantage of being grounded in the normative theory of RL. However, it lacks the uncertainty-tracking mechanisms of the Kalman filter, which I argued are important for understanding CS pre-exposure and post-training recovery effects. I now turn to the problem of unifying the Kalman filter and TD models.</p>
</sec>
<sec id="sec009">
<title>A unifying view: Kalman temporal difference learning</title>
<p>Bayesian versions of TD learning have been developed in a number of different forms [<xref ref-type="bibr" rid="pcbi.1004567.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref054">54</xref>]; all of them have in common the idea that an agent tracks the entire distribution over discounted future returns, not just the mean. Of particular interest is <italic>Kalman TD</italic>, an elegant adaptation of the Kalman filtering machinery to TD learning developed by Geist and Pietquin [<xref ref-type="bibr" rid="pcbi.1004567.ref013">13</xref>]. Operationally, the only change from the Kalman filter model described above is to replace the stimulus features <bold>x</bold><sub><italic>n</italic></sub> with their discounted time derivative, <bold>h</bold><sub><italic>t</italic></sub> = <italic>γ</italic> <bold>x</bold><sub><italic>t</italic> + 1</sub>−<bold>x</bold><sub><italic>t</italic></sub>. To see why this makes sense, note that the immediate reward can be expressed in terms of the difference between two values:
<disp-formula id="pcbi.1004567.e024"><alternatives><graphic id="pcbi.1004567.e024g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e024"/><mml:math id="M24" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo> <mml:mi>γ</mml:mi> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo> <mml:mi>γ</mml:mi> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>t</mml:mi></mml:mrow> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>t</mml:mi></mml:mrow> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>t</mml:mi></mml:mrow> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula>
I have assumed here, as in the previous section, that values are linear in the stimulus features. As the derivation shows, this implies that rewards are linear in the discounted time derivative of the stimulus features. Under the assumption that the weights evolve over time as a Gaussian random walk and the rewards are corrupted by Gaussian noise, we can use the same LDS formulation described earlier, for which the Kalman filter implements Bayesian estimation.</p>
<p>Kalman TD combines the strengths of Kalman filtering and TD learning: it is a real-time model that that represents a distribution over weights rather than a point estimate. These properties allow the model to capture both within-trial structure and retrospective revaluation. In the remainder of this section, I present several examples that illustrate the intersection of these phenomena, and compare the predictions of TD and Kalman TD (since these examples involve within-trial structure, I do not consider the Kalman filter or Rescorla-Wagner).</p>
<p>Denniston et al. [<xref ref-type="bibr" rid="pcbi.1004567.ref055">55</xref>] presented a series of experiments exploring recovery from overshadowing. In one experiment (summarized in <xref ref-type="fig" rid="pcbi.1004567.g004">Fig 4A</xref>), the authors combined overshadowing and second-order conditioning to show that extinguishing an overshadowed stimulus allows its partner to better support second-order conditioning. Animals were divided into two groups, OV-A and OV-B. Both groups first learned to associate two light-tone compounds (AX and BY) with a US (a footshock in this case). This compound training protocol was expected to result in overshadowing. One element of the compound was then extinguished (A in group OV-A, B in group OV-B). Stimulus X was then used as a second-order reinforcer for conditioning of a novel stimulus, Z. Denniston et al. found that overshadowing reduced the ability of an overshadowed stimulus to support second-order conditioning, but this reduction could be attenuated if the overshadowing stimulus was extinguished. In particular, they found that responding at test to stimulus Z was greater in group OV-A than in group OV-B.</p>
<fig id="pcbi.1004567.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004567.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Overshadowing and second-order conditioning.</title>
<p>(<italic>A</italic>) Experimental design [<xref ref-type="bibr" rid="pcbi.1004567.ref055">55</xref>]. Note that two control groups have been ignored here for simplicity. (<italic>B</italic>) Simulated value of stimulus Z computed by Kalman TD (left) and TD (right). Only Kalman TD correctly predicts that extinguishing an overshadowing stimulus will allow the overshadowed stimulus to support second-order conditioning. (<italic>C</italic>) Posterior covariance between weights for stimuli A and X (left) and Kalman gain for stimulus X (right) as a function of Phase 1 trial. (<italic>D</italic>) Posterior covariance between weights for stimuli A and X (left) and Kalman gain for stimulus X (right) as a function of Phase 2 trial.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004567.g004"/>
</fig>
<p>Simulations show that KTD, but not TD, can capture this finding (<xref ref-type="fig" rid="pcbi.1004567.g004">Fig 4B</xref>). While TD can capture second-order conditioning, it cannot explain why post-training extinction changes the value of an absent stimulus, because only the weights for presented stimuli are eligible for updating. The latter phenomenon is captured by the Kalman filter, which encodes the negative covariation between stimuli. As a consequence, the Kalman gain for stimulus X during Phase 2 (despite X not appearing during this phase) is negative, meaning that extinguishing A will cause inflation of X. By contrast, extinguishing B has no effect on the value of X, since B and X did not covary during Phase 1. This is essentially the same logic that explains the post-training recovery phenomena described above, but applied to a second-order conditioning scenario outside the scope of the Kalman filter.</p>
<p>One extensively studied aspect of second-order conditioning has been the effect of extinguishing the first-order stimulus on responding to the second-order stimulus. Rashotte and colleagues [<xref ref-type="bibr" rid="pcbi.1004567.ref056">56</xref>] reported a Pavlovian autoshaping experiment with pigeons in which extinction of the first-order stimulus reduces responding to the second-order stimulus. This finding has been replicated a number of times [<xref ref-type="bibr" rid="pcbi.1004567.ref057">57</xref>–<xref ref-type="bibr" rid="pcbi.1004567.ref059">59</xref>], although notably it is not found in a number of other paradigms [<xref ref-type="bibr" rid="pcbi.1004567.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref060">60</xref>], and a comprehensive explanation for this discrepancy is still lacking. <xref ref-type="fig" rid="pcbi.1004567.g005">Fig 5</xref> shows that Kalman TD predicts sensitivity to first-order extinction, whereas TD predicts no sensitivity. The sensitivity of Kalman TD derives from the positive covariance between the first- and second-order stimuli, such that changes in the value of the first-order stimulus immediately affect the value of the second-order stimulus.</p>
<fig id="pcbi.1004567.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004567.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Second-order extinction.</title>
<p>(<italic>A</italic>) Experimental design [<xref ref-type="bibr" rid="pcbi.1004567.ref056">56</xref>]. (<italic>B</italic>) Simulated value of stimulus Z computed by Kalman TD (left) and TD (right).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004567.g005"/>
</fig>
<p>I next turn to serial compound conditioning, which illustrates the within-trial behavior of Kalman TD. As summarized in <xref ref-type="fig" rid="pcbi.1004567.g006">Fig 6A</xref>, Gibbs et al. [<xref ref-type="bibr" rid="pcbi.1004567.ref061">61</xref>] studied the effects of extinguishing stimulus X following serial compound training (Z→X→+). They found that this extinction treatment reduced the conditioned response to Z (see [<xref ref-type="bibr" rid="pcbi.1004567.ref015">15</xref>] for similar results). Kalman TD can account for this finding (<xref ref-type="fig" rid="pcbi.1004567.g006">Fig 6B</xref>) because the positive covariance between Z and X means that the value of Z is sensitive to post-training manipulations of X’s value (<xref ref-type="fig" rid="pcbi.1004567.g006">Fig 6C</xref>). TD, which lacks a covariance-tracking mechanism, cannot account for this finding.</p>
<fig id="pcbi.1004567.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004567.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Serial compound extinction.</title>
<p>(<italic>A</italic>) Experimental design [<xref ref-type="bibr" rid="pcbi.1004567.ref061">61</xref>]. (<italic>B</italic>) Simulated value of stimulus Z computed by Kalman TD (left) and TD (right). (<italic>C</italic>) Posterior covariance between the weights for stimuli Z and X as a function of conditioning trial.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004567.g006"/>
</fig>
<p>In a second experiment (<xref ref-type="fig" rid="pcbi.1004567.g007">Fig 7A</xref>), Gibbs et al. had the extinction phase occur prior to training, thereby making it a latent inhibition (CS pre-exposure) design. As with the extinction treatment, latent inhibition reduces responding to Z, a finding that can be accounted for by Kalman TD, but not TD (<xref ref-type="fig" rid="pcbi.1004567.g007">Fig 7B</xref>). The Kalman TD account is essentially the same as the Kalman filter account of latent inhibition: Pre-exposure of X causes its posterior variance to decrease, which results in a concomitant reduction of the Kalman gain (<xref ref-type="fig" rid="pcbi.1004567.g007">Fig 7C</xref>).</p>
<fig id="pcbi.1004567.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004567.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Serial compound latent inhibition.</title>
<p>(<italic>A</italic>) Experimental design [<xref ref-type="bibr" rid="pcbi.1004567.ref061">61</xref>]. (<italic>B</italic>) Simulated value of stimulus Z computed by Kalman TD (left) and TD (right). (<italic>C</italic>) Posterior variance (left) and Kalman gain (right) of stimulus X as a function of pre-exposure trial.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004567.g007"/>
</fig>
<p>A conceptually related design was studied by Shevill and Hall [<xref ref-type="bibr" rid="pcbi.1004567.ref062">62</xref>]. Instead of extinguishing the first-order stimulus X, they extinguished the second-order stimulus Z and examined the effect on responding to the first-order stimulus (<xref ref-type="fig" rid="pcbi.1004567.g008">Fig 8A</xref>). This extinction procedure increased responding to the first-order stimulus relative to another first-order stimulus (Y) whose associated second-order stimulus had not been extinguished. This finding is predicted by Kalman TD, but not TD (<xref ref-type="fig" rid="pcbi.1004567.g008">Fig 8B</xref>), because in a serial conditioning procedure the first-order stimulus overshadows the second-order stimulus, and extinguishing the first-order stimulus causes a recovery from overshadowing (a reduced first-order value is evidence that the second-order stimulus was responsible for the outcome). Note that this explanation is essentially the same as the one provided by the Kalman filter for recovery from overshadowing with simultaneous compounds [<xref ref-type="bibr" rid="pcbi.1004567.ref034">34</xref>]; the key difference here is that in serial compounds the second-order stimulus tends to differentially overshadow the first-order stimulus [<xref ref-type="bibr" rid="pcbi.1004567.ref063">63</xref>].</p>
<fig id="pcbi.1004567.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004567.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Recovery from overshadowing.</title>
<p>(<italic>A</italic>) Experimental design [<xref ref-type="bibr" rid="pcbi.1004567.ref062">62</xref>]. (<italic>B</italic>) Simulated value of stimulus X and stimulus Y computed by Kalman TD (left) and TD (right).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004567.g008"/>
</fig>
</sec>
</sec>
<sec id="sec010" sec-type="conclusions">
<title>Discussion</title>
<p>While the theoretical literature on associative learning is vast and complex, a few principles continue to play a central role in contemporary thinking. Some of these principles are embodied in the Rescorla-Wagner model and its generalizations—the TD model and the Bayesian Kalman filter model. Each model has strengths and weaknesses, as reviewed above. I have argued that Kalman TD represents a synthesis of these models that combines their strengths and remedies some of their weaknesses.</p>
<p>These models are by no means the only generalizations of the Rescorla-Wagner model (see, for example, [<xref ref-type="bibr" rid="pcbi.1004567.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref064">64</xref>]), and there are other theoretical frameworks that offer different perspectives on the mechanisms underlying associative learning (e.g., [<xref ref-type="bibr" rid="pcbi.1004567.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref065">65</xref>]). Nonetheless, the synthesis of Bayesian and TD models has special significance given their influence on contemporary experimental research, particularly in neuroscience [<xref ref-type="bibr" rid="pcbi.1004567.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref066">66</xref>]. These models offer different normative views of the associative learning problem—the Kalman filter views associative learning as tracking a changing reward distribution over time, while the TD model views associative learning as predicting long-term future reward (value). A central goal of this paper was to provide a unifying view, according to which associative learning is the tracking of a changing value distribution over time. The fruit of this unification is a model that can account for a number of complex phenomena that cannot be accounted for by either model on its own.</p>
<p>While Kalman TD can capture a number of phenomena qualitatively, a task for future research is to validate the model’s quantitative predictions. Such a validation is hampered by the fact that associative learning paradigms differ in many procedural details. Thus, it is important to adopt a single paradigm whose parameters can be explored systematically. Quantitative evaluation of Kalman filtering has been extensively studied in the motor control literature [<xref ref-type="bibr" rid="pcbi.1004567.ref067">67</xref>], and similar experimental techniques could be applied to associative learning. Among the predictions made by Kalman TD are: (1) uncertainty should grow linearly with the intertrial interval, and (2) the strength of association should grow linearly with the magnitude of the temporal derivative of the features.</p>
<sec id="sec011">
<title>Limitations and extensions</title>
<p>One of the important insights of the Pearce-Hall model [<xref ref-type="bibr" rid="pcbi.1004567.ref017">17</xref>] was that learning rate should increase with surprise—formalized as the absolute value of recent prediction errors. This model successfully predicts that inconsistently pairing a CS with an outcome enhances its learning rate in a subsequent training phase with a different outcome [<xref ref-type="bibr" rid="pcbi.1004567.ref068">68</xref>]. In the Kalman filter (as well as in Kalman TD), changes in learning rate are driven solely by changes in the covariance matrix, which does not depend on outcomes. Thus, the model cannot explain any changes in learning rate that depend on prediction errors.</p>
<p>One way to deal with this problem is to recognize that the animal may have uncertainty about the transition dynamics (parameterized by <italic>τ</italic>), so that it learns simultaneously about the associative weights and <italic>τ</italic>. It is straightforward to show that the partial derivative of the log-likelihood with respect to <italic>τ</italic> monotonically increases with <inline-formula id="pcbi.1004567.e025"><alternatives><graphic id="pcbi.1004567.e025g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e025"/><mml:math id="M25" display="inline" overflow="scroll"><mml:msubsup><mml:mi>δ</mml:mi> <mml:mi>t</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>, which means that gradient ascent will increase <italic>τ</italic> when the squared prediction error is greater than 0. This will give rise to qualitatively similar behavior to the Pearce-Hall model. Closely related Bayesian treatments have been recently explored, although not in the context of TD learning [<xref ref-type="bibr" rid="pcbi.1004567.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref069">69</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref070">70</xref>].</p>
<p>Another issue that arises in models of associative learning is the problem of feature (or state space) representation [<xref ref-type="bibr" rid="pcbi.1004567.ref071">71</xref>]. When we present an animal with a stimulus configuration, it is reasonable to expect that the animal applies some kind of processing to the stimulus representation. Some neural network models conceive this processing as the application of a non-linear transformation to the stimulus inputs, resulting in a hidden-layer representation that encodes configural features [<xref ref-type="bibr" rid="pcbi.1004567.ref064">64</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref072">72</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref073">73</xref>]. Other models derive stimulus representation from a clustering process that partitions stimulus inputs into a discrete set of states [<xref ref-type="bibr" rid="pcbi.1004567.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref071">71</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref074">74</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref075">75</xref>]. A related line of work has studied the representation of temporally extended stimuli; for example, a number of theories postulate a distributed representation of stimuli using basis functions with temporal receptive fields (see [<xref ref-type="bibr" rid="pcbi.1004567.ref052">52</xref>] for a review). In general, any of these representations are compatible with Kalman TD as long as values are linear functions of the representation. While this may sound limiting, it is in fact extremely powerful, since any smooth function can be arbitrarily well approximated by a linear combination of suitably chosen basis functions [<xref ref-type="bibr" rid="pcbi.1004567.ref076">76</xref>].</p>
<p>The final issue I will mention here concerns instrumental learning: A complete theory of associative learning must account for associations between actions and outcomes. One influential framework for combining Pavlovian and instrumental learning processes is the actor-critic architecture [<xref ref-type="bibr" rid="pcbi.1004567.ref077">77</xref>], according to which a Pavlovian “critic” learns state values, while an instrumental “actor” optimizes its policy using the critic’s prediction errors. Within this architecture, Kalman TD could function as a Bayesian critic. An interesting question that then arises is what role the critic’s uncertainty should play in guiding policy updating (see [<xref ref-type="bibr" rid="pcbi.1004567.ref078">78</xref>] for one possibility).</p>
</sec>
<sec id="sec012">
<title>Conclusions</title>
<p>This paper makes several contributions. First, it provides a unifying review of several associative learning models, elucidating their connections and their grounding in normative computational principles. Second, it presents new simulations that highlight previously unappreciated aspects of these models. Third, it presents Kalman TD, a synthesis of these models. While this model has been described in other papers [<xref ref-type="bibr" rid="pcbi.1004567.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref014">14</xref>], this is the first systematic application to associative learning. This paper demonstrates that several prominent themes in associative learning theory can be coherently unified.</p>
</sec>
</sec>
<sec id="sec013" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec014">
<title>Simulation details</title>
<sec id="sec015">
<title>Latent learning</title>
<p>In the “Pre” condition, the agent was exposed to 10 pre-exposure trials (A→-) followed by 10 conditioning trials (A→+). In the “No-Pre” condition, the pre-exposure phase was omitted.</p>
</sec>
<sec id="sec016">
<title>Overshadowing</title>
<p>In the “overshadowing” condition, the agent was exposed to 10 compound conditioning trials (AB→+) followed by a test of responding to B. In the “unovershadowing” condition, the agent was additionally exposed to 10 extinction trials (A→-) between conditioning and test.</p>
</sec>
<sec id="sec017">
<title>Forward blocking</title>
<p>In the “blocking” condition, the agent was exposed to 10 conditioning trials (A→+) followed by 10 compound conditioning trials (AB→+) and a test of responding to B. In the “unblocking” condition, the agent was additionally exposed to 10 extinction trials (A→-) between compound conditioning and test.</p>
</sec>
<sec id="sec018">
<title>Overexpectation</title>
<p>In the “overexpectation” condition, the agent was exposed to 10 conditioning trials for each stimulus (A→+ / B→+) followed by 10 compound conditioning trials (AB→+) and a test of responding to B. In the “unoverexpectation” condition, the agent was additionally exposed to 10 extinction trials (A→-) between compound conditioning and test.</p>
</sec>
<sec id="sec019">
<title>Conditioned inhibition</title>
<p>In the “inhibition” condition, the agent was exposed to 10 A→+ trials and 10 AB→- trials, followed by a test of responding to B. In the “uninhibition” condition, the agent was additionally exposed to 10 extinction trials (A→-) prior to test.</p>
</sec>
<sec id="sec020">
<title>Overshadowing and second-order conditioning</title>
<p>The design is summarized in <xref ref-type="fig" rid="pcbi.1004567.g004">Fig 4A</xref>. Each phase consisted of 10 trials.</p>
</sec>
<sec id="sec021">
<title>Serial compound extinction and latent inhibition</title>
<p>The designs are summarized in Figs <xref ref-type="fig" rid="pcbi.1004567.g006">6A</xref> and <xref ref-type="fig" rid="pcbi.1004567.g007">7A</xref>. Each phase consisted of 10 trials.</p>
</sec>
<sec id="sec022">
<title>Recovery from overshadowing</title>
<p>The design is summarized in <xref ref-type="fig" rid="pcbi.1004567.g008">Fig 8A</xref>. Each phase consisted of 10 trials.</p>
</sec>
</sec>
<sec id="sec023">
<title>Model parameters</title>
<sec id="sec024">
<title>Kalman filter</title>
<p>For all simulations, the following parameters were used: <inline-formula id="pcbi.1004567.e026"><alternatives><graphic id="pcbi.1004567.e026g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004567.e026"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:msup><mml:mi>τ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>01</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec025">
<title>Temporal difference learning</title>
<p>For all simulations, the following parameters were used: <italic>α</italic> = 0.3, <italic>γ</italic> = 0.98. A <italic>complete serial compound</italic> [<xref ref-type="bibr" rid="pcbi.1004567.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004567.ref048">48</xref>] was used for the temporal representation: Each stimulus was divided into 4 time bins, and each bin acted as a stimulus feature that was active only at a specific time relative to the stimulus onset. The precise duration of the stimuli was not important for our results.</p>
</sec>
<sec id="sec026">
<title>Kalman temporal difference learning</title>
<p>For all simulations, the parameters were the same as for the Kalman filter, with the addition of a discount factor <italic>γ</italic> = 0.98. The temporal representation was the same complete serial compound used in the TD simulations.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1004567.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Shanks</surname> <given-names>DR</given-names></name>. <source>The Psychology of Associative Learning</source>. <publisher-name>Cambridge University Press</publisher-name>; <year>1995</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pearce</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Bouton</surname> <given-names>ME</given-names></name>. <article-title>Theories of associative learning in animals</article-title>. <source>Annual Review of Psychology</source>. <year>2001</year>;<volume>52</volume>:<fpage>111</fpage>–<lpage>139</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.psych.52.1.111" xlink:type="simple">10.1146/annurev.psych.52.1.111</ext-link></comment> <object-id pub-id-type="pmid">11148301</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Kakade</surname> <given-names>S</given-names></name>. <chapter-title>Explaining Away in Weight Space</chapter-title>. In: <name name-style="western"><surname>Leen</surname> <given-names>TK</given-names></name>, <name name-style="western"><surname>Dietterich</surname> <given-names>TG</given-names></name>, <name name-style="western"><surname>Tresp</surname> <given-names>V</given-names></name>, editors. <source>Advances in Neural Information Processing Systems 13</source>. <publisher-name>MIT Press</publisher-name>; <year>2001</year>. p. <fpage>451</fpage>–<lpage>457</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kakade</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Acquisition and extinction in autoshaping</article-title>. <source>Psychological Review</source>. <year>2002</year>;<volume>109</volume>:<fpage>533</fpage>–<lpage>544</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.109.3.533" xlink:type="simple">10.1037/0033-295X.109.3.533</ext-link></comment> <object-id pub-id-type="pmid">12088244</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Courville</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Touretzky</surname> <given-names>DS</given-names></name>. <article-title>Bayesian theories of conditioning in a changing world</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2006</year>;<volume>10</volume>:<fpage>294</fpage>–<lpage>300</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2006.05.004" xlink:type="simple">10.1016/j.tics.2006.05.004</ext-link></comment> <object-id pub-id-type="pmid">16793323</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kruschke</surname> <given-names>JK</given-names></name>. <article-title>Bayesian approaches to associative learning: From passive to active learning</article-title>. <source>Learning &amp; Behavior</source>. <year>2008</year>;<volume>36</volume>:<fpage>210</fpage>–<lpage>226</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/LB.36.3.210" xlink:type="simple">10.3758/LB.36.3.210</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Blei</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Context, learning, and extinction</article-title>. <source>Psychological Review</source>. <year>2010</year>;<volume>117</volume>:<fpage>197</fpage>–<lpage>209</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0017808" xlink:type="simple">10.1037/a0017808</ext-link></comment> <object-id pub-id-type="pmid">20063968</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Exploring a latent cause theory of classical conditioning</article-title>. <source>Learning &amp; Behavior</source>. <year>2012</year>;<volume>40</volume>:<fpage>255</fpage>–<lpage>268</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/s13420-012-0080-8" xlink:type="simple">10.3758/s13420-012-0080-8</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <chapter-title>Time-derivative models of Pavlovian reinforcement</chapter-title>. In: <name name-style="western"><surname>Gabriel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>J</given-names></name>, editors. <source>Learning and Computational Neuroscience: Foundations of Adaptive Networks</source>. <publisher-name>MIT Press</publisher-name>; <year>1990</year>. p. <fpage>497</fpage>–<lpage>537</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Reinforcement learning in the brain</article-title>. <source>Journal of Mathematical Psychology</source>. <year>2009</year>;<volume>53</volume>:<fpage>139</fpage>–<lpage>154</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jmp.2008.12.005" xlink:type="simple">10.1016/j.jmp.2008.12.005</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ludvig</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Kehoe</surname> <given-names>EJ</given-names></name>. <article-title>Evaluating the TD model of classical conditioning</article-title>. <source>Learning &amp; Behavior</source>. <year>2012</year>;<volume>40</volume>:<fpage>305</fpage>–<lpage>319</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/s13420-012-0082-6" xlink:type="simple">10.3758/s13420-012-0082-6</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Wagner</surname> <given-names>AR</given-names></name>. <chapter-title>A theory of of Pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement</chapter-title>. In: <name name-style="western"><surname>Black</surname> <given-names>AH</given-names></name>, <name name-style="western"><surname>Prokasy</surname> <given-names>WF</given-names></name>, editors. <source>Classical Conditioning II: Current Research and theory</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Appleton-Century-Crofts</publisher-name>; <year>1972</year>. p. <fpage>64</fpage>–<lpage>99</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Geist</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pietquin</surname> <given-names>O</given-names></name>. <article-title>Kalman temporal differences</article-title>. <source>Journal of Artificial Intelligence Research</source>. <year>2010</year>;<volume>39</volume>:<fpage>483</fpage>–<lpage>532</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Keramati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Dezfouli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Piray</surname> <given-names>P</given-names></name>. <article-title>Speed/accuracy trade-off between the habitual and the goal-directed processes</article-title>. <source>PLoS Computational Biology</source>. <year>2011</year>;<volume>7</volume>:<fpage>e1002055</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002055" xlink:type="simple">10.1371/journal.pcbi.1002055</ext-link></comment> <object-id pub-id-type="pmid">21637741</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Holland</surname> <given-names>PC</given-names></name>, <name name-style="western"><surname>Ross</surname> <given-names>RT</given-names></name>. <article-title>Within-compound associations in serial compound conditioning</article-title>. <source>Journal of Experimental Psychology: Animal Behavior Processes</source>. <year>1981</year>;<volume>7</volume>:<fpage>228</fpage>–<lpage>241</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Miller</surname> <given-names>RR</given-names></name>, <name name-style="western"><surname>Barnet</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Grahame</surname> <given-names>NJ</given-names></name>. <article-title>Assessment of the Rescorla-Wagner model</article-title>. <source>Psychological Bulletin</source>. <year>1995</year>;<volume>117</volume>:<fpage>363</fpage>–<lpage>386</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-2909.117.3.363" xlink:type="simple">10.1037/0033-2909.117.3.363</ext-link></comment> <object-id pub-id-type="pmid">7777644</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pearce</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Hall</surname> <given-names>G</given-names></name>. <article-title>A model for Pavlovian learning: Variations in the effectiveness of conditioned but not of unconditioned stimuli</article-title>. <source>Psychological Review</source>. <year>1980</year>;<volume>87</volume>:<fpage>532</fpage>–<lpage>552</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.87.6.532" xlink:type="simple">10.1037/0033-295X.87.6.532</ext-link></comment> <object-id pub-id-type="pmid">7443916</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Van Hamme</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Wasserman</surname> <given-names>EA</given-names></name>. <article-title>Cue competition in causality judgments: The role of nonpresentation of compound stimulus elements</article-title>. <source>Learning and Motivation</source>. <year>1994</year>;<volume>25</volume>:<fpage>127</fpage>–<lpage>151</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1006/lmot.1994.1008" xlink:type="simple">10.1006/lmot.1994.1008</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schmajuk</surname> <given-names>NA</given-names></name>, <name name-style="western"><surname>Larrauri</surname> <given-names>JA</given-names></name>. <article-title>Experimental challenges to theories of classical conditioning: application of an attentional model of storage and retrieval</article-title>. <source>Journal of Experimental Psychology: Animal Behavior Processes</source>. <year>2006</year>;<volume>32</volume>:<fpage>1</fpage>–<lpage>20</lpage>. <object-id pub-id-type="pmid">16435961</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Robbins</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Monro</surname> <given-names>S</given-names></name>. <article-title>A stochastic approximation method</article-title>. <source>The Annals of Mathematical Statistics</source>. <year>1951</year>;<volume>22</volume>:<fpage>400</fpage>–<lpage>407</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/aoms/1177729586" xlink:type="simple">10.1214/aoms/1177729586</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Yuille</surname> <given-names>AL</given-names></name>. <chapter-title>The Rescorla-Wagner Algorithm and Maximum Likelihood Estimation of Causal Parameters</chapter-title>. In: <name name-style="western"><surname>Saul</surname> <given-names>LK</given-names></name>, <name name-style="western"><surname>Weiss</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, editors. <source>Advances in Neural Information Processing Systems 17</source>. <publisher-name>MIT Press</publisher-name>; <year>2005</year>. p. <fpage>1585</fpage>–<lpage>1592</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="book"><name name-style="western"><surname>Widrow</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Hoff</surname> <given-names>ME</given-names></name>. <chapter-title>Adaptive switching circuits</chapter-title>. <source>Proc Of WESCON Conv Rec, part 4</source>. <year>1960</year>;p. <fpage>96</fpage>–<lpage>140</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bach</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Knowing how much you don’t know: a neural organization of uncertainty estimates</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2012</year>;<volume>13</volume>:<fpage>572</fpage>–<lpage>586</lpage>. <object-id pub-id-type="pmid">22781958</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>. <article-title>Probabilistic brains: knowns and unknowns</article-title>. <source>Nature Neuroscience</source>. <year>2013</year>;<volume>16</volume>:<fpage>1170</fpage>–<lpage>1178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3495" xlink:type="simple">10.1038/nn.3495</ext-link></comment> <object-id pub-id-type="pmid">23955561</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>. <article-title>Reduction in the effectiveness of reinforcement after prior excitatory conditioning</article-title>. <source>Learning and Motivation</source>. <year>1970</year>;<volume>1</volume>:<fpage>372</fpage>–<lpage>381</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0023-9690(70)90101-3" xlink:type="simple">10.1016/0023-9690(70)90101-3</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Pavlov</surname> <given-names>IP</given-names></name>. <source>Conditioned Reflexes</source>. <publisher-name>Oxford University Press</publisher-name>; <year>1927</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>. <article-title>Pavlovian conditioned inhibition</article-title>. <source>Psychological Bulletin</source>. <year>1969</year>;<volume>72</volume>:<fpage>77</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0027760" xlink:type="simple">10.1037/h0027760</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="book"><name name-style="western"><surname>Kamin</surname> <given-names>LJ</given-names></name>. <chapter-title>Attention-like associative processes in classical conditioning</chapter-title>. In: <source>Miami symposium on the prediction of behavior: Aversive stimulation</source>. <publisher-name>University of Miami Press</publisher-name>, <publisher-loc>Miami, FL</publisher-loc>; <year>1968</year>. p. <fpage>9</fpage>–<lpage>31</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lubow</surname> <given-names>RE</given-names></name>. <article-title>Latent inhibition</article-title>. <source>Psychological Bulletin</source>. <year>1973</year>;<volume>79</volume>:<fpage>398</fpage>–<lpage>407</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0034425" xlink:type="simple">10.1037/h0034425</ext-link></comment> <object-id pub-id-type="pmid">4575029</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rizley</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>. <article-title>Associations in second-order conditioning and sensory preconditioning</article-title>. <source>Journal of Comparative and Physiological Psychology</source>. <year>1972</year>;<volume>81</volume>:<fpage>1</fpage>–<lpage>11</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0033333" xlink:type="simple">10.1037/h0033333</ext-link></comment> <object-id pub-id-type="pmid">4672573</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Shanks</surname> <given-names>DR</given-names></name>. <article-title>Forward and backward blocking in human contingency judgement</article-title>. <source>The Quarterly Journal of Experimental Psychology</source>. <year>1985</year>;<volume>37</volume>:<fpage>1</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/14640748508402082" xlink:type="simple">10.1080/14640748508402082</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chapman</surname> <given-names>GB</given-names></name>. <article-title>Trial order affects cue interaction in contingency judgment</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>. <year>1991</year>;<volume>17</volume>:<fpage>837</fpage>–<lpage>854</lpage>. <object-id pub-id-type="pmid">1834767</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Miller</surname> <given-names>RR</given-names></name>, <name name-style="western"><surname>Matute</surname> <given-names>H</given-names></name>. <article-title>Biological significance in forward and backward blocking: Resolution of a discrepancy between animal conditioning and human causal judgment</article-title>. <source>Journal of Experimental Psychology: General</source>. <year>1996</year>;<volume>125</volume>:<fpage>370</fpage>–<lpage>386</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0096-3445.125.4.370" xlink:type="simple">10.1037/0096-3445.125.4.370</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Matzel</surname> <given-names>LD</given-names></name>, <name name-style="western"><surname>Schachtman</surname> <given-names>TR</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>RR</given-names></name>. <article-title>Recovery of an overshadowed association achieved by extinction of the overshadowing stimulus</article-title>. <source>Learning and Motivation</source>. <year>1985</year>;<volume>16</volume>:<fpage>398</fpage>–<lpage>412</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0023-9690(85)90023-2" xlink:type="simple">10.1016/0023-9690(85)90023-2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Blaisdell</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Gunther</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>RR</given-names></name>. <article-title>Recovery from blocking achieved by extinguishing the blocking CS</article-title>. <source>Animal Learning &amp; Behavior</source>. <year>1999</year>;<volume>27</volume>:<fpage>63</fpage>–<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/BF03199432" xlink:type="simple">10.3758/BF03199432</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Blaisdell</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Denniston</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>RR</given-names></name>. <article-title>Recovery from the overexpectation effect: Contrasting performance-focused and acquisition-focused models of retrospective revaluation</article-title>. <source>Animal Learning &amp; Behavior</source>. <year>2001</year>;<volume>29</volume>:<fpage>367</fpage>–<lpage>380</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/BF03192902" xlink:type="simple">10.3758/BF03192902</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kepecs</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mainen</surname> <given-names>ZF</given-names></name>. <article-title>A computational framework for the study of confidence in humans and animals</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>. <year>2012</year>;<volume>367</volume>:<fpage>1322</fpage>–<lpage>1337</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.2012.0037" xlink:type="simple">10.1098/rstb.2012.0037</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Behrens</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Woolrich</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Walton</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MF</given-names></name>. <article-title>Learning the value of information in an uncertain world</article-title>. <source>Nature Neuroscience</source>. <year>2007</year>;<volume>10</volume>:<fpage>1214</fpage>–<lpage>1221</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1954" xlink:type="simple">10.1038/nn1954</ext-link></comment> <object-id pub-id-type="pmid">17676057</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>. <article-title>The penumbra of learning: A statistical theory of synaptic tagging and capture</article-title>. <source>Network: Computation in Neural Systems</source>. <year>2014</year>;<volume>25</volume>:<fpage>97</fpage>–<lpage>115</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Aguado</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Symonds</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hall</surname> <given-names>G</given-names></name>. <article-title>Interval between preexposure and test determines the magnitude of latent inhibition: Implications for an interference account</article-title>. <source>Animal Learning &amp; Behavior</source>. <year>1994</year>;<volume>22</volume>:<fpage>188</fpage>–<lpage>194</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/BF03199919" xlink:type="simple">10.3758/BF03199919</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hallam</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Matzel</surname> <given-names>LD</given-names></name>, <name name-style="western"><surname>Sloat</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>RR</given-names></name>. <article-title>Excitation and inhibition as a function of posttraining extinction of the excitatory cue used in Pavlovian inhibition training</article-title>. <source>Learning and Motivation</source>. <year>1990</year>;<volume>21</volume>:<fpage>59</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0023-9690(90)90004-8" xlink:type="simple">10.1016/0023-9690(90)90004-8</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Dickinson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Burke</surname> <given-names>J</given-names></name>. <article-title>Within compound associations mediate the retrospective revaluation of causality judgements</article-title>. <source>The Quarterly Journal of Experimental Psychology: Section B</source>. <year>1996</year>;<volume>49</volume>:<fpage>60</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/713932614" xlink:type="simple">10.1080/713932614</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Stout</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>RR</given-names></name>. <article-title>Sometimes-competing retrieval (SOCR): A formalization of the comparator hypothesis</article-title>. <source>Psychological Review</source>. <year>2007</year>;<volume>114</volume>:<fpage>759</fpage>–<lpage>783</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.114.3.759" xlink:type="simple">10.1037/0033-295X.114.3.759</ext-link></comment> <object-id pub-id-type="pmid">17638505</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hull</surname> <given-names>CL</given-names></name>. <article-title>The problem of stimulus equivalence in behavior theory</article-title>. <source>Psychological Review</source>. <year>1939</year>;<volume>46</volume>:<fpage>9</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0054032" xlink:type="simple">10.1037/h0054032</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Desmond</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>J</given-names></name>. <article-title>Adaptive timing in neural networks: The conditioned response</article-title>. <source>Biological Cybernetics</source>. <year>1988</year>;<volume>58</volume>:<fpage>405</fpage>–<lpage>415</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00361347" xlink:type="simple">10.1007/BF00361347</ext-link></comment> <object-id pub-id-type="pmid">3395634</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Grossberg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schmajuk</surname> <given-names>NA</given-names></name>. <article-title>Neural dynamics of adaptive timing and temporal discrimination during associative learning</article-title>. <source>Neural Networks</source>. <year>1989</year>;<volume>2</volume>:<fpage>79</fpage>–<lpage>102</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0893-6080(89)90026-9" xlink:type="simple">10.1016/0893-6080(89)90026-9</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Williams</surname> <given-names>BA</given-names></name>. <article-title>Conditioned reinforcement: Experimental and theoretical issues</article-title>. <source>The Behavior Analyst</source>. <year>1994</year>;<volume>17</volume>:<fpage>261</fpage>–<lpage>285</lpage>. <object-id pub-id-type="pmid">22478192</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>. <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source>. <year>1997</year>;<volume>275</volume>:<fpage>1593</fpage>–<lpage>1599</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.275.5306.1593" xlink:type="simple">10.1126/science.275.5306.1593</ext-link></comment> <object-id pub-id-type="pmid">9054347</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <source>Reinforcement Learning: An Introduction</source>. <publisher-name>MIT Press</publisher-name>; <year>1998</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Courville</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Touretzky</surname> <given-names>DS</given-names></name>. <article-title>Representation and timing in theories of the dopamine system</article-title>. <source>Neural Computation</source>. <year>2006</year>;<volume>18</volume>:<fpage>1637</fpage>–<lpage>1677</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2006.18.7.1637" xlink:type="simple">10.1162/neco.2006.18.7.1637</ext-link></comment> <object-id pub-id-type="pmid">16764517</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref051">
<label>51</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ludvig</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Kehoe</surname> <given-names>EJ</given-names></name>. <article-title>Stimulus representation and the timing of reward-prediction errors in models of the dopamine system</article-title>. <source>Neural Computation</source>. <year>2008</year>;<volume>20</volume>:<fpage>3034</fpage>–<lpage>3054</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2008.11-07-654" xlink:type="simple">10.1162/neco.2008.11-07-654</ext-link></comment> <object-id pub-id-type="pmid">18624657</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref052">
<label>52</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Moustafa</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Ludvig</surname> <given-names>EA</given-names></name>. <article-title>Time representation in reinforcement learning models of the basal ganglia</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2013</year>;<volume>7</volume>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref053">
<label>53</label>
<mixed-citation xlink:type="simple" publication-type="other">Dearden R, Friedman N, Russell S. Bayesian Q-learning. In: Proceedings of the AAAI; 1998. p. 761–768.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref054">
<label>54</label>
<mixed-citation xlink:type="simple" publication-type="other">Engel Y, Mannor S, Meir R. Bayes meets Bellman: The Gaussian process approach to temporal difference learning. In: International Conference on Machine Learning. vol. 20; 2003.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref055">
<label>55</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Denniston</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Savastano</surname> <given-names>HI</given-names></name>, <name name-style="western"><surname>Blaisdell</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>RR</given-names></name>. <article-title>Cue competition as a retrieval deficit</article-title>. <source>Learning and Motivation</source>. <year>2003</year>;<volume>34</volume>:<fpage>1</fpage>–<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0023-9690(02)00505-2" xlink:type="simple">10.1016/S0023-9690(02)00505-2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref056">
<label>56</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rashotte</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Griffin</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Sisk</surname> <given-names>CL</given-names></name>. <article-title>Second-order conditioning of the pigeon’s keypeck</article-title>. <source>Animal Learning &amp; Behavior</source>. <year>1977</year>;<volume>5</volume>:<fpage>25</fpage>–<lpage>38</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/BF03209127" xlink:type="simple">10.3758/BF03209127</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref057">
<label>57</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Leyland</surname> <given-names>CM</given-names></name>. <article-title>Higher order autoshaping</article-title>. <source>The Quarterly Journal of Experimental Psychology</source>. <year>1977</year>;<volume>29</volume>:<fpage>607</fpage>–<lpage>619</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/14640747708400636" xlink:type="simple">10.1080/14640747708400636</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref058">
<label>58</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>. <article-title>Aspects of the reinforcer learned in second-order Pavlovian conditioning</article-title>. <source>Journal of Experimental Psychology: Animal Behavior processes</source>. <year>1979</year>;<volume>5</volume>(<issue>1</issue>):<fpage>79</fpage>–<lpage>95</lpage>. <object-id pub-id-type="pmid">528880</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref059">
<label>59</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nairne</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>. <article-title>Second-order conditioning with diffuse auditory reinforcers in the pigeon</article-title>. <source>Learning and Motivation</source>. <year>1981</year>;<volume>12</volume>:<fpage>65</fpage>–<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0023-9690(81)90025-4" xlink:type="simple">10.1016/0023-9690(81)90025-4</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref060">
<label>60</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Holland</surname> <given-names>PC</given-names></name>, <name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>. <article-title>Second-order conditioning with food unconditioned stimulus</article-title>. <source>Journal of Comparative and Physiological Psychology</source>. <year>1975</year>;<volume>88</volume>:<fpage>459</fpage>–<lpage>467</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0076219" xlink:type="simple">10.1037/h0076219</ext-link></comment> <object-id pub-id-type="pmid">1120816</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref061">
<label>61</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gibbs</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Kehoe</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Gormezano</surname> <given-names>I</given-names></name>. <article-title>Conditioning of the rabbit’s nictitating membrane response to a CSA-CSB-US serial compound: Manipulations of CSB’s associative character</article-title>. <source>Journal of Experimental Psychology: Animal Behavior Processes</source>. <year>1991</year>;<volume>17</volume>:<fpage>423</fpage>–<lpage>432</lpage>. <object-id pub-id-type="pmid">1744596</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref062">
<label>62</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Shevill</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hall</surname> <given-names>G</given-names></name>. <article-title>Retrospective revaluation effects in the conditioned suppression procedure</article-title>. <source>Quarterly Journal of Experimental Psychology Section B</source>. <year>2004</year>;<volume>57</volume>:<fpage>331</fpage>–<lpage>347</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref063">
<label>63</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Egger</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>NE</given-names></name>. <article-title>Secondary reinforcement in rats as a function of information value and reliability of the stimulus</article-title>. <source>Journal of Experimental Psychology</source>. <year>1962</year>;<volume>64</volume>(<issue>2</issue>):<fpage>97</fpage>–<lpage>104</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0040364" xlink:type="simple">10.1037/h0040364</ext-link></comment> <object-id pub-id-type="pmid">13889429</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref064">
<label>64</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Schmajuk</surname> <given-names>NA</given-names></name>. <source>Mechanisms in Classical Conditioning: A Computational Approach</source>. <publisher-name>Cambridge University Press</publisher-name>; <year>2010</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref065">
<label>65</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pearce</surname> <given-names>JM</given-names></name>. <article-title>A model for stimulus generalization in Pavlovian conditioning</article-title>. <source>Psychological Review</source>. <year>1987</year>;<volume>94</volume>:<fpage>61</fpage>–<lpage>73</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.94.1.61" xlink:type="simple">10.1037/0033-295X.94.1.61</ext-link></comment> <object-id pub-id-type="pmid">3823305</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref066">
<label>66</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>O’Reilly</surname> <given-names>JX</given-names></name>, <name name-style="western"><surname>Jbabdi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TE</given-names></name>. <article-title>How can a Bayesian approach inform neuroscience?</article-title> <source>European Journal of Neuroscience</source>. <year>2012</year>;<volume>35</volume>:<fpage>1169</fpage>–<lpage>1179</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1460-9568.2012.08010.x" xlink:type="simple">10.1111/j.1460-9568.2012.08010.x</ext-link></comment> <object-id pub-id-type="pmid">22487045</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref067">
<label>67</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Mussa-Ivaldi</surname> <given-names>S</given-names></name>. <source>Biological Learning and Control: How the Brain Builds Representations, Predicts Events, and Makes Decisions</source>. <publisher-name>MIT Press</publisher-name>; <year>2012</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004567.ref068">
<label>68</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Swan</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Pearce</surname> <given-names>JM</given-names></name>. <article-title>The orienting response as an index of stimulus associability in rats</article-title>. <source>Journal of Experimental Psychology: Animal Behavior Processes</source>. <year>1988</year>;<volume>14</volume>:<fpage>292</fpage>–<lpage>301</lpage>. <object-id pub-id-type="pmid">3404083</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref069">
<label>69</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nassar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Heasly</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>. <article-title>An approximately Bayesian delta-rule model explains the dynamics of belief updating in a changing environment</article-title>. <source>The Journal of Neuroscience</source>. <year>2010</year>;<volume>30</volume>:<fpage>12366</fpage>–<lpage>12378</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0822-10.2010" xlink:type="simple">10.1523/JNEUROSCI.0822-10.2010</ext-link></comment> <object-id pub-id-type="pmid">20844132</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref070">
<label>70</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mathys</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>. <article-title>A Bayesian foundation for individual learning under uncertainty</article-title>. <source>Frontiers in Human Neuroscience</source>. <year>2011</year>;<volume>5</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnhum.2011.00039" xlink:type="simple">10.3389/fnhum.2011.00039</ext-link></comment> <object-id pub-id-type="pmid">21629826</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref071">
<label>71</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Learning latent structure: carving nature at its joints</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2010</year>;<volume>20</volume>:<fpage>251</fpage>–<lpage>256</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2010.02.008" xlink:type="simple">10.1016/j.conb.2010.02.008</ext-link></comment> <object-id pub-id-type="pmid">20227271</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref072">
<label>72</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pearce</surname> <given-names>JM</given-names></name>. <article-title>Similarity and discrimination: a selective review and a connectionist model</article-title>. <source>Psychological Review</source>. <year>1994</year>;<volume>101</volume>:<fpage>587</fpage>–<lpage>607</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.101.4.587" xlink:type="simple">10.1037/0033-295X.101.4.587</ext-link></comment> <object-id pub-id-type="pmid">7984708</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref073">
<label>73</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Rudy</surname> <given-names>JW</given-names></name>. <article-title>Conjunctive representations in learning and memory: principles of cortical and hippocampal function</article-title>. <source>Psychological Review</source>. <year>2001</year>;<volume>108</volume>:<fpage>311</fpage>–<lpage>345</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.108.2.311" xlink:type="simple">10.1037/0033-295X.108.2.311</ext-link></comment> <object-id pub-id-type="pmid">11381832</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref074">
<label>74</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Jensen</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kurth-Nelson</surname> <given-names>Z</given-names></name>. <article-title>Reconciling reinforcement learning models with behavioral extinction and renewal: Implications for addiction, relapse, and problem gambling</article-title>. <source>Psychological Review</source>. <year>2007</year>;<volume>114</volume>:<fpage>784</fpage>–<lpage>805</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.114.3.784" xlink:type="simple">10.1037/0033-295X.114.3.784</ext-link></comment> <object-id pub-id-type="pmid">17638506</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref075">
<label>75</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Soto</surname> <given-names>FA</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Explaining Compound Generalization in Associative and Causal Learning Through Rational Principles of Dimensional Generalization</article-title>. <source>Psychological Review</source>. <year>2014</year>;<volume>121</volume>:<fpage>526</fpage>–<lpage>558</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0037018" xlink:type="simple">10.1037/a0037018</ext-link></comment> <object-id pub-id-type="pmid">25090430</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref076">
<label>76</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Park</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sandberg</surname> <given-names>IW</given-names></name>. <article-title>Universal approximation using radial-basis-function networks</article-title>. <source>Neural Computation</source>. <year>1991</year>;<volume>3</volume>:<fpage>246</fpage>–<lpage>257</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1991.3.2.246" xlink:type="simple">10.1162/neco.1991.3.2.246</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref077">
<label>77</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Joel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Ruppin</surname> <given-names>E</given-names></name>. <article-title>Actor-critic models of the basal ganglia: New anatomical and computational perspectives</article-title>. <source>Neural Networks</source>. <year>2002</year>;<volume>15</volume>:<fpage>535</fpage>–<lpage>547</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0893-6080(02)00047-3" xlink:type="simple">10.1016/S0893-6080(02)00047-3</ext-link></comment> <object-id pub-id-type="pmid">12371510</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004567.ref078">
<label>78</label>
<mixed-citation xlink:type="simple" publication-type="other">Ghavamzadeh M, Engel Y. Bayesian actor-critic algorithms. In: Proceedings of the 24th international conference on Machine learning. ACM; 2007. p. 297–304.</mixed-citation>
</ref>
</ref-list>
</back>
</article>