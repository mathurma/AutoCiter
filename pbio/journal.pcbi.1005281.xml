<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005281</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-01110</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Geometry</subject><subj-group><subject>Ellipses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Geometry</subject><subj-group><subject>Tangents</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Accuracy Maximization Analysis for Sensory-Perceptual Tasks: Computational Improvements, Filter Robustness, and Coding Advantages for Scaled Additive Noise</article-title>
<alt-title alt-title-type="running-head">Accuracy Maximization Analysis for Sensory-Perceptual Tasks</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Burge</surname>
<given-names>Johannes</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7689-2557</contrib-id>
<name name-style="western">
<surname>Jaini</surname>
<given-names>Priyank</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Psychology, University of Pennsylvania, Philadelphia, PA, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Neuroscience Graduate Group, University of Pennsylvania, Philadelphia, PA, United States of America</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Kording</surname>
<given-names>Konrad P.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Northwestern University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"> <list-item><p><bold>Conceived and designed the experiments:</bold> JB.</p></list-item> <list-item><p><bold>Performed the experiments:</bold> JB PJ.</p></list-item> <list-item><p><bold>Analyzed the data:</bold> JB PJ.</p></list-item> <list-item><p><bold>Contributed reagents/materials/analysis tools:</bold> JB PJ.</p></list-item> <list-item><p><bold>Wrote the paper:</bold> JB PJ.</p></list-item></list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">jburge@sas.upenn.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>8</day>
<month>2</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<month>2</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>2</issue>
<elocation-id>e1005281</elocation-id>
<history>
<date date-type="received">
<day>10</day>
<month>7</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>4</day>
<month>12</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Burge, Jaini</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005281"/>
<abstract>
<p>Accuracy Maximization Analysis (AMA) is a recently developed Bayesian ideal observer method for task-specific dimensionality reduction. Given a training set of proximal stimuli (e.g. retinal images), a response noise model, and a cost function, AMA returns the filters (i.e. receptive fields) that extract the most useful stimulus features for estimating a user-specified latent variable from those stimuli. Here, we first contribute two technical advances that significantly reduce AMA’s compute time: we derive gradients of cost functions for which two popular estimators are appropriate, and we implement a stochastic gradient descent (AMA-SGD) routine for filter learning. Next, we show how the method can be used to simultaneously probe the impact on neural encoding of natural stimulus variability, the prior over the latent variable, noise power, and the choice of cost function. Then, we examine the geometry of AMA’s unique combination of properties that distinguish it from better-known statistical methods. Using binocular disparity estimation as a concrete test case, we develop insights that have general implications for understanding neural encoding and decoding in a broad class of fundamental sensory-perceptual tasks connected to the energy model. Specifically, we find that non-orthogonal (partially redundant) filters with scaled additive noise tend to outperform orthogonal filters with constant additive noise; non-orthogonal filters and scaled additive noise can interact to sculpt noise-induced stimulus encoding uncertainty to match task-irrelevant stimulus variability. Thus, we show that some properties of neural response thought to be biophysical nuisances can confer coding advantages to neural systems. Finally, we speculate that, if repurposed for the problem of neural systems identification, AMA may be able to overcome a fundamental limitation of standard subunit model estimation. As natural stimuli become more widely used in the study of psychophysical and neurophysiological performance, we expect that task-specific methods for feature learning like AMA will become increasingly important.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>In psychophysics and neurophysiology, the stimulus features that are manipulated in experiments are often selected based on intuition, trial-and-error, and historical precedence. Accuracy Maximization Analysis (AMA) is a Bayesian ideal observer method for determining the task-relevant features (i.e. filters) from natural stimuli that nervous systems <italic>should</italic> select for. In other words, AMA is a method for finding optimal receptive fields for specific tasks. Early results suggest that this method has the potential to be of fundamental importance to neuroscience and perception science. First, we develop AMA-SGD, a new version of AMA that significantly reduces filter-learning time, and use it to learn optimal filters for the classic task of binocular disparity estimation. Then, we find that measureable, task-relevant properties of natural stimuli are the most important determinants of the optimal filters; changes to the prior, cost function, and internal noise have little effect on the filters. Last, we demonstrate that some ubiquitous properties of neural systems, generally thought to be biophysical nuisances, can actually improve the fidelity of neural codes. In particular, we show for the first time that scaled additive noise and redundant (non-orthogonal) filters can interact to sculpt uncertainty due to internal noise to match task-irrelevant natural stimulus variability.</p>
</abstract>
<funding-group>
<funding-statement>The authors received no specific funding for this work.</funding-statement>
</funding-group>
<counts>
<fig-count count="16"/>
<table-count count="0"/>
<page-count count="32"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data analyzed in this paper are available from the cited researchers.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Perception science seeks to determine how perceiving organisms estimate behaviorally relevant properties of the environment based on proximal stimuli captured by the senses. Understanding the details of the sensory-perceptual processing that support these abilities with natural stimuli is a primary focus of research. It is widely appreciated that some stimulus features are more useful for some tasks than others, more likely to increase a given neuron’s response rate than others, and more likely to excite neurons in one brain area than another. This specificity suggests that perceptual and neural performance in particular tasks is driven by sets of features that are of much lower dimensionality than the proximal stimuli themselves. As a consequence, methods for reducing stimulus dimensionality are in widespread use in perception and neuroscience research.</p>
<p>Models of information encoding with natural stimuli are often developed without regard to what information will be decoded from the encoded signals. Efficient coding, and many statistical methods for data characterization (e.g. PCA, ICA), are designed to capture statistical properties of proximal (observable) stimuli without explicit consideration of the sensory-perceptual or behavioral goals for which the encoded information will be used [<xref ref-type="bibr" rid="pcbi.1005281.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref002">2</xref>] [<xref ref-type="bibr" rid="pcbi.1005281.ref003">3</xref>–<xref ref-type="bibr" rid="pcbi.1005281.ref009">9</xref>]. The efficient coding hypothesis has been remarkably influential. However, as Simoncelli &amp; Olshausen (2001) point out, the hypothesis “states only that information must be represented efficiently; it does not say anything about what information should be represented” [<xref ref-type="bibr" rid="pcbi.1005281.ref007">7</xref>]. Empirical studies in psychophysics and systems neuroscience often focus on the behavioral limits and neurophysiological underpinnings of performance in specific tasks [<xref ref-type="bibr" rid="pcbi.1005281.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1005281.ref021">21</xref>]. Thus, there is a partial disconnect between popular task-independent theories of encoding (e.g. efficient coding) and the methodological practices often followed by psychophysics and sensory and systems neuroscience.</p>
<p>Accuracy Maximization Analysis (AMA) provides a principled, data-driven approach to finding the stimulus features that are most useful for specific tasks (e.g. estimation of a variable latent in the stimulus) [<xref ref-type="bibr" rid="pcbi.1005281.ref022">22</xref>]. AMA thus addresses a need that is not directly addressed by standard efficient encoding frameworks. In conjunction with carefully calibrated natural image databases [<xref ref-type="bibr" rid="pcbi.1005281.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1005281.ref026">26</xref>], AMA has provided predictions for the encoding filters (receptive fields) that support optimal performance in several fundamental tasks in early vision [<xref ref-type="bibr" rid="pcbi.1005281.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1005281.ref024">24</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref026">26</xref>]. These receptive fields have, in turn, aided the development of ideal observers for the estimation of figure-ground, defocus blur, binocular disparity, retinal speed, and motion-in-depth [<xref ref-type="bibr" rid="pcbi.1005281.ref021">21</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref024">24</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref027">27</xref>]. The predictions of these ideal observers are biologically plausible, dovetail with available neurophysiological data, and can tightly predict human performance with natural and artificial stimuli [<xref ref-type="bibr" rid="pcbi.1005281.ref021">21</xref>]. These results may represent the beginnings of an important step forward in our ability, as a science, to develop ideal observer theories of mid-level visual tasks that act directly on natural retinal images.</p>
<p>AMA does not come without a set of constraints and disadvantages. The most important constraint is that the stimuli must be contrast normalized before processing. This constraint is appropriate for many perceptual tasks for which the task-relevant information is contained in the pattern of contrast over space and time, but it renders the method ill-suited for tasks in which the primary source of information is contained in the magnitude (intensity) of a stimulus. Second, the AMA cost landscape is non-convex, so guarantees cannot be made that local minima found by the method represent the global minimum; standard techniques for protecting against non-global local minima must be used (e.g. random starts). However, for the set of problems for which AMA is well-suited, its most glaring disadvantage is its computational cost: compute time is quadratic in the number of elements in the training set. Without specialized computing resources, the computational cost renders the method impractical for use on large-scale problems.</p>
<p>The aims of this paper are four-fold. First, to set our contribution in context, we re-derive the original equations for AMA [<xref ref-type="bibr" rid="pcbi.1005281.ref022">22</xref>], developing intuitions along the way. Second, we derive the gradient of the cost (objective function) for two popular cost functions—0,1 cost (L0 norm) and squared error cost (L2 norm)—and implement a stochastic gradient descent procedure for filter learning, which we call AMA-SGD. (source code at: <ext-link ext-link-type="uri" xlink:href="http://www.github.com/BurgeLab/AMA" xlink:type="simple">http://www.github.com/BurgeLab/AMA</ext-link>). These advances significantly reduce the method’s compute time, thereby rendering it a more practical tool for research on problems of wide spread interest in vision research and sensory and systems neuroscience. Third, we show that AMA can be used to examine the relative impact on optimal coding of stimulus variability and priors over the latent variable. Fourth, we show how scaled additive encoding noise (i.e. additive noise with response variance proportional to the response mean) and correlated (i.e. non-orthogonal) filters can interact to confer coding advantages in certain tasks. We believe that the work presented here may help establish a general normative framework for understanding the diversity of tuning and response properties exhibited by neurophysiological receptive fields in cortex, and how they may contribute to task-specific processing of sensory stimuli.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Methods</title>
<p>In this section, we first review the derivation of the main equations for Accuracy Maximization Analysis [<xref ref-type="bibr" rid="pcbi.1005281.ref022">22</xref>], explaining the logic and geometric intuitions behind the method. This review is meant to provide context for the current paper and a tutorial on the original method. Second, we derive the gradient of the cost function with respect to the filters for two popular cost functions. Third, we develop a constrained batch stochastic gradient descent algorithm for filter learning, and provide recommendations to users for best practices.</p>
<sec id="sec003">
<title>Background and Setup</title>
<p>Accuracy Maximization Analysis (AMA) provides a closed-form expression for the optimal (nonlinear) decoding rule given five factors: i) a well-defined task (i.e. a latent variable to estimate from high-dimensional stimuli), ii) a labeled training set of stimuli, iii) a particular set of filters (receptive fields), iv) a noisy filter response model, and v) a cost function (<xref ref-type="fig" rid="pcbi.1005281.g001">Fig 1A</xref>). Given these factors, the problem of finding the encoding filters that are optimal for a particular task reduces to searching for the filters that minimize the cost (<xref ref-type="fig" rid="pcbi.1005281.g001">Fig 1B</xref>). The Background and Setup section is ordered to follow the block diagram in <xref ref-type="fig" rid="pcbi.1005281.g001">Fig 1A</xref>.</p>
<fig id="pcbi.1005281.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The logic of Accuracy Maximization Analysis.</title>
<p><bold>A</bold> Factors that determine the optimal non-linear decoder, <italic>g</italic>(⋅). For any particular filter set, the optimal decoder provides a closed form expression for the cost by i) computing the posterior probability over the latent variable <italic>p</italic>(<italic>X</italic>|<bold>R</bold>), and ii) reading out the optimal estimate <inline-formula id="pcbi.1005281.e001"><alternatives><graphic id="pcbi.1005281.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> from the posterior that minimizes the cost. <bold>B</bold> AMA begins with a labeled training set. Each individual stimulus in the training set, <bold>s</bold><sub><italic>ij</italic></sub>, must be labeled with a particular value of the latent variable of interest, <italic>X</italic><sub><italic>i</italic></sub>. The labeling of the training set implicitly defines the task. Subsequent steps to finding optimal task-specific filters via AMA are: i) select a particular stimulus <bold>s</bold><sub><italic>kl</italic></sub> from the labeled training set; ii) obtain a noisy filter response <bold>R</bold><sub><italic>kl</italic></sub> from a given (possibly non-optimal) set of initial filters; iii) use the optimal non-linear decoder to obtain the optimal estimate and its expected cost; iv) repeat for each stimulus in the training set and compute the average cost across the training set; v) update the filters to reduce the cost; vi) repeat until the average cost across the training set is minimized. The filters that minimize the cost are the optimal task-specific filters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g001" xlink:type="simple"/>
</fig>
<sec id="sec004">
<title>Specifying the task with a labeled training set</title>
<p>Accuracy Maximization Analysis requires a training set. Each stimulus in the training set is labeled by a value of the latent variable to be estimated. The task is implicitly defined by the labeling of the training set. If the training set is too small, or if the stimuli contained within the training set are not representative, results obtained via AMA may generalize poorly. The task-specific filters learned via AMA are therefore only as solid as the training set itself. Thus, the first (and often quite difficult) step in the fruitful use of AMA is to obtain labeled training sets that are accurate, and are sufficiently large to be representative of the general case.</p>
<p>The training set and the latent variable labels define the task and specify the joint probability distribution <italic>p</italic>(<italic>X</italic>,<bold>s</bold>) between the latent variable and the stimuli (<xref ref-type="fig" rid="pcbi.1005281.g001">Fig 1B</xref>). Thus, the training set implicitly defines the prior probability distribution over the latent variable, which can be obtained by marginalizing out the stimuli from the joint distribution: <inline-formula id="pcbi.1005281.e002"><alternatives><graphic id="pcbi.1005281.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi mathvariant="bold">s</mml:mi></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula>. If AMA is being used to make normative prescriptions for the design of biological and/or machine vision systems, it is of potential interest to examine the influence of the prior on the encoding functions, and on eventual performance in the task. The experimenter has at least two options in this regard.</p>
<p>First, the experimenter can attempt to match the prior probability distribution in the training set to the prior probability of occurrence in natural viewing conditions. Unfortunately, accurate measurements of prior probability distributions relevant to particular perceptual tasks have proved notoriously difficult to obtain, especially if the latent variable of interest is i) a property of the distal environment (e.g. depth, object motion, surface reflectance), or ii) a property of the relationship between the environment and the vision system (e.g. distance, focus error, binocular disparity, retinal image motion). Progress has been made in recent years [<xref ref-type="bibr" rid="pcbi.1005281.ref026">26</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1005281.ref030">30</xref>], but with this approach comes significant technical challenges.</p>
<p>Second, the experimenter can manipulate the prior probability distribution over the latent variable by varying the number of stimuli per latent variable value in the training set. This approach is simple (in comparison to the first approach) and provides the experimenter a useful tool for examining the influence of the prior on the properties of the optimal filters <xref ref-type="fig" rid="pcbi.1005281.g002">Fig 2</xref>). If the optimal filters are brittle—that is, if they are very sensitive to modest variations in the shape of the prior—then the effort required by the first approach may be justified. On the other hand, if the optimal filters are insensitive to reasonable variations in the prior, then the prior can be safely ignored [<xref ref-type="bibr" rid="pcbi.1005281.ref024">24</xref>]. In general, the better the information in the proximal stimuli about the latent variable (the more reliable the measurements), the less important will become the prior.</p>
<fig id="pcbi.1005281.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g002</object-id>
<label>Fig 2</label>
<caption>
<title>The training set implicitly represents the prior probability distribution over the latent variable to be estimated.</title>
<p>Different prior probability distributions can be represented by varying the number of stimuli in the training set at each level of the latent variable. <bold>A</bold> Flat prior probability distribution over the range of represented latent variable values. <bold>B</bold> Prior probability with more mass at the central value of the latent variable. <bold>C</bold> Prior probability with less mass at the central value of the latent variable. By manipulating the number of training set stimuli as a function of the latent variable, the effect of the prior can be examined on the optimal task-specific encoding filters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Filter response model</title>
<p>The response model specifies how a particular filter <bold>f</bold> responds to an arbitrary stimulus <bold>s</bold>, thereby providing the conditional probability <italic>p</italic>(<italic>R</italic>|<bold>s</bold>) of a noisy filter response <italic>R</italic> to an arbitrary proximal stimulus <bold>s</bold> (see <xref ref-type="fig" rid="pcbi.1005281.g001">Fig 1B</xref>). Given that our specific interest is to understand task-specific information processing in sensory-perceptual tasks, it is advantageous for the encoding model to be consistent with the properties of biological encoders (i.e. receptive fields). Here, we consider a Gaussian response model with scaled additive (i.e. Poisson-like) noise[<xref ref-type="bibr" rid="pcbi.1005281.ref022">22</xref>]. We chose this response model for two reasons. First, its Gaussian form is mathematically convenient. Second, and more importantly, scaled additive noise is a widely accepted simple model of neural noise in early visual cortex. However, the filter response model can in principle be modified to be consistent with other noise models (e.g. Poisson, Bernoulli).</p>
<p>For a given encoding filter <bold>f</bold><sub><italic>t</italic></sub> from set of filters <bold>f</bold> = [<bold>f</bold><sub>1</sub> <bold>f</bold><sub>2</sub> ⋯ <bold>f</bold><sub><italic>q</italic></sub>], its mean response <italic>r</italic><sub><italic>t</italic></sub>, noisy response <italic>R</italic><sub><italic>t</italic></sub>, noise samples <italic>η</italic>, and response noise variance <inline-formula id="pcbi.1005281.e003"><alternatives><graphic id="pcbi.1005281.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> to stimulus <italic>j</italic> having latent variable level <italic>i</italic> are given by
<disp-formula id="pcbi.1005281.e004">
<alternatives>
<graphic id="pcbi.1005281.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">f</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow>
</mml:math>
</alternatives>
<label>(1a)</label>
</disp-formula>
<disp-formula id="pcbi.1005281.e005">
<alternatives>
<graphic id="pcbi.1005281.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi></mml:mrow>
</mml:math>
</alternatives>
<label>(1b)</label>
</disp-formula>
<disp-formula id="pcbi.1005281.e006">
<alternatives>
<graphic id="pcbi.1005281.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mrow><mml:mi>η</mml:mi><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>)</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(1c)</label>
</disp-formula>
<disp-formula id="pcbi.1005281.e007">
<alternatives>
<graphic id="pcbi.1005281.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow>
</mml:math>
</alternatives>
<label>(1d)</label>
</disp-formula>
where <inline-formula id="pcbi.1005281.e008"><alternatives><graphic id="pcbi.1005281.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>‖</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> is a mean-subtracted, contrast normalized (‖<bold>s</bold>‖ = 1.0) version of a (possibly noisy) intensity stimulus <bold>x</bold><sub><italic>ij</italic></sub>, <bold>f</bold><sub><italic>t</italic></sub> is a vector of encoding weights constrained to have a magnitude of 1.0 (‖<bold>f</bold><sub><italic>t</italic></sub>‖ = 1.0), <italic>η</italic> is a sample of zero-mean Gaussian noise with variance <inline-formula id="pcbi.1005281.e009"><alternatives><graphic id="pcbi.1005281.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. In the general case, the noise variance is given by a linear function of the mean response with fano-factor <italic>α</italic> and baseline variance <inline-formula id="pcbi.1005281.e010"><alternatives><graphic id="pcbi.1005281.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. When the fano-factor equals 0.0, the noise model is additive and the response variance is a constant, regardless of the mean response. When the fano-factor is non-zero, response noise variance increases approximately in proportion to the mean response. For the results presented in the paper, we set the fano-factor equal to 1.36 and the baseline variance equal to 0.23 (spk/sec)<sup>2</sup>, values that are consistent with neural response properties in early visual cortex [<xref ref-type="bibr" rid="pcbi.1005281.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref031">31</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref032">32</xref>]. If <italic>N</italic><sub><italic>q</italic></sub> filters are considered simultaneously, the variables in Eqs <xref ref-type="disp-formula" rid="pcbi.1005281.e004">1a</xref>–<xref ref-type="disp-formula" rid="pcbi.1005281.e007">1d</xref> become vectors—mean response vector <bold>r</bold> = [<italic>r</italic><sub>1</sub> <italic>r</italic><sub>2</sub> ⋯ <italic>r</italic><sub><italic>q</italic></sub>], noisy response vector <bold>R</bold> = [<italic>R</italic><sub>1</sub> <italic>R</italic><sub>2</sub> ⋯ <italic>R</italic><sub><italic>q</italic></sub>], and response covariance matrix ∑ with on-diagonal elements <inline-formula id="pcbi.1005281.e011"><alternatives><graphic id="pcbi.1005281.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>q</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>—and the filter response distribution <italic>p</italic>(<bold>R</bold>|<bold>s</bold><sub><italic>ij</italic></sub>) becomes <italic>N</italic><sub><italic>q</italic></sub> dimensional. In this manuscript, we consider independent response noise (diagonal covariance matrix), but the impact of correlated response noise could also be examined.</p>
</sec>
<sec id="sec006">
<title>Bayes Optimal Decoder: Posterior Probability Distribution and Cost of Optimal Estimator</title>
<p>The optimal decoder provides a closed form expression for the cost for any particular filter set given the training stimuli. The decoder determines the cost by first computing the posterior probability over the latent variable <italic>p</italic>(<italic>X</italic>|<bold>R</bold>), and then reading out the optimal estimate <inline-formula id="pcbi.1005281.e012"><alternatives><graphic id="pcbi.1005281.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> from the posterior that minimizes the cost. Here, following Geisler et al (2009), we present the derivation of the posterior probability of the latent variable <italic>X</italic> in a labeled training set given the responses of a noisy set of encoders (i.e. filters) to a given stimulus <bold>s</bold><sub><italic>kl</italic></sub> with latent variable value <italic>X</italic><sub><italic>k</italic></sub>
<disp-formula id="pcbi.1005281.e013">
<alternatives>
<graphic id="pcbi.1005281.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e013" xlink:type="simple"/>
<mml:math display="block" id="M13">
<mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>v</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula></p>
<p>The conditional probability of the encoder response given can be expressed as <inline-formula id="pcbi.1005281.e014"><alternatives><graphic id="pcbi.1005281.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula> where <italic>p</italic>(<bold>R</bold>|<bold>s</bold>) is defined by Eqs <xref ref-type="disp-formula" rid="pcbi.1005281.e004">1a</xref>–<xref ref-type="disp-formula" rid="pcbi.1005281.e007">1d</xref>. Plugging in
<disp-formula id="pcbi.1005281.e015">
<alternatives>
<graphic id="pcbi.1005281.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e015" xlink:type="simple"/>
<mml:math display="block" id="M15">
<mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle><mml:mo>]</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>v</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle><mml:mo>]</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula></p>
<p>Next, note the prior probability <italic>p</italic>(<italic>X</italic><sub><italic>i</italic></sub>) is known, and the conditional probability of a particular stimulus given a level <italic>p</italic>(<bold>s</bold><sub><italic>ij</italic></sub>|<italic>X</italic><sub><italic>i</italic></sub>) is also known (because these quantities are determined by the training set). Specifically, the prior probability of each latent variable value <italic>p</italic>(<italic>X</italic><sub><italic>i</italic></sub>) is the number of stimuli having that label over the total number of stimuli in the training set <italic>N</italic><sub><italic>i</italic></sub>/<italic>N</italic>. The probability of each stimulus, conditioned on its latent variable value <italic>X</italic><sub><italic>i</italic></sub> is 1/<italic>N</italic><sub><italic>i</italic></sub> where <italic>N</italic><sub><italic>i</italic></sub> is the number of stimuli with that label in the training set. Substituting
<disp-formula id="pcbi.1005281.e016">
<alternatives>
<graphic id="pcbi.1005281.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e016" xlink:type="simple"/>
<mml:math display="block" id="M16">
<mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle><mml:mo>]</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>v</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle><mml:mo>]</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula></p>
<p>Canceling terms yields the relatively simple expression for the posterior probability
<disp-formula id="pcbi.1005281.e017">
<alternatives>
<graphic id="pcbi.1005281.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e017" xlink:type="simple"/>
<mml:math display="block" id="M17">
<mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>v</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula></p>
<p><xref ref-type="disp-formula" rid="pcbi.1005281.e017">Eq 5</xref> indicates that the posterior probability is given by the sum of the within-level stimulus likelihoods, normalized by the sum of all stimulus likelihoods. <xref ref-type="fig" rid="pcbi.1005281.g003">Fig 3A</xref> provides a graphical representation of AMA posterior, for a simple hypothetical case in which there is one filter and two latent variable values, each with two stimuli (i.e. four stimuli total). <xref ref-type="fig" rid="pcbi.1005281.g003">Fig 3B</xref> shows response distributions for the same hypothetical stimuli, in the slightly more complicated case in which there are two filters.</p>
<fig id="pcbi.1005281.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Relationship between filter response distributions, the likelihood, and the posterior probability.</title>
<p><bold>A</bold> Hypothetical one-dimensional conditional response distributions from a single filter (receptive field). Each distribution represents noisy filter responses to each stimulus in the training set. Blue distributions represent the filter response distributions for the two stimuli having the first latent variable value. Red distributions respresent the response distributions to the two stimuli having the second (i.e. incorrect) value. The striped blue distribution corresponds to the distribution of responses to the current stimulus <bold>s</bold><sub><italic>kl</italic></sub> which, in this case, has the first value of the latent variable. The solid blue circle represents the likelihood that a random observed response <italic>R</italic><sub>1</sub>(<italic>k</italic>,<italic>l</italic>) was elicited by stimulus <bold>s</bold><sub><italic>kl</italic></sub> the stimulus that actually elicited the response. The open blue circle represents the likelihood that the same response was elicited by stimulus <bold>s</bold><sub><italic>km</italic></sub>, the other stimulus having latent value <italic>X</italic><sub><italic>k</italic></sub>. The sum of these stimulus likelihoods represents the likelihood that the observed response was elicited by a stimulus having latent variable value, <italic>X</italic><sub><italic>k</italic></sub>. The open red circles represent the likelihoods that the observed response resulted from the two stimuli having value <italic>X</italic><sub><italic>i</italic> ≠ <italic>k</italic></sub> (i.e. from stimuli with the incorrect latent variable value). The posterior probability (<xref ref-type="disp-formula" rid="pcbi.1005281.e017">Eq 5</xref>) of the correct latent variable value (i.e. the latent variable value <italic>X</italic><sub><italic>k</italic></sub> corresponding to stimulus <bold>s</bold><sub><italic>kl</italic></sub>) is given by the sum of the likelihoods for within-level stimuli normalized by the sum of all likelihoods; the posterior probability distribution for this hypothetical case is shown in the next figure. Confusions between stimuli with the correct latent variable value increase the posterior probability of the correct level by contributing to the numerator (blue-boxed entries in the equation above figure panel). Confusions between correct and incorrect levels decrease the posterior probability of the correct level by contributing to the denominator (red-boxed entries in equation above figure panel). <bold>B</bold> Two-dimensional (i.e. two filter) case, under the assumption of independent response noise (note how the noise ellipses are aligned with the axes of response space). The second filter should help increase performance by selecting for useful stimulus features that the first filter does not.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g003" xlink:type="simple"/>
</fig>
<p>With the expression for the posterior probability distribution, the next step is to define a cost function. The cost function specifies the penalty assigned to different types of error. For certain cost functions, the optimal estimator associated with that cost function can be determined analytically (see <xref ref-type="sec" rid="sec031">Supporting Information</xref>). Here, we remain agnostic about the particular cost function to be used. Later, we derive the cost (and the gradient of the cost) associated with two popular cost functions for which the maximum a posteriori (MAP) and minimum measured squared error estimators (MMSE) are the optimal estimators.</p>
<p>The cost associated with the noisy response to an individual stimulus is
<disp-formula id="pcbi.1005281.e018">
<alternatives>
<graphic id="pcbi.1005281.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e018" xlink:type="simple"/>
<mml:math display="block" id="M18">
<mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>v</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi>γ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
where <inline-formula id="pcbi.1005281.e019"><alternatives><graphic id="pcbi.1005281.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:mi>γ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the cost associated with the difference between the estimate and the true latent variable value <italic>X</italic><sub><italic>k</italic></sub> when the estimate is the optimal estimate <inline-formula id="pcbi.1005281.e020"><alternatives><graphic id="pcbi.1005281.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> for the cost function.</p>
<p>The overall cost for a given set of filters applied to the training set data is given by the expected cost across for each stimulus averaged over all stimuli
<disp-formula id="pcbi.1005281.e021">
<alternatives>
<graphic id="pcbi.1005281.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e021" xlink:type="simple"/>
<mml:math display="block" id="M21">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>[</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="0.75em"/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
where <inline-formula id="pcbi.1005281.e022"><alternatives><graphic id="pcbi.1005281.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the expected cost associated with the <italic>kl</italic><sup>th</sup> stimulus.</p>
<p>The goal of the accuracy maximization analysis is to obtain the filters <bold>f</bold> that minimize the overall cost. Namely,
<disp-formula id="pcbi.1005281.e023">
<alternatives>
<graphic id="pcbi.1005281.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e023" xlink:type="simple"/>
<mml:math display="block" id="M23">
<mml:mrow><mml:msup><mml:mi mathvariant="bold">f</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">arg</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:munder><mml:mspace width="0.25em"/><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
where the optimal filters <bold>f</bold><sup><italic>opt</italic></sup> are the filters that minimize the expected cost across the training set. We use numerical methods to determine the optimal filters because there exists no closed form solution.</p>
<p>A schematic of the filter learning process via gradient descent is shown in <xref ref-type="fig" rid="pcbi.1005281.g004">Fig 4</xref>. It shows how the filter response distributions, the corresponding posterior probability distribution over the latent variable, and the cost evolves as the filters improve. As the filters improve, response distributions to stimuli having the same level of the latent variable become more similar, while response distributions to stimuli with different latent variable values become more dissimilar. This increases the likelihood of within-level stimulus confusions, and decreases the likelihood of between-level stimulus confusions.</p>
<fig id="pcbi.1005281.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Schematic showing the evolution of hypothetical filter response distributions, posterior probability distributions, and cost with one filter, two latent variable levels, and two stimuli per level.</title>
<p><bold>A</bold> Response distributions, posterior, and cost for intermediate filters. <bold>B</bold> Response distributions, posterior, and cost for optimal filters. Three effects are worth noting. First, as the filters evolve, response distributions to stimuli sharing the same latent variable value become more similar, and response distributions to stimuli having different latent variable values become more different. Second, as the filters improve, more posterior probability mass is concentrated at the correct latent variable value, and cost decreases commensurately. Third, the cost landscape is non-convex.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g004" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec007">
<title>AMA Cost Function: Derivation of Gradients</title>
<p>Gradient descent routines require the gradient of the cost function. The gradient must be determined numerically (e.g. finite differences) if an analytic expression is not known. The computational cost of numerically evaluating the gradient is proportional to the number of dimensions required to define the gradient itself. Methods for numerical evaluation typically proceed by taking a small step of size <italic>ε</italic> in each of <italic>N</italic><sub><italic>d</italic></sub> directions. In our case, <italic>N</italic><sub><italic>d</italic></sub> is the number of dimensions that define each filter (i.e. receptive field). For example, a filter that processes a 15x15 pixel image is defined by 225 dimensions, one dimension for each pixel. Thus, the gradient of the cost with respect to the filter is 225 dimensional. An analytical expression for the gradient can be expected to yield computational savings equal to a factor <italic>N</italic><sub><italic>d</italic></sub> less the time required to evaluate the gradient. This improvement in speed can be substantial for problems in which the stimuli and filters are relatively high dimensional.</p>
<p>Here, we derive the gradient of the cost for two popular cost functions: the 0,1 cost function (i.e. L0 norm) and the squared error cost function (i.e. L2 norm). These two cost functions are commonly used in the fields of vision research, visual neuroscience, statistics, and machine learning. They also represent opposite extremes of commonly used cost functions. The 0,1 cost function penalizes all errors equally, regardless of their magnitude. The squared error cost function penalizes small errors minimally and large errors severely. We reason that if the behavior of the algorithm is understood for these two cost functions, reasonable inferences can be made about algorithm’s behavior for intermediate cost functions (e.g. L1 norm).</p>
<p>The optimal estimator for the L0 norm cost function is the maximum a posteriori (MAP) estimator (see <xref ref-type="supplementary-material" rid="pcbi.1005281.s007">S4 Text</xref>). In the present case, the expected L0 cost across all stimuli is closely related to the Kullback-Leibler (KL) divergence between the observed posterior and an idealized posterior with all its mass at the correct level of the latent variable (<xref ref-type="supplementary-material" rid="pcbi.1005281.s008">S5 Text</xref>); for both statistics, the expected cost is a function only of the probability mass at the correct level of the latent variable. Thus, the appropriate estimator for both measures is the posterior maximum (i.e. the MAP estimator). The optimal estimator for the L2 norm cost function is the mean of the posterior probability distribution, or the minimum mean squared error (MMSE) estimator (see <xref ref-type="supplementary-material" rid="pcbi.1005281.s009">S6 Text</xref>).</p>
<sec id="sec008">
<title>Gradient of 0,1 (L0 norm) cost function</title>
<p>This expression for the 0,1 cost is closely related to the average KL-divergence between the posterior probability distribution and a hypothetical posterior probability distribution that has all its mass at the correct latent variable level (<xref ref-type="supplementary-material" rid="pcbi.1005281.s008">S5 Text</xref>). The KL-divergence cost for a noisy response to a particular stimulus is given by the negative log-posterior probability at the correct level [<xref ref-type="bibr" rid="pcbi.1005281.ref022">22</xref>]
<disp-formula id="pcbi.1005281.e024">
<alternatives>
<graphic id="pcbi.1005281.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e024" xlink:type="simple"/>
<mml:math display="block" id="M24">
<mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:mspace width="0.15em"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula></p>
<p>The expected cost across all stimuli depends on the expected cost for each individual stimulus (<xref ref-type="disp-formula" rid="pcbi.1005281.e021">Eq 7</xref>). We use the approximation log <italic>p</italic>(<italic>X</italic><sub><italic>k</italic></sub>|<bold>r</bold>(<italic>k</italic>,<italic>l</italic>)) ≅ <italic>E</italic><sub><bold>R</bold>(<italic>k</italic>,<italic>l</italic>)</sub>[log <italic>p</italic>(<italic>X</italic><sub><italic>k</italic></sub>|<bold>R</bold>(<italic>k</italic>,<italic>l</italic>))] to calculate the expected cost for each stimulus (see Appendix, [<xref ref-type="bibr" rid="pcbi.1005281.ref022">22</xref>]). Note that AMA-SGD can learn filters with noisy responses and without the approximation, but results are robust to this choice, so we use the approximation for convenience.</p>
<p>By defining <italic>Y</italic><sub><italic>k</italic></sub>(<italic>k</italic>,<italic>l</italic>) and <italic>Z</italic>(<italic>k</italic>,<italic>l</italic>) as the numerator and denominator of the posterior probability distribution given a noisy response <bold>R</bold>(<italic>k</italic>,<italic>l</italic>) to stimulus <bold>s</bold><sub><italic>kl</italic></sub> (see <xref ref-type="disp-formula" rid="pcbi.1005281.e017">Eq 5</xref>), we obtain
<disp-formula id="pcbi.1005281.e025">
<alternatives>
<graphic id="pcbi.1005281.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e025" xlink:type="simple"/>
<mml:math display="block" id="M25">
<mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">log</mml:mi><mml:msub><mml:mi>Y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:mi>Z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula></p>
<p>Taking the gradient of the cost with respect to the receptive fields <bold>f</bold> and dropping the index (<italic>k</italic>,<italic>l</italic>) for notational simplicity yields
<disp-formula id="pcbi.1005281.e026">
<alternatives>
<graphic id="pcbi.1005281.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e026" xlink:type="simple"/>
<mml:math display="block" id="M26">
<mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>Z</mml:mi></mml:mrow><mml:mi>Z</mml:mi></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula></p>
<p>Finally, plugging <xref ref-type="disp-formula" rid="pcbi.1005281.e026">Eq 11</xref> into <xref ref-type="disp-formula" rid="pcbi.1005281.e021">Eq 7</xref>, and yields the expression for the expected cost over the entire training set
<disp-formula id="pcbi.1005281.e027">
<alternatives>
<graphic id="pcbi.1005281.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e027" xlink:type="simple"/>
<mml:math display="block" id="M27">
<mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>Z</mml:mi></mml:mrow><mml:mi>Z</mml:mi></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula></p>
<p>Thus, finding the gradient of the KL-divergence cost reduces to finding the gradient of the log posterior probability, which further reduces to finding the gradient of the numerator and the gradient of the denominator of the posterior probability distribution. <xref ref-type="supplementary-material" rid="pcbi.1005281.s005">S2 Text</xref> derives the full expression for the gradient of the cost. The accuracy of the analytic expressions for the gradient have been verified by numerical evaluation using finite differences.</p>
</sec>
<sec id="sec009">
<title>Gradient of squared error (L2 norm) cost function</title>
<p>The squared error between the groundtruth value of the latent variable and the optimal estimate given a noisy response to a particular stimulus is
<disp-formula id="pcbi.1005281.e028">
<alternatives>
<graphic id="pcbi.1005281.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e028" xlink:type="simple"/>
<mml:math display="block" id="M28">
<mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula></p>
<p>The gradient of the cost is
<disp-formula id="pcbi.1005281.e029">
<alternatives>
<graphic id="pcbi.1005281.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e029" xlink:type="simple"/>
<mml:math display="block" id="M29">
<mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula></p>
<p>The optimal estimate for the squared error (i.e. L2 norm) cost function is the posterior mean (see <xref ref-type="supplementary-material" rid="pcbi.1005281.s007">S4 Text</xref>). The gradient of the optimal estimate is given by
<disp-formula id="pcbi.1005281.e030">
<alternatives>
<graphic id="pcbi.1005281.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e030" xlink:type="simple"/>
<mml:math display="block" id="M30">
<mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>v</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula></p>
<p>The gradient of the posterior probability (<xref ref-type="supplementary-material" rid="pcbi.1005281.s006">S3 Text</xref>) at each level of the latent variable is given by
<disp-formula id="pcbi.1005281.e031">
<alternatives>
<graphic id="pcbi.1005281.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e031" xlink:type="simple"/>
<mml:math display="block" id="M31">
<mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow><mml:mi>Z</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>Z</mml:mi></mml:mrow><mml:mi>Z</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(16)</label>
</disp-formula>
where <italic>Y</italic><sub><italic>u</italic></sub> and <italic>Z</italic> are the numerator and denominator of the posterior probability, as above. By substituting <xref ref-type="disp-formula" rid="pcbi.1005281.e031">Eq 16</xref> into <xref ref-type="disp-formula" rid="pcbi.1005281.e030">Eq 15</xref>, we obtain the gradient of the optimal estimate
<disp-formula id="pcbi.1005281.e032">
<alternatives>
<graphic id="pcbi.1005281.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e032" xlink:type="simple"/>
<mml:math display="block" id="M32">
<mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>v</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow><mml:mi>Z</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>Z</mml:mi></mml:mrow><mml:mi>Z</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(17)</label>
</disp-formula></p>
<p>Substituting <xref ref-type="disp-formula" rid="pcbi.1005281.e032">Eq 17</xref> into <xref ref-type="disp-formula" rid="pcbi.1005281.e029">Eq 14</xref>, using an approximation (see Appendix, [<xref ref-type="bibr" rid="pcbi.1005281.ref022">22</xref>]), substituting into <xref ref-type="disp-formula" rid="pcbi.1005281.e021">Eq 7</xref>, and taking the gradient yields the expression for the gradient of cost over the training set
<disp-formula id="pcbi.1005281.e033">
<alternatives>
<graphic id="pcbi.1005281.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e033" xlink:type="simple"/>
<mml:math display="block" id="M33">
<mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(18)</label>
</disp-formula></p>
<p>The full derivation for the gradient of the squared error cost is given in <xref ref-type="supplementary-material" rid="pcbi.1005281.s006">S3 Text</xref>. With the gradient of the cost in hand, we develop a stochastic gradient descent routine for finding the optimal filters. The accuracy of the analytic expressions for the gradient have been verified by numerical evaluation of the gradient using finite differences.</p>
</sec>
</sec>
<sec id="sec010">
<title>AMA-SGD (Accuracy Maximization Analysis with Stochastic Gradient Descent)</title>
<sec id="sec011">
<title>Motivation: reducing computational run-time</title>
<p>The primary drawback of AMA is its computational expense. The compute time associated with the evaluation of the posterior probability distribution for all stimuli in the dataset requires <italic>N</italic><sup>2</sup><italic>N</italic><sub><italic>lvl</italic></sub> operations, where <italic>N</italic> is the total number of samples in the training set and <italic>N</italic><sub><italic>lvl</italic></sub> is the number of levels (i.e. values) of the latent variable represented in the training set. For example, a training set with 10,000 stimuli and 20 categories requires 2 billion operations per evaluation of the posterior probability distribution. The required compute time is significant enough as to render the method impractical for use on large-scale problems.</p>
<p>There are at least two methods for achieving significant computational savings in optimization problems: employing stochastic gradient descent routines, and employing models with strong distributional or parametric assumptions. Each has its drawbacks. Stochastic gradient descent routines are noisy and may not converge to the optimum value when the problem is non-convex. Models with strong parametric assumptions will, in general, only be appropriate for a restricted set of cases for which the assumptions approximately hold. Both approaches, however, offer the potential benefit of drastic improvements in the speed of convergence. In this paper, we focus on stochastic gradient descent; future work will explore models with stronger parametric assumptions.</p>
<p>Stochastic gradient descent has the potential to significantly reduce compute-time when the time to evaluate the objective function increases super-linearly with the number of elements in the training set [<xref ref-type="bibr" rid="pcbi.1005281.ref033">33</xref>–<xref ref-type="bibr" rid="pcbi.1005281.ref035">35</xref>], which is the case here. The expected reduction in compute-time depends on the size of the batch relative to the size of the full training set. On each iteration, a batch of stimuli of size <italic>N</italic><sub><italic>bch</italic></sub> is selected randomly from the total number of stimuli in the training set. Let <italic>k</italic> = <italic>N</italic>/<italic>N</italic><sub><italic>bch</italic></sub> be the ratio between the size of the training set and the size of each batch. Evaluating the <italic>N</italic><sub><italic>bch</italic></sub> posterior probability distributions associated with each batch requires <inline-formula id="pcbi.1005281.e034"><alternatives><graphic id="pcbi.1005281.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>v</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> operations. On each pass through the dataset, <italic>k</italic> batches must be evaluated so that the full training set is used during filter learning. All other things equal, evaluating the cost for each pass through the full dataset is therefore of order <italic>N</italic><sub><italic>bch</italic></sub><italic>N</italic><sub><italic>lvl</italic></sub><italic>N</italic>, a factor of <italic>k</italic> faster than AMA. Thus, AMA-SGD has the potential to reduce the time required to learn filters from quadratic to linear in the number of elements in the training set.</p>
</sec>
<sec id="sec012">
<title>Updating the filters</title>
<p>The problem under consideration is a constrained optimization problem because the filters must have a vector magnitude (L2 norm) of 1.0. The geometric interpretation of this constraint is that the filters lie on a hyper-sphere of unit radius that is centered at the origin. Therefore, the direction of steepest descent that satisfies the constraint lies on the tangent plane of the hyper-sphere at the point specified by the current filter values <bold>f</bold>.</p>
<p>To determine this direction, the gradient of the cost function in the unconstrained space is first obtained, <bold>f</bold><sub><italic>euclid</italic></sub> (Eqs <xref ref-type="disp-formula" rid="pcbi.1005281.e027">12</xref> and <xref ref-type="disp-formula" rid="pcbi.1005281.e033">18</xref>). Next, the gradient in the unconstrained space is projected onto the tangent plane of the hypersphere (<xref ref-type="fig" rid="pcbi.1005281.g005">Fig 5</xref>). The gradient in the unconstrained space can be expressed as a vector sum of its component in the tangent plane and its component in a direction perpendicular to the tangent plane at the point <bold>f</bold>. On a hyper-sphere, the direction perpendicular to the tangent plane at <bold>f</bold> is the vector <bold>f</bold> itself. Hence, the projection of the gradient in this direction is (<bold>f</bold><sup><italic>T</italic></sup><bold>f</bold><sub><italic>euclid</italic></sub>)<bold>f</bold>. Therefore, from vector addition, the projection of the gradient on the tangent plane of the sphere at <bold>f</bold> is
<disp-formula id="pcbi.1005281.e035">
<alternatives>
<graphic id="pcbi.1005281.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e035" xlink:type="simple"/>
<mml:math display="block" id="M35">
<mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">f</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mi mathvariant="bold">f</mml:mi></mml:mrow>
</mml:math>
</alternatives>
<label>(19)</label>
</disp-formula></p>
<fig id="pcbi.1005281.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Using projection to satisfy the filter constraint.</title>
<p><bold>A</bold> To enforce the constraint that the filters have unit vector magnitude (i.e. ‖<bold>f</bold>‖ = 1.0), the filters are projected onto the tangent plane of the unit hypersphere. The vector difference between the gradient in the unconstrained space <bold>f</bold><sub><italic>euclid</italic></sub>, and the projection of that gradient onto a unit vector perpendicular to the tangent plane of the hypersphere at <bold>f</bold> (which is identically equal to <bold>f</bold>) gives the gradient of the cost in the tangent plane of the hypersphere <bold>f</bold><sub><italic>grd</italic></sub>. Changing the value of the filters by taking a small step in the direction of <bold>f</bold><sub><italic>grd</italic></sub> ensures that the step will be in the direction that reduces the cost the fastest while still satisfying the constraint that the vector magnitude (i.e. L2 norm) of the filter remain 1.0.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g005" xlink:type="simple"/>
</fig>
<p>The unit vector <bold>f</bold><sub><italic>grd</italic></sub>/‖<bold>f</bold><sub><italic>grd</italic></sub>‖ therefore represents the direction satisfying the constraints in which the cost function is changing most rapidly.</p>
</sec>
<sec id="sec013">
<title>Taking a step</title>
<p>The stochastic gradient descent algorithm is iterative. After each iteration, the filter values are updated by taking a step in the direction of steepest descent that satisfies the constraints. We take a step from the current (old) value of the receptive fields <bold>f</bold><sup>(<italic>old</italic>)</sup> to the new value of the receptive fields <bold>f</bold><sup>(<italic>new</italic>)</sup> in the direction of steepest descent that satisfies the constraint. In particular, the updated receptive fields are given by <inline-formula id="pcbi.1005281.e036"><alternatives><graphic id="pcbi.1005281.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:msup><mml:mi mathvariant="bold">f</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">f</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>ε</mml:mi><mml:mfrac><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">f</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">f</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> where <italic>ε</italic> is the step size.</p>
<p>A step in the direction of steepest descent generally updates the filter values such that cost decreases. However, because batch stochastic gradient descent randomly selects random batches of training stimuli on each iteration, stimuli in some batches may be ‘easy’ while stimuli in other batches may be ‘hard’. Thus, some batches may produce lower costs irrespective of the properties of the filters. Therefore, on each iteration, the updated filter values are preserved for the next iteration only if the value of the cost function for the current batch decreases after the update. By randomly choosing the batches over a large number of iterations, the algorithm, in expectation, converges to the optimum.</p>
</sec>
<sec id="sec014">
<title>Choosing a step size</title>
<p>The problem of choosing an appropriate step size in a gradient descent algorithm has received a good deal of attention in the statistics and machine learning literature. Various methods have been proposed for how to choose step sizes that optimize the rate of convergence [<xref ref-type="bibr" rid="pcbi.1005281.ref036">36</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref037">37</xref>]. Many of these methods adapt the step size to the structure of the cost function, and have demonstrated desirable convergence properties. A formal investigation of how best to choose the step size is beyond the scope of this paper. We followed three basic principles. First, the step size should not be too big; otherwise the algorithm may never converge to the optimum value. Second, the step size should not be too small; otherwise, the algorithm may require a very large number of iterations to achieve convergence. Third, the step size should decrease as the number of iterations increases. We obtained good performance by programming our routine to decrease step size 1.0% on each iteration and to quit after a certain limiting number of iterations. There is clearly room for improvement in this procedure. The results presented here thus represent a lower bound on performance.</p>
</sec>
<sec id="sec015">
<title>Choosing a batch size</title>
<p>The AMA-SGD method developed here uses stochastic batch gradient descent. On each iteration of a batch gradient descent method, a batch of stimuli of a certain size is chosen at random from the training set, the cost and gradient is computed from the batch, and then a step is taken in the direction of the gradient. The choice of batch size is left to the user. It is tempting to choose the smallest possible batch size because the smaller the batches, the more significant the improvement in speed (see above). However, if batch size is too small, filters learned via AMA-SGD will not converge to the filters learned with AMA (see <xref ref-type="sec" rid="sec016">Results</xref>). Choosing a batch size is therefore a trade-off between computational speed and accuracy.</p>
</sec>
</sec>
</sec>
<sec id="sec016" sec-type="results">
<title>Results</title>
<p>To demonstrate the value of AMA-SGD, we use the task of estimating binocular disparity from natural stereo-images [<xref ref-type="bibr" rid="pcbi.1005281.ref024">24</xref>] as a concrete test case. In the context of this task, we show that AMA-SGD converges, dramatically improves the speed of filter learning, and returns the same filters as AMA given sufficiently large batch sizes. Then, we demonstrate that the optimal filters are highly robust to changes in the prior probability distribution, overall noise power, and cost function. We note that these results are not unique to the task of disparity estimation; similar convergence and filter robustness results are obtained for several other tasks. (Labeled training sets for the related tasks of estimating binocular disparity and retinal speed from natural stimuli are available at <ext-link ext-link-type="uri" xlink:href="http://www.github.com/burgelab/AMA" xlink:type="simple">http://www.github.com/burgelab/AMA</ext-link>). Finally, in the discussion section, we examine the general implications of the these results for understanding neural coding with biologically realistic noise models (i.e. noise variance that increases with the mean).</p>
<sec id="sec017">
<title>Binocular Disparity Estimation</title>
<p>Binocular disparities are the local differences between the left and right eye retinal images due to the different vantage point each eye has on the world. Binocular disparities are used for fixating the eyes and for computing the depth structure of scenes (<xref ref-type="fig" rid="pcbi.1005281.g006">Fig 6A</xref>). But the disparities themselves must be estimated before they can be used for depth perception.</p>
<fig id="pcbi.1005281.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g006</object-id>
<label>Fig 6</label>
<caption>
<title>AMA results for disparity estimation with natural stereo images.</title>
<p><bold>A</bold> Stereo-geometry for three different disparities: uncrossed disparity (<italic>δ</italic> = -15 arcmin, eyes fixated in front of target; brown), zero disparity (<italic>δ</italic> = 0.0 arcmin, eyes fixated on target; turquoise), crossed disparity (<italic>δ</italic> = +15 arcmin, eyes fixated behind target; blue). <bold>B</bold> Optimal AMA filters. <bold>C</bold> Conditional response distributions <italic>p</italic>(<bold>r</bold>|<italic>x</italic>) for five different values of the disparity (i.e. latent variable): -15.0, -7.5, 0.0, +7.5, +15.0 arcmin. Each dot represents the expected joint response to an individual stereo image. The responses are the projection of the stimuli basis elements defined by the filters. The conditional response distributions are well characterized by Gaussians (large colored ellipses). For reference, a small ellipse representing filter response noise associated with one stimulus is shown (upper middle part of plot). <bold>D</bold> Posterior probability distributions for three stimuli having -7.5 arcmin of disparity (oversized response dots in C). The posteriors decrease in peakiness and increase in bias as the responses approach the origin, reflecting the fact that responses nearer the origin are more difficult to decode.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g006" xlink:type="simple"/>
</fig>
<p>The estimation of binocular disparity is a classic problem in vision science, and is often referred to as the stereo-correspondence problem. The behavioral limits and neural mechanisms of disparity estimation have been extensively investigated [<xref ref-type="bibr" rid="pcbi.1005281.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref018">18</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref038">38</xref>–<xref ref-type="bibr" rid="pcbi.1005281.ref041">41</xref>]. However, until recently there was no ideal observer for estimating disparity in natural images [<xref ref-type="bibr" rid="pcbi.1005281.ref024">24</xref>]. To develop this ideal observer, Burge &amp; Geisler (2014) first obtained a labeled training set of randomly selected 1 deg binocular retinal images of natural scenes with disparities ranging between -15 to 15 arcmin (400 binocular stimuli x 19 disparity levels = 7600 total). Physiological optics, and the wavelength sensitivity and spatial sampling of the foveal photoreceptors were accurately modeled. AMA was then used to find the small population (n = 8) of binocular filters that extract the most useful information in natural images for the task. Additional filters yielded little improvement, suggesting that eight binocular filters capture most of the available task-relevant information. The properties of the filters mimic the receptive fields of disparity sensitive neurons in cortex, and optimal disparity decoding predicts many aspects of human disparity estimation and discrimination performance. Please see Burge &amp; Geisler (2014) for extensive details on the training set, the ideal observer for disparity estimation, and the role AMA played in its development.</p>
<p>The two most useful filters in the disparity estimation task are shown in <xref ref-type="fig" rid="pcbi.1005281.g006">Fig 6B</xref>. These receptive fields took approximately 1 hour to learn on a 2012 MacBook Pro. The disparity-conditioned filter responses <italic>p</italic>(<bold>r</bold>|<italic>X</italic>) to the contrast normalized stimuli are approximately Gaussian (<xref ref-type="fig" rid="pcbi.1005281.g006">Fig 6C</xref>), and the optimal filters are somewhat anti-correlated: <inline-formula id="pcbi.1005281.e037"><alternatives><graphic id="pcbi.1005281.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">f</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.22</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. Posterior probability distributions for three joint filter responses (oversized dots) are shown in <xref ref-type="fig" rid="pcbi.1005281.g006">Fig 6D</xref>. As the responses get farther from the origin, the posterior probability distributions have more of their mass at the correct level of the latent variable.</p>
<p>The filter response distributions in <xref ref-type="fig" rid="pcbi.1005281.g006">Fig 6C</xref>, and the manner in which they change with the value of the latent variable, are similar to the response distributions obtained for other tasks/coding problems that have been modeled with ‘energy-like’ computations (e.g. disparity-energy, motion-energy) [<xref ref-type="bibr" rid="pcbi.1005281.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref021">21</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref024">24</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref042">42</xref>]: the information about the latent variable is carried primarily by the covariance of the filter responses. This characteristic pattern of filter response will inform subsequent analyses of how interactions between filter correlation, response noise, and stimulus distributions impact encoding fidelity (see <xref ref-type="sec" rid="sec025">Discussion</xref>).</p>
</sec>
<sec id="sec018">
<title>AMA-SGD Performance</title>
<sec id="sec019">
<title>Convergence &amp; run-time improvements</title>
<p>In this section, we demonstrate AMA-SGD’s convergence properties. The disparity filters (c.f. <xref ref-type="fig" rid="pcbi.1005281.g006">Fig 6</xref>) were learned with the original AMA model and therefore constitute a benchmark for AMA-SGD. Here, we examine the effect of batch size on the convergence properties, run-time improvements, and the validity of AMA-SGD filters. Stochastic gradient descent is a noisy process by design. Thus, it is important to verify that AMA-SGD converges. Descent of the cost function should be noisier with small batches and smoother with large batches. <xref ref-type="fig" rid="pcbi.1005281.g007">Fig 7</xref> confirms these expectations and shows that the cost converges noisily but systematically for a wide range of different batch sizes.</p>
<fig id="pcbi.1005281.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Stochastic gradient descent of cost landscape, over two hundred iterations, for three different batch sizes.</title>
<p>Cost computed from each batch as a function of the iteration number as the filters evolve. Jagged black curves show the cost associated with each batch. Red curves show the average cost for each pass through the entire training set. More passes are made through the training set with larger batch sizes and the same number of iterations. <bold>A</bold> 19 stimuli per batch, one stimulus per level. <bold>B</bold> 190 stimuli per batch, ten stimuli per level. <bold>C</bold> 570 stimuli per batch, thirty stimuli per level.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g007" xlink:type="simple"/>
</fig>
<p>To verify the expected improvements in run-time, we compared the time required to evaluate the cost using AMA-SGD for different batch sizes and training set sizes. Evaluating the cost with AMA-SGD is expected to be linear in the number of elements in the training set, for a fixed batch size (see <xref ref-type="sec" rid="sec002">Methods</xref>). <xref ref-type="fig" rid="pcbi.1005281.g008">Fig 8</xref> shows the time required to evaluate the cost for 50 passes through training sets of varying size using AMA (black) and AMA-SGD with batch sizes of 475 stimuli (25 stm/lvl; red) and 950 stimuli (50 stm/lvl; blue). Results show that AMA is quadratic in the number of elements in the training set. Results also show, as expected, that the stochastic gradient descent routine is linear in the number of elements in the training set for a fixed batch size. Thus, AMA-SGD can yield dramatic improvements in the speed of filter learning.</p>
<fig id="pcbi.1005281.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Run-time comparison between AMA and AMA-SGD.</title>
<p>Minimization time as a function of training set size on: <bold>A</bold> linear-linear axes, and <bold>B</bold> log-log axes. Minimization time for AMA increases quadratically while AMA-SGD increases linearly (for a fixed batch size) with the number of elements in the training set. A comparison of AMA and AMA-SGD costs is shown in <xref ref-type="fig" rid="pcbi.1005281.g009">Fig 9</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g008" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec020">
<title>Batch size effects</title>
<p>The faster convergence times obtained with AMA-SGD are advantageous only if the resulting filters are the same as those obtained by AMA. The previous section showed that the most dramatic reductions in run-time occur with the smallest batches. It is therefore tempting to use very small batches when learning filters. However, there is a trade-off between computational gains and accuracy of the filters. AMA-SGD only returns filters identical to those returned by AMA if the batch size is sufficiently large.</p>
<p>To understand why the filters critically depend on batch size, consider the case in which the batch size is so small that there is only one stimulus per level in the average batch. In this case, the probability of response conditioned on a particular value of the latent variable is identical to the probability of the response conditioned on the stimulus having that level: <italic>p</italic>(<bold>R</bold>|<italic>X</italic><sub><italic>i</italic></sub>) = <italic>p</italic>(<bold>R</bold>|<bold>s</bold><sub><italic>ij</italic></sub>). Thus, the posterior probability of the latent variable is identical to the posterior probability of the stimulus, and the filters that best identify the latent variable are identical to the filters that best identify each stimulus. Therefore, as the number of stimuli per level decreases to one, the distinction between identifying the latent variable and identifying a particular stimulus ceases to exist. Hence, a primary distinction vanishes between AMA and other more widely known methods for dimensionality reduction. Under these conditions, one should obtain AMA-SGD filters that are similar to PCA filters.</p>
<p>To illustrate this point, we learned filters multiple times using AMA-SGD where the only difference between each run was the batch size (<xref ref-type="fig" rid="pcbi.1005281.g009">Fig 9</xref>). Indeed, we find that when the batch has only one stimulus per level (~19 stimuli/batch), the resultant AMA-SGD filters are highly correlated with PCA filters. When the batch has 30 or more stimuli per level (~570 stimuli/batch), the resultant AMA-SGD filters are highly correlated with the AMA filters that were learned using AMA (<xref ref-type="fig" rid="pcbi.1005281.g009">Fig 9A–9C</xref>). Costs associated with AMA and AMA-SGD filters become identical as well (<xref ref-type="fig" rid="pcbi.1005281.g009">Fig 9D</xref>). Thus, users should be wary of using small batch sizes when learning filters via AMA-SGD. (See <xref ref-type="supplementary-material" rid="pcbi.1005281.s003">S3 Fig</xref> for more on the distinction between AMA, PCA, and ICA).</p>
<fig id="pcbi.1005281.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g009</object-id>
<label>Fig 9</label>
<caption>
<title>The effect of batch size on AMA-SGD filter shapes.</title>
<p><bold>A</bold> As batch size decreases to ~1 stimulus per level, AMA-SGD filters become similar to PCA filters. As batch size increases to ~30 stimuli per level, AMA-SGD filters converge to AMA filters. <bold>B</bold> Average AMA-SGD filter correlation with PCA filters (dashed) and AMA filters (solid) as a function of the number of stimuli per level. Arrow marks the critical number of stimuli per level, above which AMA-SGD filters are consistent with AMA filters. <bold>C</bold> Filter 1 and filter 2 correlation matrices. <bold>D</bold> Cost, computed over the full dataset with AMA-SGD filters, as a function of the number stimuli per level. Arrow marks the number of stimuli per level above which the total cost computed on the full dataset, is minimized. When learning filters via AMA-SGD, it is critical to have a sufficient number of stimuli per level.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g009" xlink:type="simple"/>
</fig>
<p>We have not fully explored how many stimuli per level are required in a batch for AMA-SGD to converge to the filters returned by AMA. It most likely depends on the use case. However, for the tasks we have examined, a good rule of thumb is to start with batches having approximately 30 stimuli per latent variable level and to systematically increase the batch size until the learned filters are stable.</p>
</sec>
</sec>
<sec id="sec021">
<title>Filter Robustness</title>
<p>In this section, we examine the robustness of the optimal filters to changes in the prior probability distribution, overall noise power, and cost function. We find that the optimal filters are remarkably stable, suggesting that natural stimulus properties are the primary determinants of the optimal filter shapes.</p>
<sec id="sec022">
<title>The effect of the prior</title>
<p>In a closed system, the prior probability distribution can be experimentally manipulated, and its effects can be empirically determined. Here, we examine how the prior impacts the optimal AMA filters for the task of estimating binocular disparity with natural stimuli. The effects of seven different prior distributions are examined. The first is the flat prior probability distribution in the training set used throughout the paper: 400 natural stimuli at each of nineteen disparity levels from -15 to 15 arcmin [<xref ref-type="bibr" rid="pcbi.1005281.ref024">24</xref>]. Of the remaining six priors, three had excess probability mass at zero (zero-disparity priors; <xref ref-type="fig" rid="pcbi.1005281.g010">Fig 10A</xref>), and three had excess mass at large non-zero disparities(<xref ref-type="fig" rid="pcbi.1005281.g010">Fig 10B</xref>). These priors are enforced by randomly culling stimuli in appropriate numbers from each level of the latent variable in the training set (<xref ref-type="fig" rid="pcbi.1005281.g010">Fig 10A and 10B</xref>).</p>
<fig id="pcbi.1005281.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Effect of prior on optimal filters for disparity estimation.</title>
<p><bold>A</bold> Prior probability distributions used to learn the filters presented in C-E. <bold>B</bold> Prior probability distributions used to learn filters in G-I. <bold>C-E</bold> Filters obtained with prior probability distributions having peaks at zero. <bold>F</bold> Filters obtained with flat prior probability distribution; this prior was used throughout the main section of the paper. <bold>G-I.</bold> Filters obtained with prior probability distributions having less mass at zero.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g010" xlink:type="simple"/>
</fig>
<p>In general, the filter shapes are largely robust to substantial variation in the prior probability distribution. The correlations between the filters in F (flat prior) and the filters in C-I are 0.96, 0.98, 0.98, 1.00, 0.97, 0.96, and 0.96, respectively. The priors do have some effect, however. Zero-disparity priors (<xref ref-type="fig" rid="pcbi.1005281.g010">Fig 10A</xref>) cause the optimal filters to select for somewhat higher spatial frequencies and smaller phase shifts than a flat prior. Priors with excess mass at large non-zero disparities (<xref ref-type="fig" rid="pcbi.1005281.g010">Fig 10B</xref>) cause the optimal filters to select for somewhat lower spatial frequencies and larger phase shifts. These trends are as expected [<xref ref-type="bibr" rid="pcbi.1005281.ref024">24</xref>]. However, the optimal filters in the disparity estimation task are largely robust to the variations in the prior examined here. These results suggest, consistent with intuition, that the task-relevant features of natural (proximal) stimuli are the primary determinants of the optimal stimulus encoders. This result should not come as a surprise. In Bayesian signal detection theory, for example, the primary effect of a prior is to shift the decision boundary [<xref ref-type="bibr" rid="pcbi.1005281.ref043">43</xref>].</p>
<p>This general approach- manipulation of the prior in a closed system- may prove useful for investigations of optimal information processing in other sensory-perceptual tasks. It may also prove useful in evaluating claims in the literature about the constraints priors place on the design of neural systems and the subsequent limits of sensory-perceptual processing[<xref ref-type="bibr" rid="pcbi.1005281.ref044">44</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref045">45</xref>] in the presence of natural stimulus variation.</p>
</sec>
<sec id="sec023">
<title>The effect of noise power</title>
<p>Here, we examine the effect of the overall level of encoding noise power on the optimal receptive field shapes. We considered five noise variances over a range spanning two orders of magnitude. The low noise condition contained 1/10th the original noise variance (<italic>α</italic> = 0.136; <inline-formula id="pcbi.1005281.e038"><alternatives><graphic id="pcbi.1005281.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.023</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>), and the high noise variance condition contained 10x the original noise variance ((<italic>α</italic> = 13.6; <inline-formula id="pcbi.1005281.e039"><alternatives><graphic id="pcbi.1005281.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>2.30</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>). To isolate the effect of cut noise variance, the training set and all other parameters were held constant across the conditions.</p>
<p><xref ref-type="fig" rid="pcbi.1005281.g011">Fig 11</xref> shows that the optimal filters are largely robust to substantial changes in response noise variance. Specifically, the correlations between the filters in C (original noise variance) and the filters in A-E are 0.99, 0.99, 1.00, 0.98, and 0.90, respectively. The filters are nearly unchanged for a 30-fold change in noise (<xref ref-type="fig" rid="pcbi.1005281.g011">Fig 11A–11D</xref>). Increasing noise variance by a factor of 10, however, starts to break things down (<italic>α</italic> = 13.6 and <inline-formula id="pcbi.1005281.e040"><alternatives><graphic id="pcbi.1005281.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>2.30</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>; <xref ref-type="fig" rid="pcbi.1005281.g011">Fig 11E</xref>). This result should cut not come as a surprise. From classic ideal observer theory on target detection and discrimination [<xref ref-type="bibr" rid="pcbi.1005281.ref046">46</xref>–<xref ref-type="bibr" rid="pcbi.1005281.ref048">48</xref>], a proportional increase in noise will lower overall performance, but in general it will not change the optimal receptive field shapes. Thus, if the filters are learned with noise parameters that are ‘in the ballpark’ of the noise characteristics of neurons in cortex, the estimated filters should be near optimal for neurons in cortex even if the estimated noise parameters are off by some amount.</p>
<fig id="pcbi.1005281.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Effect of noise power on optimal filters.</title>
<p>Optimal filters for 1/10<sup>th</sup> the original noise variance, 1/3<sup>rd</sup> the original noise variance, the original noise variance, 3x the original noise variance, and 10x the original noise variance. The filters are largely robust to substantial changes in noise variance.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g011" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec024">
<title>The effect of the cost function</title>
<p>Here, we examine the effect of changing the cost function that is used to learn the optimal receptive fields. To isolate the effect of the cost function, the training set and all other model parameters were identical to those used for the main results in the paper. The only change was to use an L2 norm (squared error) cost function.</p>
<p>Changing the cost function has a minimal effect on the optimal encoding filters in this task (<xref ref-type="fig" rid="pcbi.1005281.g012">Fig 12</xref>), just as changing the prior and noise power have minimal effects on the optimal encoding filters. The L2 norm cost function yields filters that are most similar (<italic>ρ</italic> = 0.95) to the L0 norm filters learned with a prior having excess mass at non-zero values. Again, this result should perhaps not be a surprise. Just as changing the cost function from L0 to L2 norm increases the penalty assigned to large errors, increasing the prior probability mass at non-zero values increases the importance of making fewer errors at those latent variable levels.</p>
<fig id="pcbi.1005281.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Effect of cost function on optimal filter shapes.</title>
<p><bold>A</bold> Optimal filters learned with L0 norm (KL divergence) cost function three different priors (c.f. <xref ref-type="fig" rid="pcbi.1005281.g010">Fig 10C, 10F and 10I</xref>). These priors correspond to the most extreme prior with a peak at zero, the flat prior, and the most extreme prior with a nadir at zero. <bold>B</bold> Optimal filters learned with the L2 norm (squared error) cost function and a flat prior. The L2 norm cost function has a subtle but systematic effect on the optimal filters. that is similar to the effect of a prior with excess mass at non-zero values.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g012" xlink:type="simple"/>
</fig>
<p>It is advantageous that the filters are generally robust to the different factors considered here (i.e. the prior, response noise power, and cost function). It suggests that for biologically plausible noise parameters, natural stimulus properties and the task of interest are the primary determinants of the filters that optimize performance in the task. This result is sensible: the properties of the stimulus should primarily determine the most useful receptive field shapes for extracting task relevant information from the stimuli.</p>
</sec>
</sec>
</sec>
<sec id="sec025" sec-type="conclusions">
<title>Discussion</title>
<p>Accuracy Maximization Analysis (AMA) is a method for task-specific dimensionality reduction that has contributed to the development of ideal observers for particular sensory-perceptual tasks in early- and mid-level vision [<xref ref-type="bibr" rid="pcbi.1005281.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1005281.ref024">24</xref>]. It returns the encoding filters (receptive fields) that select the most useful information in proximal stimuli for estimating the value of a latent variable relevant for the task. In conjunction with psychophysical experimental techniques and carefully collected databases of natural images and scenes, the method has helped shed light on the fundamental computations that might be performed by the visual system in the service of particular tasks. Unfortunately, the method has a computational cost high enough as to render the method impractical for many purposes.</p>
<p>To improve the compute time, we derived the gradient for AMA and developed a batch stochastic gradient descent routine to increase the rate at which optimal task-specific filters can be learned. This method, AMA-SGD, finds the optimal filters in compute time that is linear, rather than quadratic, in the number of elements in the training set. In the process, we recognized that filters learned with batches with very few stimuli per level of the latent variable tend to be non-representative. AMA-SGD must therefore be used with caution. However, as our empirical demonstrations make clear, the benefits associated with AMA-SGD greatly outweigh its minor drawbacks, and make AMA a more practical tool for research in perception science.</p>
<p>In what follows, we contrast AMA and AMA-SGD with other methods for dimensionality reduction and neural characterization that provide encodings that are unique only up to a subspace spanned by a set of encoding filters. AMA has the potential to return not only the subspace, but the particular basis elements defining the subspace. This feature of the method is due to the interacting effects of filter correlation and response noise. Scaled additive (e.g. Poisson-like) response noise and non-orthogonal (correlated) receptive fields are widely documented features of neural systems. Many methods for dimensionality reduction and neural characterization are constrained to consider orthogonal filters only [<xref ref-type="bibr" rid="pcbi.1005281.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref049">49</xref>–<xref ref-type="bibr" rid="pcbi.1005281.ref052">52</xref>] [<xref ref-type="bibr" rid="pcbi.1005281.ref053">53</xref>], and/or have response models that assume encodings that are noiseless or are corrupted by constant additive noise only [<xref ref-type="bibr" rid="pcbi.1005281.ref049">49</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref050">50</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref054">54</xref>–<xref ref-type="bibr" rid="pcbi.1005281.ref058">58</xref>].</p>
<p>We find that scaled additive response noise tends to provide an encoding advantage over orthogonal filters with constant additive noise. We conclude by proposing a novel use for AMA. Specifically, we speculate that, if repurposed for the task of obtaining a descriptive model of the feature space driving a neuron’s response, AMA may be able to overcome a fundamental limitation of standard subunit models for neural characterization that prevents links from being established between model components and their biophysical analogs.</p>
<sec id="sec026">
<title>Encoding Fidelity and Uniqueness within a Subspace</title>
<p>Standard forms of the most popular methods for dimensionality reduction (e.g. PCA) and statistical characterization (e.g. ICA) do not include a specific model of encoding noise. In such models, any set of receptive fields (i.e. basis elements) spanning the same subspace encode an arbitrary stimulus with equivalent fidelity. In other words, the encoding provided by a given pair of filters within their spanned subspace is not unique. This fact is due to an assumption common to a large class of popular methods for dimensionality reduction: namely, that the filters encode stimuli noiselessly.</p>
<p>Encoding noise corrupts measurements by real biological or machine vision systems. AMA incorporates an explicit noise model at the level of the encoding filters (Eqs <xref ref-type="disp-formula" rid="pcbi.1005281.e004">1a</xref>–<xref ref-type="disp-formula" rid="pcbi.1005281.e007">1d</xref>), as do probabilistic extensions to PCA and ICA[<xref ref-type="bibr" rid="pcbi.1005281.ref050">50</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref058">58</xref>]. Encoding noise (i.e. the filter response model) can make the stimulus encoding unique within the subspace that the encoding filters define. Figs <xref ref-type="supplementary-material" rid="pcbi.1005281.s001">S1</xref>, <xref ref-type="fig" rid="pcbi.1005281.g013">13</xref> and <xref ref-type="fig" rid="pcbi.1005281.g014">14</xref> are designed to help develop a geometric intuition for why filter response noise can make the encoding of particular filters within a subspace unique. After building intuition, we discuss the implications of this fact for our understanding of neural coding. We consider three classes of encoding filter response: i) a noiseless response model (<italic>α</italic> = 0 and <inline-formula id="pcbi.1005281.e041"><alternatives><graphic id="pcbi.1005281.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, see <xref ref-type="disp-formula" rid="pcbi.1005281.e007">Eq 1d</xref>; <xref ref-type="supplementary-material" rid="pcbi.1005281.s001">S1 Fig</xref>) ii) an constant additive response model (<italic>α</italic> = 0 and <inline-formula id="pcbi.1005281.e042"><alternatives><graphic id="pcbi.1005281.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>; <xref ref-type="fig" rid="pcbi.1005281.g013">Fig 13</xref>), and iii) a scaled additive response noise model (<italic>α</italic> &gt; 0 and <inline-formula id="pcbi.1005281.e043"><alternatives><graphic id="pcbi.1005281.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>); <xref ref-type="fig" rid="pcbi.1005281.g014">Fig 14</xref>). We also consider the impact of having orthogonal (i.e. uncorrelated) encoding filters vs. non-orthogonal (i.e. correlated or anti-correlated) encoding filters. We will see that the type of noise (constant additive or scaled additive), filter correlation (i.e. redundancy), and filter orientation in the subspace can interact non-trivially to confer coding advantages.</p>
<fig id="pcbi.1005281.g013" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g013</object-id>
<label>Fig 13</label>
<caption>
<title>Stimulus encoding fidelity and uniqueness with constant additive noise.</title>
<p>The original stimuli are represented as points in a three dimensional space (bigger red and blue dots, one stimulus from each of two levels of the latent variable). The original stimuli are then projected into a standard (i.e. orthogonal) basis {<bold>e</bold><sub>1</sub>,<bold>e</bold><sub>2</sub>} that spans the same subspace as two (possibly non-orthogonal) filters {<bold>f</bold><sub>1</sub>,<bold>f</bold><sub>2</sub>}. This subspace lies in the <bold>e</bold><sub>1</sub>,<bold>e</bold><sub>2</sub> plane. The ellipse represents uncertainty about each encoded stimulus. The size and orientation of each uncertainty ellipse is determined by the stimulus (red dot), each filter’s response noise, and correlation between the filters. Red Gaussian bumps represent the noisy response distributions of F1 and F2 to the red stimulus. <bold>A</bold> Positively correlated (<inline-formula id="pcbi.1005281.e044"><alternatives><graphic id="pcbi.1005281.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">f</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>) filters. <bold>B</bold> Orthogonal (i.e. uncorrelated; <inline-formula id="pcbi.1005281.e045"><alternatives><graphic id="pcbi.1005281.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">f</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>) filters. <bold>C</bold> Negatively correlated (<inline-formula id="pcbi.1005281.e046"><alternatives><graphic id="pcbi.1005281.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">f</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>) filters. <bold>D-F</bold> Rotated versions of A-C. Orthogonal filters (B,E) provide rotation invariant encoding; non-orthogonal (i.e. positively and negatively correlated) filters do not (A,C,D,F).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g013" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005281.g014" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g014</object-id>
<label>Fig 14</label>
<caption>
<title>Stimulus encoding fidelity and uniqueness with scaled additive noise (i.e. additive noise with response variance multiplicatively related to the response mean).</title>
<p>The original stimuli are represented as points in a three dimensional space (bigger red and blue dots, one stimulus from each of two levels of the latent variable). The original stimuli are then projected into a standard (i.e. orthogonal) basis {<bold>e</bold><sub>1</sub>,<bold>e</bold><sub>2</sub>} that spans the same subspace as two (possibly non-orthogonal) filters {<bold>f</bold><sub>1</sub>,<bold>f</bold><sub>2</sub>}. The uncertainty ellipse represents uncertainty about the encoded stimulus given the filter responses. The size and orientation of each uncertainty ellipse is determined by the stimulus (dot), the filter response noise, and the correlation between the filters. <bold>A-F</bold> Unlike with constant additive noise, stimulus encoding with scaled additive noise is unique (up to a sign flip) regardless of whether the filters are orthogonal. Filters that are somewhat anti-correlated yield uncertainty ellipses that are oriented approximately with lines radiating from the origin.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g014" xlink:type="simple"/>
</fig>
<p>Consider two stimuli that are projected into a standard basis spanned by an arbitrary pair of filters <bold>f</bold>; let this subspace be represented by orthonormal basis <bold>e</bold> (Figs <xref ref-type="supplementary-material" rid="pcbi.1005281.s001">S1</xref>, <xref ref-type="fig" rid="pcbi.1005281.g013">13</xref> and <xref ref-type="fig" rid="pcbi.1005281.g014">14</xref>) If noiseless encoding is assumed (which is of course biologically unrealistic), the stimuli are encoded with equal fidelity no matter the filter correlation (redundancy) or rotation, so long as the filters lie in the same subspace. Specifically, filters F1 and F2 encode the stimulus identically well, regardless of whether the encoding filters are positively correlated, orthogonal, or anti-correlated. Rotating the encoding filters in the subspace also has no impact on coding fidelity. Thus, with no encoding noise, every set of filters spanning the same subspace provides an equivalent stimulus encoding (<xref ref-type="supplementary-material" rid="pcbi.1005281.s001">S1 Fig</xref>).</p>
<p>With constant additive response noise the situation changes. Now, filters F1 and F2 encode the stimulus with different fidelity when they are correlated vs. when they are orthogonal; note the differences in the uncertainty ellipses (<xref ref-type="fig" rid="pcbi.1005281.g013">Fig 13A–13C</xref>). When the filters are orthogonal (<xref ref-type="fig" rid="pcbi.1005281.g013">Fig 13B and 13E</xref>), the uncertainty ellipses are circular, and stimulus encoding remains invariant to rotation (<xref ref-type="fig" rid="pcbi.1005281.g013">Fig 13E</xref>). Stimulus encoding by correlated filters, however, is no longer invariant to filter rotation (<xref ref-type="fig" rid="pcbi.1005281.g013">Fig 13A, 13D, 13C and 13F</xref>).</p>
<p>With scaled additive response noise, the situation changes still further. Filters F1 and F2 now provide a unique encoding of the stimulus, regardless of whether they are correlated or uncorrelated, and regardless of whether the filters are rotated or not (<xref ref-type="fig" rid="pcbi.1005281.g014">Fig 14A–14F</xref>).</p>
<p>The fact that the fidelity of stimulus encoding changes as a function of filter correlation and rotation within a subspace suggests that encoding cost (i.e. the value of AMA objective function) may depend on the particular filters within a given subspace. To examine this issue quantitatively, we rotated the optimal receptive field pair within their spanned subspace and computed the cost for each rotation angle <italic>θ</italic>. (see <xref ref-type="supplementary-material" rid="pcbi.1005281.s011">S8 Text</xref>). Recall that the optimal filters in the current task are somewhat anti-correlated (<inline-formula id="pcbi.1005281.e047"><alternatives><graphic id="pcbi.1005281.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">f</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.22</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>). We also examined the cost of forcing the filters to be orthogonal. To do so, we performed Gram Schmidt orthogonalization, rotated the orthogonalized filters, and computed the cost for each rotation angle <italic>θ</italic>.</p>
<p><xref ref-type="fig" rid="pcbi.1005281.g015">Fig 15</xref> shows that some filter pairs within the subspace yield lower cost than others (<xref ref-type="fig" rid="pcbi.1005281.g015">Fig 15A–15C</xref>). Example filter pairs that have been rotated by different amounts are depicted in <xref ref-type="fig" rid="pcbi.1005281.g015">Fig 15D</xref>. With scaled additive response noise (the noise model with which the filters were learned), cost is lowest for the optimal filters. For all non-zero rotation angles cost increases, except for 180° (<xref ref-type="fig" rid="pcbi.1005281.g015">Fig 15A</xref>). (A 180° rotation angle corresponds to contrast reversal of both receptive fields, which by assumption (Eqs <xref ref-type="disp-formula" rid="pcbi.1005281.e004">1a</xref>–<xref ref-type="disp-formula" rid="pcbi.1005281.e007">1d</xref>), gives identical performance to the original filters). If the filters are orthogonalized, cost increases on average. More importantly, the minimum cost of the best pair of orthgonalized filters is higher than the minimum cost of the original somewhat anti-correlated filters. This result shows that correlated filters can provide a coding advantage over orthogonal filters in the AMA framework.</p>
<fig id="pcbi.1005281.g015" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g015</object-id>
<label>Fig 15</label>
<caption>
<title>Encoding cost in the subspace spanned by the filters.</title>
<p><bold>A</bold> Cost as a function of rotation angle for response noise models with scaled additive and constant additive noise. With scaled additive noise, the optimal filters (lower solid curve) provide a unique encoding up to a sign flip (i.e. rotation angle = 180°). Orthogonal filters with scaled additive noise that span the same subspace (lower dashed curve) provide an encoding that is periodic on 90°. For comparison, cost as a function of rotation angle for filters with constant additive noise and matched noise power is also shown (see text). (Note that the original, optimal filters (c.f. <xref ref-type="fig" rid="pcbi.1005281.g006">Fig 6A</xref>) have a cosine similarity (i.e. correlation) of <italic>ρ</italic> = -0.22, corresponding to an angle difference of 103°.) <bold>B</bold> Cost landscape for scaled additive noise within the subspace spanned by filters 1 and 2 for all possible rotation angles and angle differences (i.e. correlations). The curves in A show vertical slices through this space. Arrow marks optimal filters. <bold>C</bold> Cost landscape with additive noise. <bold>D</bold> Filters as a function of rotation angle in the subspace.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g015" xlink:type="simple"/>
</fig>
<p>Next, we examined constant additive response noise models. With constant additive (instead of scaled additive) response noise, cost is also modulated by the rotation angle within the subspace, but only if the filters are non-orthogonal. If the filters are orthogonal, all filter rotations within the subspace provide an identical encoding (<xref ref-type="fig" rid="pcbi.1005281.g015">Fig 15A</xref>). These results are consistent with the intuitions developed in <xref ref-type="fig" rid="pcbi.1005281.g013">Fig 13</xref>. To make a quantitative comparison between the encoding costs associated with the two noise models, we matched the noise power between the two models. Specifically, we set the constant additive noise variance equal to the average variance of the scaled additive noise <inline-formula id="pcbi.1005281.e048"><alternatives><graphic id="pcbi.1005281.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula> where <italic>N</italic> is the total number of stimuli in the training set. Encoding filters having this constant additive noise never achieve costs as low as the scaled additve noise model (<xref ref-type="fig" rid="pcbi.1005281.g015">Fig 15A and 15C</xref>). So long as noise power is matched, this result holds whether the filters are learned with scaled additive or constant additive noise. Therefore, for the task considered here (disparity estimation), scaled additive noise provides a coding advantage.</p>
<p>This same result holds for several other fundamental tasks in early vision with natural images (retinal speed estimation, motion-in-depth estimation). These tasks have all been successfully modeled with energy-like computations (disparity energy model, motion energy model, etc.). We conclude that scaled additive noise provides a coding advantage over constant additive noise in an important class of estimation tasks in early- and mid-level vision for which energy-like computations are appropriate.</p>
<p>There are several take-away points. First, in AMA, all encoding filters, even those spanning the same subspace, do not provide equivalent encodings. Second, correlated filters can yield lower cost encodings than orthogonal filters. Third, scaled additive response noise can yield lower cost encodings than constant additive response noise when the noise power (i.e. average noise variance) is matched. These results have implications for how to think about the pros and cons of the constraints imposed on many methods for dimensionality reduction.</p>
</sec>
<sec id="sec027">
<title>Scaled Additive Neural Noise and Filter Correlation</title>
<p>In this section, we examine why scaled additive response noise can provide an advantage over constant additive response noise. In early visual areas, neural response variance increases approximately linearly with the mean response [<xref ref-type="bibr" rid="pcbi.1005281.ref031">31</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref032">32</xref>]. Much attention has been paid to this property of neural response, especially as it relates psychophysical performance in target detection, a paradigmatic task in the spatial vision literature. When response variance is proportional to the mean response, a single neuron’s signal-to-noise ratio for detection of a particular target is proportional to the square-root of the mean response, <inline-formula id="pcbi.1005281.e049"><alternatives><graphic id="pcbi.1005281.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005281.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>∝</mml:mo><mml:msqrt><mml:mi>r</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>. On the other hand, if neural response variance is independent of the mean response (i.e. constant), the signal to noise ratio is proportional to the mean response, <italic>SNR</italic> ∝ <italic>r</italic>. Thus, it has been sensibly argued that, all other things equal, scaled additive noise must have deleterious effects on neural coding compared to constant additive noise.</p>
<p>However, in the case considered above, scaled additive noise (response noise variance proportional to mean response) outperforms constant additive noise with matched noise powers. Many (most?) visual tasks are performed at super threshold contrasts and involve estimating the value of a variable that is latent in the proximal stimulus. In latent variable estimation and discrimination tasks, scaled additive noise can benefit, rather than deteriorate the quality of neural encoding. This is true in the current task of disparity estimation (Figs <xref ref-type="fig" rid="pcbi.1005281.g006">6</xref> and <xref ref-type="fig" rid="pcbi.1005281.g014">14</xref>–<xref ref-type="fig" rid="pcbi.1005281.g016">16</xref>). It is also true of other related tasks in early vision (e.g. speed estimation and motion-in-depth estimation). This result raises the possibility that a ubiquitous neural response property that hurts performance in contrast detection tasks may actually benefit performance in tasks that are somewhat ‘higher-level’ (e.g. disparity estimation).</p>
<fig id="pcbi.1005281.g016" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005281.g016</object-id>
<label>Fig 16</label>
<caption>
<title>Filter correlation, scaled additive noise, and effects on stimulus encoding.</title>
<p><bold>A-C</bold> Conditional stimulus distributions, projected into the subspace spanned by the filters, represented two ways. Upper row: stimulus distributions <italic>p</italic>(<bold>e</bold><sup><italic>T</italic></sup><bold>s</bold>|<italic>X</italic><sub><italic>i</italic></sub>) conditioned on different values of the latent variable (red, green, blue) projected into the subspace spanned by the filters. The cardinal axes in the standard basis (<italic>e</italic><sub>1</sub> and <italic>e</italic><sub>2</sub>) are orthonormal by definition whereas the filters are not necessarily orthogonal. Lower row: conditional filter response distributions. Changing the correlation between the filters from positive (A), to orthogonal (B), to anti-correlated (C) alters how the uncertainty ellipses are aligned with the stimulus distributions in the standard basis. <bold>D</bold> Definition of rotation angle and angle difference. <bold>E</bold> Cost landscape in the subspace defined by the filters. The minimum occurs for the situation in C when the filters are anti-correlated (angle difference &gt; 90°). The interaction with scaled additivenoise causes the uncertainty ellipses to be maximally aligned with the stimulus distributions.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.g016" xlink:type="simple"/>
</fig>
<p>Why, in latent variable estimation and discrimination, can a vision system with scaled additive noise outperform a vision system with matched constant additive noise? Some development is necessary to answer this question because the answer depends on a set of interlocking dependencies between the noise model, filter correlations, and the latent-variable-conditioned stimulus distributions <italic>p</italic>(<bold>s</bold>|<italic>X</italic><sub><italic>i</italic></sub>). When the task is to discriminate one latent variable value from another (as opposed to detecting a well-defined contrast pattern—a signal-known-exactly task), it is less clear what constitutes ‘signal’ and what constitutes ‘noise’. We have found it useful to approach the problem with standard techniques in the pattern classification literature [<xref ref-type="bibr" rid="pcbi.1005281.ref059">59</xref>].</p>
<p>Consider a hypothetical case that illustrates the relevant principles. <xref ref-type="fig" rid="pcbi.1005281.g016">Fig 16A–16C</xref> shows three simulated stimulus distributions projected into the subspace spanned by a pair of filters. (These simulated distributions are superficially similar to the disparity conditioned stimulus distributions shown in <xref ref-type="fig" rid="pcbi.1005281.g006">Fig 6C</xref>.) These same exact stimuli are encoded by three pairs of filters that are differently correlated, but that have the same scaled additive noise and that span the same subspace (Eqs <xref ref-type="disp-formula" rid="pcbi.1005281.e004">1a</xref>–<xref ref-type="disp-formula" rid="pcbi.1005281.e006">1c</xref>). This subspace is represented by the orthonormal basis <bold>e</bold> that spans the same subspace as the filters <bold>f</bold>. The upper and lower rows of <xref ref-type="fig" rid="pcbi.1005281.g016">Fig 16A–16C</xref> represent the same information in different forms. In the upper row, three latent-variable-conditioned stimulus distributions are projected into the subspace defined by a pair of filters <italic>p</italic>(<bold>e</bold><sup><italic>T</italic></sup><bold>s</bold>|<italic>X</italic><sub><italic>i</italic></sub>); the dots represent the stimulus projections in the standard basis and the ellipses represent encoding uncertainty. In the lower row, the exact same stimulus projections are represented by the mean responses that they elicit from each filter pair, <italic>p</italic>(<bold>f</bold><sup><italic>T</italic></sup><bold>s</bold>|<italic>X</italic><sub><italic>i</italic></sub>); the dots represent the stimulus projections onto the filters and the ellipses represent filter response noise. We refer to the lower row as the <italic>filter basis</italic>. Consistent with the assumption that the filter response noise is independent (Eqs <xref ref-type="disp-formula" rid="pcbi.1005281.e006">1c</xref> and <xref ref-type="disp-formula" rid="pcbi.1005281.e007">1d</xref>), all the noise ellipses in the filter basis (lower row) are aligned with the axes of the space (i.e. the noise covariance matrix is diagonal). The linear mappings from the filter basis to the standard basis and back are derived in <xref ref-type="supplementary-material" rid="pcbi.1005281.s011">S8 Text</xref>. The oblique orientations of the uncertainty ellipses in the upper row of 16ac therefore <italic>do not</italic> reflect noise correlations [<xref ref-type="bibr" rid="pcbi.1005281.ref060">60</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref061">61</xref>].</p>
<p>Now, examine the effect of changing the filters from being positively correlated (<xref ref-type="fig" rid="pcbi.1005281.g016">Fig 16A</xref>), to orthogonal (<xref ref-type="fig" rid="pcbi.1005281.g016">Fig 16B</xref>), to anti-correlated (<xref ref-type="fig" rid="pcbi.1005281.g016">Fig 16C</xref>). As filter correlation and orientation within the subspace changes (see <xref ref-type="fig" rid="pcbi.1005281.g016">Fig 16D</xref>), the uncertainty ellipses (upper row) change their orientation. Cost is minimized when the uncertainty ellipses maximally align with the projections of the conditional stimulus distributions (<xref ref-type="fig" rid="pcbi.1005281.g016">Fig 16E</xref>). Remarkably, filter correlation (i.e. cosine similarity), filter orientation in the subspace, and scaled additive noise can conspire to align the uncertainty ellipses with the conditional stimulus distributions.</p>
<p>The conditional distributions of filter responses are shown in the lower row of <xref ref-type="fig" rid="pcbi.1005281.g016">Fig 16A–16C</xref>. The mean filter response to each stimulus is obtained by projecting the stimulus onto each filter (c.f. <xref ref-type="fig" rid="pcbi.1005281.g016">Fig 16D</xref>; <bold>r</bold> = <bold>f</bold><sup><italic>T</italic></sup><bold>s</bold>). In filter response space (i.e. the filter basis), two effects occur as filter correlation changes. The most dramatic effect is the change in the distribution the response means. A secondary effect is that the height and/or width of the response noise ellipses decrease as the corresponding mean filter response approaches zero. Note, however, that the noise ellipses always remain aligned with the cardinal axes. In other words, the noise ellipses have diagonal covariance matrices, consistent with the assumption of independent response noise. In filter response space, the algorithm’s aim is to position the filters such that the conditional response distributions <italic>p</italic>(<bold>R</bold>|<italic>X</italic><sub><italic>i</italic></sub>) are as discriminable from each other as possible.</p>
<p>With a constant additive noise model, in the standard basis, all uncertainty ellipses have the same orientation (c.f. <xref ref-type="fig" rid="pcbi.1005281.g013">Fig 13B</xref>); in the filter basis, all noise ellipses are circular (i.e. equal variance diagonal covariance matrices). Thus, if the conditional stimulus distributions change orientation as a function of the value in the latent variable (as they do here), the constant additive noise model cannot align the uncertainty ellipses with the stimulus distributions across the space. As a consequence, encoding cost increases (<xref ref-type="fig" rid="pcbi.1005281.g015">Fig 15A</xref>, <xref ref-type="supplementary-material" rid="pcbi.1005281.s002">S2 Fig</xref>).</p>
<p>In general, cost is minimized when encoding uncertainty is maximized within, and minimized between, latent-variable-conditioned stimulus distributions. That is, when uncertainty due to noise maximally overlaps the uncertainty due to ‘nuisance’ stimulus variation, coding of the latent variable is improved. A related claim about the potential utility of noise correlations has recently been made [<xref ref-type="bibr" rid="pcbi.1005281.ref061">61</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref062">62</xref>]. Additionally, scaled additive noise yields lower response variance for stimuli near the origin than stimuli far from the origin of the response space, thereby reducing the relative cost of ‘hard’ stimuli and increasing the relative cost of ‘easy’ stimuli. Each stimulus therefore contributes more evenly to the cost. In the tasks considered, this property causes the algorithm to make better use of the information provided by each stimulus. If the expressions underlying AMA were reformulated as a learning rule, we suspect that scaled additive noise would enable the system to learn more efficiently. Most importantly, we have shown that scaled additive noise and non-orthogonal filters can confer significant benefits to neural encoding. These benefits are obtained when uncertainty due to noise is shaped to match within-level stimulus variation.</p>
</sec>
<sec id="sec028">
<title>Limitations and Future Directions</title>
<p>The AMA cost landscape is non-convex, so there is no guarantee that the filters found by the algorithm indeed represent the global minimum. However, there are several reasons to suspect that the filters for disparity estimation presented here indeed found a global minimum. First, the recovered filters occupy the minimum of the cost landscape within the subspace that they span (<xref ref-type="fig" rid="pcbi.1005281.g015">Fig 15A–15C</xref>). Second, somewhat surprisingly, correlated (non-orthogonal) filters with scaled additive noise tend yield lower cost landscapes with deeper minima (<xref ref-type="fig" rid="pcbi.1005281.g015">Fig 15B and 15C</xref>) than orthogonal filters with constant additive noise. Third, the work presented here and in previous publications has found that different random initializations tend to yield equivalent filters.</p>
<p>The response model used here allows both positive and negative encoding filter responses whereas real neurons give only positive responses. Future work will examine the pros and cons of incorporating half-rectification into the response model (Eqs <xref ref-type="disp-formula" rid="pcbi.1005281.e004">1a</xref>–<xref ref-type="disp-formula" rid="pcbi.1005281.e007">1d</xref>). One drawback of incorporating half-rectification is that more filters will be required to cover the same response space, thereby increasing the dimensionality of the search space, perhaps leading to less stable performance. However, incorporating half-rectification will increase biological realism, allow for differential sensitivity to ON/OFF contrast changes [<xref ref-type="bibr" rid="pcbi.1005281.ref063">63</xref>], and increase the flexibility of the system to match stimulus encoding uncertainty to task-irrelevant stimulus variation (Figs <xref ref-type="fig" rid="pcbi.1005281.g013">13</xref>–<xref ref-type="fig" rid="pcbi.1005281.g016">16</xref>).</p>
</sec>
<sec id="sec029">
<title>AMA for Neural Systems Identification</title>
<p>Interest in neural systems identification has surged in recent years. The field has generated a slew of models with ever increasing sophistication and descriptive power. Many of these models are known as ‘subunit models’. Subunit models seek to provide a computational level description of a neuron’s processing that can predict a neuron’s response to arbitrary stimuli.</p>
<p>The spike-triggered average (STA) and spike-triggered covariance (STC) analysis are early examples of subunit models [<xref ref-type="bibr" rid="pcbi.1005281.ref064">64</xref>–<xref ref-type="bibr" rid="pcbi.1005281.ref067">67</xref>]. The generalized linear model (GLM) and generalized quadratic model (GQM) are examples of more recently developed subunit models that are more flexible and powerful [<xref ref-type="bibr" rid="pcbi.1005281.ref068">68</xref>–<xref ref-type="bibr" rid="pcbi.1005281.ref070">70</xref>]. (All of these methods have been adapted to handle non-spiking, real-valued data (e.g. response rates or intracellular voltages[<xref ref-type="bibr" rid="pcbi.1005281.ref069">69</xref>,<xref ref-type="bibr" rid="pcbi.1005281.ref071">71</xref>]). These methods have been widely adopted by the neuroscience community because of their success in providing compact, interpretable characterizations of the input-output relationship between stimuli and neural response. In general, subunit models describe neural response with a low-dimensional set of stimulus features (i.e. subunit receptive fields), a nonlinear pooling rule, a static output non-linearity, and noise function that generates output noise. As these models have increased in descriptive power and mathematical elegance, interest has increased in whether the computational components can be mapped back to specific biophysical components. For example, in a subunit model description of a complex cell, one may ask whether presynaptic simple cells are the biophysical analogs of the model subunits.</p>
<p>A limitation of this class of subunit models is that although they can recover the subspace spanned by a set of receptive fields, the models cannot recover the subunit receptive fields themselves. In traditional subunit models, any set of receptive fields spanning the same subspace encodes a given stimulus with equal fidelity. This property of subunit models is due to the fact that they implicitly assume noiseless encoding. AMA, on the other hand, has an explicit model of response noise for each filter (i.e. subunit receptive field). As discussed above, response yields encodings that are unique within the subspace defined by the filters (Figs <xref ref-type="fig" rid="pcbi.1005281.g015">15</xref>, <xref ref-type="fig" rid="pcbi.1005281.g016">16</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005281.s001">S1</xref>). By adapting AMA as a method for neural system’s identification, we speculate that it may be possible to identify both the subspace spanned by the subunit receptive fields, and the individual subunit receptive fields themselves. As neural datasets come online having simultaneous recordings between ‘target cells’ and their presynaptic inputs (e.g. connected V1 and LGN units), these possibilities can be tested explicitly.</p>
<p>Explicitly modeling noise at the level of the subunit receptive field responses does not come without its own set of drawbacks. The GLM and GQM have cost landscapes that are convex; the local minimum is guaranteed to be the global minimum under the model. The cost landscape in AMA is non-convex, so guarantees cannot be made that the minima found via AMA are global minima. However, in the cases we have examined (see above), AMA results tend to be stable. Future work must determine whether this research direction is viable, but the ingredients are there to justify searching for a productive way forward.</p>
</sec>
<sec id="sec030">
<title>Conclusions</title>
<p>This manuscript presents technical improvements to and conceptual insights about Accuracy Maximization Analysis (AMA), a recently developed Bayesian method for task-specific dimensionality reduction [<xref ref-type="bibr" rid="pcbi.1005281.ref022">22</xref>]. The manuscript has four primary aims. First, it provides a thorough and intuitive review of AMA, explaining the logic behind method’s setup and its solutions. Second, it contributes two technical advances- the gradient of the cost function and a stochastic gradient descent routine- that markedly decrease compute time, thereby making it a more practical tool for research in sensation and perception. Third, it shows that the effects of the prior over the latent variable, internal noise, and the cost function can be examined relative to the effect of stimulus variability. Fourth, it examines several non-standard features of the method—its ability to model scaled additive noise and learn correlated filters—that make it more flexible than other more widely known methods. This flexibility confers a coding advantage, and renders the method capable of identifying particular filters (receptive fields) within the subspace that they span. This capability is due primarily to the explicit modeling of noise at the level of the encoding filter responses, which all biological systems suffer from. Perceptual psychology and visual neuroscience are relatively young fields, but they are advancing rapidly, and cross-pollination between the sub-disciplines is increasingly common. As research with natural stimuli becomes increasingly common, widespread application of this method may help speed progress.</p>
</sec>
</sec>
<sec id="sec031">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1005281.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Stimulus encoding and uniqueness without filter response noise.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005281.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Filter correlation, constant additive noise, and effects on stimulus encoding.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005281.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Comparison of ICA, PCA, and AMA filters in a simulated case.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005281.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.s004" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Posterior probability distribution over the latent variable.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005281.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.s005" xlink:type="simple">
<label>S2 Text</label>
<caption>
<title>AMA gradient with the 0,1/KL-divergence cost function.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005281.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.s006" xlink:type="simple">
<label>S3 Text</label>
<caption>
<title>AMA gradient with the squared error cost function.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005281.s007" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.s007" xlink:type="simple">
<label>S4 Text</label>
<caption>
<title>Optimal estimator for 0,1 cost function is MAP estimator.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005281.s008" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.s008" xlink:type="simple">
<label>S5 Text</label>
<caption>
<title>KL-divergence is negative log-probability of correct latent variable.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005281.s009" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.s009" xlink:type="simple">
<label>S6 Text</label>
<caption>
<title>Posterior mean is optimal estimator for squared error cost function.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005281.s010" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.s010" xlink:type="simple">
<label>S7 Text</label>
<caption>
<title>Rotating correlated filters within the spanned subspace.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005281.s011" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005281.s011" xlink:type="simple">
<label>S8 Text</label>
<caption>
<title>Uncertainty ellipses for encoding with correlated filters in standard basis.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Wilson S. Geisler, Kenneth Latimer, and Pedro Ortega for helpful comments on a draft version of this manuscript. We thank Jonathan Pillow for helpful discussions. We also thank Wilson S. Geisler for suggesting the stochastic gradient descent approach.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005281.ref001"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Hotelling H. Analysis of a Complex of Statistical Variables Into Principal Components. 1933.</mixed-citation></ref>
<ref id="pcbi.1005281.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hotelling</surname> <given-names>H</given-names></name>. <article-title>Relations Between Two Sets of Variates</article-title>. <source>Biometrika</source>. <year>1936</year>;<volume>28</volume>: <fpage>321</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bell</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>The “independent components” of natural scenes are edge filters</article-title>. <source>Vision Research</source>. <year>1997</year>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laughlin</surname> <given-names>SB</given-names></name>. <article-title>A simple coding procedure enhances a neuron's information capacity</article-title>. <source>Zeitschrift für Naturforschung c</source>. <year>1981</year>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>: <fpage>607</fpage>–<lpage>609</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/381607a0" xlink:type="simple">10.1038/381607a0</ext-link></comment> <object-id pub-id-type="pmid">8637596</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ruderman</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Statistics of natural images: Scaling in the woods</article-title>. <source>Phys Rev Lett</source>. <year>1994</year>;<volume>73</volume>: <fpage>814</fpage>–<lpage>817</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.73.814" xlink:type="simple">10.1103/PhysRevLett.73.814</ext-link></comment> <object-id pub-id-type="pmid">10057546</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>. <article-title>Natural image statistics and neural representation</article-title>. <source>Annu Rev Neurosci</source>. <year>2001</year>;<volume>24</volume>: <fpage>1193</fpage>–<lpage>1216</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.24.1.1193" xlink:type="simple">10.1146/annurev.neuro.24.1.1193</ext-link></comment> <object-id pub-id-type="pmid">11520932</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>. <article-title>Efficient coding of natural sounds</article-title>. <source>Nat Neurosci</source>. <year>2002</year>;<volume>5</volume>: <fpage>356</fpage>–<lpage>363</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn831" xlink:type="simple">10.1038/nn831</ext-link></comment> <object-id pub-id-type="pmid">11896400</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Hateren</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>van der Schaaf</surname> <given-names>A</given-names></name>. <article-title>Independent component filters of natural images compared with simple cells in primary visual cortex</article-title>. <source>Proc Biol Sci</source>. <year>1998</year>;<volume>265</volume>: <fpage>359</fpage>–<lpage>366</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rspb.1998.0303" xlink:type="simple">10.1098/rspb.1998.0303</ext-link></comment> <object-id pub-id-type="pmid">9523437</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunswick</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Kamiya</surname> <given-names>J</given-names></name>. <article-title>Ecological cue-validity of proximity and of other Gestalt factors</article-title>. <source>Am J Psychol</source>. <year>1953</year>;<volume>66</volume>: <fpage>20</fpage>–<lpage>32</lpage>. <object-id pub-id-type="pmid">13030843</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fowlkes</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Malik</surname> <given-names>J</given-names></name>. <article-title>Local figure-ground cues are valid for natural images</article-title>. <source>J Vis</source>. <year>2007</year>;<volume>7</volume>: <fpage>2</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hecht</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Shlaer</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pirenne</surname> <given-names>MH</given-names></name>. <article-title>Energy, Quanta, and Vision</article-title>. <source>J Gen Physiol</source>. <year>1942</year>;<volume>25</volume>: <fpage>819</fpage>–<lpage>840</lpage>. <object-id pub-id-type="pmid">19873316</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Sequential ideal-observer analysis of visual discriminations</article-title>. <source>Psychol Rev</source>. <year>1989</year>;<volume>96</volume>: <fpage>267</fpage>–<lpage>314</lpage>. <object-id pub-id-type="pmid">2652171</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Mikami</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wurtz</surname> <given-names>RH</given-names></name>. <article-title>Motion selectivity in macaque visual cortex. III. Psychophysics and physiology of apparent motion</article-title>. <source>J Neurophysiol</source>. <year>1986</year>;<volume>55</volume>: <fpage>1340</fpage>–<lpage>1351</lpage>. <object-id pub-id-type="pmid">3734859</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeAngelis</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Ohzawa</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Freeman</surname> <given-names>RD</given-names></name>. <article-title>Depth is encoded in the visual cortex by a specialized receptive field structure</article-title>. <source>Nature</source>. <year>1991</year>;<volume>352</volume>: <fpage>156</fpage>–<lpage>159</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/352156a0" xlink:type="simple">10.1038/352156a0</ext-link></comment> <object-id pub-id-type="pmid">2067576</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Weiss</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Adelson</surname> <given-names>EH</given-names></name>. <article-title>Motion illusions as optimal percepts</article-title>. <source>Nat Neurosci</source>. <year>2002</year>;<volume>5</volume>: <fpage>598</fpage>–<lpage>604</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn858" xlink:type="simple">10.1038/nn858</ext-link></comment> <object-id pub-id-type="pmid">12021763</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>. <year>2002</year>;<volume>415</volume>: <fpage>429</fpage>–<lpage>433</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/415429a" xlink:type="simple">10.1038/415429a</ext-link></comment> <object-id pub-id-type="pmid">11807554</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Gepshtein</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>. <article-title>Why is spatial stereoresolution so low?</article-title> <source>J Neurosci</source>. <year>2004</year>;<volume>24</volume>: <fpage>2077</fpage>–<lpage>2089</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3852-02.2004" xlink:type="simple">10.1523/JNEUROSCI.3852-02.2004</ext-link></comment> <object-id pub-id-type="pmid">14999059</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>, <name name-style="western"><surname>Perry</surname> <given-names>JS</given-names></name>. <article-title>Contour statistics in natural images: grouping across occlusions</article-title>. <source>Vis Neurosci</source>. <year>2009</year>;<volume>26</volume>: <fpage>109</fpage>–<lpage>121</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1017/S0952523808080875" xlink:type="simple">10.1017/S0952523808080875</ext-link></comment> <object-id pub-id-type="pmid">19216819</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blakemore</surname> <given-names>C</given-names></name>. <article-title>The range and scope of binocular depth discrimination in man</article-title>. <source>J Physiol (Lond)</source>. <year>1970</year>;<volume>211</volume>: <fpage>599</fpage>–<lpage>622</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burge</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Optimal speed estimation in natural image movies predicts human performance</article-title>. <source>Nat Commun</source>. <year>2015</year>;<volume>6</volume>: <fpage>7900</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/ncomms8900" xlink:type="simple">10.1038/ncomms8900</ext-link></comment> <object-id pub-id-type="pmid">26238697</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>, <name name-style="western"><surname>Najemnik</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ing</surname> <given-names>AD</given-names></name>. <article-title>Optimal stimulus encoders for natural tasks</article-title>. <source>J Vis</source>. <year>2009</year>;<volume>9</volume>: <fpage>17.1</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burge</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Optimal defocus estimation in individual natural images</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2011</year>;<volume>108</volume>: <fpage>16849</fpage>–<lpage>16854</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1108491108" xlink:type="simple">10.1073/pnas.1108491108</ext-link></comment> <object-id pub-id-type="pmid">21930897</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burge</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Optimal disparity estimation in natural stereo images</article-title>. <source>J Vis</source>. <year>2014</year>;<volume>14</volume>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>, <name name-style="western"><surname>Perry</surname> <given-names>JS</given-names></name>. <article-title>Statistics for optimal point prediction in natural images</article-title>. <source>J Vis</source>. <year>2011</year>;<volume>11</volume>: <fpage>14</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burge</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>McCann</surname> <given-names>BC</given-names></name>, <name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Estimating 3D tilt from local image cues in natural scenes</article-title>. <source>J Vis</source>. <year>2016</year>;<volume>16</volume>: <fpage>2</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref027"><label>27</label><mixed-citation publication-type="other" xlink:type="simple">Burge J, Geisler WS. Optimal defocus estimates from individual images for autofocusing a digital camera. Proceedings of SPIE; 2012.</mixed-citation></ref>
<ref id="pcbi.1005281.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yang</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Purves</surname> <given-names>D</given-names></name>. <article-title>Image/source statistics of surfaces in natural scenes</article-title>. <source>Network</source>. <year>2003</year>;<volume>14</volume>: <fpage>371</fpage>–<lpage>390</lpage>. <object-id pub-id-type="pmid">12938763</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Potetz</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>TS</given-names></name>. <article-title>Statistical correlations between two-dimensional images and three-dimensional structures in natural scenes</article-title>. <source>J Opt Soc Am A Opt Image Sci Vis</source>. <year>2003</year>;<volume>20</volume>: <fpage>1292</fpage>–<lpage>1303</lpage>. <object-id pub-id-type="pmid">12868635</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sprague</surname> <given-names>WW</given-names></name>, <name name-style="western"><surname>Cooper</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Tosic</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>Stereopsis is adaptive for the natural environment</article-title>. <source>Science Advances</source>. American Association for the Advancement of Science; <year>2015</year>;<volume>1</volume>: <fpage>e1400254</fpage>–<lpage>e1400254</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/sciadv.1400254" xlink:type="simple">10.1126/sciadv.1400254</ext-link></comment> <object-id pub-id-type="pmid">26207262</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tolhurst</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Dean</surname> <given-names>AF</given-names></name>. <article-title>The statistical reliability of signals in single neurons in cat and monkey visual cortex</article-title>. <source>Vision Research</source>. <year>1983</year>;<volume>23</volume>: <fpage>775</fpage>–<lpage>785</lpage>. <object-id pub-id-type="pmid">6623937</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>, <name name-style="western"><surname>Albrecht</surname> <given-names>DG</given-names></name>. <article-title>Visual cortex neurons in monkeys and cats: detection, discrimination, and identification</article-title>. <source>Vis Neurosci</source>. <year>1997</year>;<volume>14</volume>: <fpage>897</fpage>–<lpage>919</lpage>. <object-id pub-id-type="pmid">9364727</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref033"><label>33</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Spall</surname> <given-names>JC</given-names></name>. <source>Introduction to stochastic search and optimization: estimation, simulation, and control</source>. <publisher-loc>New Jersey</publisher-loc>: <publisher-name>Wiley &amp; Sons, Inc</publisher-name>; <year>2003</year>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kiwiel</surname> <given-names>KC</given-names></name>. <article-title>Convergence of Approximate and Incremental Subgradient Methods for Convex Optimization</article-title>. <source>SIAM J Optim</source>. Society for Industrial and Applied Mathematics; <year>2004</year>;<volume>14</volume>: <fpage>807</fpage>–<lpage>840</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref035"><label>35</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bertsekas</surname> <given-names>DP</given-names></name>. <source>Nonlinear Programming</source>. <edition>2nd ed.</edition> <publisher-name>Athena Scientific</publisher-name>; <year>1999</year>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barzilai</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Borwein</surname> <given-names>JM</given-names></name>. <article-title>Two-point step size gradient methods</article-title>. <source>IMA Journal of Numerical …</source>. <year>1988</year>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mathews</surname> <given-names>VJ</given-names></name>, <name name-style="western"><surname>Xie</surname> <given-names>Z</given-names></name>. <article-title>A stochastic gradient adaptive filter with gradient adaptive step size</article-title>. <source>IEEE Transactions on Signal Processing</source>. <year>1993</year>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cormack</surname> <given-names>LK</given-names></name>, <name name-style="western"><surname>Stevenson</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Schor</surname> <given-names>CM</given-names></name>. <article-title>Interocular correlation, luminance contrast and cyclopean processing</article-title>. <source>Vision Research</source>. <year>1991</year>;<volume>31</volume>: <fpage>2195</fpage>–<lpage>2207</lpage>. <object-id pub-id-type="pmid">1771799</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tyler</surname> <given-names>CW</given-names></name>, <name name-style="western"><surname>Julesz</surname> <given-names>B</given-names></name>. <article-title>Binocular cross-correlation in time and space</article-title>. <source>Vision Research</source>. Elsevier; <year>1978</year>;<volume>18</volume>: <fpage>101</fpage>–<lpage>105</lpage>. <object-id pub-id-type="pmid">664265</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nienborg</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Bridge</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Parker</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Cumming</surname> <given-names>BG</given-names></name>. <article-title>Neuronal computation of disparity in V1 limits temporal resolution for detecting disparity modulation</article-title>. <source>J Neurosci</source>. <year>2005</year>;<volume>25</volume>: <fpage>10207</fpage>–<lpage>10219</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2342-05.2005" xlink:type="simple">10.1523/JNEUROSCI.2342-05.2005</ext-link></comment> <object-id pub-id-type="pmid">16267228</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref041"><label>41</label><mixed-citation publication-type="other" xlink:type="simple">Haefner R, Bethge M. Evaluating neuronal codes for inference using Fisher information. 2010. pp. 1–9.</mixed-citation></ref>
<ref id="pcbi.1005281.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Adelson</surname> <given-names>EH</given-names></name>, <name name-style="western"><surname>Bergen</surname> <given-names>JR</given-names></name>. <article-title>Spatiotemporal energy models for the perception of motion</article-title>. <source>J Opt Soc Am A</source>. <year>1985</year>;<volume>2</volume>: <fpage>284</fpage>–<lpage>299</lpage>. <object-id pub-id-type="pmid">3973762</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref043"><label>43</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Green</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Swets</surname> <given-names>JA</given-names></name>. <source>Signal detection theory and psychophysics</source>. <publisher-name>Wiley New York</publisher-name>; <year>1966</year>;<fpage>1</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ganguli</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Efficient sensory encoding and Bayesian inference with heterogeneous neural populations</article-title>. <source>Neural Comput</source>. <year>2014</year>;<volume>26</volume>: <fpage>2103</fpage>–<lpage>2134</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00638" xlink:type="simple">10.1162/NECO_a_00638</ext-link></comment> <object-id pub-id-type="pmid">25058702</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wei</surname> <given-names>X-X</given-names></name>, <name name-style="western"><surname>Stocker</surname> <given-names>AA</given-names></name>. <article-title>A Bayesian observer model constrained by efficient coding can explain “anti-Bayesian” percepts</article-title>. Nature Publishing Group. <year>2015</year>;<volume>18</volume>: <fpage>1509</fpage>–<lpage>1517</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.4105" xlink:type="simple">10.1038/nn.4105</ext-link></comment> <object-id pub-id-type="pmid">26343249</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanner</surname> <given-names>WP</given-names></name>, <name name-style="western"><surname>Swets</surname> <given-names>JA</given-names></name>. <article-title>A Decision-Making Theory of Visual Detection</article-title>. <source>Psychol Rev</source>. <year>1954</year>;<volume>61</volume>: <fpage>401</fpage>–<lpage>409</lpage>. <object-id pub-id-type="pmid">13215690</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref047"><label>47</label><mixed-citation publication-type="other" xlink:type="simple">Tanner WP, Clark-Jones RC. Visual Search Techniques: Proceedings of a Symposium, Held in the…—Armed Forces-NRC Vision Committee—Google Books. Vision Research Problems. 1960.</mixed-citation></ref>
<ref id="pcbi.1005281.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>, <name name-style="western"><surname>Davila</surname> <given-names>KD</given-names></name>. <article-title>Ideal discriminators in spatial vision: two-point stimuli</article-title>. <source>J Opt Soc Am A</source>. <year>1985</year>;<volume>2</volume>: <fpage>1483</fpage>–<lpage>1497</lpage>. <object-id pub-id-type="pmid">4045582</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eckart</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>G</given-names></name>. <article-title>The approximation of one matrix by another of lower rank</article-title>. <source>Psychometrika</source>. <year>1936</year>;<volume>1</volume>: <fpage>211</fpage>–<lpage>218</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref050"><label>50</label><mixed-citation publication-type="other" xlink:type="simple">Tipping ME, Bishop CM. Probabilistic principal component analysis. … of the Royal Statistical Society: Series …. 1999.</mixed-citation></ref>
<ref id="pcbi.1005281.ref051"><label>51</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Borg</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Groenen</surname> <given-names>PJ</given-names></name>. <source>Modern Multidimensional Scaling: Theory and Applications</source>. <publisher-name>Springer Verlag</publisher-name>; <year>2005</year>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fisher</surname> <given-names>RA</given-names></name>. <article-title>The use of multiple measurements in taxonomic problems</article-title>. <source>Annals of Eugenics</source>. <year>1936</year>;<volume>7</volume>: <fpage>179</fpage>–<lpage>188</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref053"><label>53</label><mixed-citation publication-type="other" xlink:type="simple">Roweis S. EM algorithms for PCA and SPCA. Advances in Neural Information Processing systems. 1998.</mixed-citation></ref>
<ref id="pcbi.1005281.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Spearman</surname> <given-names>C</given-names></name>. <article-title>“General Intelligence,” Objectively Determined and Measured</article-title>. <source>Am J Psychol</source>. <year>1904</year>;<volume>15</volume>: <fpage>201</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref055"><label>55</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hyvärinen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Karhunen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Oja</surname> <given-names>E</given-names></name>. <source>Independent Component Analysis</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>John Wiley and Sons, Inc</publisher-name>; <year>2001</year>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stone</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Brooks</surname> <given-names>RJ</given-names></name>. <article-title>Continuum regression: cross-validated sequentially constructed prediction embracing ordinary least squares, partial least squares and principal components …</article-title>. <source>Journal of the Royal Statistical Society Series B (…</source>. <year>1990</year>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>LQ</given-names></name>, <name name-style="western"><surname>Cichocki</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Amari</surname> <given-names>S</given-names></name>. <article-title>Natural gradient algorithm for blind separation of overdetermined mixture with additive noise</article-title>. <source>Signal Processing Letters</source>. <year>1999</year>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amari</surname> <given-names>S</given-names></name>. <article-title>Natural gradient learning for over- and under-complete bases In ICA</article-title>. <source>Neural Comput</source>. <year>1999</year>;<volume>11</volume>: <fpage>1875</fpage>–<lpage>1883</lpage>. <object-id pub-id-type="pmid">10578035</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref059"><label>59</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bishop</surname> <given-names>CM</given-names></name>. <source>Pattern Recognition and Machine Learning</source>. <publisher-name>Springer Verlag</publisher-name>; <year>2006</year>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohen</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Kohn</surname> <given-names>A</given-names></name>. <article-title>Measuring and interpreting neuronal correlations</article-title>. Nature Publishing Group. <year>2011</year>;<volume>14</volume>: <fpage>811</fpage>–<lpage>819</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2842" xlink:type="simple">10.1038/nn.2842</ext-link></comment> <object-id pub-id-type="pmid">21709677</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Cafaro</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Turner</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>. <article-title>Direction-Selective Circuits Shape Noise to Ensure a Precise Population Code</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>89</volume>: <fpage>369</fpage>–<lpage>383</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2015.11.019" xlink:type="simple">10.1016/j.neuron.2015.11.019</ext-link></comment> <object-id pub-id-type="pmid">26796691</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Franke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Fiscella</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sevelev</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Roska</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Hierlemann</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>da Silveira</surname> <given-names>RA</given-names></name>. <article-title>Structures of Neural Correlation and How They Favor Coding</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>89</volume>: <fpage>409</fpage>–<lpage>422</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2015.12.037" xlink:type="simple">10.1016/j.neuron.2015.12.037</ext-link></comment> <object-id pub-id-type="pmid">26796692</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cooper</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Norcia</surname> <given-names>AM</given-names></name>. <article-title>Predicting cortical dark/bright asymmetries from natural image statistics and early visual transforms</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>: <fpage>e1004268</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1004268" xlink:type="simple">10.1371/journal.pcbi.1004268</ext-link></comment> <object-id pub-id-type="pmid">26020624</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Steveninck</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Real-time performance of a movement-sensitive neuron in the blowfly visual system: coding and information transfer in short spike sequences</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source>. <year>1988</year>;: <fpage>379</fpage>–<lpage>414</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005281.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brenner</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>de Ruyter van Steveninck</surname> <given-names>R</given-names></name>. <article-title>Adaptive rescaling maximizes information transmission</article-title>. <source>Neuron</source>. <year>2000</year>;<volume>26</volume>: <fpage>695</fpage>–<lpage>702</lpage>. <object-id pub-id-type="pmid">10896164</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Spike-triggered neural characterization</article-title>. <source>J Vis</source>. <year>2006</year>;<volume>6</volume>: <fpage>484</fpage>–<lpage>507</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/6.4.13" xlink:type="simple">10.1167/6.4.13</ext-link></comment> <object-id pub-id-type="pmid">16889482</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Samengo</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Gollisch</surname> <given-names>T</given-names></name>. <article-title>Spike-triggered covariance: geometric proof, symmetry properties, and extension beyond Gaussian stimuli</article-title>. <source>J Comput Neurosci</source>. <year>2013</year>;<volume>34</volume>: <fpage>137</fpage>–<lpage>161</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10827-012-0411-y" xlink:type="simple">10.1007/s10827-012-0411-y</ext-link></comment> <object-id pub-id-type="pmid">22798148</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Spatiotemporal elements of macaque v1 receptive fields</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>46</volume>: <fpage>945</fpage>–<lpage>956</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2005.05.021" xlink:type="simple">10.1016/j.neuron.2005.05.021</ext-link></comment> <object-id pub-id-type="pmid">15953422</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Park</surname> <given-names>IM</given-names></name>, <name name-style="western"><surname>Archer</surname> <given-names>EW</given-names></name>, <name name-style="western"><surname>Priebe</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>J</given-names></name>. <article-title>Spectral methods for neural characterization using generalized quadratic models</article-title>. <source>Advances in neural …</source>. <year>2013</year>;: <fpage>1</fpage>–<lpage>9</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4993-spectral-methods-for-neural-characterization-using-generalized-quadratic-models.pdf" xlink:type="simple">http://papers.nips.cc/paper/4993-spectral-methods-for-neural-characterization-using-generalized-quadratic-models.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1005281.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McFarland</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Cui</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Butts</surname> <given-names>DA</given-names></name>. <article-title>Inferring nonlinear neuronal computation based on physiologically plausible inputs</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>: <fpage>e1003143</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003143" xlink:type="simple">10.1371/journal.pcbi.1003143</ext-link></comment> <object-id pub-id-type="pmid">23874185</object-id></mixed-citation></ref>
<ref id="pcbi.1005281.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hunter</surname> <given-names>IW</given-names></name>, <name name-style="western"><surname>Korenberg</surname> <given-names>MJ</given-names></name>. <article-title>The identification of nonlinear biological systems: Wiener and Hammerstein cascade models</article-title>. <source>Biol Cybern</source>. <year>1986</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>