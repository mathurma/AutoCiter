<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="correction" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005908</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-02003</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Correction</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Correction: Sequential inference as a mode of cognition and its correlates in fronto-parietal and hippocampal brain regions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<collab>The <italic>PLOS Computational Biology</italic> Staff</collab>
</contrib>
</contrib-group>
<pub-date pub-type="epub">
<day>13</day>
<month>12</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<month>12</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>12</issue>
<elocation-id>e1005908</elocation-id>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>The PLOS Computational Biology Staff</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005908"/>
<related-article ext-link-type="uri" id="related001" related-article-type="corrected-article" xlink:href="info:doi/10.1371/journal.pcbi.1005418" xlink:type="simple">
<article-title>Sequential inference as a mode of cognition and its correlates in fronto-parietal and hippocampal brain regions</article-title>
</related-article>
<counts>
<fig-count count="0"/>
<table-count count="0"/>
<page-count count="3"/>
</counts>
</article-meta>
</front>
<body>
<p>The authors describe how the interpretation of the paper has changed as a result of this conceptual error below, though the analyses and results themselves are unaffected:</p>
<p>“During the course of extending on our prior work we became aware that our paper contains a conceptual error that alters the interpretation of the results we describe (though we add that the analyses and results themselves are unaffected).</p>
<p>In the paper we hypothesised that human subjects use a ‘sequential inference’ strategy, and provide evidence that our model, based on this hypothesis, captures behaviour on a probabilistic reversal task better than Bayesian filtering (where agents infer on the probability of individual states considered in isolation). On this basis we inferred that this constitutes evidence that humans adopt an alternative, but equally optimal, strategy to Bayesian filtering when performing the task.</p>
<p>Since publishing the paper, we have realised that this description has shortcomings. In fact, the sequential inference model that we use is suboptimal, for reasons we describe below. Moreover, a truly optimal sequential inference strategy makes behavioural predictions identical to Bayesian filtering on this task. Thus, our results do provide evidence that subjects infer over sequences of states stretching into the past as we originally claimed. We now also conclude that the data does not support a claim about normativity that we make in the paper.</p>
<p>The voxel-based morphometry results are, in themselves, unaffected by this error. However, interpreting the results is now less straightforward, due to the fact that optimal sequential inference produces identical predictions about behaviour as does filtering. In the case of the effects relating to the sequence length measure <italic>L</italic>, it is possible that subjects with <italic>L</italic> = 1 (11 out of 79 subjects) may be performing optimal sequential inference rather than filtering. Similarly, the measure <italic>ΔLL</italic> that indexes evidence for sequential inference can now be taken either to reflect evidence in favour of performing sequential inference as opposed to filtering (as we describe it in the paper), or evidence in favour of performing sequential inference suboptimally as opposed to optimally. Given the clear behavioural evidence that we found in favour of our sequential inference model, the interpretations given in the paper still seem reasonably compelling.</p>
<p>Notwithstanding the issue of interpretation we believe our work still makes a potentially valuable contribution. However, based upon our revisiting of the data we now consider that the findings need to be interpreted in a more nuanced fashion than described in the original publication.</p>
<sec id="sec001">
<title>Mathematical explanation of error</title>
<p>To see why the model that we describe in the paper is suboptimal, consider a simple Hidden Markov Model (HMM) in which an agent seeks infers on a series of <italic>T</italic> time-varying hidden states <italic>x</italic><sub>1:<italic>T</italic></sub> = {<italic>x</italic><sub>1</sub>, …, <italic>x</italic><sub><italic>T</italic></sub>} given a series of observations <italic>o</italic><sub>1:<italic>T</italic></sub> = {<italic>o</italic><sub>1</sub>, …, <italic>o</italic><sub><italic>T</italic></sub>}. For clarity of exposition we drop the dependence on parameters <italic>θ</italic> and initial distribution <italic>p</italic>(<italic>x</italic><sub>0</sub>), but these are nonetheless implied. We also define a sequence of states <inline-formula id="pcbi.1005908.e001"><alternatives><graphic id="pcbi.1005908.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005908.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and observations <inline-formula id="pcbi.1005908.e002"><alternatives><graphic id="pcbi.1005908.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005908.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:mover accent="true"><mml:mi>o</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> where <italic>n</italic> is the length of the sequence considered by the agent, and <italic>t</italic> is the present time.</p>
<p>In Bayesian filtering, we recursively estimate beliefs about states at each time point <italic>t</italic> using the following equation:
<disp-formula id="pcbi.1005908.e003">
<alternatives>
<graphic id="pcbi.1005908.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005908.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(C1)</label>
</disp-formula></p>
<p>In this paper, by contrast, we suggest that rather than inferring on individual states in isolation, agents might instead infer on the joint probability a sequence of states <inline-formula id="pcbi.1005908.e004"><alternatives><graphic id="pcbi.1005908.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005908.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and then calculate the marginal distributions over states at individual time points by summation:
<disp-formula id="pcbi.1005908.e005">
<alternatives>
<graphic id="pcbi.1005908.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005908.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:mtable columnalign="right"><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>j</mml:mi></mml:mstyle></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>,</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>j</mml:mi></mml:mstyle></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mo>{</mml:mo> <mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>ℤ</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>∧</mml:mo><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow> <mml:mo>}</mml:mo></mml:mrow><mml:mtext>.</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(C2)</label>
</disp-formula></p>
<p>We then use this to derive a recursive algorithm where the agent infers on the joint probability of sequences of states using a sequence of observations <inline-formula id="pcbi.1005908.e006"><alternatives><graphic id="pcbi.1005908.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005908.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mover accent="true"><mml:mi>o</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and beliefs about the state immediately preceding the current sequence derived by marginalising the joint distribution at the previous timepoint. (In other words, the beliefs about that state generated during the previous round of sequential inference) Thus
<disp-formula id="pcbi.1005908.e007">
<alternatives>
<graphic id="pcbi.1005908.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005908.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mi>o</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(C3)</label>
</disp-formula></p>
<p>This provides a compact algorithm that permits an agent to perform inference at the level of entire sequences, and update its beliefs about the past. However, as we have now realised, it is not strictly optimal. The HMM has the conditional independence property
<disp-formula id="pcbi.1005908.e008">
<alternatives>
<graphic id="pcbi.1005908.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005908.e008" xlink:type="simple"/>
<mml:math display="block" id="M8">
<mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(C4)</label>
</disp-formula>
(see (Bishop, 2006) for more details). This means that optimal inference over the sequence takes the form:
<disp-formula id="pcbi.1005908.e009">
<alternatives>
<graphic id="pcbi.1005908.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005908.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mi>o</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(C5)</label>
</disp-formula></p>
<p>The difference here is in the final term, which specifies beliefs about the state <italic>x</italic><sub><italic>t</italic>−<italic>n</italic></sub> (the state immediately preceding the sequence being inferred on). When following an optimal strategy, agents should use <italic>p</italic>(<italic>x</italic><sub><italic>t</italic>−<italic>n</italic></sub> | <italic>o</italic><sub>1:<italic>t</italic>−<italic>n</italic></sub>), which is the estimate of this state derived from Bayesian filtering as given by (<xref ref-type="disp-formula" rid="pcbi.1005908.e003">C1</xref>). This means that in our model observations at times early in the sequence are overweighted (effectively they are counted more than once), and leads to suboptimal inference (however, optimal sequential inference requires that an agent separately track filtered beliefs and those derived from sequential inference, which may be neurobiologically less plausible) Marginalising over the joint probability distribution generated by (<xref ref-type="disp-formula" rid="pcbi.1005908.e009">C5</xref>) produces identical beliefs about the current state to those derived from Bayesian filtering.”</p>
</sec>
</body>
<back>
<ref-list>
<title>Reference</title>
<ref id="pcbi.1005908.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>FitzGerald</surname> <given-names>THB</given-names></name>, <name name-style="western"><surname>Hämmerer</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>S-C</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name> (<year>2017</year>) <article-title>Sequential inference as a mode of cognition and its correlates in fronto-parietal and hippocampal brain regions</article-title>. <source>PLoS Comput Biol</source><volume>13</volume>(<issue>5</issue>): <fpage>e1005418</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005418" xlink:type="simple">https://doi.org/10.1371/journal.pcbi.1005418</ext-link> <object-id pub-id-type="pmid">28486504</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>