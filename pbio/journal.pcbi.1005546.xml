<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005546</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-00866</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Electromagnetic radiation</subject><subj-group><subject>Light</subject><subj-group><subject>Visible light</subject><subj-group><subject>Luminance</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Information theory</subject><subj-group><subject>Background signal noise</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Background signal noise</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Noise, multisensory integration, and previous response in perceptual disambiguation</article-title>
<alt-title alt-title-type="running-head">Perceptual disambiguation: noise, multisensory integration and previous response</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Parise</surname>
<given-names>Cesare V.</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ernst</surname>
<given-names>Marc O.</given-names>
</name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Oculus Research, Redmond, Washington, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Cognitive Neuroscience Department and Cognitive Interaction Technology-Center of Excellence, Bielefeld University, Bielefeld, Germany</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Applied Cognitive Psychology, Ulm University, Ulm, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Einhäuser</surname>
<given-names>Wolfgang</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Technische Universitat Chemnitz, GERMANY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"> <list-item>
<p><bold>Conceived and designed the experiments:</bold> CVP MOE.</p></list-item> <list-item>
<p><bold>Performed the experiments:</bold> CVP.</p></list-item> <list-item>
<p><bold>Analyzed the data:</bold> CVP.</p></list-item> <list-item>
<p><bold>Contributed reagents/materials/analysis tools:</bold> MOE.</p></list-item> <list-item>
<p><bold>Wrote the paper:</bold> CVP MOE.</p></list-item></list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">cesare.parise@googlemail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>10</day>
<month>7</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<month>7</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>7</issue>
<elocation-id>e1005546</elocation-id>
<history>
<date date-type="received">
<day>27</day>
<month>5</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>2</day>
<month>5</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Parise, Ernst</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005546"/>
<abstract>
<p>Sensory information about the state of the world is generally ambiguous. Understanding how the nervous system resolves such ambiguities to infer the actual state of the world is a central quest for sensory neuroscience. However, the computational principles of perceptual disambiguation are still poorly understood: What drives perceptual decision-making between multiple equally valid solutions? Here we investigate how humans gather and combine sensory information–within and across modalities–to disambiguate motion perception in an ambiguous audiovisual display, where two moving stimuli could appear as either streaming through, or bouncing off each other. By combining psychophysical classification tasks with reverse correlation analyses, we identified the particular spatiotemporal stimulus patterns that elicit a stream or a bounce percept, respectively. From that, we developed and tested a computational model for uni- and multi-sensory perceptual disambiguation that tightly replicates human performance. Specifically, disambiguation relies on knowledge of prototypical bouncing events that contain characteristic patterns of motion energy in the dynamic visual display. Next, the visual information is linearly integrated with auditory cues and prior knowledge about the history of recent perceptual interpretations. What is more, we demonstrate that perceptual decision-making with ambiguous displays is systematically driven by noise, whose random patterns not only promote alternation, but also provide signal-like information that biases perception in highly predictable fashion.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Sensory information is generally ambiguous, and a single sensory modality most often cannot provide enough information to univocally specify the actual state of the world. A primary task for the brain is therefore to resolve perceptual ambiguity. Here we use a dynamic audiovisual ambiguous display embedded in noise to investigate the computational mechanisms of perceptual disambiguation. Results demonstrate that the brain first extracts visual information for perceptual disambiguation through motion detectors. Such information is next combined with auditory information–and memory of recent perceptual history–through weighted averaging to determine the final percept. This study revealed the particular spatiotemporal stimulus patterns that elicit a stream or a bounce percept, respectively, and it demonstrates that perceptual disambiguation is majorly affected by noise, whose random spatiotemporal patterns provide signal-like information that bias perception in a very systematic fashion.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001659</institution-id>
<institution>Deutsche Forschungsgemeinschaft</institution>
</institution-wrap>
</funding-source>
<award-id>01GQ1002</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Ernst</surname>
<given-names>Marc</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100004963</institution-id>
<institution>Seventh Framework Programme</institution>
</institution-wrap>
</funding-source>
<award-id>601165</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Ernst</surname>
<given-names>Marc</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001659</institution-id>
<institution>Deutsche Forschungsgemeinschaft</institution>
</institution-wrap>
</funding-source>
<award-id>CITEC: ECX 277</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Ernst</surname>
<given-names>Marc</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This study is part of the research programs of the Bernstein Center for Computational Neuroscience, Tübingen, funded by the German Federal Ministry of Education and Research (German Federal Ministry of Education and Research; Förderkennzeichen: 01GQ1002), the 7th Framework Programme European Project “Wearhap” (601165), and the Deutsche Forschungsgemeinschaft Excellence Cluster Cognitive Interaction Technology—Cluster of Excellence (CITEC: ECX 277). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="0"/>
<page-count count="20"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2017-07-24</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Data has been uploaded as supplemental information.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Perception is well described as an inference process based on noisy and often ambiguous sensory signals. As such, a single sensory modality most often cannot provide enough information to univocally specify the actual state of the world. Throughout the history of vision science, numerous ambiguous displays have been put forward where the very same sensory stimulus allows for multiple, and clearly distinguishable, alternative interpretations—multistable stimuli such as the Necker Cube, the stream-bounce display and binocular rivalry [<xref ref-type="bibr" rid="pcbi.1005546.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1005546.ref004">4</xref>]. However, in our daily lives perception seems to be surprisingly devoid of such ambiguities. This is because in most real-life scenarios, the brain can rely on a large variety of information that is often not present in the minimalistic ambiguous displays used in laboratory settings. Information for perceptual disambiguation can come from different cues derived from the same or other senses [<xref ref-type="bibr" rid="pcbi.1005546.ref005">5</xref>], or it may come in form of prior knowledge representing the statistical regularities found in the natural world [<xref ref-type="bibr" rid="pcbi.1005546.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref007">7</xref>], or recent perceptual history [<xref ref-type="bibr" rid="pcbi.1005546.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1005546.ref015">15</xref>].</p>
<p>A notable example of ambiguous stimulus is the stream-bounce display (cf. <xref ref-type="supplementary-material" rid="pcbi.1005546.s006">S1 Movie</xref>): two identical objects moving towards each other along the same trajectory can be perceived as streaming through each other, or as colliding and bouncing away from one another [<xref ref-type="bibr" rid="pcbi.1005546.ref001">1</xref>]. Vision alone does not provide enough information to rule out any possible interpretation, and over repeated presentations the two percepts alternate in a seemingly arbitrary fashion. However, if a sound is presented around the time of the crossing, humans are more likely to perceive a bounce [<xref ref-type="bibr" rid="pcbi.1005546.ref005">5</xref>]. This finding demonstrates that humans integrate multisensory information for perceptual disambiguation, that is, to infer the most likely interpretation of the sensory data. However, the underlying mechanism of this inference process is still poorly understood. What drives perceptual decision making, and which strategy does the brain use to extract and combine information from the different senses?</p>
<p>Sensory information is corrupted by noise arising in the brain at any stage of processing [<xref ref-type="bibr" rid="pcbi.1005546.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref017">17</xref>]. Therefore, a possible reason for perceptual alternation (in cases where the two states are about equally likely) relies on the random fluctuations of the internal noise [<xref ref-type="bibr" rid="pcbi.1005546.ref018">18</xref>]. The role of noise on ambiguous displays has been widely investigated over the years; for example, motion coherence (i.e., noise in the motion signal) can reliably predict the time-course of perceptual switches in binocular rivalry with moving stimuli [<xref ref-type="bibr" rid="pcbi.1005546.ref019">19</xref>]. Likewise, internal noise is at the heart of most computational models for perceptual alternation in binocular rivalry [<xref ref-type="bibr" rid="pcbi.1005546.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1005546.ref024">24</xref>]. Specifically, perceptual alternation is often interpreted in terms of a double-well energy landscape [<xref ref-type="bibr" rid="pcbi.1005546.ref024">24</xref>], where both adaptation-recovery and noise contribute perceptual shifts [<xref ref-type="bibr" rid="pcbi.1005546.ref021">21</xref>]. Moreover, the effects of noise on binocular rivalry have been successfully simulated with biologically plausible spiking neuron models [<xref ref-type="bibr" rid="pcbi.1005546.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref023">23</xref>].</p>
<p>While these models can predict perception in binocular rivalry given the statistical properties of the noise, it is currently unclear how the individual instances of the noise affect perceptual disambiguation. For example, the spatiotemporal patterns present in the noise may contain signal-like information that systematically biases perception towards one specific interpretation (and against its alternative). Unfortunately, scientists do not have direct access to the noise that is present within the brain, thus making it hard to test this hypothesis. To overcome this problem, and systematically study the effects of noise on perception, psychophysicists often try to override the noise in the system by introducing (external) noise directly in the stimulus [<xref ref-type="bibr" rid="pcbi.1005546.ref025">25</xref>]. Given that noise does not come with a label, the brain often cannot infer its internal vs. external nature, so it is reasonable to assume that the brain usually treats these two sources of noise in the same way [<xref ref-type="bibr" rid="pcbi.1005546.ref017">17</xref>], (though see [<xref ref-type="bibr" rid="pcbi.1005546.ref026">26</xref>] for a recent finding on how the brain may sometimes be capable of distinguishing, and filtering out, different types of noise). Once the exact properties of the noise are known, reverse correlation techniques [<xref ref-type="bibr" rid="pcbi.1005546.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1005546.ref029">29</xref>] can be used to investigate whether the brain looks for certain patterns in the noise that might help resolving ambiguity. That is, by investigating how the distribution of external noise on each given trial biases perception, it becomes possible–by averaging the classified stimuli and their noises–to estimate the spatiotemporal decision template used for perceptual disambiguation. In turn, this allows one to explore a number of important aspects of perceptual disambiguation, such as what biases perception, and how sensory information is combined across the senses–and over time and space–to determine the final percept.</p>
<p>In the present study we used reverse correlation techniques to investigate the multisensory mechanisms for perceptual disambiguation in the stream-bounce display. Research on multisensory integration has demonstrated that when integrating redundant and unambiguous information from different sensory modalities, the brain operates in a statistically optimal fashion by taking a weighted average of the individual sensory cues. Thereby, the weights are assigned according to the precision of each cue [<xref ref-type="bibr" rid="pcbi.1005546.ref030">30</xref>]. This solution is statistically optimal because it provides the most accurate and precise perceptual estimate given the noisy sensory information as input. In the case of the stream-bounce display, however, the information provided by vision and audition is complementary in nature. More specifically, while vision informs us about the spatiotemporal trajectories of the moving objects (while information about the nature of the impact is ambiguous: present or not), audition informs us about the presence of an impact by an appropriately timed sound (or the absence of an impact if the sound is absent or presented with inappropriate timing). That is, vision and audition provide information in qualitatively different formats, which cannot be directly averaged without further transformations (e.g., see [<xref ref-type="bibr" rid="pcbi.1005546.ref031">31</xref>]). The aim of the present study is to characterize how the brain extracts, transforms, and combines complementary information within and across the senses to resolve perceptual ambiguity.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Experiment 1</title>
<p>Three participants (the author CP, age 33, male; and two female naïve observers, CG and VL age 24 and 23, respectively) were presented with two small vertical light gray bars (0.085° x 0.426° each) moving horizontally along the same trajectory in opposite directions, crossing at the center (signal: <xref ref-type="fig" rid="pcbi.1005546.g001">Fig 1</xref>, left). The moving stimuli were embedded in dynamic visual noise (noise: <xref ref-type="fig" rid="pcbi.1005546.g001">Fig 1</xref>, center; signal+noise: <xref ref-type="fig" rid="pcbi.1005546.g001">Fig 1</xref>, right; <xref ref-type="supplementary-material" rid="pcbi.1005546.s007">S2 Movie</xref>) randomly increasing or decreasing in luminance from the middle grey background. In a forced choice task, participant had to report whether they perceived the bars as streaming across each other or as colliding and bouncing off each other (see <xref ref-type="sec" rid="sec008">Methods</xref>). The experiment consisted of ~10,000 trials per participant with the dynamic visual noise randomly varying across trials. In half of the trials, a sound (10ms white noise click) was presented at the time of the crossing. Sound and no-sound trials alternated throughout the experiment in blocks of 40 trials.</p>
<fig id="pcbi.1005546.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005546.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Stimuli.</title>
<p>The visual stimuli consisted of two light gray bars moving in opposite directions at constant speed (see also <xref ref-type="supplementary-material" rid="pcbi.1005546.s007">S2 Movie</xref>). The moving bars were embedded in dynamic visual noise, and on half of the blocks, a white noise click was presented at the time of the crossing of the moving bars.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.g001" xlink:type="simple"/>
</fig>
<p>Overall, participants perceived the stimulus as bouncing on 43% of the trial (CP: 48%; CG: 40%; VL: 42%). In line with previous studies, sounds significantly modulated participants’ responses and systematically biased the percept toward a bounce: only 27% of the trials without sound were perceived as bouncing, against 61% of the trials with sounds (<xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2C</xref>). Also, participants had a strong tendency towards interpreting the stimulus just as they did in the previous trial (<xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2C</xref>). Such a perceptual stability over time is a classic finding in the study of ambiguous displays [<xref ref-type="bibr" rid="pcbi.1005546.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref032">32</xref>], and it has recently been demonstrated also for the stream-bounce display [<xref ref-type="bibr" rid="pcbi.1005546.ref033">33</xref>]. This shows that perceptual disambiguation also relies on the combination of current sensory information with memory of recent perceptual interpretations [<xref ref-type="bibr" rid="pcbi.1005546.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref035">35</xref>]. However, it should be noted that this effect might be partially due to the fact that sound and no sound trials were presented in separate blocks, thus stabilizing the percept within each block.</p>
<fig id="pcbi.1005546.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005546.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Analyses and results.</title>
<p><bold>A</bold>. Reverse correlation analyses. Noisy stimuli were classified according to participants’ responses, and classification images were calculated from the mean (i.e., luminance) and the mean squared error (i.e., contrast) of each stimulus plus noise sample (see <xref ref-type="sec" rid="sec008">Methods</xref>). <bold>B</bold>. Luminance and contrast kernels for the aggregate observer. Warm colors represent samples positively associated to bounce responses, whereas cold colors represent samples negatively associated to a bounce response. <bold>C</bold>. Non-visual factors influencing participants’ responses for the aggregate observer. Errorbars represent the 99% confidence interval. See <xref ref-type="supplementary-material" rid="pcbi.1005546.s001">S1 Fig</xref> for individual observers’ data.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.g002" xlink:type="simple"/>
</fig>
<sec id="sec004">
<title>Reverse correlation analyses</title>
<p>As already mentioned, the random structure of the dynamic visual noise presented on each trial allows us to isolate the features of the stimulus that modulate the final percept. That is, due to its random fluctuations, noise can sometimes provide signal-like information that might be used by the observer to interpret the ambiguous display. If so, the statistics of the spatiotemporal noise patterns (<xref ref-type="fig" rid="pcbi.1005546.g001">Fig 1</xref>, center) of the stimuli classified as bouncing or streaming should differ. Such properties can be estimated through psychophysical reverse correlation techniques, also known as classification images [<xref ref-type="bibr" rid="pcbi.1005546.ref028">28</xref>]. To this end, we first calculated the mean (i.e., the average luminance of the noise) and the mean squared error (MSE, the squared-difference from the mean, which is a measure of the contrast energy of the noise) across trials classified one way or the other. We did this calculation for each cell in the space-time noisy stimulus matrix [<xref ref-type="bibr" rid="pcbi.1005546.ref029">29</xref>]. This procedure was performed separately for those trials classified as bouncing and streaming, to obtain the matrices representing the first (mean) and second order (MSE) statistical properties of the noise selectively associated to either percept (see <xref ref-type="sec" rid="sec008">Methods</xref>). The difference between the noise matrices for bounce and stream, known as classification images, represent the templates (or kernels) for perceptual decision making.</p>
<p>The luminance kernel (<xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2B</xref> left; see also <xref ref-type="supplementary-material" rid="pcbi.1005546.s001">S1 Fig</xref> for individual participants’ data) indicates how deviations from the mean luminance of each spatiotemporal stimulus sample are associated to the perception of a bounce (as opposed to a stream). The luminance kernel reveals a positive association between luminance along the spatiotemporal trajectory of the moving objects (especially the one moving rightward) and the perception of a bounce. In contrast, the luminance of pixels that do not correspond to the moving target (i.e., the points off the diagonals in <xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2B</xref>, left) should be darker than average for them to trigger more likely a bounce response. In other words, this indicates that light stimuli moving against a dark background are more likely perceived as bouncing (see [<xref ref-type="bibr" rid="pcbi.1005546.ref036">36</xref>]). Additionally, the luminance kernel has more energy prior to the intersection than after. This demonstrates that visual information before the crossing is especially important in determining the final percept. That is, it does not matter too much for the decision what occurs after the intersection, which makes sense if the perceptual decision is already made at the moment of the intersection.</p>
<p>Next, we analyzed the contrast-related (second order) statistical properties of the stimulus, which are known to modulate performance in both visual and audiovisual tasks [<xref ref-type="bibr" rid="pcbi.1005546.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref037">37</xref>]. To this end, we calculated the contrast kernel (MSE), representing how the contrast energy of each pixel is associated to a bounce as opposed to a stream response (<xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2B</xref>, right; see <xref ref-type="supplementary-material" rid="pcbi.1005546.s001">S1 Fig</xref> for individual participants’ data). The contrast kernel displays a striking similarity with the moving stimulus (<xref ref-type="fig" rid="pcbi.1005546.g001">Fig 1</xref>, left panel), with the contrast of the moving bars (especially the one moving rightward) positively associated to a bounce response. That is, high contrast moving bars are more likely perceived as bouncing. Like in the luminance kernel, the effect of contrast is higher before the intersection.</p>
<p>To phenomenologically demonstrate that these classification images do indeed represent the spatiotemporal stimulus pattern that constitute a stream or a bounce percept, we used the classification images to generate disambiguates stimuli (see <xref ref-type="supplementary-material" rid="pcbi.1005546.s009">S4 Movie</xref> and <xref ref-type="sec" rid="sec008">Methods</xref>). These displays clearly show that the spatiotemporal patterns of the classification images represent the templates for prototypical streaming or bouncing events, as the stimuli in the video are by-and-large devoid of the intrinsic ambiguity of the standard stream-bounce display (compare <xref ref-type="supplementary-material" rid="pcbi.1005546.s006">S1 Movie</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005546.s009">S4 Movie</xref>). This was corroborated by showing <xref ref-type="supplementary-material" rid="pcbi.1005546.s009">S4 Movie</xref> to a pool of 12 naïve observers, all of which classified the top stimulus as bouncing while the lower one as streaming.</p>
<p>Classification images can also be used to investigate whether sound alters early visual processing. Specifically, if concurrent acoustic stimuli modulate visual processing, the patterns emerging from the classification images for trials with and without sounds should display consistent differences. Therefore, we calculated classification images separately for trials with and without sounds (see <xref ref-type="supplementary-material" rid="pcbi.1005546.s002">S2 Fig</xref>, bottom left and middle panels; <xref ref-type="supplementary-material" rid="pcbi.1005546.s003">S3 Fig</xref>, bottom left and middle panels). Overall, auditory information had virtually no influence on the shape of the luminance and contrast kernels. This suggests independence between auditory and visual sensory processing [<xref ref-type="bibr" rid="pcbi.1005546.ref038">38</xref>]–at least as far as it concerns extracting information for perceptual disambiguation.</p>
<p>In a similar fashion, we assessed whether memory of recent perceptual interpretations influenced visual processing, i.e. whether decision on the trial back had any influence on the current decision. To do so, we separately calculated the classification images from trials following a “stream” and a “bounce” response (see <xref ref-type="supplementary-material" rid="pcbi.1005546.s002">S2</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005546.s003">S3</xref> Figs, right panels). Previous responses do not alter the patterns emerging from the classification images for lightness and contrast, hence arguing against an effect of perceptual memory on early sensory processing.</p>
<p>Both classification images display more energy in the rightward direction, and this effect seems rather consistent across participants (see <xref ref-type="supplementary-material" rid="pcbi.1005546.s001">S1 Fig</xref>). Perceptual anisotropies are well documented in motion perception [<xref ref-type="bibr" rid="pcbi.1005546.ref039">39</xref>], and in the present display they may arise either from an asymmetric allocation of visual attention to the moving bars, or leftward-rightward asymmetries in motion processing. As studying perceptual anisotropies is beyond the scope of the current study, we did not further investigate this serendipitous finding.</p>
</sec>
<sec id="sec005" sec-type="materials|methods">
<title>Modeling</title>
<p>A fundamental advantage in using psychophysical reverse correlation analyses with no explicit parametric manipulations of the visual stimuli is that they allow one to generate hypothesis a-posteriori, based on the patterns emerging from the classification images. In this case, reverse correlation analyses reveal that high contrast bars are more likely perceived as bouncing. However, it is not clear why contrast should modulate perception in such a systematic fashion. Studies in visual motion perception demonstrate that, due to the properties of visual motion detectors [<xref ref-type="bibr" rid="pcbi.1005546.ref040">40</xref>], the perception of motion critically depends on the contrast of the moving objects [<xref ref-type="bibr" rid="pcbi.1005546.ref041">41</xref>]. This seems to suggest that also visual perceptual disambiguation might rely on the basic filtering properties of motion detection mechanisms.</p>
<p>To gain insights into the role of motion detection in resolving ambiguity in the stream-bounce display, we used the motion energy model [<xref ref-type="bibr" rid="pcbi.1005546.ref040">40</xref>], a classic model of visual motion perception (see <xref ref-type="sec" rid="sec008">Methods</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005546.s004">S4 Fig</xref>). According to this model, humans detect motion and its direction based on a series of spatiotemporally oriented filters, whose output determines the amount of perceived motion and its direction. That is, such model detects what is known as motion energy, a quantity that jointly depends on the speed of motion and the contrast of the moving object with respect to the background. Notably, studies with random-dot kinematograms, demonstrated that the motion energy model can predict human and primate’s performance even when the motion signal was corrupted by external noise (i.e., motion coherence) [<xref ref-type="bibr" rid="pcbi.1005546.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref043">43</xref>]. <xref ref-type="fig" rid="pcbi.1005546.g003">Fig 3A</xref> shows the average motion energy profile of the noisy moving stimuli (see <xref ref-type="sec" rid="sec008">Methods</xref>). Different colors represent the direction of the motion (blue = rightward; red = leftward), and their saturation represents the amount of motion energy. Due to the crossing of the trajectories of the moving stimuli, there is no motion energy at the intersection. To better highlight this, we calculated the motion energy profile over time and space, by integrating the absolute motion energy matrix (i.e., discarding the direction of motion) over space (<xref ref-type="fig" rid="pcbi.1005546.g003">Fig 3A</xref>, right) and time (<xref ref-type="fig" rid="pcbi.1005546.g003">Fig 3A</xref>, top), respectively. The resulting motion profiles show a clear drop in motion energy at the intersection.</p>
<fig id="pcbi.1005546.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005546.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Motion energy analyses.</title>
<p><bold>A</bold>. Average motion energy matrix calculated from the noisy ambiguous displays (note that this is not a classification image). The plots above and to the right of the motion energy matrix represent the motion energy profile averaged over space and time, respectively. Note the drop in motion energy profiles at the intersection of the trajectories. The darkness of the lines represents the amount of total motion energy in the display (darker = more energy). To derive these plots we binned the displays in 5 groups depending on their total motion energy. The drop in motion energy, that is the difference between the maximum and the minimum motion energy of each noisy display, is linearly related in both space (<bold>B</bold>; see also <bold>A</bold>, top plot) and time (<bold>C</bold>; see also <bold>A</bold>, right plot) to the total motion energy–and hence to the contrast–of the display.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.g003" xlink:type="simple"/>
</fig>
<p>Such a drop in motion energy at the intersection might be crucial for perceptual disambiguation: ideally when two non-rigid moving objects collide, their velocity should briefly drop to zero, whereas this should not occur–or at least it should be less evident–in the absence of a collision. Assuming the brain to have knowledge about this generative model, we looked for the footprint of such a perceptual inference in the empirical data. Due to the noise added to the moving stimuli, there should be substantial trial-by-trial variability in the extent of the drop, which could systematically bias perceptual disambiguation. The empirical classification images (<xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2B</xref>) demonstrate that stimuli with high contrast are more likely perceived as bouncing. Therefore, given that contrast modulates motion perception [<xref ref-type="bibr" rid="pcbi.1005546.ref040">40</xref>], we looked at how changes in motion energy in the noisy stimuli modulated the energy drop at the intersection.</p>
<p>To do so, we calculated the absolute motion energy of each display. This was done by taking the absolute value of motion energy (<xref ref-type="supplementary-material" rid="pcbi.1005546.s004">S4 Fig</xref>, bottom) for each display, and averaging the results over both time and space. To better emphasize how the drop in motion energy changes with total motion energy, we binned the stimuli based on their total motion energy and examined how this affected the time-averaged and space-averaged motion energy profile. Results demonstrate that the peak in motion energy increases with increasing total motion energy, while, due to the design of the stimuli, the dip at the crossing remains relatively stable at near-zero motion energy (<xref ref-type="fig" rid="pcbi.1005546.g003">Fig 3A</xref>, top and right plots, darker lines represent more total motion energy). That is, the drop in perceived speed is more pronounced when the total motion energy of the display is higher. Notably, the extent of the drop–calculated as the difference in motion energy between the peak and the dip–is linearly related to the total motion energy of each display (<xref ref-type="fig" rid="pcbi.1005546.g003">Fig 3B and 3C</xref>; see also <xref ref-type="fig" rid="pcbi.1005546.g003">Fig 3A</xref>, note the luminance of the lines in the marginal plots), and hence to the contrast of the moving bars.</p>
<p>This result provides a first hint on the role of motion energy filters in perceptual disambiguation, however this modeling approach further makes a number of predictions that we systematically tested in the current study. First of all, we should be able to predict the percept given the noise pattern in each stimulus on a trial-by-trial basis. Second, if motion energy filters are indeed the underlying mechanisms for visual perceptual disambiguation, the model should also be able to replicate the empirical classification images. Third, given that motion energy filters are not sensitive to the luminance of the moving stimuli, but only to their contrast with respect to the background [<xref ref-type="bibr" rid="pcbi.1005546.ref040">40</xref>], we should find the same pattern of results even if the moving bars are darker than the background (rather than lighter, like in the current experiment). In the next sections we put the first two predictions to the test, while the third one was tested in a second experiment.</p>
<p>To computationally capture the sensory processes underlying perceptual inference in the stream-bounce display, we developed a simple perceptual classification model (<xref ref-type="fig" rid="pcbi.1005546.g004">Fig 4A</xref>). In the first stage, visual information for perceptual disambiguation is computed for each stimulus in terms of total motion energy. Given that in the present visual display the motion of the bars is identical in both directions, we simply summed the total absolute motion energy (<xref ref-type="supplementary-material" rid="pcbi.1005546.s004">S4 Fig</xref>, bottom) to compute, for each noisy stimulus, a measure of the overall motion energy (and hence of the extent of energy drop at the intersection), irrespective of direction (see <xref ref-type="sec" rid="sec008">Methods</xref>). The result of such motion energy computation is then combined with auditory information and recent perceptual memory into a single proxy variable representing the overall evidence towards one interpretation or the other. This is done in the following way: In line with current models of sensory integration [<xref ref-type="bibr" rid="pcbi.1005546.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref044">44</xref>], assuming neural noise to be independent across the channels and normally distributed, we modeled sensory integration of visual motion energy, auditory signals, and recent perceptual memory as weighed linear integration. To make the stream-bounce information provided by the different channels (motion energy, audio signal, memory) directly comparable, linear coefficients <italic>ω</italic><sub><italic>i</italic></sub> consisted of a combination of both weights and scaling factors [<xref ref-type="bibr" rid="pcbi.1005546.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref045">45</xref>]. As a consequence, unlike standard weighted averaging models of sensory integration (e.g., [<xref ref-type="bibr" rid="pcbi.1005546.ref044">44</xref>]), linear coefficients are not constrained to sum to one. This scaled and integrated information represents the evidence towards streaming or bouncing, and eventually determines the final percept. The predictive power of the model was assessed through a cross-validation procedure (see <xref ref-type="sec" rid="sec008">Methods</xref>). Overall, the model tightly reproduced the observed responses (<xref ref-type="fig" rid="pcbi.1005546.g004">Fig 4B</xref>, see <xref ref-type="supplementary-material" rid="pcbi.1005546.s001">S1 Fig</xref>, right column, for single observers’ data).</p>
<fig id="pcbi.1005546.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005546.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Perceptual classification model.</title>
<p><bold>A</bold>. Model. Visual motion energy is first computed from the stimuli through motion energy filters, and the result is linearly integrated with the auditory information and recent perceptual memory into a single estimate to determine the Z-score of streaming/bouncing responses. <bold>B</bold>. Scatterplot of empirical vs. predicted responses for the aggregate observer. Each dot is the average of 608 responses. Light red area represents the 99% confidence interval of the identity line. <bold>C</bold>. Luminance and contrast kernels calculated from the model responses. <bold>D</bold>. Cross-correlation between predicted and empirical kernels. The red lines represent the thresholds for statistical significance (p = 0.05) as calculated based on the permutation test.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.g004" xlink:type="simple"/>
</fig>
<p>To further validate the current model, and to test whether motion energy is indeed the primary visual cue underlying perceptual disambiguation in the stream-bounce display, we calculated the classification images for both luminance and contrast based on the responses of the model. This was done using the classification responses provided by the model (see before). The results (<xref ref-type="fig" rid="pcbi.1005546.g004">Fig 4C</xref>) show a remarkable similarity with the empirical classification images (<xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2B</xref>) and this was true for both the luminance and the contrast kernels. Notably, the model could even replicate the fine details of the empirical classification images, including the regular alternation of positive and negative peaks that we found in the original data (compare <xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2B</xref> and <xref ref-type="fig" rid="pcbi.1005546.g004">Fig 4C</xref>). The main difference between predicted and empirical kernels is that the empirical classification images are rather asymmetric, and assign more weight before the crossing. To formally quantify such a similarity, we computed the normalized pixel-by-pixel correlation between the empirical and the predicted classification images for both luminance (ρ = 0.71) and contrast (ρ = 0.69, <xref ref-type="fig" rid="pcbi.1005546.g004">Fig 4D</xref>). These correlations were highly significant, as assessed using a permutation test whereby we randomly permuted the value of each sample of the classification images over 20,000 iterations. Notably, a simpler model that only responds to contrast (but not to motion), was not sufficient to replicate the current findings (see <xref ref-type="sec" rid="sec008">Methods</xref>).</p>
<p>It is important to note that this model also reveals why the manipulation of the timing and luminance of the moving stimuli in the study by Zhou and colleagues [<xref ref-type="bibr" rid="pcbi.1005546.ref021">21</xref>] modulated perceptual disambiguation in such a systematic fashion. This is because both manipulations varied the motion energy drop of the stimuli at the intersection.</p>
</sec>
</sec>
<sec id="sec006">
<title>Experiment 2</title>
<p>The responses of motion energy filters are strongly modulated by the contrast of the moving object, while being relatively insensitive to luminance. Therefore, a critical test for the role of motion energy filters in disambiguating the stream-bounce display would be to invert the luminance polarity of the moving bars with respect to the background, while keeping their contrast constant. If motion energy computation is indeed the underlying mechanism for perceptual disambiguation, it should not matter whether the moving bars are lighter than the background (like in the previous experiment) or darker. Rather, what would matter should be the amount of the drop of motion energy at the intersection, which correlates with the total motion energy in the display. To directly test this hypothesis, we generated stimuli analogous to the ones used in the previous experiment, but with an additional modulation of both the total motion energy and the luminance polarity of the moving bars (<xref ref-type="fig" rid="pcbi.1005546.g005">Fig 5A</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005546.s008">S3 Movie</xref>, see <xref ref-type="sec" rid="sec008">Methods</xref> for further details). Next, we run a psychophysical task where we asked participants to classify such displays as streaming or bouncing, with the hypothesis that stimuli with high motion energy, and hence with a large drop of motion energy at the intersection, should be more likely classified as bouncing, irrespective of the luminance polarity of the moving bars.</p>
<fig id="pcbi.1005546.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005546.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Experiment 2.</title>
<p><bold>A</bold>. Luminance and contrast kernels estimated from the motion energy model for light (top) and dark (bottom) moving bars. <bold>B</bold>. Results of Experiment 2. The bars represent the probability of responding bounce for high (HI) and low (LO) motion energy drop and for light and dark bars. Errorbars represent the standard error of the mean. <bold>C</bold>. Scatterplot and bagplot of the probability of responding bounce for stimuli with high and low motion energy drop. Thin dashed lines connect data from the same participants in the two lightness conditions. The red cross represents the depth median.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.g005" xlink:type="simple"/>
</fig>
<p>As hypothesized, displays with a large drop in motion energy (i.e., high motion energy) were systematically classified as bouncing more often than those with a smaller drop (i.e., low motion energy, <xref ref-type="fig" rid="pcbi.1005546.g005">Fig 5B</xref>), whereas luminance did not significantly affect participants’ responses. Notably, the magnitude of the effect of motion energy drop on perceptual disambiguation was comparable to the effect of sound presence/absence in Experiment 1 (compare <xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2C</xref> left, and <xref ref-type="fig" rid="pcbi.1005546.g005">Fig 5B</xref>), and it was highly consistent across participants (<xref ref-type="fig" rid="pcbi.1005546.g005">Fig 5C</xref>). This result demonstrates that motion energy, and its drop at the intersection, is indeed the key visual factor driving perceptual disambiguation, and further validates the current classification model.</p>
</sec>
</sec>
<sec id="sec007" sec-type="conclusions">
<title>Discussion</title>
<p>The mechanisms underlying perceptual disambiguation are a central topic in sensory neuroscience. Resolving ambiguity requires both extracting and combining sensory information. The present results demonstrate which cues are relevant to resolve perceptual ambiguity in the stream-bounce display, and highlight the mechanisms underlying both the computation and the combination of such multisensory cues for the perception of dynamic ambiguous displays. Previous research has already investigated the stimulus properties biasing the interpretation of the stream-bounce display (e.g., [<xref ref-type="bibr" rid="pcbi.1005546.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref046">46</xref>]). Such studies relied on a parametric manipulation of one or more stimulus features. However, this approach requires researchers to decide a priori which features might be relevant to solve the task. The key advantage of using noisy stimuli and reverse correlation analyses relies on the absence of any prior assumptions, which allows us to determine a-posteriori how random fluctuations in the (external) noise systematically modulate participants’ responses. This, in turn, allows gathering detailed information about the precise spatiotemporal features buried in the stimuli that underlie perceptual disambiguation. Such key features and their relative contribution to perceptual decision making can only be obtained using standard psychophysical procedures by lucky guessing, and it is never clear whether some key features have been missed. An example from this study is the importance for disambiguation of motion energy, and its drop at the intersection, which might not have been discovered with traditional psychophysical methods. What is more, reverse correlation analyses revealed that sound does not alter early visual processing; rather, it modulates the percept after the unimodal information for disambiguation has been independently extracted from all modalities.</p>
<p>Over the last decade, multisensory integration has often been modeled in terms Bayesian decision theory. Empirical results demonstrate that the brain operates in a statistically optimal way by maximizing the accuracy and precision of combined sensory estimates when integrating redundant and unambiguous sensory information [<xref ref-type="bibr" rid="pcbi.1005546.ref030">30</xref>]. Before integrating multisensory information for perceptual disambiguation, however, the brain needs to transform sensory information into a common format to make it directly comparable. That is, the different signals should be separately processed to extract stream-bounce information (i.e., probability of impact) from the continuous stream of sensory signals. Here, we modeled this transformation in terms of motion energy filters [<xref ref-type="bibr" rid="pcbi.1005546.ref040">40</xref>], which transforms complex, dynamic visual information into proxy decision variables that represent the strength of sensory evidence. Once transformed into a common format, sensory evidence from vision and audition can be directly compared and integrated by weighted averaging. A simple linear integration model (without interactions across the cues) captures human perception with a high degree of accuracy [<xref ref-type="bibr" rid="pcbi.1005546.ref036">36</xref>]. This result demonstrates that the brain applies analogous integration principles for disambiguation as it does for integrating redundant information [<xref ref-type="bibr" rid="pcbi.1005546.ref044">44</xref>]. What is more, the close correspondence between the empirical classification images and those predicted based on the motion energy model, further supports the motion energy model itself.</p>
<p>A pressing question in the study of perceptual ambiguities concerns the conditions or parameters that drive perceptual biases. That is, why on each trial a given interpretation is chosen over the competing one. Internal noise has often been advocated to explain perceptual alternation [<xref ref-type="bibr" rid="pcbi.1005546.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1005546.ref024">24</xref>]. Namely, noise was advocated as causing perceptual switches during prolonged presentations of bistable stimuli, like in binocular rivalry. However, to date we still do not know exactly which spatiotemporal (i.e., “signal-like”) patterns within the noise selectively drive perceptual disambiguation between two equally valid alternatives. By embedding the stimulus in external noise with known properties, and using reverse correlation analyses, this study demonstrates not only that noise is indeed the key element driving alternation, but also which pixel-by-pixel properties of the noise are systematically associated to each interpretation. More specifically, due to its random structure, noise often contains information that is used in the process of resolving ambiguity. Here, we characterize what such properties are in the case of the stream-bounce display, and how they get extracted though spatiotemporal visual filters. Notably, the existence of systematic links between low-level stimulus properties and perceptual responses–as measured through reverse correlation analyses–argues against interpretations of disambiguation purely in terms of attention or response biases [<xref ref-type="bibr" rid="pcbi.1005546.ref047">47</xref>–<xref ref-type="bibr" rid="pcbi.1005546.ref050">50</xref>].</p>
<p>In recent years, the stream-bounce display has often been used to investigate the neural underpinnings of multisensory integration. The main findings support the involvement of multimodal cortical regions [<xref ref-type="bibr" rid="pcbi.1005546.ref051">51</xref>] and large-scale synchronizations of oscillatory neural activity [<xref ref-type="bibr" rid="pcbi.1005546.ref052">52</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref053">53</xref>] in resolving multisensory perceptual ambiguity. However, the computational principles underlying the multi-stability of the stream-bounce display remained poorly understood. The current results fill this important gap and provide evidence for the fundamental role of motion energy computation and linear integration of evidence in multisensory perceptual disambiguation.</p>
</sec>
<sec id="sec008" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec009">
<title>Psychophysical experiment</title>
<p>Three participants (2 naïve females, VL and CG, and one male, the author CP) took part in Experiment 1. All participants were right handed and had normal or corrected to normal vision and audition. Participants sat in front of a computer screen with their head constrained by a chin and headrest. Each trial started with the presentation of a red fixation cross at the center of the screen (600ms), after which the visual stimulus appeared, consisting of two light vertical bars (0.085° x 0.426° each) moving in opposite direction and embedded in dynamic visual noise. The dynamic stimulus comprised 20 frames (60Hz screen, overall duration 333ms) of uni-dimensional visual noise consisting of 20 vertical bars (0.085° x 0.426° each) with random luminance. The luminance of each noise sample varied between 14.6 cd/m<sup>2</sup> and 48.3 cd/m<sup>2</sup>. The moving visual stimuli were defined by a 50 cd/m<sup>2</sup> luminance increment. The stimuli used in Experiment 1 are contained in <xref ref-type="supplementary-material" rid="pcbi.1005546.s010">S1 Dataset</xref>. On half of the blocks, a 16ms white noise auditory click was played from two speakers flanking the screen when the two moving stimuli met at the center of the screen. Participants were informed about the presence or absence of the sound at the beginning of each block. Such a blocked design was necessary as in preliminary observations we found that when sound and no-sound trials alternated randomly, participants’ responses were almost-exclusively determined by the presence/absence of the sound. Therefore, a blocked design made it easier to empirically estimate visual classification images.</p>
<p>The relatively small size and short duration of the stimuli were selected to discourage eye-movements. Observers’ task was to look at the stimuli without moving their eyes, and to report by a key-press whether they perceived the stimuli as bouncing or streaming through each other. Participants were explicitly told that there was no correct or wrong answer. The experiment was performed in a dark anechoic chamber and it was controlled by a custom-built software based on the Psychtoolbox 3 [<xref ref-type="bibr" rid="pcbi.1005546.ref054">54</xref>]. Experiment 1 consisted of ~10,000 trials per participant (CP: 10240 trials; CG: 10320; VL: 9840 trials). Psychophysical data is contained in <xref ref-type="supplementary-material" rid="pcbi.1005546.s011">S2 Dataset</xref>. Sound significantly modulated the percept (overall: χ<sup>2</sup> = 3404; p&lt;0.001; CP: χ<sup>2</sup> = 584; p&lt;0.001; CG: χ<sup>2</sup> = 1503; p&lt;0.001; VL: χ<sup>2</sup> = 1491; p&lt;0.001): only 27% (CP: 36%; CG: 21%; VL: 23%) of the trials without sound were perceived as bouncing, against 61% (CP: 60%; CG: 59%; VL: 62%) of the trials with sounds (<xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2C</xref>). Also, participants had a strong bias towards interpreting the stimulus just as they did in the previous trial (overall χ<sup>2</sup> = 2693; p&lt;0.001; CP: χ<sup>2</sup> = 531; p&lt;0.001; CG: χ<sup>2</sup> = 1818; p&lt;0.001; VL: χ<sup>2</sup> = 547; p&lt;0.001). Given that we were interested on the effects of the previous response, the first trial of each block (of 40 trials) was discarded from these analyses.</p>
<p>In Experiment 2 we used the motion energy model to generate noisy stream-bounce displays–with dark and light moving bars–with a parametric manipulation of motion energy (and hence of motion energy drop, see <xref ref-type="supplementary-material" rid="pcbi.1005546.s008">S3 Movie</xref>). This was achieved by first calculating classification images based on total motion energy for noisy visual stimuli with both light and dark moving bars. To estimate the classification images for motion energy, we generated a series of 400,000 noisy stimuli like in the previous experiment, with the only difference that the moving bars could be either lighter or darker than the background. Such stimuli were then fed into the motion energy model (see below) to calculate their total motion energy (i.e., the sum of rightward and leftward motion energy), and finally we discretized the models’ response by classifying the 50% of the stimuli with higher motion energy as bounce and the remaining ones as streaming. This procedure was separately performed for stimuli with light and dark bars. Classification images were then calculated following the same procedure used in Experiment 1. The resulting classification images looked similar to those of Experiment 1 (<xref ref-type="fig" rid="pcbi.1005546.g004">Fig 4A</xref>), however, the luminance kernels for light and dark bars had opposite polarities.</p>
<p>Such classification images were then used to create stream-bounce displays with high or low motion energy (<xref ref-type="supplementary-material" rid="pcbi.1005546.s008">S3 Movie</xref>). First off, we generated a series of stimuli similar to the ones used in Experiment 1, but again the moving bars could be either lighter or darker than the background. Then, we experimentally manipulated the amount of motion energy by either adding or subtracting the luminance kernels to obtain displays with high or low motion energy, respectively. This procedure was separately performed for light and dark bars using their respective classification images. Such manipulated displays were then used in a psychophysical classification task in Experiment 2.</p>
<p>Overall, the task was very similar to Experiment 1. However, in Experiment 2 we did not play any sounds. Visual stimuli with high and low motion energy randomly alternated across trials, while light and dark bars were presented in separate blocks of 16 trials each. In total, the experiment consisted of 126 trials. That is, 32 trials for each of the four combinations of bars’ luminance (light vs. dark) and total motion energy (high vs. low). Ten naïve participants (7 females) took part in Experiment 2. Compared to Experiment 1, the larger number of participants in Experiment 2 was due to the fact that in the latter experiment each participant performed a much smaller number of trials (126 vs. ~10,000 trials). Before starting the experiment, all participants underwent a short practice session to familiarize with the stimuli and the task.</p>
<p>The probability of reporting a bounce for each condition and participant was normalized through a Z-score transformation and was analyzed using a 2x2 repeated-measures ANOVA with motion energy and luminance as within-participants factors. Motion energy strongly modulated participants’ responses (F(1,9) = 35.489, p&lt;0.001), with no effects of luminance (F(1,9) = 2.317, p = 0.162) and no interactions (F(1,9) = 1.036, p = 0.335).</p>
<p>This study was conducted in accordance with the Declaration of Helsinki and the experiments had ethical approval from the ethics committee of the University of Tübingen.</p>
</sec>
<sec id="sec010">
<title>Reverse correlation analyses</title>
<p>To calculate visual classification images we first sorted the noisy visual stimuli presented in the experiment according to participants’ (or model’s) classification responses (stream vs. bounce). For each class, we calculated the mean luminance (<italic>μ</italic>) and contrast (mean squared error, <italic>MSE</italic>) of the noisy visual stimuli and we combined them as follows to obtain the classification images for visual luminance (<italic>K</italic><sub><italic>L</italic></sub>) and contrast (<italic>K</italic><sub><italic>C</italic></sub>):
<disp-formula id="pcbi.1005546.e001">
<alternatives>
<graphic id="pcbi.1005546.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005546.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
<disp-formula id="pcbi.1005546.e002">
<alternatives>
<graphic id="pcbi.1005546.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005546.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
Where <italic>μ</italic><sub>[<italic>R</italic>]</sub> and <italic>MSE</italic><sub>[<italic>R</italic>]</sub> are the mean and the mean squared error templates for the stimuli <italic>S</italic><sub>[<italic>R</italic>]</sub>, respectively. <italic>R</italic> denotes participants’ responses. Visual classification images (<italic>K</italic><sub><italic>L</italic></sub>, <italic>K</italic><sub><italic>C</italic></sub>) were smoothed by convolution with a low-pass spatiotemporal filter of the form [0.49, 0.7, 0.49; 0.70, 1.0, 0.70; 0.49, 0.7, 0.49] [<xref ref-type="bibr" rid="pcbi.1005546.ref055">55</xref>]. Finally, all classification images were range-scaled so that their maximum absolute values equal to 1.</p>
<p>Classification images were calculated individually for each participant and on the aggregate observer obtained by combining data from all participants.</p>
</sec>
<sec id="sec011">
<title>Creating unambiguous stream-bounce displays</title>
<p>As a proof-of-principle to demonstrate that the classification images really represent the templates of prototypical streaming or bouncing events, we reverse engineered the stimuli, and used the classification images to create unambiguous stimuli. To do so, we first generated noisy stimuli like in Experiment 1. Next, we added or subtracted the empirical luminance kernel of the aggregate observer of Experiment 1 (<xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2B</xref>) to modulate luminance over time and space, and hence to generate disambiguated ‘bouncing’ and ‘streaming’ stimuli, respectively. The resulting stimuli (<xref ref-type="supplementary-material" rid="pcbi.1005546.s009">S4 Movie</xref>) provide a striking example of unambiguous dynamic displays: the “bouncing” stimulus (<xref ref-type="supplementary-material" rid="pcbi.1005546.s009">S4 Movie</xref>, top) is most likely perceived as bouncing, while the “streaming” stimulus (<xref ref-type="supplementary-material" rid="pcbi.1005546.s009">S4 Movie</xref>, bottom) is mostly seen as streaming. This was corroborated by showing the video to a pool of 12 naïve participants (3 female) and asking them which of the two stimuli appeared to be streaming and which bouncing. As expected, all participants indicated the top stimulus as bouncing and the lower one as streaming. This further corroborates the validity of the present reverse correlation analyses, and phenomenologically demonstrates that the empirical classification images do indeed represent the templates for prototypical streaming or bouncing events.</p>
</sec>
<sec id="sec012" sec-type="materials|methods">
<title>Modeling</title>
<p>The extraction of visual sensory evidence <italic>E</italic><sub><italic>v</italic></sub> for perceptual classification is modeled in terms of total motion energy (see below). In order to keep the model simple and because characterizing early stages of sensory processing is beyond the scope of the current study, we assumed the stimuli to be linearly transduced.</p>
<p>The integration of task-relevant information <italic>E</italic> was modeled in terms of weighted linear summation of the form:
<disp-formula id="pcbi.1005546.e003">
<alternatives>
<graphic id="pcbi.1005546.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005546.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mstyle displaystyle="false"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where <italic>ω</italic><sub><italic>i</italic></sub> denotes the linear coefficient (i.e., the weight), <italic>E</italic><sub><italic>i</italic></sub> the evidence, <italic>ω</italic><sub>0</sub> the bias (i.e., the decision criterion), and the subscripts <italic>i</italic> the source of the evidence (i.e., visual motion energy, auditory click, previous response). An assumption of this model is that the internal noise for each task-relevant evidence <italic>E</italic><sub><italic>i</italic></sub> is independent and normally distributed.</p>
<p>Such a linear model has one covariate <italic>E</italic><sub><italic>v</italic></sub> corresponding to the evidence from visual motion energy and two factors <italic>E</italic><sub><italic>A</italic></sub> and <italic>E</italic><sub><italic>R</italic>(<italic>t</italic>−1)</sub> denoting the presence or absence of a sound and the response given on the previous trial. The coefficients of the model were fitted individually for each participant and for the aggregate observer using the Matlab routine <italic>glmfit</italic>. Given that we were interested in understanding the effect of the previous trials on subsequent responses, the first trial of each block was not used for modeling purposes.</p>
<p>The model was validated using a 39-fold cross-validation procedure (for both individual and aggregate observers). The training set was used on each iteration to fit the coefficients of the model. Next, we fed the stimuli of the test set into the model and we compared the response of the model to the empirical data. Each trial was included in the test set in only one iteration. To evaluate how well the model could reproduce the observed responses, we partitioned all trials in 50 bins according to the model response <italic>Z</italic>(<italic>bounce</italic>) in the test set of the cross-validation. The predicted probability of reporting a bounce on each trial, <italic>p</italic>(<italic>bounce</italic>), was calculated from the models’ response using the following equation:
<disp-formula id="pcbi.1005546.e004">
<alternatives>
<graphic id="pcbi.1005546.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005546.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>[</mml:mo><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>]</mml:mo>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
where Φ[∙] is the cumulative normal distribution function. For each bin, we also calculated the observed probability of reporting a bounce and we plotted predicted vs. observed responses (<xref ref-type="fig" rid="pcbi.1005546.g004">Fig 4B</xref>, <xref ref-type="supplementary-material" rid="pcbi.1005546.s001">S1 Fig</xref> right column). If the model accurately captures participants’ responses, data should lie along the identity line. The 99% confidence interval along the identity line was calculated based on the binomial distribution and the number of responses of each bin. Overall, the model could replicate observed responses with high accuracy.</p>
<p>The current linear integration model was compared to alternative models generated by taking only a subset of the three predictors (i.e., motion energy, sound, or previous response) or by also including interaction terms. The selected model outperformed all such alternatives, as assessed in terms of Akaike information criterion.</p>
</sec>
<sec id="sec013">
<title>Motion energy model</title>
<p>The motion energy model [<xref ref-type="bibr" rid="pcbi.1005546.ref040">40</xref>] is a classic and biologically plausible model of visual motion detection based on the combination of a series of spatiotemporally tuned filters. Although a full description and rationale of the motion energy model can be found elsewhere (e.g., [<xref ref-type="bibr" rid="pcbi.1005546.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1005546.ref056">56</xref>], here we briefly describe its main features and provide the equations of the spatial and temporal filters. In the current study we implemented a recent version (including the values of the relevant parameters) of the motion energy model [<xref ref-type="bibr" rid="pcbi.1005546.ref056">56</xref>]. Motion filters had the same spatial and temporal extent as the visual stimuli and they were sampled at the same spatial and temporal resolution. The spatial filters consisted of even (E) and odd (O) Gabor functions:
<disp-formula id="pcbi.1005546.e005">
<alternatives>
<graphic id="pcbi.1005546.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005546.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>∙</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
<disp-formula id="pcbi.1005546.e006">
<alternatives>
<graphic id="pcbi.1005546.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005546.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>∙</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula></p>
<p>The spatial constant <italic>σ</italic> was 0.5 deg and its spatial frequency <italic>f</italic> was 1.1 cpd. Such values approximate the spatial sensitivity of the magnocellular system [<xref ref-type="bibr" rid="pcbi.1005546.ref057">57</xref>].</p>
<p>Temporal filters were defined by the following equation:
<disp-formula id="pcbi.1005546.e007">
<alternatives>
<graphic id="pcbi.1005546.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005546.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>∙</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>∙</mml:mo><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>∕</mml:mo><mml:mi>n</mml:mi><mml:mo>!</mml:mo><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>∕</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow><mml:mo>]</mml:mo>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula></p>
<p>The parameter <italic>k</italic> represents the center temporal frequency of the filter and its value was set to 100. The parameter <italic>n</italic> represents the temporal constant of the filter and its value was set to 9 for the slow temporal filter and 6 for the fast one. The parameter <italic>β</italic> represents the weighting of the negative relative to the positive phase of the filter and its value was set to 0.9.</p>
<p>The model also includes a normalization step. A graphical representation of the full model is displayed in <xref ref-type="supplementary-material" rid="pcbi.1005546.s004">S4 Fig</xref> and a Matlab implementation of the model is available online (<ext-link ext-link-type="uri" xlink:href="http://www.georgemather.com/Code/AdelsonBergen.m" xlink:type="simple">http://www.georgemather.com/Code/AdelsonBergen.m</ext-link>).</p>
<p>In the current study, the total motion energy of each stimulus was calculated. The motion energy matrix displayed in <xref ref-type="fig" rid="pcbi.1005546.g003">Fig 3</xref> was obtained by computing the motion energy matrices from all the stimuli presented in the experiment, and averaging the results.</p>
</sec>
<sec id="sec014">
<title>Alternative model</title>
<p>Given that the results of Experiment 1 point to the role of contrast in perceptual disambiguation, we assessed whether a simpler model which is sensitive to contrast but not to motion is sufficient to explain the current results. For this, we implemented a model consisting of two biphasic spatial filters in quadrature pair with a Gaussian temporal profile. Such a model has been adopted, and fully described, in a related study which also relied on reverse correlation analyses [<xref ref-type="bibr" rid="pcbi.1005546.ref029">29</xref>]. The spatial filters (and the values of the relevant parameters) of this model are identical to those of the energy model presented here, and based on [<xref ref-type="bibr" rid="pcbi.1005546.ref029">29</xref>] we set standard deviation of the Gaussian temporal filter to 40ms. We used this alternative model to calculate the predicted classification image, just as we did for the motion energy model. <xref ref-type="supplementary-material" rid="pcbi.1005546.s005">S5 Fig</xref> shows that this simpler model, which does not respond to motion, simply cannot account for human performance.</p>
</sec>
</sec>
<sec id="sec015">
<title>Supporting information</title>
<supplementary-material id="pcbi.1005546.s001" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Single observer analyses.</title>
<p>Results of Experiment 1 for each individual observer (from top CP, CG, VL) and for the aggregate observer (bottom). See <xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2</xref> for further details.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005546.s002" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Effect of noise and previous response on the luminance kernel.</title>
<p>Classification images for luminance calculated separately for sound presence/absence and for trials following a stream/bounce response (aggregate observer) in Experiment 1. The bottom-right panel corresponds to the classification image presented in <xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2</xref>.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005546.s003" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Effect of noise and previous response on the contrast kernel.</title>
<p>Classification images for contrast calculated separately for sound presence/absence and for trials following a stream/bounce response (aggregate observer) in Experiment 1. The bottom-right panel corresponds to the classification image presented in <xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2</xref>.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005546.s004" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>The motion energy model.</title>
<p>The stimulus matrix is convolved by a series of spatiotemporally tuned filters, whose output are combined, squared and normalized to calculate leftward and rightward motion energy. Rightward and leftward energy matrices are subtracted to compute the opponent energy matrix (see <xref ref-type="fig" rid="pcbi.1005546.g003">Fig 3</xref>).</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005546.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Alternative model.</title>
<p>Luminance and contrast kernels calculated from the alternative model. This model is sensitive to contrast but not to motion, and it is unable to replicate the empirical classification images (see <xref ref-type="fig" rid="pcbi.1005546.g002">Fig 2B</xref>).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005546.s006" mimetype="video/x-msvideo" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.s006" xlink:type="simple">
<label>S1 Movie</label>
<caption>
<title>Stream-bounce display.</title>
<p>Switch the audio volume on and off to assess the effect of sound on perception; we recommend using VLC player.</p>
<p>(AVI)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005546.s007" mimetype="video/x-msvideo" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.s007" xlink:type="simple">
<label>S2 Movie</label>
<caption>
<title>Stimuli used in Experiment 1.</title>
<p>A complete description of how the stimuli were generated can be found in <xref ref-type="fig" rid="pcbi.1005546.g001">Fig 1</xref>. Switch the audio volume on and off to assess the effect of sound on perception; we recommend using VLC player.</p>
<p>(AVI)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005546.s008" mimetype="video/x-msvideo" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.s008" xlink:type="simple">
<label>S3 Movie</label>
<caption>
<title>Stimuli used in Experiment 2.</title>
<p>The upper display has more motion energy than the lower display. Dark and light moving bars alternate in the movie. To better appreciate the effect of motion energy on perception, it is recommended to look at one display at a time (either the upper or lower one), while covering the other: the upper display should appear to bounce more often than the lower display. We recommend using VLC player.</p>
<p>(AVI)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005546.s009" mimetype="video/x-msvideo" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.s009" xlink:type="simple">
<label>S4 Movie</label>
<caption>
<title>Disambiguated stimuli.</title>
<p>To better appreciate the effect, it is recommended to look at one display at a time (either the upper or lower one), while covering the other: the upper display should appear to bounce more often than the lower display. We recommend using VLC player.</p>
<p>(AVI)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005546.s010" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.s010" xlink:type="simple">
<label>S1 Dataset</label>
<caption>
<title>Stimuli.</title>
<p>Each layer of this 3D matrix contains the time-space luminance diagram of the stimuli presented in each trials (e.g., see <xref ref-type="fig" rid="pcbi.1005546.g001">Fig 1</xref> bottom-right). Each row represents one video frame (first row is the first frame). Columns represent the spatial location of each element in the display (first column is left). The different layers represent different trials, and they correspond to the rows of the data matrix. Each cell in the 3D matrix represents the luminance of each element in the display (cd/m<sup>2</sup>). Data is available as a Matlab MAT-file.</p>
<p>(MAT)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005546.s011" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005546.s011" xlink:type="simple">
<label>S2 Dataset</label>
<caption>
<title>Data matrix.</title>
<p>The first column contains the ID of the observer (CP = 1; CG = 2; VL = 3). The second column indicates the presence/absence of the sound (1 = present). The third column contains the response of the previous trial (1 = bounce). The fourth column indicates the order of the trial within each block (given that we were interested in the effects of the previous response, the first trial of each block is not included). The fifth column contains the response (1 = bounce). The sixth trial contains the reaction time (in seconds). Data is available as a Matlab MAT-file.</p>
<p>(MAT)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We would like to express our gratitude to Loes van Dam, Irene Senna, and Marieke Rohde for insightful comments on an earlier version of this manuscript and for helpful discussion throughout.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005546.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Metzger</surname> <given-names>W</given-names></name>. <article-title>Beobachtungen über phänomenale Identität (Observations about phenomenal identity)</article-title>. <source>Psychologische Forschung</source>. <year>1934</year>;<volume>19</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>60</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005546.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Necker</surname> <given-names>L</given-names></name>. <article-title>Observations on some remarkable phenomenon which occurs in viewing a figure of a crystal or geometrical solid</article-title>. <source>London and Edinburgh Philosophical Magazine and Journal of Science</source> <year>1832</year>;<volume>3</volume>:<fpage>329</fpage>–<lpage>37</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005546.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yellott</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kaiwi</surname> <given-names>J</given-names></name>. <article-title>Depth inversion despite stereopsis: The appearance of random-dot stereograms on surfaces seen in reverse perspective</article-title>. <source>Perception</source>. <year>1979</year>; <volume>8</volume>:<fpage>135</fpage>–<lpage>42</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1068/p080135" xlink:type="simple">10.1068/p080135</ext-link></comment> <object-id pub-id-type="pmid">471677</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref004"><label>4</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Alais</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Blake</surname> <given-names>R</given-names></name>. <source>Binocular rivalry</source>: <publisher-name>MIT press</publisher-name>; <year>2005</year>.</mixed-citation></ref>
<ref id="pcbi.1005546.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sekuler</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sekuler</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>R</given-names></name>. <article-title>Sound alters visual motion perception</article-title>. <source>Nature</source>. <year>1997</year>;<volume>385</volume>(<issue>6614</issue>):<fpage>308</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/385308a0" xlink:type="simple">10.1038/385308a0</ext-link></comment> <object-id pub-id-type="pmid">9002513</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Di Luca</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Backus</surname> <given-names>B</given-names></name>. <article-title>Learning to use an invisible visual signal for perception</article-title>. <source>Current Biology</source>. <year>2010</year>;<volume>20</volume>(<issue>20</issue>):<fpage>1860</fpage>–<lpage>3</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2010.09.047" xlink:type="simple">10.1016/j.cub.2010.09.047</ext-link></comment> <object-id pub-id-type="pmid">20933421</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haijiang</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>Saunders</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Stone</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Backus</surname> <given-names>BT</given-names></name>. <article-title>Demonstration of cue recruitment: Change in visual appearance by means of Pavlovian conditioning</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2006</year>;<volume>103</volume>(<issue>2</issue>):<fpage>483</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0506728103" xlink:type="simple">10.1073/pnas.0506728103</ext-link></comment> <object-id pub-id-type="pmid">16387858</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wolfe</surname> <given-names>JM</given-names></name>. <article-title>Reversing ocular dominance and suppression in a single flash</article-title>. <source>Vision research</source>. <year>1984</year>;<volume>24</volume>(<issue>5</issue>):<fpage>471</fpage>–<lpage>8</lpage>. <object-id pub-id-type="pmid">6740966</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brascamp</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Knapen</surname> <given-names>TH</given-names></name>, <name name-style="western"><surname>Kanai</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Van Ee</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Van Den Berg</surname> <given-names>AV</given-names></name>. <article-title>Flash suppression and flash facilitation in binocular rivalry</article-title>. <source>Journal of Vision</source>. <year>2007</year>;<volume>7</volume>(<issue>12</issue>):<fpage>12</fpage>-. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/7.12.12" xlink:type="simple">10.1167/7.12.12</ext-link></comment> <object-id pub-id-type="pmid">17997654</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leopold</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Wilke</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Maier</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>. <article-title>Stable perception of visually ambiguous patterns</article-title>. <source>Nature Neuroscience</source>. <year>2002</year>;<volume>5</volume>(<issue>6</issue>):<fpage>605</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn851" xlink:type="simple">10.1038/nn851</ext-link></comment> <object-id pub-id-type="pmid">11992115</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Orbach</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ehrlich</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Heath</surname> <given-names>HA</given-names></name>. <article-title>Reversibility of the Necker cube: I. An examination of the concept of “satiation of orientation”</article-title>. <source>Perceptual and motor skills</source>. <year>1963</year>;<volume>17</volume>(<issue>2</issue>):<fpage>439</fpage>–<lpage>58</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005546.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pastukhov</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Braun</surname> <given-names>J</given-names></name>. <article-title>A short-term memory of multi-stable perception</article-title>. <source>Journal of Vision</source>. <year>2008</year>;<volume>8</volume>(<issue>13</issue>):<fpage>7</fpage>-. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/8.13.7" xlink:type="simple">10.1167/8.13.7</ext-link></comment> <object-id pub-id-type="pmid">19146337</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pearson</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Brascamp</surname> <given-names>J</given-names></name>. <article-title>Sensory memory for ambiguous vision</article-title>. <source>Trends in cognitive sciences</source>. <year>2008</year>;<volume>12</volume>(<issue>9</issue>):<fpage>334</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2008.05.006" xlink:type="simple">10.1016/j.tics.2008.05.006</ext-link></comment> <object-id pub-id-type="pmid">18684661</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brascamp</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Knapen</surname> <given-names>TH</given-names></name>, <name name-style="western"><surname>Kanai</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Noest</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Van Ee</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Van Den Berg</surname> <given-names>AV</given-names></name>. <article-title>Multi-timescale perceptual history resolves visual ambiguity</article-title>. <source>PloS one</source>. <year>2008</year>;<volume>3</volume>(<issue>1</issue>):<fpage>e1497</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0001497" xlink:type="simple">10.1371/journal.pone.0001497</ext-link></comment> <object-id pub-id-type="pmid">18231584</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maloney</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Dal Martello</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sahm</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Spillmann</surname> <given-names>L</given-names></name>. <article-title>Past trials influence perception of ambiguous motion quartets through pattern completion</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2005</year>;<volume>102</volume>(<issue>8</issue>):<fpage>3164</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0407157102" xlink:type="simple">10.1073/pnas.0407157102</ext-link></comment> <object-id pub-id-type="pmid">15710897</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref016"><label>16</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Green</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Swets</surname> <given-names>J</given-names></name>. <source>Signal detection theory and psychophysics</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>1966</year>.</mixed-citation></ref>
<ref id="pcbi.1005546.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Faisal</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Selen</surname> <given-names>LPJ</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>Noise in the nervous system</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2008</year>;<volume>9</volume>(<issue>4</issue>):<fpage>292</fpage>–<lpage>303</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn2258" xlink:type="simple">10.1038/nrn2258</ext-link></comment> <object-id pub-id-type="pmid">18319728</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Simonotto</surname> <given-names>E</given-names></name>. <article-title>Stochastic resonance in the perceptual interpretation of ambiguous figures: A neural network model</article-title>. <source>Physical review letters</source>. <year>1994</year>;<volume>72</volume>(<issue>19</issue>):<fpage>3120</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.72.3120" xlink:type="simple">10.1103/PhysRevLett.72.3120</ext-link></comment> <object-id pub-id-type="pmid">10056072</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lankheet</surname> <given-names>MJ</given-names></name>. <article-title>Unraveling adaptation and mutual inhibition in perceptual rivalry</article-title>. <source>Journal of Vision</source>. <year>2006</year>;<volume>6</volume>(<issue>4</issue>):<fpage>1</fpage>-.</mixed-citation></ref>
<ref id="pcbi.1005546.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moreno-Bote</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rinzel</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>N</given-names></name>. <article-title>Noise-induced alternations in an attractor network model of perceptual bistability</article-title>. <source>Journal of neurophysiology</source>. <year>2007</year>;<volume>98</volume>(<issue>3</issue>):<fpage>1125</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00116.2007" xlink:type="simple">10.1152/jn.00116.2007</ext-link></comment> <object-id pub-id-type="pmid">17615138</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brascamp</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Van Ee</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Noest</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Jacobs</surname> <given-names>RHAH</given-names></name>, <name name-style="western"><surname>van den Berg</surname> <given-names>AV</given-names></name>. <article-title>The time course of binocular rivalry reveals a fundamental role of noise</article-title>. <source>Journal of Vision</source>. <year>2006</year>;<volume>6</volume>(<issue>11</issue>):<fpage>8</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005546.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Noest</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Van Ee</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Nijs</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Van Wezel</surname> <given-names>R</given-names></name>. <article-title>Percept-choice sequences driven by interrupted ambiguous stimuli: a low-level neural model</article-title>. <source>Journal of Vision</source>. <year>2007</year>;<volume>7</volume>(<issue>8</issue>):<fpage>10</fpage>-. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/7.8.10" xlink:type="simple">10.1167/7.8.10</ext-link></comment> <object-id pub-id-type="pmid">17685817</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laing</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Chow</surname> <given-names>CC</given-names></name>. <article-title>A spiking neuron model for binocular rivalry</article-title>. <source>Journal of computational neuroscience</source>. <year>2002</year>;<volume>12</volume>(<issue>1</issue>):<fpage>39</fpage>–<lpage>53</lpage>. <object-id pub-id-type="pmid">11932559</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname> <given-names>Y-J</given-names></name>, <name name-style="western"><surname>Grabowecky</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Suzuki</surname> <given-names>S</given-names></name>. <article-title>Stochastic resonance in binocular rivalry</article-title>. <source>Vision research</source>. <year>2006</year>;<volume>46</volume>(<issue>3</issue>):<fpage>392</fpage>–<lpage>406</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2005.08.009" xlink:type="simple">10.1016/j.visres.2005.08.009</ext-link></comment> <object-id pub-id-type="pmid">16183099</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pelli</surname> <given-names>DG</given-names></name>, <name name-style="western"><surname>Farell</surname> <given-names>B</given-names></name>. <article-title>Why use noise?</article-title> <source>Journal of the Optical Society of America A</source>. <year>1999</year>;<volume>16</volume>(<issue>3</issue>):<fpage>647</fpage>–<lpage>53</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005546.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moreno-Bote</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kanitscheider</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Pitkow</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Information-limiting correlations</article-title>. <source>Nature neuroscience</source>. <year>2014</year>;<volume>17</volume>(<issue>10</issue>):<fpage>1410</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3807" xlink:type="simple">10.1038/nn.3807</ext-link></comment> <object-id pub-id-type="pmid">25195105</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahumada</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lovell</surname> <given-names>J</given-names></name>. <article-title>Stimulus features in signal detection</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1971</year>;<volume>49</volume>:<fpage>1751</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005546.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahumada</surname> <given-names>A</given-names></name>. <article-title>Perceptual classification images from Vernier acuity masked by noise</article-title>. <source>Perception</source>. <year>1996</year>;<volume>26</volume>(<issue>18</issue>):<fpage>1831</fpage>–<lpage>40</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005546.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neri</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>Spatiotemporal mechanisms for detecting and identifying image features in human vision</article-title>. <source>Nature Neuroscience</source>. <year>2002</year>;<volume>5</volume>(<issue>8</issue>):<fpage>812</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn886" xlink:type="simple">10.1038/nn886</ext-link></comment> <object-id pub-id-type="pmid">12101403</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref030"><label>30</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>van Dam</surname> <given-names>LCJ</given-names></name>, <name name-style="western"><surname>Parise</surname> <given-names>CV</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>. <chapter-title>Modeling multisensory integration</chapter-title>. In: <name name-style="western"><surname>Bennett</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hill</surname> <given-names>C</given-names></name>, editors. <source>Sensory Integration and the Unity of Consciousness</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT press</publisher-name>; <year>2014</year>. p. <fpage>209</fpage>–<lpage>29</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005546.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Johnston</surname> <given-names>EB</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>M</given-names></name>. <article-title>Measurement and modeling of depth cue combination: In defense of weak fusion</article-title>. <source>Vision Research</source>. <year>1995</year>;<volume>35</volume>(<issue>3</issue>):<fpage>389</fpage>–<lpage>412</lpage>. <object-id pub-id-type="pmid">7892735</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Noest</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Van Ee</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Nijs</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Van Wezel</surname> <given-names>RJA</given-names></name>. <article-title>Percept-choice sequences driven by interrupted ambiguous stimuli: a low-level neural model</article-title>. <source>Journal of Vision</source>. <year>2007</year>;<volume>7</volume>(<issue>8</issue>):<fpage>10</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/7.8.10" xlink:type="simple">10.1167/7.8.10</ext-link></comment> <object-id pub-id-type="pmid">17685817</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pastukhov</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vivian-Griffiths</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Braun</surname> <given-names>J</given-names></name>. <article-title>Transformation priming helps to disambiguate sudden changes of sensory inputs</article-title>. <source>Vision research</source>. <year>2015</year>;<volume>116</volume>:<fpage>36</fpage>–<lpage>44</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2015.09.005" xlink:type="simple">10.1016/j.visres.2015.09.005</ext-link></comment> <object-id pub-id-type="pmid">26416529</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maier</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wilke</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>, <name name-style="western"><surname>Leopold</surname> <given-names>DA</given-names></name>. <article-title>Perception of temporally interleaved ambiguous patterns</article-title>. <source>Current Biology</source>. <year>2003</year>;<volume>13</volume>(<issue>13</issue>):<fpage>1076</fpage>–<lpage>85</lpage>. <object-id pub-id-type="pmid">12842006</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname> <given-names>HR</given-names></name>. <article-title>Minimal physiological conditions for binocular rivalry and rivalry memory</article-title>. <source>Vision research</source>. <year>2007</year>;<volume>47</volume>(<issue>21</issue>):<fpage>2741</fpage>–<lpage>50</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2007.07.007" xlink:type="simple">10.1016/j.visres.2007.07.007</ext-link></comment> <object-id pub-id-type="pmid">17764714</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhou</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Wong</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Sekuler</surname> <given-names>R</given-names></name>. <article-title>Multi-sensory integration of spatio-temporal segmentation cues: one plus one does not always equal two</article-title>. <source>Experimental Brain Research</source>. <year>2007</year>;<volume>180</volume>(<issue>4</issue>):<fpage>641</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00221-007-0897-0" xlink:type="simple">10.1007/s00221-007-0897-0</ext-link></comment> <object-id pub-id-type="pmid">17333010</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pascucci</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Megna</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Panichi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Baldassi</surname> <given-names>S</given-names></name>. <article-title>Acoustic cues to visual detection: A classification image study</article-title>. <source>Journal of Vision</source>. <year>2011</year>;<volume>11</volume>(<issue>6</issue>).</mixed-citation></ref>
<ref id="pcbi.1005546.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grove</surname> <given-names>PM</given-names></name>, <name name-style="western"><surname>Ashton</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kawachi</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Sakurai</surname> <given-names>K</given-names></name>. <article-title>Auditory transients do not affect visual sensitivity in discriminating between objective streaming and bouncing events</article-title>. <source>Journal of Vision</source>. <year>2012</year>;<volume>12</volume>(<issue>8</issue>).</mixed-citation></ref>
<ref id="pcbi.1005546.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Karim</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Kojima</surname> <given-names>H</given-names></name>. <article-title>The what and why of perceptual asymmetries in the visual domain</article-title>. <source>Advances in Cognitive Psychology</source>. <year>2010</year>;<volume>6</volume>:<fpage>103</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2478/v10053-008-0080-6" xlink:type="simple">10.2478/v10053-008-0080-6</ext-link></comment> <object-id pub-id-type="pmid">21228922</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Adelson</surname> <given-names>EH</given-names></name>, <name name-style="western"><surname>Bergen</surname> <given-names>JR</given-names></name>. <article-title>Spatiotemporal energy models for the perception of motion</article-title>. <source>Journal of the Optical Society of America</source>. <year>1985</year>;<volume>2</volume>(<issue>2</issue>):<fpage>284</fpage>–<lpage>99</lpage>. <object-id pub-id-type="pmid">3973762</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thompson</surname> <given-names>P</given-names></name>. <article-title>Perceived rate of movement depends on contrast</article-title>. <source>Vision research</source>. <year>1982</year>;<volume>22</volume>(<issue>3</issue>):<fpage>377</fpage>–<lpage>80</lpage>. <object-id pub-id-type="pmid">7090191</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Resulaj</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>. <article-title>Changes of mind in decision-making</article-title>. <source>Nature</source>. <year>2009</year>;<volume>461</volume>(<issue>7261</issue>):<fpage>263</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature08275" xlink:type="simple">10.1038/nature08275</ext-link></comment> <object-id pub-id-type="pmid">19693010</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hanks</surname> <given-names>TD</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>. <article-title>Bounded integration in parietal cortex underlies decisions even when viewing duration is dictated by the environment</article-title>. <source>The Journal of Neuroscience</source>. <year>2008</year>;<volume>28</volume>(<issue>12</issue>):<fpage>3017</fpage>–<lpage>29</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4761-07.2008" xlink:type="simple">10.1523/JNEUROSCI.4761-07.2008</ext-link></comment> <object-id pub-id-type="pmid">18354005</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>. <year>2002</year>;<volume>415</volume>(<issue>6870</issue>):<fpage>429</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/415429a" xlink:type="simple">10.1038/415429a</ext-link></comment> <object-id pub-id-type="pmid">11807554</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref045"><label>45</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Ho</surname> <given-names>Y-X</given-names></name>, <name name-style="western"><surname>Serwe</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Trommershäuser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>. <chapter-title>Cues and pseudocues in texture and shape perception</chapter-title>. In: <name name-style="western"><surname>Trommershäuser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Körding</surname> <given-names>K</given-names></name>, editors. <source>Sensory Cue Integration</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2011</year>. p. <fpage>263</fpage>–<lpage>78</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005546.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fujisaki</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Shimojo</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kashino</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nishida</surname> <given-names>S</given-names></name>. <article-title>Recalibration of audiovisual simultaneity</article-title>. <source>Nature Neuroscience</source>. <year>2004</year>;<volume>7</volume>(<issue>7</issue>):<fpage>773</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1268" xlink:type="simple">10.1038/nn1268</ext-link></comment> <object-id pub-id-type="pmid">15195098</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watanabe</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Shimojo</surname> <given-names>S</given-names></name>. <article-title>Attentional modulation in perception of visual motion events</article-title>. <source>Perception</source>. <year>1998</year>;<volume>27</volume>:<fpage>1041</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1068/p271041" xlink:type="simple">10.1068/p271041</ext-link></comment> <object-id pub-id-type="pmid">10341934</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grassi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Casco</surname> <given-names>C</given-names></name>. <article-title>Revealing the origin of the audiovisual bounce-inducing effect</article-title>. <source>Seeing and Perceiving</source>. <year>2012</year>;<volume>25</volume>(<issue>2</issue>):<fpage>223</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1163/187847612X626372" xlink:type="simple">10.1163/187847612X626372</ext-link></comment> <object-id pub-id-type="pmid">22370960</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grassi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Casco</surname> <given-names>C</given-names></name>. <article-title>Audiovisual bounce-inducing effect: When sound congruence affects grouping in vision</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>. <year>2010</year>;<volume>72</volume>(<issue>2</issue>):<fpage>378</fpage>–<lpage>86</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005546.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grassi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Casco</surname> <given-names>C</given-names></name>. <article-title>Audiovisual bounce-inducing effect: attention alone does not explain why the discs are bouncing</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>2009</year>;<volume>35</volume>(<issue>1</issue>):<fpage>235</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0013031" xlink:type="simple">10.1037/a0013031</ext-link></comment> <object-id pub-id-type="pmid">19170485</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bushara</surname> <given-names>KO</given-names></name>, <name name-style="western"><surname>Hanakawa</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Immisch</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Toma</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kansaku</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Hallett</surname> <given-names>M</given-names></name>. <article-title>Neural correlates of cross-modal binding</article-title>. <source>Nature neuroscience</source>. <year>2003</year>;<volume>6</volume>(<issue>2</issue>):<fpage>190</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn993" xlink:type="simple">10.1038/nn993</ext-link></comment> <object-id pub-id-type="pmid">12496761</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hipp</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Engel</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Siegel</surname> <given-names>M</given-names></name>. <article-title>Oscillatory synchronization in large-scale cortical networks predicts perception</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>69</volume>(<issue>2</issue>):<fpage>387</fpage>–<lpage>96</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2010.12.027" xlink:type="simple">10.1016/j.neuron.2010.12.027</ext-link></comment> <object-id pub-id-type="pmid">21262474</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Singer</surname> <given-names>W</given-names></name>. <article-title>Dynamic formation of functional networks by synchronization</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>69</volume>(<issue>2</issue>):<fpage>191</fpage>–<lpage>3</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.01.008" xlink:type="simple">10.1016/j.neuron.2011.01.008</ext-link></comment> <object-id pub-id-type="pmid">21262459</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kleiner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Brainard</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Pelli</surname> <given-names>D</given-names></name>. <article-title>What's new in Psychtoolbox-3</article-title>. <source>Perception</source>. <year>2007</year>;<volume>36</volume>(<issue>14</issue>):<fpage>1</fpage>.-16.</mixed-citation></ref>
<ref id="pcbi.1005546.ref055"><label>55</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Knoblauch</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>L</given-names></name>. <chapter-title>Classification images</chapter-title>. In: <name name-style="western"><surname>Knoblauch</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>L</given-names></name>, editors. <source>Modeling psychophysical data in R</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2012</year>. p. 167/94.</mixed-citation></ref>
<ref id="pcbi.1005546.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Challinor</surname> <given-names>KL</given-names></name>, <name name-style="western"><surname>Mather</surname> <given-names>G</given-names></name>. <article-title>A motion-energy model predicts the direction discrimination and MAE duration of two-stroke apparent motion at high and low retinal illuminance</article-title>. <source>Vision research</source>. <year>2010</year>;<volume>50</volume>(<issue>12</issue>):<fpage>1109</fpage>–<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2010.04.002" xlink:type="simple">10.1016/j.visres.2010.04.002</ext-link></comment> <object-id pub-id-type="pmid">20380846</object-id></mixed-citation></ref>
<ref id="pcbi.1005546.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Derrington</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Lennie</surname> <given-names>P</given-names></name>. <article-title>Spatial and temporal contrast sensitivities of neurones in lateral geniculate nucleus of macaque</article-title>. <source>The Journal of Physiology</source>. <year>1984</year>;<volume>357</volume>(<issue>1</issue>):<fpage>219</fpage>–<lpage>40</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>