<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id>
<journal-title-group>
<journal-title>PLOS Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pbio.1002611</article-id>
<article-id pub-id-type="publisher-id">PBIOLOGY-D-16-01188</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Clinical medicine</subject><subj-group><subject>Clinical neurophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Signal filtering</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Electromagnetic radiation</subject><subj-group><subject>Light</subject><subj-group><subject>Visible light</subject><subj-group><subject>Luminance</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Confidence intervals</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Object segmentation controls image reconstruction from natural scenes</article-title>
<alt-title alt-title-type="running-head">Object segmentation controls image reconstruction from natural scenes</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1981-5309</contrib-id>
<name name-style="western">
<surname>Neri</surname>
<given-names>Peter</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Laboratoire des Systèmes Perceptifs, Département d'études cognitives, Ecole Normale Supérieure, PSL Research University, CNRS, Paris, France</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Pack</surname>
<given-names>Christopher C.</given-names>
</name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>McGill University, Canada</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The author has declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">neri.peter@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>21</day>
<month>8</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<month>8</month>
<year>2017</year>
</pub-date>
<volume>15</volume>
<issue>8</issue>
<elocation-id>e1002611</elocation-id>
<history>
<date date-type="received">
<day>9</day>
<month>12</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>25</day>
<month>7</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Peter Neri</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pbio.1002611"/>
<abstract>
<p>The structure of the physical world projects images onto our eyes. However, those images are often poorly representative of environmental structure: well-defined boundaries within the eye may correspond to irrelevant features of the physical world, while critical features of the physical world may be nearly invisible at the retinal projection. The challenge for the visual cortex is to sort these two types of features according to their utility in ultimately reconstructing percepts and interpreting the constituents of the scene. We describe a novel paradigm that enabled us to selectively evaluate the relative role played by these two feature classes in signal reconstruction from corrupted images. Our measurements demonstrate that this process is quickly dominated by the inferred structure of the environment, and only minimally controlled by variations of raw image content. The inferential mechanism is spatially global and its impact on early visual cortex is fast. Furthermore, it retunes local visual processing for more efficient feature extraction without altering the intrinsic transduction noise. The basic properties of this process can be partially captured by a combination of small-scale circuit models and large-scale network architectures. Taken together, our results challenge compartmentalized notions of bottom-up/top-down perception and suggest instead that these two modes are best viewed as an integrated perceptual mechanism.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Biological vision is designed to discover the structure of the environment around us. To do this, it relies on ambiguous and often misleading information from the eyes: the boundary of a critical object may be invisible against a background of similar appearance, and may be overlooked in favour of the sharp contour projected by an irrelevant shadow. It remains unclear how human vision sorts different image features according to their relevance to the layout of objects within the scene. We demonstrate that vision achieves this goal via a specialized perceptual system for object segmentation that is one and the same with the feature extraction system: immediately after information is relayed to cortex by the eyes, the process of reconstructing image content from local features is controlled by a dedicated inferential mechanism that attempts to recover the underlying environmental structure; perception is quickly organized around the operation of this mechanism, which becomes the primary contextual influence on image reconstruction. The integrated nature of this perceptual mechanism defies current notions of separate top-down and bottom-up processes, offering a fresh view of how human vision operates on natural signals.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>CNRS (Centre national de la recherche scientifique)</institution>
</funding-source>
</award-group>
<award-group id="award002">
<funding-source>
<institution>Royal Society of London</institution>
</funding-source>
<award-id>URF</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1981-5309</contrib-id>
<name name-style="western">
<surname>Neri</surname>
<given-names>Peter</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001665</institution-id>
<institution>Agence Nationale de la Recherche</institution>
</institution-wrap>
</funding-source>
<award-id>ANR-16-CE28-0016</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1981-5309</contrib-id>
<name name-style="western">
<surname>Neri</surname>
<given-names>Peter</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001665</institution-id>
<institution>Agence Nationale de la Recherche</institution>
</institution-wrap>
</funding-source>
<award-id>ANR-10-LABX-0087 IEC</award-id>
</award-group>
<award-group id="award005">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001665</institution-id>
<institution>Agence Nationale de la Recherche</institution>
</institution-wrap>
</funding-source>
<award-id>ANR-10-IDEX-0001-02 PSL</award-id>
</award-group>
<funding-statement>This work was supported by a University Research Fellowship from the Royal Society of London (UK); the CNRS (France); grants ANR-10-LABX-0087 IEC, ANR-10-IDEX-0001-02 PSL and ANR-16-CE28-0016 from the Agence Nationale de la Recherche. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="9"/>
<table-count count="0"/>
<page-count count="32"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Data have been submitted as Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Consider the image in <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1A</xref>. During the twentieth century, knowledge of how it may be represented in early visual cortex was galvanized by the discovery that neurons respond to specific features defining the image, such as the orientation and size of its edges and lines [<xref ref-type="bibr" rid="pbio.1002611.ref001">1</xref>]. In its simplest form [<xref ref-type="bibr" rid="pbio.1002611.ref002">2</xref>], this representation may resemble the feature map in <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1C</xref>, where the intensity of each location mimics the response of a human edge detector positioned within that region of the image [<xref ref-type="bibr" rid="pbio.1002611.ref003">3</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref004">4</xref>].</p>
<fig id="pbio.1002611.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002611.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Mapping features from natural scenes.</title>
<p>Intensity (brightness) on the top-down map (<bold>B</bold>) reflects saliency of perceptual object representation within the original scene [<xref ref-type="bibr" rid="pbio.1002611.ref005">5</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref006">6</xref>] (<bold>A</bold>), while the bottom-up map (<bold>C</bold>) indicates edge energy content [<xref ref-type="bibr" rid="pbio.1002611.ref003">3</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref007">7</xref>]. We identify 4 locations that are rich/poor on the top-down map (green/red circles in <bold>B</bold>) and/or rich/poor on the bottom-up map (solid/dashed circles in <bold>C</bold>); the two locations indicated by dashed-green and solid-red circles in <bold>D</bold> are rich on one map and poor on the other. An oriented wavelet is inserted at one location in congruent (<bold>E</bold>) or incongruent (<bold>F</bold>) configuration, orientation noise is added [<xref ref-type="bibr" rid="pbio.1002611.ref008">8</xref>] (<bold>G</bold>), and observers must determine whether probe is congruent or not [<xref ref-type="bibr" rid="pbio.1002611.ref009">9</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref010">10</xref>].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.g001" xlink:type="simple"/>
</fig>
<p>Our current understanding of cortical feature encoding is much richer than <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1C</xref>, extending to gain control [<xref ref-type="bibr" rid="pbio.1002611.ref011">11</xref>], surround modulation [<xref ref-type="bibr" rid="pbio.1002611.ref012">12</xref>], attentional effects [<xref ref-type="bibr" rid="pbio.1002611.ref013">13</xref>], crowding [<xref ref-type="bibr" rid="pbio.1002611.ref014">14</xref>], and many other phenomena [<xref ref-type="bibr" rid="pbio.1002611.ref004">4</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref015">15</xref>]. Nevertheless, whether these additional factors are included or left out, there remains a fundamental problem with feature-driven representations such as <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1C</xref>: they fail to capture the essential structure of the underlying scene in its relevance to perception and behaviour [<xref ref-type="bibr" rid="pbio.1002611.ref016">16</xref>]. For the purpose of relating to this image in the form of scene understanding and potential motor interaction [<xref ref-type="bibr" rid="pbio.1002611.ref017">17</xref>–<xref ref-type="bibr" rid="pbio.1002611.ref019">19</xref>], our representation of its content is better captured by the map in <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1B</xref>: the critical boundaries are those that define the 2 human characters, while everything else is of incidental significance [<xref ref-type="bibr" rid="pbio.1002611.ref016">16</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref020">20</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref021">21</xref>].</p>
<p>Where and how, specifically, does <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1C</xref> fail in capturing <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1B</xref>? Consider the 4 locations indicated by circles in <bold>C</bold>. The 2 solid circles correspond to well-defined boundaries in the physical stimulus: large luminance transitions occur at those locations in the original image (<bold>A</bold>) so that they are richly represented in the edge map of <bold>C</bold>. On the contrary, the 2 dashed circles correspond to boundaries that barely exist within the physical stimulus: locally within the original image, there is little to indicate that a boundary is present at those locations. This is not to say, however, that those boundaries do not exist at a different level of representation: they do exist, but at the level of the scene representation afforded by our mind [<xref ref-type="bibr" rid="pbio.1002611.ref020">20</xref>], as indicated by the top green circle in <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1B</xref>. That specific location marks the boundary between the person in the foreground and the landscape in the background, which is a critical demarcation for representing scene content and supporting image interpretation [<xref ref-type="bibr" rid="pbio.1002611.ref022">22</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref023">23</xref>]. It is, however, nearly invisible within the edge map in <bold>C</bold> (top dashed circle). A complementary inconsistency between <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1B and 1C</xref> is indicated by the red circle on the right-hand side of <bold>B</bold>: this location corresponds to an irrelevant boundary for scene understanding (i.e., poorly represented in <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1B</xref>), yet it is well-defined in the physical image (richly represented within the edge map in <bold>C</bold>, as indicated by the solid circle on the right-hand side of that image).</p>
<p>How are the two representational levels cartooned in <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1B and 1C</xref> combined in the visual system? Recent electrophysiological measurements from visual cortex have established that neuronal response properties are sensitive to natural signals [<xref ref-type="bibr" rid="pbio.1002611.ref018">18</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref024">24</xref>], thus consolidating the notion that cortical neurons must be viewed as adaptive devices under the control of both bottom-up and top-down information [<xref ref-type="bibr" rid="pbio.1002611.ref025">25</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref026">26</xref>]. But how do these flexible cortical effects impact human behaviour? In other words, what are the perceptual signatures of the neuronal effects associated with natural stimulation? We know surprisingly little about this fundamental question [<xref ref-type="bibr" rid="pbio.1002611.ref027">27</xref>]. To make progress in this direction, here we deliberately focus on a simplest visual task: reconstructing the local orientation content of a corrupted image region (<xref ref-type="fig" rid="pbio.1002611.g001">Fig 1E–1G</xref>). This choice of perceptual operation enables us to investigate early visual mechanisms using established low-level tools, while at the same time recasting the ensuing characterization into the higher-level coordinates defined by the natural scene [<xref ref-type="bibr" rid="pbio.1002611.ref010">10</xref>], so as to gauge the interplay between the 2 representational levels.</p>
<p>Surprisingly, we find that human reconstruction of local image regions is almost exclusively controlled by the kind of scene representation exemplified by <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1B</xref>, with only limited signatures of the low-level account returned by <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1C</xref>. The control exerted by the object-based segmentation map of <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1B</xref> operates very quickly (within the first 100 ms) and is not altered by spatial attention. In combination with electrophysiological recordings of scalp signals, these results offer a new perspective on the notion of how so-called bottom-up and top-down representations may interface in human vision [<xref ref-type="bibr" rid="pbio.1002611.ref020">20</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref028">28</xref>]. They suggest that the conceptual compartmentalization associated with the bottom-up/top-down dichotomy may be more productively replaced by regarding these two processing modes as intimately integrated into a single adaptive mechanism [<xref ref-type="bibr" rid="pbio.1002611.ref025">25</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref026">26</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref029">29</xref>], possibly providing a more appropriate framework for understanding early visual perception in humans.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec003">
<title>Observers and data mass</title>
<p>All experiments have been approved by the CERB committee (ID470) at the University of Aberdeen and were conducted according to the principles expressed in the Declaration of Helsinki. We tested 8 naive observers (3 males); of these, 6 took part in the double-pass experiments and 7 took part in the electroencephalogram (EEG) experiments. We attempted to bring electrodes into contact with the scalp of the remaining observer; however, this was not possible due to thick hair growth (dreadlocks) that caused high electrode noise and unreliable contact; this observer was therefore excluded from subsequent EEG experiments immediately after the first (unsuccessful) attempt. We collected a total of approximately 310,000 trials, of which approximately 30,000 (&gt;4,000 per observer) during the EEG experiments (evenly divided between intact and cut-out scenes) and approximately 21,000 (&gt;3,500 per observer) during double-pass experiments. For the psychophysical experiments, this corresponds to approximately 50 hours of data collection per observer. For the EEG experiments, 6 observers completed 4 sessions lasting 3 hours each, while the remaining observer completed 2 sessions. Observers were paid 9 EUR/hour for data collection in the psychophysical experiments and 20 EUR/hour for participation in the EEG experiments.</p>
</sec>
<sec id="sec004">
<title>Presentation protocol and task</title>
<p>All scenes were rescaled to have the same contrast energy; when projected onto the CRT monitor (Iiyama Vision Master Pro 500) by ViSaGe hardware (Cambridge Research Systems), they spanned a luminance range of 4–60 cd/m<sup>2</sup> against a gray background of 32 cd/m<sup>2</sup> and occupied (width × height) 13° × 20° or 20° × 13° (depending on whether the scene was in portrait or landscape format) at the adopted viewing distance of 57 cm. Stimulus duration was 300 ms except for a subset of the experiments (approximately 30% of total data mass) during which it was deliberately reduced on a near-logarithmic scale (200, 100, 50, 30, 20, 10 ms) to study the impact of this parameter. Before being displayed, the stimulus could be flipped around its vertical axis (left-right with respect to fixation) with 50% probability (randomly and independently selected on every trial), so that each probe location/type could appear within either left or right hemifield with equal probability. On each trial, observers saw one natural scene containing either congruent (<xref ref-type="fig" rid="pbio.1002611.g001">Fig 1E</xref>) or incongruent probe (<xref ref-type="fig" rid="pbio.1002611.g001">Fig 1F</xref>; see below for details on probe design). The scene was centered on a fixation cross that never disappeared. Observers were required to press one of two buttons to indicate whether the probe was congruent or incongruent (this task was worded to them as “determine whether the orientation of the texture within the probe is aligned with the scene, or is orthogonal to the scene”). Their response was followed by trial-by-trial feedback (correct/incorrect) and the next trial was initiated after a random delay uniformly distributed between 200 and 400 ms. Feedback was introduced to push observers into their optimal performance regime so that interpretation of sensitivity (d′) measurements would not be confounded by extraneous factors such as lack of motivation and/or response bias [<xref ref-type="bibr" rid="pbio.1002611.ref030">30</xref>] (refer to <xref ref-type="supplementary-material" rid="pbio.1002611.s001">S1 Text</xref> and <xref ref-type="supplementary-material" rid="pbio.1002611.s004">S3 Fig</xref> for detailed analysis of response bias effects). At the end of each block (100 trials), observers were provided with a summary of their overall performance (% of correct responses on the last block as well as across all blocks).</p>
</sec>
<sec id="sec005">
<title>Primary stimulus design</title>
<sec id="sec006">
<title>Construction of bottom-up/top-down maps</title>
<p>Our goal was to associate each natural image with 2 maps: the “bottom-up” map, detailing the degree of low-level edge definition at each location within the physical image; the “top-down” map, reflecting the perceptual significance of each location within the image for segmenting and reconstructing the layout of the scene. We express size as [x,y] where x is in degrees of visual angle and y is in percentage units of the smaller side of the natural scene (measuring 13°); we include the latter specification to ease interpretation of how different stimulus elements relate to each other. Images were obtained from the Berkeley Segmentation Dataset [<xref ref-type="bibr" rid="pbio.1002611.ref005">5</xref>] (BSD500), a database that is available in the public domain and downloadable from <ext-link ext-link-type="uri" xlink:href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/" xlink:type="simple">https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/</ext-link>. Images were then processed by purpose-written software (Matlab) for determining adequate probe insertion points. Except for designing the algorithm and selecting appropriate initialization parameters, there was no human intervention, and the algorithm was fully automated. All scenes were converted to gray-level images. The algorithm built 2 maps from each scene. The bottom-up map was constructed by processing the image with a Sobel-like differentiation filter [<xref ref-type="bibr" rid="pbio.1002611.ref007">7</xref>] measuring (active area) ∼ [0.9°,7%]; an example is shown in <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1C</xref> where filter output (normalized to range between 0 and 1) scales with surface intensity (increasing from dark to bright). The top-down map was constructed by combining segmentations from different BSD500 participants. Lines within individual segmentations were first thickened via blurring/thresholding to measure ∼ [0.5°,4%] (line width) and subsequently combined across participants by assigning to each pixel the proportion of participants for whom that pixel corresponded to a line. An example is shown in <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1B</xref> where pixel value (ranging between 0 and 1) scales with surface intensity: darkest value (0) for pixels not marked by any participant, brightest value (1) for pixels marked by all participants.</p>
</sec>
<sec id="sec007">
<title>Identification of target insertion points</title>
<p>The algorithm for selecting insertion points was designed to identify 4 points per image, one for each combination of rich versus poor on top-down versus bottom-up maps: an insertion point corresponding to a poor location on the top-down map and a rich location on the bottom-up map, which we refer to as poor/rich for top-down/bottom-up content; 3 more points corresponding to poor/poor, rich/poor, and rich/rich. For each nonzero pixel on the top-down map, we computed the elongation of nonzero pixels within a square region (sized [1.6°,12%]) centred on that pixel. Elongation ranges between 0 (circle-like) and 1 (line-like); it specifies the half-focal separation of the ellipse that takes the same second-moments as the analyzed region, which we refer to as the “best-fitting” ellipse. We then excluded all pixels with elongation &lt;0.9 and intensity &lt;0.1 on the bottom-up map (i.e., those that did not conform to an elongated boundary), as well as those located near the edge of the image (within <bold>∼</bold> [2°,15%] from edge). The remaining pixels were labelled top-down “rich” if their value on the top-down map was 1, or “poor” if it was &lt;1. We then selected, among top-down rich pixels, the pixel corresponding to the smallest value on the bottom-up map. We call the latter value <italic>v</italic>. We also selected, among top-down poor pixels, the pixel corresponding to the value on the bottom-up map that was closest to <italic>v</italic>. These two selections were labelled respectively rich/poor and poor/poor for top-down/bottom-up content. We then selected, among poor pixels on the top-down map, the pixel corresponding to the largest value on the bottom-up map. We call the latter value <italic>V</italic>. We also selected, among rich pixels on the top-down map, the pixel corresponding to the value on the bottom-up map that was closest to <italic>V</italic>. These two selections were labelled respectively poor/rich and rich/rich for top-down/bottom-up content. We further imposed the constraint that the 4 selected insertion points should not be within [1.6°,12%] of each other to reduce potential overlap of the image elements targeted by the different insertions. Once an insertion point is selected, we identified the orientation of the best-fitting ellipse to non-zero pixels within a square [0.8°,6%] region centred on that point, and labelled it as the local congruent orientation. Of the 500 images within BSD500, 53 did not contain enough pixels with characteristics that satisfied the above constraints, and were therefore excluded.</p>
</sec>
<sec id="sec008">
<title>Probe design and insertion</title>
<p>Our goal was to generate a low-level stimulus that could be smoothly grafted into the natural scene, enabling us to retain full control over the statistical properties of the probe while at the same time embedding it within a complex stimulus (i.e. the natural scene) for which we lack the same degree of control. To achieve this goal, we build upon prior work with both isolated [<xref ref-type="bibr" rid="pbio.1002611.ref008">8</xref>] and embedded [<xref ref-type="bibr" rid="pbio.1002611.ref009">9</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref010">10</xref>] low-level elements. Probes (see example in <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1G</xref>) consisted of 16 superimposed pseudo-Gabor wavelets at 16 different orientations uniformly spanning the 0-<italic>π</italic> range, each taking a random phase. Carrier spatial frequency was fixed at ∼1 cycle/degree. The envelope was constant over a circular region spanning ∼ [1.6°,12%] (diameter); outside this region, it decreased smoothly to 0 following a Gaussian envelope of SD ∼ [0.1°,0.8%]. The 16 contrast values assigned to the different wavelets on each trial are denoted by vector <bold>s</bold><sup><bold>[<italic>q</italic>]</bold></sup> (<italic>q</italic> = 1 for congruent probe, <italic>q</italic> = 0 for incongruent probe; see next section for more details). With relation to this vector representation, the congruent orientation corresponds to the fifth entry into the vector; in actual image space, the congruent orientation is selected as the best match to the orientation associated with the insertion point (see above), while the incongruent orientation is always orthogonal to the congruent orientation. The probe was smoothly inserted (by using wavelet envelope to control probe/image ratio contribution to image) into the local region of the natural scene identified by the automated procedure detailed above (see examples in <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1E and 1F</xref> for congruent and incongruent probes respectively). Probe insertion density gradually declined away from fixation (see <xref ref-type="supplementary-material" rid="pbio.1002611.s003">S2A and S2B Fig</xref>), partly as a result of stimulus geometry.</p>
</sec>
<sec id="sec009">
<title>Injection of orientation noise</title>
<p>Our goal was to perturb the orientation content of the probe via random fluctuations of different orientation components, for the purpose of subsequently establishing the link between specific fluctuations in the stimulus on the one hand, and the associated response choices made by human observers on the other hand (psychophysical reverse correlation [<xref ref-type="bibr" rid="pbio.1002611.ref031">31</xref>]). On each trial <bold>s</bold><sup><bold>[<italic>q</italic>]</bold></sup> = <bold>t</bold><sup><bold>[<italic>q</italic>]</bold></sup> + <bold>n</bold><sup><bold>[<italic>q</italic>]</bold></sup>: the contrast distribution across orientation consisted of a target signal <bold>t</bold> (deterministic) summed onto a noise sample <bold>n</bold> (statistically defined and therefore different on every trial). The target signal vector <bold>t</bold><sup><bold>[<italic>q</italic>]</bold></sup> consisted of 0's everywhere except the fifth entry when <italic>q</italic> = 1 (congruent probe) or the thirteenth entry when <italic>q</italic> = 0 (incongruent probe), which was assigned a value denoted by <italic>ρ</italic> (target intensity). Each entry of the noise vector <bold>n</bold> followed a Gaussian distribution with mean 3% (contrast units) and SD 0.7% clipped to ± 4 SD. We adjusted <italic>ρ</italic> individually for each subject to target threshold performance; when expressed as multiple of noise mean, <italic>ρ</italic> was approximately 4 (mean across observers; equivalent to approximately 12% contrast). When the task was particularly challenging due to specific manipulations (e.g., extremely short durations), noise was removed (<bold>n</bold> = 0) and <italic>ρ</italic> ∼ 18% (contrast units); this large SNR (effectively ∞) was applied on approximately 20% of total data mass to ensure that overall performance was above chance (d′&gt;0).</p>
</sec>
</sec>
<sec id="sec010">
<title>Spatial cueing</title>
<p>We designed a cueing paradigm to manipulate spatial attention so that observers were given the opportunity to deploy attention to the local probe on some trials but not others (each trial being of other type with equal probability). On “precue” trials, the main stimulus described above was preceded by a spatial cue (duration 100 ms) consisting of a blob (defined by probe envelope and therefore matched to probe size) that colocalized with the probe (see <xref ref-type="supplementary-material" rid="pbio.1002611.s005">S1 Video</xref>); the interval between cue and main stimulus was uniformly distributed between 150 and 300 ms. On “postcue” trials, the same cue was presented but it followed the main stimulus (after an interval specified using the same parameters adopted for precue trials). Under particularly challenging task conditions (large gaps, short durations, power-only stimuli), we only adopted the precue condition to help performance maintain above-chance levels. Spatial cueing was not adopted with zooming stimuli (see below for detailed description) to avoid introducing additional dynamic elements to an already dynamic stimulus; cueing was redundant in this case because the zooming process implicitly cues probe location.</p>
</sec>
<sec id="sec011">
<title>Scene manipulations</title>
<p>The main effects reported in this study are measured with scenes that, except for probe insertion, retain their natural characteristics. It is important to determine exactly which of those many characteristics play a role in driving the top-down effect [<xref ref-type="bibr" rid="pbio.1002611.ref032">32</xref>]. To achieve this goal, we manipulated the global appearance of the scene.</p>
<sec id="sec012">
<title>Lowpass/highpass filtering</title>
<p>Lowpass filtering attenuated power uniformly for all frequencies above 0.5 cycles/deg by approximately 70%; highpass filtering reduced power progressively for all frequencies below 7 cycles/deg, starting at approximately 10% attenuation for 7 cycles/deg and progressing to 100% attenuation for frequencies below 0.5 cycles/deg (those preserved by the lowpass filter). A circular region of size ∼ [3.1°,24%] (diameter of tapered envelope) immediately surrounding the probe was left intact.</p>
</sec>
<sec id="sec013">
<title>Warping</title>
<p>Warping was applied by first selecting 40 lattice points uniformly spanning the scene with the exclusion of those within [3.2°,25%] of the probe insertion point. The latter exclusion was adopted to ensure that the warped image would merge smoothly with the circular region immediately surrounding the probe, which was left intact as detailed above for filtered images. The remaining points served as centers for 2 image warping manipulations: swirling and lensing. Swirling consists of local rotation controlled by an angle that depends on distance from center. Lensing consists of an exponential distortion of local coordinates not dissimilar from converting linear to log coordinates to emphasize values near the origin. Each selected center could either be swirled or lensed, the distortion chosen randomly and independently for each center, but only performed once for a given scene and insertion point (i.e., the warped scene did not change from trial to trial). The degree of warping was controlled by allowing distortions to extend over a limited region surrounding each center of application; the region was defined by a Gaussian envelope with standard deviation [0.6°,5%] for weak warping and [1.2°,10%] for strong warping.</p>
</sec>
<sec id="sec014">
<title>Cut-out/Lines</title>
<p>For the “lines” manipulation, the top-down map was thresholded (&gt;0) to binary, i.e., all boundaries were assigned the largest luminance value (60 cd/m<sup>2</sup>) if they had been selected by at least 1 participant in BSD500. For the “cut-out” manipulation, each region defined by those boundaries was assigned a constant luminance value chosen from a predetermined set of values uniformly spanning the entire luminance range (4–60 cd/m<sup>2</sup>) and randomly permuted. This means that 1) no two regions took the same luminance value, ensuring that different regions would never merge; and 2) the luminance difference between any two regions was above visibility threshold. Similarly to warping, randomization was only applied once for any given natural scene, and the same cut-out scene was then used on multiple trials. For these 2 manipulations, the region surrounding the probe was not preserved. The latter detail is important particularly in relation to the cut-out manipulation, because it ensures that all potential low-level cues in the original scene (including second-order ones) had been eliminated (see <xref ref-type="supplementary-material" rid="pbio.1002611.s001">S1 Text</xref>).</p>
</sec>
<sec id="sec015">
<title>Phase/power scrambling</title>
<p>For each scene in the database, we generated an image consisting of white noise (each pixel being independently assigned a random luminance value). We then replaced the phase spectrum of the scene with that from the white-noise image to generate power-only scenes, or we replaced the power spectrum to generate phase-only scenes. Similarly to filtered scenes, the region surrounding the probe was preserved.</p>
</sec>
</sec>
<sec id="sec016">
<title>Zooming stimuli</title>
<p>To study the spatiotemporal characteristics of scene-probe interactions, we designed “zooming” variants of the stimulus where a smooth transition is enacted between a probeless scene and a sceneless probe. We generated a zooming envelope for each probe insertion point via thin-plate spline warping [<xref ref-type="bibr" rid="pbio.1002611.ref033">33</xref>] of reference points specified around scene edge and probe edge. This envelope was constructed so that it would toggle between 2 views: one in which most of the scene was visible but the probe was not visible, and one in which only the probe region was visible. All edges of visible regions were tapered. In the zoom-in variant, the envelope smoothly transitioned from the scene-without-probe image to the probe-without-scene image over 30 frames (total duration 300 ms) or 10 frames (100 ms) in the “long” and “short” configurations, respectively. The opposite direction was applied for the zoom-out variant. Please refer to <xref ref-type="supplementary-material" rid="pbio.1002611.s006">S2 Video</xref> for examples of both zoom-in/zoom-out and long/short stimuli.</p>
</sec>
<sec id="sec017">
<title>Gap stimuli and short stimulus presentations</title>
<p>The zooming stimulus compounds spatial with temporal manipulations. We studied these 2 factors separately by either introducing a gap between probe and scene (spatial manipulation) or decreasing stimulus duration (temporal manipulation). For spatial gaps, the probe was surrounded by a circular mean-luminance region smoothly merging into the scene and extending out to a diameter of (in units of probe diameter size) 1.25, 1.5, 2.5, and 5. The discrimination task was particularly challenging for larger gap sizes (covering up to approximately 20% of the area occupied by the scene), requiring that noise be removed from the probe in order to support above-chance performance. Some observers were, nevertheless, unable to perform above chance under those conditions; those instances were excluded from the individual-observer analysis by removing all log-ratios associated with d′ ≤ 0. A similar issue arose in connection with very brief stimuli (10–20 ms); again, those instances were excluded from the composite analysis. We nevertheless display estimates for all observers because, for a given set of conditions across which log-ratios were computed (e.g., 10 ms and 20 ms), all observers returned a viable estimate for at least one of the conditions within that set (e.g., 10 ms or 20 ms).</p>
</sec>
<sec id="sec018">
<title>Orientation tuning</title>
<p>Above-chance performance in the congruent/incongruent task requires observers to assign perceptual weight to different orientation channels in a nonuniform fashion; the weight profile can be summarized in the form of an orientation tuning function. To derive an empirical estimate of tuning characteristics, we capitalized on the presence of orientation noise within the probe combined with the perceptual coupling between specific noise perturbations and the trial-by-trial response generated by the human observer [<xref ref-type="bibr" rid="pbio.1002611.ref008">8</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref031">31</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref034">34</xref>].</p>
<sec id="sec019">
<title>Derivation of tuning functions</title>
<p>To maximize data mass, we pooled noise samples from all configurations that demonstrated top-down effects: intact, inverted (upside-down), cut-out, highpass, phase-only, intact/cut-out during EEG experiments (total of approximately 140,000 trials). Each noise sample is denoted by <bold>n</bold><sup>[<italic>q</italic>,<italic>z</italic>]</sup>: the sample added to congruent (<italic>q</italic> = 1) or incongruent (<italic>q</italic> = 0) probe on a trial to which the observer responded correctly (<italic>z</italic> = 1) or incorrectly (<italic>z</italic> = 0). The corresponding orientation tuning function <bold>p</bold> is derived via the standard formula for combining averages from stimulus-response classes [<xref ref-type="bibr" rid="pbio.1002611.ref034">34</xref>]:
<disp-formula id="pbio.1002611.e001">
<alternatives>
<graphic id="pbio.1002611.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.1002611.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo>=</mml:mo><mml:mo>〈</mml:mo><mml:msup><mml:mi mathvariant="bold">n</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msup><mml:mo>〉</mml:mo><mml:mo>+</mml:mo><mml:mo>〈</mml:mo><mml:msup><mml:mi mathvariant="bold">n</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msup><mml:mo>〉</mml:mo><mml:mo>−</mml:mo><mml:mo>〈</mml:mo><mml:msup><mml:mi mathvariant="bold">n</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msup><mml:mo>〉</mml:mo><mml:mo>−</mml:mo><mml:mo>〈</mml:mo><mml:msup><mml:mi mathvariant="bold">n</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msup><mml:mo>〉</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where 〈〉 is average across trials of the indexed type. Under some commonly adopted assumptions regarding the nature of sensory transduction [<xref ref-type="bibr" rid="pbio.1002611.ref031">31</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref034">34</xref>], <bold>p</bold> provides a description of the “perceptual weight” assigned by human observers to different parts of the stimulus. For example, a peak corresponding to the fifth entry of vector <bold>p</bold> indicates that observers were more likely to report the probe as being “congruent” when noise orientation energy happened (by chance) to be more prominent within the orientation range corresponding to the congruent orientation (fifth entry into vector <bold>n</bold>). To increase measurement SNR, the tuning functions have been symmetrically averaged around congruent/incongruent coordinates under the reasonable assumption that observers show no bias either clockwise or counterclockwise of the 2 orientations defining the task. The validity of this assumption (which is a logical necessity-given stimulus/task symmetry) was verified empirically by attempting to detect differences between values immediately clockwise and counterclockwise of the 2 reference orientations (congruent/incongruent); all attempts failed without approaching statistical significance.</p>
</sec>
<sec id="sec020">
<title>Retuning index</title>
<p>Under the simplest signal detection theory (SDT) model [<xref ref-type="bibr" rid="pbio.1002611.ref035">35</xref>] that is applicable to the problem at hand, the expected sensitivity of the orientation tuning function is controlled by the differential energy output to congruent and incongruent signals, normalized by the overall energy output to signal plus noise: [<bold>p</bold>(13) − <bold>p</bold>(5)]<sup>2</sup> / Σ<bold>p</bold><sup>2</sup> (entries 5 and 13 into vector <bold>p</bold> correspond to congruent/incongruent orientations). This metric is designed to be ≥ 0 (effectively &gt;0) to enable log-ratio computation.</p>
</sec>
<sec id="sec021">
<title>Retuning model</title>
<p>Stimulus image (2D) <bold>S</bold> was generated using specifications identical to those adopted in the psychophysical experiments (except for lower SNR) and processed by a quadrature-pair operator:
<disp-formula id="pbio.1002611.e002">
<alternatives>
<graphic id="pbio.1002611.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.1002611.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mrow><mml:msup><mml:mrow><mml:mo>〈</mml:mo><mml:mi mathvariant="bold">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>〉</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>〈</mml:mo><mml:mi mathvariant="bold">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>〉</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where 〈,〉 is 2D inner product (also termed Frobenius) and filter <bold>W</bold>(<italic>f</italic><sub><italic>t</italic></sub>, <italic>p</italic>) is a Gabor wavelet of spatial frequency <italic>f</italic> oriented along congruent (<italic>t</italic> = 1) or incongruent (<italic>t</italic> = 0) axes at even (<italic>p</italic> = 0) or odd (<italic>p</italic> = 1) phases. We define the output of this operation <italic>r</italic><sub><italic>t</italic></sub>. The decision variable generated by the model is <inline-formula id="pbio.1002611.e003"><alternatives><graphic id="pbio.1002611.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.1002611.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, i.e., the output from the congruent-oriented operator normalized by the summed output of congruent and incongruent operators; when it exceeds a prespecified threshold value, the model responds “congruent,” otherwise it responds “incongruent.” The threshold value is the average decision variable across 8,000 trials, half with <bold>S</bold> containing a congruent signal and half an incongruent signal (unbiased criterion). In the “Poor” state <italic>f</italic><sub>1</sub> = 4 and <italic>f</italic><sub>0</sub> = 21 in multiples of the spatial frequency assigned to the target signal carried by <bold>S</bold>; in the “Rich” state <italic>f</italic><sub>1</sub> = 1 and <italic>f</italic><sub>0</sub> = 3. In words, the model filter aligned with the congruent signal undergoes Poor→Rich sharpening from 4 × broader than the signal (mildly tuned) to match the signal (<italic>f</italic><sub>1</sub> = 1), while the model filter aligned with the incongruent signal sharpens from virtually untuned (21 × broader than congruent/incongruent signal) to mildly tuned (3 × broader).</p>
</sec>
</sec>
<sec id="sec022">
<title>Internal noise estimation</title>
<p>We performed additional experiments specifically designed to enable internal noise estimates using double-pass protocols [<xref ref-type="bibr" rid="pbio.1002611.ref036">36</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref037">37</xref>]. Observers collected 100-trial blocks during which the second 50 trials (51–100) were identical to the first 50 trials (1–50) except for random permutation of their order [<xref ref-type="bibr" rid="pbio.1002611.ref038">38</xref>]. The degree to which observers reproduce their own responses to the first pass of 50 trials when those trials are re-presented during the second pass is controlled by their intrinsic variability [<xref ref-type="bibr" rid="pbio.1002611.ref039">39</xref>]. Under the standard SDT model [<xref ref-type="bibr" rid="pbio.1002611.ref035">35</xref>] where this variability is captured by a late additive internal noise source, the intensity of the latter can be estimated by reverse application of the SDT model to the empirically measured values of percent correct and percent agreement [<xref ref-type="bibr" rid="pbio.1002611.ref036">36</xref>] (% of trials on which observers gave the same response to 2 identical passes). Because participants demonstrated an appreciable amount of response bias (see <xref ref-type="supplementary-material" rid="pbio.1002611.s004">S3 Fig</xref> for details), the procedure originally developed for the unbiased case [<xref ref-type="bibr" rid="pbio.1002611.ref036">36</xref>] was adapted to the yes–no single-interval protocol used in this study via application of established techniques [<xref ref-type="bibr" rid="pbio.1002611.ref034">34</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref040">40</xref>].</p>
</sec>
<sec id="sec023">
<title>Segmentation algorithms</title>
<p>We applied the following 6 computer vision algorithms from published literature: visual saliency [<xref ref-type="bibr" rid="pbio.1002611.ref003">3</xref>] (Itti-Koch), graph-based visual saliency [<xref ref-type="bibr" rid="pbio.1002611.ref041">41</xref>] (GBVS), hierarchical segmentation [<xref ref-type="bibr" rid="pbio.1002611.ref005">5</xref>] (gPb-HS), normalized cuts [<xref ref-type="bibr" rid="pbio.1002611.ref042">42</xref>] (nCuts), contour detection using superpixel-based candidates and hierarchical visual cues [<xref ref-type="bibr" rid="pbio.1002611.ref043">43</xref>] (HVC), conditional random fields as recurrent neural networks [<xref ref-type="bibr" rid="pbio.1002611.ref023">23</xref>] (CRF-RNN). HVC and gPb-HS had already been applied to BSD500; for those, we obtained results from the algorithm creators. For the remaining models, we implemented them on our hardware and fed them images from BSD500. For some of these algorithms, the output maps are unsuitable for determining whether model output scales with the graded sensitivity measurements via correlation due to the following issues: 1) the output is binary rather than continuous; 2) the identified boundaries are excessively thin (1-pixel width) so that a specific probe insertion point may fail to return a large value on the model map due to slight misregistration, even though the model has effectively labelled that point in the image as belonging to an identified boundary. These issues arose in connection with nCuts, gPb-HS, and CRF-RNN. We thickened the boundaries generated by these models via convolution with an averager of size ∼[0.13°,1%] (gPb-HS), ∼[0.4°,3%] (nCuts), or ∼[0.5°,4%] (CRF-RNN). We verified that the specific choice of thickening parameter/method did not impact the significance of the correlation between model output and human sensitivity. For the purpose of computing rich/poor log-ratios, model output is classified as rich if it exceeds its median value across all probe insertion points, and poor otherwise.</p>
</sec>
<sec id="sec024">
<title>EEG</title>
<sec id="sec025">
<title>Stimulus adjustments</title>
<p>Stimulus/task design was very similar to that described above for experiments not involving neuroimaging except for 3 important adjustments geared towards the EEG: 1) because our focus was on probe-specific waveforms obtained via differential contralateral-ipsilateral activity, we replaced the CRT monitor with a wide-field LCD monitor (active area 88 × 50 cm, luminance range 0–200 cd/m<sup>2</sup>) to maximize probe eccentricity via approximately 1.8 × scaling (in degrees) of the visual stimulus (see [<xref ref-type="bibr" rid="pbio.1002611.ref044">44</xref>] for advantages/disadvantages associated with using TFT monitors in EEG measurements); 2) we cued probe location via slight red tinting (1:4 tint:image ratio) to avoid asynchronous cue presentation (the latter design, used in the behavioural experiments, would result in overlapping visual evoked potentials (VEPs) from cue and stimulus, reducing interpretability of the EEG waveform [<xref ref-type="bibr" rid="pbio.1002611.ref044">44</xref>]); 3) the key press (immediately followed by feedback lasting 100 ms) triggered presentation of the next stimulus after a random interval uniformly distributed between 1.5 and 2 seconds (this longer interval was adopted to avoid ERP overlap across trials).</p>
</sec>
<sec id="sec026">
<title>Data acquisition/analysis</title>
<p>EEG was recorded from 32 active electrodes (10/20 layout) at a sampling rate of 256 Hz by a BrainAmp DC-amplifier (Biosemi). Data analysis was performed with Fieldtrip; we confirmed that virtually identical results were returned by Eeglab. We extracted 1-second segments (re-referenced to Cz) from each trial starting at 200 ms before stimulus onset. Baseline was subtracted from the 200-ms interval preceding the stimulus. We applied 2 different (causal [<xref ref-type="bibr" rid="pbio.1002611.ref045">45</xref>]) filtering regimes for highpass/lowpass cut-offs: 1/20 Hz and 0.5/40 Hz. To steer our analysis towards effects with clear contralateral/ipsilateral characteristics, we linearly rescaled waveforms from individual trials by the distance of the probe from the vertical meridian before averaging across trials (the greater the distance, the larger the waveform). The effects we report do not depend on this procedure, as demonstrated by the results obtained with artefact rejection for which rescaling was not applied. When artefact rejection was applied, it involved an automatic Fieldtrip routine that excluded EOG, muscle, and jump artefacts with conservative parameters that led to a high trial rejection rate of approximately 19%. We confirmed that more lax parameter choices (resulting in lower rejection rates) produced equivalent results. We do not detail this procedure further because artefact rejection made no difference to the primary results. After trial averaging, we obtained 4 waveforms (1 for each probe insertion type) from each electrode in each observer and normalized the traces separately to have equal RMS. Further data analysis involved simple waveform subtractions and/or pooling as described in the main text. Electrodes Oz/Fz are not included because our focus is on lateralized activity [<xref ref-type="bibr" rid="pbio.1002611.ref046">46</xref>].</p>
</sec>
<sec id="sec027">
<title>Confirmatory experiment with cut-out images</title>
<p>Except for replacing natural scenes with their cut-out variants to enhance the conceptual significance of the results, this experiment was deliberately conducted in such a manner as to match the original experiment as closely as possible. The same observers who participated in the original experiment were asked to attend the same number of 3-hour sessions in order to match data mass, resulting in almost identical number of trials for the 2 conditions (14,300 versus 15,700 for natural and cut-out scenes, respectively). All details of the experimental protocol were matched (within margin of inevitable differences such as exact electrode placement, quality of electrophysiological signal, time of day, and similar factors), and identical analysis tools were applied to obtain the processed results.</p>
</sec>
</sec>
<sec id="sec028">
<title>Statistical analysis</title>
<p>We adopt a combination of confidence intervals and <italic>p</italic>-values returned by two-tailed Wilcoxon signed-rank tests to avoid the limitations associated with <italic>p</italic>-values alone [<xref ref-type="bibr" rid="pbio.1002611.ref047">47</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref048">48</xref>]. There are only a few instances in this study where these two approaches are in conflict; we highlight those instances explicitly and investigate them further. The experiments were designed so that the null hypothesis adopted for the Wilcoxon tests would be transparently and unambiguously defined as involving no difference between 2 measurements of the same variable under 2 different conditions. In general, the primary effect reported in this study (top-down modulation of sensitivity) is sufficiently large and robust to eliminate any concern as to its statistical reliability. To verify robustness/replicability, we also adopt a confirmatory approach where we tackle the primary result from multiple directions.</p>
</sec>
</sec>
<sec id="sec029" sec-type="results">
<title>Results</title>
<sec id="sec030">
<title>Image reconstruction is controlled by top-down information</title>
<p>We grafted an oriented wavelet into a natural scene (<xref ref-type="fig" rid="pbio.1002611.g001">Fig 1E and 1F</xref>), and asked human observers to determine whether its orientation was congruent (<bold>E</bold>) or incongruent (<bold>F</bold>) with the structure locally defined by the scene [<xref ref-type="bibr" rid="pbio.1002611.ref010">10</xref>] (see <xref ref-type="sec" rid="sec002">Methods</xref>). The orientation content of the wavelet was disrupted via the addition of orientation noise [<xref ref-type="bibr" rid="pbio.1002611.ref008">8</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref010">10</xref>] (<xref ref-type="fig" rid="pbio.1002611.g001">Fig 1G</xref>). This manipulation engaged observers in active image reconstruction of a locally corrupted signal and supported noise-based characterization of relevant phenomena using state-of-the-art psychophysical tools [<xref ref-type="bibr" rid="pbio.1002611.ref008">8</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref031">31</xref>].</p>
<p>The grafted wavelet, which we refer to as the “probe” stimulus, could be inserted at 1 of 4 different locations within each image from a database of approximately 450 scenes. The 4 insertion points represented all combinations of poor versus rich locations within 2 maps derived from the natural scene, which we refer to as the “bottom-up” and “top-down” maps, and were selected across the image database so that map values could be independently manipulated (<xref ref-type="supplementary-material" rid="pbio.1002611.s002">S1 Fig</xref>). The “bottom-up” map (<xref ref-type="fig" rid="pbio.1002611.g001">Fig 1C</xref>) reflects energy content as returned by common edge detection algorithms [<xref ref-type="bibr" rid="pbio.1002611.ref007">7</xref>] (see <xref ref-type="sec" rid="sec002">Methods</xref>); designated poor/rich locations on this map are indicated by dashed/solid circles. The “top-down” map (<xref ref-type="fig" rid="pbio.1002611.g001">Fig 1B</xref>) reflects consensus across human participants when asked to segment individual scenes by marking relevant boundaries as part of the Berkeley Segmentation Dataset project [<xref ref-type="bibr" rid="pbio.1002611.ref005">5</xref>] (BSD500). Rich regions (indicated by green circles) denote boundaries marked by all participants, while poor regions (red circles) were selected by some participants but not others (see <xref ref-type="sec" rid="sec002">Methods</xref>). We rely on the “top-down” map as a proxy for the segmented representation of the scene afforded by the human visual system [<xref ref-type="bibr" rid="pbio.1002611.ref005">5</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref006">6</xref>]. Our choice of the terms “bottom-up” and “top-down” was motivated by lack of better options [<xref ref-type="bibr" rid="pbio.1002611.ref030">30</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref049">49</xref>], rather than accurate descriptive purposes. As we discuss later in the article, we do not think these terms are adequate for describing the role played by the information contained within the maps, but they are nevertheless useful as labels for facilitating exposition of the results.</p>
<p><xref ref-type="fig" rid="pbio.1002611.g002">Fig 2A–2D</xref> show small square regions around various probe insertion points from randomly selected scenes. From simply looking at these images, it is apparent that the examples in <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2A and 2C</xref>, corresponding to poor locations on the bottom-up map, contain visibly less edge-contrast than those in <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2B and 2D</xref>, corresponding to rich locations on the bottom-up map. This is not surprising: it is the criterion by which insertion points were selected in the first place with reference to the bottom-up map (see <xref ref-type="sec" rid="sec002">Methods</xref>). It is more interesting to compare, by the same token of summary visual inspection, the collection in <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2A and 2B</xref>, corresponding to rich locations on the top-down map, versus that in <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2C and 2D</xref>, corresponding to poor locations on the top-down map: there is no obvious difference between the 2 collections at this level of inspection. In other words, if we consider the most immediate and perceptually obvious content of the image around the probe, the difference is much greater as we move along the bottom-up map (left to right in <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2A–2D</xref>) than it is as we move along the top-down map (top to bottom). In fact, there seems to be virtually no change of statistical properties for the latter transition.</p>
<fig id="pbio.1002611.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002611.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Performance is driven by top-down map.</title>
<p><bold>A</bold>-<bold>D</bold> show collections of image regions (approximately 3 × probe size) surrounding probe insertion points (with embedded congruent probe) at rich/poor locations on top-down map (<bold>A</bold>-<bold>B</bold> versus <bold>C</bold>-<bold>D</bold>) or bottom-up map (<bold>B</bold>,<bold>D</bold> versus <bold>A</bold>,<bold>C</bold>). The poor→rich transition is perceptually evident across the bottom-up map (left → right). <bold>E</bold> plots sensitivity (d′) for poor (<italic>y</italic> axis) versus rich (<italic>x</italic> axis) locations on the bottom-up (black) or top-down map (green) in individual observers (1 symbol per observer), as well as precue (<italic>y</italic> axis) versus postcue (<italic>x</italic> axis) configurations (magenta). <bold>F</bold>-<bold>G</bold> plot sensitivity rich/poor log-ratios for top-down (<italic>y</italic> axis) and bottom-up (<italic>x</italic> axis) comparisons when scenes were upright or inverted (black or red in <bold>F</bold>) and precued or postcued (black or magenta in <bold>G</bold>). Error bars plot ±1 SEM. Coloured diagonal segments in <bold>E</bold> plot 95% confidence intervals for data projected along negative diagonal. Horizontal/vertical segments near <italic>x</italic>/<italic>y</italic> axes in <bold>F</bold>-<bold>G</bold> plot confidence intervals for bottom-up/top-down log-ratio effects; light-coloured contours indicate data spread for visualization aid. Data for this figure is available from <xref ref-type="supplementary-material" rid="pbio.1002611.s008">S2 Data</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.g002" xlink:type="simple"/>
</fig>
<p>It therefore comes as a surprising finding that, when we measure how well human observers are able to reconstruct the orientation of wavelet signals inserted at those locations, their performance displays the opposite trend: there is no difference in sensitivity for rich (<italic>x</italic> axis in <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2E</xref>) versus poor (<italic>y</italic> axis) locations on the bottom-up map (black symbols scatter around diagonal equality line at <italic>p</italic> = 0.74), while there is a marked difference for the poor–rich comparison on the top-down map (green symbols fall below equality line at <italic>p</italic> &lt; 0.01; see also confidence intervals indicated by black/green diagonal segments). This effect is plotted more compactly in <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F</xref> (black symbols) in the form of rich/poor log-ratio values: for the bottom-up comparison (<italic>x</italic> axis), log-ratios scatter around 0 (corresponding to no difference between rich and poor values), while for the top-down comparison (<italic>y</italic> axis) they all fall above 0 (rich &gt; poor; <italic>p</italic> &lt; 0.01; see confidence interval indicated by black segment near <italic>y</italic> axis). It appears that the ability of the human visual system to extract local orientation signals depends greatly on whether those signals correspond to richly versus poorly represented boundaries within the top-down map, and not at all on whether the boundaries are rich or poor on the bottom-up map (see also [<xref ref-type="bibr" rid="pbio.1002611.ref050">50</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref051">51</xref>]), even though visual inspection of those local boundaries demonstrates no difference for the former comparison (<xref ref-type="fig" rid="pbio.1002611.g002">Fig 2A and 2B</xref> versus <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2C and 2D</xref>) and an easily perceptible difference for the latter (<xref ref-type="fig" rid="pbio.1002611.g002">Fig 2B and 2D</xref> versus <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2A and 2C</xref>).</p>
</sec>
<sec id="sec031">
<title>Top-down effects are reduced when scene intelligibility is degraded</title>
<p>We carried out an extensive series of additional experiments to determine whether the top-down effect is robust and how far it generalizes across manipulations of cognitive factors and scene characteristics. We found that it does not require semantic labelling of scene content (it is unaffected by image inversion [<xref ref-type="bibr" rid="pbio.1002611.ref052">52</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref053">53</xref>] or contrast reversal [<xref ref-type="bibr" rid="pbio.1002611.ref054">54</xref>], see red symbols in <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F</xref> and <xref ref-type="supplementary-material" rid="pbio.1002611.s001">S1 Text</xref>), operates independently of spatial attention (it is unaffected by spatial cueing, see <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2G</xref>, <xref ref-type="supplementary-material" rid="pbio.1002611.s001">S1 Text</xref> and <xref ref-type="supplementary-material" rid="pbio.1002611.s005">S1 Video</xref>) but does depend on specific manipulations of various image properties such as spatial frequency (it is retained with highpass but lost with lowpass scenes, see blue/red symbols/confidence intervals in <xref ref-type="fig" rid="pbio.1002611.g003">Fig 3C</xref>), orientation (it is reduced by image warping, see <xref ref-type="fig" rid="pbio.1002611.g003">Fig 3D–3F</xref>), object-boundary definition (it is retained with cut-out scenes but lost when object boundaries are defined solely by lines, see blue/red symbols/confidence intervals in <xref ref-type="fig" rid="pbio.1002611.g003">Fig 3I</xref>) and others (see <xref ref-type="supplementary-material" rid="pbio.1002611.s001">S1 Text</xref>). For example, although the top-down effect is retained with highpass-filtered images, its magnitude is smaller than observed with undistorted scenes (green symbols in inset to <xref ref-type="fig" rid="pbio.1002611.g003">Fig 3C</xref> fall above diagonal equality line at <italic>p</italic> &lt; 0.01). Similarly, although this effect is measurable for cut-out scenes, its magnitude is again smaller than observed with intact images (green symbols in inset to <xref ref-type="fig" rid="pbio.1002611.g003">Fig 3I</xref> fall above equality line at <italic>p</italic> &lt; 0.03).</p>
<fig id="pbio.1002611.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002611.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Scene manipulations may eliminate top-down effects and/or produce bottom-up effects.</title>
<p>Natural scenes were highpass/lowpass filtered (<bold>A</bold>-<bold>B</bold>), warped a bit or a lot (<bold>D</bold>-<bold>E</bold>), and converted to cut-out or line versions (<bold>G</bold>-<bold>H</bold>). <bold>C</bold>,<bold>F</bold>,<bold>I</bold> are plotted to the conventions adopted in <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F and 2G</xref>; insets plot top-down effects for specific comparisons. Data for this figure is available from <xref ref-type="supplementary-material" rid="pbio.1002611.s008">S2 Data</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.g003" xlink:type="simple"/>
</fig>
<p>Interestingly, under some conditions (undistorted scenes) we only observe a top-down effect (<xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F</xref>), under other conditions top-down and bottom-up effects coexist (highpass scenes), and finally there are conditions for which only the bottom-up effect is observed (lowpass scenes; please refer to <xref ref-type="supplementary-material" rid="pbio.1002611.s001">S1 Text</xref> for detailed statistics including outlier detection). The top-down effect is therefore not a trivial inevitable consequence of task/probe design, and our measurement protocols are adequate for exposing both top-down and bottom-up effects: when one is not observed, this reflects a genuine lack of contribution from the corresponding information source, rather than failure on the part of our methods to resolve its presence.</p>
<p>We summarize the results of all image manipulations in <xref ref-type="fig" rid="pbio.1002611.g004">Fig 4</xref>: whenever the natural scene is manipulated in some way, whether by warping its orientation content (purple/blue), filtering its spatial frequency structure (yellow/orange), or artifically perturbing its segmentation content (green/red), the top-down effect is consistently reduced (data points shift downward towards origin), to the extent that it may be entirely eliminated (red/cyan/yellow). This is not to say that the full natural appearance of the image is necessary to observe top-down effects: cut-out scenes, although still interpretable as containing natural elements, do not “look” natural. In general, however, top-down effects were reduced for a number of image manipulations, indicating that their origin involves various aspects of natural scene content. <xref ref-type="fig" rid="pbio.1002611.g004">Fig 4</xref> also illustrates that top-down effects were not related to the absolute difficulty of the discrimination task (efficiency is indicated by symbol size; there does not appear to be any lawful relationship between symbol size and symbol position within the graph).</p>
<fig id="pbio.1002611.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002611.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Summary of image manipulations.</title>
<p>Top-down (<italic>y</italic> axis) and bottom-up (<italic>x</italic> axis) effects are plotted for all scene manipulations averaged across observers (each symbol shows average for the indicated configuration, ovals plot ±1 SD across observers). Symbol size scales with absolute efficiency [<xref ref-type="bibr" rid="pbio.1002611.ref035">35</xref>] (directly proportional to d′ and inversely proportional to stimulus SNR). Data for this figure is available from <xref ref-type="supplementary-material" rid="pbio.1002611.s008">S2 Data</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec032">
<title>Top-down effects are ultrafast and spatially distributed</title>
<p>Feedback interpretations of top-down effects [<xref ref-type="bibr" rid="pbio.1002611.ref055">55</xref>] may lead to the expectation that these effects should depend on the temporal order of information accrual from the probe in relation to the surrounding natural scene [<xref ref-type="bibr" rid="pbio.1002611.ref056">56</xref>]. We tested this prediction by designing “zooming” variants of our stimulus, where a smooth transition was enacted between a probeless scene and a sceneless probe for any probe location and type (<xref ref-type="fig" rid="pbio.1002611.g005">Fig 5A–5D</xref>; see <xref ref-type="sec" rid="sec002">Methods</xref> and <xref ref-type="supplementary-material" rid="pbio.1002611.s006">S2 Video</xref>). We found that, although the spatiotemporal relationship between scene and probe enhances absolute sensitivity in the direction of scene analysis facilitating probe analysis (blue symbols in <xref ref-type="fig" rid="pbio.1002611.g005">Fig 5E</xref>), the qualitative operation of the system in relation to the top-down/bottom-up differential effect is independent of spatiotemporal dynamics (<xref ref-type="fig" rid="pbio.1002611.g005">Fig 5F and 5G</xref>).</p>
<fig id="pbio.1002611.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002611.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Scene-probe dynamics impacts absolute sensitivity but not differential effects.</title>
<p>Zooming stimuli involve smooth transitions from scenes without probes (leftmost icons in <bold>A</bold>-<bold>D</bold>) to probes without scenes (rightmost icons) in either scene-to-probe “zoom-in” direction (left to right in <bold>A</bold>-<bold>D</bold>) or probe-to-scene “zoom-out” direction (right to left). <bold>E</bold> plots sensitivity (d′) for zoom-in (<italic>y</italic> axis) versus zoom-out (<italic>x</italic> axis) configurations (blue symbols) and long-duration (300 ms, <italic>x</italic> axis) versus short-duration (100 ms, <italic>y</italic> axis) stimuli (red) using conventions similar to <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2E</xref>. <bold>F</bold>-<bold>G</bold> plot corresponding log-ratios using conventions similar to <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F and 2G</xref>. Data for this figure is available from <xref ref-type="supplementary-material" rid="pbio.1002611.s008">S2 Data</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.g005" xlink:type="simple"/>
</fig>
<p>We also investigated spatial and temporal factors independently, rather than compounded in a zooming stimulus. To study space, we inserted a gap between the probe and surrounding scene (<xref ref-type="fig" rid="pbio.1002611.g006">Fig 6B–6E</xref>); we found no impact on top-down effects up to large gaps (<xref ref-type="fig" rid="pbio.1002611.g006">Fig 6F and 6G</xref>), indicating that the origin of the top-down signal is spatially global. To study time, we varied stimulus duration and found that the perceptual system becomes gradually dominated by top-down information very early in the processing pipeline (<xref ref-type="fig" rid="pbio.1002611.g006">Fig 6H</xref>, see also inset), to the extent that the initial dynamics of top-down control over bottom-up information may be sufficiently fast (&lt; 30 ms) to approach the limit of reliable empirical characterization using psychophysical methods (see <xref ref-type="supplementary-material" rid="pbio.1002611.s001">S1 Text</xref> for detailed analysis of the bottom-up effect suggested by the black trace in <xref ref-type="fig" rid="pbio.1002611.g006">Fig 6H</xref> at 10–20 ms). These limitations are exacerbated by known conceptual difficulties with the interpretation of stimulus duration as reflecting processing time [<xref ref-type="bibr" rid="pbio.1002611.ref057">57</xref>], whether in the presence or absence of a postmask (see <xref ref-type="supplementary-material" rid="pbio.1002611.s001">S1 Text</xref> for detailed justification of the deliberate choice to avoid a postmask for the duration experiments [<xref ref-type="bibr" rid="pbio.1002611.ref058">58</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref059">59</xref>]). In the next section, we gain further insight into the ultrafast range via electrophysiological measurements.</p>
<fig id="pbio.1002611.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002611.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Top-down effect is spatially global (F) but reduced at ultrashort durations (H).</title>
<p><bold>F</bold> plots d′ log-ratios for bottom-up (black) and top-down (green) effects as a function of gap size (<italic>x</italic> axis) for spatial gaps of differing size between probe and scene (<bold>A</bold>-<bold>E</bold>), pooled across observers. Red trace plots overall d′. Shading shows ±1 SEM. <bold>G</bold> plots log-ratios for individual observers (conventions similar to <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F</xref>) pooled separately from small (gap &lt; probe, blue) and large (gap &gt; probe, magenta) gap sizes. <bold>H</bold>-<bold>I</bold> show similar measurements for varying stimulus durations, short (&lt; 30 ms, blue) and long (≥30 ms, magenta). Inset to <bold>H</bold> replots green data with rescaled <italic>y</italic> axis to emphasize positive trend (solid line shows best linear fit, dashed lines 95% confidence intervals for fit). Vertical/horizontal arrows in <bold>G</bold>,<bold>I</bold> point to average <italic>y</italic>/<italic>x</italic> values for effects associated with significant <italic>p</italic>-values (&lt;0.05) from Wilcoxon signed-rank test for different than 0 (<italic>p</italic>-values are indicated next to arrow). Thin blue segments near axes in <bold>I</bold> show confidence intervals for blue dataset after removal of data point at bottom-right of panel. Data for this figure is available from <xref ref-type="supplementary-material" rid="pbio.1002611.s008">S2 Data</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.g006" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec033">
<title>Electrophysiological markers of bottom-up/top-down effects</title>
<p>The evidence presented above exposes signatures of a process that is spatially delocalized (<xref ref-type="fig" rid="pbio.1002611.g006">Fig 6F and 6G</xref>) but highly localized in time (<xref ref-type="fig" rid="pbio.1002611.g006">Fig 6H and 6I</xref>). The appropriate neuroimaging tool for probing these characteristics is the EEG with associated VEP [<xref ref-type="bibr" rid="pbio.1002611.ref044">44</xref>]. Unsurprisingly, the bulk of EEG activity was measured from the occipital electrodes (black circles in <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7B</xref>) in both hemispheres (blue/red contours). Our focus is not on the VEP per se, but on the probe-specific component of the VEP; for this reason, we render the analysis probe-selective by computing the difference between the VEP contralateral to probe location (red contours) and its ipsilateral counterpart [<xref ref-type="bibr" rid="pbio.1002611.ref046">46</xref>] (blue contours). <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7A</xref> plots the time course of these two waveforms pooled across the occipital electrodes, alongside their difference (green trace). With simple multistimulus arrays, the occipital contra-minus-ipsi waveform has been termed negativity 200-ms posterior contralateral (N2pc) [<xref ref-type="bibr" rid="pbio.1002611.ref046">46</xref>] or posterior contralateral negativity (PCN) [<xref ref-type="bibr" rid="pbio.1002611.ref060">60</xref>]. The green waveform we measure here must bear some relationship to these lateralized evoked potentials; however, our stimulus consists of a complex natural scene (as opposed to stereotyped search arrays [<xref ref-type="bibr" rid="pbio.1002611.ref046">46</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref060">60</xref>]), so that it is not necessarily the case that we should observe measurable differences between contralateral and ipsilateral waveforms. The fact that we do observe an idiosyncratic difference specific to a probe embedded within a natural scene is, in itself, a noteworthy result.</p>
<fig id="pbio.1002611.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002611.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Top-down effects operate quickly within occipital cortex.</title>
<p><bold>A</bold>,<bold>C</bold>,<bold>D</bold> plot evoked potentials from occipital, central, and frontal electrodes marked by black, magenta, and orange circles in <bold>B</bold>. Blue/red trace shows waveform from electrodes ipsilateral/contralateral to probe location; green trace shows contralateral-minus-ipsilateral difference (shading shows ±2 SEM). Contour plots in <bold>B</bold> show interpolated scalp distribution of potential RMS for ipsilateral/contralateral waveforms (blue/red), as well as the ratio between contra-minus-ipsi waveform RMS and overall (ipsi + contra) RMS (green). <bold>E</bold>-<bold>F</bold> show the difference between rich and poor probe insertions for contra-minus-ipsi vaweform with respect to top-down (<bold>E</bold>) and bottom-up (<bold>F</bold>) maps, separately for the different electrodes (indexed on the <italic>y</italic> axis as pairs from which individual rows were computed), in the form of <italic>Z</italic> scores across participants. <bold>G</bold> plots RMS-normalized modulations (see <xref ref-type="sec" rid="sec002">Methods</xref>) in <bold>E</bold>/<bold>F</bold> on <italic>y</italic>/<italic>x</italic> axes pooled within black rectangles (occipital electrodes) in <bold>E</bold>/<bold>F</bold>, separately for different participants (1 symbol per participant, conventions similar to <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F</xref>); solid symbols refer to intact scenes, open symbols to cut-out variant, blue symbols to results following artefact rejection (see <xref ref-type="sec" rid="sec002">Methods</xref>). <bold>H</bold> plots similar results from modulations pooled within magenta rectangles (central electrodes) in <bold>E</bold>/<bold>F</bold>; inset to <bold>H</bold> from modulations within orange rectangles (frontal electrodes). <bold>I</bold>-<bold>K</bold> plot the pooled quantities in <bold>G</bold>-<bold>H</bold> for specific comparisons on <italic>x</italic>- versus <italic>y</italic>-axes (top-down and bottom-up values are collated without distinction for this analysis): intact (undistorted) scenes versus cut-out variant (<bold>I</bold>); highpass/lowpass filtering of 1/20 Hz versus 0.5/40 Hz (<bold>J</bold>); values for intact scenes versus d′ log-ratios from <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F</xref> (<bold>K</bold>). Ovals in <bold>I</bold>-<bold>K</bold> are aligned with best-fit line, with axes matched to 2 SD for values projected onto axes parallel/orthogonal to line. Data for this figure is available from <xref ref-type="supplementary-material" rid="pbio.1002611.s008">S2 Data</xref>. EEG, electroencephalogram; RMS, root-mean-square; VEP, visual evoked potential.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.g007" xlink:type="simple"/>
</fig>
<p>The amplitude of the difference waveform is nearly 1 order of magnitude smaller than the original waveforms (compare scaling of <italic>y</italic> axis for green versus black labels in <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7A</xref>), and modulates primarily within 2 time epochs roughly corresponding to 50–150 ms and 250–350 ms. To gauge the relative amplitude of the difference waveform against the amplitude of the original waveforms across the scalp, we plot a related quantity in <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7B</xref> (green contours). In relative terms, the difference waveform is most pronounced within central (magenta) and frontal (orange) electrodes. When we examine the time course of the relevant waveforms for these 2 electrode regions, however, we find that the difference waveform modulates only within the late phase (green traces in <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7C and 7D</xref>). Therefore, although the difference modulation is comparatively larger within central/frontal than occipital electrodes, the bulk of this modulation happens around stimulus disappearance, possibly reflecting offset responses and/or decisional/memory processes [<xref ref-type="bibr" rid="pbio.1002611.ref061">61</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref062">62</xref>]. In contrast, the difference waveform returned by occipital electrodes presents an early modulation more consistent with previous measurements of sensory-related activity [<xref ref-type="bibr" rid="pbio.1002611.ref063">63</xref>] and clearly connected with visually-specific responses to stimulus information [<xref ref-type="bibr" rid="pbio.1002611.ref064">64</xref>].</p>
<p>Based on the above observations, we focus our subsequent analysis on the 2 EEG processes that appear to dominate the electrophysiological measurements: an early process occurring within the occipital region (<xref ref-type="fig" rid="pbio.1002611.g007">Fig 7A</xref>), possibly connected with what has been termed N1pc in previous studies [<xref ref-type="bibr" rid="pbio.1002611.ref065">65</xref>], and a later process occurring within the central/frontal region (<xref ref-type="fig" rid="pbio.1002611.g007">Fig 7C and 7D</xref>), possibly connected with the sustained posterior contralateral negativity (SPCN) waveform [<xref ref-type="bibr" rid="pbio.1002611.ref061">61</xref>] (both may be at least partly connected with the N2pc in light of its potentially multicomponent nature [<xref ref-type="bibr" rid="pbio.1002611.ref066">66</xref>] and variable latency [<xref ref-type="bibr" rid="pbio.1002611.ref067">67</xref>]). Because our interest is in differential poor/rich effects for probe insertion, we compute contra-minus-ipsi waveforms separately for rich and poor locations, subtract rich from poor, and plot the result across all electrodes and time points for both top-down and bottom-up maps in <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7E and 7F</xref> respectively. These modulations are, therefore, 2 steps removed from the VEP: first by taking the difference between contralateral and ipsilateral waveforms to expose probe-specific effects [<xref ref-type="bibr" rid="pbio.1002611.ref064">64</xref>], and further by taking the difference between rich and poor locations to expose the differential impact of bottom-up/top-down maps.</p>
<p>The only region where rich/poor differential effects display robust intersubject consistency (large <italic>Z</italic> scores) is indicated by the black rectangular outline in <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7E</xref>, corresponding to the early epoch (0–100 ms) of occipital activity. There appear to be other modulations within the surface plots; however, they are not robust and are unlikely to reflect relevant processes (see below). Based on our previous observations from <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7B–7D</xref>, we consider the 2 additional regions indicated by magenta/orange rectangles, corresponding to the late epoch (250–350 ms) of central/frontal activity. To evaluate occipital activity quantitatively, we sum modulations within the black rectangles from both bottom-up (<xref ref-type="fig" rid="pbio.1002611.g007">Fig 7F</xref>) and top-down (<bold>E</bold>) descriptors, and plot them on <italic>x</italic> and <italic>y</italic> axes respectively in <bold>G</bold> (1 symbol per observer). Similarly to the behavioural effects (black symbols in <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F</xref>), these measurements present a top-down effect with no bottom-up effect (black solid symbols fall above horizontal dashed line at <italic>p</italic> &lt; 0.02 and scatter around vertical dashed line at <italic>p</italic> = 0.37). When we sum activity within the magenta rectangles for central electrodes, the resulting measurements display neither effect (solid symbols in <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7H</xref> scatter around the origin with no significant (<italic>p</italic> &gt; 0.05) departures from the dashed lines). A similar result is obtained for frontal electrodes (inset to <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7H</xref>).</p>
<p>In additional experiments, we replicated the above effects using manipulated cut-out scenes (open symbols in <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7G</xref>) for which we had determined that the behavioural top-down effects also survived (blue data in <xref ref-type="fig" rid="pbio.1002611.g003">Fig 3I</xref>). More specifically, differential effects for cut-out scenes (<italic>x</italic>/<italic>y</italic> values from <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7G</xref>) are strongly correlated across observers with those for undistorted scenes (r value is 0.76 at <italic>p</italic> &lt; 0.002) only for occipital electrodes (black symbols in <bold>I</bold>), not for other electrode clusters (see <xref ref-type="supplementary-material" rid="pbio.1002611.s001">S1 Text</xref>); the two datasets come from independent experiments, providing clear evidence that the adopted analysis/metric exposes genuine structure in the EEG. Furthermore, this structure is related to the perceptual effects, as evidenced not only by the similarity between electrophysiological and behavioural patterns (compare Figs <xref ref-type="fig" rid="pbio.1002611.g002">2F</xref> with <xref ref-type="fig" rid="pbio.1002611.g007">7G</xref>) but also by the significant correlation (r = 0.45, <italic>p</italic> &lt; 0.02) between EEG and psychophysical markers for occipital electrodes in <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7K</xref>. Finally, we established that the EEG effects remain measurable when relevant aspects of the analysis are modified, such as choice of low/high-pass cut-off frequencies (<xref ref-type="fig" rid="pbio.1002611.g007">Fig 7J</xref>) and exclusion/inclusion of common artefacts (see <xref ref-type="supplementary-material" rid="pbio.1002611.s001">S1 Text</xref>).</p>
<p>We conclude that scalp signals originating from occipital cortex are modulated by neural constructs connected with the top-down map. These modulations become measurable very quickly (approximately 50 ms after stimulus onset), reflect the behavioural measurements (<xref ref-type="fig" rid="pbio.1002611.g007">Fig 7G and 7K</xref>), generalize across independent experiments (<bold>I</bold>) and are not restricted to narrow filtering specifications (<bold>J</bold>). These measurements do not support more precise estimates of the timescale involved, not least because the exact figures will depend on several (and to a large extent arbitrary) constraints on the relevant analysis (e.g. which specific electrodes, filtering regime and others). The relevant region within <xref ref-type="fig" rid="pbio.1002611.g007">Fig 7E</xref> (indicated by black rectangular outline) presents substantial modulations (reflected by red tint) between 30 ms and 70 ms. More accurate estimates of the timescale involved will require further EEG investigations combined with relevant single-unit measurements [<xref ref-type="bibr" rid="pbio.1002611.ref053">53</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref068">68</xref>].</p>
</sec>
<sec id="sec034">
<title>Sensitivity is enhanced by orientation retuning, not internal noise reduction</title>
<p>We have extensively documented that human sensitivity for performing probe discrimination is enhanced when the probe is inserted at rich locations on the top-down map. These effects are large (approximately 2× with mean enhancement across observers of an added 83%), dissociated from response criterion shifts (<xref ref-type="supplementary-material" rid="pbio.1002611.s004">S3 Fig</xref>), and easily measurable: when 121 independent log-ratio estimates are combined from all experiments that individually showed statistically measurable top-down effects, the aggregate dataset is significant (different than 0) at <italic>p</italic> &lt; 10<sup>−20</sup> with 99.9% confidence interval of 0.43–0.63, meaning that the existence of the top-down effect is beyond doubt. Furthermore, this effect can be measured in the form of correlated scatter between sensitivity and top-down value, demonstrating that the performance enhancement tracks top-down information in a proportional fine-grained fashion (see <xref ref-type="supplementary-material" rid="pbio.1002611.s001">S1 Text</xref>). Sensitivity measurements by themselves, however, do not impose sufficient constraints on possible sources of improved discrimination to allow conclusions about potentially underlying mechanisms [<xref ref-type="bibr" rid="pbio.1002611.ref069">69</xref>].</p>
<p>If we adopt a minimal SDT model [<xref ref-type="bibr" rid="pbio.1002611.ref035">35</xref>] (<xref ref-type="fig" rid="pbio.1002611.g008">Fig 8A</xref>) whereby orientation energy within the probe is processed by an orientation-selective filter [<xref ref-type="bibr" rid="pbio.1002611.ref008">8</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref070">70</xref>], a source of intrinsic variability is added to the filter response [<xref ref-type="bibr" rid="pbio.1002611.ref037">37</xref>], and an output binary decision is produced [<xref ref-type="bibr" rid="pbio.1002611.ref071">71</xref>], there are 3 fundamentally distinct ways in which the sensitivity of this mechanism may be enhanced: 1) by reducing its internal noise (red in <xref ref-type="fig" rid="pbio.1002611.g008">Fig 8A</xref>); 2) by sharpening filter tuning around the congruent signal (blue thick line in <bold>A</bold>); 3) by sharpening filter tuning around the incongruent signal (blue thin line). We sought to determine which of these alternatives apply. To estimate internal noise, we performed additional experiments using an established double-pass methodology [<xref ref-type="bibr" rid="pbio.1002611.ref036">36</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref040">40</xref>] (see <xref ref-type="sec" rid="sec002">Methods</xref>). The resulting estimates (0.64/0.84/1.38 at 5/50/95 percentiles) fall within the expected range [<xref ref-type="bibr" rid="pbio.1002611.ref037">37</xref>]; more importantly, they are not modulated by probe location along either the top-down or bottom-up map (red data points in <xref ref-type="fig" rid="pbio.1002611.g008">Fig 8B</xref> scatter around origin).</p>
<fig id="pbio.1002611.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002611.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Top-down enhancement is driven by sensory retuning.</title>
<p><bold>A</bold> sketches minimal SDT model consisting of front-end filter (grey box) followed by additive internal noise (black random trace pointing to + symbol); sensitivity may be enhanced by reducing internal noise (red trace), sharpening filter around congruent (thick blue line) and/or incongruent orientation (thin blue line). <bold>B</bold> plots rich/poor log-ratios for internal noise estimates (red) and projected sensitivity from filter estimates (blue) returned by psychophysical reverse correlation (plotting conventions similar to <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F</xref>). Aggregate perceptual filters are shown in <bold>C</bold>-<bold>D</bold> for rich vs poor locations on top-down (<bold>C</bold>, green versus red) and bottom-up (<bold>D</bold>, solid versus open) maps. Congruent/incongruent orientations are indicated by orange/magenta vertical lines (0 and <italic>π</italic>/2 on <italic>x</italic> axis). Error bars show ±1 SEM. Lines show fits from 2 Gaussian functions of opposite sign centred on congruent/incongruent orientations (for visualization only). Shading in <bold>C</bold> plots ±1 SD across simulations from gain-control model (inset), consisting of 2 front-end filters oriented along congruent (left icon in inset) and incongruent (right icon) orientations. Model simulations for red/green shading were generated by red/green-tinted front-end filters (transition indicated by blue arrows). Data for this figure is available from <xref ref-type="supplementary-material" rid="pbio.1002611.s008">S2 Data</xref>. SDT, signal detection theory.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.g008" xlink:type="simple"/>
</fig>
<p>To estimate orientation tuning of the perceptual process, we exploited a psychophysical variant of reverse correlation [<xref ref-type="bibr" rid="pbio.1002611.ref031">31</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref034">34</xref>] applied to the orientation noise injected into the probe. In line with previous work using isolated probes [<xref ref-type="bibr" rid="pbio.1002611.ref008">8</xref>], the retrieved orientation-tuning functions peak at the congruent orientation (indicated by orange vertical line in <xref ref-type="fig" rid="pbio.1002611.g008">Fig 8C and 8D</xref>) and present a negative modulation at the incongruent orientation (indicated by magenta vertical line). This characteristic remains unchanged when probes target rich as opposed to poor locations on the bottom-up map (compare solid with open data in <bold>D</bold>); however, it undergoes substantial retuning along the top-down map: at rich locations, tuning is sharper around both congruent and incongruent orientations (compare green with red data in <bold>C</bold>). To quantify these effects and make them directly comparable to the performance measurements, we compute the expected sensitivity associated with the shape of individual tuning functions and plot it as log-ratios in <xref ref-type="fig" rid="pbio.1002611.g008">Fig 8B</xref> (see <xref ref-type="sec" rid="sec002">Methods</xref>). We observe a sizeable top-down effect (blue symbols fall above the horizontal dashed line at <italic>p</italic> &lt; 0.01) without any bottom-up effect (blue symbols scatter around vertical dashed line at <italic>p</italic> = 0.46), mirroring the effects produced by direct sensitivity measurements (<xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F</xref>).</p>
<p>We conclude that the sensitivity enhancement associated with rich locations on the top-down map is the outcome of sensory retuning [<xref ref-type="bibr" rid="pbio.1002611.ref010">10</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref072">72</xref>] and not internal noise reduction, in line with other aspects of sensory processing [<xref ref-type="bibr" rid="pbio.1002611.ref069">69</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref073">73</xref>]. Further, our data demonstrate that retuning occurs at both congruent and incongruent orientations (thick/thin blue lines in <xref ref-type="fig" rid="pbio.1002611.g008">Fig 8A</xref>). Retuning of this kind can be modelled by a physiologically plausible circuit such as the gain control operator [<xref ref-type="bibr" rid="pbio.1002611.ref011">11</xref>] in the inset to <xref ref-type="fig" rid="pbio.1002611.g008">Fig 8C</xref>. More specifically, small changes to the parameterization of this minimal model (indicated by blue arrows, see <xref ref-type="sec" rid="sec002">Methods</xref>) produce orientation-sharpening effects that closely match those exposed by data (model predictions are shown by green/red shaded regions).</p>
</sec>
<sec id="sec035">
<title>Deep networks generate proxy top-down maps</title>
<p>What is the potential origin of the signal driving the circuit transition in <xref ref-type="fig" rid="pbio.1002611.g008">Fig 8C</xref> and the associated sensitivity enhancement? We implemented a selection of computer vision algorithms (see <xref ref-type="sec" rid="sec002">Methods</xref>) to determine whether the resulting scene representations correlate positively with human sensitivity as observed along the top-down map (<xref ref-type="fig" rid="pbio.1002611.g009">Fig 9A</xref>; see <xref ref-type="supplementary-material" rid="pbio.1002611.s001">S1 Text</xref>). There is no such correlation when sensitivity is similarly plotted against the bottom-up map (r = −0.02, <italic>p</italic> = 0.3, <xref ref-type="fig" rid="pbio.1002611.g008">Fig 8B</xref>) or established saliency/segmentation algorithms (orange/blue in <xref ref-type="fig" rid="pbio.1002611.g009">Fig 9D</xref>); however, last-generation deep networks (red) generate map values that correlate significantly with human sensitivity (see also <xref ref-type="fig" rid="pbio.1002611.g009">Fig 9C</xref>). We focus on a recent deep convolutional network (DCN) for semantic segmentation [<xref ref-type="bibr" rid="pbio.1002611.ref023">23</xref>] (CRF-RNN) that is able to achieve a correlation value comparable to that returned by nonconsensus values on the top-down map (open green symbol in <xref ref-type="fig" rid="pbio.1002611.g009">Fig 9D</xref>). When sensitivity log-ratios from individual observers are computed with relation to the probe rich/poor classification generated by the DCN, we observe a nontrivial top-down effect (red data in <bold>E</bold>, <italic>p</italic> &lt; 0.01) not captured by established segmentation algorithms [<xref ref-type="bibr" rid="pbio.1002611.ref005">5</xref>] (blue data). The DCN seems able to exclude physically rich locations in the image that are not perceptually interesting (examples are indicated by yellow circles in <xref ref-type="fig" rid="pbio.1002611.g009">Fig 9E</xref>), while this task remains challenging for some other computer vision algorithms [<xref ref-type="bibr" rid="pbio.1002611.ref006">6</xref>]. Indeed, it has long been recognized that this is one of the core unresolved issues in vision science [<xref ref-type="bibr" rid="pbio.1002611.ref002">2</xref>].</p>
<fig id="pbio.1002611.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002611.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Deep networks generate good proxy for top-down representation.</title>
<p><bold>A</bold>-<bold>C</bold> plot human sensitivity (y axis) for individual probe insertions (one small dot per insertion) separately for different scenes (pooled across participants), against values corresponding to probe insertion point on top-down/bottom-up maps (<bold>A</bold>/<bold>B</bold>) and the map generated by the CRF-RNN deep convolutional network [<xref ref-type="bibr" rid="pbio.1002611.ref023">23</xref>] (<bold>C</bold>; abscissa values for this plot have been rescaled to range between 0 and 1). Dashed lines show 80%, 90%, and 95% (from thick to thin) confidence intervals for linear fit. Green symbols in <bold>A</bold> show average <italic>y</italic> value for individual abscissa values; symbol size scales with number of data points. <bold>D</bold> shows correlation values for scatter plots in <bold>A</bold>-<bold>C</bold> and those generated by other computer vision algorithms (Itti-Koch [<xref ref-type="bibr" rid="pbio.1002611.ref003">3</xref>], GBVS [<xref ref-type="bibr" rid="pbio.1002611.ref041">41</xref>], gPb-HS [<xref ref-type="bibr" rid="pbio.1002611.ref005">5</xref>], nCuts [<xref ref-type="bibr" rid="pbio.1002611.ref042">42</xref>], HVC [<xref ref-type="bibr" rid="pbio.1002611.ref043">43</xref>]); open green symbol plots correlation for top-down map when consensus probe locations (indicated by solid green symbol in <bold>A</bold>) are excluded. Error bars in <bold>D</bold> show 95% confidence intervals. <bold>E</bold> plots rich/poor log-ratios to the conventions of <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F</xref> where human sensitivity estimation for <italic>y</italic> axis is relabelled against rich/poor probe locations on the maps generated by CRF-RNN (red) and gPb-HS (blue) algorithms instead of top-down map (black). Values on the <italic>x</italic> axis are computed with respect to bottom-up map (same as <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F</xref>). Icons show example segmentations from the two algorithms for the natural scene in <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1A</xref>; coloured overlay indicates segmented regions/boundaries, orange circle corresponds to red solid circle (top-down poor, bottom-up rich location) in <xref ref-type="fig" rid="pbio.1002611.g001">Fig 1D</xref>. Data for this figure is available from <xref ref-type="supplementary-material" rid="pbio.1002611.s008">S2 Data</xref>. HVC, hierarchical visual cues; GBVS, graph-based visual saliency.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.g009" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec036" sec-type="conclusions">
<title>Discussion</title>
<p>The main experimental result of this study can be summarized by the notion that local visual operators are controlled by the segmented layout implied by the physical image, rather than by the physical image itself. This result should not be interpreted to mean that the physical content of the image is entirely irrelevant: in the trivial limit-case when image contrast is reduced below detection threshold, the scene becomes invisible and no visual processing can take place. Our results indicate that, provided the physical content of the image is minimally sufficient to support inference of the underlying environmental structure, the latter process becomes the primary contextual influence on image reconstruction, overriding strictly image-driven aspects of visual processing (e.g., collinear facilitation [<xref ref-type="bibr" rid="pbio.1002611.ref074">74</xref>]). These findings suggest that once the inferential mechanism is kick-started, perception is quickly organized around the operation of this mechanism [<xref ref-type="bibr" rid="pbio.1002611.ref075">75</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref076">76</xref>]; detailed variations of physical content are then sidelined by the object segmentation projected by the inferred layout of the scene. The behavioural impact of this process can be measured in the form of a top-down effect on perceptual efficiency for local image reconstruction. We speculate that object segmentation is actively engaged at all times; in the laboratory, it may go unnoticed unless experiments are specifically designed to expose its impact on feature extraction.</p>
<p>At a cursory level, it may appear that the top-down effect simply reflects known phenomena such as spatial uncertainty [<xref ref-type="bibr" rid="pbio.1002611.ref077">77</xref>], crowding [<xref ref-type="bibr" rid="pbio.1002611.ref078">78</xref>], flanker facilitation/inhibition [<xref ref-type="bibr" rid="pbio.1002611.ref012">12</xref>], texture (second-order) cues [<xref ref-type="bibr" rid="pbio.1002611.ref079">79</xref>], and eye-movement scanning strategy [<xref ref-type="bibr" rid="pbio.1002611.ref080">80</xref>]. However, we can exclude these factors by evaluating them against the wide-ranging collection of our experimental results (see <xref ref-type="supplementary-material" rid="pbio.1002611.s001">S1 Text</xref> for detailed argumentation relating to these and other factors). For example, top-down effects show no dependence on eccentricity (<xref ref-type="supplementary-material" rid="pbio.1002611.s003">S2 Fig</xref>) and cueing of probe location (<xref ref-type="fig" rid="pbio.1002611.g002">Fig 2G</xref>), the former inconsistent with crowding [<xref ref-type="bibr" rid="pbio.1002611.ref081">81</xref>] and the latter inconsistent with a role for spatial uncertainty. We must conclude that the phenomena exposed by our probe-insertion paradigm cannot be accounted for by commonly proposed mechanisms and reflect a genuinely novel class of perceptual processes unexplored previously. Past literature has established that object identity is represented as quickly as approximately 100 ms after the visual stimulus has appeared [<xref ref-type="bibr" rid="pbio.1002611.ref082">82</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref083">83</xref>]; however, little is known about the perceptual operations that lead up to said representation [<xref ref-type="bibr" rid="pbio.1002611.ref084">84</xref>], and that must occur during those initial 100 ms [<xref ref-type="bibr" rid="pbio.1002611.ref085">85</xref>]. The present results speak to the nature and timescale of those operations and their relationship to natural scenes.</p>
<p>In our prior research with embedded probes, probe insertion was restricted to one specific location for each natural scene [<xref ref-type="bibr" rid="pbio.1002611.ref009">9</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref010">10</xref>], thus providing no useful information relating to the topics addressed in the present study. Instead, it focused on the effect of image inversion, a manipulation believed to selectively target semantic representations [<xref ref-type="bibr" rid="pbio.1002611.ref052">52</xref>], and found that it may impact the structure of perceptual filters with no concomitant change in discrimination performance [<xref ref-type="bibr" rid="pbio.1002611.ref009">9</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref010">10</xref>]. In apparent contrast, the top-down effects we report here involve marked improvements in sensitivity. These apparently conflicting results owe their distinct patterns to the different stages/mechanisms probed by the 2 different sets of experiments. Image inversion has no impact on the top-down effects that represent the focus of this investigation (red symbols in <xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F</xref>), and overall performance was no different between upright and inverted trials (<italic>p</italic> = 0.64), just as in the earlier work [<xref ref-type="bibr" rid="pbio.1002611.ref009">9</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref010">10</xref>], and in line with related single-unit measurements [<xref ref-type="bibr" rid="pbio.1002611.ref053">53</xref>]. It is clear that the representational stage targeted by manipulating probe location is distinct from the stage interrogated by scene inversion [<xref ref-type="bibr" rid="pbio.1002611.ref052">52</xref>]. We speculate that the current experiments probe a stage corresponding to the segmented image where object boundaries are delineated and objects possibly segregated [<xref ref-type="bibr" rid="pbio.1002611.ref053">53</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref068">68</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref085">85</xref>], without necessarily assigning semantic content to the segmentation. This concept builds on the distinction between tracing out an object from a scene on the one hand and knowing what that object is on the other hand [<xref ref-type="bibr" rid="pbio.1002611.ref002">2</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref021">21</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref068">68</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref086">86</xref>]. We propose that only the former operation is probed effectively by the image manipulations adopted in this study.</p>
<p>The above distinction is critical for correctly situating our electrophysiological results in relation to those associated with ultrafast image recognition [<xref ref-type="bibr" rid="pbio.1002611.ref082">82</xref>]. In those classic studies, the earliest EEG signatures of image recognition occur at approximately 100 ms [<xref ref-type="bibr" rid="pbio.1002611.ref082">82</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref087">87</xref>], after the top-down modulation in our data has completed (<xref ref-type="fig" rid="pbio.1002611.g007">Fig 7E</xref>). We speculate that our experiments probe the segmentation stage immediately preceding scene recognition [<xref ref-type="bibr" rid="pbio.1002611.ref002">2</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref016">16</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref053">53</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref068">68</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref088">88</xref>], and in doing so expose the temporal evolution of different phases in the perceptual reconstruction and interpretation of natural scenes [<xref ref-type="bibr" rid="pbio.1002611.ref085">85</xref>]. In this sense, our study is not only compatible with classic results from the ultrafast recognition EEG literature [<xref ref-type="bibr" rid="pbio.1002611.ref087">87</xref>] but also provides novel and distinct information about the underlying mechanisms that has not been exposed by those previous studies. Furthermore, it is entirely consistent with spike measurements from single neurons in the primary visual cortex [<xref ref-type="bibr" rid="pbio.1002611.ref053">53</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref068">68</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref085">85</xref>]. Those measurements have identified at least three stages in image processing: detection, segmentation, and attention, unfolding in temporal succession at approximately 50, 60, and 140 ms after stimulus onset [<xref ref-type="bibr" rid="pbio.1002611.ref085">85</xref>]. The effects exposed here naturally speak to the second stage: they must originate beyond detection but before attentional deployment (<xref ref-type="fig" rid="pbio.1002611.g002">Fig 2G</xref>). Consistent with this interpretation, their EEG dynamic characteristics dovetail the single-neuron measurements [<xref ref-type="bibr" rid="pbio.1002611.ref053">53</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref068">68</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref085">85</xref>] (although the link can only be tentative at this stage, given innumerable differences in the adopted stimuli, such as artificial boundaries defined by motion [<xref ref-type="bibr" rid="pbio.1002611.ref085">85</xref>] as opposed to static boundaries defined by natural scenes used here).</p>
<p>In contemporary accounts of natural image understanding, this process is almost invariably connected with the notion of feedback and top-down signals [<xref ref-type="bibr" rid="pbio.1002611.ref020">20</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref089">89</xref>], informing our own choice of map labels and associated probe-insertion protocol (<xref ref-type="fig" rid="pbio.1002611.g001">Fig 1</xref>). It is unclear, however, whether the bottom-up/top-down distinction [<xref ref-type="bibr" rid="pbio.1002611.ref028">28</xref>] represents the most productive conceptual framework for understanding the results presented here [<xref ref-type="bibr" rid="pbio.1002611.ref029">29</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref030">30</xref>]. We find that electrophysiological signatures of top-down effects become measurable shortly after stimulus onset [<xref ref-type="bibr" rid="pbio.1002611.ref053">53</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref068">68</xref>] (<xref ref-type="fig" rid="pbio.1002611.g007">Fig 7E</xref>); furthermore, they are restricted to occipital cortex (<xref ref-type="fig" rid="pbio.1002611.g007">Fig 7G</xref>). This result, combined with stable behavioural counterparts at very short stimulus durations (<xref ref-type="fig" rid="pbio.1002611.g006">Fig 6H and 6I</xref>), indicates that the perceptual system is dominated by object segmentation from immediately after stimulus presentation throughout the subsequent 100-ms epoch [<xref ref-type="bibr" rid="pbio.1002611.ref050">50</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref063">63</xref>]: there is little in our dataset that points to a feedback mechanism as typically conceptualized in the literature [<xref ref-type="bibr" rid="pbio.1002611.ref028">28</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref089">89</xref>]. For example, allowing the scene to be analyzed only after the probe has already disappeared, a manipulation expected to impact our measurements [<xref ref-type="bibr" rid="pbio.1002611.ref090">90</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref091">91</xref>] under feedback accounts [<xref ref-type="bibr" rid="pbio.1002611.ref020">20</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref089">89</xref>], does not reduce top-down effects (<xref ref-type="fig" rid="pbio.1002611.g005">Fig 5G</xref>). We propose that the process involved in these experiments is best understood as an integrated module where the distinction between bottom-up and top-down processing is not transparently attached to identifiable submodules [<xref ref-type="bibr" rid="pbio.1002611.ref029">29</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref063">63</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref092">92</xref>–<xref ref-type="bibr" rid="pbio.1002611.ref094">94</xref>].</p>
<p>If we accept the notion of an integrated module [<xref ref-type="bibr" rid="pbio.1002611.ref088">88</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref092">92</xref>], the processing mode engaged by the perceptual system reflects a sensitivity bottleneck [<xref ref-type="bibr" rid="pbio.1002611.ref095">95</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref096">96</xref>], rather than absence/presence of top-down feedback. The clearest dissociation between top-down-dominated and bottom-up-dominated processing modes is offered by intact scenes on the one hand, and blurred (lowpass) scenes on the other (black and yellow symbols occupy extreme positions along negative diagonal direction in <xref ref-type="fig" rid="pbio.1002611.g004">Fig 4</xref>). We propose that the perceptual system operates all along in a manner that depends on both types of information contained within the two maps provisionally labelled “top-down” and “bottom-up;” however, the bottleneck for performing local image reconstruction is defined by the former kind of information in the presence of an intact interpretable scene (<xref ref-type="fig" rid="pbio.1002611.g002">Fig 2F</xref>), while it is defined by the latter kind of information in the presence of a degraded unintelligible image (<xref ref-type="fig" rid="pbio.1002611.g003">Fig 3B and 3C</xref>).</p>
<p>The above notion is not meant to challenge the wider applicability of feedback and top-down control [<xref ref-type="bibr" rid="pbio.1002611.ref028">28</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref089">89</xref>]: our own dataset displays characteristics suggestive of a general role for feedback, such as the impact of scene-probe temporal asymmetry on asbolute sensitivity (blue symbols in <xref ref-type="fig" rid="pbio.1002611.g005">Fig 5E</xref>). Rather, the notion of an integrated module is intended in a restricted and specific sense. First, as discussed above, it only applies to the processing stage probed by the manipulations investigated here, which we have provisionally described as the segmented representation of the scene [<xref ref-type="bibr" rid="pbio.1002611.ref016">16</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref022">22</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref023">23</xref>]; second, it relates to perception, not to the underlying anatomy or physiology: it is conceivable that the neural implementation may involve a default network of early visual areas [<xref ref-type="bibr" rid="pbio.1002611.ref053">53</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref068">68</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref088">88</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref097">97</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref098">98</xref>] communicating in a fashion that may be characterized as feedback [<xref ref-type="bibr" rid="pbio.1002611.ref099">99</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref100">100</xref>], although on a faster timescale than typically associated with top-down control [<xref ref-type="bibr" rid="pbio.1002611.ref063">63</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref088">88</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref101">101</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref102">102</xref>]. Our results demonstrate that the visual process of reconstructing meaningful boundaries from natural scenes immediately engages such integrated extraction/segmentation perceptual module [<xref ref-type="bibr" rid="pbio.1002611.ref029">29</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref053">53</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref068">68</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref088">88</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref092">92</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref101">101</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref103">103</xref>], the operation of which is not dependent upon attentional deployment [<xref ref-type="bibr" rid="pbio.1002611.ref010">10</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref104">104</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref105">105</xref>] (<xref ref-type="fig" rid="pbio.1002611.g002">Fig 2G</xref>), relies on various statistical properties of the scene (<xref ref-type="fig" rid="pbio.1002611.g004">Fig 4</xref>), extends over large spatial scales [<xref ref-type="bibr" rid="pbio.1002611.ref076">76</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref098">98</xref>] (<xref ref-type="fig" rid="pbio.1002611.g006">Fig 6A–6G</xref> and <xref ref-type="supplementary-material" rid="pbio.1002611.s003">S2D Fig</xref>), resides within occipital cortex [<xref ref-type="bibr" rid="pbio.1002611.ref085">85</xref>] (<xref ref-type="fig" rid="pbio.1002611.g007">Fig 7G</xref>), and retunes its machinery to hone into the expected signal without changing its intrinsic variability [<xref ref-type="bibr" rid="pbio.1002611.ref013">13</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref054">54</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref069">69</xref>, <xref ref-type="bibr" rid="pbio.1002611.ref073">73</xref>] (<xref ref-type="fig" rid="pbio.1002611.g008">Fig 8B</xref>). It is feasible to construct a computational support for this integrated architecture based around plausible [<xref ref-type="bibr" rid="pbio.1002611.ref011">11</xref>] and primarily feedforward (i.e., fast) neural networks [<xref ref-type="bibr" rid="pbio.1002611.ref106">106</xref>] (<xref ref-type="fig" rid="pbio.1002611.g009">Fig 9C–9E</xref>). Future research will be necessary to characterize the biological circuits that support this process [<xref ref-type="bibr" rid="pbio.1002611.ref107">107</xref>] and establish their connection with the perceptual phenomena which we have documented here [<xref ref-type="bibr" rid="pbio.1002611.ref108">108</xref>].</p>
</sec>
<sec id="sec037">
<title>Supporting information</title>
<supplementary-material id="pbio.1002611.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Document providing additional information and detailed discussion of specific issues.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002611.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.s002" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Orthogonality of bottom-up/top-down maps.</title>
<p>Value on top-down map (<italic>y</italic> axis) is plotted against value on bottom-up map (<italic>x</italic> axis) across all probe insertions (1 symbol per insertion); open/solid indicates poor/rich on bottom-up map, red/green indicates poor/rich on top-down map. Marginal distributions along the bottom-up map (top histograms) are virtually identical for poor/rich locations on top-down map (red/green solid histograms); similarly, marginal distributions along the top-down map (right histograms) are indistinguishable for poor/rich locations on bottom-up map (open/solid histograms). Standard correlation tests are not applicable because this dataset is not normally distributed (Henze-Zirkler test) and it is heteroscedastic (test based on conditional variances).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002611.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.s003" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Visual field distribution of probe insertions and discrimination performance.</title>
<p>Probe density declines with eccentricity on both bottom-up (<bold>A</bold>) and top-down (<bold>B</bold>) maps (see overall decreasing characteristic of plots); in both cases, there is little difference between poor and rich insertions (open/solid in <bold>A</bold>, red/green in <bold>B</bold>; smooth lines show polynomial 2-degree fits), although there appears to be a moderate trend for rich insertions to exceed poor insertions near the fovea, and poor insertions to exceed rich insertions at 6–8 degrees of eccentricity (see blue trace plotting rich/poor log-ratios; shading shows ±1 SEM). Human sensitivity also declines with eccentricity as expected [<xref ref-type="bibr" rid="pbio.1002611.ref109">109</xref>] (overall decreasing characteristic in <bold>C</bold>-<bold>D</bold>), but it displays different trends for poor/rich differential effects: no difference between poor and rich insertions on the bottom-up map at any eccentricity (<bold>C</bold>), and clear differences on the top-down map at all eccentricities (<bold>D</bold>). Error bars show ±1 SEM.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002611.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.s004" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Criterion shifts associated with enhanced sensitivity.</title>
<p><bold>A</bold> shows ROC plot [<xref ref-type="bibr" rid="pbio.1002611.ref035">35</xref>] of individual data (1 symbol per observer) pooled across conditions that showed a top-down effect without bottom-up effect, for bottom-up poor/rich (black open/solid) and top-down poor/rich (red/green) insertions. Solid lines show best-fits of equal-variance SDT model for variations of sensitivity (d′), dashed lines in inset to <bold>A</bold> show fits for variations of criterion c; gray/black lines refer to bottom-up poor/rich data, red/green to top-down poor/rich data. Inset magnifies top-down rich/poor data clusters with associated d′/c fits. <bold>B</bold> plots d′ against c computed under the equal-variance assumption for all data points in <bold>A</bold>; the 2 quantities are clearly correlated. Error bars show ±1 SEM. Solid line shows best linear fit, dashed lines show 95% confidence intervals for fit. <bold>C</bold> plots rich/poor log-ratios computed from both d′ (<italic>y</italic> axis) and c (<italic>x</italic> axis) with reference to bottom-up (black) and top-down (green) maps; segments near <italic>x</italic>/<italic>y</italic> axes show 95% confidence intervals around mean values and demonstrate that the top-down fractional effect for sensitivity (green vertical segment near right <italic>y</italic> axis) is much greater than the effect for criterion shifts (green horizontal segment near top <italic>x</italic> axis). Axes in <bold>C</bold> have been scaled to match for direct comparison. ROC, receiver operating characteristic.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002611.s005" mimetype="video/x-msvideo" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.s005" xlink:type="simple">
<label>S1 Video</label>
<caption>
<title>Example sequence of 10 trials for main condition (unperturbed natural scenes except for probe insertion).</title>
<p>In this demo, the first trial is an example of “postcue” trial where the spatial cue (bright blob indicating probe location) appears after the natural scene, the second trial is an example of “precue” trial (cue appears before scene), and subsequent trials alternate between postcue and precue. In the actual experiments, trials were randomly assigned to precue or postcue categories (i.e., there was no regular repeating sequence; the postcue-precue repeating sequence was adopted in this demo for clarity of exposition).</p>
<p>(AVI)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002611.s006" mimetype="video/x-msvideo" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.s006" xlink:type="simple">
<label>S2 Video</label>
<caption>
<title>Example sequence of 8 trials for the “zooming” condition where the stimulus involves a smooth transition between a probeless scene and a sceneless probe.</title>
<p>In this demo, the first trial shows an example of “zoom-in” (scene-to-probe) transition at longer (300-ms) duration, the second trial shows an example of “zoom-out” (probe-to-scene) transition at longer duration, the third trial shows an example of “zoom-in” transition at shorter (100-ms) duration, the fourth trial shows an example of “zoom-out” transition at shorter duration. The subsequent 4 trials repeat this sequence. In the actual experiments, trials were randomly assigned to zoom-in/zoom-out and long/short categories (i.e., there was no regular repeating sequence; the zoom-in/zoom-out and long/short repeating sequence was adopted in this demo for clarity of exposition).</p>
<p>(AVI)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002611.s007" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.s007" xlink:type="simple">
<label>S1 Data</label>
<caption>
<title>Data dump.</title>
<p>(ZIP)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002611.s008" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pbio.1002611.s008" xlink:type="simple">
<label>S2 Data</label>
<caption>
<title>Figure data dump.</title>
<p>(ZIP)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>I am grateful to Patrick Cavanagh, Andrei Gorea, Michael Herzog, Paolo Martini, Guillaume Masson and Josh Solomon for comments relating to the psychophysical experiments; Alain de Cheveigné, Valentin Wyart and Giandomenico Iannetti for advice on the EEG analysis; Charles Gilbert and John Tsotsos for discussions on the role of feedback in top-down control; Stephane Mallat and Josef Sivic for prompting inclusion of deep convolutional networks; Xiao Sun and Shuai Zheng for sharing software material relating to the HVC and CRF-RNN algorithms; Shihab Shamma for comments relating to the entire manuscript.</p>
</ack>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item><term>DCN</term>
<def><p>deep convolutional network</p></def>
</def-item>
<def-item><term>EEG</term>
<def><p>electroencephalogram</p></def>
</def-item>
<def-item><term>N2pc</term>
<def><p>negativity 200-ms posterior contralateral</p></def>
</def-item>
<def-item><term>PCN</term>
<def><p>posterior contralateral negativity</p></def>
</def-item>
<def-item><term>SDT</term>
<def><p>signal detection theory</p></def>
</def-item>
<def-item><term>SPCN</term>
<def><p>sustained posterior contralateral negativity</p></def>
</def-item>
<def-item><term>VEP</term>
<def><p>visual evoked potential</p></def>
</def-item>
</def-list>
</glossary>
<ref-list>
<title>References</title>
<ref id="pbio.1002611.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubel</surname> <given-names>DH</given-names></name>. <article-title>The visual cortex of the brain</article-title>. <source>Sci Am</source>. <year>1963</year>;<volume>209</volume>:<fpage>54</fpage>–<lpage>62</lpage>. <object-id pub-id-type="pmid">14075682</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref002"><label>2</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Marr</surname> <given-names>DC</given-names></name>. <source>Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Freeman</publisher-name>; <year>1982</year>.</mixed-citation></ref>
<ref id="pbio.1002611.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Itti</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>. <article-title>Computational modelling of visual attention</article-title>. <source>Nat Rev Neurosci</source>. <year>2001</year>;<volume>2</volume>(<issue>3</issue>):<fpage>194</fpage>–<lpage>203</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/35058500" xlink:type="simple">10.1038/35058500</ext-link></comment> <object-id pub-id-type="pmid">11256080</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morgan</surname> <given-names>MJ</given-names></name>. <article-title>Features and the 'primal sketch'</article-title>. <source>Vision Res</source>. <year>2011</year>;<volume>51</volume>:<fpage>738</fpage>–<lpage>753</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2010.08.002" xlink:type="simple">10.1016/j.visres.2010.08.002</ext-link></comment> <object-id pub-id-type="pmid">20696182</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arbelaez</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Maire</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fowlkes</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Malik</surname> <given-names>J</given-names></name>. <article-title>Contour Detection and Hierarchical Image Segmentation</article-title>. <source>IEEE Trans Patt Anal Mach Intell</source>. <year>2011</year>;<volume>33</volume>(<issue>5</issue>):<fpage>898</fpage>–<lpage>916</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TPAMI.2010.161" xlink:type="simple">10.1109/TPAMI.2010.161</ext-link></comment> <object-id pub-id-type="pmid">20733228</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref006"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Hou X, Yuille AL, Koch C. Boundary Detection Benchmarking: Beyond F-Measures. In: CVPR. IEEE Computer Society; 2013. p. 2123–2130.</mixed-citation></ref>
<ref id="pbio.1002611.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Farid</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Differentiation of discrete multidimensional signals</article-title>. <source>IEEE Trans Image Process</source>. <year>2004</year>;<volume>13</volume>(<issue>4</issue>):<fpage>496</fpage>–<lpage>508</lpage>. <object-id pub-id-type="pmid">15376584</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neri</surname> <given-names>P</given-names></name>. <article-title>The elementary operations of human vision are not reducible to template matching</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>(<issue>11</issue>):<fpage>e1004499</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004499" xlink:type="simple">10.1371/journal.pcbi.1004499</ext-link></comment> <object-id pub-id-type="pmid">26556758</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neri</surname> <given-names>P</given-names></name>. <article-title>Global properties of natural scenes shape local properties of human edge detectors</article-title>. <source>Front Psychol</source>. <year>2011</year>;<volume>2</volume>:<fpage>172</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2011.00172" xlink:type="simple">10.3389/fpsyg.2011.00172</ext-link></comment> <object-id pub-id-type="pmid">21886631</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neri</surname> <given-names>P</given-names></name>. <article-title>Semantic control of feature extraction from natural scenes</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>(<issue>6</issue>):<fpage>2374</fpage>–<lpage>2388</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1755-13.2014" xlink:type="simple">10.1523/JNEUROSCI.1755-13.2014</ext-link></comment> <object-id pub-id-type="pmid">24501376</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>Normalization as a canonical neural computation</article-title>. <source>Nat Rev Neurosci</source>. <year>2011</year>;<volume>13</volume>:<fpage>51</fpage>–<lpage>62</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3136" xlink:type="simple">10.1038/nrn3136</ext-link></comment> <object-id pub-id-type="pmid">22108672</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Levi</surname> <given-names>DM</given-names></name>. <article-title>Surround modulation in human vision unmasked by masking experiments</article-title>. <source>Nat Neurosci</source>. <year>2000</year>;<volume>3</volume>:<fpage>724</fpage>–<lpage>728</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/76687" xlink:type="simple">10.1038/76687</ext-link></comment> <object-id pub-id-type="pmid">10862706</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paltoglou</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Neri</surname> <given-names>P</given-names></name>. <article-title>Attentional control of sensory tuning in human visual perception</article-title>. <source>J Neurophysiol</source>. <year>2012</year>;<volume>107</volume>:<fpage>1260</fpage>–<lpage>1274</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00776.2011" xlink:type="simple">10.1152/jn.00776.2011</ext-link></comment> <object-id pub-id-type="pmid">22131380</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Levi</surname> <given-names>DM</given-names></name>. <article-title>Crowding–an essential bottleneck for object recognition: a mini-review</article-title>. <source>Vision Res</source>. <year>2008</year>;<volume>48</volume>:<fpage>635</fpage>–<lpage>654</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2007.12.009" xlink:type="simple">10.1016/j.visres.2007.12.009</ext-link></comment> <object-id pub-id-type="pmid">18226828</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Demb</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tolhurst</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Dan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <etal>et al</etal>. <article-title>Do we know what the early visual system does?</article-title> <source>J Neurosci</source>. <year>2005</year>;<volume>25</volume>:<fpage>10577</fpage>–<lpage>10597</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3726-05.2005" xlink:type="simple">10.1523/JNEUROSCI.3726-05.2005</ext-link></comment> <object-id pub-id-type="pmid">16291931</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref016"><label>16</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Ullman</surname> <given-names>S</given-names></name>. <source>High-level vision</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1996</year>.</mixed-citation></ref>
<ref id="pbio.1002611.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>. <article-title>Natural image statistics and neural representation</article-title>. <source>Annu Rev Neurosci</source>. <year>2001</year>;<volume>24</volume>:<fpage>1193</fpage>–<lpage>1216</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.neuro.24.1.1193" xlink:type="simple">10.1146/annurev.neuro.24.1.1193</ext-link></comment> <object-id pub-id-type="pmid">11520932</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Felsen</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Dan</surname> <given-names>Y</given-names></name>. <article-title>A natural approach to studying vision</article-title>. <source>Nat Neurosci</source>. <year>2005</year>;<volume>8</volume>:<fpage>1643</fpage>–<lpage>1646</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1608" xlink:type="simple">10.1038/nn1608</ext-link></comment> <object-id pub-id-type="pmid">16306891</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Visual perception and the statistical properties of natural scenes</article-title>. <source>Annu Rev Psychol</source>. <year>2008</year>;<volume>59</volume>:<fpage>167</fpage>–<lpage>192</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.psych.58.110405.085632" xlink:type="simple">10.1146/annurev.psych.58.110405.085632</ext-link></comment> <object-id pub-id-type="pmid">17705683</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bar</surname> <given-names>M</given-names></name>. <article-title>Visual objects in context</article-title>. <source>Nat Rev Neurosci</source>. <year>2004</year>;<volume>5</volume>(<issue>8</issue>):<fpage>617</fpage>–<lpage>629</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn1476" xlink:type="simple">10.1038/nrn1476</ext-link></comment> <object-id pub-id-type="pmid">15263892</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cox</surname> <given-names>DD</given-names></name>. <article-title>Do we understand high-level vision?</article-title> <source>Curr Opin Neurobiol</source>. <year>2014</year>;<volume>25</volume>:<fpage>187</fpage>–<lpage>193</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2014.01.016" xlink:type="simple">10.1016/j.conb.2014.01.016</ext-link></comment> <object-id pub-id-type="pmid">24552691</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref022"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Arbelaez P, Hariharan B, Gu C, Gupta S, Bourdev L, Malik J. Semantic segmentation using regions and parts. In: CVPR; 2012.</mixed-citation></ref>
<ref id="pbio.1002611.ref023"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Zheng S, Jayasumana S, Romera-Paredes B, Vineet V, Su Z, Du D, et al. Conditional Random Fields as Recurrent Neural Networks. In: International Conference on Computer Vision (ICCV); 2015.</mixed-citation></ref>
<ref id="pbio.1002611.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>David</surname> <given-names>SV</given-names></name>, <name name-style="western"><surname>Vinje</surname> <given-names>WE</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Natural stimulus statistics alter the receptive field structure of v1 neurons</article-title>. <source>J Neurosci</source>. <year>2004</year>;<volume>24</volume>:<fpage>6991</fpage>–<lpage>7006</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1422-04.2004" xlink:type="simple">10.1523/JNEUROSCI.1422-04.2004</ext-link></comment> <object-id pub-id-type="pmid">15295035</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gilbert</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>W</given-names></name>. <article-title>Top-down influences on visual processing</article-title>. <source>Nat Rev Neurosci</source>. <year>2013</year>;<volume>14</volume>(<issue>5</issue>):<fpage>350</fpage>–<lpage>363</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3476" xlink:type="simple">10.1038/nrn3476</ext-link></comment> <object-id pub-id-type="pmid">23595013</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamma</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fritz</surname> <given-names>J</given-names></name>. <article-title>Adaptive auditory computations</article-title>. <source>Curr Opin Neurobiol</source>. <year>2014</year>;<volume>25</volume>:<fpage>164</fpage>–<lpage>168</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2014.01.011" xlink:type="simple">10.1016/j.conb.2014.01.011</ext-link></comment> <object-id pub-id-type="pmid">24525107</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref027"><label>27</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>. <chapter-title>20 years of learning about vision: Questions answered, Questions unanswered, and Questions not yet asked</chapter-title>. <source>20 Years of Computational Neuroscience</source> (<name name-style="western"><surname>Bower</surname> <given-names>J</given-names></name> ed), <collab>Springer</collab>. <year>2012</year>; p. <fpage>243</fpage>–<lpage>270</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lamme</surname> <given-names>VA</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>. <article-title>The distinct modes of vision offered by feedforward and recurrent processing</article-title>. <source>Trends Neurosci</source>. <year>2000</year>;<volume>23</volume>(<issue>11</issue>):<fpage>571</fpage>–<lpage>579</lpage>. <object-id pub-id-type="pmid">11074267</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herzog</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Clarke</surname> <given-names>AM</given-names></name>. <article-title>Why vision is not both hierarchical and feedforward</article-title>. <source>Front Comput Neurosci</source>. <year>2014</year>;<volume>8</volume>:<fpage>135</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2014.00135" xlink:type="simple">10.3389/fncom.2014.00135</ext-link></comment> <object-id pub-id-type="pmid">25374535</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Firestone</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Scholl</surname> <given-names>BJ</given-names></name>. <article-title>Cognition does not affect perception: Evaluating the evidence for 'top-down' effects</article-title>. <source>Behav Brain Sci</source>. <year>2015</year>; p. <fpage>1</fpage>–<lpage>77</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Murray</surname> <given-names>RF</given-names></name>. <article-title>Classification images: A review</article-title>. <source>J Vis</source>. <year>2011</year>;<volume>11</volume> (<issue>5</issue>):<fpage>1</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/11.5.2" xlink:type="simple">10.1167/11.5.2</ext-link></comment> <object-id pub-id-type="pmid">21536726</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elder</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Velisavljević</surname> <given-names>L</given-names></name>. <article-title>Cue dynamics underlying rapid detection of animals in natural scenes</article-title>. <source>J Vis</source>. <year>2009</year>;<volume>9</volume>(<issue>7</issue>):<fpage>7</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/9.7.7" xlink:type="simple">10.1167/9.7.7</ext-link></comment> <object-id pub-id-type="pmid">19761322</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bookstein</surname> <given-names>FL</given-names></name>. <article-title>Principal Warps: Thin-Plate Splines and the Decomposition of Deformations</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>1989</year>;<volume>11</volume>(<issue>6</issue>):<fpage>567</fpage>–<lpage>585</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahumada</surname> <given-names>AJ</given-names></name>. <article-title>Classification image weights and internal noise level estimation</article-title>. <source>J Vis</source>. <year>2002</year>;<volume>2</volume>:<fpage>121</fpage>–<lpage>131</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/2.1.8" xlink:type="simple">10.1167/2.1.8</ext-link></comment> <object-id pub-id-type="pmid">12678600</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref035"><label>35</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Green</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Swets</surname> <given-names>JA</given-names></name>. <source>Signal Detection Theory and Psychophysics</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>1966</year>.</mixed-citation></ref>
<ref id="pbio.1002611.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burgess</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Colborne</surname> <given-names>B</given-names></name>. <article-title>Visual signal detection. IV. Observer inconsistency</article-title>. <source>J Opt Soc Am A</source>. <year>1988</year>;<volume>5</volume>:<fpage>617</fpage>–<lpage>627</lpage>. <object-id pub-id-type="pmid">3404312</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neri</surname> <given-names>P</given-names></name>. <article-title>How inherently noisy is human sensory processing?</article-title> <source>Psychon Bull Rev</source>. <year>2010</year>;<volume>17</volume>:<fpage>802</fpage>–<lpage>808</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/PBR.17.6.802" xlink:type="simple">10.3758/PBR.17.6.802</ext-link></comment> <object-id pub-id-type="pmid">21169572</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Awwad Shiekh Hasan</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Joosten</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Neri</surname> <given-names>P</given-names></name>. <article-title>Estimation of internal noise using double passes: does it matter how the second pass is delivered?</article-title> <source>Vision Res</source>. <year>2012</year>;<volume>69</volume>:<fpage>1</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2012.06.014" xlink:type="simple">10.1016/j.visres.2012.06.014</ext-link></comment> <object-id pub-id-type="pmid">22835631</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Green</surname> <given-names>DM</given-names></name>. <article-title>Consistency of auditory detection judgments</article-title>. <source>Psychol Rev</source>. <year>1964</year>;<volume>71</volume>:<fpage>392</fpage>–<lpage>407</lpage>. <object-id pub-id-type="pmid">14208857</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Diependaele</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Brysbaert</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Neri</surname> <given-names>P</given-names></name>. <article-title>How noisy is lexical decision?</article-title> <source>Front Psychol</source>. <year>2012</year>;<volume>3</volume>:<fpage>348</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2012.00348" xlink:type="simple">10.3389/fpsyg.2012.00348</ext-link></comment> <object-id pub-id-type="pmid">23015793</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref041"><label>41</label><mixed-citation publication-type="other" xlink:type="simple">Harel J, Koch C, Perona P. Graph-Based Visual Saliency? NIPS. 2006.</mixed-citation></ref>
<ref id="pbio.1002611.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Malik</surname> <given-names>J</given-names></name>. <article-title>Normalized Cuts and Image Segmentation</article-title>. <source>IEEE Trans Patt Anal Mach Intell</source>. <year>1997</year>;<volume>22</volume>:<fpage>888</fpage>–<lpage>905</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sun</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Shang</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ming</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Tian</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>J</given-names></name>. <article-title>A Biologically-Inspired Framework for Contour Detection Using Superpixel-Based Candidates and Hierarchical Visual Cues</article-title>. <source>Sensors (Basel)</source>. <year>2015</year>;<volume>15</volume>(<issue>10</issue>):<fpage>26654</fpage>–<lpage>26674</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref044"><label>44</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Luck</surname> <given-names>SJ</given-names></name>. <source>An Introduction to the Event-Related Potential Technique</source>. <publisher-name>MIT Press</publisher-name>; <year>2005</year>.</mixed-citation></ref>
<ref id="pbio.1002611.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rousselet</surname> <given-names>GA</given-names></name>. <article-title>Does Filtering Preclude Us from Studying ERP Time-Courses?</article-title> <source>Front Psychol</source>. <year>2012</year>;<volume>3</volume>:<fpage>131</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2012.00131" xlink:type="simple">10.3389/fpsyg.2012.00131</ext-link></comment> <object-id pub-id-type="pmid">22586415</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luck</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Hillyard</surname> <given-names>SA</given-names></name>. <article-title>Spatial filtering during visual search: evidence from human electrophysiology</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>1994</year>;<volume>20</volume>(<issue>5</issue>):<fpage>1000</fpage>–<lpage>1014</lpage>. <object-id pub-id-type="pmid">7964526</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cumming</surname> <given-names>G</given-names></name>. <article-title>The new statistics: why and how</article-title>. <source>Psychol Sci</source>. <year>2014</year>;<volume>25</volume>(<issue>1</issue>):<fpage>7</fpage>–<lpage>29</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0956797613504966" xlink:type="simple">10.1177/0956797613504966</ext-link></comment> <object-id pub-id-type="pmid">24220629</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wasserstein</surname> <given-names>RL</given-names></name>, <name name-style="western"><surname>Lazar</surname> <given-names>NA</given-names></name>. <article-title>The ASA's Statement on p-Values: Context, Process, and Purpose</article-title>. <source>The American Statistician</source>. <year>2016</year>;<volume>70</volume>(<issue>2</issue>):<fpage>129</fpage>–<lpage>133</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rauss</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Pourtois</surname> <given-names>G</given-names></name>. <article-title>What is Bottom-Up and What is Top-Down in Predictive Coding?</article-title> <source>Front Psychol</source>. <year>2013</year>;<volume>4</volume>:<fpage>276</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2013.00276" xlink:type="simple">10.3389/fpsyg.2013.00276</ext-link></comment> <object-id pub-id-type="pmid">23730295</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nyström</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Holmqvist</surname> <given-names>K</given-names></name>. <article-title>Semantic override of low-level features in image viewing - both initially and overall</article-title>. <source>J Eye Movement Research</source>. <year>2008</year>;<volume>2</volume>(<issue>2</issue>):<fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Velisavljević</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Elder</surname> <given-names>JH</given-names></name>. <article-title>Visual short-term memory of local information in briefly viewed natural scenes: configural and non-configural factors</article-title>. <source>J Vis</source>. <year>2008</year>;<volume>8</volume>(<issue>16</issue>):<fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Valentine</surname> <given-names>T</given-names></name>. <article-title>Upside-down faces: a review of the effect of inversion upon face recognition</article-title>. <source>Br J Psychol</source>. <year>1988</year>;<volume>79</volume> (<issue>Pt 4</issue>):<fpage>471</fpage>–<lpage>491</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williford</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>von der Heydt</surname> <given-names>R</given-names></name>. <article-title>Figure-Ground Organization in Visual Cortex for Natural Scenes</article-title>. <source>eNeuro</source>. <year>2016</year>;<volume>3</volume>(<issue>6</issue>).</mixed-citation></ref>
<ref id="pbio.1002611.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gaspar</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Bennett</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Sekuler</surname> <given-names>AB</given-names></name>. <article-title>The effects of face inversion and contrast-reversal on efficiency and internal noise</article-title>. <source>Vision Res</source>. <year>2008</year>;<volume>48</volume>:<fpage>1084</fpage>–<lpage>1095</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2007.12.014" xlink:type="simple">10.1016/j.visres.2007.12.014</ext-link></comment> <object-id pub-id-type="pmid">18314157</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hupe</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>James</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Payne</surname> <given-names>BR</given-names></name>, <name name-style="western"><surname>Lomber</surname> <given-names>SG</given-names></name>, <name name-style="western"><surname>Girard</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bullier</surname> <given-names>J</given-names></name>. <article-title>Cortical feedback improves discrimination between figure and background by V1, V2 and V3 neurons</article-title>. <source>Nature</source>. <year>1998</year>;<volume>394</volume>:<fpage>784</fpage>–<lpage>787</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/29537" xlink:type="simple">10.1038/29537</ext-link></comment> <object-id pub-id-type="pmid">9723617</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rieger</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Kochy</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Schalk</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Gruschow</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Heinze</surname> <given-names>HJ</given-names></name>. <article-title>Speed limits: orientation and semantic context interactions constrain natural scene discrimination dynamics</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>2008</year>;<volume>34</volume>(<issue>1</issue>):<fpage>56</fpage>–<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0096-1523.34.1.56" xlink:type="simple">10.1037/0096-1523.34.1.56</ext-link></comment> <object-id pub-id-type="pmid">18248140</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Rullen</surname> <given-names>R</given-names></name>. <article-title>Four common conceptual fallacies in mapping the time course of recognition</article-title>. <source>Front Psychol</source>. <year>2011</year>;<volume>2</volume>:<fpage>365</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2011.00365" xlink:type="simple">10.3389/fpsyg.2011.00365</ext-link></comment> <object-id pub-id-type="pmid">22162973</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eriksen</surname> <given-names>CW</given-names></name>. <article-title>The use of a visual mask may seriously confound your experiment</article-title>. <source>Percept Psychophys</source>. <year>1980</year>;<volume>28</volume>(<issue>1</issue>):<fpage>89</fpage>–<lpage>92</lpage>. <object-id pub-id-type="pmid">7413417</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hermens</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Herzog</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Francis</surname> <given-names>G</given-names></name>. <article-title>Combining simultaneous with temporal masking</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>2009</year>;<volume>35</volume>(<issue>4</issue>):<fpage>977</fpage>–<lpage>988</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0014252" xlink:type="simple">10.1037/a0014252</ext-link></comment> <object-id pub-id-type="pmid">19653743</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wascher</surname> <given-names>E</given-names></name>. <article-title>The timing of stimulus localisation and the Simon effect: an ERP study</article-title>. <source>Exp Brain Res</source>. <year>2005</year>;<volume>163</volume>(<issue>4</issue>):<fpage>430</fpage>–<lpage>439</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00221-004-2198-1" xlink:type="simple">10.1007/s00221-004-2198-1</ext-link></comment> <object-id pub-id-type="pmid">15711792</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jolicoeur</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Brisson</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Robitaille</surname> <given-names>N</given-names></name>. <article-title>Dissociation of the N2pc and sustained posterior contralateral negativity in a choice response task</article-title>. <source>Brain Res</source>. <year>2008</year>;<volume>1215</volume>:<fpage>160</fpage>–<lpage>172</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.brainres.2008.03.059" xlink:type="simple">10.1016/j.brainres.2008.03.059</ext-link></comment> <object-id pub-id-type="pmid">18482718</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ratcliff</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Philiastides</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Sajda</surname> <given-names>P</given-names></name>. <article-title>Quality of evidence for perceptual decision making is indexed by trial-to-trial variability of the EEG</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2009</year>;<volume>106</volume>(<issue>16</issue>):<fpage>6539</fpage>–<lpage>6544</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0812589106" xlink:type="simple">10.1073/pnas.0812589106</ext-link></comment> <object-id pub-id-type="pmid">19342495</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rauss</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pourtois</surname> <given-names>G</given-names></name>. <article-title>Top-down effects on early visual processing in humans: a predictive coding framework</article-title>. <source>Neurosci Biobehav Rev</source>. <year>2011</year>;<volume>35</volume>(<issue>5</issue>):<fpage>1237</fpage>–<lpage>1253</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neubiorev.2010.12.011" xlink:type="simple">10.1016/j.neubiorev.2010.12.011</ext-link></comment> <object-id pub-id-type="pmid">21185860</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mazza</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Turatto</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Umilta</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Eimer</surname> <given-names>M</given-names></name>. <article-title>Attentional selection and identification of visual objects are reflected by distinct electrophysiological responses</article-title>. <source>Exp Brain Res</source>. <year>2007</year>;<volume>181</volume>(<issue>3</issue>):<fpage>531</fpage>–<lpage>536</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00221-007-1002-4" xlink:type="simple">10.1007/s00221-007-1002-4</ext-link></comment> <object-id pub-id-type="pmid">17602216</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Casiraghi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fortier-Gauthier</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Sessa</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dell'Acqua</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Jolicœur</surname> <given-names>P</given-names></name>. <article-title>N1pc reversal following repeated eccentric visual stimulation</article-title>. <source>Psychophysiology</source>. <year>2013</year>;<volume>50</volume>(<issue>4</issue>):<fpage>351</fpage>–<lpage>364</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/psyp.12021" xlink:type="simple">10.1111/psyp.12021</ext-link></comment> <object-id pub-id-type="pmid">23317174</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hickey</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Di Lollo</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>McDonald</surname> <given-names>JJ</given-names></name>. <article-title>Electrophysiological indices of target and distractor processing in visual search</article-title>. <source>J Cogn Neurosci</source>. <year>2009</year>;<volume>21</volume>(<issue>4</issue>):<fpage>760</fpage>–<lpage>775</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2009.21039" xlink:type="simple">10.1162/jocn.2009.21039</ext-link></comment> <object-id pub-id-type="pmid">18564048</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dowdall</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Luczak</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Tata</surname> <given-names>MS</given-names></name>. <article-title>Temporal variability of the N2pc during efficient and inefficient visual search</article-title>. <source>Neuropsychologia</source>. <year>2012</year>;<volume>50</volume>(<issue>10</issue>):<fpage>2442</fpage>–<lpage>2453</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2012.06.015" xlink:type="simple">10.1016/j.neuropsychologia.2012.06.015</ext-link></comment> <object-id pub-id-type="pmid">22743179</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hesse</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>. <article-title>Consistency of Border-Ownership Cells across Artificial Stimuli, Natural Stimuli, and Stimuli with Ambiguous Contours</article-title>. <source>J Neurosci</source>. <year>2016</year>;<volume>36</volume>(<issue>44</issue>):<fpage>11338</fpage>–<lpage>11349</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1857-16.2016" xlink:type="simple">10.1523/JNEUROSCI.1857-16.2016</ext-link></comment> <object-id pub-id-type="pmid">27807174</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lutfi</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Gilbertson</surname> <given-names>L</given-names></name>. <article-title>Effect of decision weights and internal noise on the growth of d' with N</article-title>. <source>J Acoust Soc Am</source>. <year>2011</year>;<volume>130</volume>(<issue>5</issue>):<fpage>L329</fpage>–<lpage>333</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunelli</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Template matching: matched spatial filters and beyond</article-title>. <source>Pattern Recognition</source>. <year>1997</year>;<volume>30</volume>:<fpage>751</fpage>–<lpage>768</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pritchett</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>RF</given-names></name>. <article-title>Classification images reveal decision variables and strategies in forced choice tasks</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2015</year>;<volume>112</volume>(<issue>23</issue>):<fpage>7321</fpage>–<lpage>7326</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1422169112" xlink:type="simple">10.1073/pnas.1422169112</ext-link></comment> <object-id pub-id-type="pmid">26015584</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Barbot</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Carrasco</surname> <given-names>M</given-names></name>. <article-title>Saccade Preparation Reshapes Sensory Tuning</article-title>. <source>Curr Biol</source>. <year>2016</year>;<volume>26</volume>(<issue>12</issue>):<fpage>1564</fpage>–<lpage>1570</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2016.04.028" xlink:type="simple">10.1016/j.cub.2016.04.028</ext-link></comment> <object-id pub-id-type="pmid">27265397</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gold</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bennett</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Sekuler</surname> <given-names>AB</given-names></name>. <article-title>Signal but not noise changes with perceptual learning</article-title>. <source>Nature</source>. <year>1999</year>;<volume>402</volume>:<fpage>176</fpage>–<lpage>178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/46027" xlink:type="simple">10.1038/46027</ext-link></comment> <object-id pub-id-type="pmid">10647007</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maniglia</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pavan</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Aedo-Jury</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Trotter</surname> <given-names>Y</given-names></name>. <article-title>The spatial range of peripheral collinear facilitation</article-title>. <source>Sci Rep</source>. <year>2015</year>;<volume>5</volume>:<fpage>15530</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep15530" xlink:type="simple">10.1038/srep15530</ext-link></comment> <object-id pub-id-type="pmid">26502834</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>He</surname> <given-names>ZJ</given-names></name>, <name name-style="western"><surname>Nakayama</surname> <given-names>K</given-names></name>. <article-title>Surfaces versus features in visual search</article-title>. <source>Nature</source>. <year>1992</year>;<volume>359</volume>(<issue>6392</issue>):<fpage>231</fpage>–<lpage>233</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/359231a0" xlink:type="simple">10.1038/359231a0</ext-link></comment> <object-id pub-id-type="pmid">1528263</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Manassi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sayim</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Herzog</surname> <given-names>MH</given-names></name>. <article-title>Grouping, pooling, and when bigger is better in visual crowding</article-title>. <source>J Vis</source>. <year>2012</year>;<volume>12</volume>(<issue>10</issue>):<fpage>13</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/12.10.13" xlink:type="simple">10.1167/12.10.13</ext-link></comment> <object-id pub-id-type="pmid">23019118</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pelli</surname> <given-names>DG</given-names></name>. <article-title>Uncertainty explains many aspects of visual contrast detection and discrimination</article-title>. <source>J Opt Soc Am A</source>. <year>1985</year>;<volume>2</volume>:<fpage>1508</fpage>–<lpage>1532</lpage>. <object-id pub-id-type="pmid">4045584</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Levi</surname> <given-names>DM</given-names></name>. <article-title>Visual crowding</article-title>. <source>Curr Biol</source>. <year>2011</year>;<volume>21</volume>:<fpage>R678</fpage>–<lpage>679</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2011.07.025" xlink:type="simple">10.1016/j.cub.2011.07.025</ext-link></comment> <object-id pub-id-type="pmid">21959149</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parkhurst</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Niebur</surname> <given-names>E</given-names></name>. <article-title>Texture contrast attracts overt visual attention in natural scenes</article-title>. <source>Eur J Neurosci</source>. <year>2004</year>;<volume>19</volume>(<issue>3</issue>):<fpage>783</fpage>–<lpage>789</lpage>. <object-id pub-id-type="pmid">14984430</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Ort</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Kruijne</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Meeter</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Donk</surname> <given-names>M</given-names></name>. <article-title>It depends on when you look at it: Salience influences eye movements in natural scene viewing and search early in time</article-title>. <source>J Vis</source>. <year>2015</year>;<volume>15</volume>(<issue>5</issue>):<fpage>9</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/15.5.9" xlink:type="simple">10.1167/15.5.9</ext-link></comment> <object-id pub-id-type="pmid">26067527</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pelli</surname> <given-names>DG</given-names></name>, <name name-style="western"><surname>Palomares</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Majaj</surname> <given-names>NJ</given-names></name>. <article-title>Crowding is unlike ordinary masking: distinguishing feature integration from detection</article-title>. <source>J Vis</source>. <year>2004</year>;<volume>4</volume>:<fpage>1136</fpage>–<lpage>1169</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/4.12.12" xlink:type="simple">10.1167/4.12.12</ext-link></comment> <object-id pub-id-type="pmid">15669917</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorpe</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fize</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Marlot</surname> <given-names>C</given-names></name>. <article-title>Speed of processing in the human visual system</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>(<issue>6582</issue>):<fpage>520</fpage>–<lpage>522</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/381520a0" xlink:type="simple">10.1038/381520a0</ext-link></comment> <object-id pub-id-type="pmid">8632824</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Agam</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Madsen</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Kreiman</surname> <given-names>G</given-names></name>. <article-title>Timing, timing, timing: fast decoding of object information from intracranial field potentials in human visual cortex</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>62</volume>(<issue>2</issue>):<fpage>281</fpage>–<lpage>290</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.02.025" xlink:type="simple">10.1016/j.neuron.2009.02.025</ext-link></comment> <object-id pub-id-type="pmid">19409272</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Jolij</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Fahrenfort</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VA</given-names></name>. <article-title>Feedforward and recurrent processing in scene segmentation: electroencephalography and functional magnetic resonance imaging</article-title>. <source>J Cogn Neurosci</source>. <year>2008</year>;<volume>20</volume>(<issue>11</issue>):<fpage>2097</fpage>–<lpage>2109</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2008.20142" xlink:type="simple">10.1162/jocn.2008.20142</ext-link></comment> <object-id pub-id-type="pmid">18416684</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Tolboom</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Khayat</surname> <given-names>PS</given-names></name>. <article-title>Different processing phases for features, figures, and selective attention in the primary visual cortex</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>56</volume>(<issue>5</issue>):<fpage>785</fpage>–<lpage>792</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2007.10.006" xlink:type="simple">10.1016/j.neuron.2007.10.006</ext-link></comment> <object-id pub-id-type="pmid">18054856</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kubilius</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wagemans</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Op de Beeck</surname> <given-names>HP</given-names></name>. <article-title>A conceptual framework of computations in mid-level vision</article-title>. <source>Front Comput Neurosci</source>. <year>2014</year>;<volume>8</volume>:<fpage>158</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2014.00158" xlink:type="simple">10.3389/fncom.2014.00158</ext-link></comment> <object-id pub-id-type="pmid">25566044</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bieniek</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Bennett</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Sekuler</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Rousselet</surname> <given-names>GA</given-names></name>. <article-title>A robust and representative lower bound on object processing speed in humans</article-title>. <source>Eur J Neurosci</source>. <year>2016</year>;<volume>44</volume>(<issue>2</issue>):<fpage>1804</fpage>–<lpage>1814</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/ejn.13100" xlink:type="simple">10.1111/ejn.13100</ext-link></comment> <object-id pub-id-type="pmid">26469359</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Foxe</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Simpson</surname> <given-names>GV</given-names></name>. <article-title>Flow of activation from V1 to frontal cortex in humans. A framework for defining "early" visual processing</article-title>. <source>Exp Brain Res</source>. <year>2002</year>;<volume>142</volume>(<issue>1</issue>):<fpage>139</fpage>–<lpage>150</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00221-001-0906-7" xlink:type="simple">10.1007/s00221-001-0906-7</ext-link></comment> <object-id pub-id-type="pmid">11797091</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bullier</surname> <given-names>J</given-names></name>. <article-title>Feedback connections and conscious vision</article-title>. <source>Trends Cogn Sci (Regul Ed)</source>. <year>2001</year>;<volume>5</volume>(<issue>9</issue>):<fpage>369</fpage>–<lpage>370</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watt</surname> <given-names>RJ</given-names></name>. <article-title>Scanning from coarse to fine spatial scales in the human visual system after the onset of a stimulus</article-title>. <source>J Opt Soc Am A</source>. <year>1987</year>;<volume>4</volume>:<fpage>2006</fpage>–<lpage>2021</lpage>. <object-id pub-id-type="pmid">3430211</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McSorley</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Findlay</surname> <given-names>JM</given-names></name>. <article-title>Are spatial frequencies integrated from coarse to fine?</article-title> <source>Perception</source>. <year>2002</year>;<volume>31</volume>(<issue>8</issue>):<fpage>955</fpage>–<lpage>967</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1068/p3257" xlink:type="simple">10.1068/p3257</ext-link></comment> <object-id pub-id-type="pmid">12269589</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Joubert</surname> <given-names>OR</given-names></name>, <name name-style="western"><surname>Rousselet</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Fize</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Fabre-Thorpe</surname> <given-names>M</given-names></name>. <article-title>Processing scene context: fast categorization and object interference</article-title>. <source>Vision Res</source>. <year>2007</year>;<volume>47</volume>(<issue>26</issue>):<fpage>3286</fpage>–<lpage>3297</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2007.09.013" xlink:type="simple">10.1016/j.visres.2007.09.013</ext-link></comment> <object-id pub-id-type="pmid">17967472</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schneider</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Beste</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wascher</surname> <given-names>E</given-names></name>. <article-title>On the time course of bottom-up and top-down processes in selective visual attention: an EEG study</article-title>. <source>Psychophysiology</source>. <year>2012</year>;<volume>49</volume>(<issue>11</issue>):<fpage>1492</fpage>–<lpage>1503</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1469-8986.2012.01462.x" xlink:type="simple">10.1111/j.1469-8986.2012.01462.x</ext-link></comment> <object-id pub-id-type="pmid">22978270</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Katsuki</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Constantinidis</surname> <given-names>C</given-names></name>. <article-title>Bottom-up and top-down attention: different processes and overlapping neural systems</article-title>. <source>Neuroscientist</source>. <year>2014</year>;<volume>20</volume>(<issue>5</issue>):<fpage>509</fpage>–<lpage>521</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/1073858413514136" xlink:type="simple">10.1177/1073858413514136</ext-link></comment> <object-id pub-id-type="pmid">24362813</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref095"><label>95</label><mixed-citation publication-type="other" xlink:type="simple">Van Essen D, Olshausen BA, Anderson CH, Gallant JTL. Pattern recognition, attention, and information bottlenecks in the primate visual system. In: SPIE. vol. 1473; 1991.</mixed-citation></ref>
<ref id="pbio.1002611.ref096"><label>96</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Whitney</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Haberman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sweeney</surname> <given-names>TD</given-names></name>. <chapter-title>From textures to crowds: multiple levels of summary statistical perception</chapter-title>. <source>The New Visual Neurosciences</source> (<name name-style="western"><surname>Werner</surname> <given-names>JS</given-names></name> and <name name-style="western"><surname>Chalupa</surname> <given-names>LM</given-names></name> eds), <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>. <year>2014</year>; p. <fpage>695</fpage>–<lpage>710</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref097"><label>97</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Motter</surname> <given-names>BC</given-names></name>. <article-title>Focal attention produces spatially selective processing in visual cortical areas V1, V2, and V4 in the presence of competing stimuli</article-title>. <source>J Neurophysiol</source>. <year>1993</year>;<volume>70</volume>:<fpage>909</fpage>–<lpage>919</lpage>. <object-id pub-id-type="pmid">8229178</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref098"><label>98</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peelen</surname> <given-names>MV</given-names></name>, <name name-style="western"><surname>Fei-Fei</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Kastner</surname> <given-names>S</given-names></name>. <article-title>Neural mechanisms of rapid natural scene categorization in human visual cortex</article-title>. <source>Nature</source>. <year>2009</year>;<volume>460</volume>(<issue>7251</issue>):<fpage>94</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature08103" xlink:type="simple">10.1038/nature08103</ext-link></comment> <object-id pub-id-type="pmid">19506558</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref099"><label>99</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Epshtein</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lifshitz</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Ullman</surname> <given-names>S</given-names></name>. <article-title>Image interpretation by a single bottom-up top-down cycle</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2008</year>;<volume>105</volume>(<issue>38</issue>):<fpage>14298</fpage>–<lpage>14303</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0800968105" xlink:type="simple">10.1073/pnas.0800968105</ext-link></comment> <object-id pub-id-type="pmid">18796607</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref100"><label>100</label><mixed-citation publication-type="other" xlink:type="simple">Shi WB X, Tsotsos JK. Early recurrence improves edge detection. In: British Machine Vision Conference; 2013.</mixed-citation></ref>
<ref id="pbio.1002611.ref101"><label>101</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ito</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gilbert</surname> <given-names>CD</given-names></name>. <article-title>Attention modulates contextual influences in the primary visual cortex of alert monkeys</article-title>. <source>Neuron</source>. <year>1999</year>;<volume>22</volume>(<issue>3</issue>):<fpage>593</fpage>–<lpage>604</lpage>. <object-id pub-id-type="pmid">10197538</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref102"><label>102</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Katsuki</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Constantinidis</surname> <given-names>C</given-names></name>. <article-title>Early involvement of prefrontal cortex in visual bottom-up attention</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>(<issue>8</issue>):<fpage>1160</fpage>–<lpage>1166</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3164" xlink:type="simple">10.1038/nn.3164</ext-link></comment> <object-id pub-id-type="pmid">22820465</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref103"><label>103</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ullman</surname> <given-names>S</given-names></name>. <article-title>Object recognition and segmentation by a fragment-based hierarchy</article-title>. <source>Trends Cogn Sci (Regul Ed)</source>. <year>2007</year>;<volume>11</volume>(<issue>2</issue>):<fpage>58</fpage>–<lpage>64</lpage>.</mixed-citation></ref>
<ref id="pbio.1002611.ref104"><label>104</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>. <article-title>Perceiving real-world scenes</article-title>. <source>Science</source>. <year>1972</year>;<volume>177</volume>:<fpage>77</fpage>–<lpage>80</lpage>. <object-id pub-id-type="pmid">5041781</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref105"><label>105</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>FF</given-names></name>, <name name-style="western"><surname>VanRullen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Perona</surname> <given-names>P</given-names></name>. <article-title>Rapid natural scene categorization in the near absence of attention</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2002</year>;<volume>99</volume>:<fpage>9596</fpage>–<lpage>9601</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.092277599" xlink:type="simple">10.1073/pnas.092277599</ext-link></comment> <object-id pub-id-type="pmid">12077298</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref106"><label>106</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing</article-title>. <source>Annual Review of Vision Science</source>. <year>2015</year>;<volume>1</volume>:<fpage>417</fpage>–<lpage>446</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-vision-082114-035447" xlink:type="simple">10.1146/annurev-vision-082114-035447</ext-link></comment> <object-id pub-id-type="pmid">28532370</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref107"><label>107</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Zoccolan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>. <article-title>How does the brain solve visual object recognition?</article-title> <source>Neuron</source>. <year>2012</year>;<volume>73</volume>(<issue>3</issue>):<fpage>415</fpage>–<lpage>434</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2012.01.010" xlink:type="simple">10.1016/j.neuron.2012.01.010</ext-link></comment> <object-id pub-id-type="pmid">22325196</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref108"><label>108</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>. <article-title>From circuits to behavior: a bridge too far?</article-title> <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>(<issue>4</issue>):<fpage>507</fpage>–<lpage>509</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3043" xlink:type="simple">10.1038/nn.3043</ext-link></comment> <object-id pub-id-type="pmid">22449960</object-id></mixed-citation></ref>
<ref id="pbio.1002611.ref109"><label>109</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Strasburger</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Rentschler</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Juttner</surname> <given-names>M</given-names></name>. <article-title>Peripheral vision and pattern recognition: a review</article-title>. <source>J Vis</source>. <year>2011</year>;<volume>11</volume>(<issue>5</issue>):<fpage>13</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/11.5.13" xlink:type="simple">10.1167/11.5.13</ext-link></comment> <object-id pub-id-type="pmid">22207654</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>