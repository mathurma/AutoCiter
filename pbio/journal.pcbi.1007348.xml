<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1007348</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-19-00084</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Agriculture</subject><subj-group><subject>Crop science</subject><subj-group><subject>Crops</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Fluorescence imaging</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Image processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Microscopy</subject><subj-group><subject>Light microscopy</subject><subj-group><subject>Fluorescence microscopy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Proteins</subject><subj-group><subject>Luminescent proteins</subject><subj-group><subject>Green fluorescent protein</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Animal studies</subject><subj-group><subject>Experimental organism systems</subject><subj-group><subject>Yeast and fungal models</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Learning unsupervised feature representations for single cell microscopy images with paired cell inpainting</article-title>
<alt-title alt-title-type="running-head">Paired Cell Inpainting</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Lu</surname>
<given-names>Alex X.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6328-9492</contrib-id>
<name name-style="western">
<surname>Kraus</surname>
<given-names>Oren Z.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Supervision</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Cooper</surname>
<given-names>Sam</given-names>
</name>
<role content-type="http://credit.casrai.org/">Supervision</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3118-3121</contrib-id>
<name name-style="western">
<surname>Moses</surname>
<given-names>Alan M.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Computer Science, University of Toronto, Toronto, Canada</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Phenomic AI, Toronto, Canada</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Department of Cell and Systems Biology, University of Toronto, Toronto, Canada</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Center for Analysis of Genome Evolution and Function, University of Toronto, Toronto, Canada</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Caicedo</surname>
<given-names>Juan</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>Oren Z Kraus and Sam Cooper are employees of Phenomic AI.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">alan.moses@utoronto.ca</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>3</day>
<month>9</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>9</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>9</issue>
<elocation-id>e1007348</elocation-id>
<history>
<date date-type="received">
<day>15</day>
<month>1</month>
<year>2019</year>
</date>
<date date-type="accepted">
<day>20</day>
<month>8</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Lu et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1007348"/>
<abstract>
<p>Cellular microscopy images contain rich insights about biology. To extract this information, researchers use features, or measurements of the patterns of interest in the images. Here, we introduce a convolutional neural network (CNN) to automatically design features for fluorescence microscopy. We use a self-supervised method to learn feature representations of single cells in microscopy images without labelled training data. We train CNNs on a simple task that leverages the inherent structure of microscopy images and controls for variation in cell morphology and imaging: given one cell from an image, the CNN is asked to predict the fluorescence pattern in a second different cell from the same image. We show that our method learns high-quality features that describe protein expression patterns in single cells both yeast and human microscopy datasets. Moreover, we demonstrate that our features are useful for exploratory biological analysis, by capturing high-resolution cellular components in a proteome-wide cluster analysis of human proteins, and by quantifying multi-localized proteins and single-cell variability. We believe paired cell inpainting is a generalizable method to obtain feature representations of single cells in multichannel microscopy images.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>To understand the cell biology captured by microscopy images, researchers use features, or measurements of relevant properties of cells, such as the shape or size of cells, or the intensity of fluorescent markers. Features are the starting point of most image analysis pipelines, so their quality in representing cells is fundamental to the success of an analysis. Classically, researchers have relied on features manually defined by imaging experts. In contrast, deep learning techniques based on convolutional neural networks (CNNs) automatically learn features, which can outperform manually-defined features at image analysis tasks. However, most CNN methods require large manually-annotated training datasets to learn useful features, limiting their practical application. Here, we developed a new CNN method that learns high-quality features for single cells in microscopy images, without the need for any labeled training data. We show that our features surpass other comparable features in identifying protein localization from images, and that our method can generalize to diverse datasets. By exploiting our method, researchers will be able to automatically obtain high-quality features customized to their own image datasets, facilitating many downstream analyses, as we highlight by demonstrating many possible use cases of our features in this study.</p>
</abstract>
<funding-group>
<funding-statement>This work was conducted on a GPU generously provided by Nvidia through their academic seeding grant. This work was funded by the National Science and Engineering Research Council (Pre-Doctoral Award), Canada Research Chairs (Tier II Chair), and the Canadian Foundation for Innovation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="3"/>
<page-count count="27"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-09-13</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The cyclops data is downloadable from <ext-link ext-link-type="uri" xlink:href="http://cyclops.ccbr.utoronto.ca" xlink:type="simple">http://cyclops.ccbr.utoronto.ca</ext-link>. The HPA data is downloadable from <ext-link ext-link-type="uri" xlink:href="https://www.proteinatlas.org/humanproteome/cell" xlink:type="simple">https://www.proteinatlas.org/humanproteome/cell</ext-link>, and a script for batch download can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/alexxijielu/paired_cell_inpainting/blob/master/human_model/data_download/download_hpa.py" xlink:type="simple">https://github.com/alexxijielu/paired_cell_inpainting/blob/master/human_model/data_download/download_hpa.py</ext-link>. Single cell features and additional data are downloadable at <ext-link ext-link-type="uri" xlink:href="http://hershey.csb.utoronto.ca/paired_cell_inpainting_features/" xlink:type="simple">http://hershey.csb.utoronto.ca/paired_cell_inpainting_features/</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Feature representations of cells within microscopy images are critical for quantifying cell biology in an objective way. Classically, researchers have manually designed features that measure phenomena of interest within images: for example, a researcher studying protein subcellular localization may measure the distance of a fluorescently-tagged protein from the edge of the cell [<xref ref-type="bibr" rid="pcbi.1007348.ref001">1</xref>], or the correlation of punctuate proteins with microtubules [<xref ref-type="bibr" rid="pcbi.1007348.ref002">2</xref>]. By extracting a range of different features, an image of a cell can be represented as a set of values: these feature representations can then be used for numerous downstream applications, such as classifying the effects of pharmaceuticals on cancer cells [<xref ref-type="bibr" rid="pcbi.1007348.ref003">3</xref>], or exploratory analyses of protein localization [<xref ref-type="bibr" rid="pcbi.1007348.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref004">4</xref>]. The success of these applications depends highly on the quality of the features used: good features are challenging to define, as they must be sensitive to differences in biology, but robust to nuisance variation such as microscopy illumination effects or single cell variation [<xref ref-type="bibr" rid="pcbi.1007348.ref005">5</xref>].</p>
<p>Convolutional neural networks (CNNs) have achieved state-of-the-art performance in tasks such as classifying cell biology in high-content microscopy and imaging flow cytometry screens [<xref ref-type="bibr" rid="pcbi.1007348.ref006">6</xref>–<xref ref-type="bibr" rid="pcbi.1007348.ref009">9</xref>], or segmenting single cells in images [<xref ref-type="bibr" rid="pcbi.1007348.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref011">11</xref>]. A key property driving this performance is that CNNs automatically learn features that are optimized to represent the components of an image necessary for solving a training task [<xref ref-type="bibr" rid="pcbi.1007348.ref012">12</xref>]. Donahue <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1007348.ref013">13</xref>] and Razavian <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1007348.ref014">14</xref>] demonstrate that the feature representations extracted by the internal layers of CNNs trained as classifiers achieve state-of-the-art results on even very different applications than the task the CNN was originally trained for; studies specific to bio-imaging report similar observations about the quality of CNN features [<xref ref-type="bibr" rid="pcbi.1007348.ref006">6</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref016">16</xref>]. The features learned by CNNs are thought to be more sensitive to relevant image content than human-designed features, offering a promising alternative for feature-based image analysis applications.</p>
<p>However, learning high-quality features with CNNs is a current research challenge because it depends highly on the training task. For example, autoencoders, or unsupervised CNNs trained to reconstruct input images, usually do not learn features that generalize well to other tasks [<xref ref-type="bibr" rid="pcbi.1007348.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1007348.ref019">19</xref>]. On the other hand, classification tasks result in high-quality features [<xref ref-type="bibr" rid="pcbi.1007348.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1007348.ref014">14</xref>], but they rely on large, manually-labeled datasets, which are expensive and time-consuming to generate. For example, to address the challenge of labeling microscopy images in the Human Protein Atlas, the project launched a massive crowd-sourcing initiative in collaboration with an online video game, spanning over one year and involving 322,006 gamers [<xref ref-type="bibr" rid="pcbi.1007348.ref020">20</xref>]. Because advances in high-content throughput microscopy are leading to routine generation of thousands of images [<xref ref-type="bibr" rid="pcbi.1007348.ref021">21</xref>], the new cell morphologies and phenotypes discovered and need for integration of datasets [<xref ref-type="bibr" rid="pcbi.1007348.ref004">4</xref>] may require the continuous update of models. Unsupervised methods that result in the learning of high-quality features without the use of manual labels would resolve the bottleneck of collecting and updating labels: we would, in principle, be able to learn feature representations for any dataset, without the need for experts to curate, label, and maintain training images.</p>
<p>If obtaining expert-assigned labels for microscopy images is challenging, then obtaining labels for the single cells within these images is even more difficult. Studying biological phenomena at a single-cell level is of current research interest [<xref ref-type="bibr" rid="pcbi.1007348.ref022">22</xref>]: even in genetically-identical cell cultures, single-cell variability can originate from a number of important regulatory mechanisms [<xref ref-type="bibr" rid="pcbi.1007348.ref023">23</xref>], including the cell cycle [<xref ref-type="bibr" rid="pcbi.1007348.ref024">24</xref>], stochastic shuttling of transcription factors between compartments [<xref ref-type="bibr" rid="pcbi.1007348.ref025">25</xref>], or variability in the DNA damage response [<xref ref-type="bibr" rid="pcbi.1007348.ref026">26</xref>]. Thus, ideally, a method would efficiently generate feature representations of single cells for arbitrary datasets using deep learning, without the need for labelled single-cell training data.</p>
<p>In this study, we asked if CNNs could automatically learn high-quality features for representing single cells in microscopy images, without using manual labels for training. We investigate <italic>self-supervised learning</italic>, which proposes training CNNs using labels that are automatically available from input images. Self-supervised learning aims to develop a feature representation in the CNN that is useful for other tasks: the training task is only a pretext, and the CNN may not achieve a useful level of performance at this pretext task. This differs from weakly-supervised learning, where a learnt network is used directly for an auxiliary task [<xref ref-type="bibr" rid="pcbi.1007348.ref027">27</xref>], such as segmenting tumors in histology images using a network trained to predict disease class [<xref ref-type="bibr" rid="pcbi.1007348.ref028">28</xref>]. After training, the output of the CNN is discarded, and internal layers of the CNN are used as the features. The logic is that by learning to solve the pretext task, the CNN will develop features that are useful for other applications.</p>
<p>The central challenge in self-supervised learning is defining a pretext task that encourages the learning of generalizable features [<xref ref-type="bibr" rid="pcbi.1007348.ref029">29</xref>]. Successful self-supervised learning strategies in the context of natural images, include CNNs trained to predict the appearance of withheld image patches based upon its context [<xref ref-type="bibr" rid="pcbi.1007348.ref018">18</xref>], the presence and location of synthetic artifacts in images [<xref ref-type="bibr" rid="pcbi.1007348.ref029">29</xref>], or geometric rotations applied to input images [<xref ref-type="bibr" rid="pcbi.1007348.ref030">30</xref>]. The idea is that to succeed at the pretext task, the CNN needs to develop a strong internal representation of the objects and patterns in the images. When transferred to tasks such as classification, segmentation, or detection, features developed by self-supervised methods have shown state-of-the-art results compared to other unsupervised methods, and in some cases, perform competitively with features learned by supervised CNNs [<xref ref-type="bibr" rid="pcbi.1007348.ref017">17</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref029">29</xref>–<xref ref-type="bibr" rid="pcbi.1007348.ref032">32</xref>].</p>
<p>Here, we present a novel self-supervised learning method designed for microscopy images of protein expression in single cells. Our approach leverages the typical structure of these images to define the pretext training task: in many cases, each image contains multiple genetically identical cells, growing under the same experimental condition, and these cells exhibit similar patterns of protein expression. The cells are imaged in multiple “channels” (such as multiple fluorescence colours) that contain very different information. By exploiting this structure, we define a pretext task that relies only upon image content, with no prior human labels or annotations incorporated. In our examples, one set of channels represents one or more structures in the cell (e.g. the cytosol, nucleus, or cytoskeleton), and another channel represents proteins of interest that have been fluorescently tagged to visualize their localization (with a different protein tagged in each image). Then, given both channels for one cell and the structural markers for a different cell from the same image, our CNN is trained to predict the appearance of the protein of interest in the second cell, a pretext task that we term “paired cell inpainting” (<xref ref-type="fig" rid="pcbi.1007348.g001">Fig 1A</xref>). To solve this pretext task, we reason that the CNN must identify protein localization in the first (or “source”) cell and reconstruct a similar protein localization in the second (or “target”) cell in a way that adapts to single cell variability. In <xref ref-type="fig" rid="pcbi.1007348.g001">Fig 1</xref>, the protein is localized to the nucleoli of human cells–the network must recognize the localization of the protein in the source cell, but also transfer it to the equivalent structures in the target cell, despite differences in the morphology of the nucleus between the two cells. Thus, by the design of our pretext task, our method learns representations of single cells.</p>
<fig id="pcbi.1007348.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007348.g001</object-id>
<label>Fig 1</label>
<caption>
<title>An overview of the inputs and targets to the network in paired cell inpainting, and of our proposed architecture.</title>
<p>(A) Inputs and targets to the network. We crop a source cell (green border) and a target cell (orange border) from the same image. Then, given all channels for the source cell, and the structural markers for the target cell (in this dataset, the nucleus and the microtubule channels), the network is trained to predict the appearance of the protein channel in the target cell. Images shown are of human cells, with the nucleus colored blue, microtubules colored red, and a specific protein colored green. (B) Example images from the proteome-scale datasets we use in this study. We color the protein channel for each dataset in green. The CyCLOPS yeast dataset has a cytosolic RFP, colored in red. The NOP1pr-ORF yeast dataset has a brightfield channel, colored grey. The Nup49-RFP GFP-ORF yeast dataset has a RFP fused to a nuclear pore marker, colored in red. The Human Protein Atlas images are shown as described above in A. (C) Our proposed architecture. Our architecture consists of a source cell encoder and a target marker encoder. The final layers of both encoders are concatenated and fed into a decoder that outputs the prediction of the target protein <inline-formula id="pcbi.1007348.e001"><alternatives><graphic id="pcbi.1007348.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007348.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. Layers in our CNN are shown as solid-colored boxes; we label each of the convolutional layers (solid grey boxes) with the number of filters used (e.g. 3x3 Conv 96 means 96 filters are used). We show a real example of a prediction from our trained human cell model given the input image patches in this schematic.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.g001" xlink:type="simple"/>
</fig>
<p>To demonstrate the generalizability of our method, we automatically learn feature representations for datasets of both human and yeast cells, with different morphologies, microscopes and resolutions, and fluorescent tagging schemes (<xref ref-type="fig" rid="pcbi.1007348.g001">Fig 1B</xref>). We exemplify the quality of the features learned by our models in several use-cases. First, we show that for building classifiers, the features learned through paired cell inpainting improve in discriminating protein subcellular localization classes at a single-cell level compared to other unsupervised methods. Next, we establish that our features can be used for unsupervised exploratory analysis, by performing an unsupervised proteome-wide cluster analysis of protein analysis in human cells, capturing clusters of proteins in cellular components at a resolution challenging to annotate by human eye. Finally, we determine that our features are useful for single-cell analyses, showing that our features can distinguish phenotypes in spatially-variable single cell populations.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec003">
<title>Paired cell inpainting</title>
<p>We would like to learn a representation for single cells in a collection of microscopy images, <italic>I</italic>. We define each image <italic>i</italic> as a collection of single cells, <italic>i</italic> = {<italic>c</italic><sub><italic>i</italic>,1</sub> …<italic>c</italic><sub><italic>i</italic>,<italic>n</italic></sub>}. We note that the only constraint on <italic>i</italic> is that its single cells <italic>C</italic><sub><italic>i</italic></sub> must be considered similar to each other, so <italic>i</italic> does not need to be strictly defined as a single digital image so long as this is satisfied; in our experiments, we consider an “image” <italic>i</italic> to be all fields of view corresponding to an experimental well.</p>
<p>We define single cells to be image patches, so <inline-formula id="pcbi.1007348.e002"><alternatives><graphic id="pcbi.1007348.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007348.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi><mml:mo>×</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>, where <italic>Z</italic> are the channels. We split the images by channel into <italic>c</italic> = (<italic>x</italic>, <italic>y</italic>), where <inline-formula id="pcbi.1007348.e003"><alternatives><graphic id="pcbi.1007348.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007348.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>,<inline-formula id="pcbi.1007348.e004"><alternatives><graphic id="pcbi.1007348.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007348.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>, and <italic>Z</italic><sub>1</sub>, <italic>Z</italic><sub>2</sub>, ⊆ <italic>Z</italic>. For this work, we assign to <italic>Z</italic><sub>1</sub> channels corresponding to structural markers, or fluorescent tags designed to visualize structural components of the cell, where all cells in the collection of images have been labeled with the tag. We assign to <italic>Z</italic><sub>2</sub> channels corresponding to proteins, or channels where the tagged biomolecule will vary from image to image.</p>
<p>We define a source cell <italic>c</italic><sub><italic>s</italic></sub>, which is associated with a target cell <italic>c</italic><sub><italic>t</italic></sub> satisfying constraints that both cells are from the same image, <italic>c</italic><sub><italic>s</italic></sub> ∈ <italic>i</italic><sub><italic>s</italic></sub>, <italic>c</italic><sub><italic>t</italic></sub> ∈ <italic>i</italic><sub><italic>t</italic></sub>, <italic>i</italic><sub><italic>s</italic></sub> = <italic>i</italic><sub><italic>t</italic></sub>, and <italic>c</italic><sub><italic>s</italic></sub> ≠ <italic>c</italic><sub><italic>t</italic></sub>. Our goal is to train a neural network that will solve the prediction problem <inline-formula id="pcbi.1007348.e005"><alternatives><graphic id="pcbi.1007348.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007348.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>∀</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>I</mml:mi></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1007348.e006"><alternatives><graphic id="pcbi.1007348.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007348.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> represents the predicted protein channels that vary between images.</p>
<p>For this work, we train the network on the prediction problem by minimizing a standard pixel-wise mean-squared error loss between the predicted target protein <inline-formula id="pcbi.1007348.e007"><alternatives><graphic id="pcbi.1007348.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007348.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and the actual target protein <italic>y</italic><sub><italic>t</italic></sub>:
<disp-formula id="pcbi.1007348.e008">
<alternatives>
<graphic id="pcbi.1007348.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007348.e008" xlink:type="simple"/>
<mml:math display="block" id="M8">
<mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>∙</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula></p>
<p>As with other self-supervised methods, our pretext training task is only meant to develop the internal feature representation of the CNN. After training, <inline-formula id="pcbi.1007348.e009"><alternatives><graphic id="pcbi.1007348.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007348.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is discarded, and the CNN is used as a feature extractor. Importantly, while our pretext task predicts a label <italic>y</italic><sub><italic>t</italic></sub>, we consider our overall method to be unsupervised, because these labels are defined automatically from image content without any human supervision.</p>
<p>One limitation in our pretext task is that some protein localizations are not fully deterministic in respect to the structure of the cell and are therefore challenging to predict given the inputs we define. In these cases, we observe that the network produces smoothed predictions that we hypothesize are an averaged guess of the localization. We show an example in <xref ref-type="fig" rid="pcbi.1007348.g001">Fig 1C</xref>; while the source protein is localized to the nucleus in a punctate pattern, the target protein is predicted as a smooth distribution throughout the nucleoplasm. However, as our inpainting task is a pretext and discarded after training, we are not concerned with outputting fully realistic images. The averaging effect is likely due to our choice of a mean squared loss function; should more realistic images be desired, different loss functions, such as adversarial losses [<xref ref-type="bibr" rid="pcbi.1007348.ref033">33</xref>], may produce better results.</p>
</sec>
<sec id="sec004">
<title>Architecture</title>
<p>As the goal of our training task is to obtain a CNN that can encode single cell image patches into feature representations, we construct independent encoders for (<italic>x</italic><sub><italic>s</italic></sub>, <italic>y</italic><sub><italic>s</italic></sub>) and for <italic>x</italic><sub><italic>t</italic></sub>, which we call the “source cell encoder” and the “target marker encoder”, respectively. After training with our self-supervised task, we isolate the source cell encoder, and discard all other components of our model. This architecture allows us to obtain a feature representation of any single cell image patch independently without also having to input target cell markers. To obtain single cell features, we simply input a single cell image patch, and extract the output of an intermediate convolutional layer in the source cell encoder.</p>
<p>We show a summary of our architecture in <xref ref-type="fig" rid="pcbi.1007348.g001">Fig 1C</xref>. Following other work in self-supervised learning [<xref ref-type="bibr" rid="pcbi.1007348.ref017">17</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref018">18</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref030">30</xref>], we use an AlexNet architecture for the source cell encoder, although we set all kernel sizes to 3 due to the smaller sizes of our image patches, and we add batch normalization after each convolutional layer. We use a smaller number of filters and fewer convolutional layers in the architecture of the target marker encoder; we use three convolutional layers, with 16, 32, and 32 filters, respectively. Finally, for the decoder, we reverse the AlexNet architecture.</p>
<p>The goal of our training is to develop the learned features of our source cell encoder, which will later be used to extract single cell feature representations. If the network were to utilize bleed-through from the fluorescent marker channels to predict the target marker image, or overfit to the data and ‘memorize’ the target cell images, then the network would not need to learn to extract useful information from the source cell images. To rule out that our trained models were subject to these effects, we produced inpainting results from a trained model, where we paired source cells with target cells where there is a mismatch between source and target protein localization. Here, the model has seen both the source and target cells during training, but never the specific pair due to the structure of our training task, as the cells originate from different images. We qualitatively confirmed that our trained networks were capable of synthesizing realistic results agreeing with the protein localization of the source cell (<xref ref-type="supplementary-material" rid="pcbi.1007348.s001">S1 Fig</xref>), suggesting that our models are not trivially overfitting to the target marker images.</p>
</sec>
<sec id="sec005">
<title>Sampling pairs</title>
<p>Because the training inputs to our self-supervised task consist of pairs of cells, the number of possible combinations is large, as each cell may be paired with one of many other cells. This property is analogous to data augmentation, increasing the number of unique training inputs to our network. To sample training inputs, for each epoch, we iterate through every single cell in our training dataset, set it as the source cell, and draw with uniform probability a target cell from the set of all valid possible target cells.</p>
<p>Our pretext task relies on the assumption that protein expression in single cells from the same image is similar. This assumption is not always true: in the datasets used in this work, some proteins exhibit significant single cell variability in their protein abundance or localization [<xref ref-type="bibr" rid="pcbi.1007348.ref034">34</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref035">35</xref>]. These proteins may contribute noise because paired single cells will have (unpredictably) different protein expression patterns and the model will not learn. Although the Human Protein Atlas documents variable proteins [<xref ref-type="bibr" rid="pcbi.1007348.ref024">24</xref>], for our experiments we do not remove these and confirm that our model still learns good features in spite of this noise.</p>
</sec>
<sec id="sec006">
<title>Datasets, preprocessing, and training</title>
<p>For yeast cells, we used the WT2 dataset from the CYCLoPS database [<xref ref-type="bibr" rid="pcbi.1007348.ref036">36</xref>]. This collection expresses a cytosolic red fluorescent protein (RFP) in all cells, and tags proteins of interest with green fluorescent protein (GFP). We use the RFP channel as the structural marker, and the GFP channel as the protein. To extract single cell crops from the images in this dataset, we segmented our images using YeastSpotter on the structural marker channel [<xref ref-type="bibr" rid="pcbi.1007348.ref037">37</xref>], and extract a 64x64 pixel crop around the identified cell centers; we discarded any single cells with an area smaller than 5% or greater than 95% of the image crop, as these are likely artifacts arising from under- or over-segmentation. We discard any images with fewer than 30 cells. We preprocessed crops by rescaling each crop to be in the range of [0, 1]. These preprocessing operations result in a total of 1,165,713 single cell image patches grouped into 4,069 images (where each image is 4 fields of view), with a total of 4,069 of 4,138 proteins passing this filter.</p>
<p>We also trained a second different yeast cell model, using a NOP1pr-GFP library previously published by Weill <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1007348.ref037">37</xref>] This collection tags all proteins of interest with GFP at the N-terminal of proteins with a NOP1 promoter, and is also imaged in brightfield. We used the brightfield channel as a structural marker and the GFP channel as the protein. We extracted and preprocessed single cell crops using the same procedure for the CYCLoPS dataset, and discarded any images with fewer than 30 single cells. These preprocessing operations result in a total of 563,075 single cell image patches grouped into 3,067 images (where each image is 3 fields of view), with a total of 3,067 of 3,916 proteins passing this filter.</p>
<p>Finally, we trained a third yeast cell model, using a dataset previously published by Tkach <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1007348.ref038">38</xref>], a Nup49-RFP GFP-ORF library. This collection expresses a nuclear pore protein (Nup49) fused to RFP in all cells, and tags proteins of interest with green fluorescent protein (GFP). We use the RFP channel as the structural marker, and the GFP channel as the protein. We extracted and preprocessed single cell crops using the same procedure for the CYCLoPS dataset, and discarded any images with fewer than 30 single cells. These preprocessing operations result in a total of 1,733,127 single cell image patches grouped into 4,085 images (where each image is 3 fields of view), with a total of 4,085 of 4,149 proteins passing this filter.</p>
<p>For human cells, we use images from version 18 of the Human Protein Atlas [<xref ref-type="bibr" rid="pcbi.1007348.ref024">24</xref>]. We were able to download jpeg images for a total of 12,068 proteins. Each protein may have multiple experiments, which image different cell line and antibody combinations. We consider an image to be of a protein for the same cell line and antibody combination; accordingly, we have 41,517 images (where each image is usually 2 fields of view). We downloaded 3 channels for these images. Two visualize the nuclei and microtubules, which we use as the structural marker channels. The third channel is an antibody for the protein of interest, which we use as the protein channel. To extract single cell crops from this image, we binarize the nuclear channel with an Otsu filter and find nuclei by labeling connected components as objects using the scikit-image package [<xref ref-type="bibr" rid="pcbi.1007348.ref039">39</xref>]. We filter any objects with an area of less than 400 pixels, and extract a 512x512 pixel crop around the center of mass of remaining objects. To reduce training time, we rescale the size of each crop to 64x64 pixels. We preprocessed crops by rescaling each crop to be in the range of [0, 1], and clipped pixels under 0.05 intensity for the microtubule and nuclei channels to 0 to improve contrast. Finally, we remove any images with fewer than 5 cells, leaving a total of 638,640 single cell image patches grouped into 41,285 images, with a total of 11,995 of 12,068 proteins passing this filter.</p>
<p>Crop sizes for our datasets (64x64 pixels for yeast cells and 512x512 pixels for human cells) were chosen such that a crop fully encompasses an average cell from each of these datasets. We note that different image datasets may require different crop sizes, depending on the resolution of the images and the size of the cells. While each crop is centered around a segmentation, we did not filter crops with overlapping or clumped cells, so some crops may contain multiple cells. In general, we observed that our models did not have an issue learning to inpaint protein expression from a crop with multiple cells to a crop with a single cell, or vice versa: <xref ref-type="supplementary-material" rid="pcbi.1007348.s001">S1 Fig</xref> shows an example of a case where we inpaint protein expression from a crop with two cells to a crop with one cell.</p>
<p>During training, we apply random horizontal and vertical flips to source and target cells independently as data augmentation. We trained models for 30 epochs using an Adam optimizer with an initial learning rate of 1e-4.</p>
<p>After training, we extract representations by maximum pooling the output of an intermediate convolutional layer, across spatial dimensions. This strategy follows previous unsupervised representation extraction from self-supervised learning methods [<xref ref-type="bibr" rid="pcbi.1007348.ref017">17</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref030">30</xref>], which sample activations from feature maps.</p>
</sec>
<sec id="sec007">
<title>Baseline feature extraction and benchmarking</title>
<p>To benchmark the performance of features learned using paired cell painting, we obtained features from other commonly-used feature representation strategies.</p>
<p>As classic computer vision baselines for our yeast cell benchmarks, we obtained features extracted using CellProfiler [<xref ref-type="bibr" rid="pcbi.1007348.ref040">40</xref>] for a classification dataset of 30,889 image crops of yeast cells directly from the authors [<xref ref-type="bibr" rid="pcbi.1007348.ref041">41</xref>]. These features include measurements of intensity, shape, and texture, and have been optimized for classification performance. Further details are available from [<xref ref-type="bibr" rid="pcbi.1007348.ref041">41</xref>]. We also extracted features from these yeast cell image crops using interpretable expert-designed features by Handfield <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1007348.ref001">1</xref>] We followed procedures previously established by the authors: we segmented cells using provided software, and calculated features from the center cell in each crop.</p>
<p>For our transfer learning baselines in our yeast cell benchmarks, we used a VGG16 model pretrained on ImageNet, using the Keras package. We benchmarked three different input strategies: (1) we mapped channels arbitrarily to RGB channels (RFP to red, GFP to green, blue channel left empty); (2) we inputted each channel separately as a greyscale image and concatenated the representations; (3) we inputted only the GFP channel as a greyscale image and used this representation alone. In addition, we benchmarked representations from each convolutional layer of VGG16, post-processed by maximum pooling across spatial dimensions (as we did for our self-supervised features). <xref ref-type="supplementary-material" rid="pcbi.1007348.s002">S2 Fig</xref> shows classification performance using each layer in VGG16 with the <italic>k</italic>NN classifier described in our benchmarks, across all three strategies. In general, we observed that the 3<sup>rd</sup> input strategy resulted in superior performance, with performance peaking in the 4<sup>th</sup> convolutional block of VGG16. We report results from the top-performing layer using the top-performing input strategy in our benchmarks.</p>
<p>Contrary to previous work in transfer learning on microscopy images by Pawlowski <italic>et al</italic>., we extract features from the intermediate convolutional layers of our transferred VGG16 model instead of the final fully-connected layer [<xref ref-type="bibr" rid="pcbi.1007348.ref015">15</xref>]. This modification allows us to input our images at their original size, instead of resizing them to the size of the images originally used to train the transferred model. As our work operates on single cell crops, which are much smaller than the full images benchmarked in previous work (64x64 pixels compared to 1280x1024 pixels), we found that inputting images at their original size instead of stretching them resulted in performance gains: our top-performing convolutional layer (block4_conv1) with inputs at original size achieved 69.33% accuracy, whereas resizing the images to 224x224 and using features from the final fully-connected layer (as-is, without max-pooling, as described in [<xref ref-type="bibr" rid="pcbi.1007348.ref015">15</xref>]) achieves 65.64% accuracy. In addition, we found that extracting features from images at original resolution improves run-time: on our machine, inputting 64x64 crops and extracting features from the best-performing layer was about 16 times faster than inputting resized 224x224 images and extracting features from the final fully-connected layer.</p>
<p>Finally, for the supervised baseline, we used the model and pretrained weights provided by Kraus <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1007348.ref006">6</xref>]. We inputted images as previously described. To ensure that metrics reported for these features were comparable with the other accuracies we reported, we extracted features from this model and built the same classifier used for the other feature sets. We found that features from the final fully connected layer before the classification layer performed the best, and report results from this layer.</p>
<p>To compare the performance of various feature representations with our single yeast cell dataset, we built <italic>k</italic>NN classifiers. We preprocessed each dataset by centering and scaling features by their mean and standard deviation. We employed leave-one-out cross-validation and predicted the label of each cell based on its neighbors using Euclidean distance. <xref ref-type="supplementary-material" rid="pcbi.1007348.s006">S1 Table</xref> shows classification accuracy with various parameterizations of <italic>k</italic>. We observed that regardless of <italic>k</italic>, feature representations were ranked the same in their classification performance, with our paired cell inpainting features always outperforming other unsupervised feature sets. However, <italic>k</italic> = 11 produced the best results for all feature sets, so we report results for this parameterization.</p>
<p>As classic computer vision baselines for our human cell benchmarks, we curated a set of texture, correlation, and intensity features. For each crop, we measured the sum, mean, and standard deviation of intensity from pixels in the protein channels, and the Pearson correlation between the protein channel and the microtubule and nucleus channels. We extracted Haralick texture features from the protein channel at 5 scales (1, 2, 4, 8, and 16 pixels).</p>
<p>Finally, as the transfer learning baseline for our human cell benchmarks, we extracted features from the pretrained VGG16 model using the same input strategies and layer as established in our yeast cell benchmark.</p>
</sec>
<sec id="sec008">
<title>Comparing pairwise distances in feature spaces</title>
<p>To directly measure and compare how a feature set groups together cells with similar localizations in their feature spaces, measured the average pairwise distance between cells in the feature space. We preprocess single cell features by scaling to zero mean and unit variance, to control for feature-to-feature differences in scaling within feature sets. Then, to calculate the distance between two single cells <italic>c</italic>, we use the Euclidean distance between their features <inline-formula id="pcbi.1007348.e010"><alternatives><graphic id="pcbi.1007348.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007348.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>.</p>
<p>Given two images with the same localization term, we calculate the average distance of all cells in the first image paired with all cells in the second image and normalize these distances to an expectation of the average pairwise distance between images with different localization terms. A negative normalized average pairwise distance score indicates that the distances are smaller than expectation (so single cells in images with the same label are on average closer in the feature space).</p>
<p>For the Human Protein Atlas images, we restricted our analysis to proteins that only had a single localization term shared by at least 30 proteins, resulting in proteins with 18 distinct localization terms (as listed in <xref ref-type="fig" rid="pcbi.1007348.g002">Fig 2B</xref>). For each localization term, we calculated average pairwise distances for 1,000 random protein pairs with the same localization term, relative to an expectation from 1,000 random protein pairs with each of the possible other localization terms (for a total of 17,000 pairs sampled to control for class imbalance). For our experiments controlling for cell line, we also introduce the constraint that the images must be of cells of the same or different cell lines, depending on the experiment. Because some localization terms and cell line combinations are rare, we did not control for class imbalance and drew 10,000 random protein pairs with any different localization terms (not necessarily each other different term), and compared this to 10,000 random protein pairs with the same localization term. Hence, the distances in the two experiments are not directly comparable.</p>
<fig id="pcbi.1007348.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007348.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Quantitative comparisons of paired cell inpainting features with other unsupervised feature sets.</title>
<p>(A) Overall and class-by-class performance benchmark for yeast single cell protein localization classes using unsupervised feature sets and a <italic>k</italic>NN classifier (<italic>k</italic> = 11) on our test set. For all approaches extracting features from CNNs (Transfer Learning, Autoencoder, and Paired Cell Inpainting), we extract representations by maximum pooling across spatial dimensions, and report the top-performing layer. We report the overall accuracy as the balanced accuracy of all classes. (B) The normalized pairwise distance between cells with the same localization terms according to gene ontology labels, for proteins in the Human Protein Atlas. A more negative score indicates that cells are closer in the feature set compared to a random expectation.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec009">
<title>Single cell analysis of multiply-localized proteins</title>
<p>For proteins localized to two compartments, we calculated a score for each cell based upon its distance to the first compartment versus its distance to the second compartment. To do so, we averaged the feature vectors for all single-cells in images annotated to localize to each compartment alone to define the average features for the two compartments. Then, for every single-cell, we calculated the distance of the single-cell’s features relative to the two compartments’ averages and take a log ratio of the distance to the first compartment divided by the distance to the second compartment. A negative number reflects that the single-cell is closer to the first compartment in the feature space, while a positive number reflects that the single-cell is closer to the second compartment.</p>
</sec>
<sec id="sec010">
<title>Code availability</title>
<p>Code and pre-trained weights for the models used in this work are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/alexxijielu/paired_cell_inpainting" xlink:type="simple">https://github.com/alexxijielu/paired_cell_inpainting</ext-link>.</p>
</sec>
</sec>
<sec id="sec011" sec-type="results">
<title>Results</title>
<sec id="sec012">
<title>Paired cell inpainting features discriminate protein subcellular localization in yeast single cells</title>
<p>To assess the quality of feature learned through paired cell inpainting we trained a model for yeast fluorescent microscopy images using paired cell inpainting, on an unlabelled training set comprising of the entire WT2 screen in the CyCLOPS dataset, encompassing 1,165,713 single cells from 4,069 images.</p>
<p>Good features are sensitive to differences in biology, but robust to nuisance variation [<xref ref-type="bibr" rid="pcbi.1007348.ref005">5</xref>]. As a first step to understanding if our features had these properties, we compared paired cell inpainting features with other feature sets at the task of discriminating different subcellular localization classes in yeast single cells. To do this, we made use of a test set of 30,889 single cell image patches manually assigned to 17 different protein subcellular localization classes by Chong <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1007348.ref041">41</xref>]. These single cells have been curated from a different image screen than the one we used to train our model, and thus represent an independent test dataset that was never seen by the model during training.</p>
<p>To evaluate feature sets, we constructed a simple <italic>k</italic>NN classifier (<italic>k</italic> = 11) and evaluated classification performance by comparing the predicted label of each single cell based on its neighbors to its actual label. While more elaborate classifiers could be used, the <italic>k</italic>NN classifier is simple and transparent, and is frequently employed to compare feature sets for morphological profiling [<xref ref-type="bibr" rid="pcbi.1007348.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref016">16</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref042">42</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref043">43</xref>]. Like these previous works, the goal of our experiment is to assess the relative performance of various feature sets in a controlled setting, not to present an optimal classifier.</p>
<p>To use the feature representation from a self-supervised CNN, we must first identify which layers represent generalizable information about protein expression patterns. Different layers in self-supervised models may have different properties: the earlier layers may only extract low-level features, but the later layers may be too specific to production of the pretext task [<xref ref-type="bibr" rid="pcbi.1007348.ref012">12</xref>]. For this reason, identifying self-supervised features with a layer-by-layer evaluation of the model’s properties is standard [<xref ref-type="bibr" rid="pcbi.1007348.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1007348.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1007348.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1007348.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1007348.ref032">32</xref>]. Since our model has five convolutional layers, it can be interpreted as outputting five different feature sets for each input image. To determine which feature set would be best for the task of classifying yeast single cells, we constructed a classifier on our test set for the features from each layer independently, as shown in <xref ref-type="table" rid="pcbi.1007348.t001">Table 1</xref>. Overall, we observe that the third (Conv3) and fourth (Conv4) convolutional layers result in the best performance.</p>
<table-wrap id="pcbi.1007348.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007348.t001</object-id>
<label>Table 1</label> <caption><title>Classification accuracies for various layers in our CNN.</title></caption>
<alternatives>
<graphic id="pcbi.1007348.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Layer</th>
<th align="center">Overall Classification Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" style="background-color:#CCCCCC"><bold>Conv1</bold></td>
<td align="char" char="." style="background-color:#CCCCCC">43.09</td>
</tr>
<tr>
<td align="center"><bold>Conv2</bold></td>
<td align="char" char=".">70.52</td>
</tr>
<tr>
<td align="center" style="background-color:#CCCCCC"><bold>Conv3</bold></td>
<td align="char" char="." style="background-color:#CCCCCC"><bold>84.30</bold></td>
</tr>
<tr>
<td align="center"><bold>Conv4</bold></td>
<td align="char" char="."><bold>87.98</bold></td>
</tr>
<tr>
<td align="center" style="background-color:#CCCCCC"><bold>Conv5</bold></td>
<td align="char" char="." style="background-color:#CCCCCC">81.89</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001">
<p>Classification accuracies using various layers in our CNN, for single yeast cell localization classes using a <italic>k</italic>NN classifier (<italic>k</italic> = 11) on our test set. For each layer, we extract a representation by maximum pooling feature maps across spatial dimensions. We report the overall accuracy as the balanced accuracy of all classes.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Next, we sought to compare our features from paired cell painting with other unsupervised feature sets commonly used for cellular microscopy images. First, we compared our features with feature sets designed by experts: CellProfiler features [<xref ref-type="bibr" rid="pcbi.1007348.ref040">40</xref>] that were previously extracted from segmented single cells by Chong <italic>et al</italic>. to train their single cell classifier [<xref ref-type="bibr" rid="pcbi.1007348.ref041">41</xref>], and with interpretable features measuring yeast protein localization calculated using yeast-specific segmentation and feature extraction software developed by Handfield <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1007348.ref001">1</xref>]. Second, we compared our features with those obtained from transfer learning, i.e., repurposing features from previously-trained supervised CNNs in other domains [<xref ref-type="bibr" rid="pcbi.1007348.ref013">13</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref014">14</xref>]: we used features from a VGG16 model [<xref ref-type="bibr" rid="pcbi.1007348.ref044">44</xref>], a previous supervised CNN trained on Imagenet, a large-scale classification dataset of natural images [<xref ref-type="bibr" rid="pcbi.1007348.ref045">45</xref>]. Finally, we transferred features from an autoencoder (an unsupervised CNN) trained on the same dataset as our paired cell inpainting models, using our source cell encoder and decoder architecture.</p>
<p>Overall, the CellProfiler and VGG16 features perform at 68.15% and 69.33% accuracy respectively on our evaluation set, while the yeast-specific, interpretable features are worse at 62.04%. Autoencoder features achieve 42.50% accuracy. Our features from convolutional layer 4 (Conv4) perform better than any of the other features benchmarked by a large margin, achieving 87.98% (<xref ref-type="fig" rid="pcbi.1007348.g002">Fig 2A</xref>).</p>
<p>As an estimate of the upper bound of classification performance with our <italic>k</italic>NN classifier, we evaluated the performance of features extracted by the layers of the state-of-the-art supervised CNN single-cell classifier trained with most of the test set used here [<xref ref-type="bibr" rid="pcbi.1007348.ref006">6</xref>]. The features from the best performing layer from this supervised model achieve 92.39% accuracy, which is comparable to the 87.98% accuracy achieved by the self-supervised paired cell inpainting features. Taken together, these results suggest that paired cell inpainting learns features comparable with those learned by supervised neural networks for discriminating protein subcellular localization in yeast single cells.</p>
<p>To test if these improvements in classification performance depended on the kind of classifier used, we also benchmarked the performance of logistic regression and random forest classifiers built with all feature sets (<xref ref-type="supplementary-material" rid="pcbi.1007348.s007">S2 Table</xref>). We observed that our paired cell inpainting features continue to outperform other unsupervised feature sets by a large margin, even with different classification models.</p>
<p>To assess if the improvements in classification performance over other unsupervised methods was because our features form a general global representation of protein localization, as opposed to just clustering similar cells locally, we visualized features for this yeast single cell dataset using UMAP [<xref ref-type="bibr" rid="pcbi.1007348.ref046">46</xref>] (<xref ref-type="supplementary-material" rid="pcbi.1007348.s003">S3 Fig</xref>). We observed that with our paired cell inpainting features, single cells with the same label cluster together in a continuous distribution in the UMAP-reduced space, similar to those learned by the fully-supervised model, whereas this effect is not as strong with CellProfiler features or VGG16 features.</p>
<p>Finally, to assess if our method could learn effective representations for yeast image datasets regardless of modality or fluorescent tagging scheme, we trained two new models on two additional datasets: a yeast GFP-ORF collection imaged in brightfield [<xref ref-type="bibr" rid="pcbi.1007348.ref047">47</xref>], and a yeast GFP-ORF collection imaged with a nuclear pore structural marker [<xref ref-type="bibr" rid="pcbi.1007348.ref038">38</xref>]. We averaged the features of all single cells for each protein for each model and visualized the protein representations using UMAP (shown in <xref ref-type="fig" rid="pcbi.1007348.g003">Fig 3</xref>, along with example images from each dataset), coloring points using previous manual labels on the protein level [<xref ref-type="bibr" rid="pcbi.1007348.ref047">47</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref048">48</xref>]. We observed distinct clusters for most protein localization labels for the representation produced by each model. Remarkably, even though each model is independent and trained on an independent dataset, we observed a substantial degree of consistency between clusters in the distances of their learned representations: for example, in all three representations, the “nucleolus” cluster was close to the “nucleus” cluster, and the “ER” and “cytoplasm and nucleus” clusters were adjacent to the “cytoplasm” cluster.</p>
<fig id="pcbi.1007348.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007348.g003</object-id>
<label>Fig 3</label>
<caption>
<title>UMAP representations of protein-level paired cell inpainting representations for three independent yeast image datasets.</title>
<p>Protein-level representations are produced by averaging the features for all single cells for each protein. A) shows the CyCLOPS WT2 dataset, B) shows the dataset of Weill <italic>et al</italic>., and C) shows the dataset of Tkach <italic>et al</italic>. All UMAPs are generated with the same parameters (Euclidean distance, 20 neighbors, minimum distance of 0.3). Embedded points are visualized as a scatterplot and are colored according to their label, as shown in the shared legend to the bottom right; to reduce clutter, we only show a subset of the protein localization classes. Under each UMAP, we show representative images of mitochondrial, nucleus, and cytoplasm labels from each dataset.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec013">
<title>Human cells with similar proteins are closer in the paired cell inpainting feature space</title>
<p>To further test the generalizability of the method, we trained a model for human fluorescent microscopy images with paired cell inpainting, on the entirety of the Human Protein Atlas, encompassing 638,640 single cells from 41,285 images. While the Human Protein Atlas does contain information about the general protein localization for each image, our goal is to demonstrate that our method can work on high-content human datasets without expert labels. Hence, for training our model, we do not make use of these labels in any form.</p>
<p>To evaluate the feature representation of single human cells, we directly analyzed the pairwise distances in the feature space. In contrast to the previous analysis (<xref ref-type="fig" rid="pcbi.1007348.g002">Fig 2A</xref>), we do not have a labeled single cell dataset for human cells: annotations in the Human Protein Atlas are at image-level. We therefore did not analyze classification performance. Instead, since we expect features to give similar representations to cells with similar protein localization patterns (ideally independent of cell morphologies, illumination effects, or cell lines), we compared the distance between cells with similar localization patterns in the feature space to cells with different localization patterns. We computed the normalized average distance between cells in images for proteins with the same gene ontology (GO) localization term (see <xref ref-type="sec" rid="sec002">Methods</xref>): a more negative score indicates that cells with the same GO term are on average closer together in the feature space (<xref ref-type="fig" rid="pcbi.1007348.g002">Fig 2B</xref>). We observed that features from both the Conv3 (-0.574 overall) and Conv4 (-0.349 overall) layers show smaller distances for proteins with the same GO localizations than classic computer vision features (-0.274 overall), autoencoder features (-0.281 overall), and features transferred from a pre-trained VGG16 model (-0.325 overall).</p>
<p>To test the robustness of the feature representation to morphological differences, we repeated the above experiment, but controlling for cell line. We reasoned that if a feature set was robust, cells with similar localization patterns would show smaller distances, even if the cells had different morphologies. We compared the normalized average pairwise distance between cells from images with the same GO localization term, when both images were constrained to be from different cell lines, compared to when both images were constrained to be from the same cell lines (<xref ref-type="table" rid="pcbi.1007348.t002">Table 2</xref>). We note that we normalized the results shown in <xref ref-type="table" rid="pcbi.1007348.t002">Table 2</xref> differently from that of <xref ref-type="fig" rid="pcbi.1007348.g002">Fig 2B</xref>, so that the distances are not directly comparable (see <xref ref-type="sec" rid="sec002">Methods</xref>).</p>
<table-wrap id="pcbi.1007348.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007348.t002</object-id>
<label>Table 2</label> <caption><title>Normalized average pairwise distances between cells with the same GO localization term in various feature spaces.</title></caption>
<alternatives>
<graphic id="pcbi.1007348.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Feature Set</th>
<th align="center">Normalized Average Pairwise Distance<break/>(Same Cell Lines)</th>
<th align="center">Normalized Average Pairwise Distance<break/>(Different Cell Lines)</th>
<th align="center">Difference</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" style="background-color:#CCCCCC"><bold>Classic</bold></td>
<td align="char" char="." style="background-color:#CCCCCC">-0.3027</td>
<td align="char" char="." style="background-color:#CCCCCC">-0.1813</td>
<td align="char" char="." style="background-color:#CCCCCC">0.1214</td>
</tr>
<tr>
<td align="center"><bold>Transfer Learning (VGG16)</bold></td>
<td align="char" char=".">-0.2525</td>
<td align="char" char=".">-0.1515</td>
<td align="char" char=".">0.1010</td>
</tr>
<tr>
<td align="center" style="background-color:#CCCCCC"><bold>Autoencoder</bold></td>
<td align="char" char="." style="background-color:#CCCCCC">-0.2486</td>
<td align="char" char="." style="background-color:#CCCCCC">-0.0965</td>
<td align="char" char="." style="background-color:#CCCCCC">0.1521</td>
</tr>
<tr>
<td align="center"><bold>Paired Cell Inpainting (Conv3)</bold></td>
<td align="char" char="."><bold>-0.4855</bold></td>
<td align="char" char="."><bold>-0.3953</bold></td>
<td align="char" char="."><bold>0.0902</bold></td>
</tr>
<tr>
<td align="center" style="background-color:#CCCCCC"><bold>Paired Cell Inpainting (Conv4)</bold></td>
<td align="char" char="." style="background-color:#CCCCCC"><bold>-0.2656</bold></td>
<td align="char" char="." style="background-color:#CCCCCC"><bold>-0.2271</bold></td>
<td align="char" char="." style="background-color:#CCCCCC"><bold>0.0385</bold></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t002fn001">
<p>Normalized average pairwise distance scores between cells in image pairs with the same GO localization term, with the additional constraint of both images between from either the same cell line, or from different cell lines. We also report the difference between the two sets of scores; a smaller difference means that the model’s performance drops less when cell lines are different.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>As expected, all feature sets have higher distances in their feature spaces for cells with similar localizations, when cells come from different cell lines. However, paired cell inpainting features have the smallest drop, and still perform the best of all feature sets, even when cells come from different cell lines with different typical cell morphologies. These results indicate that our method produces single cell features that not only place similar cells closer together in the feature space, but that they are more robust to morphological variation.</p>
<p>Interestingly, while features from Conv4 were worse at separating different protein localizations than Conv3, its features were the most robust to cell line. Based upon this observation, we hypothesize that the earlier layers in our model may represent sensitive information directly extracted from the images, whereas later layers assemble this information into a more robust, morphology-invariant form. We also noticed that the best overall performing layer differed from our yeast model to our human model. These results suggest that the behavior of layers may differ from dataset to dataset, and a preliminary evaluation of layer-by-layer properties is important for selecting the best layer for subsequent applications of the features. However, the middle layers in our model (Conv3 and Conv4) still outperformed other unsupervised feature sets in both models, suggesting that the selection of layer to use is still robust even in the absence of this preliminary evaluation.</p>
</sec>
<sec id="sec014">
<title>Proteome-wide clustering of human proteins with paired cell inpainting features discover localization classes not visible by human eye</title>
<p>We next sought to demonstrate the utility of the features to various applications in cell biology. First, we used the paired cell inpainting features from Conv3 of our human model in an exploratory cluster analysis of human proteins (<xref ref-type="fig" rid="pcbi.1007348.g004">Fig 4</xref>). Similar clustering experiments have been performed for yeast proteins, and have found clusters that not only recapitulate previously-defined subcellular localization patterns, but discover protein localization and function at a higher resolution than manual assignment [<xref ref-type="bibr" rid="pcbi.1007348.ref001">1</xref>]. For this analysis, we included every protein in the Human Protein Atlas that we obtained enough single cells for (a total of 11995 of 12068 proteins). All of these proteins are shown in <xref ref-type="fig" rid="pcbi.1007348.g004">Fig 4</xref>. We averaged the features of all single cells for each protein (pooling together all single cells from different cell lines and antibodies), and clustered the averaged features using hierarchical agglomerative clustering. While hierarchical clustering has previously been applied to a subset of human proteins in a single cell line to identify misannotated proteins [<xref ref-type="bibr" rid="pcbi.1007348.ref049">49</xref>], our analysis is, to our knowledge, the most comprehensive for the Human Protein Atlas.</p>
<fig id="pcbi.1007348.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007348.g004</object-id>
<label>Fig 4</label>
<caption>
<title>A clustered heat map of paired cell inpainting feature representation of proteins in the Human Protein Atlas.</title>
<p>Proteins are ordered using maximum likelihood agglomerative hierarchical clustering. We visualize the average of paired cell inpainting features for all cells for each protein as a heat map, where positive values are colored yellow and negative values are colored blue, with the intensity of the color corresponding to magnitude. Columns in this heat map are features, while rows are proteins. Features have been mean-centered and normalized. We manually select major clusters (grey and black bars on the right), as well as some sub-clusters within these major clusters (orange and purple bars on the right), for enrichment analysis, presented in <xref ref-type="table" rid="pcbi.1007348.t003">Table 3</xref>. On the right panels (panels a-e), we show representative examples of the protein channel (green) of cells from proteins (as labeled) in the clusters indicated by arrows from the images.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.g004" xlink:type="simple"/>
</fig>
<p>Gene ontology (GO) enrichments for manually identified clusters in <xref ref-type="fig" rid="pcbi.1007348.g004">Fig 4</xref>. For each cluster, we report selected GO annotation terms in the component ontology for <italic>Homo sapiens</italic>, their enrichment (adjusted with the false discovery rate), and the number of proteins with the enrichment versus the total number of proteins in the cluster. We used all human proteins in our clustering solution as the background for these enrichments. We used GOrilla [<xref ref-type="bibr" rid="pcbi.1007348.ref050">50</xref>], a bioinformatics tool for identifying enriched GO terms. Full lists of the proteins in each cluster are available as part of <xref ref-type="supplementary-material" rid="pcbi.1007348.s008">S1 Data</xref>.</p>
<p>We find that proteins in the same cellular components generally cluster together (<xref ref-type="table" rid="pcbi.1007348.t003">Table 3</xref>). We identified clusters enriched for about 15 of the 18 localization classes with more than 30 proteins. As these are the cellular components manually annotated by the Human Protein Atlas, these results suggest that our unbiased, unsupervised clustering of proteins broadly agrees with the protein localization classes assigned by biologists. Within large clusters of more general localizations, we also identified smaller sub-clusters of protein complexes or functional protein classes, including the spliceosomal complex (Cluster A2), the ribonucleoprotein complex (Cluster E3), and the preribosome (Cluster D2). These are cellular components that are not typically identifiable by human eye, suggesting that our features are describing the localization of the proteins at a higher-resolution than human annotation.</p>
<table-wrap id="pcbi.1007348.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007348.t003</object-id>
<label>Table 3</label> <caption><title>Enrichments for clusters identified in <xref ref-type="fig" rid="pcbi.1007348.g004">Fig 4</xref>.</title></caption>
<alternatives>
<graphic id="pcbi.1007348.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.t003" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Cluster</th>
<th align="center">GO Annotation</th>
<th align="center">Enrichment<break/>(FDR Adjusted)</th>
<th align="center">Number of Proteins</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">A</td>
<td align="center">Nucleoplasm</td>
<td align="center">5.25E-139</td>
<td align="center">752/1383</td>
</tr>
<tr>
<td align="center">A1</td>
<td align="center">Chromosomal Part</td>
<td align="center">3.25E-12</td>
<td align="center">34/114</td>
</tr>
<tr>
<td align="center" rowspan="2">A2</td>
<td align="center">Nuclear Speck</td>
<td align="center">4.37E-16</td>
<td align="center">26/91</td>
</tr>
<tr>
<td align="center">Spliceosomal Complex</td>
<td align="center">3.39E-17</td>
<td align="center">21/91</td>
</tr>
<tr>
<td align="center">A3</td>
<td align="center">Nuclear Speck</td>
<td align="center">8.92E-17</td>
<td align="center">25/76</td>
</tr>
<tr>
<td align="center" rowspan="2">B</td>
<td align="center">Nucleoplasm</td>
<td align="center">2.63E-24</td>
<td align="center">303706</td>
</tr>
<tr>
<td align="center">Cytosol</td>
<td align="center">1.55E-19</td>
<td align="center">360/706</td>
</tr>
<tr>
<td align="center">C</td>
<td align="center">Nuclear Membrane</td>
<td align="center">6.32E-40</td>
<td align="center">44/122</td>
</tr>
<tr>
<td align="center">D</td>
<td align="center">Nucleolus</td>
<td align="center">6.48E-122</td>
<td align="center">209/553</td>
</tr>
<tr>
<td align="center">D1</td>
<td align="center">Nucleolar Rim</td>
<td align="center">(Manual)</td>
<td align="center">14/32</td>
</tr>
<tr>
<td align="center">D2</td>
<td align="center">Preribosome</td>
<td align="center">2.81E-22</td>
<td align="center">58/112</td>
</tr>
<tr>
<td align="center">E</td>
<td align="center">Cytosol</td>
<td align="center">2.22E-64</td>
<td align="center">564/935</td>
</tr>
<tr>
<td align="center">E1</td>
<td align="center">Endoplasmic Reticulum</td>
<td align="center">1.28E-8</td>
<td align="center">28/117</td>
</tr>
<tr>
<td align="center">E2</td>
<td align="center">Endoplasmic Reticulum Membrane</td>
<td align="center">1.72E-10</td>
<td align="center">17/38</td>
</tr>
<tr>
<td align="center">E3</td>
<td align="center">Ribonucleoprotein Complex</td>
<td align="center">7.01E-15</td>
<td align="center">26/54</td>
</tr>
<tr>
<td align="center" rowspan="2">F</td>
<td align="center">Plasma Membrane</td>
<td align="center">7.83E-19</td>
<td align="center">68/115</td>
</tr>
<tr>
<td align="center">Adherens Junction</td>
<td align="center">3.39E-16</td>
<td align="center">30/115</td>
</tr>
<tr>
<td align="center">G</td>
<td align="center">Plasma Membrane</td>
<td align="center">1.09E-11</td>
<td align="center">230/781</td>
</tr>
<tr>
<td align="center">H</td>
<td align="center">Microtubule Cytoskeleton</td>
<td align="center">4.14E-18</td>
<td align="center">24/152</td>
</tr>
<tr>
<td align="center">I</td>
<td align="center">Vesicle</td>
<td align="center">4.94E-5</td>
<td align="center">115/439</td>
</tr>
<tr>
<td align="center">J</td>
<td align="center">Mitochondrion</td>
<td align="center">1.09E-108</td>
<td align="center">230/564</td>
</tr>
<tr>
<td align="center">J1</td>
<td align="center">Mitochondrial Membrane</td>
<td align="center">1.19E-30</td>
<td align="center">51/88</td>
</tr>
<tr>
<td align="center">K</td>
<td align="center">Golgi Apparatus</td>
<td align="center">5.03E-5</td>
<td align="center">22/99</td>
</tr>
<tr>
<td align="center">L</td>
<td align="center">Membrane Part</td>
<td align="center">1.75E-59</td>
<td align="center">964/2170</td>
</tr>
<tr>
<td align="center">L1</td>
<td align="center">Mitochondrial Part</td>
<td align="center">2.16E-14</td>
<td align="center">31/82</td>
</tr>
<tr>
<td align="center">L2</td>
<td align="center">Golgi Membrane</td>
<td align="center">1.12E-4</td>
<td align="center">18/114</td>
</tr>
<tr>
<td align="center">L3</td>
<td align="center">Extracellular Region</td>
<td align="center">1.32E-7</td>
<td align="center">42/226</td>
</tr>
<tr>
<td align="center">M</td>
<td align="center">Nucleus</td>
<td align="center">5.63E-43</td>
<td align="center">1190/2480</td>
</tr>
<tr>
<td align="center">M1</td>
<td align="center">Nuclear Body</td>
<td align="center">4.17E-8</td>
<td align="center">26/97</td>
</tr>
<tr>
<td align="center">M2</td>
<td align="center">Nuclear Speck</td>
<td align="center">9.86E-19</td>
<td align="center">26/72</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>To assess the ability of our clustering to find rare localization patterns, we asked if we could identify proteins localized to the nucleolar rim. Out of 226,732 images in the Human Protein Atlas, Sullivan <italic>et al</italic>. only classify 401, or 0.18%, as nucleolar rim [<xref ref-type="bibr" rid="pcbi.1007348.ref020">20</xref>]; based on this proportion, we estimated that ~20 proteins in our clustering solution would localize in this manner. The version of the Human Protein Atlas used in this study did not incorporate the nucleolar rim annotation, but the documentation gives one example, MKI67 [<xref ref-type="bibr" rid="pcbi.1007348.ref024">24</xref>]. We found MKI67 in Cluster D1. Of the 32 proteins closest to MKI67 in our hierarchical clustering, we identified 13 other proteins with an unambiguous nucleolar rim localization (we list these proteins in <xref ref-type="supplementary-material" rid="pcbi.1007348.s008">S1 Data</xref> and provide links to their images in the Human Protein Atlas), although there may also be others too ambiguous to determine by eye. We show three of these in <xref ref-type="fig" rid="pcbi.1007348.g004">Fig 4</xref>, NCL (<xref ref-type="fig" rid="pcbi.1007348.g004">Fig 4b</xref>), EN1 (<xref ref-type="fig" rid="pcbi.1007348.g004">Fig 4c</xref>), and ETV4 (<xref ref-type="fig" rid="pcbi.1007348.g004">Fig 4d</xref>). This supports the idea that unsupervised analysis in the feature space can identify rare patterns for which training classifiers would be difficult.</p>
<p>We observed several clusters of nuclear speck proteins, including clusters A1, A2, and M2. We decided to evaluate images for proteins in these clusters to determine if there is any distinction in their phenotype. We observed that proteins in clusters A1 and A2 tended to localize purely to nuclear specks (<xref ref-type="fig" rid="pcbi.1007348.g004">Fig 4a</xref>), whereas proteins in cluster M2 tended to localize to both nuclear specks and the cytosol (<xref ref-type="fig" rid="pcbi.1007348.g004">Fig 4e</xref>). Consistent with this interpretation, we found a strong enrichment for the spliceosomal complex for cluster A1, but no enrichment for cluster M2. To further test the possibility that the unsupervised analysis in the feature space reveals higher resolution functional information than expert annotation, we asked if our features could be used to discover higher-resolution subclasses of proteins that are difficult for experts to annotate visually. First, previous work with vesicle-localized proteins in the Human Protein Atlas has suggested that these proteins can be distinguished into many subclasses [<xref ref-type="bibr" rid="pcbi.1007348.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref051">51</xref>]. We clustered features for human proteins annotated to localize to the vesicle only, and found structure in the feature representation that corresponds to visually-distinct vesicle patterns (Abundantly and more evenly distributed in the cytosol, concentrated closer to the nucleus or sparse puncta, <xref ref-type="supplementary-material" rid="pcbi.1007348.s004">S4 Fig</xref>, Clusters A, B and C respectively). However, we were unable to associate these with any biological functions, so it is unclear if these distinctions in the patterns of vesicle distribution are functional. Next, we used features obtained from paired cell inpainting to organize proteins labelled as “punctate” in yeast dataset of images of a NOP1pr-GFP library [<xref ref-type="bibr" rid="pcbi.1007348.ref047">47</xref>] (see <xref ref-type="sec" rid="sec002">Methods</xref>). We find that we can distinguish Golgi, peroxisome and another cluster that contained singular or sparse foci in the cells (<xref ref-type="supplementary-material" rid="pcbi.1007348.s005">S5 Fig</xref>, Cluster B, C and A respectively). These analyses support the idea that unsupervised analysis of our features can reveal higher biological resolution than visual inspection of images.</p>
</sec>
<sec id="sec015">
<title>Paired cell inpainting features enable quantitative analysis of multi-localizing and spatially variable proteins</title>
<p>Next, we asked if features from our models could be used for the analysis of single cells of multiply-localized proteins. While multiply-localized proteins are critical to study as they are often hubs for protein-protein interactions [<xref ref-type="bibr" rid="pcbi.1007348.ref052">52</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref053">53</xref>], these proteins have posed a challenge for imaging methods. Most supervised methods remove multiply-localized proteins from their training and evaluation sets [<xref ref-type="bibr" rid="pcbi.1007348.ref006">6</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref041">41</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref054">54</xref>], focusing on only proteins localized to a single subcellular compartment. In supervised efforts, Sullivan <italic>et al</italic>. propose training with multi-label data [<xref ref-type="bibr" rid="pcbi.1007348.ref020">20</xref>], but curating this training data is challenging due to sparsity in some combinatorial classes and difficulty in manually annotating complex mixtures of patterns. In unsupervised work, unmixing algorithms [<xref ref-type="bibr" rid="pcbi.1007348.ref055">55</xref>] and generative models [<xref ref-type="bibr" rid="pcbi.1007348.ref056">56</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref057">57</xref>] have been proposed, but these methods generally rely on elaborate user-defined models of the conditional dependencies between different compartments. Thus, extending models to multiply-localizing proteins is non-trivial and an ongoing area of research.</p>
<p>We applied our features to analyze human single cells in images annotated to localize to the cytosol-and-nucleoplasm and to the nucleus-and-nucleoli by the Human Protein Atlas. Using the single-cell features from Conv3 of our human model, we measured a simple score based on distances in our feature space (see <xref ref-type="sec" rid="sec002">Methods</xref>). For our cytosol-and-nucleoplasm analysis, a negative score indicates that the cell is closer to the average cytosol-only cell than the average nucleoplasm-only cell in the feature space, while a positive score indicates that the cell is closer to the average nucleoplasm-only cell than the average cytosol-only cell. For our nucleoli-and-nucleus analysis, a negative score indicates that the cell is closer to the average nucleoli cell than the average nucleus cell, while a positive score indicates that the cell is closer to the average nucleus cell than the average nucleoli cell. We calculated this score for every single-cell localized to both compartments, or to either compartment alone (<xref ref-type="fig" rid="pcbi.1007348.g005">Fig 5</xref>).</p>
<fig id="pcbi.1007348.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007348.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Single-cell scores (as explained in the text) for cytosolic-and-nucleoplasm cells (A), and nucleoli-and-nucleus cells (B).</title>
<p>We visualize single cells as violin plots. The black boxes inside the violin plots show the interquartile ranges and the white dots show the medians. To the right of the violin plots, we show all of the single-cells for four different images of proteins annotated to be multi-localizing to either the cytosol-or-nucleoplasm or the nucleolus-and-nucleus as black dots, as labeled in the x-axis. For each image, we show either a representative crop of the image, or multiple single cell crops; in both cases, the protein is shown in green, while the microtubule is shown as red. Note that while the violin plots include all cell-lines, we show four examples from the same cell-line for each plot (the U-251 MG cell line for cytosol-nucleoplasm and the A-431 cell line for nucleoli-nucleus), to demonstrate that images from the same cell-line can have different scores depending on their phenotype.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.g005" xlink:type="simple"/>
</fig>
<p>We observe that single-cells with a cytosol-only annotation have a more positive score, while single-cells with a nucleus-only annotation have a more negative score (<xref ref-type="fig" rid="pcbi.1007348.g005">Fig 5A</xref>). In contrast, single-cells annotated to localize to both compartments have a score closer to 0. We observed that the skew of multiply-localized single-cells towards a positive or negative score reflects the proportion of protein localized to the two compartments: for example, for single cells stained for the spatially-variable protein LYPD5 in the U-251 MG cell line, we observed that positive scores corresponded to a more dominant nuclear localization, negative scores corresponded to a more dominant cytosol localization, and scores closer to 0 corresponded to a more even mix of both localizations (<xref ref-type="fig" rid="pcbi.1007348.g005">Fig 5A</xref>). We made similar observations for nucleoli-and-nucleus single-cells (<xref ref-type="fig" rid="pcbi.1007348.g005">Fig 5B</xref>). However, we also noticed that while most of the single-cells with a nucleoli-only annotation had negative scores, some had more positive scores, suggesting that the nucleoli-and-nucleus images are often annotated as nucleoli-only. These results suggest that simple distances in our feature space could be used to not only identify multiply-localized proteins, but also quantitate the relative distribution of protein between its compartments. We have included the mean scores for every cytosol-and-nucleoplasm and nucleoli-and-nucleus protein by cell line as <xref ref-type="supplementary-material" rid="pcbi.1007348.s009">S2 Data</xref>.</p>
<p>We observed that proteins with spatial-variability sometimes had different distributions over our scores compared to more homogeneously-localized proteins (<xref ref-type="fig" rid="pcbi.1007348.g005">Fig 5</xref>). This observation suggested that the standard deviation could be useful in discovering proteins with single-cell variability. We looked at the 10 images with the highest standard deviations for cytosol-and-nucleoplasm single-cells, and for nucleoli-and-nucleus single-cells. For cytosol-and-nucleoplasm single-cells, 6/10 are previously annotated by the Human Protein Atlas to have single-cell variability, and we observed unambiguous, but previously unannotated, single-cell variability in the images for the remaining 4. For nucleus-and-nucleoli single-cells, 5/10 are previously annotated, and we observed unambiguous and previously unannotated single-cell variability for the remaining 5 (we show one example of a previously unannotated image in <xref ref-type="fig" rid="pcbi.1007348.g005">Fig 5B</xref>, for MYL5 in the A-431 cell line). While this simple metric would be less sensitive to variable proteins where the single-cells are imbalanced in their distribution of the different phenotypes, these results suggest that simple statistics in our feature space can identify at least some kinds of single-cell variability. We include the standard deviation of scores for every cytosol-and-nucleoplasm and nucleoli-and-nucleus protein by cell line in <xref ref-type="supplementary-material" rid="pcbi.1007348.s009">S2 Data</xref>.</p>
</sec>
<sec id="sec016">
<title>Clustering of single cells organizes spatial variability in proteins</title>
<p>To organize single-cell variability, without prior knowledge of the specific compartments the protein is localized to, we decided to analyze single-cells using clustering in the feature space. We selected three proteins with spatial variability at the single-cell level: SMPDL3A, which localizes to the nucleoli and mitochondria (<xref ref-type="fig" rid="pcbi.1007348.g006">Fig 6A</xref>); DECR1, which localizes to the cytosol and mitochondria (<xref ref-type="fig" rid="pcbi.1007348.g006">Fig 6B</xref>); and NEK1, which localizes to the nucleoli and nuclear membrane (<xref ref-type="fig" rid="pcbi.1007348.g006">Fig 6C</xref>). We clustered the single cells of each protein using hierarchical agglomerative clustering. <xref ref-type="fig" rid="pcbi.1007348.g006">Fig 6</xref> shows the dendograms of the clustering solution for each protein, as well as the single-cell image crops associated with representative branches of the dendogram.</p>
<fig id="pcbi.1007348.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007348.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Dendograms from clustering the single-cell paired cell inpainting features of three spatially variable proteins: (A) SMPDL3A, (B) DECR1, (C) NEK1.</title>
<p>For each dendogram, we show single-cell image crops associated with representative branches of the dendogram. For (A) SMPDL3A, we also show a field of view of the image, and where various single-cell crops originate from in the full image (pink boxes). We label clades in the dendogram (grey boxes) with the apparent localization of the single cells.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.g006" xlink:type="simple"/>
</fig>
<p>In general, we observe distinct clusters of single cells for spatially variable proteins, visible as distinct clades in the dendograms (<xref ref-type="fig" rid="pcbi.1007348.g006">Fig 6</xref>). For SMPDL3A, we observe clusters of mitochondria-localized and of nucleoli-localized single cells, and for DECR1, we observe clusters of mitochondria-localized and cytosol-and-mitochondria-localized single cells, consistent with their labels in the Human Protein Atlas. For NEK1, we observed distinct clusters for the nuclear membrane and nucleoli, consistent with its label, but also for single-cells of a third phenotype, which appeared as fine and diffuse clumps of protein within the nucleus, not recorded in the Human Protein Atlas [<xref ref-type="bibr" rid="pcbi.1007348.ref024">24</xref>]. We hypothesize this may be due to the challenges in manually annotating a visually complex image with a heterogenous mixture of cells. These results suggest that single-cell clustering in the feature space can organize the phenotypes in a heterogeneous image and quantify the relative proportion of each phenotype.</p>
</sec>
</sec>
<sec id="sec017" sec-type="conclusions">
<title>Discussion</title>
<p>We present a simple, but effective self-supervised learning task for learning feature representations of single cells in fluorescent microscopy images. Features learned through paired cell inpainting easily achieve state-of-the-art performance in unsupervised benchmarks for both yeast and human cells. We believe that this effectiveness is because our pretext training task controls for nuisance variation. Because our task requires the network to predict the phenotype in a different cell, the network must ignore cell-specific variation to learn more general features about the phenotype. As we show the network the target cell structural markers during training, the network is directly given experimental effects like local illumination, meaning that it does not have to learn features for these in the source cell encoder.</p>
<p>A key advantage to self-supervised methods is that they learn features directly from data. Our method exploits the natural structure of many imaging experiments and can be applied to learn single-cell representations for virtually any multi-channel microscopy image dataset. To demonstrate the generality of paired cell inpainting, we learned representations for four independent image datasets in this study. Two of the yeast datasets have previously only been analyzed through laborious manual inspection of thousands of images [<xref ref-type="bibr" rid="pcbi.1007348.ref038">38</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref047">47</xref>]. We believe this is likely due to a lack of generality in previous automated methods: both of the classic computer vision methods we applied on the CyCLOPS dataset rely on accurate single-cell segmentation [<xref ref-type="bibr" rid="pcbi.1007348.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref058">58</xref>] which is challenging on the NOP1-ORF library due to highly overlapping and clumped cells, and impossible for the Nup49-RFP GFP-ORF dataset due to a lack of any cytoplasmic indicator. Similarly, neither dataset has a labeled single-cell dataset associated with it, preventing the application of supervised methods until this laborious work is complete. These datasets demonstrate important points about the context our proposed method operates in: while we show we can achieve performance gains on well-studied datasets like the CyCLOPS dataset, many biological imaging datasets are not as amenable for automated analysis. We believe that for these kinds of datasets, our method can produce similarly discriminative representations, at least on the protein level (<xref ref-type="fig" rid="pcbi.1007348.g003">Fig 3</xref>), enabling the fully automated analysis of these datasets without accurate segmentation or expert image labeling.</p>
<p>For this reason, we believe that self-supervised learning is an effective way to handle the increasing volume and diversity of image datasets as high-content imaging continues to accelerate, especially with the development of scalable tagging technologies and new libraries [<xref ref-type="bibr" rid="pcbi.1007348.ref047">47</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref059">59</xref>]. To address this demand, other self-supervised methods are also emerging: Caicedo <italic>et al</italic>. propose training a CNN to classify cells by their drug treatment [<xref ref-type="bibr" rid="pcbi.1007348.ref019">19</xref>]. We note that in contrast to their proposed proxy task, our task is predictive, rather than discriminative, and has some technical advantages in theory: since we do not force the CNN to discriminate between experiments that yield identical phenotypes, we expect that our method is less susceptible to overfitting on experimental variation. In practice, both methods work well on for their respective benchmarked applications. The choice of proxy task may be therefore be constrained by practical considerations: for example, our task cannot be applied on single-channel images, or images where cell centers cannot be automatically identified. While self-supervised learning methods for biological images are still emerging, in the future, a comparative analysis of the performance of different methods on different applications would be highly informative to researchers deciding which proxy task to apply to their own data.</p>
<p>While the quality of the features we learned were generally robust to different parameters and preprocessing operations on the dataset, outperforming other unsupervised methods regardless of settings, we found several factors that we believe could improve the features. First, filtering proteins that have single-cell variability in their protein expression, for example, by using labels in the Human Protein Atlas [<xref ref-type="bibr" rid="pcbi.1007348.ref024">24</xref>]. Likely, this would reduce amount of noise caused by pairing single cells with different protein expression during training. Second, while our method does not rely on accurate single-cell segmentations [<xref ref-type="bibr" rid="pcbi.1007348.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref040">40</xref>], it uses a crop centered around single cells in the image. We found that the method we used to detect cell centers for cropping was important, with classic segmentation techniques based on watershedding [<xref ref-type="bibr" rid="pcbi.1007348.ref001">1</xref>] resulting in poorer features than the methods used here (see <xref ref-type="sec" rid="sec002">Methods</xref>). We hypothesize that this is because artifacts included as cells increase noise during training when a cell gets paired with an artifact.</p>
<p>Here, we focused on the proxy task. Like most previous self-supervised work, we used a simple AlexNet architecture [<xref ref-type="bibr" rid="pcbi.1007348.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1007348.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref029">29</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref030">30</xref>]. However, future optimizations to the architecture will likely improve the applicability and performance of our method. A major limitation of self-supervised methods that use the AlexNet architecture is that the layer to extract features with must be determined. For most proxy tasks, the best performing-layer is an intermediate convolutional layer in AlexNet [<xref ref-type="bibr" rid="pcbi.1007348.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1007348.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref029">29</xref>,<xref ref-type="bibr" rid="pcbi.1007348.ref030">30</xref>]. However, recent work optimizing architectures for self-supervised methods suggests that architectures with skip-connections can prevent the quality of self-supervised representations from degrading in later layers [<xref ref-type="bibr" rid="pcbi.1007348.ref060">60</xref>]. Applying these insights to our method might mean that the final layer could always be chosen for transfer to new tasks, improving the practicality of the method.</p>
<p>We note that some technical points specific to our method may also improve performance. First, some proteins localize in a pattern that cannot be predicted deterministically from the structural markers of the cell alone. The inability of the network to accurately reconstruct these proteins may limit its feature learning. Future work in loss functions that encourage more realistic outputs, such as adversarial losses, could improve this issue. Second, for our experiments, we iterated over all cells in our training dataset and sampled pairs randomly. A more sophisticated pair sampling scheme may improve the quality of the features: for example, focusing on harder pairs where the morphology of the cells differs more drastically. There are also some protein localization patterns correlated with the cell cycle, such as the mitotic spindle. Due to their low penetrance in an image (for example, only a fraction of the cells will be undergoing mitosis), we do not expect our model to learn these patterns very well. Even if these cells are paired correctly, there may only be one or two cells with the specific cell cycle stage required to exhibit the pattern in the entire image. Better datasets with more fields of view, or work in finding an unsupervised method for balancing the patterns in the data, may improve this issue.</p>
<p>In the final aspect of this work, we performed, to our knowledge, the most comprehensive unsupervised analysis of protein localization spanning the entire Human Protein Atlas to date. Despite pooling all cell lines and antibodies together for proteins, we capture enrichments for high-resolution cellular components in our cluster analysis. Moreover, we showed that we could identify rare phenotypes, and use our features to discover distinct subclasses of proteins unsupervised. These results emphasize the importance of unbiased analysis, in contrast to supervised approaches. Rare phenotypes may be excluded from classifiers due to lack of training data, or lack of knowledge of which images to mine training data from. We discovered several proteins with a nucleolar rim localization, based upon the one example given by the Human Protein Atlas. Similarly, pre-defined classes may hide functionally-important variability within classes, and may be biased by human pre-conceptions of the data. We showed that coarse human-annotated classes can be clustered into distinct subclasses, and that functional protein classes such as the preribosome or splicesomal complex are clustered in protein localization data. Overall, these results demonstrate the power of unsupervised analysis in discovering new and unexpected biology, a task that supervised methods are fundamentally unable to perform, no matter how accurate they are at annotating pre-defined human knowledge.</p>
</sec>
<sec id="sec018">
<title>Supporting information</title>
<supplementary-material id="pcbi.1007348.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>End-to-end paired cell inpainting results for pairs of cells unseen during training.</title>
<p>We take four source human cells with different localizations (left-most boxes), belonging in the cytosol, nuclear membrane, nucleoli, and nucleoplasm, from top to bottom. For each of our source cells, we run our human cell model end-to-end with target markers from different images (left-center boxes) as input to generate predictions of the protein localization in these target cells (right-center boxes). We show the actual protein localization of the target cells (right-most boxes) to demonstrate that our model is capable of synthesizing results based on the source cells even when there is a mismatch between the source and target cell protein localization.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007348.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Classification accuracies for representations extracted over each convolutional layer of a VGG16 model pretrained on ImageNet, for different input strategies.</title>
<p>Images in the yeast single cell classification dataset were inputted using three different strategies: for “Concatenated”, we inputted the RFP and GFP channels independently as greyscale images and concatenated the representations; for “GFP-Only”, we inputted the GFP channel as a greyscale image and used this representation alone; and for “RGB Image”, we arbitrarily mapped channels to RGB channels (RFP to red, GFP to green, and blue left empty). Feature representations were extracted by maximum pooling the feature maps over spatial dimensions. We report the balanced classification accuracy using a leave-one-out kNN classifier (<italic>k</italic> = 11) for these representations, identical to the one described in the “Paired cell inpainting features discriminate protein subcellular localization in yeast single cells” section of the Results.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007348.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>UMAP representations of various features, as labeled above each scatterplot, for our labeled single yeast cell benchmark dataset.</title>
<p>All UMAPs are generated with the same parameters (Euclidean distance, 30 neighbors, minimum distance of 0.3). Embedded points are visualized as a scatterplot and are colored according to their label, as shown in the legend to the right.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007348.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Averaged paired cell inpainting features for vesicle-only proteins in the Human Protein Atlas, using features from Conv3 of our human model trained on the Human Protein Atlas dataset, ordered using maximum likelihood agglomerative hierarchical clustering.</title>
<p>We visualize features as a heat map, where positive values are colored yellow and negative values are colored blue, with the intensity of the color corresponding to magnitude. Columns in this heat map are features, while rows are proteins. Features have been mean-centered and normalized using all proteins in the dataset. We show three clusters (black and grey bars on the right), and crops of three representative images of the proteins within each of the clusters. For image crops, we show the protein channel in green, and the nucleus channel in blue.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007348.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Averaged paired cell inpainting features from Conv4 of our yeast model trained on the NOP1pr-GFP dataset, for proteins labeled as punctuate in the NOP1pr-GFP library images, ordered using maximum likelihood agglomerative hierarchical clustering.</title>
<p>We visualize features as a heat map, where positive values are colored yellow and negative values are colored blue, with the intensity of the color corresponding to magnitude. Columns in this heat map are features, while rows are proteins. Features have been mean-centered and normalized using all proteins in the dataset. We show three clusters (black and grey bars on the right), and crops of three representative images of the proteins within each of the clusters. For image crops, we show the protein channel only in green. For clusters B and C, we show the GO enrichment of the clusters relative to all punctate proteins; we list the q-value (the FDR-corrected p-value) and the number of proteins in the cluster with that annotation relative to the full size of the cluster.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007348.s006" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.s006" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Classification accuracies for feature sets with various parameterizations of <italic>k</italic>.</title>
<p>Classification accuracies for single yeast cell localization classes using a <italic>k</italic>NN classifier on our test set of 30,889 labeled single cells, using various feature representations. We report the overall accuracy as the balanced accuracy of all classes.</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007348.s007" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.s007" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Classification accuracies for feature sets with logistic regression and random forest classifiers.</title>
<p>Classification accuracies for single yeast cell localization classes using logistic regression and random forest classifiers on our dataset of 30,889 labeled single cells, built on various feature representations. We also report accuracy of the classifier end-to-end for the fully-supervised CNN in the last row of the table. Metrics are reported as the average balanced accuracy on the test sets under 5-fold cross-validation. We implemented all classifiers in Python using the scikit-learn package. For our logistic regression classifiers, we used a L1 penalty with a balanced class weight. For our random forest classifiers, we used 500 trees with 20% of the features used to determine best splits, and a balanced class weight.</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007348.s008" mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.s008" xlink:type="simple">
<label>S1 Data</label>
<caption>
<title>Full lists of the proteins in each cluster.</title>
<p>(XLSX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007348.s009" mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007348.s009" xlink:type="simple">
<label>S2 Data</label>
<caption>
<title>Multi-localization scores for every cytosol-and-nucleoplasm and nucleoli-and-nucleus protein by cell line.</title>
<p>(XLSX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Shadi Zabad, Taraneh Zarin, Nirvana Nursimulu, and Purnima Kompella for valuable comments on the manuscript. We thank Brenda Andrews and Helena Friesen for help in accessing the CyCLOPS data. We thank Maya Schuldiner, Uri Weill, and Amir Fadel for help in accessing the NOP1pr-ORF yeast collection. We thank Grant Brown and Brandon Ho for help in accessing the Nup49-RFP GFP-ORF yeast collection.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1007348.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Handfield</surname> <given-names>L-F</given-names></name>, <name name-style="western"><surname>Chong</surname> <given-names>YT</given-names></name>, <name name-style="western"><surname>Simmons</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Andrews</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Moses</surname> <given-names>AM</given-names></name>. <article-title>Unsupervised clustering of subcellular protein expression patterns in high-throughput microscopy images reveals protein complexes and functional relationships between proteins</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>: <fpage>e1003085</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003085" xlink:type="simple">10.1371/journal.pcbi.1003085</ext-link></comment> <object-id pub-id-type="pmid">23785265</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Shariff</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rohde</surname> <given-names>GK</given-names></name>, <name name-style="western"><surname>Murphy</surname> <given-names>RF</given-names></name>. <article-title>Automated Learning of Subcellular Variation among Punctate Protein Patterns and a Generative Model of Their Relation to Microtubules</article-title>. <name name-style="western"><surname>Tresch</surname> <given-names>A</given-names></name>, editor. <source>PLOS Comput Biol</source>. <year>2015</year>;<volume>11</volume>: <fpage>e1004614</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004614" xlink:type="simple">10.1371/journal.pcbi.1004614</ext-link></comment> <object-id pub-id-type="pmid">26624011</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Caicedo</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Cooper</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Heigwer</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Warchal</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Qiu</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Molnar</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Data-analysis strategies for image-based cell profiling</article-title>. <source>Nat Methods</source>. <year>2017</year>;<volume>14</volume>: <fpage>849</fpage>–<lpage>863</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nmeth.4397" xlink:type="simple">10.1038/nmeth.4397</ext-link></comment> <object-id pub-id-type="pmid">28858338</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lu</surname> <given-names>AX</given-names></name>, <name name-style="western"><surname>Chong</surname> <given-names>YT</given-names></name>, <name name-style="western"><surname>Hsu</surname> <given-names>IS</given-names></name>, <name name-style="western"><surname>Strome</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Handfield</surname> <given-names>L-F</given-names></name>, <name name-style="western"><surname>Kraus</surname> <given-names>O</given-names></name>, <etal>et al</etal>. <article-title>Integrating images from multiple microscopy screens reveals diverse patterns of change in the subcellular localization of proteins</article-title>. <source>Elife</source>. <year>2018</year>;<volume>7</volume>: <fpage>e31872</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.31872" xlink:type="simple">10.7554/eLife.31872</ext-link></comment> <object-id pub-id-type="pmid">29620521</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Uchida</surname> <given-names>S</given-names></name>. <article-title>Image processing and recognition for biological images</article-title>. <source>Dev Growth Differ</source>. <year>2013</year>;<volume>55</volume>: <fpage>523</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/dgd.12054" xlink:type="simple">10.1111/dgd.12054</ext-link></comment> <object-id pub-id-type="pmid">23560739</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kraus</surname> <given-names>OZ</given-names></name>, <name name-style="western"><surname>Grys</surname> <given-names>BT</given-names></name>, <name name-style="western"><surname>Ba</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chong</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Frey</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Boone</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Automated analysis of high-content microscopy data with deep learning</article-title>. <source>Mol Syst Biol</source>. <year>2017</year>;<volume>13</volume>: <fpage>924</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.15252/msb.20177551" xlink:type="simple">10.15252/msb.20177551</ext-link></comment> <object-id pub-id-type="pmid">28420678</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eulenberg</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Köhler</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Blasi</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Filby</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Carpenter</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Rees</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Reconstructing cell cycle and disease progression using deep learning</article-title>. <source>Nat Commun</source>. <year>2017</year>;<volume>8</volume>: <fpage>463</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-017-00623-3" xlink:type="simple">10.1038/s41467-017-00623-3</ext-link></comment> <object-id pub-id-type="pmid">28878212</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dürr</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Sick</surname> <given-names>B</given-names></name>. <article-title>Single-Cell Phenotype Classification Using Deep Convolutional Neural Networks</article-title>. <source>J Biomol Screen</source>. <year>2016</year>;<volume>21</volume>: <fpage>998</fpage>–<lpage>1003</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/1087057116631284" xlink:type="simple">10.1177/1087057116631284</ext-link></comment> <object-id pub-id-type="pmid">26950929</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Godinez</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Hossain</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Lazic</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Davies</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>X</given-names></name>. <article-title>A multi-scale convolutional neural network for phenotyping high-content cellular images</article-title>. <source>Bioinformatics</source>. <year>2017</year>;<volume>33</volume>: <fpage>2010</fpage>–<lpage>2019</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/bioinformatics/btx069" xlink:type="simple">10.1093/bioinformatics/btx069</ext-link></comment> <object-id pub-id-type="pmid">28203779</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref010"><label>10</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Ronneberger</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Fischer</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Brox</surname> <given-names>T</given-names></name>. <chapter-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</chapter-title>. <source>Medical Image Computing and Computer-Assisted Intervention (MICCAI)</source>. <publisher-name>Springer</publisher-name>; <year>2015</year>. pp. <fpage>234</fpage>–<lpage>241</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007348.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kraus</surname> <given-names>OZ</given-names></name>, <name name-style="western"><surname>Ba</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Frey</surname> <given-names>BJ</given-names></name>. <article-title>Classifying and segmenting microscopy images with deep multiple instance learning</article-title>. <source>Bioinformatics</source>. <year>2016</year>;<volume>32</volume>: <fpage>i52</fpage>–<lpage>i59</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/bioinformatics/btw252" xlink:type="simple">10.1093/bioinformatics/btw252</ext-link></comment> <object-id pub-id-type="pmid">27307644</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>: <fpage>436</fpage>–<lpage>444</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14539" xlink:type="simple">10.1038/nature14539</ext-link></comment> <object-id pub-id-type="pmid">26017442</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref013"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Donahue J, Jia Y, Vinyals O, Hoffman J, Zhang N, Tzeng E, et al. DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. Proceedings of the 31st International Conference on Machine Learning. 2014. pp. 647–655.</mixed-citation></ref>
<ref id="pcbi.1007348.ref014"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Razavian AS, Azizpour H, Sullivan J, Carlsson S. CNN Features off-the-shelf: an Astounding Baseline for Recognition. CVPRW ‘14 Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2014. pp. 512–519.</mixed-citation></ref>
<ref id="pcbi.1007348.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pawlowski</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Caicedo</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Carpenter</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Storkey</surname> <given-names>A</given-names></name>. <article-title>Automating Morphological Profiling with Generic Deep Convolutional Networks</article-title>. <source>bioRxiv</source>. <year>2016</year>; 085118. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/085118" xlink:type="simple">10.1101/085118</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1007348.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ando</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>McLean</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Berndl</surname> <given-names>M</given-names></name>. <article-title>Improving Phenotypic Measurements in High-Content Imaging Screens</article-title>. <source>bioRxiv</source>. <year>2017</year>; 161422. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/161422" xlink:type="simple">10.1101/161422</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1007348.ref017"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Zhang R, Isola P, Efros AA. Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction. 2017 IEEE Conference on Computer Vision and Pattern Recognition. 2017.</mixed-citation></ref>
<ref id="pcbi.1007348.ref018"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Pathak D, Krahenbuhl P, Donahue J, Darrell T, Efros AA. Context Encoders: Feature Learning by Inpainting. CVPR 2016. 2016.</mixed-citation></ref>
<ref id="pcbi.1007348.ref019"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Caicedo JC, McQuin C, Goodman A, Singh S, Carpenter AE. Weakly Supervised Learning of Single-Cell Feature Embeddings. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE; 2018. pp. 9309–9318.</mixed-citation></ref>
<ref id="pcbi.1007348.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sullivan</surname> <given-names>DP</given-names></name>, <name name-style="western"><surname>Winsnes</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Åkesson</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Hjelmare</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wiking</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schutten</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Deep learning is combined with massive-scale citizen science to improve large-scale image classification</article-title>. <source>Nat Biotechnol</source>. <year>2018</year>;<volume>36</volume>: <fpage>820</fpage>–<lpage>828</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nbt.4225" xlink:type="simple">10.1038/nbt.4225</ext-link></comment> <object-id pub-id-type="pmid">30125267</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mattiazzi Usaj</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Styles</surname> <given-names>EB</given-names></name>, <name name-style="western"><surname>Verster</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Friesen</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Boone</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Andrews</surname> <given-names>BJ</given-names></name>. <article-title>High-Content Screening for Quantitative Cell Biology</article-title>. <source>Trends Cell Biol</source>. <year>2016</year>;<volume>26</volume>: <fpage>598</fpage>–<lpage>611</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tcb.2016.03.008" xlink:type="simple">10.1016/j.tcb.2016.03.008</ext-link></comment> <object-id pub-id-type="pmid">27118708</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pelkmans</surname> <given-names>L</given-names></name>. <article-title>Using Cell-to-Cell Variability—A New Era in Molecular Biology</article-title>. <source>Science (80-)</source>. <year>2012</year>;<volume>336</volume>: <fpage>425</fpage>–<lpage>426</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1222161" xlink:type="simple">10.1126/science.1222161</ext-link></comment> <object-id pub-id-type="pmid">22539709</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Snijder</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pelkmans</surname> <given-names>L</given-names></name>. <article-title>Origins of regulated cell-to-cell variability</article-title>. <source>Nat Rev Mol Cell Biol</source>. <year>2011</year>;<volume>12</volume>: <fpage>119</fpage>–<lpage>125</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrm3044" xlink:type="simple">10.1038/nrm3044</ext-link></comment> <object-id pub-id-type="pmid">21224886</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thul</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Åkesson</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Wiking</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Mahdessian</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Geladaki</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ait Blal</surname> <given-names>H</given-names></name>, <etal>et al</etal>. <article-title>A subcellular map of the human proteome</article-title>. <source>Science (80-)</source>. <year>2017</year>;<volume>356</volume>: eaal3321. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.aal3321" xlink:type="simple">10.1126/science.aal3321</ext-link></comment> <object-id pub-id-type="pmid">28495876</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dalal</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Cai</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Rahbar</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Elowitz</surname> <given-names>MB</given-names></name>. <article-title>Pulsatile dynamics in the yeast proteome</article-title>. <source>Curr Biol</source>. <year>2014</year>;<volume>24</volume>: <fpage>2189</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2014.07.076" xlink:type="simple">10.1016/j.cub.2014.07.076</ext-link></comment> <object-id pub-id-type="pmid">25220054</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Karanam</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Loewer</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lahav</surname> <given-names>G</given-names></name>. <article-title>Dynamics of the DNA damage response: insights from live-cell imaging</article-title>. <source>Brief Funct Genomics</source>. <year>2013</year>;<volume>12</volume>: <fpage>109</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/bfgp/els059" xlink:type="simple">10.1093/bfgp/els059</ext-link></comment> <object-id pub-id-type="pmid">23292635</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhou</surname> <given-names>Z-H</given-names></name>. <article-title>A brief introduction to weakly supervised learning</article-title>. <source>Natl Sci Rev</source>. <year>2018</year>;<volume>5</volume>: <fpage>44</fpage>–<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/nsr/nwx106" xlink:type="simple">10.1093/nsr/nwx106</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1007348.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>J-Y</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>EI-C</given-names></name>, <name name-style="western"><surname>Lai</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tu</surname> <given-names>Z</given-names></name>. <article-title>Weakly supervised histopathology cancer image segmentation and classification</article-title>. <source>Med Image Anal</source>. <year>2014</year>;<volume>18</volume>: <fpage>591</fpage>–<lpage>604</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.media.2014.01.010" xlink:type="simple">10.1016/j.media.2014.01.010</ext-link></comment> <object-id pub-id-type="pmid">24637156</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref029"><label>29</label><mixed-citation publication-type="other" xlink:type="simple">Jenni S, Favaro P. Self-Supervised Feature Learning by Learning to Spot Artifacts. CVPR 2018. 2018.</mixed-citation></ref>
<ref id="pcbi.1007348.ref030"><label>30</label><mixed-citation publication-type="other" xlink:type="simple">Gidaris S, Singh P, Komodakis N. Unsupervised Representation Learning by Predicting Image Rotations. ICLR 2018. 2018.</mixed-citation></ref>
<ref id="pcbi.1007348.ref031"><label>31</label><mixed-citation publication-type="other" xlink:type="simple">Kim KAIST D, Cho D, Yoo KAIST D. Learning Image Representations by Completing Damaged Jigsaw Puzzles. WACV 2018. 2018.</mixed-citation></ref>
<ref id="pcbi.1007348.ref032"><label>32</label><mixed-citation publication-type="other" xlink:type="simple">Mundhenk TN, Ho D, Chen BY. Improvements to context based self-supervised learning. CVPR 2018. 2018.</mixed-citation></ref>
<ref id="pcbi.1007348.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goodfellow</surname> <given-names>IJ</given-names></name>, <name name-style="western"><surname>Pouget-Abadie</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mirza</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Warde-Farley</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ozair</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <source>Generative Adversarial Networks</source>. <year>2014</year>;</mixed-citation></ref>
<ref id="pcbi.1007348.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Grima</surname> <given-names>R</given-names></name>. <article-title>Single-cell variability in multicellular organisms</article-title>. <source>Nat Commun</source>. <year>2018</year>;<volume>9</volume>: <fpage>345</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-017-02710-x" xlink:type="simple">10.1038/s41467-017-02710-x</ext-link></comment> <object-id pub-id-type="pmid">29367605</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Handfield</surname> <given-names>L-F</given-names></name>, <name name-style="western"><surname>Strome</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Chong</surname> <given-names>YT</given-names></name>, <name name-style="western"><surname>Moses</surname> <given-names>AM</given-names></name>. <article-title>Local statistics allow quantification of cell-to-cell variability from high-throughput microscope images</article-title>. <source>Bioinformatics</source>. <year>2015</year>;<volume>31</volume>: <fpage>940</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/bioinformatics/btu759" xlink:type="simple">10.1093/bioinformatics/btu759</ext-link></comment> <object-id pub-id-type="pmid">25398614</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koh</surname> <given-names>JLY</given-names></name>, <name name-style="western"><surname>Chong</surname> <given-names>YT</given-names></name>, <name name-style="western"><surname>Friesen</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Moses</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Boone</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Andrews</surname> <given-names>BJ</given-names></name>, <etal>et al</etal>. <article-title>CYCLoPs: A Comprehensive Database Constructed from Automated Analysis of Protein Abundance and Subcellular Localization Patterns in Saccharomyces cerevisiae</article-title>. <source>G3 (Bethesda)</source>. <year>2015</year>;<volume>5</volume>: <fpage>1223</fpage>–<lpage>32</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1534/g3.115.017830" xlink:type="simple">10.1534/g3.115.017830</ext-link></comment> <object-id pub-id-type="pmid">26048563</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lu</surname> <given-names>AX</given-names></name>, <name name-style="western"><surname>Zarin</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hsu</surname> <given-names>IS</given-names></name>, <name name-style="western"><surname>Moses</surname> <given-names>AM</given-names></name>. <article-title>YeastSpotter: Accurate and parameter-free web segmentation for microscopy images of yeast cells</article-title>. <name name-style="western"><surname>Murphy</surname> <given-names>R</given-names></name>, editor. <source>Bioinformatics</source>. <year>2019</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/bioinformatics/btz402" xlink:type="simple">10.1093/bioinformatics/btz402</ext-link></comment> <object-id pub-id-type="pmid">31095270</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tkach</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Yimit</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>AY</given-names></name>, <name name-style="western"><surname>Riffle</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Costanzo</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jaschob</surname> <given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Dissecting DNA damage response pathways by analysing protein localization and abundance changes during DNA replication stress</article-title>. <source>Nat Cell Biol</source>. <year>2012</year>;<volume>14</volume>: <fpage>966</fpage>–<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncb2549" xlink:type="simple">10.1038/ncb2549</ext-link></comment> <object-id pub-id-type="pmid">22842922</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van der Walt</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schönberger</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Nunez-Iglesias</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Boulogne</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Warner</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Yager</surname> <given-names>N</given-names></name>, <etal>et al</etal>. <article-title>scikit-image: image processing in Python</article-title>. <source>PeerJ</source>. <year>2014</year>;<volume>2</volume>: <fpage>e453</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7717/peerj.453" xlink:type="simple">10.7717/peerj.453</ext-link></comment> <object-id pub-id-type="pmid">25024921</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carpenter</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>TR</given-names></name>, <name name-style="western"><surname>Lamprecht</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Clarke</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Kang</surname> <given-names>IH</given-names></name>, <name name-style="western"><surname>Friman</surname> <given-names>O</given-names></name>, <etal>et al</etal>. <article-title>CellProfiler: image analysis software for identifying and quantifying cell phenotypes</article-title>. <source>Genome Biol</source>. <year>2006</year>;<volume>7</volume>: <fpage>R100</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/gb-2006-7-10-r100" xlink:type="simple">10.1186/gb-2006-7-10-r100</ext-link></comment> <object-id pub-id-type="pmid">17076895</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chong</surname> <given-names>YT</given-names></name>, <name name-style="western"><surname>Koh</surname> <given-names>JLY</given-names></name>, <name name-style="western"><surname>Friesen</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Kaluarachchi Duffy</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cox</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Moses</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Yeast Proteome Dynamics from Single Cell Imaging and Automated Analysis</article-title>. <source>Cell</source>. <year>2015</year>;<volume>161</volume>: <fpage>1413</fpage>–<lpage>1424</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cell.2015.04.051" xlink:type="simple">10.1016/j.cell.2015.04.051</ext-link></comment> <object-id pub-id-type="pmid">26046442</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ljosa</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Caie</surname> <given-names>PD</given-names></name>, <name name-style="western"><surname>Ter Horst</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sokolnicki</surname> <given-names>KL</given-names></name>, <name name-style="western"><surname>Jenkins</surname> <given-names>EL</given-names></name>, <name name-style="western"><surname>Daya</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Comparison of methods for image-based profiling of cellular morphological responses to small-molecule treatment</article-title>. <source>J Biomol Screen</source>. <year>2013</year>;<volume>18</volume>: <fpage>1321</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/1087057113503553" xlink:type="simple">10.1177/1087057113503553</ext-link></comment> <object-id pub-id-type="pmid">24045582</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Singh</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bray</surname> <given-names>M-A</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>TR</given-names></name>, <name name-style="western"><surname>Carpenter</surname> <given-names>AE</given-names></name>. <article-title>Pipeline for illumination correction of images for high-throughput microscopy</article-title>. <source>J Microsc</source>. <year>2014</year>;<volume>256</volume>: <fpage>231</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/jmi.12178" xlink:type="simple">10.1111/jmi.12178</ext-link></comment> <object-id pub-id-type="pmid">25228240</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref044"><label>44</label><mixed-citation publication-type="other" xlink:type="simple">Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition. ILSVRC-2014. 2014.</mixed-citation></ref>
<ref id="pcbi.1007348.ref045"><label>45</label><mixed-citation publication-type="other" xlink:type="simple">Deng J, Dong W, Socher R, Li L-J, Kai Li, Li Fei-Fei. ImageNet: A large-scale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE; 2009. pp. 248–255.</mixed-citation></ref>
<ref id="pcbi.1007348.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McInnes</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Healy</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Melville</surname> <given-names>J</given-names></name>. <source>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</source>. <year>2018</year>;</mixed-citation></ref>
<ref id="pcbi.1007348.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Weill</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Yofe</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Sass</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Stynen</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Davidi</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Natarajan</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Genome-wide SWAp-Tag yeast libraries for proteome exploration</article-title>. <source>Nat Methods</source>. <year>2018</year>;<volume>15</volume>: <fpage>617</fpage>–<lpage>622</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41592-018-0044-9" xlink:type="simple">10.1038/s41592-018-0044-9</ext-link></comment> <object-id pub-id-type="pmid">29988094</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huh</surname> <given-names>W-K</given-names></name>, <name name-style="western"><surname>Falvo</surname> <given-names>JV</given-names></name>, <name name-style="western"><surname>Gerke</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>Carroll</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Howson</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Weissman</surname> <given-names>JS</given-names></name>, <etal>et al</etal>. <article-title>Global analysis of protein localization in budding yeast</article-title>. <source>Nature</source>. <year>2003</year>;<volume>425</volume>: <fpage>686</fpage>–<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature02026" xlink:type="simple">10.1038/nature02026</ext-link></comment> <object-id pub-id-type="pmid">14562095</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Newberg</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Uhlén</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lundberg</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Murphy</surname> <given-names>RF</given-names></name>. <article-title>Automated Analysis and Reannotation of Subcellular Locations in Confocal Images from the Human Protein Atlas</article-title>. <name name-style="western"><surname>Ranganathan</surname> <given-names>S</given-names></name>, editor. <source>PLoS One</source>. <year>2012</year>;<volume>7</volume>: <fpage>e50514</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0050514" xlink:type="simple">10.1371/journal.pone.0050514</ext-link></comment> <object-id pub-id-type="pmid">23226299</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eden</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Navon</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Steinfeld</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Lipson</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Yakhini</surname> <given-names>Z</given-names></name>. <article-title>GOrilla: a tool for discovery and visualization of enriched GO terms in ranked gene lists</article-title>. <source>BMC Bioinformatics</source>. <year>2009</year>;<volume>10</volume>: <fpage>48</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/1471-2105-10-48" xlink:type="simple">10.1186/1471-2105-10-48</ext-link></comment> <object-id pub-id-type="pmid">19192299</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Majarian</surname> <given-names>TD</given-names></name>, <name name-style="western"><surname>Naik</surname> <given-names>AW</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Murphy</surname> <given-names>RF</given-names></name>. <article-title>Point process models for localization and interdependence of punctate cellular structures</article-title>. <source>Cytom Part A</source>. <year>2016</year>;<volume>89</volume>: <fpage>633</fpage>–<lpage>643</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/cyto.a.22873" xlink:type="simple">10.1002/cyto.a.22873</ext-link></comment> <object-id pub-id-type="pmid">27327612</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ota</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gonja</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Koike</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Fukuchi</surname> <given-names>S</given-names></name>. <article-title>Multiple-Localization and Hub Proteins</article-title>. <source>PLoS One</source>. <year>2016</year>;<volume>11</volume>: <fpage>e0156455</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0156455" xlink:type="simple">10.1371/journal.pone.0156455</ext-link></comment> <object-id pub-id-type="pmid">27285823</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Xia</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Shen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Sun</surname> <given-names>Z</given-names></name>. <article-title>DBMLoc: a Database of proteins with multiple subcellular localizations</article-title>. <source>BMC Bioinformatics</source>. <year>2008</year>;<volume>9</volume>: <fpage>127</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/1471-2105-9-127" xlink:type="simple">10.1186/1471-2105-9-127</ext-link></comment> <object-id pub-id-type="pmid">18304364</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pärnamaa</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Parts</surname> <given-names>L</given-names></name>. <article-title>Accurate Classification of Protein Subcellular Localization from High-Throughput Microscopy Images Using Deep Learning</article-title>. <source>G3 (Bethesda)</source>. <year>2017</year>;<volume>7</volume>: <fpage>1385</fpage>–<lpage>1392</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1534/g3.116.033654" xlink:type="simple">10.1534/g3.116.033654</ext-link></comment> <object-id pub-id-type="pmid">28391243</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Coelho</surname> <given-names>LP</given-names></name>, <name name-style="western"><surname>Peng</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Murphy</surname> <given-names>RF</given-names></name>. <article-title>Quantifying the distribution of probes between subcellular locations using unsupervised pattern unmixing</article-title>. <source>Bioinformatics</source>. <year>2010</year>;<volume>26</volume>: <fpage>i7</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/bioinformatics/btq220" xlink:type="simple">10.1093/bioinformatics/btq220</ext-link></comment> <object-id pub-id-type="pmid">20529939</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Murphy</surname> <given-names>RF</given-names></name>. <article-title>Building cell models and simulations from microscope images</article-title>. <source>Methods</source>. <year>2016</year>;<volume>96</volume>: <fpage>33</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.ymeth.2015.10.011" xlink:type="simple">10.1016/j.ymeth.2015.10.011</ext-link></comment> <object-id pub-id-type="pmid">26484733</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Donovan-Maiye</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Maleckar</surname> <given-names>MM</given-names></name>. <source>Generative Modeling with Conditional Autoencoders: Building an Integrated Cell</source>. <year>2017</year>;</mixed-citation></ref>
<ref id="pcbi.1007348.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chong</surname> <given-names>YT</given-names></name>, <name name-style="western"><surname>Koh</surname> <given-names>JLY</given-names></name>, <name name-style="western"><surname>Friesen</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Duffy</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Cox</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Moses</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Yeast Proteome Dynamics from Single Cell Imaging and Automated Analysis</article-title>. <source>Cell</source>. <year>2015</year>;<volume>161</volume>: <fpage>1413</fpage>–<lpage>1424</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cell.2015.04.051" xlink:type="simple">10.1016/j.cell.2015.04.051</ext-link></comment> <object-id pub-id-type="pmid">26046442</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leonetti</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Sekine</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kamiyama</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Weissman</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>B</given-names></name>. <article-title>A scalable strategy for high-throughput GFP tagging of endogenous human proteins</article-title>. <source>Proc Natl Acad Sci</source>. <year>2016</year>;<volume>113</volume>: <fpage>E3501</fpage>–<lpage>E3508</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1606731113" xlink:type="simple">10.1073/pnas.1606731113</ext-link></comment> <object-id pub-id-type="pmid">27274053</object-id></mixed-citation></ref>
<ref id="pcbi.1007348.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kolesnikov</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Zhai</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Beyer</surname> <given-names>L</given-names></name>. <source>Revisiting Self-Supervised Visual Representation Learning</source>. <year>2019</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>