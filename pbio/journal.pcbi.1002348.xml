<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-00365</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002348</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Maximization of Learning Speed in the Motor Cortex Due to Neuronal Redundancy</article-title><alt-title alt-title-type="running-head">Neuronal Redundancy Maximizes Learning Speed</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Takiyama</surname>
            <given-names>Ken</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Okada</surname>
            <given-names>Masato</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Graduate School of Frontier Sciences, The University of Tokyo, Complex Science and Engineering, Chiba, Japan</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>RIKEN Brain Science Institute, Wako, Japan</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Diedrichsen</surname>
            <given-names>Jörn</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">okada@k.u-tokyo.ac.jp</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: KT. Performed the experiments: KT. Analyzed the data: KT. Contributed reagents/materials/analysis tools: KT MO. Wrote the paper: KT MO.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>1</month>
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>12</day>
        <month>1</month>
        <year>2012</year>
      </pub-date><volume>8</volume><issue>1</issue><elocation-id>e1002348</elocation-id><history>
        <date date-type="received">
          <day>16</day>
          <month>3</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>26</day>
          <month>11</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2012</copyright-year><copyright-holder>Takiyama, Okada</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>Many redundancies play functional roles in motor control and motor learning. For example, kinematic and muscle redundancies contribute to stabilizing posture and impedance control, respectively. Another redundancy is the number of neurons themselves; there are overwhelmingly more neurons than muscles, and many combinations of neural activation can generate identical muscle activity. The functional roles of this neuronal redundancy remains unknown. Analysis of a redundant neural network model makes it possible to investigate these functional roles while varying the number of model neurons and holding constant the number of output units. Our analysis reveals that learning speed reaches its maximum value if and only if the model includes sufficient neuronal redundancy. This analytical result does not depend on whether the distribution of the preferred direction is uniform or a skewed bimodal, both of which have been reported in neurophysiological studies. Neuronal redundancy maximizes learning speed, even if the neural network model includes recurrent connections, a nonlinear activation function, or nonlinear muscle units. Furthermore, our results do not rely on the shape of the generalization function. The results of this study suggest that one of the functional roles of neuronal redundancy is to maximize learning speed.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>There are overwhelmingly more neurons than muscles in the motor system. The functional roles of this neuronal redundancy remains unknown. Our analysis, which uses a redundant neural network model, reveals that learning speed reaches its maximum value if and only if the model includes sufficient neuronal redundancy. This result does not depend on whether the distribution of the preferred direction is uniform or a skewed bimodal, both of which have been reported in neurophysiological studies. We have confirmed that our results are consistent, regardless of whether the model includes recurrent connections, a nonlinear activation function, or nonlinear muscle units. Additionally, our results are the same when using either a broad or a narrow generalization function. These results suggest that one of the functional roles of neuronal redundancy is to maximize learning speed.</p>
      </abstract><funding-group><funding-statement>This work was partially supported by a Grant-in-Aid for Scientific Research (A) (Grant No. 20240020) and a Grant-in-Aid for Special Purposes (Grant No. 10J04910) from the Ministry of Education, Culture, Sports, Science and Technology of Japan. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="12"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>In the human brain, numerous neurons encode information about external stimuli, e.g., visual or auditory stimuli, and internal stimuli, e.g., attention or motor planning. Each neuron exhibits different responses to stimuli, but neural encoding, especially in the visual and auditory cortices, can be explained by the maximization of stimulus information <xref ref-type="bibr" rid="pcbi.1002348-Barlow1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1002348-Lewicki1">[3]</xref>. This maximization framework can also explain learning that occurs when the same stimuli are repeatedly presented; previous neurophysiological experiments have suggested that perceptual learning causes changes in neural encoding to enhance the Fisher information of a visual stimulus <xref ref-type="bibr" rid="pcbi.1002348-Gutnisky1">[4]</xref>. However, a recent study has suggested that information maximization alone is insufficient to explain neural encoding. Salinas has suggested that “how encoded information is used” needs to be taken into account: neural encoding is influenced by the downstream circuits and output units to which neurons project, and it is ultimately influenced by animal behavior <xref ref-type="bibr" rid="pcbi.1002348-Salinas1">[5]</xref>. In the motor cortex, neural encoding is influenced by the characteristics of muscles (output units) because motor cortex neurons send motor commands to muscles through the spinal cord. In adaptation experiments, some motor cortex neurons exhibit rotations in their preferred directions (PDs), and these rotations result in a population vector that is directed toward a planned target <xref ref-type="bibr" rid="pcbi.1002348-Li1">[6]</xref>. Neural encoding therefore changes to minimize errors between planning and behavior, suggesting that neural encoding is influenced by behavior and properties of output units.</p>
      <p>A critical problem exists in the relationship between motor cortex neurons and output units: the neuronal redundancy problem, or overcompleteness, which refers to the fact that the number of motor cortex neurons far exceeds the number of output units. Many different combinations of neural activities can therefore generate identical outputs. Neurophysiological and computational studies have revealed that the motor cortex exhibits neuronal redundancy <xref ref-type="bibr" rid="pcbi.1002348-Rokni1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002348-Narayanan1">[8]</xref>. However, it remains unknown how neuronal redundancy influences neural encoding. In other words, we do not yet understand the functional roles of neuronal redundancy in motor control and learning, though other types of redundancies are known to play various functional roles <xref ref-type="bibr" rid="pcbi.1002348-Bernstein1">[9]</xref>.</p>
      <p>One of these types of redundancy is muscle redundancy: many combinations of muscle activities can generate identical movements. The functional roles of this muscle redundancy include impedance control to achieve accurate movements <xref ref-type="bibr" rid="pcbi.1002348-Gribble1">[10]</xref>, reduction of motor variance by constructing muscle synergies <xref ref-type="bibr" rid="pcbi.1002348-Latash1">[11]</xref>, and learning internal models by changing muscle activities <xref ref-type="bibr" rid="pcbi.1002348-Thoroughman1">[12]</xref>. Another redundancy is kinematic redundancy: many combinations of joint angles result in identical hand positions. This redundancy ensures the stability of posture even if one joint is perturbed <xref ref-type="bibr" rid="pcbi.1002348-Latash2">[13]</xref>, and it facilitates of motor learning by increasing motor variance in a dimension irrelevant to the desired movements <xref ref-type="bibr" rid="pcbi.1002348-Yang1">[14]</xref>. Redundancies therefore play important functional roles in motor control and learning.</p>
      <p>Similar to the muscle and kinematic redundancies, neuronal redundancy likely has functional roles in motor control and learning. However, the functional roles of this redundancy are unclear. Here, using a redundant neural network, we investigate these functional roles by varying the number of model neurons while holding the number of output units constant. This manipulation allows us to control the degree of neuronal redundancy because, if a neural network includes a large number of neurons and a small number of output units, many different combinations of neural activities can generate identical outputs. It should be noted that we used a redundant neural network model that can explain neurophysiological motor cortex data <xref ref-type="bibr" rid="pcbi.1002348-Rokni1">[7]</xref>. The key conclusion arising from our study is that one of the functional roles of neuronal redundancy is the maximization of learning speed.</p>
      <p>Initially, a linear model with a fixed decoder was used. Analytical calculations revealed that neuronal redundancy is a necessary and sufficient condition to maximize learning speed. This maximization is invariant whether the distribution of PDs is unimodal <xref ref-type="bibr" rid="pcbi.1002348-Li1">[6]</xref> or bimodal <xref ref-type="bibr" rid="pcbi.1002348-Scott1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1002348-Naselaris1">[17]</xref>; both distributions have been reported in neurophysiological investigations. Second, numerical simulations confirmed the invariance of our results, even when the neural network included an adaptable decoder, a nonlinear activation function, recurrent connections, or nonlinear muscle units. Third, we show that our results do not depend on learning rules by using weight and node perturbation, both of which are representative stochastic gradient methods <xref ref-type="bibr" rid="pcbi.1002348-Werfel1">[18]</xref>. Finally, we demonstrate that our hypothesis does not depend on the shape of the generalization function which shape depends on the task (broad or sharp in force field <xref ref-type="bibr" rid="pcbi.1002348-Thoroughman2">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002348-Donchin1">[20]</xref> or visuomotor rotation adaptation <xref ref-type="bibr" rid="pcbi.1002348-Krakauer1">[21]</xref>, respectively). Our results strongly support our hypothesis that neuronal redundancy maximizes learning speed.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>Neuronal redundancy is defined as the dimensional gap between the number of neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e001" xlink:type="simple"/></inline-formula> and the number of outputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e002" xlink:type="simple"/></inline-formula>. It is synonymous with overcompleteness <xref ref-type="bibr" rid="pcbi.1002348-Lewicki2">[22]</xref>: many combinations of neural activities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e003" xlink:type="simple"/></inline-formula> can generate identical outputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e004" xlink:type="simple"/></inline-formula> through a decoder <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e005" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e006" xlink:type="simple"/></inline-formula>) because there are more neurons than necessary, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e007" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1002348-g001">Figure 1</xref>). It should be noted that neuronal redundancy is defined not by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e008" xlink:type="simple"/></inline-formula> but by the relationship between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e009" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e010" xlink:type="simple"/></inline-formula>. In most parts of this study, the number of constrained tasks <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e011" xlink:type="simple"/></inline-formula> is the same as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e012" xlink:type="simple"/></inline-formula> and is set to two, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e013" xlink:type="simple"/></inline-formula>, so there is neuronal redundancy if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e014" xlink:type="simple"/></inline-formula>. Thus, throughout this paper, the extent of neuronal redundancy can be expressed simply using the number of neurons. In this study, we can change only the neuronal redundancy; <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e015" xlink:type="simple"/></inline-formula> can be increased while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e016" xlink:type="simple"/></inline-formula> is held constant at two, enabling the investigation of the functional roles of neuronal redundancy. In the <italic>Importance of Neuronal Redundancy</italic> section, we distinguish the effects of neuronal redundancy from the effects of neuron number by varying both <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e017" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e018" xlink:type="simple"/></inline-formula>.</p>
      <fig id="pcbi-1002348-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002348.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Graphical model of a redundant neural network.</title>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.g001" xlink:type="simple"/>
      </fig>
      <p>In this study, we discuss the relationship between neuronal redundancy and learning speed by assuming adaptation to either a visuomotor rotation or a force field. These tasks are simulated by using a rotational perturbation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e019" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e020" xlink:type="simple"/></inline-formula> is the rotational angle. Due to this perturbation, if an error occurs between target position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e021" xlink:type="simple"/></inline-formula> and output (motor command) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e022" xlink:type="simple"/></inline-formula> in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e023" xlink:type="simple"/></inline-formula>th trial, neural activities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e024" xlink:type="simple"/></inline-formula> are modified to minimize the error, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e025" xlink:type="simple"/></inline-formula> is the angle of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e026" xlink:type="simple"/></inline-formula>th target which is radially and equally distributed (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e027" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e028" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e029" xlink:type="simple"/></inline-formula>). To model the learning process in the motor cortex, we used a linear rate model, which can reproduce neurophysiological data <xref ref-type="bibr" rid="pcbi.1002348-Rokni1">[7]</xref> and be easily analyzed. In this model, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e030" xlink:type="simple"/></inline-formula> is given by a weighted average of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e031" xlink:type="simple"/></inline-formula>, and each component of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e032" xlink:type="simple"/></inline-formula> is accordingly set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e033" xlink:type="simple"/></inline-formula>, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e034" xlink:type="simple"/></inline-formula>th component of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e035" xlink:type="simple"/></inline-formula> is defined as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e036" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e037" xlink:type="simple"/></inline-formula> is a variable that is independent of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e038" xlink:type="simple"/></inline-formula>. Because of this assumption, the learning rate is set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e039" xlink:type="simple"/></inline-formula> such that the trial-to-trial variation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e040" xlink:type="simple"/></inline-formula> do not depend on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e041" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e042" xlink:type="simple"/></inline-formula>), but the optimized learning rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e043" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e044" xlink:type="simple"/></inline-formula> (see <xref ref-type="supplementary-material" rid="pcbi.1002348.s006">Text S1</xref>), i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e045" xlink:type="simple"/></inline-formula>, suggesting that we consider the quasi-optimal learning rate in this study. It should be noted that, because the following results do not depend on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e046" xlink:type="simple"/></inline-formula>, our results hold when the optimal learning rate is used. Furthermore, even when each component of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e047" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e048" xlink:type="simple"/></inline-formula>, the following results are invariant if we set the learning rate to its optimal value (see <xref ref-type="supplementary-material" rid="pcbi.1002348.s006">Text S1</xref>). Our study shows that neuronal redundancy is necessary and sufficient to maximize learning speed.</p>
      <sec id="s2a">
        <title>Neuronal redundancy maximizes learning speed</title>
        <sec id="s2a1">
          <title>Fixed homogeneous decoder</title>
          <p>In the case of a fixed decoder, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e049" xlink:type="simple"/></inline-formula>, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e050" xlink:type="simple"/></inline-formula>th neuron has uniform force amplitude (FA) (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e051" xlink:type="simple"/></inline-formula>) and force direction (FD), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e052" xlink:type="simple"/></inline-formula>, which is randomly sampled from a uniform distribution. Because of its uniformity, we refer to this decoder as a fixed homogeneous decoder. This model corresponds to the one proposed by Rokni et al. <xref ref-type="bibr" rid="pcbi.1002348-Rokni1">[7]</xref>.</p>
          <p>In this case, the squared error can be calculated recursively as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e053" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e054" xlink:type="simple"/></inline-formula>. Here, we assume that a single target is repeatedly presented for simplicity (general case is discussed in the <italic><xref ref-type="sec" rid="s4">Methods</xref></italic> section), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e055" xlink:type="simple"/></inline-formula> is the identity matrix, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e056" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e057" xlink:type="simple"/></inline-formula> is the learning rate, and neural activity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e058" xlink:type="simple"/></inline-formula> is updated as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e059" xlink:type="simple"/><label>(2)</label></disp-formula>for the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e060" xlink:type="simple"/></inline-formula>th trial to minimize the squared error. Multiplication by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e061" xlink:type="simple"/></inline-formula> in equation (2) is included for the purpose of scaling; it ensures that the amount of trial-to-trial variation in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e062" xlink:type="simple"/></inline-formula> does not explicitly depend on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e063" xlink:type="simple"/></inline-formula>. Equation (1) can thus be simplified as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e064" xlink:type="simple"/><label>(3)</label></disp-formula>where the diagonal elements of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e065" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e066" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e067" xlink:type="simple"/></inline-formula>, are eigenvalues of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e068" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e069" xlink:type="simple"/></inline-formula> is decomposed as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e070" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e071" xlink:type="simple"/></inline-formula>), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e072" xlink:type="simple"/></inline-formula>, and learning speed is therefore determined based on the eigenvalues of<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e073" xlink:type="simple"/><label>(4)</label></disp-formula>each component of which is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e074" xlink:type="simple"/></inline-formula>. The larger <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e075" xlink:type="simple"/></inline-formula> becomes, the faster learning becomes (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e076" xlink:type="simple"/></inline-formula>). It should be noted that learning speed and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e077" xlink:type="simple"/></inline-formula> do not explicitly depend on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e078" xlink:type="simple"/></inline-formula>.</p>
          <p>Analytical calculations can yield necessary and sufficient conditions to maximize learning speed (see the <italic><xref ref-type="sec" rid="s4">Methods</xref></italic> section). The following self-averaging properties <xref ref-type="bibr" rid="pcbi.1002348-Hidetoshi1">[23]</xref> maximize learning speed or maximize the minimum eigenvalue of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e079" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e080" xlink:type="simple"/><label>(5)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e081" xlink:type="simple"/><label>(6)</label></disp-formula>and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e082" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e083" xlink:type="simple"/></inline-formula> is the probability distribution in which FDs are randomly sampled. It remains unknown what kind of conditions can satisfy the self-averaging properties. The self-averaging properties are satisfied if and only if the neural network model includes sufficient neuronal redundancy. In other words, learning speed is maximized if and only if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e084" xlink:type="simple"/></inline-formula>. If the neural network includes neuronal redundancy, the self-averaging properties exist. Conversely, if the self-averaging properties exist, the neural network model should include sufficient neuronal redundancy because Monte Carlo integration shows a fluctuation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e085" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002348-Bishop1">[24]</xref>. Thus, in the case of a fixed homogeneous decoder, neuronal redundancy plays a functional role in maximizing learning speed.</p>
          <p>We numerically confirmed the above analytical results. <xref ref-type="fig" rid="pcbi-1002348-g002">Figures 2A and 2B</xref> show the learning speed and learning curves calculated using the results of 1,000 sets of randomly sampled <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e086" xlink:type="simple"/></inline-formula> values, an identical target sequence (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e087" xlink:type="simple"/></inline-formula>), and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e088" xlink:type="simple"/></inline-formula>. The more neuronal redundancy grows, the faster learning speed becomes. <xref ref-type="fig" rid="pcbi-1002348-g002">Figure 2C</xref> shows the relationship between learning speed and neuronal redundancy. The horizontal axis denotes the number of neurons, and the vertical axis denotes the increase in learning speed. Although a saturation of the increase can be seen, greater neuronal redundancy still yields faster learning speed. Therefore, these figures support our analytical results: in the case of a fixed homogeneous decoder, neuronal redundancy maximizes learning speed.</p>
          <fig id="pcbi-1002348-g002" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002348.g002</object-id>
            <label>Figure 2</label>
            <caption>
              <title>Relationship between learning speed and neuronal redundancy (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e089" xlink:type="simple"/></inline-formula>).</title>
              <p>(A): Learning speed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e090" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e091" xlink:type="simple"/></inline-formula>. The bar graph and error bars depict sample means and standard deviations, both of which are calculated using the results of randomly sampled sets of 1000 <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e092" xlink:type="simple"/></inline-formula> values. (B): Learning curves when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e093" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e094" xlink:type="simple"/></inline-formula>. These curves and error bars show averaged values and standard deviations of errors. (C): Relationship between learning speed and the number of model neurons when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e095" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e096" xlink:type="simple"/></inline-formula>. The horizontal axis represents the number of neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e097" xlink:type="simple"/></inline-formula> and the vertical axis represents <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e098" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e099" xlink:type="simple"/></inline-formula> is the learning speed when the number of neurons is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e100" xlink:type="simple"/></inline-formula>. Dotted and solid lines denote the average learning speed and power functions fitted to the values, respectively.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.g002" xlink:type="simple"/>
          </fig>
        </sec>
        <sec id="s2a2">
          <title>Fixed non-homogeneous decoder</title>
          <p>The question remains whether it is necessary for FD and FA to be distributed uniformly, so we assume that the values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e101" xlink:type="simple"/></inline-formula> are randomly sampled from the probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e102" xlink:type="simple"/></inline-formula> to make FD and FA non-homogeneous, i.e., FDs are non-uniformly distributed, and FAs are different for each neuron. In the case of a non-homogeneous decoder, the necessary and sufficient conditions to maximize learning speed are also the following self-averaging properties:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e103" xlink:type="simple"/><label>(8)</label></disp-formula>and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e104" xlink:type="simple"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e105" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e106" xlink:type="simple"/></inline-formula> are marginalized distributions. <xref ref-type="fig" rid="pcbi-1002348-g003">Figures 3A and 3D</xref> show distributions of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e107" xlink:type="simple"/></inline-formula> that satisfy equations (8) and (9). <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e108" xlink:type="simple"/></inline-formula> is randomly sampled from unimodal and bimodal Gaussian distributions in <xref ref-type="fig" rid="pcbi-1002348-g003">Figures 3A and 3D</xref>, respectively. Because these figures show the non-uniformity in both FD and FA, neuronal redundancy maximizes learning speed regardless of these non-uniformities.</p>
          <fig id="pcbi-1002348-g003" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002348.g003</object-id>
            <label>Figure 3</label>
            <caption>
              <title>Network properties when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e109" xlink:type="simple"/></inline-formula> satisfies equations (8) and (9).</title>
              <p>(A): Scatter plot of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e110" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e111" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e112" xlink:type="simple"/></inline-formula> are randomly sampled from a unimodal Gaussian distribution (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e113" xlink:type="simple"/></inline-formula>). (B), (C): Histogram of preferred direction and modulation depth when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e114" xlink:type="simple"/></inline-formula> is randomly sampled as shown in (A). (D): Scatter plot of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e115" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e116" xlink:type="simple"/></inline-formula> are randomly sampled from a bimodal Gaussian distribution. (E), (F): Histograms of preferred direction and modulation depth when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e117" xlink:type="simple"/></inline-formula> is randomly sampled as shown in (D).</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.g003" xlink:type="simple"/>
          </fig>
        </sec>
        <sec id="s2a3">
          <title>Distribution of preferred directions</title>
          <p>Some neurophysiological studies have suggested that the distribution of PD is a skewed bimodal <xref ref-type="bibr" rid="pcbi.1002348-Scott1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1002348-Naselaris1">[17]</xref>, but other neurophysiological studies have suggested that the distribution of PD is uniform <xref ref-type="bibr" rid="pcbi.1002348-Li1">[6]</xref>. We investigated whether our results were consistent with the results of these neurophysiological studies. <xref ref-type="fig" rid="pcbi-1002348-g003">Figures 3B and 3E</xref> depict the distribution of preferred directions (PDs) that results when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e118" xlink:type="simple"/></inline-formula> is randomly sampled as shown in <xref ref-type="fig" rid="pcbi-1002348-g003">Figures 3A and 3D</xref>, respectively, with PDs calculated as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e119" xlink:type="simple"/></inline-formula> (see the <italic><xref ref-type="sec" rid="s4">Methods</xref></italic> section). <xref ref-type="fig" rid="pcbi-1002348-g003">Figures 3B and 3E</xref> show that both a skewed bimodal distribution and a uniform distribution can be observed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e120" xlink:type="simple"/></inline-formula> satisfies equations (8) and (9), suggesting that our hypothesis is consistent with the results of previous neurophysiological experiments.</p>
          <p><xref ref-type="fig" rid="pcbi-1002348-g003">Figures 3C and 3F</xref> show the distribution of modulation depth, which is calculated as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e121" xlink:type="simple"/></inline-formula> (see the <italic><xref ref-type="sec" rid="s4">Methods</xref></italic> section). Our results suggest that the distribution of modulation depth is skewed.</p>
        </sec>
        <sec id="s2a4">
          <title>Adaptable decoder</title>
          <p>We have analytically elucidated the relevance of neuronal redundancy to learning speed only when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e122" xlink:type="simple"/></inline-formula> is fixed, but the question remains of whether neuronal redundancy can maximize learning speed even when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e123" xlink:type="simple"/></inline-formula> is adaptable. In this case, it is analytically intractable to calculate learning speed, so we used numerical simulations. <xref ref-type="fig" rid="pcbi-1002348-g004">Figure 4A</xref> shows the learning speed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e124" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e125" xlink:type="simple"/></inline-formula> in the case of an adaptable decoder. Although there was no significant difference in learning speed between the cases in which <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e126" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e127" xlink:type="simple"/></inline-formula>, neuronal redundancy maximized learning speed even if the decoder was adaptable. <xref ref-type="fig" rid="pcbi-1002348-g004">Figure 4B</xref>, which shows the learning curve when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e128" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e129" xlink:type="simple"/></inline-formula>, also supports the maximization.</p>
          <fig id="pcbi-1002348-g004" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002348.g004</object-id>
            <label>Figure 4</label>
            <caption>
              <title>Relationship between learning speed and neuronal redundancy when the decoder is adaptable (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e130" xlink:type="simple"/></inline-formula>).</title>
              <p>(A): Bar graphs and error bars depict sample means and standard deviations both of which are calculated using the results from 1000 sets of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e131" xlink:type="simple"/></inline-formula> values. (B): Learning curves when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e132" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e133" xlink:type="simple"/></inline-formula>. These curves and error bars show averaged values and the standard deviations of the errors.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.g004" xlink:type="simple"/>
          </fig>
        </sec>
      </sec>
      <sec id="s2b">
        <title>Importance of neuronal redundancy</title>
        <p>Although we have revealed that neuronal redundancy maximizes learning speed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e134" xlink:type="simple"/></inline-formula>, it is important to verify that the effect is caused by the neuronal redundancy, i.e., the dimensional gap between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e135" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e136" xlink:type="simple"/></inline-formula>, and not simply the number of neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e137" xlink:type="simple"/></inline-formula>. In this section, we investigate this question by varying both <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e138" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e139" xlink:type="simple"/></inline-formula> while assuming that each component of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e140" xlink:type="simple"/></inline-formula> is randomly sampled from a Gaussian distribution.</p>
        <p><xref ref-type="fig" rid="pcbi-1002348-g005">Figures 5A and 5B</xref> show the learning speed and the learning curve produced when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e141" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e142" xlink:type="simple"/></inline-formula> with a fixed non-homogeneous decoder. If <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e143" xlink:type="simple"/></inline-formula> alone were important for maximizing learning speed, learning speed would be faster when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e144" xlink:type="simple"/></inline-formula> than when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e145" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e146" xlink:type="simple"/></inline-formula>. However, the results shown in these figures support the opposite conclusion, i.e., learning speed becomes slower when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e147" xlink:type="simple"/></inline-formula> compared to the other cases. This result suggests that the number of neurons alone is not important for maximizing learning speed.</p>
        <fig id="pcbi-1002348-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002348.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Importance of neuronal redundancy (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e148" xlink:type="simple"/></inline-formula>).</title>
            <p>(A): Learning speed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e149" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e150" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e151" xlink:type="simple"/></inline-formula>, where N and T are the number of neurons and constrained tasks, respectively. The bar graphs and error bars depict the sample means and standard deviations, both of which are calculated using the results of 1000 sets of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e152" xlink:type="simple"/></inline-formula> values. (B): Learning curves when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e153" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e154" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e155" xlink:type="simple"/></inline-formula>. These curves and error bars show the average values and the standard deviations of the errors. (C): Learning speed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e156" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e157" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e158" xlink:type="simple"/></inline-formula>. The bar graphs and error bars depict the sample means and the standard deviations, both of which are calculated using the results of 1000 sets of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e159" xlink:type="simple"/></inline-formula> values. (D): Learning curves when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e160" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e161" xlink:type="simple"/></inline-formula>. These curves and error bars show the average values and the standard deviations of the errors. (E): Learning curves calculated when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e162" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e163" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e164" xlink:type="simple"/></inline-formula> and decoder <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e165" xlink:type="simple"/></inline-formula> is adaptable. (F): Learning curves calculated when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e166" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e167" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e168" xlink:type="simple"/></inline-formula>; and the decoder <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e169" xlink:type="simple"/></inline-formula> is adaptable.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.g005" xlink:type="simple"/>
        </fig>
        <p><xref ref-type="fig" rid="pcbi-1002348-g005">Figures 5C and 5D</xref> show the learning speed and learning curve produced when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e170" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e171" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e172" xlink:type="simple"/></inline-formula> and a fixed non-homogeneous decoder. If neuronal redundancy were important, the learning speed would be faster when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e173" xlink:type="simple"/></inline-formula> than when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e174" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e175" xlink:type="simple"/></inline-formula>. These figures support this hypothesis; learning speed increased when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e176" xlink:type="simple"/></inline-formula> compared to the other cases. Taken together, these results indicate that the important factor for maximizing learning speed is in fact neuronal redundancy and not simply the number of neurons.</p>
        <p>In addition, we investigated whether neuronal redundancy or neuron number is important when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e177" xlink:type="simple"/></inline-formula> is adaptable. In this case, we only show learning curves because learning speed cannot be exponentially fitted, which makes it impossible to calculate learning speed. <xref ref-type="fig" rid="pcbi-1002348-g005">Figures 5E and 5F</xref> show the learning curves calculated when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e178" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e179" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e180" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e181" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e182" xlink:type="simple"/></inline-formula>. These figures show the same results as the case when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e183" xlink:type="simple"/></inline-formula> is fixed; even when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e184" xlink:type="simple"/></inline-formula> is adaptable, the important factor for maximizing learning speed is neuronal redundancy, not simply the number of neurons.</p>
      </sec>
      <sec id="s2c">
        <title>Generality of our results</title>
        <p>The generality of our results should be investigated because we analyzed only linear and feed-forward networks, but neurophysiological experiments have suggested the existence of recurrent connections <xref ref-type="bibr" rid="pcbi.1002348-Capaday1">[25]</xref> and nonlinear neural activation functions <xref ref-type="bibr" rid="pcbi.1002348-Tsodyks1">[26]</xref>. Also, only a linear rotational perturbation task was considered, so we need to investigate whether our results hold when the constrained tasks are nonlinear because, in fact, motor cortex neurons solve nonlinear tasks. The neurons send motor commands and control muscles whose activities are nonlinearly determined: muscles can pull but cannot push. Using numerical simulations, we show that neuronal redundancy maximizes learning speed, even when the neural network includes recurrent connections (<xref ref-type="supplementary-material" rid="pcbi.1002348.s001">Figure S1</xref>), when it includes nonlinear activation functions (<xref ref-type="supplementary-material" rid="pcbi.1002348.s002">Figure S2</xref>), and when the task is nonlinear (<xref ref-type="supplementary-material" rid="pcbi.1002348.s003">Figure S3</xref>).</p>
        <p>In addition, we used only deterministic gradient descent, so the generality regarding the learning rule needs to be investigated. In fact, previous studies have suggested that stochastic gradient methods are more biologically relevant than deterministic ones <xref ref-type="bibr" rid="pcbi.1002348-Seung1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1002348-Fiete1">[28]</xref>. Analytical and numerical calculations confirm that our results are invariant even when the learning rule is stochastic (<xref ref-type="supplementary-material" rid="pcbi.1002348.s004">Figure S4</xref>). Our results therefore have strong generality.</p>
        <sec id="s2c1">
          <title>Activity noise and plasticity noise</title>
          <p>Although our results have strong generality, there is still an open question regarding the robustness of noise: does neuronal redundancy maximize learning speed even in the presence of neural noise? Actually, neural activities show trial-to-trial variation <xref ref-type="bibr" rid="pcbi.1002348-Lee1">[29]</xref>, and the neural plasticity mechanism also includes trial-to-trial fluctuations <xref ref-type="bibr" rid="pcbi.1002348-Rokni1">[7]</xref>. This section investigates the relationships between neuronal redundancy, learning speed, and neural noise.</p>
          <p><xref ref-type="fig" rid="pcbi-1002348-g006">Figures 6A and 6D</xref> show the variance of learning curves when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e185" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e186" xlink:type="simple"/></inline-formula>, respectively, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e187" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e188" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e189" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e190" xlink:type="simple"/></inline-formula> representing the standard deviations of activity noise and plasticity noise, respectively. The definition of the variance is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e191" xlink:type="simple"/></inline-formula>, which is a measure of the stability of learning. Examples of learning curves are shown in <xref ref-type="fig" rid="pcbi-1002348-g006">Figures 6B, 6C, 6E, and 6F</xref>. These figures show that neuronal redundancy enhances the stability of learning by eliminating the influences of activity and plasticity noise. Neuronal redundancy therefore not only maximizes learning speed but also facilitates robustness in response to neural noise.</p>
          <fig id="pcbi-1002348-g006" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002348.g006</object-id>
            <label>Figure 6</label>
            <caption>
              <title>Relationship between neuronal redundancy and neural noise (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e192" xlink:type="simple"/></inline-formula>).</title>
              <p>(A): Variance of the learning curve when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e193" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e194" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e195" xlink:type="simple"/></inline-formula>. The bar graphs show the average values of randomly sampled sets of 1000 <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e196" xlink:type="simple"/></inline-formula> values. (B): Learning curves calculated when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e197" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e198" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e199" xlink:type="simple"/></inline-formula>. These curves and error bars show the average values and the standard deviations of the errors. (C): Learning curves calculated when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e200" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e201" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e202" xlink:type="simple"/></inline-formula>. (D): Variance of the learning curve when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e203" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e204" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e205" xlink:type="simple"/></inline-formula>. (E): Learning curves when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e206" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e207" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e208" xlink:type="simple"/></inline-formula>. (F): Learning curves calculated when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e209" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e210" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e211" xlink:type="simple"/></inline-formula>.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.g006" xlink:type="simple"/>
          </fig>
        </sec>
        <sec id="s2c2">
          <title>Shape of the generalization function</title>
          <p>In many situations, learning in one context is generalized to different contexts, such as different postures <xref ref-type="bibr" rid="pcbi.1002348-Shadmehr1">[30]</xref>, different arms <xref ref-type="bibr" rid="pcbi.1002348-CriscimagnaHemminger1">[31]</xref>, and different movement directions <xref ref-type="bibr" rid="pcbi.1002348-Thoroughman2">[19]</xref>–<xref ref-type="bibr" rid="pcbi.1002348-Krakauer1">[21]</xref>, with the degree of generalization depending on the task. In this study, we define the generalization function as the degree of generalization to different movement directions. The performance of reaching towards <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e212" xlink:type="simple"/></inline-formula> is generalized to that of reaching towards <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e213" xlink:type="simple"/></inline-formula>, and the degree of this generalization is determined by the generalization function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e214" xlink:type="simple"/></inline-formula>. In visuomotor rotation adaptation, the generalization function is narrow in the direction metric <xref ref-type="bibr" rid="pcbi.1002348-Krakauer1">[21]</xref>. In contrast, the generalization function is broad in force field adaptation <xref ref-type="bibr" rid="pcbi.1002348-Thoroughman2">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002348-Donchin1">[20]</xref>. To investigate the generality of our results with respect to various kinds of tasks, it is necessary to investigate the relationships between neuronal redundancy, learning speed, and the shape of the generalization function.</p>
          <p><xref ref-type="fig" rid="pcbi-1002348-g007">Figure 7</xref> shows the relationship between the shape of the generalization function and learning speed. <xref ref-type="fig" rid="pcbi-1002348-g007">Figures 7A and 7B</xref> show the learning speed and learning curve calculated when the generalization function is broad (<xref ref-type="fig" rid="pcbi-1002348-g007">Figure 7C</xref>). <xref ref-type="fig" rid="pcbi-1002348-g007">Figures 7D and 7E</xref> show the learning speed and learning curve calculated when the generalization function is narrow (<xref ref-type="fig" rid="pcbi-1002348-g007">Figure 7F</xref>). Although these figures show that narrower generalization results in a slower learning speed, neuronal redundancy maximizes learning speed independently of the shape of the generalization function.</p>
          <fig id="pcbi-1002348-g007" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002348.g007</object-id>
            <label>Figure 7</label>
            <caption>
              <title>Relationship between neuronal redundancy, learning speed, and the shape of the generalization function (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e215" xlink:type="simple"/></inline-formula>).</title>
              <p>(A): Learning speed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e216" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e217" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e218" xlink:type="simple"/></inline-formula>. The bar graphs and error bars depict sample means and standard deviations, both of which are calculated using the results of randomly sampled sets of 1000 <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e219" xlink:type="simple"/></inline-formula> values in the case of a broad generalization function. (B): Learning curves calculated when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e220" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e221" xlink:type="simple"/></inline-formula>. These curves and error bars show the average values and standard deviations of the errors. (C): The generalization function with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e222" xlink:type="simple"/></inline-formula>. (D): The learning speed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e223" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e224" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e225" xlink:type="simple"/></inline-formula>. Bar graphs and error bars depict the sample means and standard deviations when the generalization function is narrow (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e226" xlink:type="simple"/></inline-formula>). (E): Learning curves calculated when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e227" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e228" xlink:type="simple"/></inline-formula>. (F): The generalization function with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e229" xlink:type="simple"/></inline-formula>.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.g007" xlink:type="simple"/>
          </fig>
        </sec>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We have quantitatively demonstrated that neuronal redundancy maximizes learning speed. The larger the dimensional gap grows between the number of neurons and the number of constrained tasks, the faster learning speed becomes. This maximization does not depend on whether the PD distribution is unimodal or bimodal, the decoder is fixed or adaptable, the network is linear or nonlinear, the task is linear or nonlinear, or the learning rule is stochastic or non-stochastic. Additionally, we have shown that neuronal redundancy has another important functional role: it provides robustness in response to neural noise. Furthermore, neuronal redundancy maximizes learning speed in a manner independent of the shape of the generalization function. These results strongly support the generality of our results.</p>
      <p>Neuronal redundancy maximizes learning speed because only <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e230" xlink:type="simple"/></inline-formula> equalities, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e231" xlink:type="simple"/></inline-formula>, need to be satisfied, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e232" xlink:type="simple"/></inline-formula>-dimensional neural activity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e233" xlink:type="simple"/></inline-formula> is adaptable (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e234" xlink:type="simple"/></inline-formula>). This dimensional gap yields the large <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e235" xlink:type="simple"/></inline-formula> dimensional subspace of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e236" xlink:type="simple"/></inline-formula> in which the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e237" xlink:type="simple"/></inline-formula> equalities are satisfied. The more <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e238" xlink:type="simple"/></inline-formula> increases, the greater the fraction of the subspace becomes: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e239" xlink:type="simple"/></inline-formula>. Neuronal redundancy, rather than the number of neurons, thus enables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e240" xlink:type="simple"/></inline-formula> to rapidly reach a single point in the subspace. This interpretation likely applies even in the cases of an adaptable decoder, recurrent connections, a nonlinear network, a nonlinear task, and a stochastic learning rule. Furthermore, this interpretation is supported by the results shown in <xref ref-type="fig" rid="pcbi-1002348-g005">Figure 5</xref>; the bigger <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e241" xlink:type="simple"/></inline-formula> grows, the faster learning speed becomes.</p>
      <p>At first glance, our results may seem inconsistent with the results of Werfel et al. <xref ref-type="bibr" rid="pcbi.1002348-Werfel1">[18]</xref>, who concluded that learning speed is inversely proportional to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e242" xlink:type="simple"/></inline-formula>. In their model, because they considered the single-layer linear model, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e243" xlink:type="simple"/></inline-formula> is the same as the number of input units, which is defined as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e244" xlink:type="simple"/></inline-formula>( = <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e245" xlink:type="simple"/></inline-formula>) in the present study. A similar tendency can be observed in <xref ref-type="fig" rid="pcbi-1002348-g005">Figure 5</xref>; the more <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e246" xlink:type="simple"/></inline-formula> increases, the slower learning speed becomes. We calculated the optimal learning rate and speed as shown in <xref ref-type="supplementary-material" rid="pcbi.1002348.s006">Text S1</xref>, and confirmed that learning speed is inversely proportional to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e247" xlink:type="simple"/></inline-formula>. Thus, our results are consistent with Werfel's study and additionally suggest that neuronal redundancy maximizes learning speed.</p>
      <p>Neuronal redundancy plays another important role: generating robustness in response to neural noise (<xref ref-type="fig" rid="pcbi-1002348-g006">Figure 6</xref>). Because neuronal redundancy has the same meaning as overcompleteness, its functional role is the same as the robustness of overcompleteness in the face of perturbations in signals <xref ref-type="bibr" rid="pcbi.1002348-Simoncelli1">[32]</xref>. This additional functional role further supports our hypothesis that neuronal redundancy is a special neural basis on which to maximize learning speed. For example, if we increase the learning rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e248" xlink:type="simple"/></inline-formula> in a non-redundant network, the learning speed approaches the maximal speed in a redundant network in which the learning rate is fixed to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e249" xlink:type="simple"/></inline-formula>. As shown in <xref ref-type="fig" rid="pcbi-1002348-g006">Figure 6</xref>, however, a non-redundant network is not robust with respect to neural noise. Furthermore, neuronal redundancy minimizes residual errors when the neural network includes synaptic decay <xref ref-type="bibr" rid="pcbi.1002348-Rokni1">[7]</xref> (see the <italic><xref ref-type="sec" rid="s4">Methods</xref></italic> section and <xref ref-type="supplementary-material" rid="pcbi.1002348.s005">Figure S5</xref>). Thus, neuronal redundancy represents a special neural basis for maximizing learning speed while minimizing residual error and maintaining robustness in response to neural noise.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Model definition</title>
        <p>Our study assumed the following task: participants move their arms towards one of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e250" xlink:type="simple"/></inline-formula> radially distributed targets. If the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e251" xlink:type="simple"/></inline-formula>th target is presented in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e252" xlink:type="simple"/></inline-formula>th trial, the neural network model receives the input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e253" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e254" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e255" xlink:type="simple"/></inline-formula>), where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e256" xlink:type="simple"/></inline-formula>. The input units project to neurons (hidden units), the activities of which are determined by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e257" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e258" xlink:type="simple"/></inline-formula> is synaptic weight in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e259" xlink:type="simple"/></inline-formula>th trial, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e260" xlink:type="simple"/></inline-formula> is the standard deviation of neural activity noise, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e261" xlink:type="simple"/></inline-formula> denotes independent normal Gaussian random variables, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e262" xlink:type="simple"/></inline-formula> is the number of neurons (<xref ref-type="fig" rid="pcbi-1002348-g001">Figure 1</xref>). The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e263" xlink:type="simple"/></inline-formula>th neuron has a PD given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e264" xlink:type="simple"/></inline-formula> and a modulation depth <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e265" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e266" xlink:type="simple"/></inline-formula>, this cosine tuning having been reported by many neurophysiological studies.</p>
        <p>The neural population generates a force of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e267" xlink:type="simple"/></inline-formula> through a decoder matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e268" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e269" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e270" xlink:type="simple"/></inline-formula> is the number of outputs, which, in most cases, is set to 2. When <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e271" xlink:type="simple"/></inline-formula> is fixed and homogeneous, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e272" xlink:type="simple"/></inline-formula>th and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e273" xlink:type="simple"/></inline-formula>th components of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e274" xlink:type="simple"/></inline-formula> are defined as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e275" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e276" xlink:type="simple"/></inline-formula>, respectively, where division by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e277" xlink:type="simple"/></inline-formula> is used for scaling and FD <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e278" xlink:type="simple"/></inline-formula> is randomly sampled from a uniform distribution (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e279" xlink:type="simple"/></inline-formula>). When <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e280" xlink:type="simple"/></inline-formula> is fixed and non-homogeneous, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e281" xlink:type="simple"/></inline-formula> is randomly sampled from a probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e282" xlink:type="simple"/></inline-formula> and divided by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e283" xlink:type="simple"/></inline-formula>. As a result, the neural network generates a final hand coordinate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e284" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e285" xlink:type="simple"/><label>(12)</label></disp-formula>which means that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e286" xlink:type="simple"/></inline-formula> is perturbed by a rotation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e287" xlink:type="simple"/></inline-formula> which assumes a visuomotor rotation or curl force field. Rotational perturbations are assumed because many behavioral studies have used them. Because we discuss only the endpoint of the movement, we refer to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e288" xlink:type="simple"/></inline-formula> as the motor command. The constrained tasks are those that the neural network generates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e289" xlink:type="simple"/></inline-formula> toward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e290" xlink:type="simple"/></inline-formula>, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e291" xlink:type="simple"/></inline-formula>, which means the number of constrained tasks <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e292" xlink:type="simple"/></inline-formula> is the same as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e293" xlink:type="simple"/></inline-formula>. We used <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e294" xlink:type="simple"/></inline-formula> instead of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e295" xlink:type="simple"/></inline-formula> in the following sections.</p>
        <p>If the error occurs between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e296" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e297" xlink:type="simple"/></inline-formula>, synaptic weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e298" xlink:type="simple"/></inline-formula> are adapted to reduce the squared error, which is defined as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e299" xlink:type="simple"/></inline-formula>, based on a gradient descent method<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e300" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e301" xlink:type="simple"/></inline-formula> is the synaptic decay rate, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e302" xlink:type="simple"/></inline-formula> is the learning rate (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e303" xlink:type="simple"/></inline-formula> is set to 0.2 in most parts of the present study), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e304" xlink:type="simple"/></inline-formula> is the strength of synaptic drift, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e305" xlink:type="simple"/></inline-formula> denotes normal Gaussian random variables. Since each component of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e306" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e307" xlink:type="simple"/></inline-formula>, multiplying <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e308" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e309" xlink:type="simple"/></inline-formula> allows trial-by-trial variation of both <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e310" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e311" xlink:type="simple"/></inline-formula> to be <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e312" xlink:type="simple"/></inline-formula>. As shown in <xref ref-type="supplementary-material" rid="pcbi.1002348.s006">Text S1</xref>, the optimal learning rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e313" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e314" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e315" xlink:type="simple"/></inline-formula>), suggesting that we consider a quasi-optimal learning rate. It should be noted that our results hold whether the learning rate is optimal or quasi-optimal because the results do not depend on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e316" xlink:type="simple"/></inline-formula>. It should also be noted that the amount of variation in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e317" xlink:type="simple"/></inline-formula> does not explicitly depend on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e318" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s4b">
        <title>Learning curve</title>
        <p>Equation (13) yields the following update rule of squared error:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e319" xlink:type="simple"/><label>(14)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e320" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e321" xlink:type="simple"/></inline-formula> denotes the identity matrix. At first, we assume a case in which <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e322" xlink:type="simple"/></inline-formula> for simplicity. Because <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e323" xlink:type="simple"/></inline-formula> is symmetric, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e324" xlink:type="simple"/></inline-formula> can be decomposed as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e325" xlink:type="simple"/></inline-formula>, where each row of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e326" xlink:type="simple"/></inline-formula> is one of the eigenvectors (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e327" xlink:type="simple"/></inline-formula>) and each diagonal component of a diagonal matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e328" xlink:type="simple"/></inline-formula> is one of the eigenvalues of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e329" xlink:type="simple"/></inline-formula>. This decomposition transforms equation (14) into the simple form<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e330" xlink:type="simple"/><label>(15)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e331" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e332" xlink:type="simple"/></inline-formula>. This recurrence formula yields the analytical form of the learning curve:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e333" xlink:type="simple"/><label>(16)</label></disp-formula></p>
        <p>Equation (16) requires that the larger the eigenvalues become, the faster the learning speed becomes and the smaller the residual error becomes (<xref ref-type="supplementary-material" rid="pcbi.1002348.s005">Figure S5</xref>). Because<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e334" xlink:type="simple"/><label>(17)</label></disp-formula>whose component is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e335" xlink:type="simple"/></inline-formula>, simple algebra gives the analytical form of the eigenvalues,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e336" xlink:type="simple"/><label>(18)</label></disp-formula>which are also <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e337" xlink:type="simple"/></inline-formula>, suggesting that learning speed does not depend explicitly on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e338" xlink:type="simple"/></inline-formula>. Because the learning speed is determined by the smaller eigenvalue, the necessary and sufficient conditions to maximize learning speed, or to maximize the smaller eigenvalue, are<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e339" xlink:type="simple"/><label>(19)</label></disp-formula>and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e340" xlink:type="simple"/><label>(20)</label></disp-formula></p>
        <p>What kind of conditions can simultaneously satisfy equations (19) and (20)? The only answer is sufficient neuronal redundancy, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e341" xlink:type="simple"/></inline-formula>, because sufficient neuronal redundancy enables self-averaging properties to exist in a neural network, i.e.,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e342" xlink:type="simple"/><label>(21)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e343" xlink:type="simple"/><label>(22)</label></disp-formula>and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e344" xlink:type="simple"/><label>(23)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e345" xlink:type="simple"/></inline-formula> is the probability distribution in which FDs are randomly sampled. Conversely, if equations (21), (22), and (23) are satisfied in all of the sets of randomly sampled FDs, the number of neurons needs to satisfy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e346" xlink:type="simple"/></inline-formula> because the fluctuation of Monte Carlo integrals is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e347" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002348-Bishop1">[24]</xref>. Therefore, to maximize learning speed, the necessary and sufficient condition is sufficient neuronal redundancy.</p>
        <p>The above analytical calculations hold even when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e348" xlink:type="simple"/></inline-formula>. Equation (13) yields the recurrence equation of the squared error:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e349" xlink:type="simple"/><label>(24)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e350" xlink:type="simple"/></inline-formula> is set to 1 for simplicity. Using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e351" xlink:type="simple"/></inline-formula>, this equation can be written as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e352" xlink:type="simple"/><label>(25)</label></disp-formula>The larger the eigenvalue becomes, the faster learning speed becomes if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e353" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e354" xlink:type="simple"/></inline-formula> have the same sign, or if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e355" xlink:type="simple"/></inline-formula>. This inequality is appropriate if the equality <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e356" xlink:type="simple"/></inline-formula> can be proved, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e357" xlink:type="simple"/></inline-formula> is a positive constant. To prove this equality, let us assume that in the 1st trial after the rotational perturbation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e358" xlink:type="simple"/></inline-formula> is applied, output can be written as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e359" xlink:type="simple"/></inline-formula> because the neural network can generate accurate outputs if there is no perturbation. In this case,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e360" xlink:type="simple"/><label>(26)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e361" xlink:type="simple"/></inline-formula> is a positive constant. Thus, the larger <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e362" xlink:type="simple"/></inline-formula> becomes, the faster learning speed becomes even when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e363" xlink:type="simple"/></inline-formula>; analytical calculations show that neuronal redundancy maximizes learning speed even when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e364" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s4c">
        <title>Fixed non-homogeneous decoder</title>
        <p>When <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e365" xlink:type="simple"/></inline-formula> is fixed and non-homogeneous, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e366" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e367" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e368" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e369" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e370" xlink:type="simple"/></inline-formula>, the necessary and sufficient conditions for maximizing learning speed are given by the following equations:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e371" xlink:type="simple"/><label>(27)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e372" xlink:type="simple"/><label>(28)</label></disp-formula>with neuronal redundancy assumed. Equations (27) and (28) can be satisfied when, for example,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e373" xlink:type="simple"/><label>(29)</label></disp-formula>(shown in <xref ref-type="fig" rid="pcbi-1002348-g003">Figure 3A</xref> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e374" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e375" xlink:type="simple"/></inline-formula>), or<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e376" xlink:type="simple"/><label>(30)</label></disp-formula>(shown in <xref ref-type="fig" rid="pcbi-1002348-g003">Figure 3D</xref> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e377" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e378" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e379" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e380" xlink:type="simple"/></inline-formula>), where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e381" xlink:type="simple"/></inline-formula>. Because the learning rate of motor commands is determined by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e382" xlink:type="simple"/></inline-formula> (see the following section), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e383" xlink:type="simple"/></inline-formula> is determined based on the results of behavioral studies <xref ref-type="bibr" rid="pcbi.1002348-Cheng1">[33]</xref>. We cannot analytically calculate the general class of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e384" xlink:type="simple"/></inline-formula> and the distributions of PDs satisfying equations (27) and (28), but broad classes of those distributions can satisfy these equations because the classes include even asymmetric distributions, e.g., when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e385" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e386" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s4d">
        <title>Learning rule of decoder <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e387" xlink:type="simple"/></inline-formula></title>
        <p>When <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e388" xlink:type="simple"/></inline-formula> is adaptable, this is also adapted to minimize the squared error:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e389" xlink:type="simple"/><label>(31)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e390" xlink:type="simple"/></inline-formula> is set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e391" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e392" xlink:type="simple"/></inline-formula> is a normal Gaussian random variable, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e393" xlink:type="simple"/></inline-formula> is set to 0.1 in the Adaptable Decoder section and 0.05 in the Importance of Neuronal Redundancy section. This learning rule corresponds to back-propagation <xref ref-type="bibr" rid="pcbi.1002348-Rumelhart1">[34]</xref>.</p>
      </sec>
      <sec id="s4e">
        <title>High dimensional tasks</title>
        <p>In the Importance of Neuronal Redundancy section, the neural network generates the output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e394" xlink:type="simple"/></inline-formula>, which is determined by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e395" xlink:type="simple"/><label>(32)</label></disp-formula>for the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e396" xlink:type="simple"/></inline-formula>th trial. An initial value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e397" xlink:type="simple"/></inline-formula> is randomly sampled from the normal Gaussian distribution and divided by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e398" xlink:type="simple"/></inline-formula> for scaling. The input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e399" xlink:type="simple"/></inline-formula> is randomly sampled from the normal Gaussian distribution and is normalized to satisfy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e400" xlink:type="simple"/></inline-formula> to avoid the effect of this value on learning speed. In addition, we used a fixed value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e401" xlink:type="simple"/></inline-formula> because the generalization function (see the following section) strongly depends on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e402" xlink:type="simple"/></inline-formula>, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e403" xlink:type="simple"/></inline-formula>. It should be noted that learning speed does not explicitly depend on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e404" xlink:type="simple"/></inline-formula> because learning speed is determined only by the minimum eigenvalue of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e405" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s4f">
        <title>The generalization function and the update rule for motor commands</title>
        <p>Equation (13) yields the following update rule for motor commands:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e406" xlink:type="simple"/><label>(33)</label></disp-formula>If equations (27) and (28) (or (22) and (23)) are satisfied, equation (33) can be written as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e407" xlink:type="simple"/><label>(34)</label></disp-formula>where the cross term of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e408" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e409" xlink:type="simple"/></inline-formula> determines the generalization function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e410" xlink:type="simple"/></inline-formula>, e.g., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e411" xlink:type="simple"/></inline-formula>, if we define <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e412" xlink:type="simple"/></inline-formula>. We set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e413" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e414" xlink:type="simple"/></inline-formula> to satisfy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e415" xlink:type="simple"/></inline-formula>. It should be noted that equation (34) corresponds to a model for sensorimotor learning that can explain the results of behavioral experiments <xref ref-type="bibr" rid="pcbi.1002348-vanBeers1">[35]</xref>, suggesting that our hypothesis is consistent with the results of behavioral experiments.</p>
        <p>Because the shape of the generalization function depends on the task, we need to confirm the generality of our results with regard to the shape of the generalization function. To simulate various shapes of generalization functions, we used the von-Mises function<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e416" xlink:type="simple"/><label>(35)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e417" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e418" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e419" xlink:type="simple"/></inline-formula> are the precision parameter, the preferred direction of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e420" xlink:type="simple"/></inline-formula>th input unit, and the number of input units, respectively. The normalization factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e421" xlink:type="simple"/></inline-formula> is determined to make <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e422" xlink:type="simple"/></inline-formula> to avoid the influence of this value on the learning speed, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e423" xlink:type="simple"/></inline-formula>. This normalization permits us to investigate the influence of the shape of the generalization function alone on learning speed. The larger the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e424" xlink:type="simple"/></inline-formula>, the sharper the shape of the generalization function becomes. We set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e425" xlink:type="simple"/></inline-formula> to 100 throughout this study.</p>
      </sec>
      <sec id="s4g">
        <title>Numerical simulation procedure</title>
        <p>We conducted 100 baseline trials with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e426" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e427" xlink:type="simple"/></inline-formula> to identify the baseline values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e428" xlink:type="simple"/></inline-formula>. The initial value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e429" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e430" xlink:type="simple"/></inline-formula>, was set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e431" xlink:type="simple"/></inline-formula>. After these trials, 100 learning trials were conducted using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e432" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e433" xlink:type="simple"/></inline-formula>. Learning speed <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e434" xlink:type="simple"/></inline-formula> was calculated by fitting the exponential function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e435" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e436" xlink:type="simple"/></inline-formula>. All the figures denote <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e437" xlink:type="simple"/></inline-formula> which was obtained only in learning trials. The present study calculated learning speed and learning curves by averaging the results of 1000 sets of baseline and learning trials, each set including an identical target sequence that was randomly sampled, and each set using different FD values.</p>
        <p>For all of the statistical tests, we used the Wilcoxon sign rank test. It should be noted that the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e438" xlink:type="simple"/></inline-formula>-value was indicated only if the value was significantly different from 0; no statistically significant differences were detected.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002348.s001" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.s001" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <p><bold>Relationship between learning speed, neuronal redundancy, and adaptable recurrent connections (</bold><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e439" xlink:type="simple"/></inline-formula><bold>).</bold> (A): Learning speed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e440" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e441" xlink:type="simple"/></inline-formula>. The whiter the color, the faster the learning speed. (B): Learning curves obtained when N = 10, 50, or 100 and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e442" xlink:type="simple"/></inline-formula>. These curves show the average values of 1,000 randomly sampled sets of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e443" xlink:type="simple"/></inline-formula>. Error bars represent the standard deviations of the errors. (C): Learning curves obtained when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e444" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e445" xlink:type="simple"/></inline-formula>. These curves and error bars show average values and standard deviations. (D): Variance of the learning curve when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e446" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e447" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e448" xlink:type="simple"/></inline-formula>). These variances are average values from 1,000 randomly sampled sets of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e449" xlink:type="simple"/></inline-formula>.</p>
          <p>(EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002348.s002" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.s002" xlink:type="simple">
        <label>Figure S2</label>
        <caption>
          <p><bold>Relationship between learning speed and neuronal redundancy in the case of a nonlinear neural network (</bold><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e450" xlink:type="simple"/></inline-formula><bold>).</bold> (A): Learning speed when N = 10, 50, 100, and 1000. The bar graphs and error bars depict sample means and standard deviations, both of which are calculated using the results of 1,000 randomly sampled sets of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e451" xlink:type="simple"/></inline-formula> values. (B): Learning curves obtained when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e452" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e453" xlink:type="simple"/></inline-formula>. These curves and error bars show average values and the standard deviations of the errors.</p>
          <p>(EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002348.s003" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.s003" xlink:type="simple">
        <label>Figure S3</label>
        <caption>
          <p><bold>Relationship between learning speed and neuronal redundancy when the neural network includes nonlinear muscle units (</bold><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e454" xlink:type="simple"/></inline-formula><bold>).</bold> (A): The bar graphs and error bars depict sample means and standard deviations, both of which were calculated using the results of 1,000 randomly sampled sets of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e455" xlink:type="simple"/></inline-formula> values. (B): Learning curves obtained when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e456" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e457" xlink:type="simple"/></inline-formula>. These curves and error bars show average values and the standard deviations of the errors.</p>
          <p>(EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002348.s004" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.s004" xlink:type="simple">
        <label>Figure S4</label>
        <caption>
          <p><bold>Relationship between learning speed and neuronal redundancy in the case of weight perturbation and node perturbation (</bold><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e458" xlink:type="simple"/></inline-formula><bold>).</bold> (A): Learning speed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e459" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e460" xlink:type="simple"/></inline-formula>, with weight perturbation as the learning rule. The bar graphs and error bars depict sample means and standard deviations, both of which are calculated using the results of 1,000 randomly sampled sets of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e461" xlink:type="simple"/></inline-formula>. (B): Learning curves obtained when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e462" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e463" xlink:type="simple"/></inline-formula>, with weight perturbation as the learning rule. These curves and error bars show the average values and the standard deviations of the errors. (C): Learning speed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e464" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e465" xlink:type="simple"/></inline-formula>, with node perturbation as the learning rule. The bar graphs and error bars depict sample means and standard deviations, both of which are calculated using the results of 1,000 randomly sampled sets of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e466" xlink:type="simple"/></inline-formula>. (D): Learning curves obtained when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e467" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e468" xlink:type="simple"/></inline-formula>, with node perturbation as the learning rule. These curves and error bars show average values and the standard deviations of the errors.</p>
          <p>(EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002348.s005" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.s005" xlink:type="simple">
        <label>Figure S5</label>
        <caption>
          <p><bold>Relationship between residual error, learning speed, and neuronal redundancy with synaptic decay included (</bold><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e469" xlink:type="simple"/></inline-formula><bold>).</bold> (A): Residual error when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e470" xlink:type="simple"/></inline-formula>. The bar graphs and error bars denote sample means and standard deviations, both of which are calculated using the results of 1,000 randomly sampled sets of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e471" xlink:type="simple"/></inline-formula> values. (B): Learning speed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e472" xlink:type="simple"/></inline-formula>. The bar graphs and error bars depict sample means and standard deviations. (C): Learning curves obtained when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e473" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e474" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e475" xlink:type="simple"/></inline-formula>. These curves and error bars show average values and standard deviations. (D): Residual error when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e476" xlink:type="simple"/></inline-formula>. (E): Learning speed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e477" xlink:type="simple"/></inline-formula>. (F): Learning curve when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e478" xlink:type="simple"/></inline-formula>. (G): Residual error when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e479" xlink:type="simple"/></inline-formula>. (H): Learning speed when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e480" xlink:type="simple"/></inline-formula>. (I): Learning curve when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002348.e481" xlink:type="simple"/></inline-formula>.</p>
          <p>(EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002348.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002348.s006" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <p><bold>Generality of our results.</bold> This file contains the detailed descriptions of <italic>Generality of our results</italic> section.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank D. Nozaki, Y. Sakai, Y. Naruse, K. Katahira, T. Toyoizumi, and T. Omori for their helpful discussions.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002348-Barlow1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barlow</surname><given-names>H</given-names></name></person-group>             <year>2001</year>             <article-title>Redundancy reduction revisited.</article-title>             <source>Network</source>             <volume>12</volume>             <fpage>241</fpage>             <lpage>253</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Olshausen1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name><name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name></person-group>             <year>1996</year>             <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images.</article-title>             <source>Nature</source>             <volume>381</volume>             <fpage>607</fpage>             <lpage>609</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Lewicki1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name></person-group>             <year>2002</year>             <article-title>Efficient coding of natural sounds.</article-title>             <source>Nat Neurosci</source>             <volume>5</volume>             <fpage>356</fpage>             <lpage>363</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Gutnisky1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gutnisky</surname><given-names>D</given-names></name><name name-style="western"><surname>Dragoi</surname><given-names>V</given-names></name></person-group>             <year>2008</year>             <article-title>Adaptive coding of visual information in neural populations.</article-title>             <source>Nature</source>             <volume>452</volume>             <fpage>220</fpage>             <lpage>224</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Salinas1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Salinas</surname><given-names>E</given-names></name></person-group>             <year>2006</year>             <article-title>How behavioral constraints may determine optimal sensory representations.</article-title>             <source>PLoS Biol</source>             <volume>4</volume>             <fpage>2383</fpage>             <lpage>2392</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Li1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>CS</given-names></name><name name-style="western"><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name name-style="western"><surname>Bizzi</surname><given-names>E</given-names></name></person-group>             <year>2001</year>             <article-title>Neuronal correlates of motor performance and motor learning in the primary motor cortex of monkeys adapting to an external force field.</article-title>             <source>Neuron</source>             <volume>30</volume>             <fpage>593</fpage>             <lpage>607</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Rokni1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rokni</surname><given-names>U</given-names></name><name name-style="western"><surname>Richardson</surname><given-names>AG</given-names></name><name name-style="western"><surname>Bizzi</surname><given-names>E</given-names></name><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name></person-group>             <year>2007</year>             <article-title>Motor learning with unstable neural representations.</article-title>             <source>Neuron</source>             <volume>54</volume>             <fpage>653</fpage>             <lpage>666</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Narayanan1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Narayanan</surname><given-names>NS</given-names></name><name name-style="western"><surname>Kimchi</surname><given-names>EY</given-names></name><name name-style="western"><surname>Laubach</surname><given-names>M</given-names></name></person-group>             <year>2005</year>             <article-title>Redundancy and synergy of neuronal ensembles in motor cortex.</article-title>             <source>J Neurosci</source>             <volume>25</volume>             <fpage>4207</fpage>             <lpage>4216</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Bernstein1">
        <label>9</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bernstein</surname><given-names>N</given-names></name></person-group>             <year>1967</year>             <source>The coordination and regulation of movements</source>             <publisher-loc>London</publisher-loc>             <publisher-name>Pergamon</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Gribble1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gribble</surname><given-names>PL</given-names></name><name name-style="western"><surname>Mullin</surname><given-names>LI</given-names></name><name name-style="western"><surname>Cothros</surname><given-names>N</given-names></name><name name-style="western"><surname>Mattar</surname><given-names>A</given-names></name></person-group>             <year>2003</year>             <article-title>Role of cocontraction in arm movement accuracy.</article-title>             <source>J Neurophysiol</source>             <volume>89</volume>             <fpage>2396</fpage>             <lpage>2405</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Latash1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Latash</surname><given-names>ML</given-names></name><name name-style="western"><surname>Scholz</surname><given-names>JP</given-names></name><name name-style="western"><surname>Schoner</surname><given-names>G</given-names></name></person-group>             <year>2002</year>             <article-title>Motor control strategies revealed in the structure of motor variability.</article-title>             <source>Exerc Sport Sci Rev</source>             <volume>30</volume>             <fpage>26</fpage>             <lpage>31</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Thoroughman1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Thoroughman</surname><given-names>KA</given-names></name><name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name></person-group>             <year>1999</year>             <article-title>Electromyographic correlates of learning an internal model of reaching movements.</article-title>             <source>J Neurosci</source>             <volume>19</volume>             <fpage>8573</fpage>             <lpage>8588</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Latash2">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Latash</surname><given-names>ML</given-names></name></person-group>             <year>2000</year>             <article-title>The organization of quick corrections within a two-joint synergy in conditions of unexpected blocking and release of a fast movement.</article-title>             <source>Clin Neurophysiol</source>             <volume>11</volume>             <fpage>975</fpage>             <lpage>987</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Yang1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>JF</given-names></name><name name-style="western"><surname>Scholz</surname><given-names>JP</given-names></name><name name-style="western"><surname>Latash</surname><given-names>ML</given-names></name></person-group>             <year>2007</year>             <article-title>The role of kinematic redundancy in adaptation of reaching.</article-title>             <source>Exp Brain Res</source>             <volume>176</volume>             <fpage>54</fpage>             <lpage>69</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Scott1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Scott</surname><given-names>SH</given-names></name><name name-style="western"><surname>Gribble</surname><given-names>PL</given-names></name><name name-style="western"><surname>Cabel</surname><given-names>DW</given-names></name></person-group>             <year>2001</year>             <article-title>Dissociation between hand motion and population vectors from neural activity in motor cortex.</article-title>             <source>Nature</source>             <volume>413</volume>             <fpage>161</fpage>             <lpage>165</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Kurtzer1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kurtzer</surname><given-names>I</given-names></name><name name-style="western"><surname>Pruszynski</surname><given-names>JA</given-names></name><name name-style="western"><surname>Herter</surname><given-names>TM</given-names></name><name name-style="western"><surname>Scott</surname><given-names>SH</given-names></name></person-group>             <year>2006</year>             <article-title>Nonuniform distribution of reach-related and torque-related activity in upper arm muscles and neurons of primary motor cortex.</article-title>             <source>J Neurophysiol</source>             <volume>96</volume>             <fpage>3220</fpage>             <lpage>3230</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Naselaris1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Naselaris</surname><given-names>T</given-names></name><name name-style="western"><surname>Merchant</surname><given-names>H</given-names></name><name name-style="western"><surname>Amirikian</surname><given-names>B</given-names></name><name name-style="western"><surname>Georgopoulos</surname><given-names>AP</given-names></name></person-group>             <year>2006</year>             <article-title>Large-scale organization of preferred directions in the motor cortex. I. motor cortical hyperacuity for forward reaching.</article-title>             <source>J Neurophysiol</source>             <volume>96</volume>             <fpage>3231</fpage>             <lpage>3236</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Werfel1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Werfel</surname><given-names>J</given-names></name><name name-style="western"><surname>Xie</surname><given-names>X</given-names></name><name name-style="western"><surname>Seung</surname><given-names>S</given-names></name></person-group>             <year>2005</year>             <article-title>Learning curves for stochastic gradient descent in linear feedforward networks.</article-title>             <source>Neural Compt</source>             <volume>17</volume>             <fpage>2699</fpage>             <lpage>2718</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Thoroughman2">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Thoroughman</surname><given-names>KA</given-names></name><name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name></person-group>             <year>2000</year>             <article-title>Learning of action through adaptive combination of motor primitives.</article-title>             <source>Nature</source>             <volume>407</volume>             <fpage>742</fpage>             <lpage>747</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Donchin1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Donchin</surname><given-names>O</given-names></name><name name-style="western"><surname>Francis</surname><given-names>JT</given-names></name><name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name></person-group>             <year>2003</year>             <article-title>Quantifying generalization from trial-by-trial behavior of adaptive systems that learn with basis functions.</article-title>             <source>J Neurosci</source>             <volume>23</volume>             <fpage>9032</fpage>             <lpage>9045</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Krakauer1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Krakauer</surname><given-names>JW</given-names></name><name name-style="western"><surname>Pine</surname><given-names>ZM</given-names></name><name name-style="western"><surname>Ghilardi</surname><given-names>MF</given-names></name><name name-style="western"><surname>Ghez</surname><given-names>C</given-names></name></person-group>             <year>2000</year>             <article-title>Learning of visuomotor transformations for vectorial planning of reaching trajectories.</article-title>             <source>J Neurosci</source>             <volume>20</volume>             <fpage>8916</fpage>             <lpage>892</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Lewicki2">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>             <year>2000</year>             <article-title>Learning Overcomplete Representations.</article-title>             <source>Neural Comput</source>             <volume>12</volume>             <fpage>337</fpage>             <lpage>365</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Hidetoshi1">
        <label>23</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hidetoshi</surname><given-names>N</given-names></name></person-group>             <year>2001</year>             <article-title>Statistical Physics of Spin Glasses and Information Processing: An Introduction.</article-title>             <comment>Oxford University Press</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Bishop1">
        <label>24</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bishop</surname><given-names>CM</given-names></name></person-group>             <year>2006</year>             <article-title>Pattern Recognition and Machine Learning.</article-title>             <comment>Springer</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Capaday1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Capaday</surname><given-names>C</given-names></name><name name-style="western"><surname>Ethier</surname><given-names>C</given-names></name><name name-style="western"><surname>Brizzi</surname><given-names>L</given-names></name><name name-style="western"><surname>Sik</surname><given-names>A</given-names></name><name name-style="western"><surname>van Vreewijk</surname><given-names>C</given-names></name></person-group>             <year>2009</year>             <article-title>On the nature of the intrinsic connectivity of the cat motor cortex: evidence for a recurrent neural network topology.</article-title>             <source>J Neurophysiol</source>             <volume>102</volume>             <fpage>2131</fpage>             <lpage>2141</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Tsodyks1">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name></person-group>             <year>1997</year>             <article-title>The neural code between neocortical pyramidal neurons depends on neurotransmitter release probability.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>94</volume>             <fpage>719</fpage>             <lpage>723</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Seung1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name></person-group>             <year>2003</year>             <article-title>Learning in spiking neural networks by reinforcement of stochastic synaptic transmission.</article-title>             <source>Neuron</source>             <volume>40</volume>             <fpage>1063</fpage>             <lpage>1073</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Fiete1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fiete</surname><given-names>IR</given-names></name><name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name></person-group>             <year>2007</year>             <article-title>Model of birdsong learning based on gradient estimation by dynamic perturbation of neural conductances.</article-title>             <source>J Neurophysiol</source>             <volume>98</volume>             <fpage>2038</fpage>             <lpage>2057</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Lee1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>D</given-names></name><name name-style="western"><surname>Port</surname><given-names>NL</given-names></name><name name-style="western"><surname>Kruse</surname><given-names>W</given-names></name><name name-style="western"><surname>Georgopoulos</surname><given-names>AP</given-names></name></person-group>             <year>1998</year>             <article-title>Variability and correlated noise in the discharge of neurons in motor and parietal areas of the primate cortex.</article-title>             <source>J Neurosci</source>             <volume>18</volume>             <fpage>1161</fpage>             <lpage>1170</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Shadmehr1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name><name name-style="western"><surname>Mussa-ivaldi</surname><given-names>FA</given-names></name></person-group>             <year>1994</year>             <article-title>Adaptive representation of dynamics during learning of a motor task.</article-title>             <source>J Neurosci</source>             <volume>14</volume>             <fpage>3208</fpage>             <lpage>3224</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-CriscimagnaHemminger1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Criscimagna-Hemminger</surname><given-names>SE</given-names></name><name name-style="western"><surname>Donchin</surname><given-names>O</given-names></name><name name-style="western"><surname>Gazzaniga</surname><given-names>MS</given-names></name><name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name></person-group>             <year>2003</year>             <article-title>Learned dynamics of reaching movements generalize from dominant to nondominant arm.</article-title>             <source>J Neurophysiol</source>             <volume>89</volume>             <fpage>168</fpage>             <lpage>176</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Simoncelli1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name><name name-style="western"><surname>Freeman</surname><given-names>WT</given-names></name><name name-style="western"><surname>Adelson</surname><given-names>EH</given-names></name><name name-style="western"><surname>Heeger</surname><given-names>DJ</given-names></name></person-group>             <year>1992</year>             <article-title>Shiftable multiscale transforms.</article-title>             <source>IEEE Trans Info Theory</source>             <volume>38</volume>             <fpage>587</fpage>             <lpage>607</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Cheng1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>S</given-names></name><name name-style="western"><surname>Sabes</surname><given-names>PN</given-names></name></person-group>             <year>2007</year>             <article-title>Calibration of visually guided reaching is driven by error-corrective learning and internal dynamics.</article-title>             <source>J Neurophysiol</source>             <volume>97</volume>             <fpage>3057</fpage>             <lpage>3069</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-Rumelhart1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rumelhart</surname><given-names>DE</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name><name name-style="western"><surname>Williams</surname><given-names>RJ</given-names></name></person-group>             <year>1986</year>             <article-title>Learning representations by backpropagating errors.</article-title>             <source>Nature</source>             <volume>323</volume>             <fpage>533</fpage>             <lpage>536</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002348-vanBeers1">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>van Beers</surname><given-names>RJ</given-names></name></person-group>             <year>2009</year>             <article-title>Motor learning is optimally tuned to the properties of motor noise.</article-title>             <source>Neuron</source>             <volume>63</volume>             <fpage>406</fpage>             <lpage>417</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>