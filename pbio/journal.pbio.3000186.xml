<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id>
<journal-title-group>
<journal-title>PLOS Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pbio.3000186</article-id>
<article-id pub-id-type="publisher-id">PBIOLOGY-D-19-00491</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Diagnostic medicine</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Radiology and imaging</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Visual system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Visual system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Visual system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Value-driven attentional capture enhances distractor representations in early visual cortex</article-title>
<alt-title alt-title-type="running-head">Neural signature of value-driven attentional capture in early visual cortex</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9302-0964</contrib-id>
<name name-style="western">
<surname>Itthipuripat</surname>
<given-names>Sirawaj</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9601-1297</contrib-id>
<name name-style="western">
<surname>Vo</surname>
<given-names>Vy A.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9530-2463</contrib-id>
<name name-style="western">
<surname>Sprague</surname>
<given-names>Thomas C.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Serences</surname>
<given-names>John T.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="aff" rid="aff007"><sup>7</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Learning Institute, King Mongkut’s University of Technology Thonburi, Bangkok, Thailand</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Futuristic Research in Enigmatic Aesthetics Knowledge Laboratory, King Mongkut’s University of Technology Thonburi, Bangkok, Thailand</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Department of Psychology and Center for Integrative and Cognitive Neuroscience, Vanderbilt University, Nashville, Tennessee, United States of America</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Neurosciences Graduate Program, University of California San Diego, La Jolla, California, United States of America</addr-line></aff>
<aff id="aff005"><label>5</label> <addr-line>Department of Psychology, New York University, New York, New York, United States of America</addr-line></aff>
<aff id="aff006"><label>6</label> <addr-line>Department of Psychological and Brain Sciences, University of California Santa Barbara, Santa Barbara, California, United States of America</addr-line></aff>
<aff id="aff007"><label>7</label> <addr-line>Department of Psychology and Kavli Foundation for the Brain and Mind, University of California San Diego, La Jolla, California, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Tong</surname>
<given-names>Frank</given-names>
</name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Vanderbilt University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">itthipuripat.sirawaj@gmail.com</email> (SI); <email xlink:type="simple">jserences@ucsd.edu</email> (JTS)</corresp>
</author-notes>
<pub-date pub-type="epub">
<day>9</day>
<month>8</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>8</month>
<year>2019</year>
</pub-date>
<volume>17</volume>
<issue>8</issue>
<elocation-id>e3000186</elocation-id>
<history>
<date date-type="received">
<day>20</day>
<month>2</month>
<year>2019</year>
</date>
<date date-type="accepted">
<day>17</day>
<month>7</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Itthipuripat et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pbio.3000186"/>
<abstract>
<p>When a behaviorally relevant stimulus has been previously associated with reward, behavioral responses are faster and more accurate compared to equally relevant but less valuable stimuli. Conversely, task-irrelevant stimuli that were previously associated with a high reward can capture attention and distract processing away from relevant stimuli (e.g., seeing a chocolate bar in the pantry when you are looking for a nice, healthy apple). Although increasing the value of task-relevant stimuli systematically up-regulates neural responses in early visual cortex to facilitate information processing, it is not clear whether the value of task-irrelevant distractors influences behavior via competition in early visual cortex or via competition at later stages of decision-making and response selection. Here, we measured functional magnetic resonance imaging (fMRI) in human visual cortex while subjects performed a value-based learning task, and we applied a multivariate inverted encoding model (IEM) to assess the fidelity of distractor representations in early visual cortex. We found that the fidelity of neural representations related to task-irrelevant distractors increased when the distractors were previously associated with a high reward. This finding suggests that value-driven attentional capture begins with sensory modulations of distractor representations in early areas of visual cortex.</p>
</abstract>
<abstract abstract-type="toc">
<p>Task-irrelevant distractors can automatically capture attention when they were previously associated with a high reward. This study shows that these high-valued distractors increased the fidelity of neural representations in retinotopically organized regions of visual cortex, showing that value-driven attentional capture begins with early sensory modulations of distractor representations in human visual system.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000053</institution-id>
<institution>National Eye Institute</institution>
</institution-wrap>
</funding-source>
<award-id>R01-EY025872</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Serences</surname>
<given-names>John T.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000913</institution-id>
<institution>James S. McDonnell Foundation</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name name-style="western">
<surname>Serences</surname>
<given-names>John T.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000011</institution-id>
<institution>Howard Hughes Medical Institute</institution>
</institution-wrap>
</funding-source>
<award-id>International Student Fellowship</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9302-0964</contrib-id>
<name name-style="western">
<surname>Itthipuripat</surname>
<given-names>Sirawaj</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100007275</institution-id>
<institution>Ministry of Science and Technology of Thailand</institution>
</institution-wrap>
</funding-source>
<award-id>Royal Thai Scholarship</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9302-0964</contrib-id>
<name name-style="western">
<surname>Itthipuripat</surname>
<given-names>Sirawaj</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award005">
<funding-source>
<institution>National Science Foundation (US)</institution>
</funding-source>
<award-id>GRFP</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9601-1297</contrib-id>
<name name-style="western">
<surname>Vo</surname>
<given-names>Vy A.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award006">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000053</institution-id>
<institution>National Eye Institute</institution>
</institution-wrap>
</funding-source>
<award-id>F32-EY028438</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9530-2463</contrib-id>
<name name-style="western">
<surname>Sprague</surname>
<given-names>Thomas C.</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This study was supported by NEI R01-EY025872 to JTS (<ext-link ext-link-type="uri" xlink:href="https://nei.nih.gov/" xlink:type="simple">https://nei.nih.gov/</ext-link>), a James S. McDonnell Foundation Scholar Award to JTS (<ext-link ext-link-type="uri" xlink:href="https://www.jsmf.org/" xlink:type="simple">https://www.jsmf.org/</ext-link>), the Howard Hughes Medical Institute International student fellowship to SI (<ext-link ext-link-type="uri" xlink:href="https://www.hhmi.org/developing-scientists/international-student-research-fellowships" xlink:type="simple">https://www.hhmi.org/developing-scientists/international-student-research-fellowships</ext-link>), a Royal Thai Scholarship from the Ministry of Science and Technology Thailand to SI (<ext-link ext-link-type="uri" xlink:href="https://most.go.th/main/en" xlink:type="simple">https://most.go.th/main/en</ext-link>), NSF GRFP to VAV (<ext-link ext-link-type="uri" xlink:href="https://www.nsf.gov/" xlink:type="simple">https://www.nsf.gov/</ext-link>), and NEI F32-EY028438 to TCS (<ext-link ext-link-type="uri" xlink:href="https://nei.nih.gov/" xlink:type="simple">https://nei.nih.gov/</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="1"/>
<page-count count="24"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-08-21</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data files and analysis scripts are available from the OSF database: <ext-link ext-link-type="uri" xlink:href="https://osf.io/ky4jh/" xlink:type="simple">https://osf.io/ky4jh/</ext-link></meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>In most real-world situations, stimuli that are visually salient—such as a camera flash in a theater or a green object in a sea of red—automatically capture attention [<xref ref-type="bibr" rid="pbio.3000186.ref001">1</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref004">4</xref>]. Likewise, distractors that are distinguished only by their value, not their visual salience, also capture visual attention—even on occasions when high-valued distractors are unactionable and irrelevant to current behavioral goals (e.g., seeing a piece of cake on another table, but your waiter tells you it is no longer available) [<xref ref-type="bibr" rid="pbio.3000186.ref005">5</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref010">10</xref>]. In the laboratory, the value associated with an irrelevant distractor interferes with the processing of task-relevant visual information, resulting in increased response times (RTs) and sometimes reduced accuracy in a variety of tasks ranging from simple visual discrimination to more complex scenarios in which the value of multiple competing items must be compared [<xref ref-type="bibr" rid="pbio.3000186.ref005">5</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref008">8</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref010">10</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref017">17</xref>]. Importantly, these behavioral effects of value-based attentional capture are underexpressed and overexpressed in patients with attention-deficit hyperactivity disorder and addiction, respectively [<xref ref-type="bibr" rid="pbio.3000186.ref018">18</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref019">19</xref>]. Although previous work has shown that the value of task-relevant visual information increases neural activity in areas of early visual cortex [<xref ref-type="bibr" rid="pbio.3000186.ref020">20</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref026">26</xref>], it is unclear how the learned value of irrelevant distractors modulates cortical responses in these cortical regions. Thus, here we aimed to examine the involvement of early visual cortex in supporting attentional control guided by the learned value associated with task-irrelevant distractors.</p>
<p>Several theories of reinforcement learning have emphasized the essential role of reward in prioritizing sensory information [<xref ref-type="bibr" rid="pbio.3000186.ref027">27</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref034">34</xref>]. For example, stimuli associated with high reward are thought to better compete for sensory representations such that those stimuli become visually salient and automatically capture attention, even though these highly rewarded stimuli might be physically nonsalient [<xref ref-type="bibr" rid="pbio.3000186.ref007">7</xref>]. Based on this previous work, some have proposed that dopaminergic neurons in the midbrain and in the ventral striatum relay value-related signals to cortical regions within the early visual cortex, resulting in the potentiation of visual representations related to high-reward visual stimuli [<xref ref-type="bibr" rid="pbio.3000186.ref035">35</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref037">37</xref>]. According to this proposal, we hypothesized that the learned value associated with irrelevant distractors should enhance distractor representations in early visual cortex and that this enhancement should be spatially restricted to the distractor locations, much like value-based modulations of task-relevant visual stimuli [<xref ref-type="bibr" rid="pbio.3000186.ref023">23</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref025">25</xref>]. Alternatively, we might expect no value-based modulations or even a reduction in the response to distractors, as recent studies have shown that higher reward can lead to the suppression of distractor-related neural representations (see more details in the <xref ref-type="sec" rid="sec006">Discussion</xref> section; also [<xref ref-type="bibr" rid="pbio.3000186.ref038">38</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref041">41</xref>]).</p>
<p>To test these alternative accounts, we recruited human participants to perform a value-based decision-making task and measured their brain activity in visual cortex using functional magnetic resonance imaging (fMRI). Subjects were required to select 1 of 2 task-relevant options while ignoring a third irrelevant and unactionable distractor that was rendered in a color that had been previously associated with a variable level of reward. We hypothesized that the previously assigned value of the distractor color would modulate evoked responses in early visual cortex and that this reward-based modulation would be specific to the spatial location of the distractor stimulus. To evaluate spatially selective modulations, we used an inverted encoding model (IEM) to reconstruct a representation of each stimulus using activation patterns of hemodynamic responses from retinotopically organized visual areas V1, V2, and V3. We chose this multivariate analysis over the univariate analysis of fMRI data because we were interested in examining how modulations of large-scale activation patterns across entire visual areas supported value-driven attention. The IEM method is useful here because it exploits modulations of all voxels in areas of early visual cortex, including attention-related increases and decreases in the hemodynamic response, as well as shifts in the position of voxel receptive fields [<xref ref-type="bibr" rid="pbio.3000186.ref020">20</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref042">42</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref055">55</xref>]. We found that distractors previously associated with a high value slowed choice RTs. These high-value distractors were also represented with higher fidelity in extrastriate visual areas V2 and V3. Importantly, these value-based modulations of behavior and of neural representations depended on target selection history—these modulations were only observed when participants had previously selected and learned the value of irrelevant distractors. Together, these results suggest that the influence of high-value distractors on attentional capture begins with an early modulation of sensory responses and that this value-driven attentional capture occurs when participants have learned the value associated with the visual feature of the distractor.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>High-valued distractors automatically capture attention</title>
<p>In the present study, we used fMRI to measure activity in retinotopically organized visual areas V1, V2, and V3 while human participants (<italic>N</italic> = 15) performed a 2-alternative value-based decision-making task with changing reward associations [<xref ref-type="bibr" rid="pbio.3000186.ref006">6</xref>] (<xref ref-type="fig" rid="pbio.3000186.g001">Fig 1</xref>). On each trial, 3 stimuli were presented, each rendered in a different color. Two of the stimuli were presented in fixed target locations, and subjects had to choose between them. The third stimulus, termed a “distractor,” was presented in another fixed location that subjects could never select. Participants learned that different rewards (1 or 9 cents) were associated with the colors of visual stimuli presented at the two target locations. Importantly, the distractor was not actionable and was thus completely irrelevant with respect to evaluating the relative value of the two possible targets. Across trials, the colors of the targets and the distractor changed randomly so that the distractor color on a given trial could match the color of a previously selected target that yielded either a low or a high monetary reward. Additionally, the pairings between color and reward changed across miniblocks of 8 trials so that values assigned to different colors could be counterbalanced. Thus, for behavioral and fMRI analyses, we sorted trials based on incentive values assigned to the colors of distractors (i.e., low- or high-valued distractor). The incentive value was always defined. However, a given color may not have been selected on previous trials. Therefore, the current value of the distractor was not always known to the participant. We thus used the selection of previous choices to examine the influence of learning reward contingencies on value-driven attention. To do so, we coded whether the distractor in a given trial was selected as a target in the previous 3 trials (i.e., selected or unselected; see <xref ref-type="sec" rid="sec007">Materials and methods</xref>). Note that we used the previous 3 trials in the main analysis because this data-sorting criterion yielded the most balanced number of trials across different experimental conditions (and we examine different sorting schemes in <xref ref-type="supplementary-material" rid="pbio.3000186.s002">S2 Fig</xref>, detailed below).</p>
<fig id="pbio.3000186.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000186.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Value-based decision-making task.</title>
<p>Participants selected 1 of the 2 target stimuli to learn values associated with their colors while ignoring a task-irrelevant distractor that could never be selected and was thus unactionable. Across trials, the colors of the targets and the distractor changed randomly so that the distractor color on a given trial could match the color of a previously selected target that yielded either a low or a high monetary reward (i.e., low- or high-valued distractor).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000186.g001" xlink:type="simple"/>
</fig>
<p>Overall, subjects selected higher-valued targets more often than lower-valued targets (<xref ref-type="fig" rid="pbio.3000186.g002">Fig 2A</xref>, <italic>p</italic> ≤ 1 × 10<sup>−6</sup>, 2-tailed, resampling test). This indicates that subjects were able to learn the values assigned to the different colors. Next, we fit the choice preference data as a function of differential target value with a cumulative Gaussian function (<xref ref-type="fig" rid="pbio.3000186.g002">Fig 2B</xref>; see a similar fitting procedure in [<xref ref-type="bibr" rid="pbio.3000186.ref006">6</xref>]). We found no effect of distractor value (high − low distractor value) on the standard deviation (or sigma) and the mean (or mu) of the cumulative Gaussian function on trials in which the current distractors were previously selected (<italic>p</italic> = 0.9420 and 0.0784 for sigma and mu, respectively, 2-tailed) or unselected (<xref ref-type="fig" rid="pbio.3000186.g002">Fig 2B</xref>; <italic>p</italic> = 0.5637and 0.8206 for sigma and mu, respectively, 2-tailed). Since we used a 2-parameter cumulative Gaussian model to fit 3 data points for each experimental condition, we conducted an additional analysis in which we used the reduced model that only varied sigma to ensure the reliability of the result. First, a nested F test showed that this reduced model performed as well as the full model in which both sigma and mu were optimized, suggesting that this reduced model is more parsimonious than the full model (F[4, 3] = 0.9643, <italic>p</italic> = 0.4206). Second, we found that fitting the data with this reduced model yielded consistent results in which there was no significant distractor-value modulation on the sigma value for either selected (<italic>p</italic> = 0.9428) or unselected condition (and <italic>p</italic> = 0.5365). The null distractor-value effect in the choice preference data is inconsistent with our previous study using a variant of this value-based learning task, in which we found robust distractor-value modulations on the sigma value of the choice preference function [<xref ref-type="bibr" rid="pbio.3000186.ref006">6</xref>]. We think this is due to the fact that in the current design, the value assignment for each color changed much more frequently than the task design used in our previous study (every 8 versus 36 trials) [<xref ref-type="bibr" rid="pbio.3000186.ref006">6</xref>]. That said, the null distractor effect on the choice preference data is also consistent with a large body of literature demonstrating smaller and more variable distractor-value effects on task accuracy [<xref ref-type="bibr" rid="pbio.3000186.ref011">11</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref056">56</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref057">57</xref>].</p>
<fig id="pbio.3000186.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000186.g002</object-id>
<label>Fig 2</label>
<caption>
<title>High-valued distractors increased RTs.</title>
<p>(A) Choice preference for high-valued targets for different distractor types. CW and CCW targets are targets located clockwise and counterclockwise to the distractor location, respectively. (B) The same choice preference data, overlaid with the best-fit cumulative Gaussian function (see <xref ref-type="table" rid="pbio.3000186.t001">Table 1</xref>). (C) Distractor-value modulation (high − low distractor value) of the standard deviation (or sigma) and the mean (or mu) of the cumulative Gaussian function that explains choice preference in (B) (also see <xref ref-type="table" rid="pbio.3000186.t001">Table 1</xref>). Overall, we observed no distractor-value modulation on choice preference functions: sigma and mu did not change with distractor value in trials in which the current distractor was previously selected or unselected. (D) Unlike choice preference data, we observed a robust distractor-value modulation on RTs. The RT effect was significant only for trials in which the distractor was previously selected. Black *** shows a significant distractor-value modulation compared to 0 with <italic>p</italic> &lt; 0.001 (2-tailed; resampling test). Red * shows a significant difference between trials in which the current distractors were previously selected and unselected with <italic>p</italic> &lt; 0.05 (1-tailed). All error bars show ±1 SEM. CW, clockwise; CCW, counterclockwise; n.s., no significant difference; RT, response time.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000186.g002" xlink:type="simple"/>
</fig>
<p>Although there was no distractor-value modulation on the choice preference data, RTs differed significantly across different distractor types (<xref ref-type="table" rid="pbio.3000186.t001">Table 1</xref>). We observed a significant effect of distractor value (high − low distractor value) on RTs on trials in which the current distractor was previously selected (<xref ref-type="fig" rid="pbio.3000186.g002">Fig 2D</xref>; <italic>p</italic> ≤ 1 × 10<sup>−6</sup>, 2-tailed). However, there was no distractor-value modulation on trials in which the current distractors were previously unselected (<italic>p</italic> = 0.2756, 2-tailed). Moreover, the magnitude of the distractor-value modulation was significantly higher for the current distractor that was previously selected versus unselected (<italic>p</italic> = 0.0102, 1-tailed). These RT results show that the distractor value captures attention, leading to a relative decrease in the speed with which subjects processed task-relevant targets [<xref ref-type="bibr" rid="pbio.3000186.ref005">5</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref008">8</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref013">13</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref017">17</xref>].</p>
<table-wrap id="pbio.3000186.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000186.t001</object-id>
<label>Table 1</label> <caption><title>Cumulative Gaussian best-fit variables describing choice preference data and RTs for different distractor types shown in <xref ref-type="fig" rid="pbio.3000186.g002">Fig 2</xref>.</title></caption>
<alternatives>
<graphic id="pbio.3000186.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000186.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="2">Behavioral Measurements</th>
<th align="center" colspan="4">Distractor Types: Distractor Value and Selection History (Mean ± SEM)</th>
</tr>
<tr>
<th align="left">Low and Unselected</th>
<th align="left">High and Unselected</th>
<th align="left">Low and Selected</th>
<th align="left">High and Selected</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Sigma</td>
<td align="center">23.99 ± 8.01</td>
<td align="center">29.87 ± 9.48</td>
<td align="center">32.66 ± 7.09</td>
<td align="center">33.56 ± 7.81</td>
</tr>
<tr>
<td align="left">Mu</td>
<td align="center">0.85 ± 2.41</td>
<td align="center">0.17 ± 2.80</td>
<td align="center">1.31 ± 1.58</td>
<td align="center">1.46 ± 1.13</td>
</tr>
<tr>
<td align="left">RTs (ms)</td>
<td align="center">600 ± 20</td>
<td align="center">612 ± 15</td>
<td align="center">592 ± 18</td>
<td align="center">643 ± 19</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p>Abbreviation: RT, response time</p></fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="sec004">
<title>The reward history of distractors modulates neural representations in early visual cortex</title>
<p>To examine the influence of the distractor value on spatially specific distractor- and target-related neural representations in early visual cortex, we employed a multivariate analysis of fMRI data—an IEM (Materials and methods; <xref ref-type="fig" rid="pbio.3000186.g003">Fig 3</xref>) [<xref ref-type="bibr" rid="pbio.3000186.ref020">20</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref050">50</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref052">52</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref058">58</xref>]. The IEM exploits the spatial tuning of neuronal populations in visual cortex to reconstruct model-based representations of target and distractor stimuli based on population-level activity measured via fMRI. As expected, we found that these reconstructions peaked at the center of each of the 3 locations (<xref ref-type="fig" rid="pbio.3000186.g004">Fig 4A</xref>; sorted as unselected target, selected target, and distractor). Qualitatively, the reconstructed activation at the distractor location was highest when the distractor colors matched the target colors that had been selected and rewarded with a higher value in the previous trials (i.e., the high-valued and previously selected distractor, the top right of the <xref ref-type="fig" rid="pbio.3000186.g004">Fig 4A</xref>), compared to all the other distractor types.</p>
<fig id="pbio.3000186.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000186.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Quantifying stimulus representations with an IEM.</title>
<p>(A) The IEM was trained using fMRI data from the visuospatial mapping task, in which flickering-checkerboard mapping stimuli were randomly presented at each of 111 locations (center locations shown in blue, red, and yellow dots in the first panels; these dots were not physically presented to participants). We filtered individual stimulus locations using 64 Gaussian-like spatial filters to predict channel responses for each trial. We then use the predicted channel responses and fMRI data of all trials to predict channel weights for each voxel within each visual area. (B) The IEM was tested using fMRI data from the value-based learning task (an independent data set). We inverted the estimated channel weights to compute channel responses within each visual area, resulting in a spatial reconstruction centered at 3 stimulus locations in the value-based learning task. fMRI, functional magnetic resonance imaging; IEM, inverted encoding model.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000186.g003" xlink:type="simple"/>
</fig>
<fig id="pbio.3000186.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000186.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Distractor value boosted the activation of distractor representations in early visual cortex.</title>
<p>(A) Averaged spatial reconstructions of the selected target, unselected target, and distractor based on fMRI activation patterns in early visual areas (collapsed across V1–V3). The data were sorted based on the distractor value (high and low distractor value) and the selection of previous choices (whether the current distractor was previously selected at least once in 3 prior trials: selected and unselected; also see <xref ref-type="sec" rid="sec007">Materials and methods</xref>). Before averaging, reconstructions were rotated so that the positions of each respective stimulus type were in register across subjects. In each color plot, a black dot marks the location of the central fixation, and 3 surrounding dots at 30°, 150°, and 270° polar angle indicate the centers of the selected target, unselected target, and distractor locations, respectively. The bottom panels show difference plots between high- and low-distractor-value conditions. (B) The distractor-value modulation (high − low distractor value) from the reconstruction activation (averaged across black dashed circles in [A]). Overall, we found significant distractor-value modulations in extrastriate visual areas V2 and V3, only in trials in which the current distractor was previously selected. Black ** and *** show significant distractor-value modulations compared to 0 with <italic>p</italic> &lt; 0.01 and <italic>p</italic> &lt; 0.001 (2-tailed). Red * and ** show a significant difference between trials in which the current distractors were previously selected and unselected with <italic>p</italic> &lt; 0.05 and <italic>p</italic> &lt; 0.01 (1-tailed). The statistics computed for different visual areas were corrected using the Holm-Bonferroni method. All error bars show ± 1 SEM. Blue, red, and black dashed circles in (A) represent the spatial extents of unselected targets, selected targets, and distractors, respectively. fMRI, functional magnetic resonance imaging; n.s., no significant difference.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000186.g004" xlink:type="simple"/>
</fig>
<p>To quantify this effect, we computed the mean activation level in the reconstructed stimulus representations over the space occupied by the distractors (<xref ref-type="fig" rid="pbio.3000186.g004">Fig 4A</xref>, see <xref ref-type="sec" rid="sec007">Materials and methods</xref>; also see [<xref ref-type="bibr" rid="pbio.3000186.ref020">20</xref>]). Then, we used a nonparametric resampling method (i.e., resampling subjects with replacement) to evaluate the impact of distractor value (high versus low distractor values) on the mean activation of the distractor representation. We did this separately for trials in which the current distractor had been previously selected or unselected in preceding trials to determine whether distractor-value modulations depended on the selection history associated with the color of the distractor.</p>
<p>First, we analyzed the data averaged across V1–V3 (<xref ref-type="fig" rid="pbio.3000186.g004">Fig 4B</xref>). We found a significant distractor-value modulation (high &gt; low value) for the distractor that was previously selected (<italic>p</italic> = 1 × 10<sup>−3</sup>, 2-tailed) but a null result for the distractor that was previously unselected (<italic>p</italic> = 0.4956, 2-tailed, resampling test). We directly evaluated this effect and found that selection history significantly increased distractor-value modulation (<italic>p</italic> = 0.0243, 1-tailed, resampling test). We then repeated these tests separately for individual visual areas. We found significant distractor-value modulations for the previously selected distractor in extrastriate visual areas V2 and V3 (<italic>p</italic> = 0.0011 and <italic>p</italic> = 0.0052, passing the Holm-Bonferroni-corrected thresholds of 0.0167 and 0.025, respectively, 2-tailed) but not in the primary visual cortex V1 (<italic>p</italic> = 0.3318, 2-tailed). In V2 and V3, we confirmed that selection history had a significant effect on distractor-value modulation (<italic>p</italic> = 0.0086 and <italic>p</italic> = 0.0374, respectively, 1-tailed). Similar to the data averaged across V1–V3, there was no significant distractor-value modulation for the previously unselected distractors in any visual area (<italic>p</italic> = 0.2031, <italic>p</italic> = 0.6263, and <italic>p</italic> = 0.9230, for V1, V2, and V3, respectively, 2-tailed). In sum, we used an IEM to evaluate spatially specific representations of behaviorally irrelevant stimuli with an associated reward history. We found that the value associated with irrelevant visual features is encoded in spatially specific activation in early visual areas V2 and V3.</p>
<p>To address whether the distractor-value modulations were driven by knowledge about the value associated with a given color, we split trials into early and late phases based on trial position relative to the start of each miniblock (in which the value–color assignments changed). If value learning matters, we should see robust distractor-value modulations only in the late phase but not in the early phase of each miniblock. Consistent with this prediction, we found a significant main effect of learning on distractor-value modulations in early visual cortex (late &gt; early phases; <italic>p</italic> = 0.0167, 2-tailed) (see <xref ref-type="supplementary-material" rid="pbio.3000186.s001">S1 Fig</xref>). This learning effect was driven by a significant distractor-value modulation in the late phase and only on trials in which the colors of the distractors matched the colors of the previously selected targets (i.e., selected: <italic>p</italic> = 0.0038, passing a Holm-Bonferroni threshold of 0.0125, 2-tailed). There was no significant distractor-value modulation in the late phase of the miniblocks on trials in which the color of the distractors did not match the color of the previously selected targets (i.e., unselected: <italic>p</italic> = 0.3844, 2-tailed). Importantly, there was also no significant distractor-value modulation in the early phase for either of the two distractor types (<italic>p</italic> = 0.1976 and 0.1800 for selected and unselected, respectively, 2-tailed). The null result for the early phase also speaks against the contribution of reward-based priming; as priming effects should rely only on the most recent choices, we should have seen distractor-value modulations regardless of the amount of learning that subjects acquired in the miniblock.</p>
<p>To further test the possibility of priming effects influencing our results, we conducted an auxiliary analysis in which we sorted trials by coding whether the current distractor was selected immediately in the previous trial or was selected at least once in the 2 or 3 previous trials. To ensure that trials included in these different data-sorting approaches came from a similar range of trial positions relative to the start of each miniblock, we only included trials between the fourth and the eighth (i.e., the last) trials of each miniblock. We found that distractors that were selected on the immediately preceding trial did not induce a significant distractor-value modulation in the data (collapsed across V1–V3, <italic>p</italic> = 0.1367, 2-tailed). This suggests that distractor-value modulations were unlikely to be driven entirely by reward-mediated priming effects (<xref ref-type="supplementary-material" rid="pbio.3000186.s002">S2 Fig</xref>, left). However, when we sorted trials based on past selections over the previous 2 and 3 trials, we observed a significant distractor-value modulation (<italic>p</italic> = 0.013 and <italic>p</italic> &lt; 0.001 for the data sorted based on the 2 and 3 previous trials passing Holm-Bonferroni thresholds of 0.025 and 0.0167, respectively). Since these two data sorting approaches included trials in which the same color targets could be selected more than once, these results emphasize the importance of learning the reward-color contingencies in producing value-driven modulations as opposed to reward-mediated priming effects. Note that qualitatively similar, albeit weaker, results were observed in individual visual areas V1–V3 (see <xref ref-type="supplementary-material" rid="pbio.3000186.s002">S2 Fig</xref>).</p>
</sec>
<sec id="sec005">
<title>Target selection and target value are encoded in early visual cortex</title>
<p>As shown in <xref ref-type="fig" rid="pbio.3000186.g003">Fig 3A</xref>, stimulus representations were generally higher for selected targets compared to unselected targets. To quantify this effect, we computed the mean activation level in the reconstructed stimulus representations over the space occupied by the selected and unselected targets (<xref ref-type="fig" rid="pbio.3000186.g005">Fig 5A</xref>). For the data collapsed across V1–V3, we observed a significant target selection modulation (selected &gt; unselected targets: <italic>p</italic> = 0.0011 for data collapsed across distractor types; <italic>p</italic> = 0.0642, 0.0003, 0.0228, and 0.0022 for low-valued and unselected, high-valued and unselected, low-valued and selected, and high-valued and selected distractors, with the Holm-Bonferroni-corrected thresholds of 0.05, 0.0125, 0.025, and 0.0167, respectively, 2-tailed). These target-selection modulations were significant in all visual areas (<italic>p</italic> = 0.0189, 4.600 × 10<sup>−4</sup>, and 5.600 × 10<sup>−4</sup>, V1, V2, and V3, respectively; Holm-Bonferroni-corrected, 2-tailed).</p>
<fig id="pbio.3000186.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000186.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Target-selection modulations in early visual areas.</title>
<p>(A) The difference between the selected and unselected target reconstruction activation for different target types. The activation values were obtained from averaging the reconstruction activation over circular spaces spanning the spatial extents of target stimuli (red and blue dashed circles in <xref ref-type="fig" rid="pbio.3000186.g004">Fig 4A</xref>). The data in (A) were collapsed across visual areas. (B) The same data as (A) but plotted separately for different target-value conditions and for different visual areas. ** and *** indicate significant target-selection modulations compared to 0 with <italic>p</italic> &lt; 0.01 and &lt; 0.001, respectively (2-tailed). <sup>++</sup>Significant difference across visual areas V1 and V3. Statistics in (B) were corrected for multiple comparisons with the Holm-Bonferroni method. All subfigures are plotted with ±1 SEM. val, valued.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000186.g005" xlink:type="simple"/>
</fig>
<p>Next, we evaluated the impact of distractor value on the differential activity between selected and unselected targets. We found no influence of distractor value on target representations (high- versus low-valued distractors) on trials in which the current distractor was previously selected (<italic>p</italic> = 0.2303, 2-tailed) or on trials in which the current distractor was unselected (<italic>p</italic> = 0.4463, 2-tailed). Similar null results were also observed when the data were analyzed separately in V1, V2, and V3 (<italic>p</italic> = 0.1639–0.8710 and 0.0744–0.9419 for the selected and unselected conditions, 2-tailed). These are consistent with the null distractor-value effects on the choice preference data (<xref ref-type="fig" rid="pbio.3000186.g002">Fig 2A and 2B</xref>).</p>
<p>Previous studies have reported that the relative value of targets is encoded in early visual cortex [<xref ref-type="bibr" rid="pbio.3000186.ref023">23</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref025">25</xref>]. To test this, we analyzed the target-selection modulation data both when the selected and unselected targets had the same value (i.e., selected = unselected targets) and when the selected target had a higher value compared to the unselected target (i.e., selected &gt; unselected targets). As shown in <xref ref-type="fig" rid="pbio.3000186.g005">Fig 5B</xref>, we found significant target-selection modulations only when the selected targets had a higher value compared to the unselected targets in all visual areas (<italic>p</italic> = 0.0055, 4 × 10<sup>−6</sup>, and 1 × 10<sup>−6</sup>, passing the Holm-Bonferroni-corrected thresholds of 0.0125, 0.0100, and 0.0083 for V1, V2, and V3, respectively, 2-tailed), but no significant target modulations when selected and unselected targets had the same value (<italic>p</italic> = 0.0437–0.0756, which did not pass the Holm-Bonferroni-corrected threshold of 0.0167, 2-tailed). In addition, on trials in which participants selected the higher-valued target, the target-selection effect was significantly stronger in V3 than in V1 (<italic>p</italic> = 0.0021, passing the Holm-Bonferroni-corrected threshold of 0.0167, 2-tailed). However, there was not a significant difference between V3 and V2 (<italic>p</italic> = 0.1165, 2-tailed) or between V2 and V1 (<italic>p</italic> = 0.1274, 2-tailed). Taken together with the previous section, our results show that the encoding of target value and distractor value can occur in parallel in early areas of visual cortex.</p>
</sec>
</sec>
<sec id="sec006" sec-type="conclusions">
<title>Discussion</title>
<p>Visual stimuli that are not physically salient but are paired with high-reward values are known to automatically capture attention, even when those stimuli are behaviorally irrelevant and unactionable [<xref ref-type="bibr" rid="pbio.3000186.ref005">5</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref009">9</xref>]. Although a recent study reported that neural responses associated with distractors scale with the learned value [<xref ref-type="bibr" rid="pbio.3000186.ref035">35</xref>], it is unclear whether these modulations were tied specifically to the location of the distractor and whether distractor response modulations led to attenuated target responses. Using a multivariate spatial reconstruction analysis of fMRI data, we show here that retinotopically organized regions in extrastriate visual areas V2 and V3 are modulated by the reward history of irrelevant visual stimuli. Importantly, the spatial reconstructions of these stimuli indicate that reward-based modulations occur precisely at the location of the distractor and that there is little associated impact on responses to simultaneously presented targets. Taken together, our results suggest that value-driven attentional capture may begin with early value-based modulation of sensory responses evoked by the distractor.</p>
<p>Importantly, these attentional capture effects on task-irrelevant distractors suggest that value-driven attention is not simply due to an increase in the non-stimulus-specific arousal levels induced by the reinforcement process, consistent with the finding that perceptual learning can occur even when task-irrelevant subthreshold stimuli are paired with rewards [<xref ref-type="bibr" rid="pbio.3000186.ref036">36</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref059">59</xref>]. The fact that we observed value-based modulations that were spatially specific to the distractor location strongly supports this argument.</p>
<p>At first glance, our results seem to contradict several recent studies that observed a reward-based suppression of neural representations associated with distractors in sensory cortices [<xref ref-type="bibr" rid="pbio.3000186.ref038">38</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref040">40</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref060">60</xref>]. However, in many of these studies, the reward manipulation was not specifically tied to the distractor, and distractor suppression was inferred based on modulations of neural responses related to the task-relevant targets [<xref ref-type="bibr" rid="pbio.3000186.ref039">39</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref040">40</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref060">60</xref>]. Thus, these recent results are actually in line with the current data, in which the reconstruction activation of selected targets was higher than unselected targets and low-valued distractors. That said, another recent study reported that a high-valued distractor induced weaker neural representations in early visual cortex compared to the low-valued distractor [<xref ref-type="bibr" rid="pbio.3000186.ref038">38</xref>]. However, they found that this was true only when the distractor was physically more salient than the target in a perceptually demanding task [<xref ref-type="bibr" rid="pbio.3000186.ref038">38</xref>]. They reasoned that the high sensory competition between low-salience targets and high-salience distractors required top-down attentional suppression of the high-valued distractors [<xref ref-type="bibr" rid="pbio.3000186.ref038">38</xref>]. However, this was not the case in the current experiment, in which all stimuli were suprathreshold and matched for luminance. Thus, in the context of our experimental design, we did not find evidence for distractor suppression at either the behavioral or neural level.</p>
<p>In the present study, we showed that an association between reward and color can induce neural modulations in early visual areas V1–V3. This is somewhat surprising, given evidence that neurons in higher visual areas (such as V4, V8, VO1, and inferior temporal cortex) are selectively tuned to chromatic information and responsible for processing color-based top-down modulations [<xref ref-type="bibr" rid="pbio.3000186.ref048">48</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref058">58</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref061">61</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref065">65</xref>]. We suggest that value-based modulations in early visual areas may reflect top-down feedback signals from these higher visual areas, where the association between color and reward might be computed. Related to this idea, we found significant distractor-value modulations only in extrastriate visual cortex but not in V1, which may reflect a reentrant signal backpropagated to earlier visual areas. The more robust effects in higher visual areas were also observed for the task-relevant target reconstructions, consistent with previous reports [<xref ref-type="bibr" rid="pbio.3000186.ref020">20</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref043">43</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref050">50</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref052">52</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref066">66</xref>]. Overall, this pattern of data supports theoretical frameworks suggesting that visual cortex operates as a priority map that indexes the rank-ordered importance of different sensory inputs [<xref ref-type="bibr" rid="pbio.3000186.ref020">20</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref023">23</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref025">25</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref039">39</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref040">40</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref050">50</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref052">52</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref067">67</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref068">68</xref>]. That said, the assumption that the color–reward association can only be computed in higher visual areas has to be considered with caution, because previous studies have found that reward learning can shape neural activity in early visual cortex [<xref ref-type="bibr" rid="pbio.3000186.ref024">24</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref025">25</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref069">69</xref>], and others have also found that primary and extrastriate visual areas contain neuronal populations with an inhomogeneous spatial distribution of color selectivity [<xref ref-type="bibr" rid="pbio.3000186.ref042">42</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref070">70</xref>].</p>
<p>It is possible that the voluntary allocation of feature-based attention could be one selection mechanism that is guided by learned value. For instance, as subjects learned the value associated with each color, they might have selectively attended to higher-valued colors as each miniblock progressed. Since feature-based attention has been shown to operate globally across the visual scene, the prioritization of higher-valued colors could in turn enhance the neural representations of visual stimuli rendered in those colors even when they appeared outside the focus of attention [<xref ref-type="bibr" rid="pbio.3000186.ref070">70</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref078">78</xref>]. This global effect of feature-based attention could induce attentional capture to the distractor location when the feature of the distractor matches the prioritized feature in the top-down set—akin to classic contingent capture effects [<xref ref-type="bibr" rid="pbio.3000186.ref079">79</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref081">81</xref>]. Indeed, there has been an on-going debate about whether value-driven attention should be considered as another form of top-down attention [<xref ref-type="bibr" rid="pbio.3000186.ref009">9</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref082">82</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref086">86</xref>]. However, value-driven attention capture has many characteristics that are distinct from top-down attention [<xref ref-type="bibr" rid="pbio.3000186.ref083">83</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref085">85</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref087">87</xref>]. For example, value-driven capture can happen involuntarily and can be detrimental to behavioral goals, two effects that were demonstrated via the effect of distractor value on RTs in the present study. Therefore, even though value-based modulations of distractor-related neural representations might involve some aspects of top-down feature-based attention, we believe that these modulations reflect involuntary value-driven attentional capture. Also note that although some subjects might choose to voluntarily search for a color previously associated with a high reward, this seems unlikely because this strategy would hurt overall performance and slow the process of evaluating the value of the two relevant targets, as the targets will not always be rendered in the color that the subject is searching for.</p>
<p>Recently, it has been suggested that value-driven attentional capture is valence-free because both monetary gain and loss could produce similar attentional capture effects on RTs [<xref ref-type="bibr" rid="pbio.3000186.ref088">88</xref>]. Based on this result, one might speculate that value-based modulations on the distractor-related neural representations shown here could potentially be independent of valence. However, a recent study has suggested that the value-based modulations of object-based stimulus representations in object-selective cortex depend on valence [<xref ref-type="bibr" rid="pbio.3000186.ref060">60</xref>]. That said, as discussed earlier, in this study monetary gain and loss were not tied to the distractor stimulus itself [<xref ref-type="bibr" rid="pbio.3000186.ref060">60</xref>]; thus, it is hard to conclude from this study alone whether value-based modulations on the distractor-related neural representations are valence-free. Future studies could employ our experimental approach in combination with the manipulation of monetary gain and loss to address this question further.</p>
<p>In summary, we demonstrate that the learned value of irrelevant distractors automatically captures attention and that this interferes with the processing of relevant visual information. This value-driven attentional capture results in increased RTs and heightened distractor representations in retinotopically organized areas of extrastriate visual cortex. Together, our findings suggest that value-driven attentional capture begins with early sensory modulations of distractor representations in visual cortex. Moreover, the modulations of both relevant targets and irrelevant distractors support a recent reframing of the classic dichotomy between bottom-up and top-down biasing factors in favor of a trichotomy that emphasizes a crucial role of value learning on the processing of relevant and irrelevant visual information [<xref ref-type="bibr" rid="pbio.3000186.ref009">9</xref>].</p>
</sec>
<sec id="sec007" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec008">
<title>Ethics statement</title>
<p>Participants were recruited from the University of California, San Diego (UCSD), community, and all participants provided written informed consent as required by the local institutional review board at UCSD (IRB# 081318).</p>
</sec>
<sec id="sec009">
<title>Participants</title>
<p>Sixteen neurologically healthy human observers with normal color vision and normal or corrected-to-normal acuity participated in the present study. They then completed 1 scanning session of the main experiment and 1 or 2 sessions of retinotopic mapping scans. Participants were compensated $20 per hour in the scanner, with additional monetary rewards that scaled with their behavioral performance in the value-based learning task (mean $13.13, SD 0.74). Data from 1 subject were excluded because of excessive movement artifacts during the retinotopy scans (&gt;3 mm movement in more than half of the scans), leaving a total of 15 participants in the final analysis (age range 20–34 years old, mean age = 24.6 years ± 4.29 SD).</p>
</sec>
<sec id="sec010">
<title>Stimuli and tasks</title>
<p>Visual stimuli were rear-projected onto on a 115-cm-wide flat screen placed approximately 440 cm from the participant’s eyes at the foot of the scanner bore using an LCD projector (1,024 × 768, 60 Hz, with a gray background, luminance = 8.68 cd/m<sup>2</sup>). The behavioral paradigms were programmed and presented via a laptop running Windows XP using MATLAB (Mathworks, Natick, MA, United States) and the Psychophysics Toolbox [<xref ref-type="bibr" rid="pbio.3000186.ref089">89</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref090">90</xref>].</p>
</sec>
<sec id="sec011">
<title>Value-based decision-making task</title>
<p>We adopted a value-based decision-making task that we recently used to show a robust effect of distractor reward history on behavior [<xref ref-type="bibr" rid="pbio.3000186.ref006">6</xref>]. Each block started with an instruction period telling participants the locations of the two targets and the location of the irrelevant distractor. The position of each stimulus was indicated by different letter strings located inside three 3 circular placeholders equally spaced from one another (120° polar angle apart with an eccentricity of 3.02° visual angle; <xref ref-type="fig" rid="pbio.3000186.g001">Fig 1</xref>). The placeholders remained visible for the entire run so that participants knew the precise target and distractor locations. The instruction period was followed by experimental trials in which 3 physically isoluminant checkerboard stimuli of different colors were presented (black paired with red, green, and blue; radius of 1.01° visual angle; spatial frequency of 1.98 cycles per degree visual angle). The stimuli were flickered on–off at 7.5 Hz for 1 s.</p>
<p>Participants were instructed to choose 1 of the 2 targets to maximize their reward and were told that the reward value associated with each color changed across the course of the scan. The reward values associated with each stimulus color were changed every 8 trials (a miniblock). Subjects were not explicitly informed about the length of this miniblock, but they were told that reward–color associations would change dynamically across a small chunk of trials. All 8 possible combinations of the 3 colors and 2 reward values (1 and 9 cents) were presented in each miniblock. The color assignments to each target and distractor stimulus were also counterbalanced within each miniblock. Trial order was pseudorandomized so that the colors of the visual stimuli at 3 stimulus locations swapped in an unpredictable fashion. The assignment of different values to each color was also randomized so that changes in color–reward associations were unpredictable.</p>
<p>Participants were instructed to choose 1 of the 2 targets using 2 fingers on the right hand, as indicated in a diagram displayed before the run started (<xref ref-type="fig" rid="pbio.3000186.g001">Fig 1</xref>). Importantly, the distractor could never be chosen and was thus choice-irrelevant. After a 1.25-s delay following the offset of the stimulus array, participants received visual feedback indicating the value associated with the chosen target color (“1” or “9”; feedback duration = 0.25 s). If a response was not given before the stimulus offset, they would receive a letter “M” (“miss”) to indicate that no reward was earned on that trial. In a random 20% of trials, rewards were withheld to encourage participants to explore and learn the value of each color (done independently for each of the two targets). In these trials, “0” cents were given, indicating that participants received no reward. The feedback period was followed by a blank intertrial interval with a central fixation for 1.5 s.</p>
<p>Participants completed 6 total blocks with the distractor location remaining stable for 2 consecutive blocks to ensure that participants knew the exact position of the distractor stimulus. Across all blocks the distractor location was counterbalanced across the 3 possible stimulus positions. Each block lasted 4 min 57 s and contained 48 experimental trials and 20 pseudorandomly interleaved null trials. There was a blank period of 9 s at the end of each block. We counterbalanced stimulus configurations across participants to ensure our results were not influenced by any spatial bias. To sample data from the entire circular space across subjects, the stimulus arrays were rotated by 30° polar angle to form 4 configurations (15°-135°-255°, 45°-165°-285°, 75°-195°-315°, and 105°-225°-345°), and these 4 configurations were counterbalanced across subjects. Each subject viewed 1 of these 4 configurations for their entire scanning session.</p>
</sec>
<sec id="sec012">
<title>Visuospatial mapping task</title>
<p>Participants also completed 4–7 blocks of a visuospatial mapping task (1 completed 4 blocks, 1 completed 7 blocks, and the rest completed 6 blocks). The data from this task were then used as an independent data set to train an IEM that was used to reconstruct spatial representations of the targets and distractors in the value-based learning task (see the fMRI analysis section below for more details). Participants were instructed to fixate centrally and to covertly attend to a checkerboard stimulus rendered at 100% Michelson contrast that pseudorandomly appeared at different locations on the screen (3-s duration; the same size, spatial frequency, and flicker frequency as the stimulus in the value-based learning task). The participant’s task was to detect a rare and brief dimming in contrast (19.57% target trials; 0.5-s duration; occurring between 0.5 and 2 s after stimulus onset). On each trial, the checkerboard stimulus was presented at 1 of 37 locations on a triangular grid (1.50° visual angle between vertices), covering a visual space that overlapped with the stimulus locations in the value-based learning task (the first panel in <xref ref-type="fig" rid="pbio.3000186.g003">Fig 3A</xref>). To smoothly cover the entire circular space, we randomly rotated the entire triangular grid around its center by 0°, 20°, or 40° polar angle across different runs (blue, yellow, and red dots in the first panel in <xref ref-type="fig" rid="pbio.3000186.g003">Fig 3A</xref>), so there were 111 different stimulus locations in total (see similar methods in [<xref ref-type="bibr" rid="pbio.3000186.ref020">20</xref>]). On each run, there were a total of 37 nontargets (1 repeat per location) and 9 targets. Target locations were pseudorandomly drawn from the 37 locations (never repeated within each block). The magnitude of the contrast change was adjusted across trials so that accuracy was at approximately 76% (mean hit = 77.95%, SD = 12.23%). Each stimulus presentation was followed by an intertrial interval of 2–5 s (uniformly distributed). We pseudorandomly interleaved 10 null trials and included a blank period of 8.2 s at the end of the block. Each block lasted 6.28 min.</p>
</sec>
<sec id="sec013">
<title>Behavioral analysis</title>
<p>We first sorted trials from the main value-based decision-making task based on target selection (i.e., target type: selected and unselected), target value (low and high value), distractor value based on previous target rewards associated with the color of the distractor (low and high value), and selection history (i.e., whether the distractor was previously unselected or selected at least once in 3 preceding trials). We chose the 3 previous trials as the analysis window because it yielded the most balanced number of trials between individual conditions. Note that because of the boundary between miniblocks (every 8 trials in which value–color assignments were the same), we could only go back 1 and 2 trials for the second and third trials, respectively. We excluded data from the first trial of every 8 trials in each miniblock to reduce the spillover effect from different sets of value–color assignments.</p>
<p>Next, we examined subjects’ choice preference. To do so, we labeled targets located clockwise (CW) and counterclockwise (CCW) to the distractor CW and CCW targets and computed the probability that participants chose CW over CCW targets and plotted as a function of CW target value and CCW target value (<xref ref-type="fig" rid="pbio.3000186.g002">Fig 2A</xref>). Next, we plotted the choices as a function of differential target value (CW − CCW) separately for different distractor values and fit individual subjects’ data with the cumulative Gaussian function (<xref ref-type="fig" rid="pbio.3000186.g002">Fig 2B</xref>). Specifically, we estimated the mean (or mu) and the standard deviation (or sigma) of the cumulative Gaussian function that best fit the choice preference data derived from different distractor values (see <xref ref-type="table" rid="pbio.3000186.t001">Table 1</xref> for mean and SEM; [<xref ref-type="bibr" rid="pbio.3000186.ref006">6</xref>]). To test distractor-value modulations on these values, we computed the bootstrap distribution of the difference in these values between the high- and low-distractor-value conditions (i.e., resampling subjects with replacement for 100,000 iterations) and calculated the percentage of values in this distribution that were larger or smaller than 0 to yield a 2-tailed <italic>p</italic>-value. We performed this statistical analysis separately for previously selected and unselected distractors (see above). Note that we use the 2-parameter cumulative Gaussian model that varied sigma and mu to be consistent with a previous study by our group, even though there were only 3 data points for each experimental condition [<xref ref-type="bibr" rid="pbio.3000186.ref006">6</xref>]. To ensure that the fitting procedure provided reliable results, we also used a variant of the model in which we only optimized sigma for each experimental condition while fixing mu at 0. We then did the same resampling analysis on sigma obtained from this reduced model to test the effect of distractor value. We compared the performance of this reduced model with the full model (above) using the nested F test (see [<xref ref-type="bibr" rid="pbio.3000186.ref091">91</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref093">93</xref>]). To do so, we compared the R<sup>2</sup> values between these two models using the following equation:
<disp-formula id="pbio.3000186.e001">
<alternatives>
<graphic id="pbio.3000186.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000186.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="true">/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <italic>R</italic><sub><italic>full</italic></sub><sup><italic>2</italic></sup> and <italic>R</italic><sub><italic>red</italic></sub><sup><italic>2</italic></sup> were obtained from the best fits of the full and reduced models, respectively. <italic>Df</italic><sub><italic>1</italic></sub> is the number of values in the full model (8 free parameters: 4 sigma values and 4 mu values for high/low-valued previously selected/unselected distractors) minus the number of the values in the reduced model (4 sigma values for different distractor types: high/low-valued previously selected/unselected distractors). <italic>Df</italic><sub><italic>2</italic></sub> is the number of observations (3 differential target values × 4 distractor types) minus the number of the free parameters in the full model minus 1. The <italic>F</italic> distribution was then used to estimate the probability that the full model differed significantly from the reduced model.</p>
<p>Finally, we examined the effect of distractor value on RTs. First, we computed the mean RTs across different distractor values for individual subjects. Then, we computed the bootstrap distribution of the RT difference between the high and low distractor-value conditions (i.e., resampling subjects with replacement for 100,000 iterations) and calculated the percentage of values in this distribution that were larger or smaller than 0 (a 2-tailed <italic>p</italic>-value). We performed this statistical analysis separately for previously selected and unselected distractors. We then compared whether the effect of distractor value was significantly larger in the selected condition than in the unselected condition by a similar procedure that compared the two bootstrap distributions. Since we only observed significantly larger RT differences for previously selected targets, we knew the expected direction of the effect and therefore computed a 1-tailed <italic>p</italic>-value.</p>
</sec>
<sec id="sec014">
<title>fMRI analysis</title>
<sec id="sec015">
<title>fMRI acquisition</title>
<p>All MRI data were acquired on a GE 3T MR750 scanner at the Keck Center for Functional Magnetic Resonance Imaging (CFMRI) at UCSD. Unless otherwise specified, all data were collected using a 32-channel head coil (Nova Medical). We acquired functional data using a multiband echo-planar imaging (EPI) protocol (Stanford Simultaneous Multi-Slice sequence). We acquired 9 axial slices per band at a multiband factor of 8, for 72 total slices (2 × 2 × 2 mm<sup>3</sup> voxel size; 800-ms TR; 35-ms TE; 35° flip angle; 104 × 104 cm matrix size). Prior to each functional scan, 16 TRs were acquired as reference images for image reconstruction. Raw k-space data were reconstructed into NIFTI format image files on internal servers using scripts provided by CFMRI. In each session, we also acquired forward and reverse phase-encoding blips to estimate the susceptibility off-resonance field [<xref ref-type="bibr" rid="pbio.3000186.ref094">94</xref>]. This was used to correct EPI signal distortion using FSL topup [<xref ref-type="bibr" rid="pbio.3000186.ref095">95</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref096">96</xref>], the results of which were submitted to further preprocessing stages described below. In each session, we also acquired an accelerated anatomical using parallel imaging (GE ASSET on a FSPGR T1-weighted sequence; 1 × 1 × 1 mm<sup>3</sup> voxel size; 8,136-ms TR; 3,172-ms TE; 8° flip angle; 172 slices; 1-mm slice gap; 256 × 192 cm matrix size). This same-session anatomical was coregistered to the functional data. It was also coregistered to a high-resolution anatomical from the retinotopic mapping session(s).</p>
</sec>
<sec id="sec016">
<title>Retinotopic mapping</title>
<p>To identify regions of interest (ROIs) in early visual cortex, we used a combination of retinotopic mapping methods. Individual participants completed meridian mapping (1–2 blocks of approximately 5 min), during which they saw flickering checkerboard “bowties” along the horizontal and vertical meridians while fixating centrally. They also completed several scans of a polar angle mapping task (4–6 blocks of about 6 min), during which participants covertly attended to a rotating checkerboard wedge and detected brief contrast changes (see details in [<xref ref-type="bibr" rid="pbio.3000186.ref020">20</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref052">52</xref>]). We identified retinotopically organized regions of visual areas V1, V2, and V3 using a combination of retinotopic maps of visual field meridians and polar angle preferences for each voxel in these visual areas and concatenated left and right hemispheres as well as dorsal and ventral aspects of individual areas [<xref ref-type="bibr" rid="pbio.3000186.ref097">97</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref098">98</xref>]. Visual area borders were drawn on an inflated cortical surface created from a high-resolution anatomical scan (FSPGR T1-weighted sequence; 1 × 1 × 1 mm<sup>3</sup>; 8,136-ms TR; 3,172-ms TE; 8° flip angle; 172 slices; 1-mm slice gap; 256 × 192 cm matrix size) collected with an 8-channel head coil.</p>
</sec>
<sec id="sec017">
<title>fMRI data preprocessing</title>
<p>Analysis was performed in BrainVoyager 20.2 (Brain Innovation, Maastricht, the Netherlands) supplemented with custom analysis scripts written in MATLAB R2016a (Mathworks, Natick, MA, USA). Using the distortion-corrected images, we first performed slice-time correction, affine motion correction, and temporal high-pass filtering. Then, the functional data were coregistered to the same-session anatomical and transformed to Talairach space. Each voxel’s time course was <italic>z</italic>-scored within each run. We then built a design matrix with individual trial predictors convolved with a double-gamma HRF (peak = 5 s, undershoot peak = 15 s; response undershoot ratio = 6; response dispersion = 1; undershoot dispersion = 1). We also included a baseline predictor. This allowed us to calculate single-trial beta weights using a general linear model (GLM). These beta weights served as input to the IEM described in the next section.</p>
</sec>
<sec id="sec018">
<title>IEM</title>
<p>In order to create model-based reconstructions of target and distractor stimuli in the value-based learning task from individual ROIs, we employed an IEM for retinotopic space (see <xref ref-type="fig" rid="pbio.3000186.g003">Fig 3</xref>; also see [<xref ref-type="bibr" rid="pbio.3000186.ref042">42</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref048">48</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref050">50</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref052">52</xref>]). First, we computed a spatial sensitivity profile (i.e., an encoding model) for each voxel, parameterized as a weighted sum of experimenter-defined information channels (i.e., spatial filters in the second panel of <xref ref-type="fig" rid="pbio.3000186.g003">Fig 3A</xref>) using an independent training data set acquired from the visuospatial mapping task (using only the nontarget trials). Then, we inverted the encoding models across all voxels to compute weights on the spatial information channels and used these weights to transform the fMRI data from the value-based learning task into an activation score. This inversion step provides one means of assessing how much information is encoded about a target or distractor stimulus by aggregating modulations across the entire population of voxels in a given visual area.</p>
<p>More specifically, the activation of each voxel is modeled as the weighted sum of 64 bivariate Gaussian-like spatial information channels arrayed in an 8 × 8 rectangular grid (see the second panel of <xref ref-type="fig" rid="pbio.3000186.g003">Fig 3</xref>). The filter centers were equally spaced by 1.43° visual angle with a full-width half-maximum of 2° visual angle). The Gaussian-like function of each filter is described by:
<disp-formula id="pbio.3000186.e002">
<alternatives>
<graphic id="pbio.3000186.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000186.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mspace width="0.25em"/><mml:mrow><mml:mrow><mml:mi mathvariant="italic">cos</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>π</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">for</mml:mi><mml:mspace width="0.25em"/><mml:mi>r</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>s</mml:mi><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mn>0</mml:mn><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">otherwise</mml:mi><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where <italic>r</italic> is the distance from the filter center and <italic>s</italic> is a size value indicating the distance between filter centers at which the filter returns to 0. We set values greater than <italic>s</italic> to 0 (<italic>s</italic> = 5.0332), resulting in a smooth filter at each position along the grid [<xref ref-type="bibr" rid="pbio.3000186.ref050">50</xref>].</p>
<p>We then defined the idealized response of the information channels for each given training trial. To do this, we multiplied a discretized version of the stimulus (<italic>n</italic> trials × <italic>p</italic> pixels) by the 64 channels defined by <xref ref-type="disp-formula" rid="pbio.3000186.e001">Eq 1</xref> (<italic>p</italic> pixels × <italic>k</italic> channels). We then normalized this result so that the maximum channel response is 1. This is <italic>C</italic><sub><italic>1</italic></sub> in the following equation:
<disp-formula id="pbio.3000186.e003">
<alternatives>
<graphic id="pbio.3000186.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000186.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>W</mml:mi><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where <italic>B</italic><sub><italic>1</italic></sub> (<italic>n</italic> trials × <italic>m</italic> voxels) is the measured fMRI activity of each voxel during the visuospatial mapping task (i.e., beta weights, see fMRI preprocessing section), <italic>C</italic><sub><italic>1</italic></sub> (<italic>n</italic> trials × <italic>k</italic> channels) is the predicted response of each spatial filter (i.e., information channel normalized from 0 to 1), and <italic>W</italic> is a weight matrix (<italic>k</italic> channels × <italic>m</italic> voxels) that quantifies the contribution of each information channel to each voxel. Next, we used ordinary least-squares linear regression to solve for <italic>W</italic> with the following equation:
<disp-formula id="pbio.3000186.e004">
<alternatives>
<graphic id="pbio.3000186.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000186.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula></p>
<p>Here, <inline-formula id="pbio.3000186.e005"><alternatives><graphic id="pbio.3000186.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000186.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:math></alternatives></inline-formula> represents all estimated voxel sensitivity profiles, which we computed separately for each ROI. Next, we used <inline-formula id="pbio.3000186.e006"><alternatives><graphic id="pbio.3000186.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000186.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:math></alternatives></inline-formula> and the measured fMRI activity of each voxel (i.e., beta weights) during each trial of the value-based learning task to estimate the activation of each information channel using the following equation (see <xref ref-type="fig" rid="pbio.3000186.g003">Fig 3B</xref>):
<disp-formula id="pbio.3000186.e007">
<alternatives>
<graphic id="pbio.3000186.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000186.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula></p>
<p>Here, <inline-formula id="pbio.3000186.e008"><alternatives><graphic id="pbio.3000186.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000186.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> represents the estimated activation of each information channel (<italic>n</italic><sub><italic>2</italic></sub> trials × <italic>k</italic> channels), which gives rise to the observed activation pattern across all voxels within that ROI (<italic>B</italic><sub><italic>2</italic></sub>, <italic>n</italic><sub><italic>2</italic></sub> trials × <italic>m</italic> voxels). To visualize and coregister trials across 3 stimulus locations, we computed spatial reconstructions by multiplying the spatial profile of each filter by the estimated activation level of the corresponding channel (i.e., computing a weighted sum; the last panel of <xref ref-type="fig" rid="pbio.3000186.g003">Fig 3B</xref>). We rotated the center position of the spatial filters on each trial of individual participants such that the resulting 2D reconstructions of the target and distractor stimuli share common positions across trials and participants (CCW target, CW target, and distractor locations centered at 30°, 150°, and 270° polar angle, respectively; 3.02° visual angle from the center of the 2D reconstruction). Next, we sorted trials based on selected and unselected target values (low and high), the reward history of the distractor (low and high), and whether the current distractor had been selected or unselected in the 3 previous trials in the same way as we did for the behavioral analysis. Then, we flipped all spatial reconstructions left to right on trials in which the selected target location was on the left (150°) so that the unselected and selected targets always shared common locations on the left and right of the reconstruction, respectively (150° and 30°). This step did not change the position of the distractor, so it stayed at a 270° polar angle. Finally, we averaged the 2D reconstructions across trials with the same trial types for individual participants and then averaged those reconstructions across participants, resulting in the grand-average spatial reconstructions shown in <xref ref-type="fig" rid="pbio.3000186.g004">Fig 4A</xref>.</p>
</sec>
<sec id="sec019">
<title>fMRI statistical analysis</title>
<p>Following a previous approach [<xref ref-type="bibr" rid="pbio.3000186.ref020">20</xref>,<xref ref-type="bibr" rid="pbio.3000186.ref053">53</xref>], we extracted the reconstruction activation for each trial type in individual participants by averaging the data within the circular space spanning the entire area of individual stimuli. This was used as our “reconstruction activation” measure. Like the behavioral analyses, all statistical analyses were conducted by resampling relevant values from each subject with replacement for 100,000 iterations and comparing these values across resampling iterations</p>
<p>In the main analysis, we examined the distractor-value modulation on the distractor reconstruction activation for data averaged across V1–V3. To do so, we computed the bootstrap distribution of the difference of the distractor reconstruction activation between the high and low distractor-value conditions and calculated the percentage of values in this distribution that were larger or smaller than 0 (2-tailed). We performed this statistical analysis separately for trials in which the current distractor was previously selected and unselected in preceding trials to examine whether the distractor-value modulation depended on the selection of previous choices. We then compared whether the effect of distractor value was significantly larger in the selected condition than in the unselected condition by a similar procedure that compared the two bootstrap distributions (1-tailed to the known direction of the difference). We repeated the same statistical procedures for individual visual areas and corrected for multiple comparisons using the Holm-Bonferroni method [<xref ref-type="bibr" rid="pbio.3000186.ref099">99</xref>].</p>
<p>In addition, we ran 2 auxiliary analyses to examine the influence of the knowledge about the value of a given color and the possible effect of value-mediated priming on the distractor-related representations, respectively. In the first analysis, we divided the distractor-related representation data described above into trials that occurred early or late in each miniblock (second through fifth and sixth through eighth trials, respectively). We then examined the main effect of learning on the distractor-value modulation. To do so, we computed the bootstrap distribution of the difference of the distractor reconstruction activation between the high and low distractor-value conditions for the early and late phase. Then, we calculated the percentage of values between the two phases that were larger or smaller than 0 (2-tailed). We also computed the bootstrap distribution of the difference of the distractor reconstruction activation between the high and low distractor-value conditions and calculated the percentage of values in this distribution that were larger or smaller than 0 (2-tailed). This was done separately for the early and late phases as well as for trials in which the distractors were selected and unselected in previous trials. The Holm-Bonferroni method was used to correct for multiple comparisons.</p>
<p>The second auxiliary analysis was similar to the main analysis except that the selection of previous choices was based on whether the current distractor was selected at least once in 1, 2, or 3 preceding trials. To ensure that trials included in these different data-sorting approaches came from a similar range of trial positions relative to the start of each miniblock, we only included trials from the fourth through eighth trials of each miniblock. We examined the distractor-value modulation on the distractor-related representation in each of these data-sorting approaches by computing the bootstrap distribution of the difference of the distractor reconstruction activation between the high and low distractor-value conditions and calculated the percentage of values in this distribution that were larger or smaller than 0 (2-tailed). The Holm-Bonferroni method was used to correct for multiple comparisons. The procedure was first performed in the data collapsed across V1–V3, and the same analysis was then performed in individual visual areas.</p>
<p>Next, we tested the target selection modulation on the target reconstruction activation for data averaged across V1–V3. To do so, we computed the bootstrap distribution of the difference between the selected and unselected target reconstruction activation and calculated the percentage of values in this distribution that were larger or smaller than 0 (2-tailed). We first performed this on the data collapsed across all distractor types. Then, we assessed the target selection modulations separately for individual distractor values and corrected for multiple comparisons using the Holm-Bonferroni method. Then, we tested for the distractor-value modulation on the target-selection modulation by computing the bootstrap distribution of the difference of the target-selection modulations between the high and low distractor-value conditions and computing the percentage of values in this distribution that were larger or smaller than 0 (2-tailed). This was done separately for trials in which the current distractor was previously unselected and selected in preceding trials. We repeated the same statistical procedures for individual visual areas and corrected for multiple comparisons using the Holm-Bonferroni method.</p>
<p>Finally, we tested whether target-selection modulations depended on the relative value difference between selected and unselected targets, as suggested by previous studies [<xref ref-type="bibr" rid="pbio.3000186.ref023">23</xref>–<xref ref-type="bibr" rid="pbio.3000186.ref025">25</xref>]. For each target-value condition (same versus different target values) and each visual area, we computed the bootstrap distribution of the difference between the selected and unselected target reconstruction activation and calculated the percentage of values in this distribution that were larger or smaller than 0 (2-tailed). Here, we also corrected for multiple comparisons across different target value conditions and different visual areas using the Holm-Bonferroni method (6 comparisons). Since we found more robust target-selection modulations in higher visual areas in trials in which the selected and unselected targets had different values, we further tested whether the target-selection modulation in V3 was higher than that in V1, whether the target modulation in V2 was higher than that in V1, and whether the target modulation in V2 was higher than that in V1. To do so, we compared the target-selection modulation distributions across these visual areas (1-tailed, due to the known direction of the difference) and corrected for multiple comparisons using the Holm-Bonferroni method.</p>
</sec>
</sec>
</sec>
<sec id="sec020">
<title>Supporting information</title>
<supplementary-material id="pbio.3000186.s001" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000186.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Distractor-related modulations in early visual cortex rely on knowledge of the value associated with the distractor color.</title>
<p>Same as <xref ref-type="fig" rid="pbio.3000186.g004">Fig 4B</xref> but plotted separately for trials that were early or late in each miniblock (and color–value assignments were constant across all trials in a given miniblock). We found a significant distractor-value effect only in the late phase and only on trials in which the current distractor was a selected target on at least 1 of the 3 prior trials. **Significant distractor-value modulation compared to 0 with <italic>p</italic> &lt; 0.01, Bonferroni-corrected (2-tailed). All error bars show ±1 SEM.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.3000186.s002" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000186.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Distractor-value modulations related to a current distractor that was a selected target on one of the previous 1, 2, and 3 trial(s), respectively.</title>
<p>We only included the fourth through eighth trials of each miniblock to ensure that the data across different data-sorting approaches came from a similar range of trial positions relative to the start of each miniblock. No significant effect was found when sorting the data based on just the immediately preceding trial (i.e., 1 trial back). This speaks against the possibility that reward-mediated priming effects due to the immediately preceding target selection can account for all of the value-based modulations. However, significant distractor-value modulations were observed when the current distractor was selected at least once during the last 2 and 3 trials. ** and *** represent significant distractor-value modulations compared to 0 with <italic>p</italic> &lt; 0.01 and &lt; 0.001, Bonferroni-corrected (2-tailed). All error bars show ±1 SEM.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Margaret Henderson for help with data processing, Chaipat Chunharas for assistance with data collection, and Edward Vul for useful discussions.</p>
</ack>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item><term>fMRI</term>
<def><p>functional magnetic resonance imaging</p></def>
</def-item>
<def-item><term>IEM</term>
<def><p>inverted encoding model</p></def>
</def-item>
<def-item><term>RT</term>
<def><p>response time</p></def>
</def-item>
<def-item><term>n.s.</term>
<def><p>no significant difference</p></def>
</def-item>
</def-list>
</glossary>
<ref-list>
<title>References</title>
<ref id="pbio.3000186.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yantis</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Jonides</surname> <given-names>J</given-names></name>. <article-title>Abrupt visual onsets and selective attention: evidence from visual search</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>1984</year>;<volume>10</volume>(<issue>5</issue>): <fpage>601</fpage>–<lpage>621</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0096-1523.10.5.601" xlink:type="simple">10.1037/0096-1523.10.5.601</ext-link></comment> <object-id pub-id-type="pmid">6238122</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theeuwes</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Krueger</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Chun</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pashler</surname> <given-names>H</given-names></name>. <article-title>Perceptual selectivity for color and form</article-title>. <source>Percept Psychophys</source>. <year>1992</year>;<volume>51</volume>(<issue>6</issue>): <fpage>599</fpage>–<lpage>606</lpage>. <object-id pub-id-type="pmid">1620571</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Egeth</surname> <given-names>HE</given-names></name>, <name name-style="western"><surname>Yantis</surname> <given-names>S</given-names></name>. <article-title>Visual attention: control, representation, and time course</article-title>. <source>Annu Rev Psychol</source>. <year>1997</year>;<volume>48</volume>: <fpage>269</fpage>–<lpage>297</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.psych.48.1.269" xlink:type="simple">10.1146/annurev.psych.48.1.269</ext-link></comment> <object-id pub-id-type="pmid">9046562</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref004"><label>4</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Wolfe</surname> <given-names>JM</given-names></name>. <chapter-title>Visual Search</chapter-title> In: <name name-style="western"><surname>Pashler</surname> <given-names>H.</given-names></name>, editor. <source>Attention</source>. <publisher-loc>London</publisher-loc>: <publisher-name>University College London Press</publisher-name>; <year>1998</year>.</mixed-citation></ref>
<ref id="pbio.3000186.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Laurent</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Yantis</surname> <given-names>S</given-names></name>. <article-title>Value-driven attentional capture</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2011</year>;<volume>108</volume>(<issue>25</issue>): <fpage>10367</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1104047108" xlink:type="simple">10.1073/pnas.1104047108</ext-link></comment> <object-id pub-id-type="pmid">21646524</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Itthipuripat</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cha</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Rangsipat</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>. <article-title>Value-based attentional capture influences context-dependent decision-making</article-title>. <source>J Neurophysiol</source>. <year>2015</year>;<volume>114</volume>(<issue>1</issue>): <fpage>560</fpage>–<lpage>569</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00343.2015" xlink:type="simple">10.1152/jn.00343.2015</ext-link></comment> <object-id pub-id-type="pmid">25995350</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hickey</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Chelazzi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Theeuwes</surname> <given-names>J</given-names></name>. <article-title>Reward changes salience in human vision via the anterior cingulate</article-title>. <source>J Neurosci. Society for Neuroscience</source>; <year>2010</year>;<volume>30</volume>(<issue>33</issue>): <fpage>11096</fpage>–<lpage>103</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1026-10.2010" xlink:type="simple">10.1523/JNEUROSCI.1026-10.2010</ext-link></comment> <object-id pub-id-type="pmid">20720117</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname> <given-names>BA</given-names></name>. <article-title>Going for it: The economics of automaticity in perception and action</article-title>. <source>Curr Dir Psychol Sci</source>. <year>2017</year>;<volume>26</volume>(<issue>2</issue>): <fpage>140</fpage>–<lpage>145</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0963721416686181" xlink:type="simple">10.1177/0963721416686181</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000186.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Awh</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Belopolsky A</surname> <given-names>V</given-names></name>., <name name-style="western"><surname>Theeuwes</surname> <given-names>J</given-names></name>. <article-title>Top-down versus bottom-up attentional control: A failed theoretical dichotomy</article-title>. <source>Trends Cogn Sci</source>. <year>2012</year>;<volume>16</volume>(<issue>8</issue>): <fpage>437</fpage>–<lpage>443</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2012.06.010" xlink:type="simple">10.1016/j.tics.2012.06.010</ext-link></comment> <object-id pub-id-type="pmid">22795563</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Libera C</surname> <given-names>Della</given-names></name>, <name name-style="western"><surname>Chelazzi</surname> <given-names>L</given-names></name>. <article-title>Learning to attend and to ignore is a matter of gains and losses</article-title>. <source>Psychol Sci</source>. <year>2009</year>(<issue>6</issue>);<volume>20</volume>: <fpage>778</fpage>–<lpage>784</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-9280.2009.02360.x" xlink:type="simple">10.1111/j.1467-9280.2009.02360.x</ext-link></comment> <object-id pub-id-type="pmid">19422618</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gluth</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Spektor</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Rieskamp</surname> <given-names>J</given-names></name>. <article-title>Value-based attentional capture affects multi-alternative decision making</article-title>. <source>Elife</source>. <year>2018</year>;<volume>7</volume>: <fpage>1</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.39659" xlink:type="simple">10.7554/eLife.39659</ext-link></comment> <object-id pub-id-type="pmid">30394874</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moher</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Song</surname> <given-names>JH</given-names></name>. <article-title>Dissociable Effects of Salience on Attention and Goal-Directed Action</article-title>. <source>Curr Biol</source>. Elsevier Ltd; <year>2015</year>;<volume>25</volume>(<issue>15</issue>): <fpage>2040</fpage>–<lpage>2046</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2015.06.029" xlink:type="simple">10.1016/j.cub.2015.06.029</ext-link></comment> <object-id pub-id-type="pmid">26190076</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hickey</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>van Zoest</surname> <given-names>W</given-names></name>. <article-title>Reward-associated stimuli capture the eyes in spite of strategic attentional set</article-title>. <source>Vision Res</source>. <year>2013</year>;<volume>92</volume>: <fpage>67</fpage>–<lpage>74</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2013.09.008" xlink:type="simple">10.1016/j.visres.2013.09.008</ext-link></comment> <object-id pub-id-type="pmid">24084197</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maclean</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Diaz</surname> <given-names>GK</given-names></name>, <name name-style="western"><surname>Giesbrecht</surname> <given-names>B</given-names></name>. <article-title>Irrelevant learned reward associations disrupt voluntary spatial attention</article-title>. <source>Atten Percept Psychophys</source>. <year>2016</year>;<volume>78</volume>(<issue>7</issue>): <fpage>2241</fpage>–<lpage>2252</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13414-016-1103-x" xlink:type="simple">10.3758/s13414-016-1103-x</ext-link></comment> <object-id pub-id-type="pmid">27084702</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maclean</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Giesbrecht</surname> <given-names>B</given-names></name>. <article-title>Neural evidence reveals the rapid effects of reward history on selective attention</article-title>. <source>Brain Res</source>. <year>2015</year>;<volume>1606</volume>: <fpage>86</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.brainres.2015.02.016" xlink:type="simple">10.1016/j.brainres.2015.02.016</ext-link></comment> <object-id pub-id-type="pmid">25701717</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>MacLean</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Giesbrecht</surname> <given-names>B</given-names></name>. <article-title>Irrelevant reward and selection histories have different influences on task-relevant attentional selection</article-title>. <source>Attention, Perception, Psychophys</source>. <year>2015</year>;<volume>77</volume>(<issue>5</issue>): <fpage>1515</fpage>–<lpage>1528</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13414-015-0851-3" xlink:type="simple">10.3758/s13414-015-0851-3</ext-link></comment> <object-id pub-id-type="pmid">25813737</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krebs</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Boehler</surname> <given-names>CN</given-names></name>, <name name-style="western"><surname>Egner</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Woldorff</surname> <given-names>MG</given-names></name>. <article-title>The neural underpinnings of how reward associations can both guide and misguide attention</article-title>. <source>J Neurosci</source>. 2011(<issue>26</issue>);<volume>31</volume>: <fpage>9752</fpage>–<lpage>9759</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0732-11.2011" xlink:type="simple">10.1523/JNEUROSCI.0732-11.2011</ext-link></comment> <object-id pub-id-type="pmid">21715640</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sali</surname> <given-names>AW</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Yantis</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mostofsky</surname> <given-names>SH</given-names></name>, <name name-style="western"><surname>Rosch</surname> <given-names>KS</given-names></name>. <article-title>Reduced value-driven attentional capture among children with ADHD compared to typically developing controls. J Abnorm Child Psychol</article-title>. <source>Journal of Abnormal Child Psychology</source>; <year>2018</year>;<volume>46</volume>(<issue>6</issue>): <fpage>1187</fpage>–<lpage>1200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10802-017-0345-y" xlink:type="simple">10.1007/s10802-017-0345-y</ext-link></comment> <object-id pub-id-type="pmid">28913698</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Faulkner</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Rilee</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Yantis</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Marvel</surname> <given-names>CL</given-names></name>, <etal>et al</etal>. <article-title>Attentional bias for non-drug reward is magnified in addiction</article-title>. <year>2014</year>(<issue>6</issue>);<volume>21</volume>: <fpage>499</fpage>–<lpage>506</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0034575" xlink:type="simple">10.1037/a0034575</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000186.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sprague</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Itthipuripat</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Vo</surname> <given-names>VA</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>. <article-title>Dissociable signatures of visual salience and behavioral relevance across attentional priority maps in human cortex</article-title>. <source>J Neurophysiol</source>. <year>2018</year>;<volume>119</volume>(<issue>6</issue>): <fpage>2153</fpage>–<lpage>2165</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00059.2018" xlink:type="simple">10.1152/jn.00059.2018</ext-link></comment> <object-id pub-id-type="pmid">29488841</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Zhaoping</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Fang</surname> <given-names>F</given-names></name>. <article-title>Neural activities in V1 create a bottom-up saliency map</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>73</volume>(<issue>1</issue>): <fpage>183</fpage>–<lpage>92</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.10.035" xlink:type="simple">10.1016/j.neuron.2011.10.035</ext-link></comment> <object-id pub-id-type="pmid">22243756</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Fang</surname> <given-names>F</given-names></name>. <article-title>Neural activities in V1 create the bottom-up saliency map of natural scenes</article-title>. <source>Exp Brain Res</source>. <year>2016</year>;<volume>234</volume>(<issue>6</issue>): <fpage>1769</fpage>–<lpage>1780</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00221-016-4583-y" xlink:type="simple">10.1007/s00221-016-4583-y</ext-link></comment> <object-id pub-id-type="pmid">26879771</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>. <article-title>Value-based modulations in human visual cortex</article-title>. <source>Neuron</source>. <year>2008</year>;<volume>60</volume>(<issue>6</issue>): <fpage>1169</fpage>–<lpage>1181</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2008.10.051" xlink:type="simple">10.1016/j.neuron.2008.10.051</ext-link></comment> <object-id pub-id-type="pmid">19109919</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Saproo</surname> <given-names>S</given-names></name>. <article-title>Population response profiles in early visual cortex are biased in favor of more valuable stimuli</article-title>. <source>J Neurophysiol</source>. <year>2010</year>;<volume>104</volume>(<issue>1</issue>): <fpage>76</fpage>–<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.01090.2009" xlink:type="simple">10.1152/jn.01090.2009</ext-link></comment> <object-id pub-id-type="pmid">20410360</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stănişor</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>van der Togt</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Pennartz</surname> <given-names>CMA</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>. <article-title>A unified selection signal for attention and reward in primary visual cortex</article-title>. <source>Proc Natl Acad Sci U S A</source>. National Academy of Sciences; <year>2013</year>;<volume>110</volume>(<issue>22</issue>): <fpage>9136</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1300117110" xlink:type="simple">10.1073/pnas.1300117110</ext-link></comment> <object-id pub-id-type="pmid">23676276</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baruni</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Salzman</surname> <given-names>CD</given-names></name>. <article-title>Reward expectation differentially modulates attentional behavior and activity in visual area V4</article-title>. <source>Nat Neurosci</source>. <year>2015</year>;<volume>18</volume>(<issue>11</issue>): <fpage>1656</fpage>–<lpage>1663</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4141" xlink:type="simple">10.1038/nn.4141</ext-link></comment> <object-id pub-id-type="pmid">26479590</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berridge</surname> <given-names>KC</given-names></name>, <name name-style="western"><surname>Robinson</surname> <given-names>TE</given-names></name>. <article-title>What is the role of dopamine in reward: Hedonic impact, reward learning, or incentive salience?</article-title> <source>Brain Res Rev</source>. <year>1998</year>;<volume>28</volume>(<issue>3</issue>): <fpage>309</fpage>–<lpage>369</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0165-0173(98)00019-8" xlink:type="simple">10.1016/S0165-0173(98)00019-8</ext-link></comment> <object-id pub-id-type="pmid">9858756</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ikeda</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hikosaka</surname> <given-names>O</given-names></name>. <article-title>Reward-dependent gain and bias of visual responses in primate superior colliculus</article-title>. <source>Neuron</source>. <year>2003</year>;<volume>39</volume>(<issue>4</issue>): <fpage>693</fpage>–<lpage>700</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0896-6273(03)00464-1" xlink:type="simple">10.1016/S0896-6273(03)00464-1</ext-link></comment> <object-id pub-id-type="pmid">12925282</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ikemoto</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Panksepp</surname> <given-names>J</given-names></name>. <article-title>The role of nucleus accumbens dopamine in motivated behavior: A unifying interpretation with special reference to reward-seeking</article-title>. <source>Brain Res Rev</source>. <year>1999</year>;<volume>31</volume>(<issue>1</issue>): <fpage>6</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0165-0173(99)00023-5" xlink:type="simple">10.1016/S0165-0173(99)00023-5</ext-link></comment> <object-id pub-id-type="pmid">10611493</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Redgrave</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Prescott</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Gurney</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Redgrave</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Prescott</surname> <given-names>TJ</given-names></name>. <article-title>Is the short-latency dopamine response too short to signal reward error?</article-title> <source>Trends Neurosci</source>. <year>1999</year>; <volume>22</volume>(<issue>4</issue>):<fpage>146</fpage>–<lpage>51</lpage>. <object-id pub-id-type="pmid">10203849</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wise</surname> <given-names>RA</given-names></name>. <article-title>Dopamine, learning and motivation</article-title>. <source>Nat Rev Neurosci</source>. <year>2004</year>;<volume>5</volume>(<issue>6</issue>): <fpage>483</fpage>–<lpage>494</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn1406" xlink:type="simple">10.1038/nrn1406</ext-link></comment> <object-id pub-id-type="pmid">15152198</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alcaro</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Huber</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Panksepp</surname> <given-names>J</given-names></name>. <article-title>Behavioral functions of the mesolimbic dopaminergic system: An affective neuroethological perspective</article-title>. <source>Brain Res Rev</source>. <year>2007</year>;<volume>56</volume>(<issue>2</issue>): <fpage>283</fpage>–<lpage>321</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.brainresrev.2007.07.014" xlink:type="simple">10.1016/j.brainresrev.2007.07.014</ext-link></comment> <object-id pub-id-type="pmid">17905440</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W.</given-names></name> <article-title>Getting formal with dopamine and reward</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>36</volume>(<issue>2</issue>): <fpage>241</fpage>–<lpage>263</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.cell.com/action/showPdf?pii=S0896-6273%2802%2900967-4" xlink:type="simple">https://www.cell.com/action/showPdf?pii=S0896-6273%2802%2900967-4</ext-link> <object-id pub-id-type="pmid">12383780</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>. <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source>. <year>1997</year>;<volume>275</volume>(<issue>5306</issue>): <fpage>1593</fpage>–<lpage>1600</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.275.5306.1593" xlink:type="simple">10.1126/science.275.5306.1593</ext-link></comment> <object-id pub-id-type="pmid">9054347</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Laurent</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Yantis</surname> <given-names>S</given-names></name>. <article-title>Value-driven attentional priority signals in human basal ganglia and visual cortex</article-title>. <source>Brain Res. Elsevier</source>; <year>2014</year>;<volume>1587</volume>: <fpage>88</fpage>–<lpage>96</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/J.BRAINRES.2014.08.062" xlink:type="simple">10.1016/J.BRAINRES.2014.08.062</ext-link></comment> <object-id pub-id-type="pmid">25171805</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname> <given-names>BA</given-names></name>. <article-title>The attention habit: How reward learning shapes attentional selection</article-title>. <source>Ann N Y Acad Sci</source>. <year>2016</year>;<volume>1369</volume>(<issue>1</issue>): <fpage>24</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/nyas.12957" xlink:type="simple">10.1111/nyas.12957</ext-link></comment> <object-id pub-id-type="pmid">26595376</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname> <given-names>BA</given-names></name>. <article-title>Neurobiology of value-driven attention</article-title>. <source>Curr Opin Psychol</source>. <year>2019</year>;<volume>29</volume>: <fpage>27</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.copsyc.2018.11.004" xlink:type="simple">10.1016/j.copsyc.2018.11.004</ext-link></comment> <object-id pub-id-type="pmid">30472540</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gong</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jia</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>S</given-names></name>. <article-title>Perceptual competition promotes suppression of reward salience in behavioral selection and neural representation</article-title>. <source>J Neurosci</source>. <year>2017</year>;<volume>37</volume>(<issue>26</issue>): <fpage>6242</fpage>–<lpage>6252</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0217-17.2017" xlink:type="simple">10.1523/JNEUROSCI.0217-17.2017</ext-link></comment> <object-id pub-id-type="pmid">28539425</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hickey</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Peelen M</surname> <given-names>V</given-names></name>. <article-title>Reward selectively modulates the lingering neural representation of recently attended objects in natural scenes</article-title>. <source>J Neurosci. Society for Neuroscience</source>; <year>2017</year>;<volume>37</volume>(<issue>31</issue>): <fpage>7297</fpage>–<lpage>7304</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0684-17.2017" xlink:type="simple">10.1523/JNEUROSCI.0684-17.2017</ext-link></comment> <object-id pub-id-type="pmid">28630254</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hickey</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Peelen M</surname> <given-names>V</given-names></name>. <article-title>Neural mechanisms of incentive salience in naturalistic human vision</article-title>. <source>Neuron</source>. Cell Press; <year>2015</year>;<volume>85</volume>(<issue>3</issue>): <fpage>512</fpage>–<lpage>518</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.12.049" xlink:type="simple">10.1016/j.neuron.2014.12.049</ext-link></comment> <object-id pub-id-type="pmid">25654257</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gaspelin</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Luck</surname> <given-names>SJ</given-names></name>. <article-title>The role of inhibition in avoiding distraction by salient stimuli</article-title>. <source>Trends Cogn Sci</source>. <year>2018</year>;<volume>22</volume>(<issue>1</issue>): <fpage>79</fpage>–<lpage>92</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2017.11.001" xlink:type="simple">10.1016/j.tics.2017.11.001</ext-link></comment> <object-id pub-id-type="pmid">29191511</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brouwer</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>D</given-names></name>. <article-title>Decoding and reconstructing color from responses in human visual cortex</article-title>. <source>J Neurosci</source>. <year>2009</year>;<volume>29</volume>(<issue>44</issue>): <fpage>13992</fpage>–<lpage>14003</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3577-09.2009" xlink:type="simple">10.1523/JNEUROSCI.3577-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19890009</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bressler</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Fortenbaugh</surname> <given-names>FC</given-names></name>, <name name-style="western"><surname>Robertson</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>Silver</surname> <given-names>MA</given-names></name>. <article-title>Visual spatial attention enhances the amplitude of positive and negative fMRI responses to visual stimulation in an eccentricity-dependent manner</article-title>. <source>Vision Res</source>. <year>2013</year>;<volume>85</volume>: <fpage>104</fpage>–<lpage>112</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2013.03.009" xlink:type="simple">10.1016/j.visres.2013.03.009</ext-link></comment> <object-id pub-id-type="pmid">23562388</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Puckett</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>DeYoe</surname> <given-names>EA</given-names></name>. <article-title>The attentional field revealed by single-voxel modeling of fMRI time courses</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>(<issue>12</issue>): <fpage>5030</fpage>–<lpage>5042</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3754-14.2015" xlink:type="simple">10.1523/JNEUROSCI.3754-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25810532</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gouws</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Alvarez</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Watson</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Uesaki</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rogers</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Morland</surname> <given-names>AB</given-names></name>. <article-title>On the role of suppression in spatial attention: evidence from negative BOLD in human subcortical and cortical structures</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>(<issue>31</issue>): <fpage>10347</fpage>–<lpage>10360</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0164-14.2014" xlink:type="simple">10.1523/JNEUROSCI.0164-14.2014</ext-link></comment> <object-id pub-id-type="pmid">25080595</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Muller</surname> <given-names>NG</given-names></name>, <name name-style="western"><surname>Kleinschmidt</surname> <given-names>A</given-names></name>. <article-title>Dynamic interaction of object- and space-based attention in retinotopic visual areas</article-title>. <source>J Neurosci</source>. <year>2003</year>;<volume>23</volume>(<issue>30</issue>): <fpage>9812</fpage>–<lpage>9816</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/OCEANSE.2005.1511760" xlink:type="simple">10.1109/OCEANSE.2005.1511760</ext-link></comment> <object-id pub-id-type="pmid">14586009</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fischer</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Whitney</surname> <given-names>D</given-names></name>. <article-title>Attention narrows position tuning of population responses in V1</article-title>. <source>Curr Biol</source>. <year>2009</year>;<volume>19</volume>(<issue>16</issue>): <fpage>1356</fpage>–<lpage>1361</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2009.06.059" xlink:type="simple">10.1016/j.cub.2009.06.059</ext-link></comment> <object-id pub-id-type="pmid">19631540</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brouwer</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>Categorical clustering of the neural representation of color</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>(<issue>39</issue>): <fpage>15454</fpage>–<lpage>15465</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2472-13.2013" xlink:type="simple">10.1523/JNEUROSCI.2472-13.2013</ext-link></comment> <object-id pub-id-type="pmid">24068814</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Itthipuripat</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sprague</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>J</given-names></name>. <article-title>Functional MRI and EEG index complementary attentional modulations</article-title>. <source>J Neurosci</source>. <year>2019</year>;<volume>39</volume>(<issue>31</issue>): <fpage>6162</fpage>–<lpage>6179</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2519-18.2019" xlink:type="simple">10.1523/JNEUROSCI.2519-18.2019</ext-link></comment> <object-id pub-id-type="pmid">31127004</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sprague</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>. <article-title>Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices</article-title>. <source>Nat Neurosci</source>. <year>2013</year>;<volume>16</volume>(<issue>12</issue>): <fpage>1879</fpage>–<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3574" xlink:type="simple">10.1038/nn.3574</ext-link></comment> <object-id pub-id-type="pmid">24212672</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sprague</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Ester</surname> <given-names>EF</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>. <article-title>Reconstructions of information in visual spatial working memory degrade with memory load</article-title>. <source>Curr Biol</source>. <year>2014</year>;<volume>24</volume>(<issue>18</issue>): <fpage>1</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2014.07.066" xlink:type="simple">10.1016/j.cub.2014.07.066</ext-link></comment> <object-id pub-id-type="pmid">25201683</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vo</surname> <given-names>VA</given-names></name>, <name name-style="western"><surname>Sprague</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>. <article-title>Spatial tuning shifts increase the discriminability and fidelity of population codes in visual cortex</article-title>. <source>J Neurosci</source>. <year>2017</year>;<volume>37</volume>(<issue>12</issue>): <fpage>3386</fpage>–<lpage>3401</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3484-16.2017" xlink:type="simple">10.1523/JNEUROSCI.3484-16.2017</ext-link></comment> <object-id pub-id-type="pmid">28242794</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sprague</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Ester</surname> <given-names>EF</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>. <article-title>Restoring latent visual working memory representations in human cortex Article</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>91</volume>(<issue>3</issue>): <fpage>694</fpage>–<lpage>707</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.07.006" xlink:type="simple">10.1016/j.neuron.2016.07.006</ext-link></comment> <object-id pub-id-type="pmid">27497224</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sprague</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Adam</surname> <given-names>KCS</given-names></name>, <name name-style="western"><surname>Foster</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Rahmati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sutterer</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Vo</surname> <given-names>VA</given-names></name>. <article-title>Inverted encoding models assay population-level stimulus representations, not single-unit neural tuning</article-title>. <source>Eneuro</source>. <year>2018</year>;<volume>5</volume>(<issue>3</issue>): ENEURO.0098-18.2018. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/ENEURO.0098-18.2018" xlink:type="simple">10.1523/ENEURO.0098-18.2018</ext-link></comment> <object-id pub-id-type="pmid">29876523</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tootell</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Hadjikhani</surname> <given-names>NK</given-names></name>, <name name-style="western"><surname>Vanduffel</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Mendola</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Sereno</surname> <given-names>MI</given-names></name>, <etal>et al</etal>. <article-title>Functional analysis of primary visual cortex (V1) in humans</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>1998</year>;<volume>95</volume>(<issue>3</issue>): <fpage>811</fpage>–<lpage>7</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC33802" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC33802</ext-link> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.95.3.811" xlink:type="simple">10.1073/pnas.95.3.811</ext-link></comment> <object-id pub-id-type="pmid">9448245</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Louie</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Khaw</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name>. <article-title>Normalization is a general neural mechanism for context-dependent decision making</article-title>. <source>Proc Natl Acad Sci</source>. <year>2013</year>;<volume>110</volume>(<issue>15</issue>): <fpage>6139</fpage>–<lpage>6144</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1217854110" xlink:type="simple">10.1073/pnas.1217854110</ext-link></comment> <object-id pub-id-type="pmid">23530203</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chau</surname> <given-names>BKH</given-names></name>, <name name-style="western"><surname>Kolling</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hunt</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Walton</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MFS</given-names></name>. <article-title>A neural mechanism underlying failure of optimal choice with multiple alternatives</article-title>. <source>Nat Neurosci</source>. <year>2014</year>;<volume>17</volume>(<issue>3</issue>): <fpage>463</fpage>–<lpage>470</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3649" xlink:type="simple">10.1038/nn.3649</ext-link></comment> <object-id pub-id-type="pmid">24509428</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brouwer</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>Decoding and reconstructing color from responses in human visual cortex</article-title>. <source>J Neurosci</source>. <year>2009</year>;<volume>29</volume>(<issue>44</issue>): <fpage>13992</fpage>–<lpage>14003</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3577-09.2009" xlink:type="simple">10.1523/JNEUROSCI.3577-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19890009</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seitz</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Watanabe</surname> <given-names>T</given-names></name>. <article-title>Rewards evoke learning of unconsciously processed visual stimuli in adult humans</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>61</volume>(<issue>5</issue>): <fpage>700</fpage>–<lpage>707</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.01.016" xlink:type="simple">10.1016/j.neuron.2009.01.016</ext-link></comment> <object-id pub-id-type="pmid">19285467</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barbaro</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Peelen M</surname> <given-names>V</given-names></name>., <article-title>Hickey C. Valence, not utility, underlies reward-driven prioritization in human vision</article-title>. <source>J Neurosci</source>. <year>2017</year>;<volume>37</volume>(<issue>43</issue>): <fpage>1128</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1128-17.2017" xlink:type="simple">10.1523/JNEUROSCI.1128-17.2017</ext-link></comment> <object-id pub-id-type="pmid">28951452</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zeki</surname> <given-names>SM</given-names></name>. <article-title>Functional organization of a visual area in the posterior bank of the superior temporal sulcus of the rhesus monkey</article-title>. <source>J Physiol</source>. <year>1974</year>;<volume>236</volume>(<issue>3</issue>): <fpage>549</fpage>–<lpage>573</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1113/jphysiol.1974.sp010452" xlink:type="simple">10.1113/jphysiol.1974.sp010452</ext-link></comment> <object-id pub-id-type="pmid">4207129</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Conway</surname> <given-names>BR</given-names></name>, <name name-style="western"><surname>Moeller</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>. <article-title>Specialized color modules in macaque extrastriate cortex</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>56</volume>(<issue>3</issue>): <fpage>560</fpage>–<lpage>573</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2007.10.008" xlink:type="simple">10.1016/j.neuron.2007.10.008</ext-link></comment> <object-id pub-id-type="pmid">17988638</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zeki</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bartels</surname> <given-names>A</given-names></name>. <article-title>The architecture of the colour centre in the human visual brain: new results and a review</article-title>. <source>Eur J Neurosci</source>. <year>2000</year>;<volume>12</volume>(<issue>1</issue>): <fpage>172</fpage>–<lpage>193</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1046/j.1460-9568.2000.00905.x" xlink:type="simple">10.1046/j.1460-9568.2000.00905.x</ext-link></comment> <object-id pub-id-type="pmid">10651872</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brewer</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wade</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Wandell</surname> <given-names>BA</given-names></name>. <article-title>Visual field maps and stimulus selectivity in human ventral occipital cortex</article-title>. <source>Nat Neurosci</source>. <year>2005</year>;<volume>8</volume>(<issue>8</issue>): <fpage>1102</fpage>–<lpage>1109</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1507" xlink:type="simple">10.1038/nn1507</ext-link></comment> <object-id pub-id-type="pmid">16025108</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hadjikhani</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Dale</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Cavanagh</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tootell</surname> <given-names>RBH</given-names></name>. <article-title>Retinotopy and color sensitivity in human visual cortical area V8</article-title>. <source>Nat Neurosci</source>. <year>1998</year>;<volume>1</volume>(<issue>3</issue>): <fpage>235</fpage>–<lpage>241</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/681" xlink:type="simple">10.1038/681</ext-link></comment> <object-id pub-id-type="pmid">10195149</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bressler</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Silver</surname> <given-names>MA</given-names></name>. <article-title>Spatial attention improves reliability of fMRI retinotopic mapping signals in occipital and parietal cortex</article-title>. <source>Neuroimage</source>. <year>2010</year>;<volume>53</volume>(<issue>2</issue>): <fpage>526</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2010.06.063" xlink:type="simple">10.1016/j.neuroimage.2010.06.063</ext-link></comment> <object-id pub-id-type="pmid">20600961</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Yantis</surname> <given-names>S</given-names></name>. <article-title>Selective visual attention and perceptual coherence</article-title>. <source>Trends Cogn Sci</source>. <year>2006</year>;<volume>10</volume>(<issue>1</issue>); <fpage>38</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2005.11.008" xlink:type="simple">10.1016/j.tics.2005.11.008</ext-link></comment> <object-id pub-id-type="pmid">16318922</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sprague</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Saproo</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>. <article-title>Visual attention mitigates information loss in small- and large-scale neural codes</article-title>. <source>Trends Cogn Sci</source>. <year>2015</year>; <volume>19</volume>(<issue>4</issue>): <fpage>215</fpage>–<lpage>226</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2015.02.005" xlink:type="simple">10.1016/j.tics.2015.02.005</ext-link></comment> <object-id pub-id-type="pmid">25769502</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shuler</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Bear</surname> <given-names>MF</given-names></name>. <article-title>Reward timing in the primary visual cortex</article-title>. <source>Science</source>. <year>2006</year>;<volume>311</volume>(<issue>5767</issue>): <fpage>1606</fpage>–<lpage>1609</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1123513" xlink:type="simple">10.1126/science.1123513</ext-link></comment> <object-id pub-id-type="pmid">16543459</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Treue</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Martinez-Trujillo</surname> <given-names>JC</given-names></name>. <article-title>Feature-based attention influences motion processing gain in macaque visual cortex</article-title>. <source>Nature</source>. <year>1999</year>;<volume>399</volume>(<issue>6736</issue>): <fpage>575</fpage>–<lpage>579</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/21176" xlink:type="simple">10.1038/21176</ext-link></comment> <object-id pub-id-type="pmid">10376597</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bichot</surname> <given-names>NP</given-names></name>, <name name-style="western"><surname>Bichot</surname> <given-names>NP</given-names></name>, <name name-style="western"><surname>Rossi</surname> <given-names>AF</given-names></name>, <name name-style="western"><surname>Desimone</surname> <given-names>R</given-names></name>. <article-title>Parallel and serial neural mechanisms for visual search in macaque area V4</article-title>. <year>2010</year>;<volume>529</volume>(<issue>5721</issue>): <fpage>529</fpage>–<lpage>535</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1109676" xlink:type="simple">10.1126/science.1109676</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000186.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maunsell</surname> <given-names>JHR</given-names></name>, <name name-style="western"><surname>Treue</surname> <given-names>S</given-names></name>. <article-title>Feature-based attention in visual cortex</article-title>. <source>Trends Cogn Sci</source>. <year>2006</year>; <volume>29</volume>(<issue>6</issue>): <fpage>317</fpage>–<lpage>322</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2006.04.001" xlink:type="simple">10.1016/j.tins.2006.04.001</ext-link></comment> <object-id pub-id-type="pmid">16697058</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saenz</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Buracas</surname> <given-names>GT</given-names></name>, <name name-style="western"><surname>Boynton</surname> <given-names>GM</given-names></name>. <article-title>Global effects of feature-based attention in human visual cortex</article-title>. <source>Nat Neurosci</source>. <year>2002</year>;<volume>5</volume>(<issue>7</issue>): <fpage>631</fpage>–<lpage>632</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn876" xlink:type="simple">10.1038/nn876</ext-link></comment> <object-id pub-id-type="pmid">12068304</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Andersen</surname> <given-names>SK</given-names></name>, <name name-style="western"><surname>Hillyard</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Müller</surname> <given-names>MM</given-names></name>. <article-title>Attention facilitates multiple stimulus features in parallel in human visual cortex</article-title>. <source>Curr Biol</source>. <year>2008</year>;<volume>18</volume>(<issue>13</issue>): <fpage>1006</fpage>–<lpage>1009</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2008.06.030" xlink:type="simple">10.1016/j.cub.2008.06.030</ext-link></comment> <object-id pub-id-type="pmid">18595707</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Boynton</surname> <given-names>GM</given-names></name>. <article-title>Feature-based attentional modulations in the absence of direct visual stimulation</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>55</volume>(<issue>2</issue>): <fpage>301</fpage>–<lpage>312</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2007.06.015" xlink:type="simple">10.1016/j.neuron.2007.06.015</ext-link></comment> <object-id pub-id-type="pmid">17640530</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ester</surname> <given-names>EF</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Awh</surname> <given-names>E</given-names></name>. <article-title>Spatially global representations in human primary visual cortex during working memory maintenance</article-title>. <source>J Neurosci</source>. <year>2009</year>;<volume>29</volume>(<issue>48</issue>): <fpage>15258</fpage>–<lpage>15265</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4388-09.2009" xlink:type="simple">10.1523/JNEUROSCI.4388-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19955378</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ester</surname> <given-names>EF</given-names></name>, <name name-style="western"><surname>Sutterer</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Awh</surname> <given-names>E</given-names></name>. <article-title>Feature-selective attentional modulations in human frontoparietal cortex</article-title>. <source>J Neurosci</source>. <year>2016</year>;<volume>36</volume>(<issue>31</issue>): <fpage>8188</fpage>–<lpage>8199</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3935-15.2016" xlink:type="simple">10.1523/JNEUROSCI.3935-15.2016</ext-link></comment> <object-id pub-id-type="pmid">27488638</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ester</surname> <given-names>EF</given-names></name>, <name name-style="western"><surname>Sprague</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>. <article-title>Parietal and frontal cortex encode stimulus-specific mnemonic representations during visual working memory</article-title>. <source>Neuron</source>. <year>2015</year>;<volume>87</volume>(<issue>4</issue>): <fpage>893</fpage>–<lpage>905</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2015.07.013" xlink:type="simple">10.1016/j.neuron.2015.07.013</ext-link></comment> <object-id pub-id-type="pmid">26257053</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Folk</surname> <given-names>CL</given-names></name>, <name name-style="western"><surname>Remington</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>JH</given-names></name>. <article-title>The structure of attentional control: Contingent attentional capture by apparent motion, abrupt onset, and color</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>1994</year>:<volume>20</volume>(<issue>2</issue>): <fpage>317</fpage>–<lpage>329</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0096-1523.20.2.317" xlink:type="simple">10.1037/0096-1523.20.2.317</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000186.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Folk</surname> <given-names>CL</given-names></name>, <name name-style="western"><surname>Remington</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Johnston</surname> <given-names>JC</given-names></name>. <article-title>Contingent attentional capture: A reply to Yantis (1993)</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>1993</year>;<volume>19</volume>(<issue>3</issue>): <fpage>682</fpage>–<lpage>685</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0096-1523.19.3.682" xlink:type="simple">10.1037/0096-1523.19.3.682</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000186.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Folk</surname> <given-names>CL</given-names></name>, <name name-style="western"><surname>Remington</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Johnston</surname> <given-names>JC</given-names></name>. <article-title>Involuntary covert orienting is contingent on attentional control settings</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>1992</year>:<volume>18</volume>(<issue>4</issue>): <fpage>1030</fpage>–<lpage>1044</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0096-1523.18.4.1030" xlink:type="simple">10.1037/0096-1523.18.4.1030</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000186.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wolfe</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Butcher</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Hyle</surname> <given-names>M</given-names></name>. <article-title>Changing your mind: On the contributions of top-down and bottom-up guidance in visual search for feature singletons</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>2003</year>;<volume>29</volume>(<issue>2</issue>): <fpage>483</fpage>–<lpage>502</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0096-1523.29.2.483" xlink:type="simple">10.1037/0096-1523.29.2.483</ext-link></comment> <object-id pub-id-type="pmid">12760630</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theeuwes</surname> <given-names>J.</given-names></name> <article-title>Visual selection: Usually fast and automatic; Seldom slow and volitional</article-title>. <source>J Cogn</source>. <year>2018</year>;<volume>1</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5334/joc.13" xlink:type="simple">10.5334/joc.13</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000186.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theeuwes</surname> <given-names>J.</given-names></name> <article-title>Visual selection: Usually fast and automatic; Seldom slow and volitional; A reply to commentaries</article-title>. <source>J Cogn</source>. <year>2018</year>;<volume>1</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5334/joc.32" xlink:type="simple">10.5334/joc.32</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000186.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Egeth</surname> <given-names>H.</given-names></name> <article-title>Comment on Theeuwes’s characterization of visual selection</article-title>. <source>J Cogn</source>. <year>2018</year>;<volume>1</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>3</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5334/joc.29" xlink:type="simple">10.5334/joc.29</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000186.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Failing</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Theeuwes</surname> <given-names>J</given-names></name>. <article-title>Selection history: How reward modulates selectivity of visual attention</article-title>. <source>Psychon Bull Rev</source>. <year>2018</year>;<volume>25</volume>(<issue>2</issue>): <fpage>514</fpage>–<lpage>538</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13423-017-1380-y" xlink:type="simple">10.3758/s13423-017-1380-y</ext-link></comment> <object-id pub-id-type="pmid">28986770</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>BA</given-names></name>. <article-title>Dissociable components of experience-driven attention</article-title>. <source>Curr Biol</source>. <year>2019</year>;<volume>29</volume>(<issue>5</issue>): <fpage>841</fpage>–<lpage>845.e2</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2019.01.030" xlink:type="simple">10.1016/j.cub.2019.01.030</ext-link></comment> <object-id pub-id-type="pmid">30773366</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>X</given-names></name>. <article-title>Interaction between value and perceptual salience in value-driven attentional capture</article-title>. <source>J Vis</source>. <year>2013</year>;<volume>13</volume>(<issue>3</issue>): <fpage>1</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/13.3.1" xlink:type="simple">10.1167/13.3.1</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000186.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brainard</surname> <given-names>DH</given-names></name>. <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source>. <year>1997</year>;<volume>10</volume>(<issue>4</issue>): <fpage>433</fpage>–<lpage>436</lpage>. <object-id pub-id-type="pmid">9176952</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watson</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Pelli</surname> <given-names>DG</given-names></name>. <article-title>QUEST: a Bayesian adaptive psychometric method</article-title>. <source>Percept Psychophys</source>. <year>1983</year>;<volume>33</volume>(<issue>2</issue>): <fpage>113</fpage>–<lpage>120</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03202828" xlink:type="simple">10.3758/BF03202828</ext-link></comment> <object-id pub-id-type="pmid">6844102</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Itthipuripat</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ester</surname> <given-names>EF</given-names></name>, <name name-style="western"><surname>Deering</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>. <article-title>Sensory gain outperforms efficient readout mechanisms in predicting attention-related improvements in behavior</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>(<issue>40</issue>): <fpage>13384</fpage>–<lpage>98</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2277-14.2014" xlink:type="simple">10.1523/JNEUROSCI.2277-14.2014</ext-link></comment> <object-id pub-id-type="pmid">25274817</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Itthipuripat</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cha</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Byers</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name>. <article-title>Two different mechanisms support selective attention at different phases of training</article-title>. <source>PLoS Biol</source>. <year>2017</year>;<volume>15</volume>(<issue>6</issue>): <fpage>1</fpage>–<lpage>38</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.2001724" xlink:type="simple">10.1371/journal.pbio.2001724</ext-link></comment> <object-id pub-id-type="pmid">28654635</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pestilli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Carrasco</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Gardner</surname> <given-names>JL</given-names></name>. <article-title>Attentional enhancement via selection and pooling of early sensory responses in human visual cortex</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>72</volume>(<issue>5</issue>): <fpage>832</fpage>–<lpage>846</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.09.025" xlink:type="simple">10.1016/j.neuron.2011.09.025</ext-link></comment> <object-id pub-id-type="pmid">22153378</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Andersson</surname> <given-names>JLR</given-names></name>, <name name-style="western"><surname>Skare</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ashburner</surname> <given-names>J</given-names></name>. <article-title>How to correct susceptibility distortions in spin-echo echo-planar images: application to diffusion tensor imaging</article-title>. <source>Neuroimage</source>. <year>2003</year>;<volume>20</volume>(<issue>2</issue>): <fpage>870</fpage>–<lpage>888</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S1053-8119(03)00336-7" xlink:type="simple">10.1016/S1053-8119(03)00336-7</ext-link></comment> <object-id pub-id-type="pmid">14568458</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref095"><label>95</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Jenkinson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Woolrich</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Beckmann</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TEJ</given-names></name>, <name name-style="western"><surname>Johansen-Berg</surname> <given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title>. <source>Neuroimage</source>. <year>2004</year>;<volume>23</volume>(<issue>Suppl 1</issue>): <fpage>S208</fpage>–<lpage>S219</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/J.NEUROIMAGE.2004.07.051" xlink:type="simple">10.1016/J.NEUROIMAGE.2004.07.051</ext-link></comment> <object-id pub-id-type="pmid">15501092</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref096"><label>96</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jenkinson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Beckmann</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TEJ</given-names></name>, <name name-style="western"><surname>Woolrich</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>SM</given-names></name>. <article-title>FSL</article-title>. <source>Neuroimage</source>. <year>2012</year>;<volume>62</volume>(<issue>2</issue>): <fpage>782</fpage>–<lpage>790</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2011.09.015" xlink:type="simple">10.1016/j.neuroimage.2011.09.015</ext-link></comment> <object-id pub-id-type="pmid">21979382</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref097"><label>97</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Engel</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Rumelheart</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Wandell</surname> <given-names>BA</given-names></name>. <article-title>fMRI of human visual cortex</article-title>. <source>Nature</source>. <year>1994</year>;<volume>369</volume>(<issue>6481</issue>): <fpage>525</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/369525a0" xlink:type="simple">10.1038/369525a0</ext-link></comment> <object-id pub-id-type="pmid">8031403</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref098"><label>98</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Swisher</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Halko</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Merabet</surname> <given-names>LB</given-names></name>, <name name-style="western"><surname>Mcmains</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Somers</surname> <given-names>DC</given-names></name>. <article-title>Visual topography of human intraparietal sulcus</article-title>. <source>J Neurosci</source>. <year>2007</year>;<volume>27</volume>(<issue>20</issue>): <fpage>5326</fpage>–<lpage>5337</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0991-07.2007" xlink:type="simple">10.1523/JNEUROSCI.0991-07.2007</ext-link></comment> <object-id pub-id-type="pmid">17507555</object-id></mixed-citation></ref>
<ref id="pbio.3000186.ref099"><label>99</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dunn</surname> <given-names>OJ</given-names></name>. <article-title>Multiple comparisons among means</article-title>. <source>J Am Stat Assoc</source>. <year>1961</year>;<volume>56</volume>: <fpage>52</fpage>–<lpage>64</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>