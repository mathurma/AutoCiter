<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article article-type="research-article" dtd-version="3.0" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-00766</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004790</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Data visualization</subject><subj-group><subject>Infographics</subject><subj-group><subject>Graphs</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Pharmacology</subject><subj-group><subject>Drug interactions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Genome analysis</subject><subj-group><subject>Genetic networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Genomics</subject><subj-group><subject>Genome analysis</subject><subj-group><subject>Genetic networks</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject><subj-group><subject>Genetic networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Oncology</subject><subj-group><subject>Cancers and neoplasms</subject><subj-group><subject>Breast tumors</subject><subj-group><subject>Breast cancer</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Gene regulatory networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Gene regulatory networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Gene expression</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Random variables</subject><subj-group><subject>Covariance</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Pharmaceutics</subject><subj-group><subject>Pharmaceutical processing technology</subject><subj-group><subject>Drug synthesis</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Pathway-Based Genomics Prediction using Generalized Elastic Net</article-title>
<alt-title alt-title-type="running-head">Generalized Elastic Net</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Sokolov</surname> <given-names>Artem</given-names></name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Carlin</surname> <given-names>Daniel E.</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Paull</surname> <given-names>Evan O.</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Baertsch</surname> <given-names>Robert</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Stuart</surname> <given-names>Joshua M.</given-names></name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>Department of Biomolecular Engineering and Center for Biomolecular Science and Engineering, University of California Santa Cruz, Santa Cruz, California, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Przytycka</surname> <given-names>Teresa M.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>National Center for Biotechnology Information (NCBI), UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: AS JMS. Performed the experiments: AS. Analyzed the data: AS DEC EOP RB JMS. Contributed reagents/materials/analysis tools: DEC EOP RB. Wrote the paper: AS JMS.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">sokolov@soe.ucsc.edu</email> (AS); <email xlink:type="simple">jstuart@ucsc.edu</email> (JMS)</corresp>
</author-notes>
<pub-date pub-type="collection">
<month>3</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>9</day>
<month>3</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>3</issue>
<elocation-id>e1004790</elocation-id>
<history>
<date date-type="received">
<day>11</day>
<month>5</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>4</day>
<month>2</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Sokolov et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004790"/>
<abstract>
<p>We present a novel regularization scheme called The Generalized Elastic Net (GELnet) that incorporates gene pathway information into feature selection. The proposed formulation is applicable to a wide variety of problems in which the interpretation of predictive features using known molecular interactions is desired. The method naturally steers solutions toward sets of mechanistically interlinked genes. Using experiments on synthetic data, we demonstrate that pathway-guided results maintain, and often improve, the accuracy of predictors even in cases where the full gene network is unknown. We apply the method to predict the drug response of breast cancer cell lines. GELnet is able to reveal genetic determinants of sensitivity and resistance for several compounds. In particular, for an EGFR/HER2 inhibitor, it finds a possible trans-differentiation resistance mechanism missed by the corresponding pathway agnostic approach.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>The low costs of sequencing and other high-throughput technologies have made available large amounts of data to address molecular biology problems. However, often this means thousands of measurements, for example on gene expression, are assayed for a much smaller number of samples. The imbalance complicates the identification of genes that generalize to new samples and in finding a collection of genes that suggest a theme for interpreting the data. Pathway and network-based approaches have proven their worth in these situations. They force solutions onto known biology and they produce more robust predictors. In this manuscript, we describe a new formulation of statistical learning approaches that naturally incorporates gene-gene relationships, like those found in gene network databases. The theory we present helps unify and codify an explicit formulation for gene pathway-informed machine-learning that should have wide reach.</p>
</abstract>
<funding-group>
<funding-statement>This work was funded by: NIH LINCS Consortium Grant [U54HG006097] (<ext-link ext-link-type="uri" xlink:href="http://www.lincsproject.org/" xlink:type="simple">http://www.lincsproject.org/</ext-link>); National Cancer Institute TCGA Grant [5U24CA143858]; and National Science Foundation CAREER Award [DBI 0845783]. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="0"/>
<page-count count="23"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The advent of high-throughput sequencing technologies has led to the explosion in the amount of molecular-level data collected on biological samples. However, this explosion is somewhat single dimensional: the wealth of information on any particular sample nearly always exceeds the number of samples assayed. For example, The Cancer Genome Atlas (TCGA) datasets provide some of the most comprehensive molecular profiles of human tumors with tens of thousands of genomic and epigenomic features collected for each sample. Yet, the number of samples in these datasets is several orders of magnitude less (several hundred per tumor type). The severe under sampling of the high-dimensional space makes it very difficult to mine such data for biological or clinical insight. The difficulty was confirmed empirically by Boutros, <italic>et al</italic>., who demonstrated that there were over 500,000 six-gene signatures that make effective prognostic biomarkers for lung cancer [<xref ref-type="bibr" rid="pcbi.1004790.ref001">1</xref>]. Likewise, Venet, <italic>et al</italic>. showed that most randomly selected subsets of features are significantly associated with breast cancer outcomes [<xref ref-type="bibr" rid="pcbi.1004790.ref002">2</xref>].</p>
<p>From the machine learning standpoint, the issue is related to model complexity [<xref ref-type="bibr" rid="pcbi.1004790.ref003">3</xref>]. Many popular methods are formulated in such a way that the number of model parameters is dictated directly by the number of data features. Thus, models constructed in higher-dimensional spaces tend to be more flexible, but require more samples for accurate estimation of parameter values [<xref ref-type="bibr" rid="pcbi.1004790.ref004">4</xref>]. Without an adequate number of samples, training becomes an under-constrained problem, as more than one set of parameter values is capable of modeling the data with perfect accuracy. Selecting the right model is not always trivial, and choosing poorly results in overfitting and a lack of biological relevance. The issue is further complicated by the fact that canonical methods for model selection, such as cross-validation, require further partitioning of an already small set of samples and often lead to biased estimates of performance [<xref ref-type="bibr" rid="pcbi.1004790.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004790.ref006">6</xref>].</p>
<p>One way to control for model complexity is to perform dimensionality reduction by selecting a subset of features relevant to a particular prediction task. While there is a large body of literature addressing the general problem of variable selection (e.g., [<xref ref-type="bibr" rid="pcbi.1004790.ref007">7</xref>]), biological data is a special case in that the variables, which usually correspond to features collected on a per-gene basis, are not independent. Genes do not function in isolation but rather work together within various metabolic, regulatory and signaling pathways. The inter-dependencies among the genes is often represented as a collection of interactions. This information can be used to impose additional constraints on the prediction tasks, forcing training methods to select meaningful groups of features rather than individual genes.</p>
<p>In this paper, we propose to integrate domain knowledge regarding features, such as gene interaction information, through the use of model regularization. To achieve this, we provide a generalization of elastic nets [<xref ref-type="bibr" rid="pcbi.1004790.ref008">8</xref>] and demonstrate how gene interactions can be added into a wide array of supervised and unsupervised prediction methods. While we focus primarily on linear models, we discuss extensions to other, potentially non-linear, loss functions. Our empirical results demonstrate that the proposed framework provides more robustness over the standard elastic nets in cancer-related prediction tasks.</p>
<p>The paper is structured as follows. The Related Work section highlights methods that incorporate gene interaction information into machine learning predictors. In the Methods section, we propose a generalization of the elastic net regularizer. We describe a cyclic coordinate descent method for training regression models with the new regularizers, and show how several common prediction tasks can be reduced to this regression-based formulation. In the Results section, we evaluate our method on synthetic data and then apply it to the prediction of drug sensitivity in breast cancer cell lines. We highlight several key points of the proposed regularization scheme and connect the learned models to cancer biology and mechanisms of resistance.</p>
<sec id="sec002">
<title>Related work</title>
<p>Previous approaches that combine gene interaction data with genomic features can be roughly divided into three categories. The first category focuses on feature modification, with the most popular approach being dimensionality reduction by grouping genes together according to functional categories. A single summary measure is then derived for each category, resulting in a feature space of much lower dimensionality than the original. All of the traditional analysis methods can then be applied in the resulting lower-dimensional setting, yielding higher robustness of the trained models [<xref ref-type="bibr" rid="pcbi.1004790.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1004790.ref012">12</xref>]. Outside the realm of dimensionality reduction, methods that perform feature modification on the basis of gene interaction data include computing local entropy measures [<xref ref-type="bibr" rid="pcbi.1004790.ref013">13</xref>], spin states in an Ising model [<xref ref-type="bibr" rid="pcbi.1004790.ref014">14</xref>], and diffusion kernel values [<xref ref-type="bibr" rid="pcbi.1004790.ref015">15</xref>].</p>
<p>At the other end, the second category of methods operates by first applying predictors to obtain a set of discriminative scores—one for each feature / gene—and then using gene sets to elucidate pathways saturated with highly discriminative scores. Gene Set Enrichment Analysis [<xref ref-type="bibr" rid="pcbi.1004790.ref016">16</xref>], Significance Analysis of Function and Expression [<xref ref-type="bibr" rid="pcbi.1004790.ref017">17</xref>], and a method by Lee, et al. [<xref ref-type="bibr" rid="pcbi.1004790.ref056">56</xref>] do exactly this to arrive at a single differential score for each gene set of interest. Rather than using curated sets of genes, one may also place differential scores directly onto the gene interaction network and look for saturated subnetworks. The problem is NP-hard [<xref ref-type="bibr" rid="pcbi.1004790.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1004790.ref019">19</xref>] and thus requires approximation techniques such as Simulated Annealing [<xref ref-type="bibr" rid="pcbi.1004790.ref018">18</xref>], node color coding methods [<xref ref-type="bibr" rid="pcbi.1004790.ref019">19</xref>] and diffusion heat models on graphs with single [<xref ref-type="bibr" rid="pcbi.1004790.ref020">20</xref>] and multiple [<xref ref-type="bibr" rid="pcbi.1004790.ref021">21</xref>] data types.</p>
<p>The first two categories utilize gene interaction information in a way that is entirely decoupled from the underlying predictor method: any algorithm that maps genomic input features to phenotypic outcomes and produces discriminative scores for each feature can be used. While this allows for a higher level of generality, intuitively, one would expect to achieve better accuracy if the predictor was able to utilize gene interactions directly, as part of its training. Methods that follow this intuition make up the third category, and we highlight several methods that make direct use of gene interaction networks as part of training.</p>
<p>Dutkowski and Ideker proposed Network-Guided Forests [<xref ref-type="bibr" rid="pcbi.1004790.ref022">22</xref>], a method that uses discriminative genomic features as decision tree nodes while forcing the edges between the nodes to coincide with a gene interaction network. The intuition behind Network-Guided Forests was later extended to a clustering setting, where gene network information was used with somatic mutations to derive Network-Based Stratification, a method for identifying clinically-relevant cancer subtypes [<xref ref-type="bibr" rid="pcbi.1004790.ref023">23</xref>].</p>
<p>In a linear model setting, several methods have used gene network information to guide feature selection during training. Johannes, <italic>et al</italic> performed recursive feature elimination for Support Vector Machines (SVMs), using GeneRank to assign network-based importance weights to features at each elimination step [<xref ref-type="bibr" rid="pcbi.1004790.ref024">24</xref>]. Jang, <italic>et al</italic> proposed Stepwise Group Sparse Regression (SGSR) that utilized the grouping of genes according to functional pathways, rather than network connectivity directly [<xref ref-type="bibr" rid="pcbi.1004790.ref025">25</xref>]. SGSR is an iterative procedure that is initialized to a sparse LASSO-regularized linear model; at each iteration, the method adds a functional group of genes that results in large improvement in classification accuracy until the model is saturated and no further improvement is possible. Another LASSO-based model is Sparse Group LASSO (SGL), which was utilized by Silver, et al. to find genes and pathways associated with high-density lipoprotein cholesterol in a genome-wide association study [<xref ref-type="bibr" rid="pcbi.1004790.ref052">52</xref>]. SGL accepts a collection of pathways as input and induces sparsity at both the pathway and the gene level [<xref ref-type="bibr" rid="pcbi.1004790.ref053">53</xref>].</p>
<p>Perhaps the closest two methods to the approach presented here are Network-Induced Classification Kernels (NICK) [<xref ref-type="bibr" rid="pcbi.1004790.ref026">26</xref>] and Network-Constrained Regularization by Li &amp; Li [<xref ref-type="bibr" rid="pcbi.1004790.ref055">55</xref>], which use the Laplacian of a gene interaction graph to force neighboring genes to have similar weights. As described below, both methods can be seen as a special case of our proposed framework.</p>
<p>Methods that take advantage of gene interaction data in training are often closely tied to the underlying choice of a predictor. This places a severe limitation on the scope of prediction tasks (e.g., classification) that can be addressed, and generalization to other tasks (e.g., regression) may be difficult. We propose using model regularization to integrate feature relationship information, extendable to a large spectrum of supervised and unsupervised prediction tasks. In doing so, we bridge the gap between the freedom in choosing the underlying predictor and the ability of that predictor to utilize domain knowledge.</p>
</sec>
</sec>
<sec id="sec003" sec-type="materials|methods">
<title>Methods</title>
<p>We consider the standard setting for a linear model, in which we are given a set of <italic>n</italic> training data examples <inline-formula id="pcbi.1004790.e001"><alternatives><graphic id="pcbi.1004790.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>, a feature map <italic>ϕ</italic> over the input space, and a loss function <inline-formula id="pcbi.1004790.e002"><alternatives><graphic id="pcbi.1004790.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:mi mathvariant="script">L</mml:mi> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Our aim is to learn a linear function <italic>h</italic>(<bold>x</bold>) = <bold>w</bold><sup><italic>T</italic></sup> <italic>ϕ</italic>(<bold>x</bold>) + <italic>b</italic> such that
<disp-formula id="pcbi.1004790.e003"><alternatives><graphic id="pcbi.1004790.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi mathvariant="script">R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
is minimized. Here, <inline-formula id="pcbi.1004790.e004"><alternatives><graphic id="pcbi.1004790.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mi mathvariant="script">R</mml:mi></mml:math></alternatives></inline-formula> denotes the regularization penalty on <bold>w</bold> and is the focal point of this work.</p>
<p>The two most well-known forms of regularization in linear models are ridge regression [<xref ref-type="bibr" rid="pcbi.1004790.ref027">27</xref>] and LASSO [<xref ref-type="bibr" rid="pcbi.1004790.ref028">28</xref>], which minimize the L2-norm and L1-norm of <bold>w</bold>, respectively. LASSO tends to produce sparser models, but is limited by the number of samples in the dataset, while ridge regression is better at finding sets of correlated features, but lacks the sparsity of the LASSO models. More recently, Zhou and Hastie proposed the elastic net, which combines the two via a linear combination [<xref ref-type="bibr" rid="pcbi.1004790.ref008">8</xref>]: <inline-formula id="pcbi.1004790.e005"><alternatives><graphic id="pcbi.1004790.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mi mathvariant="script">R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mrow><mml:mo>∥</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>∥</mml:mo></mml:mrow> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mfrac><mml:msub><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mn>2</mml:mn></mml:mfrac> <mml:msubsup><mml:mrow><mml:mo>∥</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. The elastic net brings together the strong points of ridge regression and LASSO, while effectively addressing the drawbacks of both.</p>
<p>In this paper, we propose the generalized elastic net (GELnet) of the form
<disp-formula id="pcbi.1004790.e006"><alternatives><graphic id="pcbi.1004790.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:munder> <mml:msub><mml:mi>d</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>|</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:msub><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mn>2</mml:mn></mml:mfrac> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi>P</mml:mi> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where <bold>d</bold> and <italic>P</italic> are additional penalty weights for individual features and pairs of features, respectively. These provide an intuitive way to guide variable selection via domain knowledge. Setting <italic>d</italic><sub><italic>j</italic></sub> = 1, ∀<italic>j</italic> and <italic>P</italic> to the identity matrix produces the traditional elastic net.</p>
<sec id="sec004">
<title>The penalty matrix <italic>P</italic></title>
<p>To make the learning problem in <xref ref-type="disp-formula" rid="pcbi.1004790.e003">Eq (1)</xref> well-defined, the regularizer must be bound from below. This translates to the requirement that <italic>d</italic><sub><italic>j</italic></sub> ≥ 0, ∀<italic>j</italic>, and <italic>P</italic> must be a positive semi-definite matrix. The latter is satisfied by any kernel matrix, allowing one to directly apply the wealth of kernels defined in the literature, with one caveat.</p>
<p>Kernels are often treated as measures of similarity [<xref ref-type="bibr" rid="pcbi.1004790.ref029">29</xref>]. However, <italic>P</italic> in <xref ref-type="disp-formula" rid="pcbi.1004790.e006">Eq (2)</xref> drives the L2 penalty term and should, therefore, align with a measure of <italic>dissimilarity</italic>. Intuitively, we would like to penalize high values of <italic>w</italic><sub><italic>i</italic></sub> and <italic>w</italic><sub><italic>j</italic></sub> if features <italic>i</italic> and <italic>j</italic> are, in some sense, dissimilar. Such a penalty is more conducive towards finding correlated features, mimicking the behavior of the traditional elastic nets [<xref ref-type="bibr" rid="pcbi.1004790.ref008">8</xref>]. For this reason, we advocate using the pseudo-inverse of a kernel matrix as a choice for <italic>P</italic>, not the kernel matrix itself. The pseudo-inverse maintains the positive semi-definite property, while being more in line with the aforementioned intuition.</p>
<p>To further motivate the use of the pseudo-inverse, consider the following example. When dealing with gene interaction networks, we may be interested in assigning similar weights to genes that are close together on the network [<xref ref-type="bibr" rid="pcbi.1004790.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1004790.ref055">55</xref>]. Given an adjacency matrix <italic>A</italic> for a graph, we formulate the following regularizer:
<disp-formula id="pcbi.1004790.e007"><alternatives><graphic id="pcbi.1004790.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mn>2</mml:mn></mml:mfrac> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:munder> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula></p>
<p>This is closely related to graph embedding [<xref ref-type="bibr" rid="pcbi.1004790.ref030">30</xref>], where a graph structure is imposed over a set of samples and one seeks to reduce data dimensionality in a way that preserves node proximity in the lower-dimensional space.</p>
<p>The regularizer in <xref ref-type="disp-formula" rid="pcbi.1004790.e007">Eq (3)</xref> can be simplified to
<disp-formula id="pcbi.1004790.e008"><alternatives><graphic id="pcbi.1004790.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mn>2</mml:mn></mml:mfrac> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi>L</mml:mi> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>L</italic> is the graph Laplacian [<xref ref-type="bibr" rid="pcbi.1004790.ref031">31</xref>]. This is a GELnet with <italic>P</italic> = <italic>L</italic> and <bold>d</bold> = 0. The spectral decomposition of the Laplacian constitutes a Hilbert space, while its pseudo-inverse, <italic>L</italic><sup>+</sup>, is the reproducing kernel of that space [<xref ref-type="bibr" rid="pcbi.1004790.ref032">32</xref>]. By now, the connection should be clear: <italic>L</italic><sup>+</sup> is a kernel matrix that captures similarity between features using their proximity on a graph, while its pseudo-inverse, (<italic>L</italic><sup>+</sup>)<sup>+</sup> = <italic>L</italic>, appears in the L2 regularizer to penalize high weights of distant features in the graph.</p>
<p>Lavi, <italic>et al</italic>. use a similar intuition to develop a method called Network-Induced Classification Kernels (NICK) for SVMs [<xref ref-type="bibr" rid="pcbi.1004790.ref026">26</xref>]. Rather than using <italic>L</italic> directly, the authors formulate an L2 regularizer around a linear combination of the Laplacian with the identity matrix: (<italic>I</italic>+<italic>βL</italic>) for some <italic>β</italic> ≥ 0. In their method, the parameter <italic>β</italic> provides a trade-off between graph-driven regularization and the traditional ridge regression penalty of the SVMs. The NICK method can be seen as a special case of the framework proposed in this paper, where a GELnet with <italic>P</italic> = (<italic>I</italic> + <italic>βL</italic>) and <bold>d</bold> = 0 regularizes the hinge loss of the SVM. Likewise, Li &amp; Li use the graph Laplacian to solve a set of regression tasks [<xref ref-type="bibr" rid="pcbi.1004790.ref055">55</xref>]. Their method can be seen as a special case of our framework, where a GELnet with <italic>P</italic> = <italic>L</italic> and <bold>d</bold> = 1 regularizes the squared-error loss.</p>
<p>One drawback of the graph Laplacian is that it characterizes a node’s immediate neighborhood only, which may be inadequate for some applications. A natural extension beyond immediate adjacency is the diffusion kernel [<xref ref-type="bibr" rid="pcbi.1004790.ref033">33</xref>]. The kernel arises from a simulated physical process, where “heat” is applied to one node in the graph and the “temperature” is measured in another node, after the heat is allowed to diffuse along the graph edges. If the two nodes are localized to the same subgraph, this heat-based similarity measure will be high. When dealing with gene interaction networks, such subgraphs may correspond to genetic pathways, motivating the use of the diffusion kernel in place of <italic>L</italic><sup>+</sup> to discover the underlying molecular mechanisms. This intuition lies behind other diffusion-based methods, such as HotNet [<xref ref-type="bibr" rid="pcbi.1004790.ref020">20</xref>]. If <italic>D</italic> is a diffusion kernel, computed as a matrix exponential of the graph Laplacian, and <italic>I</italic> is the identity matrix, then setting the penalty matrix <italic>P</italic> = <italic>I</italic> − <italic>D</italic> will correctly assign lower penalty to “hot” pairs of nodes. Note that because all eigenvalues of <italic>D</italic> lie in [0, 1], <italic>P</italic> is positive semi-definite.</p>
<p>While most of our attention has been given to gene interaction networks, we reiterate that GELnets are more general and can accommodate <italic>any</italic> positive semi-definite measure of dissimilarity between pairs of features. For example, we may be interested in grouping features together according to some predetermined factor and defining <italic>P</italic> in such a way as to penalize selection of feature pairs that do not belong to the same group. This is closely related to group LASSO [<xref ref-type="bibr" rid="pcbi.1004790.ref034">34</xref>], where the regularization penalty behaves as LASSO for predefined groups of variables, and as ridge regression for individual variables within those groups. Group LASSO is limited by its inability to identify and remove noisy variables within a particular group, without excluding the entire group altogether. This limitation is overcome by GELnets, where the LASSO penalty is assigned to individual variables.</p>
</sec>
<sec id="sec005">
<title>Learning</title>
<p>We discuss how to solve <xref ref-type="disp-formula" rid="pcbi.1004790.e003">Eq (1)</xref> for a specific form of the loss function and then show how several of the common learning problems can be expressed using this loss. The presented loss function arises directly from standard regression and is defined by the weighted sum of squared residuals. Consider the problem
<disp-formula id="pcbi.1004790.e009"><alternatives><graphic id="pcbi.1004790.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo form="prefix" movablelimits="true">min</mml:mo> <mml:mi>L</mml:mi> <mml:mo>=</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">min</mml:mo> <mml:mi mathvariant="bold">w</mml:mi></mml:munder> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>n</mml:mi></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi>ϕ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>p</mml:mi></mml:munderover> <mml:msub><mml:mi>d</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>|</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:msub><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mn>2</mml:mn></mml:mfrac> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi>P</mml:mi> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
where <inline-formula id="pcbi.1004790.e010"><alternatives><graphic id="pcbi.1004790.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is the training data, <italic>a</italic><sub><italic>i</italic></sub> are the sample weights, and <italic>d</italic><sub><italic>j</italic></sub>, <italic>P</italic> encode domain-specific information regarding feature importance and association. Note that we will generally use <italic>i</italic> to iterate over the samples and <italic>j</italic> to iterate over the features.</p>
<p>We solve the problem in <xref ref-type="disp-formula" rid="pcbi.1004790.e009">Eq (5)</xref> through cyclic coordinate descent by changing one <italic>w</italic><sub><italic>k</italic></sub> at a time, while keeping the values of <italic>w</italic><sub><italic>j</italic></sub> fixed for all <italic>j</italic> ≠ <italic>k</italic>[<xref ref-type="bibr" rid="pcbi.1004790.ref035">35</xref>]. The coordinate descent methods have been recently growing in popularity, giving rise to libraries like <italic>glmnet</italic>[<xref ref-type="bibr" rid="pcbi.1004790.ref036">36</xref>] and <italic>LIBLINEAR</italic>[<xref ref-type="bibr" rid="pcbi.1004790.ref037">37</xref>]. Their primary advantage is the fact that objective functions with a single variable can be solved in closed-form, leading to simple update rules and efficient implementations. Friedman and Hastie demonstrated that this can sometimes lead to ten-fold decreases in run time over the more traditional optimization methods for linear models [<xref ref-type="bibr" rid="pcbi.1004790.ref035">35</xref>].</p>
<p>For notational convenience, we define <inline-formula id="pcbi.1004790.e011"><alternatives><graphic id="pcbi.1004790.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>b</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, which is the prediction for sample <italic>i</italic>, made by the model when feature <italic>k</italic> is excluded. Our goal is to find the value of <italic>w</italic><sub><italic>k</italic></sub> that minimizes the remaining residual. We solve the subproblem
<disp-formula id="pcbi.1004790.e012"><alternatives><graphic id="pcbi.1004790.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo form="prefix" movablelimits="true">min</mml:mo> <mml:msub><mml:mi>L</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munder><mml:mo form="prefix" movablelimits="true">min</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:munder> <mml:mfenced close="" open="[" separators=""><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>n</mml:mi></mml:mrow></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>y</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mn>2</mml:mn></mml:msup></mml:mfenced></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>d</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>w</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>+</mml:mo></mml:mrow> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mi>d</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>|</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mfenced close="]" open="" separators=""><mml:mfrac><mml:msub><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mn>2</mml:mn></mml:mfrac> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mi>w</mml:mi> <mml:mi>k</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msub><mml:mi>w</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:munder> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:msup><mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>≠</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:msup><mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub> <mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msub><mml:mi>w</mml:mi> <mml:msup><mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:msub></mml:mfenced></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
by taking a partial derivative with respect to <italic>w</italic><sub><italic>k</italic></sub> and setting it equal to zero:
<disp-formula id="pcbi.1004790.e013"><alternatives><graphic id="pcbi.1004790.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>L</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>y</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>d</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mfrac><mml:mrow><mml:mrow><mml:mi>∂</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>w</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:msub><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>w</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mfenced> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>This results in the following update rule for <italic>w</italic><sub><italic>k</italic></sub>:
<disp-formula id="pcbi.1004790.e014"><alternatives><graphic id="pcbi.1004790.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>←</mml:mo> <mml:mfrac><mml:mrow><mml:mi>S</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:msubsup> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>y</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced> <mml:mo>-</mml:mo> <mml:msub><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="3.33333pt"/><mml:msub><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>d</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mfenced></mml:mrow> <mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:msubsup> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
where <italic>S</italic>(<italic>v</italic>, <italic>γ</italic>) = <italic>sgn</italic>(<italic>v</italic>)(|<italic>v</italic>| − <italic>γ</italic>)<sub>+</sub> is the soft-threshold operator that “snaps” values within <italic>γ</italic> of zero to be exactly zero [<xref ref-type="bibr" rid="pcbi.1004790.ref035">35</xref>]. The soft-threshold operator contributes greatly to faster run times when the LASSO penalty coefficient <italic>λ</italic><sub>1</sub> is not zero. The reason for this speedup is the fact that a <italic>w</italic><sub><italic>k</italic></sub> that was previously “snapped” to be exactly zero will remain at zero, unless its absolute value exceeds <italic>λ</italic><sub>1</sub> <italic>d</italic><sub><italic>k</italic></sub>. When <italic>w</italic><sub><italic>k</italic></sub> is zero, it makes no contribution to partial fits <inline-formula id="pcbi.1004790.e015"><alternatives><graphic id="pcbi.1004790.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msubsup><mml:mi>y</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> for all other <italic>j</italic> ≠ <italic>k</italic>. Thus, if the value of <italic>w</italic><sub><italic>k</italic></sub> remains at zero, no updates to <inline-formula id="pcbi.1004790.e016"><alternatives><graphic id="pcbi.1004790.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:msubsup><mml:mi>y</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> are required, allowing those quantities to be cached. Higher values of <italic>λ</italic><sub>1</sub> will therefore lead to both sparser solutions and faster convergence times.</p>
<p>Similar to the weight updates above, we can differentiate the objective with respect to the bias term. The derivative is given by
<disp-formula id="pcbi.1004790.e017"><alternatives><graphic id="pcbi.1004790.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi>L</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi>ϕ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
which leads to the following update rule for <italic>b</italic>:
<disp-formula id="pcbi.1004790.e018"><alternatives><graphic id="pcbi.1004790.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>b</mml:mi> <mml:mo>←</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi>ϕ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula></p>
<p>Note that this is a simple weighted average of the residuals.</p>
<p>Using Eqs (<xref ref-type="disp-formula" rid="pcbi.1004790.e014">6</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1004790.e018">8</xref>), we can derive an upper bound on the “meaningful” values of the <italic>λ</italic><sub>1</sub> meta-parameter. Specifically, by initializing all <italic>w</italic><sub><italic>k</italic></sub> to zero and <italic>b</italic> to <inline-formula id="pcbi.1004790.e019"><alternatives><graphic id="pcbi.1004790.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, setting <italic>λ</italic><sub>1</sub> to any value higher than
<disp-formula id="pcbi.1004790.e020"><alternatives><graphic id="pcbi.1004790.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mi>j</mml:mi></mml:munder> <mml:mfenced close="|" open="|" separators=""><mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:msub><mml:mi>d</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
guarantees that all <italic>w</italic><sub><italic>k</italic></sub> will remain at zero and no updates will be made.</p>
<p>The training procedure cycles through all the coordinates and the bias term until the desired stopping criterion is reached. In our experiments, we used both the number of iterations and the difference in the objective value between updates as the convergence criteria. For the latter, we terminated training whenever that difference fell below a certain threshold <italic>ϵ</italic>. We make the code available as an R package <italic>gelnet</italic>.</p>
<p>Many other loss functions can be reduced to regression. In <xref ref-type="supplementary-material" rid="pcbi.1004790.s002">S1 Text</xref>, we review how this can be done for several popular methods via Taylor-series expansion [<xref ref-type="bibr" rid="pcbi.1004790.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1004790.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1004790.ref048">48</xref>]. Our review also shows how to handle loss functions that are non-covex ratios of quadratic norms (such as Principal Component Analysis [<xref ref-type="bibr" rid="pcbi.1004790.ref050">50</xref>] and Linear Discriminant Analysis [<xref ref-type="bibr" rid="pcbi.1004790.ref049">49</xref>]) using a method developed by Witten and Tibshirani [<xref ref-type="bibr" rid="pcbi.1004790.ref051">51</xref>].</p>
</sec>
<sec id="sec006">
<title>Experimental setup</title>
<p>We begin with experiments on synthetic data to investigate a key question: under what circumstances does the prior information about the gene regulatory network help prediction performance? To answer this question, we generate synthetic data from predefined gene-gene relationships and then compare the performance of classical elastic nets to GELnets, where the gene interactions are provided to the latter via the penalty matrix <italic>P</italic>. As we show below, GELnets are able to correctly utilize such prior information to achieve better accuracy.</p>
<p>For synthetic data experiments, we consider a randomly-generated scale-free graph. The associated adjacency matrix <italic>A</italic> has an entry of 1 if two nodes share an edge in the graph and an entry of 0 otherwise. We begin the experiment by using <italic>A</italic> to generate the “true” weight vector <bold>w</bold>. The goal of <bold>w</bold> is to simulate a signaling pathway, whose activity contributes to the phenotypic observations. The prediction task then aims to uncover this pathway from the observable data. To simulate a signaling pathway, we select a connected subcomponent of our scale-free graph via a random walk. The walk is terminated when 10% of the nodes are selected. The feature weights <bold>w</bold><sub><italic>j</italic></sub> are set to 1 for <italic>j</italic> in the selected set and to 0 for all other nodes.</p>
<p>We simulate gene expression data from a multivariate normal distribution: <italic>X</italic> ∼ <italic>N</italic>(0, <italic>S</italic>), where we consider two scenarios for specifying the covariance matrix <italic>S</italic>. In the first scenario, a random covariance matrix is used. This creates a decoupling between the simulated expression data <italic>X</italic> and the simulated signaling pathway <bold>w</bold>, modeling the negative control case in which the observable data has no relationship to the gene regulatory network. The second scenario assumes that the feature covariance structure is dictated by the graph adjacency matrix <italic>A</italic>. A Gaussian Graphical Model (GGM) is used, with <italic>S</italic> selected such that <italic>S</italic><sup>−1</sup> closely approximates <italic>A</italic>[<xref ref-type="bibr" rid="pcbi.1004790.ref038">38</xref>]. The GGM imposes a coupling between <italic>X</italic> and <bold>w</bold> that models a biological scenario, where the observable phenotype is driven by a small number of genomic correlates belonging to the same signaling pathway, while the rest of the simulated transcriptome is expressed according to the regulatory relationships encoded by <italic>A</italic>.</p>
<p>To simulate a typical high-dimensional low-sample scenario found in biological applications, we made use of a 5000-by-5000 adjacency matrix and generated 50 samples for each of the two scenarios above. The observable phenotypic response in all cases was computed as <bold>y</bold> = <bold>w</bold><sup><italic>T</italic></sup> <italic>X</italic>. Note that because the data dimensionality vastly exceeds the number of samples, the problem of reconstructing the signaling pathway <bold>w</bold> from gene expression <italic>X</italic> and phenotypic observations <bold>y</bold> is under-determined.</p>
<p>The simulated data (<italic>X</italic>,<bold>y</bold>) defines a regression problem, to which we apply the classical Elastic Nets and the GELnets, comparing the two regularization schemes. We provide additional information about feature-feature relationships to GELnets through the penalty matrix <italic>P</italic>, as in <xref ref-type="disp-formula" rid="pcbi.1004790.e006">Eq (2)</xref>. All individual penalty weights <italic>d</italic><sub><italic>j</italic></sub> are left at 1.0. To evaluate how performance is affected by prior knowledge in the form of feature-feature relationships, we consider two distinct choices for <italic>P</italic>. The first choice encapsulates the true information via the normalized Laplacian of the graph adjacency matrix <italic>A</italic>. For the second choice of <italic>P</italic>, we investigate the effect of providing the “wrong” information to the GELnets, by using the normalized Laplacian of another graph adjacency matrix <italic>A</italic>′. This matrix <italic>A</italic>′ is constructed by randomly permuting the columns (and, to subsequently maintain symmetry, rows) of <italic>A</italic>; such a permutation operator maintains the overall structure of the graph, while scrambling the individual feature-feature relationships.</p>
<p>We introduced two ways to generate the data matrix <italic>X</italic> and two ways to specify the feature-feature penalty matrix <italic>P</italic>. Together, this setup gives rise to four scenarios; we refer to these as <italic>Rand+</italic>, <italic>Rand-</italic>, <italic>GGM+</italic>, and <italic>GGM-</italic>, where the prefix specifies whether the data is generated with a random covariance matrix (<italic>Rand</italic>) or via a GGM (<italic>GGM</italic>), and the suffix denotes whether the normalized Laplacian is computed over the true adjacency matrix (+) or the permuted one (−). <xref ref-type="fig" rid="pcbi.1004790.g001">Fig 1</xref> summarizes how the four scenarios differ from each other. As we show below, the relative performance of Elastic Nets and GELnets can vary drastically from one scenario to the next.</p>
<fig id="pcbi.1004790.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004790.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Overview of four synthetic data scenarios.</title>
<p>In GGM+ and GGM-, the same network is used to simulate the gene expression matrix X and the signaling pathway represented by the weight vector <bold>w</bold>. That network is then provided to the GELnet in the GGM+ scenario, while a permuted version is used in the GGM- scenario. Scenarios Rand+ and Rand- are constructed in a similar fashion, but using a random covariance matrix instead.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.g001" xlink:type="simple"/>
</fig>
<p>The performance was evaluated in a leave-pair-out cross-validation (LPOCV) setting, due to its tendency to yield less bias in performance estimation [<xref ref-type="bibr" rid="pcbi.1004790.ref005">5</xref>]. We focus on three specific performance metrics:</p>
<list list-type="bullet">
<list-item>
<p>the reconstruction error, measured as <inline-formula id="pcbi.1004790.e021"><alternatives><graphic id="pcbi.1004790.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:msqrt><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>T</mml:mi></mml:msup> <mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> using the true weight vector <bold>w</bold> and its estimate <inline-formula id="pcbi.1004790.e022"><alternatives><graphic id="pcbi.1004790.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>;</p>
</list-item>
<list-item>
<p>the root mean squared error (RMSE), <inline-formula id="pcbi.1004790.e023"><alternatives><graphic id="pcbi.1004790.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:msqrt><mml:mrow><mml:mo>‖</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:msubsup><mml:mo>‖</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>, between the true response <italic>y</italic> and the predictions <inline-formula id="pcbi.1004790.e024"><alternatives><graphic id="pcbi.1004790.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>;</p>
</list-item>
<list-item>
<p>and dispersion, measured using the normalized Laplacian <italic>L</italic> as <inline-formula id="pcbi.1004790.e025"><alternatives><graphic id="pcbi.1004790.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>Z</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>Z</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>L</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>Z</mml:mi> <mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, where <italic>Z</italic> is the set of nodes associated with the non-zero feature weights in the model.</p>
</list-item>
</list>
<p>The first performance measure captures how accurately we are able to recover the original signaling pathway that gave rise to the observable phenotypic response. Note that in real applications, the true weight vector <bold>w</bold> is unknown, and the reconstruction error is therefore not directly observable. RMSE is a standard performance metric for regression problems, capturing the deviation between predicted and observed response.</p>
<p>Dispersion measures the degree to which features found to be predictive are near one another in network space. The metric we use arises directly from the L2-norm regularization term and acts as a positive control. Specifically, when the set of nodes <italic>Z</italic> is completely disconnected, the corresponding normalized Laplacian is the identity matrix and dispersion is equal to 1. Conversely, for every edge that appears between the nodes in <italic>Z</italic>, the corresponding off-diagonal entry in the normalized Laplacian will be negative, resulting in a lower dispersion value. This is a positive control, because GELnet regularization directly minimizes dispersion in its L2-norm term. Consequently, we expect dispersion to always be lower in the GELnet models, compared to their Elastic Net counterparts.</p>
<p>To address the question of meta-parameter selection, we average performance measures across a grid of meta-parameter values to obtain a <italic>marginalized</italic> estimate. Specifically, we iterate <italic>λ</italic><sub>2</sub> over { 10,000, 1,000, 100, 10, 1 } for both regularization schemes. For the Elastic Nets, we also iterate <italic>λ</italic><sub>1</sub> over <inline-formula id="pcbi.1004790.e026"><alternatives><graphic id="pcbi.1004790.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:mo>{</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msubsup> <mml:mn>27</mml:mn></mml:mfrac> <mml:mo>,</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msubsup> <mml:mn>9</mml:mn></mml:mfrac> <mml:mo>,</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msubsup> <mml:mn>3</mml:mn></mml:mfrac> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1004790.e027"><alternatives><graphic id="pcbi.1004790.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:msubsup><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is defined in <xref ref-type="disp-formula" rid="pcbi.1004790.e020">Eq (9)</xref>. Basing the choice of <italic>λ</italic><sub>1</sub> off <inline-formula id="pcbi.1004790.e028"><alternatives><graphic id="pcbi.1004790.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:msubsup><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> allows us to consider models of varying sparsity. The values of <italic>λ</italic><sub>1</sub> for the GELnet models were specified such that the number of non-zero feature weights equaled the corresponding Elastic Net models to allow for a fair comparison. Based on our preliminary experiments with parameter tuning, we found that such a grid covers a wide range of models.</p>
<p>As discussed in the literature, marginalized performance estimates are useful for method comparison [<xref ref-type="bibr" rid="pcbi.1004790.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004790.ref039">39</xref>]. Note that while we marginalize over the meta-parameters, the performance estimates are still conditional on the training data, which effectively allows us to ask “which of the two methods yields better performance, <italic>given a particular training set</italic>?”. This is important as we are not claiming that GELnet regularization is universally better than classical Elastic Nets, nor should we expect it to be. Besides the “No Free Lunch” considerations [<xref ref-type="bibr" rid="pcbi.1004790.ref040">40</xref>], we expect a given biological network to be relevant in a subset of prediction tasks. Thus, a key question is not <italic>whether</italic> GELnet pathway-based regularization is better, but <italic>under what conditions</italic> does it boost performance. Answering this question will help us properly utilize prior biological information to gain novel insight in bioinformatics applications.</p>
</sec>
</sec>
<sec id="sec007" sec-type="results">
<title>Results</title>
<sec id="sec008">
<title>Synthetic data</title>
<p><xref ref-type="fig" rid="pcbi.1004790.g002">Fig 2</xref> presents the performance of both regularization schemes in the two scenarios where the data was generated with a GGM; the same network was used to simulate both the gene expression <italic>X</italic> and the signaling network <bold>w</bold>. The GELnets are able to more accurately recover the simulated signaling pathway <bold>w</bold> when the information about true feature-feature relationships is provided (GGM+ case), and the inverse is true when the GELnets are given the scrambled relationship information (GGM- case). The latter is explained by GELnets selecting features in close proximity on the <italic>scrambled</italic> network, which is unlikely to contain the connected subcomponent encoded by <bold>w</bold>.</p>
<fig id="pcbi.1004790.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004790.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Performance of elastic nets and GELnets on synthetic data generated via a GGM.</title>
<p>Plotted are 30 trials of the same experiment. The x- and y-axes in every plot correspond to Elastic Nets and GELnets, respectively. The top three plots show the scenario where the GELnets were provided the true feature-feature relationships, while the bottom three plots correspond to the scrambled network case. Lower values are better for all three performance metrics, and the points are colored in red whenever the performance metrics are lower in the GELnet models, and blue otherwise.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.g002" xlink:type="simple"/>
</fig>
<p>As a positive control, we note that all GELnet solutions have lower dispersion than the corresponding Elastic Net models, when evaluated on the network provided to the GELnets. This indicates that the feature-feature penalties are working as intended. <xref ref-type="fig" rid="pcbi.1004790.g003">Fig 3</xref> demonstrates that the improvement in dispersion is more pronounced in the GGM+ case. As expected, a higher improvement in RMSE is observed when the GELnets are provided with the correct network.</p>
<fig id="pcbi.1004790.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004790.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Distribution of % improvement in GELnets over elastic nets for RMSE and dispersion performance metrics.</title>
<p>Red curve corresponds to the case where GELnets were provided with the true network used to generate the data. Blue curve depicts the case where the permuted network was provided instead.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.g003" xlink:type="simple"/>
</fig>
<p>Surprisingly, GELnet regularization consistently led to better RMSE, regardless of whether the method was given the true or the scrambled network. We speculated that this was due to the effect of the false network sharing some neighbors and paths as the true network. To test this idea, GELnet-based models were retrained with increasingly scrambled information about true feature-feature relationships. To compose a partially-scrambled network, we randomly permuted a fraction of rows (and symmetrically columns) in the graph adjacency matrix, before using the graph’s Laplacian to train a GELnet model. We refer to this fraction as the “Scramble Factor” and present the results from 100 runs of the experiment in <xref ref-type="supplementary-material" rid="pcbi.1004790.s008">S6 Fig</xref>. Note that the left-hand side of the plots, where the GELnets are provided with the true network, corresponds to the GGM+ case. Likewise, the right-hand side, where the entire network is scrambled, is the GGM- case. From <xref ref-type="supplementary-material" rid="pcbi.1004790.s008">S6 Fig</xref>, we observe that GELnets maintain their performance edge over Elastic Nets in the presence of up to 20% noise in the feature-feature relationship network.</p>
<p>We also consider the performance of the two regularization methods on data generated with a random covariance matrix, with results presented in <xref ref-type="supplementary-material" rid="pcbi.1004790.s003">S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1004790.s004">S2</xref> Figs. As in the GGM case, providing the true network to GELnets leads to reconstruction improvement over Elastic Nets. Likewise, GELnets always yield better dispersion values than Elastic Nets, indicating once again that the feature-feature penalties are working as intended. Unlike in the GGM scenarios, both regularization schemes produce comparable RMSE values and, as depicted in <xref ref-type="supplementary-material" rid="pcbi.1004790.s004">S2 Fig</xref>, there is little to no distinction between Rand+ and Rand- cases. This evidence suggests that GELnets gain no benefit from the true gene-gene network when the network captures the signaling pathway that gave rise to the observed phenotypic response but not the expression data.</p>
<p>When viewed together, these synthetic data results allow us to reason about the relevance of prior information to the application of a given dataset. Specifically, by training a model regularized with the GELnet, we are not only able to extract pathway-aligned features, but to also estimate how well those pathways represent the underlying biological mechanism that gave rise to the observed phenotype. We do this by comparing the performance to a model regularized by the classical Elastic Nets.</p>
<p>We compare the ability of GELnets to make use of prior biological information relative to other regularization schemes by repeating our synthetic data experiments with Sparse Group LASSO (SGL) [<xref ref-type="bibr" rid="pcbi.1004790.ref053">53</xref>]. SGL takes as input a grouping of features and induces sparsity at both the group level and the individual feature level. By providing the method with a set of pathways, SGL can be readily used in bioinformatics applications, as has been done by Silver, et al., who identified pathways and genes associated with high-density lipoprotein cholesterol in a genome-wide association study [<xref ref-type="bibr" rid="pcbi.1004790.ref052">52</xref>]. We present the comparison of SGL and GELnets in <xref ref-type="supplementary-material" rid="pcbi.1004790.s009">S7</xref> and <xref ref-type="supplementary-material" rid="pcbi.1004790.s010">S8</xref> Figs. Similar to our application of GELnets, we marginalize the performance of SGL across a range of its parameter values using the R package <monospace>SGL</monospace>. Because SGL requires a collection of pathways rather a single graph, we apply community-based clustering (R package <monospace>igraph</monospace>) to split up the synthetic network before providing it to SGL. In <xref ref-type="supplementary-material" rid="pcbi.1004790.s009">S7 Fig</xref>, it can be observed that SGL produces a better fit to the GGM-generated data, as measured by RMSE. However, GELnets produce more tightly-clustered solutions (lower dispersion) that better capture the simulated signaling pathway <bold>w</bold> (lower reconstruction error), suggesting that SGL overfits the data in this situation. A similar trend with dispersion and reconstruction error can be observed in the Rand+/- scenarios (<xref ref-type="supplementary-material" rid="pcbi.1004790.s010">S8 Fig</xref>), except that both regularization schemes produce comparable fits to the data, as measured by RMSE.</p>
<p>We also evaluated the use of a diffusion kernel (specifically <italic>I</italic> − <italic>D</italic>, as described in the Methods section) as an alternative penalty matrix for the GGM+/- scenarios. <xref ref-type="supplementary-material" rid="pcbi.1004790.s012">S10 Fig</xref> demonstrates that models trained using the diffusion kernel penalty yield vastly lower dispersion than the Laplacian-based models. Additionally, models regularized by the diffusion kernel have lower RMSE (particularly in the GGM- scenario), but the reconstruction error is slightly worse, suggestion minor overfitting to the training data. Because the Laplacian penalty matrix produces models with lower reconstruction error, we chose to use it when building models of drug sensitivity in Gray Cell lines, which we discuss in the following section.</p>
<p>The GELNet can always be applied as a standalone regularization method; it provides as good a fit to the data in terms of RMSE as Elastic Net. However, the underlying network identified may or may not be related to the true underlying mechanism. By comparing the performance to Elastic Nets, we are able to identify the situations in which the network improves modeling accuracy. From these experiments and comparing the results of <xref ref-type="fig" rid="pcbi.1004790.g003">Fig 3</xref> with <xref ref-type="supplementary-material" rid="pcbi.1004790.s004">S2 Fig</xref> we find that using an appreciable improvement in either RMSE (at least 2.5%) or dispersion (at least 5%) give an indication that a network model is relevant. However, to maintain a conservative interpretation, we suggest using both RMSE and dispersion as criteria for identifying when an underlying network is consistent with the data observations.</p>
</sec>
<sec id="sec009">
<title>Drug sensitivity of gray cell lines</title>
<p>We trained linear regression models to predict drug sensitivity in breast cancer cell lines [<xref ref-type="bibr" rid="pcbi.1004790.ref041">41</xref>], comparing the performance of classical Elastic Nets to GELnets. In light of our results in the previous section, we reason that when GELnet regularization outperforms Elastic Nets by a large margin, it is evidence that the mechanism of resistance and the gene expression data are both captured by the same gene regulatory network (GRN). Note that because the reconstruction error is not directly observable, we have to rely on RMSE and dispersion to determine whether this network corresponds with the one provided to GELnets. <xref ref-type="fig" rid="pcbi.1004790.g003">Fig 3</xref> shows that providing to GELnets the network used to generate the data (scenario GGM+) yields higher improvement in RMSE and lower dispersion over Elastic Nets compared to when the “wrong” network is provided (scenario GGM-). It is important to note that even though GELnets always attains lower dispersion than Elastic Nets, our simulations reveal that there is information in the relative difference in RMSE and dispersion between the two methods. As revealed in the simulation experiments above, if the level of difference in performance between GELnets and Elastic Nets exceeds critical levels, it strongly suggests the pathway model is applicable to the learning task. Specifically, we use the difference in performance as an indicator of the network prior relevance. Thus, we aim to identify drugs for which we observe the highest improvement in RMSE and dispersion over Elastic Nets.</p>
<p>The dataset by Heiser, <italic>et al</italic>. (available for download from the supplement of [<xref ref-type="bibr" rid="pcbi.1004790.ref041">41</xref>]) is comprised of RNAseq expression assays of 54 breast cancer cell lines and their sensitivity profiles to 74 compounds. The RNAseq data contains expression values for 18,632 genes. The sensitivity is measured as −log10(<italic>GI</italic>50), where <italic>GI</italic>50 is the amount of compound needed to inhibit cell growth by 50%. For every drug, we trained two linear regression models, one regularized by an Elastic Net and another by a GELnet. We used the same grid of values for the <italic>λ</italic><sub>1</sub> and <italic>λ</italic><sub>2</sub> meta-parameters as in the synthetic data experiments, and provided all GELnet models with an interaction network from Pathway Commons (<ext-link ext-link-type="uri" xlink:href="http://www.pathwaycommons.org/" xlink:type="simple">http://www.pathwaycommons.org/</ext-link>) [<xref ref-type="bibr" rid="pcbi.1004790.ref042">42</xref>], reducing the feature space of the dataset to 9,984 genes that occur in the network.</p>
<p>
<xref ref-type="fig" rid="pcbi.1004790.g004">Fig 4</xref> presents the results for 27 of the 74 drugs where we observed lower RMSE values in GELnet models. The figure presents improvement in RMSE values over Elastic Net models, with the raw RMSE values shown in <xref ref-type="supplementary-material" rid="pcbi.1004790.s007">S5 Fig</xref>. GELnet models for 47 of the 74 drugs failed to provide an improvement in RMSE over Elastic Nets, suggesting that PathwayCommons is unable to accurately capture the underlying mechanism of drug resistance. Additionally, we note that GELnet models for all 74 drugs had lower dispersion values compared to their Elastic Net counterparts, as expected. As in the case of synthetic data experiments, the values presented in <xref ref-type="fig" rid="pcbi.1004790.g004">Fig 4</xref> are averages over the grid of meta-parameter values. In the cases of BIBW2992 and 5-FdUR, GELnets outperformed Elastic Nets for all values of the <italic>λ</italic><sub>1</sub> and <italic>λ</italic><sub>2</sub> meta-parameters.</p>
<fig id="pcbi.1004790.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004790.g004</object-id>
<label>Fig 4</label>
<caption>
<title>(Top) Results for predicting drug sensitivity in Gray cell lines.</title>
<p>Presented are all the drugs where GELnets outperformed Elastic Nets. The bar height displays % RMSE improvement and % dispersion improvement over Elastic Net solutions. Diamond shapes indicate instances where drugs sensitivity was significantly correlated (ANOVA, <italic>p</italic>-val &lt; 0.05) with breast cancer subtype. <bold>(Bottom Left)</bold> Distribution of % RMSE improvement over Elastic Nets across all 74 drugs and the location of BIBW2992 and 5-FdUR in that distribution. <bold>(Bottom Right)</bold> Distribution of % dispersion improvement over Elastic Nets across all 74 drugs and the location of BIBW2992 and 5-FdUR in that distribution.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.g004" xlink:type="simple"/>
</fig>
<p>Many of the drugs used in breast cancer were developed to target specific subtypes that have clear expression signatures (e.g. luminals versus basals versus HER2-amplified). We indicate drugs whose sensitivity profiles correlate significantly with breast cancer subtypes (subtype calls specified by Heiser, <italic>et al</italic>.[<xref ref-type="bibr" rid="pcbi.1004790.ref041">41</xref>]) with diamond shapes in <xref ref-type="fig" rid="pcbi.1004790.g004">Fig 4</xref>. For these drugs, we have to be mindful of the fact that the models of sensitivity are likely to be confounded by the subtype. Note that the two drugs, BIBW2992 and 5-FdUR, where GELnets outperformed Elastic Nets under all parameter settings do not fall into this category (single-factor ANOVA; <italic>p</italic>-values greater than 0.05). We note that while we expected BIBW2992 to be specific to the HER2 subtype, the sensitivity spectrum across the cell lines does indeed seem to sensitize additional lines w/o the amplification.</p>
<p>While we observe the most consistent improvement in RMSE for both BIBW2992 and 5-FdUR, the GELnet models for BIBW2992 also yield the largest reduction in dispersion over Elastic Nets. A large improvement in both performance metrics suggests that BIBW2992 falls into what we called the GGM+ scenario in our synthetic data experiments: the expression data and the mechanism of resistance are both captured by the network that is provided to GELnets. We further tested this intuition by training 30 GELnet models for BIBW2992 sensitivity using randomly scrambled versions of the PathwayCommons network. <xref ref-type="supplementary-material" rid="pcbi.1004790.s011">S9 Fig</xref> presents the distribution of performance values for these models. We observe a substantial decrease in RMSE and dispersion relative to when the unscrambled version of the network is used, providing further support that PathwayCommons captures the underlying mechanism of resistance. We now take a closer look at the solutions obtained by GELnets to investigate potential novel mechanisms of resistance to BIBW2992.</p>
<p>BIBW2992 (also known as Afatinib) is an inhibitor of kinases from the epidermal growth factor receptor family, specifically EGFR and ERBB2 (Her2). It acts by covalently binding to and irreversibly blocking the receptors, thereby shutting down the signaling networks whose deregulation is commonly known to be implicated in epithelial cancer growth and proliferation [<xref ref-type="bibr" rid="pcbi.1004790.ref043">43</xref>]. Consequently, one expects that higher expression of these receptor genes will lead to higher sensitivity to the inhibitor. This is indeed one of the trends we observe.</p>
<p>
<xref ref-type="fig" rid="pcbi.1004790.g005">Fig 5</xref> presents the GELnet models trained to predict BIBW2992 sensitivity across a grid of <italic>λ</italic><sub>1</sub> and <italic>λ</italic><sub>2</sub> meta-parameter values. The models are sorted by their improvement in RMSE over the Elastic Net equivalents. For each model, we show the feature weights for 30 genes that had the highest median rank across all meta-parameter values, where the ranking was according to the absolute values of the weights.</p>
<fig id="pcbi.1004790.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004790.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Models for sensitivity of Gray cell lines to BIBW2992 learned by GELnets.</title>
<p>We present the model weights for the top 30 genes (see text) as they change across the different values of the <italic>λ</italic><sub>1</sub> and <italic>λ</italic><sub>2</sub> meta-parameters. Higher positive (red) weights are associated with resistance, while higher negative (blue) weights are associated with sensitivity. The models are sorted by their % RMSE improvement over the corresponding Elastic Net models. The barplot on the left displays the average weight of each gene across all meta-parameter values where the weight is not zero.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.g005" xlink:type="simple"/>
</fig>
<p>As expected, higher expression of the HER2 gene is correlated with higher sensitivity to BIBW2992. This is demonstrated by the relatively high negative model weights of ERBB2 and GRB7, a positional neighbor of ERBB2 on the chromosome and often co-amplified and co-expressed. Furthermore, a positive model weight for FGFR2 and GABRA3 suggests that cells resistant to BIBW2992 may be responding to alternative stimuli (the fibroblast growth factor and GABA<sub><italic>A</italic></sub> signaling) from these overexpressed receptors. In support of this observation, the overexpression of FGFR2 has been previously observed in cells resistant to Lapatinib, another Her2 inhibitor [<xref ref-type="bibr" rid="pcbi.1004790.ref044">44</xref>]. Azuma, <italic>et al</italic>. speculated that FGFR2-targeted therapy may provide a promising salvage strategy after Lapatinib failure [<xref ref-type="bibr" rid="pcbi.1004790.ref044">44</xref>], and our findings here suggest that the same may hold true for BIBW2992 as well.</p>
<p>Note that the above trend of model weights for cell surface receptors is observed on the right-hand side of <xref ref-type="fig" rid="pcbi.1004790.g005">Fig 5</xref> only, where Elastic Nets and GELnets perform comparably. The part of the figure is also associated with the lower values of <italic>λ</italic><sub>2</sub>, implying that there is little distinction between the GELnet and Elastic Net models. Indeed, the correlation between BIBW2992 sensitivity and the expression of the cell surface receptors above is also found by the Elastic Net regularization.</p>
<p>As the value of the <italic>λ</italic><sub>2</sub> meta-parameter increases, Elastic Net and GELnet models begin to diverge and a new trend emerges on the left-hand side of <xref ref-type="fig" rid="pcbi.1004790.g005">Fig 5</xref>, which is associated with a higher improvement in prediction accuracy of GELnet-regularized models over those of Elastic net. Importantly, the GELnet models emphasize an entirely different set of genes for predicting BIBW2992 sensitivity. These models identify the expression of KLF7 and its transcriptional co-activator FBXO38 as predictors of resistance. KLF7 is a transcription factor that was recently shown to play a regulatory role in differentiation of several cell lineages, including neuronal and osteocytic [<xref ref-type="bibr" rid="pcbi.1004790.ref045">45</xref>]. Its role in breast cancer is largely unknown, but the gene’s regulation of Map2, NGF and TrkA suggests an involvement in cell proliferation and renewal.</p>
<p>Note that while GELnets demonstrate the largest improvement in performance over Elastic Nets for high values of <italic>λ</italic><sub>2</sub>, the model with the lowest RMSE was obtained when the parameters were set to <inline-formula id="pcbi.1004790.e029"><alternatives><graphic id="pcbi.1004790.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004790.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mrow><mml:msub><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msubsup> <mml:mo>/</mml:mo> <mml:mn>9</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> and <italic>λ</italic><sub>2</sub> = 1. The latter appears on the right-hand side of <xref ref-type="fig" rid="pcbi.1004790.g005">Fig 5</xref>, where the model of resistance is dominated by the cell surface receptors. These results demonstrate that the most accurate model does not necessarily recapitulate the entire biological story, and further exploration of the parameter space can produce additional insight. We present the two mechanisms of resistance in <xref ref-type="supplementary-material" rid="pcbi.1004790.s005">S3</xref> and <xref ref-type="supplementary-material" rid="pcbi.1004790.s006">S4</xref> Figs, as well as their interaction on a gene regulatory network in <xref ref-type="fig" rid="pcbi.1004790.g006">Fig 6</xref>.</p>
<fig id="pcbi.1004790.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004790.g006</object-id>
<label>Fig 6</label>
<caption>
<title>The two mechanisms of resistance to BIBW2992 placed in the context of a gene regulatory network.</title>
<p>The network was constructed via GeneMANIA using the top 15 and bottom 15 genes of the model associated with the largest increase in GELnet performance over Elastic Nets. Genes captured by the model which is focused on the cell surface receptors are highlighted with a thin yellow border. Similarly, genes captured by the KLF7-centric model are highlighted with a thick yellow border. The size and the color intensity of a node designate the corresponding gene weight in the model. Red and blue nodes correspond to positive and negative weights, respectively. Gray nodes are “linked genes” included by GeneMANIA.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.g006" xlink:type="simple"/>
</fig>
<p>Taken together, our findings in this section suggest that cells resistant to BIBW2992 might have undergone partial transdifferentiation, as indicated by the active KLF7 transcription factor and overexpressed fibroblast growth factor and GABA<sub><italic>A</italic></sub> receptors. This hypothesis is further supported by a very strong signal of SCGB2A2 and SCGB1D2 being downregulated in resistant cells, as indicated by their large negative weights in the GELnet models. The two genes are considered to be highly specific markers for the breast tissue, where their proteins form a covalent complex [<xref ref-type="bibr" rid="pcbi.1004790.ref046">46</xref>]. Further experimental validation is required to confirm the transdifferentiation hypothesis. Because KLF7 appears to play a central role in these transdifferentiated cells, the observation may suggest shRNA-mediated silencing of this transcription factor to get around resistance to BIBW2992.</p>
</sec>
<sec id="sec010">
<title>Principal components analysis of PanCan12</title>
<p>All of the prediction problems we considered so far are supervised methods. To illustrate the generality of the GELnet regularization framework, we sought to apply it to an unsupervised task as well. We constructed a regularized Principal Component decomposition of the TCGA “PanCan12” dataset representing RNA-Seq data from twelve different types of cancer. The technical details of this problem can be found in <xref ref-type="supplementary-material" rid="pcbi.1004790.s002">S1 Text</xref> where we discuss non-convex ratios of quadratic norms.</p>
<p>We downloaded the data from the Synapse TCGA_Pancancer repository (<ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn300013" xlink:type="simple">https://www.synapse.org/#!Synapse:syn300013</ext-link>). For each principal component, we constructed two GELnet models. The first model used the Laplacian of PathwayCommons as its penalty matrix, as in the previous experiments. For the second model, we set the penalty matrix as <italic>P</italic> = <italic>I</italic> − <italic>D</italic>, where <italic>D</italic> is the diffusion kernel of PathwayCommons and <italic>I</italic> is the identity matrix. Our intuition is that, by capturing indirect gene connectivity, the diffusion kernel will produce models that more tightly cluster on the corresponding interaction network. The empirical results presented in <xref ref-type="fig" rid="pcbi.1004790.g007">Fig 7</xref> confirm this intuition. We projected the PanCan12 dataset onto the first two unregularized principal components and estimated the quality of GELnet models constructed for those two components. Specifically, we measured performance of GELnet models according to how well they approximate the original, unregularized principal components (measured via RMSE) and by how tightly-clustered the solutions are on the PathwayCommons network (measured via the dispersion metric). We considered the same grid of values for the <italic>λ</italic><sub>1</sub> and <italic>λ</italic><sub>2</sub> meta-parameters.</p>
<fig id="pcbi.1004790.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004790.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Comparison of GELnet models constructed using the Laplacian and the Diffusion kernel in an unsupervised setting.</title>
<p>The left panel shows the first two principal components of the PanCan12 dataset with samples colored by tissue-of-origin. The right two panels present the performance of GELnet models constructed using the Laplacian (red) and the Diffusion kernel (blue) of Pathway Commons. The x-axis captures how well each model reconstructs the unregularized principal component, measured via RMSE. The y-axis captures the dispersion. Individual points correspond to the 15 settings of the <italic>λ</italic><sub>1</sub> and <italic>λ</italic><sub>2</sub> sampled over a grid of values.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.g007" xlink:type="simple"/>
</fig>
<p>Note that for both principal components, the use of the diffusion kernel produced models with lower dispersion while maintaining the same level of reconstruction accuracy compared to the Laplacian. We looked at the genes picked up by the diffusion-driven models and found enrichment for many pathways associated with organism development and tissue differentiation, confirming the findings of the TCGA consortium that found that cell-of-origin signatures drive the dominant information in the data [<xref ref-type="bibr" rid="pcbi.1004790.ref054">54</xref>]. The top 10 genes selected by each model are shown in <xref ref-type="supplementary-material" rid="pcbi.1004790.s001">S1 Table</xref>. The Gene Ontology (GO) enrichment analysis revealed a nearly identical set of GO terms enriched in both models, with the following terms appearing at the top: ECTODERM_DEVELOPMENT (GO:0007398), TISSUE_DEVELOPMENT (GO:0009888) and ORGAN_DEVELOPMENT (GO:0048513). The close similarity of the enriched terms between the two models is expected, because the enrichment analysis acts as a “smoothing function” on the Laplacian-based solution effectively elucidating the same set of pathways as those found by the diffusion-based solution.</p>
</sec>
</sec>
<sec id="sec011" sec-type="conclusions">
<title>Discussion</title>
<p>In molecular biology, genetic interactions provide a rich source of information encoding what is known about cellular circuitry. The proposed GELnet regularization method capitalizes on this information to improve the accuracy and interpretability of linear regression-based solutions to genome-based prediction tasks. The novel regularization scheme allows the use of domain knowledge to guide the selection of related features to steer toward intuitive solutions.</p>
<p>Because our knowledge about genetic pathways is incomplete, we expect this new framework to be applicable only in situations where current knowledge aligns adequately with the underlying biological mechanism. Obviously, this information is usually not available; the puzzle is to determine if and when such genetic pathway representations are indeed relevant for a particular study.</p>
<p>We have shown here, through a series of simulation experiments, how to identify such situations. We demonstrated that GELnets outperform their non-pathway-based counterpart, Elastic Nets, when both the dataset and the phenotype are simulated from the same genetic network, and where GELnet regularization is provided with that network. Importantly, we found critical levels in the relative difference between the methods in accuracy in prediction and the mutual closeness of features on the networks to indicate when the network used for simulation matches the network used for modeling.</p>
<p>We describe how one can use this observation to detect when drug resistance mechanisms might be inferred from regression models. In a panel of breast cancer cell lines, we show that both expected and novel mechanisms are revealed for over one third of the drugs tested in the cell line panel. One such case is the model for response to the dual EGFR/ERBB2 inhibitor, BIBW2992. Consistent with the known drug action, we find over-expression of ERBB2 and GRB7 are sensitivity markers. In addition, concrete receptors regulating parallel growth response pathways, such as FGFR2, are revealed as resistance mechanisms that may provide synergistic targets.</p>
<p>Our approach is general enough to extend to other machine learning problems, where a comparable regularization scheme can be introduced to reward and/or penalize the selection of features based on their mutual proximity within a genetic pathway diagram. Applications include linear and non-linear approaches for supervised, unsupervised, and semi-supervised strategies (see <xref ref-type="supplementary-material" rid="pcbi.1004790.s002">S1 Text</xref>).</p>
<p>Although not considered here, the use of feature-specific weights <italic>d</italic><sub><italic>j</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1004790.e006">Eq (2)</xref> can be used to further guide feature selection by placing more or less penalty on individual features. Other graph-based penalty matrices in place of the Laplacian can also be used.</p>
</sec>
<sec id="sec012">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004790.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.s001" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>The top 10 genes selected by two different gene-gene penalty matrices for principal component decomposition of the PanCan12 dataset.</title>
<p>Genes are ordered according to the absolute value of their weight in the corresponding model.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004790.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.s002" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Generalization of the GELnet framework to loss functions beyond the squared error loss.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004790.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.s003" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Performance of Elastic Nets and GELnets on synthetic data generated with a random covariance matrix.</title>
<p>Plotted are 30 trials of the same experiment. The x- and y-axes in every plot correspond to Elastic Nets and GELnets, respectively. The top three plots show the scenario where the GELnets were provided the true feature-feature relationships, while the bottom three plots correspond to the scrambled network case. Lower values are better for all three performance metrics, and the points are colored in red whenever the performance metrics are lower in the GELnet models, and blue otherwise.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004790.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.s004" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Distribution of % improvement in GELnets over Elastic Nets for RMSE and dispersion performance metrics.</title>
<p>Red curve corresponds to the case where GELnets were provided with the true network used to generate the data. Blue curve depicts the case where the permuted network was provided instead.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004790.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.s005" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Signature associated with the lowest RMSE.</title>
<p>The heatmap presents median-centered mRNA expression for 15 genes with the largest absolute weights in the corresponding model. The weights are displayed in the left barplot, while the model score for each sample is presented at the top. The samples are sorted by the signature score, and the true labels are shown in the colored bar labeled “Resistance”.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004790.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.s006" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Signature associated with the largest % improvement of GELnets over Elastic Nets.</title>
<p>The heatmap presents median-centered mRNA expression for 15 genes with the largest absolute weights in the corresponding model. The weights are displayed in the left barplot, while the model score for each sample is presented at the top. The samples are sorted by the signature score, and the true labels are shown in the colored bar labeled “Resistance”.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004790.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.s007" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>RMSE values obtained by Elastic Net models for the set of drugs where GELnets outperformed Elastic Nets.</title>
<p>As in <xref ref-type="fig" rid="pcbi.1004790.g004">Fig 4</xref>, diamond shapes denote drugs with sensitivity significantly correlated to breast cancer subtype.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004790.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.s008" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Change in performance associated with providing a scrambled network to GELnets.</title>
<p>Presented are results from 100 runs, where reconstruction error (left column), RMSE (center column) and dispersion (right column) are plotted against the fraction of edges reordered in the true network before the network was provided to GELnets. The red and blue points correspond to Elastic Net and GELnet models, respectively. The bottom row presents % improvement over Elastic Nets.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004790.s009" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.s009" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Performance of GELnets and Sparse Group LASSO on synthetic data generated with a GGM.</title>
<p>Plotted are 30 trials of the same experiment. The x- and y-axes in every plot correspond to Sparse Group LASSO and GELnets, respectively. The top three plots show the scenario where both regularization methods were provided the true feature-feature relationships, while the bottom three plots correspond to the scrambled network case. Lower values are better for all three performance metrics, and the points are colored in red whenever the performance metrics are lower in the GELnet models, and blue otherwise.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004790.s010" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.s010" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>Performance of GELnets and Sparse Group LASSO on synthetic data generated with a random covariance matrix.</title>
<p>The interpretation of the Figure is similar to that of <xref ref-type="supplementary-material" rid="pcbi.1004790.s009">S7 Fig</xref>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004790.s011" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.s011" xlink:type="simple">
<label>S9 Fig</label>
<caption>
<title>Distribution of % Improvement over Elastic Nets in GELnets constructed with a scrambled network to predict sensitivity to BIBW2992.</title>
<p>The black curves present the distribution of values over 30 random scrambles of the PathwayCommons network. The performance of GELnets with the original, unscrambled network are shown in red.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004790.s012" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004790.s012" xlink:type="simple">
<label>S10 Fig</label>
<caption>
<title>Performance of GELnets with two different gene-gene penalty matrices on synthetic data generated with a GGM.</title>
<p>Plotted are 30 trials of the same experiment. The x- and y-axes in every plot correspond to the Laplacian and diffusion penalty matrices, respectively. The top three plots show the scenario where the GELnet was provided with the true feature-feature relationships, while the bottom three plots correspond to the scrambled network case. Lower values are better for all three performance metrics, and the points are colored in red whenever the performance metrics are lower in the diffusion penalty models, and blue otherwise.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors would like to thank David Haussler for his insightful comments and valuable suggestions to improve this work.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004790.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Boutros</surname> <given-names>PC</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>SK</given-names></name>, <name name-style="western"><surname>Pintilie</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Shepherd</surname> <given-names>FA</given-names></name>, <name name-style="western"><surname>Der</surname> <given-names>SD</given-names></name>, <etal>et al</etal>. <article-title>Prognostic gene signatures for non-small-cell lung cancer</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2009</year>;<volume>106</volume>(<issue>8</issue>):<fpage>2824</fpage>–<lpage>2828</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0809444106" xlink:type="simple">10.1073/pnas.0809444106</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Venet</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Dumont</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Detours</surname> <given-names>V</given-names></name>. <article-title>Most random gene expression signatures are significantly associated with breast cancer outcome</article-title>. <source>PLoS computational biology</source>. <year>2011</year>;<volume>7</volume>(<issue>10</issue>):<fpage>e1002240</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002240" xlink:type="simple">10.1371/journal.pcbi.1002240</ext-link></comment> <object-id pub-id-type="pmid">22028643</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref003">
<label>3</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Vapnik</surname> <given-names>V</given-names></name>. <source>The nature of statistical learning theory</source>. <publisher-name>springer</publisher-name>; <year>1999</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref004">
<label>4</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Friedman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Friedman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>. <source>The elements of statistical learning</source>. vol. <volume>2</volume>. <publisher-name>Springer</publisher-name>; <year>2009</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref005">
<label>5</label>
<mixed-citation publication-type="other" xlink:type="simple">Airola A, Pahikkala T, Waegeman W, De Baets B, Salakoski T. A comparison of AUC estimators in small-sample studies. In: Proceedings of the 3rd International workshop on Machine Learning in Systems Biology; 2009. p. 15–23.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hanczar</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Hua</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sima</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Weinstein</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bittner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Dougherty</surname> <given-names>ER</given-names></name>. <article-title>Small-sample precision of ROC-related estimates</article-title>. <source>Bioinformatics</source>. <year>2010</year>;<volume>26</volume>(<issue>6</issue>):<fpage>822</fpage>–<lpage>830</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btq037" xlink:type="simple">10.1093/bioinformatics/btq037</ext-link></comment> <object-id pub-id-type="pmid">20130029</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref007">
<label>7</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Guyon</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Gunn</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Nikravesh</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Zadeh</surname> <given-names>LA</given-names></name>. <source>Feature extraction: foundations and applications</source>. vol. <volume>207</volume>. <publisher-name>Springer</publisher-name>; <year>2006</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zou</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>. <article-title>Regularization and variable selection via the elastic net</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>2005</year>;<volume>67</volume>(<issue>2</issue>):<fpage>301</fpage>–<lpage>320</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-9868.2005.00503.x" xlink:type="simple">10.1111/j.1467-9868.2005.00503.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Conesa</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bro</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>García-García</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Prats</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Götz</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kjeldahl</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Direct functional assessment of the composite phenotype through multivariate projection strategies</article-title>. <source>Genomics</source>. <year>2008</year>;<volume>92</volume>(<issue>6</issue>):<fpage>373</fpage>–<lpage>383</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.ygeno.2008.05.015" xlink:type="simple">10.1016/j.ygeno.2008.05.015</ext-link></comment> <object-id pub-id-type="pmid">18652888</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kong</surname> <given-names>SW</given-names></name>, <name name-style="western"><surname>Pu</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Park</surname> <given-names>PJ</given-names></name>. <article-title>A multivariate approach for integrating genome-wide expression data and biological knowledge</article-title>. <source>Bioinformatics</source>. <year>2006</year>;<volume>22</volume>(<issue>19</issue>):<fpage>2373</fpage>–<lpage>2380</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btl401" xlink:type="simple">10.1093/bioinformatics/btl401</ext-link></comment> <object-id pub-id-type="pmid">16877751</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nettleton</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Recknor</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Reecy</surname> <given-names>JM</given-names></name>. <article-title>Identification of differentially expressed gene categories in microarray studies using nonparametric multivariate analysis</article-title>. <source>Bioinformatics</source>. <year>2008</year>;<volume>24</volume>(<issue>2</issue>):<fpage>192</fpage>–<lpage>201</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btm583" xlink:type="simple">10.1093/bioinformatics/btm583</ext-link></comment> <object-id pub-id-type="pmid">18042553</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wei</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>H</given-names></name>. <article-title>Nonparametric pathway-based regression models for analysis of genomic data</article-title>. <source>Biostatistics</source>. <year>2007</year>;<volume>8</volume>(<issue>2</issue>):<fpage>265</fpage>–<lpage>284</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/biostatistics/kxl007" xlink:type="simple">10.1093/biostatistics/kxl007</ext-link></comment> <object-id pub-id-type="pmid">16772399</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Teschendorff</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Severini</surname> <given-names>S</given-names></name>. <article-title>Increased entropy of signal transduction in the cancer metastasis phenotype</article-title>. <source>BMC systems biology</source>. <year>2010</year>;<volume>4</volume>(<issue>1</issue>):<fpage>104</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1752-0509-4-104" xlink:type="simple">10.1186/1752-0509-4-104</ext-link></comment> <object-id pub-id-type="pmid">20673354</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Feltus</surname> <given-names>FA</given-names></name>, <name name-style="western"><surname>Sun</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>JZ</given-names></name>, <name name-style="western"><surname>Luo</surname> <given-names>F</given-names></name>. <article-title>Identifying differentially expressed genes in cancer patients using a non-parameter Ising model</article-title>. <source>Proteomics</source>. <year>2011</year>;<volume>11</volume>(<issue>19</issue>):<fpage>3845</fpage>–<lpage>3852</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/pmic.201100180" xlink:type="simple">10.1002/pmic.201100180</ext-link></comment> <object-id pub-id-type="pmid">21761563</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Qiu</surname> <given-names>YQ</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>XS</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>L</given-names></name>. <article-title>Detecting disease associated modules and prioritizing active genes based on high throughput data</article-title>. <source>BMC bioinformatics</source>. <year>2010</year>;<volume>11</volume>(<issue>1</issue>):<fpage>26</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1471-2105-11-26" xlink:type="simple">10.1186/1471-2105-11-26</ext-link></comment> <object-id pub-id-type="pmid">20070902</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Subramanian</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Tamayo</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Mootha</surname> <given-names>VK</given-names></name>, <name name-style="western"><surname>Mukherjee</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ebert</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Gillette</surname> <given-names>MA</given-names></name>, <etal>et al</etal>. <article-title>Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2005</year>;<volume>102</volume>(<issue>43</issue>):<fpage>15545</fpage>–<lpage>15550</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0506580102" xlink:type="simple">10.1073/pnas.0506580102</ext-link></comment> <object-id pub-id-type="pmid">16199517</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barry</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Nobel</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>FA</given-names></name>. <article-title>Significance analysis of functional categories in gene expression studies: a structured permutation approach</article-title>. <source>Bioinformatics</source>. <year>2005</year>;<volume>21</volume>(<issue>9</issue>):<fpage>1943</fpage>–<lpage>1949</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/bti260" xlink:type="simple">10.1093/bioinformatics/bti260</ext-link></comment> <object-id pub-id-type="pmid">15647293</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ideker</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ozier</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Schwikowski</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Siegel</surname> <given-names>AF</given-names></name>. <article-title>Discovering regulatory and signalling circuits in molecular interaction networks</article-title>. <source>Bioinformatics</source>. <year>2002</year>;<volume>18</volume>(<issue>suppl 1</issue>):<fpage>S233</fpage>–<lpage>S240</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/18.suppl_1.S233" xlink:type="simple">10.1093/bioinformatics/18.suppl_1.S233</ext-link></comment> <object-id pub-id-type="pmid">12169552</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dao</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Collins</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Ester</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lapuk</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sahinalp</surname> <given-names>SC</given-names></name>. <article-title>Optimally discriminative subnetwork markers predict response to chemotherapy</article-title>. <source>Bioinformatics</source>. <year>2011</year>;<volume>27</volume>(<issue>13</issue>):<fpage>i205</fpage>–<lpage>i213</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btr245" xlink:type="simple">10.1093/bioinformatics/btr245</ext-link></comment> <object-id pub-id-type="pmid">21685072</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vandin</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Upfal</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Raphael</surname> <given-names>BJ</given-names></name>. <article-title>Algorithms for detecting significantly mutated pathways in cancer</article-title>. <source>Journal of Computational Biology</source>. <year>2011</year>;<volume>18</volume>(<issue>3</issue>):<fpage>507</fpage>–<lpage>522</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1089/cmb.2010.0265" xlink:type="simple">10.1089/cmb.2010.0265</ext-link></comment> <object-id pub-id-type="pmid">21385051</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Paull</surname> <given-names>EO</given-names></name>, <name name-style="western"><surname>Carlin</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Niepel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sorger</surname> <given-names>PK</given-names></name>, <name name-style="western"><surname>Haussler</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Stuart</surname> <given-names>JM</given-names></name>. <article-title>Discovering causal pathways linking genomic events to transcriptional states using Tied Diffusion Through Interacting Events (TieDIE)</article-title>. <source>Bioinformatics</source>. <year>2013</year>;<volume>29</volume>(<issue>21</issue>):<fpage>2757</fpage>–<lpage>2764</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btt471" xlink:type="simple">10.1093/bioinformatics/btt471</ext-link></comment> <object-id pub-id-type="pmid">23986566</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dutkowski</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ideker</surname> <given-names>T</given-names></name>. <article-title>Protein networks as logic functions in development and cancer</article-title>. <source>PLoS computational biology</source>. <year>2011</year>;<volume>7</volume>(<issue>9</issue>):<fpage>e1002180</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002180" xlink:type="simple">10.1371/journal.pcbi.1002180</ext-link></comment> <object-id pub-id-type="pmid">21980275</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hofree</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shen</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Carter</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ideker</surname> <given-names>T</given-names></name>. <article-title>Network-based stratification of tumor mutations</article-title>. <source>Nature methods</source>. <year>2013</year>;<volume>10</volume>(<issue>11</issue>):<fpage>1108</fpage>–<lpage>1115</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.2651" xlink:type="simple">10.1038/nmeth.2651</ext-link></comment> <object-id pub-id-type="pmid">24037242</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Johannes</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Brase</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Fröhlich</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Gade</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gehrmann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fälth</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Integration of pathway knowledge into a reweighted recursive feature elimination approach for risk stratification of cancer patients</article-title>. <source>Bioinformatics</source>. <year>2010</year>;<volume>26</volume>(<issue>17</issue>):<fpage>2136</fpage>–<lpage>2144</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btq345" xlink:type="simple">10.1093/bioinformatics/btq345</ext-link></comment> <object-id pub-id-type="pmid">20591905</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref025">
<label>25</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Jang</surname> <given-names>IS</given-names></name>, <name name-style="western"><surname>Dienstmann</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Margolin</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Guinney</surname> <given-names>J</given-names></name>. <chapter-title>STEPWISE GROUP SPARSE REGRESSION (SGSR): GENE-SET-BASED PHARMACOGENOMIC PREDICTIVE MODELS WITH STEPWISE SELECTION OF FUNCTIONAL PRIORS1</chapter-title>. In: <source>Pacific Symposium on Biocomputing</source> vol. <volume>20</volume>. <publisher-name>World Scientific</publisher-name>; <year>2014</year>. p. <fpage>32</fpage>–<lpage>43</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lavi</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Dror</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Shamir</surname> <given-names>R</given-names></name>. <article-title>Network-Induced Classification Kernels for Gene Expression Profile Analysis</article-title>. <source>Journal of Computational Biology</source>. <year>2012</year>;<volume>19</volume>(<issue>6</issue>):<fpage>694</fpage>–<lpage>709</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1089/cmb.2012.0065" xlink:type="simple">10.1089/cmb.2012.0065</ext-link></comment> <object-id pub-id-type="pmid">22697242</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hoerl</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kennard</surname> <given-names>R</given-names></name>. <article-title>Ridge regression</article-title>. <source>Encyclopedia of Statistical Sciences</source>. <year>1988</year>;<volume>8</volume>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>. <article-title>Regression shrinkage and selection via the lasso</article-title>. <source>Journal of the Royal Statistical Society Series B (Methodological)</source>. <year>1996</year>; p. <fpage>267</fpage>–<lpage>288</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref029">
<label>29</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Schölkopf</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Tsuda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Vert</surname> <given-names>JP</given-names></name>. <source>Kernel methods in computational biology</source>. <publisher-name>MIT press</publisher-name>; <year>2004</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Belkin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Niyogi</surname> <given-names>P</given-names></name>. <article-title>Laplacian eigenmaps and spectral techniques for embedding and clustering</article-title>. <source>Advances in neural information processing systems</source>. <year>2001</year>;<volume>14</volume>:<fpage>585</fpage>–<lpage>591</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref031">
<label>31</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Cvetkovic</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Doob</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sachs</surname> <given-names>H</given-names></name>. <source>Spectra of Graphs—Theory and Applications, volume New York</source>. <publisher-name>Academic press</publisher-name>; <year>1980</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref032">
<label>32</label>
<mixed-citation publication-type="other" xlink:type="simple">Herbster M, Pontil M, Wainer L. Online learning over graphs. In: Proceedings of the 22nd international conference on Machine learning. ACM; 2005. p. 305–312.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref033">
<label>33</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Smola</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Kondor</surname> <given-names>R</given-names></name>. <chapter-title>Kernels and regularization on graphs</chapter-title>. In: <source>Learning theory and kernel machines</source>. <publisher-name>Springer</publisher-name>; <year>2003</year>. p. <fpage>144</fpage>–<lpage>158</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yuan</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>Y</given-names></name>. <article-title>Model selection and estimation in regression with grouped variables</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>2006</year>;<volume>68</volume>(<issue>1</issue>):<fpage>49</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-9868.2005.00532.x" xlink:type="simple">10.1111/j.1467-9868.2005.00532.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friedman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Höfling</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>. <article-title>Pathwise coordinate optimization</article-title>. <source>The Annals of Applied Statistics</source>. <year>2007</year>;<volume>1</volume>(<issue>2</issue>):<fpage>302</fpage>–<lpage>332</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/07-AOAS131" xlink:type="simple">10.1214/07-AOAS131</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friedman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>. <article-title>Regularization paths for generalized linear models via coordinate descent</article-title>. <source>Journal of statistical software</source>. <year>2010</year>;<volume>33</volume>(<issue>1</issue>):<fpage>1</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.18637/jss.v033.i01" xlink:type="simple">10.18637/jss.v033.i01</ext-link></comment> <object-id pub-id-type="pmid">20808728</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fan</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>KW</given-names></name>, <name name-style="western"><surname>Hsieh</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XR</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>CJ</given-names></name>. <article-title>LIBLINEAR: A library for large linear classification</article-title>. <source>The Journal of Machine Learning Research</source>. <year>2008</year>;<volume>9</volume>:<fpage>1871</fpage>–<lpage>1874</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref038">
<label>38</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Lauritzen</surname> <given-names>SL</given-names></name>. <source>Graphical models</source>. <publisher-name>Oxford University Press</publisher-name>; <year>1996</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schiavo</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Hand</surname> <given-names>DJ</given-names></name>. <article-title>Ten more years of error rate research</article-title>. <source>International Statistical Review</source>. <year>2000</year>;<volume>68</volume>(<issue>3</issue>):<fpage>295</fpage>–<lpage>310</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1751-5823.2000.tb00332.x" xlink:type="simple">10.1111/j.1751-5823.2000.tb00332.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wolpert</surname> <given-names>DH</given-names></name>. <article-title>The lack of a priori distinctions between learning algorithms</article-title>. <source>Neural computation</source>. <year>1996</year>;<volume>8</volume>(<issue>7</issue>):<fpage>1341</fpage>–<lpage>1390</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Heiser</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Sadanandam</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kuo</surname> <given-names>WL</given-names></name>, <name name-style="western"><surname>Benz</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Goldstein</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Ng</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Subtype and pathway specific responses to anticancer compounds in breast cancer</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2012</year>;<volume>109</volume>(<issue>8</issue>):<fpage>2724</fpage>–<lpage>2729</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1018854108" xlink:type="simple">10.1073/pnas.1018854108</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cerami</surname> <given-names>EG</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Demir</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Rodchenkov</surname> <given-names>I</given-names></name>, Babur Ö, <name name-style="western"><surname>Anwar</surname> <given-names>N</given-names></name>, <etal>et al</etal>. <article-title>Pathway Commons, a web resource for biological pathway data</article-title>. <source>Nucleic acids research</source>. <year>2011</year>;<volume>39</volume>(<issue>suppl 1</issue>):<fpage>D685</fpage>–<lpage>D690</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/nar/gkq1039" xlink:type="simple">10.1093/nar/gkq1039</ext-link></comment> <object-id pub-id-type="pmid">21071392</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Solca</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Dahl</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Zoephel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bader</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Sanderson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Klein</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Target binding properties and cellular activity of afatinib (BIBW 2992), an irreversible ErbB family blocker</article-title>. <source>Journal of Pharmacology and Experimental Therapeutics</source>. <year>2012</year>;<volume>343</volume>(<issue>2</issue>):<fpage>342</fpage>–<lpage>350</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1124/jpet.112.197756" xlink:type="simple">10.1124/jpet.112.197756</ext-link></comment> <object-id pub-id-type="pmid">22888144</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Azuma</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Tsurutani</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sakai</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kaneda</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Fujisaka</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Takeda</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Switching addictions between HER2 and FGFR2 in HER2-positive breast tumor cells: FGFR2 as a potential target for salvage after lapatinib failure</article-title>. <source>Biochemical and biophysical research communications</source>. <year>2011</year>;<volume>407</volume>(<issue>1</issue>):<fpage>219</fpage>–<lpage>224</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.bbrc.2011.03.002" xlink:type="simple">10.1016/j.bbrc.2011.03.002</ext-link></comment> <object-id pub-id-type="pmid">21377448</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Caiazzo</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Colucci-D’Amato</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Esposito</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Parisi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Stifani</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ramirez</surname> <given-names>F</given-names></name>, <etal>et al</etal>. <article-title>Transcription factor KLF7 regulates differentiation of neuroectodermal and mesodermal cell lineages</article-title>. <source>Experimental cell research</source>. <year>2010</year>;<volume>316</volume>(<issue>14</issue>):<fpage>2365</fpage>–<lpage>2376</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.yexcr.2010.05.021" xlink:type="simple">10.1016/j.yexcr.2010.05.021</ext-link></comment> <object-id pub-id-type="pmid">20580711</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zafrakas</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Petschke</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Donner</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fritzsche</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Kristiansen</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Knüchel</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Expression analysis of mammaglobin A (SCGB2A2) and lipophilin B (SCGB1D2) in more than 300 human tumors and matching normal tissues reveals their co-expression in gynecologic malignancies</article-title>. <source>BMC cancer</source>. <year>2006</year>;<volume>6</volume>(<issue>1</issue>):<fpage>88</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1471-2407-6-88" xlink:type="simple">10.1186/1471-2407-6-88</ext-link></comment> <object-id pub-id-type="pmid">16603086</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Simon</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Friedman</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>. <article-title>Regularization Paths for Cox’s Proportional Hazards Model via Coordinate Descent</article-title>. <source>Journal of Statistical Software</source>. <year>2011</year>;<volume>39</volume>(<issue>5</issue>):<fpage>1</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.18637/jss.v039.i05" xlink:type="simple">10.18637/jss.v039.i05</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zou</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>. <article-title>Sparse principal component analysis</article-title>. <source>Journal of computational and graphical statistics</source>. <year>2006</year>;<volume>15</volume>(<issue>2</issue>):<fpage>265</fpage>–<lpage>286</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1198/106186006X113430" xlink:type="simple">10.1198/106186006X113430</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Swets</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Weng</surname> <given-names>JJ</given-names></name>. <article-title>Using discriminant eigenfeatures for image retrieval</article-title>. <source>Pattern Analysis and Machine Intelligence, IEEE Transactions on</source>. <year>1996</year>;<volume>18</volume>(<issue>8</issue>):<fpage>831</fpage>–<lpage>836</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/34.531802" xlink:type="simple">10.1109/34.531802</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref050">
<label>50</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Jolliffe</surname> <given-names>IT</given-names></name>. <source>Principal component analysis</source>. <publisher-name>Springer verlag</publisher-name>; <year>2002</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Witten</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>. <article-title>Penalized classification using Fisher’s linear discriminant</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>2011</year>;<volume>73</volume>(<issue>5</issue>):<fpage>753</fpage>–<lpage>772</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-9868.2011.00783.x" xlink:type="simple">10.1111/j.1467-9868.2011.00783.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Silver</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Cheng</surname> <given-names>C-Y</given-names></name>, <name name-style="western"><surname>Wong</surname> <given-names>T-Y</given-names></name>, <name name-style="western"><surname>Tai</surname> <given-names>E-S</given-names></name>, <etal>et al</etal>. <article-title>Pathways-driven sparse regression identifies pathways and genes associated with high-density lipoprotein cholesterol in two Asian cohorts</article-title>. <source>PLoS Genet</source>. <year>2013</year>;<volume>9</volume>(<issue>11</issue>): <fpage>e1003939</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pgen.1003939" xlink:type="simple">10.1371/journal.pgen.1003939</ext-link></comment> <object-id pub-id-type="pmid">24278029</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friedman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>. <source>A note on the group lasso and a sparse group lasso</source>. <year>2010</year>;<fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004790.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hoadley</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Yau</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wolf</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Cherniack</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Tamborero</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ng</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Multiplatform analysis of 12 cancer types reveals molecular classification within and across tissues of origin</article-title>. <source>Cell</source>. <year>2015</year>;<volume>158</volume>(<issue>4</issue>): <fpage>929</fpage>–<lpage>944</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cell.2014.06.049" xlink:type="simple">10.1016/j.cell.2014.06.049</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>H</given-names></name>. <article-title>Network-constrained regularization and variable selection for analysis of genomic data</article-title>. <source>Bioinformatics</source>. <year>2008</year>;<volume>24</volume>(<issue>9</issue>):<fpage>1175</fpage>–<lpage>1182</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btn081" xlink:type="simple">10.1093/bioinformatics/btn081</ext-link></comment> <object-id pub-id-type="pmid">18310618</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004790.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lee</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Chuang</surname> <given-names>HY</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Ideker</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name>. <article-title>Inferring pathway activity toward precise disease classification</article-title>. <source>PLoS Comput Biol</source>. <year>2008</year>;<volume>4</volume>(<issue>11</issue>):<fpage>e1000217</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000217" xlink:type="simple">10.1371/journal.pcbi.1000217</ext-link></comment> <object-id pub-id-type="pmid">18989396</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>