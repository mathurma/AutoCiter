<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-00262</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002546</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Biophysics</subject>
            <subj-group>
              <subject>Biomechanics</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Biotechnology</subject>
            <subj-group>
              <subject>Bioengineering</subject>
              <subj-group>
                <subject>Biomimetics</subject>
              </subj-group>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Computational biology</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Coding mechanisms</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Biophysic al simulations</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Engineering</subject>
          <subj-group>
            <subject>Bioengineering</subject>
            <subj-group>
              <subject>Biological systems engineering</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Signal processing</subject>
            <subj-group>
              <subject>Speech signal processing</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Nonlinear dynamics</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Biotechnology</subject>
          <subject>Computational Biology</subject>
          <subject>Biophysics</subject>
          <subject>Mathematics</subject>
        </subj-group>
      </article-categories><title-group><article-title>Prosthetic Avian Vocal Organ Controlled by a Freely Behaving Bird Based on a Low Dimensional Model of the Biomechanical Periphery</article-title><alt-title alt-title-type="running-head">Subject-Controlled Prosthetic Avian Vocal Organ</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Arneodo</surname>
            <given-names>Ezequiel M.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Perl</surname>
            <given-names>Yonatan Sanz</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Goller</surname>
            <given-names>Franz</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Mindlin</surname>
            <given-names>Gabriel B.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Laboratorio de sistemas dinámicos, Departamento de Física, FCEyN, Universidad de Buenos Aires, Buenos Aires, Argentina</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Department of Biology, University of Utah, Salt Lake City, Utah, United States of America</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Ayers</surname>
            <given-names>Joseph</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Northeastern University, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">zeke@df.uba.ar</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: FG GBM. Performed the experiments: EMA FG. Analyzed the data: EMA YSP. Contributed reagents/materials/analysis tools: EMA YSP FG GBM. Wrote the paper: EMA GBM.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>6</month>
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>28</day>
        <month>6</month>
        <year>2012</year>
      </pub-date><volume>8</volume><issue>6</issue><elocation-id>e1002546</elocation-id><history>
        <date date-type="received">
          <day>15</day>
          <month>2</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>20</day>
          <month>4</month>
          <year>2012</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2012</copyright-year><copyright-holder>Arneodo et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>Because of the parallels found with human language production and acquisition, birdsong is an ideal animal model to study general mechanisms underlying complex, learned motor behavior. The rich and diverse vocalizations of songbirds emerge as a result of the interaction between a pattern generator in the brain and a highly nontrivial nonlinear periphery. Much of the complexity of this vocal behavior has been understood by studying the physics of the avian vocal organ, particularly the syrinx. A mathematical model describing the complex periphery as a nonlinear dynamical system leads to the conclusion that nontrivial behavior emerges even when the organ is commanded by simple motor instructions: smooth paths in a low dimensional parameter space. An analysis of the model provides insight into which parameters are responsible for generating a rich variety of diverse vocalizations, and what the physiological meaning of these parameters is. By recording the physiological motor instructions elicited by a spontaneously singing muted bird and computing the model on a Digital Signal Processor in real-time, we produce realistic synthetic vocalizations that replace the bird's own auditory feedback. In this way, we build a bio-prosthetic avian vocal organ driven by a freely behaving bird via its physiologically coded motor commands. Since it is based on a low-dimensional nonlinear mathematical model of the peripheral effector, the emulation of the motor behavior requires light computation, in such a way that our bio-prosthetic device can be implemented on a portable platform.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Brain Machine Interfaces (BMIs) decode motor instructions from neuro-physiological recordings and feed them to bio-mimetic effectors. Many applications achieve high accuracy on a limited number of tasks by applying statistical methods to these data to extract features corresponding to certain motor instructions. We built a bio-prosthetic avian vocal organ. The device is based on a low-dimensional mathematical model that accounts for the dynamics of the bird's vocal organ and robustly relates smooth paths in a physiologically meaningful parameter space to complex sequences of vocalizations. The two physiological motor gestures (sub-syringeal pressure and ventral syringeal muscular activity), are reconstructed from the bird's song, and the model is implemented on a portable Digital Signal Processor to produce synthetic birdsong when driven by a freely behaving bird via the sub-syringeal pressure gesture. This exemplifies the plausibility of a type of synthetic interfacing between the brain and a complex behavior. In this type of devices, the understanding of the bio-mechanics of the periphery is key to identifying a low dimensional physiological signal coding the motor instructions, therefore enabling real-time implementation at a low computational cost.</p>
      </abstract><funding-group><funding-statement>This work was funded by NIH (grant R01-DC-006876), CONICET (grant PICT-2010-2767) UBA and Fundación Bunge y Born (EA holds a Premio Fundación Bunge y Born postdoctoral fellowship). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="10"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>The complex motor behavior originating the rich vocalizations of adult oscine birds results from the interaction between a central pattern generator (the brain) and a nonlinear biomechanical periphery (the bird's vocal organ) <xref ref-type="bibr" rid="pcbi.1002546-Zeigler1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Mindlin1">[2]</xref>. The fact that this complex behavior is learned, together with the parallels between the physical mechanisms of birdsong and human speech production, make birdsong an ideal model to study how a complex motor behavior is acquired, produced and maintained <xref ref-type="bibr" rid="pcbi.1002546-Doupe1">[3]</xref>.</p>
      <p>In an effort to understand what gives rise to complexity in this behavior, a part of the birdsong community has set focus on the capabilities of the periphery to produce vocalizations owning a diverse set of nontrivial acoustic features <xref ref-type="bibr" rid="pcbi.1002546-Mindlin1">[2]</xref>. The avian vocal organ, comprised mainly by the respiratory system, the syrinx and the vocal tract, is a highly nonlinear biomechanical device. The complexity of its dynamics leaves traces in the sounds that can be produced in it. In this way, several acoustic features found in vocalizations can be related to nonlinear phenomena occurring in the syrinx <xref ref-type="bibr" rid="pcbi.1002546-Gardner1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Sitt1">[5]</xref> or introduced by acoustic interactions between the syrinx and the tract <xref ref-type="bibr" rid="pcbi.1002546-Zollinger1">[6]</xref>–<xref ref-type="bibr" rid="pcbi.1002546-Arneodo2">[9]</xref>. In all these cases, the complexity of the behavior does not require a complex motor pattern to drive the vocal organ, but rather simple, smooth gestures.</p>
      <p>Through a combination of experimental observations and theoretical analysis, low-dimensional mathematical models have been proposed that account for the physical mechanisms of sound production in the avian vocal organ <xref ref-type="bibr" rid="pcbi.1002546-Ishizaka1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Laje1">[11]</xref>. In particular, a model based on Titze's proposed flapping mechanism for oscillations in human vocal folds <xref ref-type="bibr" rid="pcbi.1002546-Titze1">[12]</xref> was recently used to synthesize the song of the Zebra finch (<italic>Taeniopygia guttata</italic>) <xref ref-type="bibr" rid="pcbi.1002546-Sitt2">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Perl1">[14]</xref>. This model captures the nonlinear dynamics of the folds oscillating to produce sound, in a way that a variety of complex vocalizations are generated by the tuning of parameters related to physiologically observable motor gestures elicited by the bird.</p>
      <p>Part of the appeal of counting with this model is the prospect of applying it to the construction of a bio-prosthetic device. In this scenario computation is relatively inexpensive because of the low dimension of the mathematical model. In addition, the physical description of the peripheral effectors led to the identification of a set of smoothly varying parameters that determine the behavior <xref ref-type="bibr" rid="pcbi.1002546-Sitt2">[13]</xref>. By recording the physiological activity related to the parameters and feeding it to a device that solves the equations of the model in real-time, vocal behavior can be emulated by a prosthesis controlled by a subject via its motor instructions.</p>
      <p>The usual strategy of BCIs and BMIs (Brain Computer Interfaces and Brain Machine Interfaces) is to decode motor commands from recordings of physiological activity in the brain and use this activity to control bio-mimetic devices <xref ref-type="bibr" rid="pcbi.1002546-Schwartz1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1002546-Carmena1">[17]</xref>. In <xref ref-type="bibr" rid="pcbi.1002546-Carmena1">[17]</xref>, for instance, multi-electrode recordings of tens to hundreds of neurons in different cortical areas of primates are used to drive a robotic arm. In recent work, Cichocki <italic>et. al.</italic> discuss the perspectives of using electroencephalographic (EEG) recordings to generate noninvasive BCI solutions <xref ref-type="bibr" rid="pcbi.1002546-Cichocki1">[16]</xref>. In these examples (as well as in many other BCI implementations), the crucial problem is the classification of the features of the large data set which correspond to a determined set of motor tasks. The feature extraction is performed by different techniques which include linear decomposition in a diversity of vector spaces and machine learning algorithms <xref ref-type="bibr" rid="pcbi.1002546-Schwartz1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Cichocki1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Parra1">[18]</xref>. In this way, accurate control of bio-mimetic effectors is achieved for a finite number of specific tasks, such as grasping or cursor moving.</p>
      <p>Our current understanding of the biophysics of the avian vocal organ, particularly our capacity to identify the dynamical mechanisms by which complex behavior occurs when the peripheral systems are driven by low dimensional, smooth instructions, allows us to propose an example of a different kind of bio-prosthetic solution. The model predicts a diversity of qualitatively different solutions to the system for continuous paths in a parameter space. Not only is this parameter space suggested by the model, but it is also physiologically pertinent.</p>
      <p>We present a device that is driven by a freely behaving Zebra finch to produce realistic, synthetic vocalizations in real-time. The device is based on the real time integration of the mathematical model of the vocal organ on a Digital Signal Processor (DSP). It is controlled by the bird's subsyringeal air sac pressure gesture, which is transduced, digitized and fed to the DSP to provide the model with the appropriate path in parameter space.</p>
      <p>The work is organized as follows. In the <xref ref-type="sec" rid="s2">Methods</xref> section we describe the Zebra finch vocal organ and the mathematical model that accounts for its dynamics. We also discuss its applicability to the construction of a model-based bio-prosthetic device, and introduce the physiological motor gestures that relate to parameters of the model. We present the steps leading to the real-time implementation of the model on a device controlled by a spontaneously singing bird. In the Results section we show the example of a successful case. Finally, we summarize the results and discuss the impact of this device as an example of a kind of bio-prosthetic device enabled by the low-dimensional dynamical model of the peripheral effector.</p>
    </sec>
    <sec id="s2" sec-type="methods">
      <title>Methods</title>
      <sec id="s2a">
        <title>Ethics statement</title>
        <p>All experiments were conducted in accordance with the Institutional Animal Care and Use Committee of the University of Utah.</p>
      </sec>
      <sec id="s2b">
        <title>Model for the vocal organ</title>
        <p>One of the most studied species of songbirds is the Zebra finch. Its song presents a set of diverse acoustic features, which can be accounted for by the dynamics displayed by the mathematical model of its vocal organ <xref ref-type="bibr" rid="pcbi.1002546-Sitt1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Laje1">[11]</xref>. This low-dimensional mathematical model, when driven by the appropriate gesture in parameter space, is capable of producing realistic, synthetic birdsong. By implementing this model on a Digital Signal Processor, we are able to construct a bio-prosthetic vocal organ.</p>
        <sec id="s2b1">
          <title>The Zebra finch vocal organ</title>
          <p>The song of an adult Zebra finch is composed of the repetition of a highly stereotyped sequence of syllables, preceded by a variable number of introductory notes. A typical sequence or <italic>motif</italic> is made up of 2 to 8 distinct syllables <xref ref-type="bibr" rid="pcbi.1002546-Zeigler1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Mindlin1">[2]</xref>. Vocalizations present a wide range of acoustic features, which include both tonal sounds where energy is concentrated in the fundamental frequency and spectrally rich ones, where energy is distributed among its harmonics.</p>
          <p>The oscine vocal system has two independent sound sources in the syrinx, where airflow is modulated to produce sound <xref ref-type="bibr" rid="pcbi.1002546-Mindlin1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Gardner1">[4]</xref>. The sounds produced in it travel then through the upper vocal tract: an acoustic pathway that introduces modifications of their acoustic features <xref ref-type="bibr" rid="pcbi.1002546-Zeigler1">[1]</xref>. This is pictured in <xref ref-type="fig" rid="pcbi-1002546-g001">Fig. 1</xref>.</p>
          <fig id="pcbi-1002546-g001" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002546.g001</object-id>
            <label>Figure 1</label>
            <caption>
              <title>Sketch of the Zebra finch vocal system (A).</title>
              <p>Sounds are produced in the syringeal valves, and then filtered through the vocal tract. In the syrinx, the labia oscillate modulating the airflow. They support two coordinated modes of oscillation: an upward propagating superficial wave, and an oscillation around their mass centers (C). In the vocal tract we highlight trachea, glottis, OEC and beak. Pressure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e001" xlink:type="simple"/></inline-formula> is the subsyringeal pressure, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e002" xlink:type="simple"/></inline-formula> stands for the pressure at the input of the tube and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e003" xlink:type="simple"/></inline-formula> stands for the pressure at the output of the trachea. In order to compute the model, we write the equivalent circuit of the post-tracheal part of the vocal tract (B).</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002546.g001" xlink:type="simple"/>
          </fig>
          <p>The bipartite syrinx of the oscines is a pair of valves, located at the junction of the bronchi and the trachea (see <xref ref-type="fig" rid="pcbi-1002546-g001">Figs. 1a and 1c</xref>). In each valve, small connective tissue pads, or labia, oscillate for high enough values of the airflow. Songbirds use their labia to produce sound in a similar way humans use vocal folds to produce speech. During vocalization, respiratory airflow sets the labia in an oscillatory regime. The oscillating labia modulate the airflow, resulting in a sound wave that then travels through the vocal tract. The airflow is controlled by the bird via the subsyringeal air sac pressure <xref ref-type="bibr" rid="pcbi.1002546-Laje1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Larsen1">[19]</xref>. The correlation of the electromyographic (EMG) activity of the ventral syringeal muscle (vS) and the fundamental frequency of a vocalization suggests that they control the tension of the oscillating labia <xref ref-type="bibr" rid="pcbi.1002546-Laje1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Goller1">[20]</xref>.</p>
          <p>The upper vocal tract consists of the air-filled passages that link the syrinx to the environment. It determines much of the distribution of energy of the sound across its harmonic frequency components, defining a perceptual property as important as the timbre <xref ref-type="bibr" rid="pcbi.1002546-Nowicki1">[21]</xref>–<xref ref-type="bibr" rid="pcbi.1002546-Assaneo1">[23]</xref>. Its filtering characteristics depend on the acoustic properties of its components. Assuming that the compartments composing the upper vocal tract do not interact acoustically, they are considered individually. The most relevant components are the trachea, the glottis and oropharyngeal-esophageal cavity, and the beak <xref ref-type="bibr" rid="pcbi.1002546-Riede1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Fletcher1">[24]</xref>.</p>
        </sec>
        <sec id="s2b2">
          <title>Model for the source</title>
          <p>A model to account for the mechanism of sound production in the bird's syrinx that presents a good compromise between level of description and computational complexity was presented in <xref ref-type="bibr" rid="pcbi.1002546-Gardner1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Laje1">[11]</xref>. It is based on Titze's flapping mechanism, one of the simplest models to account for the transfer of the kinetic energy of airflow to vocal fold oscillations in humans <xref ref-type="bibr" rid="pcbi.1002546-Titze1">[12]</xref>. Based on experimental observations, it assumes that the soft pieces of tissue or labia support two modes of vibration: an upward propagating wave and a lateral displacement around their midpoint positions. It has been suggested by videography of the folds during phonation <xref ref-type="bibr" rid="pcbi.1002546-Titze1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Larsen1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Fee1">[25]</xref> that these modes are coordinated in a way that energy is gained from the airflow in each cycle, making sustained oscillations possible.</p>
          <p>The kinematic description of this mechanism is carried in terms of the displacement from equilibrium of the midpoint position of each labium <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e004" xlink:type="simple"/></inline-formula>. We assume that the profile of the syringeal valve is trapezoidal, as shown in <xref ref-type="fig" rid="pcbi-1002546-g001">Fig. 1c</xref>. The motion of the midpoint position of the labia will obey Newton's second law:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e005" xlink:type="simple"/><label>(1)</label></disp-formula>In the right hand side of the second equation, the first term describes the nonlinear elastic restitution of the labium (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e006" xlink:type="simple"/></inline-formula>), the second term represents nonlinear dissipation (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e007" xlink:type="simple"/></inline-formula>), and the third term a nonlinear saturation that bounds the labial motion. The system is driven by the last term, which accounts for the force exerted by the inter-labial pressure (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e008" xlink:type="simple"/></inline-formula> stands for the area of the labium).</p>
          <p>In the driving term, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e009" xlink:type="simple"/></inline-formula> is the spatial average of the syringeal driving pressure. It is a function of the geometry of the folds that depends on the sub-syringeal pressure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e010" xlink:type="simple"/></inline-formula>, and the ratio of the tracheal and bronchial sections of the labial valve. Since the contribution of the pressure at the input of the tract <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e011" xlink:type="simple"/></inline-formula> to the trans-syringeal pressure is much smaller than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e012" xlink:type="simple"/></inline-formula>, it is neglected <xref ref-type="bibr" rid="pcbi.1002546-Sitt1">[5]</xref>. This means that the sound source and the vocal tract are not considered to interact acoustically. The coordination of the two oscillation modes supported by the tissue, experimentally observed as a phase difference between the upper and lower extremes of the labia during phonation <xref ref-type="bibr" rid="pcbi.1002546-Larsen1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Fee1">[25]</xref>, is taken into account in this function. By introducing a phenomenological parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e013" xlink:type="simple"/></inline-formula> that describes the time it takes the wave propagating upward in the labia to cover half its length, the bronchial and tracheal areas of the valve (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e014" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e015" xlink:type="simple"/></inline-formula>) get to be written in terms of this time constant, the midpoint displacement of the labia and the resting (pre-phonatory) positions of the edges of the labia (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e016" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e017" xlink:type="simple"/></inline-formula>) <xref ref-type="bibr" rid="pcbi.1002546-Titze1">[12]</xref>. Finally, a phenomenologically corrected version of the Bernoulli equation is used to write the average syringeal pressure in terms of the ratio of the sections and the sub-syringeal pressure <xref ref-type="bibr" rid="pcbi.1002546-Laje1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Titze1">[12]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e018" xlink:type="simple"/><label>(2)</label></disp-formula>Putting equations (1) and (2) together, an extended “flapping” model ruling the dynamics of the labia in the Zebra finch syrinx is complete.</p>
          <p>In this model, acoustic features of the solutions are determined by physiologically meaningful parameters. Assuming that the coefficients in the restitution term of system (1) are proportional to the tension of the ventral syringeal muscles (vS), this model is capable of producing synthetic, realistic birdsong. The rationale behind this assumption is that the contraction of these muscles handles the stiffness of the labia by stretching them <xref ref-type="bibr" rid="pcbi.1002546-Laje1">[11]</xref>. In <xref ref-type="bibr" rid="pcbi.1002546-Sitt2">[13]</xref>, EMG activity recordings from electrodes implanted in the vS muscles were used to obtain a time dependent parametrization of the restitution (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e019" xlink:type="simple"/></inline-formula>), and air sac pressure data for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e020" xlink:type="simple"/></inline-formula>. In this way, birdsong can be synthesized by driving the model with actual physiological data.</p>
        </sec>
        <sec id="s2b3">
          <title>Model for the vocal tract</title>
          <p>The oscillating labia in the syrinx modulate the airflow producing sound, which is modified as it goes through the vocal tract. Relevant acoustic features, such as the spectral content of vocalizations, are determined by the geometry of the vocal tract <xref ref-type="bibr" rid="pcbi.1002546-Sitt1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Perl1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Titze2">[26]</xref>. Important elements modulating birdsong in the vocal tract are the trachea, the oropharyngeal-esophageal cavity (OEC) and the beak, as noticed by Nowicki <xref ref-type="bibr" rid="pcbi.1002546-Nowicki1">[21]</xref> and Fletcher <italic>et. al.</italic> <xref ref-type="bibr" rid="pcbi.1002546-Riede1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Fletcher1">[24]</xref>.</p>
          <p>In order to produce realistic synthetic vocalizations, we introduce a model of the vocal tract as a dynamical system, which includes a tube approximating the trachea and a Helmholtz resonator to represent the OEC. The sound produced in the syrinx enters this system, and we are able to compute the sound radiated to the atmosphere.</p>
          <p>The trachea, by its effect and its physiology, is approximated by a tube that is closed in the syringeal end and open at the glottis. The pressure at its input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e021" xlink:type="simple"/></inline-formula> is determined by two main contributions: one originated from fluctuations originated in the syrinx, and the other caused by reflections at the open end. Considering both contributions, which are derived in <xref ref-type="bibr" rid="pcbi.1002546-Arneodo1">[8]</xref>, the pressure at the input of the trachea reads<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e022" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e023" xlink:type="simple"/></inline-formula> is proportional to the mean velocity of the flow, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e024" xlink:type="simple"/></inline-formula> the time it takes a sound wave entering the tube to get to the open end and be partially reflected back with coefficient <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e025" xlink:type="simple"/></inline-formula>.</p>
          <p>The transmitted part of the pressure fluctuation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e026" xlink:type="simple"/></inline-formula> forces the air at the glottis, approximated by the neck of the Helmholtz resonator that stands for the OEC. The mass of air at the glottis, forced into the cavity, is subject to a restitution force exerted by the larger mass of air in it.</p>
          <p>In acoustics, it is common to write an analog electronic computational model to describe a system of filters. The acoustic pressure is represented by an electric potential and the volume flow by the electric current <xref ref-type="bibr" rid="pcbi.1002546-Coppens1">[27]</xref>. In this framework, short constrictions are inductors, and cavities (smaller than the wavelengths) are well represented by capacitors. The equations for the equivalent circuit of the post-tracheal part of the vocal tract, shown in <xref ref-type="fig" rid="pcbi-1002546-g001">Fig. 1b</xref>, read:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e027" xlink:type="simple"/><label>(4)</label></disp-formula>where the electric components relate to geometric parameters of acoustic elements. Such relationships, which are standard in acoustics, can be found in <xref ref-type="bibr" rid="pcbi.1002546-Perl1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Coppens1">[27]</xref>. The pressure fluctuations at the glottal end of the trachea <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e028" xlink:type="simple"/></inline-formula> relate linearly to the electric tension <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e029" xlink:type="simple"/></inline-formula> driving the circuit. Following the same scheme, the electrical potential at the resistor standing for the beak <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e030" xlink:type="simple"/></inline-formula> is the analogue of the pressure fluctuations at the output of the beak <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e031" xlink:type="simple"/></inline-formula>. In our model, this quantity is the sound radiated by the vocal organ.</p>
        </sec>
        <sec id="s2b4">
          <title>Dynamical analysis of the model</title>
          <p>The mathematical model of the vocal organ represented by equations (1, 2, 3 and 4), rather than an attempt to obtain a statistical a filter or a set of causal rules between a coded motor command and a behavior, is a physical description of the complex peripheral effector. By studying the nonlinear dynamics of this model, specially that of its oscillatory solutions, we find nontrivial relationships between paths in parameter space and acoustic characteristics of sounds synthesized by it.</p>
          <p>In this model, the non-interacting vocal tract acts as a passive filter; sounds produced in the syrinx are altered by it only on the relative weight of their harmonic components. This is why the dynamical analysis searching for qualitatively different oscillatory solutions is done on the equations ruling the dynamics of the syrinx. A thorough description of the set of solutions and bifurcations of the system (1,2) was carried out in <xref ref-type="bibr" rid="pcbi.1002546-Sitt1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Amador1">[28]</xref>, and a phase portrait was constructed for the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e032" xlink:type="simple"/></inline-formula> parameter space. This set is the most plausible to determine acoustic properties of sounds, since it relates to the restitution constant of the oscillating labia and the intensity of the force exerted by the airflow. These parameters are also identified with physiological instructions used by the bird for motor control <xref ref-type="bibr" rid="pcbi.1002546-Mindlin1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Laje1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-GOLLER1">[29]</xref>.</p>
          <p>Within the rich dynamical scenario displayed by the system, two distinct mechanisms giving rise to oscillatory solutions stand out: a Hopf and a Saddle Node on an Invariant Cycle (SNIC) bifurcations. They are sketched in <xref ref-type="fig" rid="pcbi-1002546-g002">Fig. 2</xref>, together with a schematic representation of the solutions and their spectral properties. Through a sub-critical Hopf bifurcation, one fixed point loses its stability and a small, stable limit cycle is created <xref ref-type="bibr" rid="pcbi.1002546-Strogatz1">[30]</xref>. Oscillations are born with zero amplitude and finite frequency, therefore the sounds produced in this way are tonal. When a saddle and a non-saddle fixed point collide on a limit cycle, a SNIC bifurcation occurs at which oscillations with finite amplitude and infinite period are born <xref ref-type="bibr" rid="pcbi.1002546-Strogatz1">[30]</xref>. Sounds produced by this mechanism have consequently low frequency and their spectral content is rich.</p>
          <fig id="pcbi-1002546-g002" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002546.g002</object-id>
            <label>Figure 2</label>
            <caption>
              <title>Sketch of the bifurcations found in the model of the syrinx leading to oscillatory solutions.</title>
              <p>When parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e033" xlink:type="simple"/></inline-formula> crosses a SNIC bifurcation, two fixed points collide within a limit cycle (A). The limit cycle is born with large period and rich spectral content, as sketched for the temporal series of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e034" xlink:type="simple"/></inline-formula> variable and its corresponding spectrogram. When parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e035" xlink:type="simple"/></inline-formula> crosses a Hopf bifurcation, a stable fixed point loses stability against a newborn limit cycle (B). Close to the bifurcation, this limit cycle exhibits small period and ideally tonal spectral content, as sketched for the temporal series of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e036" xlink:type="simple"/></inline-formula> variable and its corresponding spectrogram. Both bifurcations occur at distinct regions in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e037" xlink:type="simple"/></inline-formula> parameter space <xref ref-type="bibr" rid="pcbi.1002546-Sitt1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Amador1">[28]</xref>.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002546.g002" xlink:type="simple"/>
          </fig>
          <p>In addition to providing an orientation in the task of seeking the relevant control parameters, the dynamical analysis of the model makes way for the real-time implementation of the model by reducing its computational cost while retaining the relevant dynamics <xref ref-type="bibr" rid="pcbi.1002546-Sitt2">[13]</xref>. A polynomial coordinate transformation around a singular point in the parameter space is usually applied to dynamical systems to reduce the system to a normal form, leaving only the nonlinear terms needed to reproduce the set of bifurcations around that singularity <xref ref-type="bibr" rid="pcbi.1002546-Guckenheimer1">[31]</xref>. Our model described by eqs. (1, 2) presents a Takens-Bogdanov singularity, where the SNIC and the Hopf bifurcation lines meet tangentially. The reduction to the normal form around that point eliminates many nonlinear terms and takes the system to a form where computation is much cheaper. When doing so, it retains the bifurcations that lead to oscillatory behavior. The new system reads:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e038" xlink:type="simple"/><label>(5)</label></disp-formula>where a proper mapping of air sac pressure and tension into the unfolding parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e039" xlink:type="simple"/></inline-formula> allows us to recover the qualitative dynamics. In <xref ref-type="bibr" rid="pcbi.1002546-Sitt2">[13]</xref> the relationship was explicitly computed; the basic operations involved are: a translation of the values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e040" xlink:type="simple"/></inline-formula> where a Takens Bogdanov bifurcation takes place in the physical model to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e041" xlink:type="simple"/></inline-formula>, a multiplicative scaling, and a rotation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e042" xlink:type="simple"/></inline-formula>.</p>
        </sec>
      </sec>
      <sec id="s2c">
        <title>On-line synthesis driven by a freely behaving bird</title>
        <p>The mathematical model for the vocal organ, the reduction of the system ruling the dynamics of the sound source, and the identification of the pertinent parameters accounting for its motor control, they all make way for the construction of a bio-prosthetic device. The parameters determining acoustic properties of vocalizations in the normal form (5) are physiologically meaningful and the set of differential equations is easy enough to compute in a portable platform such as a Digital Signal Processor. By fitting the parameters and integrating the system in real-time, synthetic song can be produced in a device controlled by the motor instructions elicited by a freely behaving bird.</p>
        <p>In many bio-prosthetic solutions, the physiological motor gestures used to drive the device are degraded respect to those recorded in the intact subjects <xref ref-type="bibr" rid="pcbi.1002546-Schwartz1">[15]</xref>. One application of this device is the performance of altered auditory feedback experiments <xref ref-type="bibr" rid="pcbi.1002546-Brainard1">[32]</xref>–<xref ref-type="bibr" rid="pcbi.1002546-Sakata1">[34]</xref>. In experiments enabled by this device, the bird's own auditory feedback can be replaced by synthetic birdsong computed in real-time. Since the synthetic feedback is produced by the integration of the model when fed with actual physiological motor gestures elicited by a freely behaving bird, alterations of the feedback are possible that are consistent with alterations in the motor gestures intended to produce them. Here, the prosthetic vocal organ is driven by a bird that is muted via the insertion of a cannula through its inter-clavicular air sac. Phonation is prevented as the airflow is bypassed away from the syrinx. As a side effect, the pressure pattern registered on the muted bird differs from that recorded in the intact animal.</p>
        <p>The device reconstructs the intended motor gesture from this degraded pressure gesture to trigger integration of the model. When the pressure pattern corresponding to the syllables comprising a motif are identified, the mathematical model for the vocal organ is computed with the appropriate paths in parameter space, to produce the corresponding synthetic output.</p>
        <sec id="s2c1">
          <title>Implementation</title>
          <p>The electronic syrinx is capable of reproducing synthetic birdsong in real-time when driven by the air sac pressure of a freely behaving, muted bird. The pressure gesture is recorded, together with the bird's song, in order to fit the parameters of the model. The bird is then muted via a bypass of airflow away from the syrinx and its pressure gesture is digitized and fed to the Digital Signal Processor (DSP), where the model is implemented. An algorithm that reconstructs the pressure gesture of the intact bird is also implemented, to trigger the integration of the model on when the gesture corresponding to the first syllable of a motif is detected and off when it corresponds. Below there is a description of the procedure.</p>
        </sec>
        <sec id="s2c2">
          <title>Record of the pressure gesture</title>
          <p>The bird is cannulated. A cannula (<italic>Silastic laboratory tubing</italic>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e043" xlink:type="simple"/></inline-formula> I.D., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e044" xlink:type="simple"/></inline-formula> O.D.) is inserted through an incision in the thoracic air sac. The other end of the cannula is connected to a <italic>Fujikura FPMC-07PGR</italic> piezoelectric transducer, amplified and digitized and recorded at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e045" xlink:type="simple"/></inline-formula>, simultaneously with the sound (recorded with a <italic>Takstar SGC568 microphone</italic>) produced by the bird. The bird is placed in an acoustic box and sound and pressure are recorded while it spontaneously sings.</p>
        </sec>
        <sec id="s2c3">
          <title>Reconstruction of the motor gesture</title>
          <p>These data are then used to fit the physiologically meaningful parameters of the vocal organ model in its normal form (system (5)): the time-varying (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e046" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e047" xlink:type="simple"/></inline-formula>) series are constructed so that the synthetic sound produced upon integration of the model matches the fundamental frequency and spectral content of the recorded song.</p>
          <p>The characteristics of sounds produced by integrating the normal form of the model are determined by paths in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e048" xlink:type="simple"/></inline-formula> parameter space, which are transformations of the subsyringeal pressure and the activity of the ventral syringeal muscle <xref ref-type="bibr" rid="pcbi.1002546-Sitt2">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Perl1">[14]</xref>. In order to produce synthetic vocalizations, the motor gesture is reconstructed from the recorded sound, looking for the excursions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e049" xlink:type="simple"/></inline-formula> that lead to solutions of matching properties. The fitting procedure and the reconstruction of the motor gesture from the recorded song are detailed in <xref ref-type="bibr" rid="pcbi.1002546-Perl1">[14]</xref>.</p>
          <p>Two important features that we seek to match are the spectral richness and the fundamental frequency of the vocalizations. One way of quantifying the spectral richness is to compute the Spectral Content Index (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e050" xlink:type="simple"/></inline-formula>) of a sound segment <xref ref-type="bibr" rid="pcbi.1002546-Sitt1">[5]</xref>. It measures how spread is the energy of a sound relative to its fundamental frequency, being its minimum <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e051" xlink:type="simple"/></inline-formula> for perfectly tonal sounds.</p>
          <p>Parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e052" xlink:type="simple"/></inline-formula> in system (5) determines the temporal scale. With this and the parameters in the description of the vocal tract fixed, a fundamental frequency and SCI <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e053" xlink:type="simple"/></inline-formula> corresponds to every point in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e054" xlink:type="simple"/></inline-formula> space. The dependence of the sound properties on the parameters is understood in terms of the distance, in parameter space, from the lines of bifurcation of the model. The further a point is from the Hopf and the SNIC bifurcation, the higher its corresponding fundamental frequency and spectral content index <xref ref-type="bibr" rid="pcbi.1002546-Sitt1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Amador1">[28]</xref>.</p>
          <p>The optimal value of the scaling factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e055" xlink:type="simple"/></inline-formula> was found minimizing the accumulated distance in the acoustic features space <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e056" xlink:type="simple"/></inline-formula> between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e057" xlink:type="simple"/></inline-formula> recorded segments (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e058" xlink:type="simple"/></inline-formula> long) of birdsong and their targeting syntheses (see details in <xref ref-type="bibr" rid="pcbi.1002546-Perl1">[14]</xref>). For every value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e059" xlink:type="simple"/></inline-formula> within a range, sounds corresponding to a grid of points in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e060" xlink:type="simple"/></inline-formula> space were synthesized, and their fundamental frequency and spectral content index were computed. For each recorded segment, the synthetic sound minimizing the distance in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e061" xlink:type="simple"/></inline-formula> space was found. The sum of these minimum distances was carried over all the recorded segments, for every <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e062" xlink:type="simple"/></inline-formula> in the range. The value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e063" xlink:type="simple"/></inline-formula> for which this overall distance is smallest is the optimal.</p>
          <p>Setting also the parameters of the tract so that the resonance of the OEC lies close to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e064" xlink:type="simple"/></inline-formula> and the tube representing the trachea is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e065" xlink:type="simple"/></inline-formula> long, we reconstruct the motor gesture that synthesizes the bird's motif. To do so, the recorded song (sampled at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e066" xlink:type="simple"/></inline-formula>) was decomposed into a sequence of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e067" xlink:type="simple"/></inline-formula> segments. For each segment, the SCI and fundamental frequency were computed. A search in the parameter space <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e068" xlink:type="simple"/></inline-formula> was performed until values were found which allow to synthesize sounds with the most similar acoustic features possible for the available parameter range. To do so, the fundamental frequency was computed for a grid in the parameter space, and the set of pairs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e069" xlink:type="simple"/></inline-formula> that produced synthetic sounds the closest to the fundamental frequency of the recording was selected. Within that set, the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e070" xlink:type="simple"/></inline-formula> minimizing the distance in SCI value was chosen.</p>
          <p>By these means and upon smoothing to interpolate the values of the parameters within each segment, a table of values of the reconstructed motor gestures is obtained for every segment of the bird's motif. When the model is driven by them, synthetic song comparable to the one that was recorded is produced. An example of the fitted parameter series is illustrated in <xref ref-type="fig" rid="pcbi-1002546-g003">Fig. 3B</xref>, along with the actual song (<xref ref-type="fig" rid="pcbi-1002546-g003">Fig. 3A</xref>) and the motif synthesized by the model (<xref ref-type="fig" rid="pcbi-1002546-g003">Fig. 3D</xref>).</p>
          <fig id="pcbi-1002546-g003" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002546.g003</object-id>
            <label>Figure 3</label>
            <caption>
              <title>Illustration of the parameter fitting and calibration procedure.</title>
              <p>The thoracic air sac pressure is recorded together with the song; the pressure and corresponding sound of a bout are shown in (A). With these records, the temporal series <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e071" xlink:type="simple"/></inline-formula> that originate the syllables corresponding to the motif are constructed (B). After muting the bird and registering the pressure gesture as it attempts to produce a motif, the detection algorithm is tuned. In (C) we show the degraded pressure gesture, together with the correlation with the chosen segment of the intact pressure gesture. The segments pointed out by arrows indicate the detection of the intention to sing a motif. Song is synthesized during these periods by integration of the model with the parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e072" xlink:type="simple"/></inline-formula> found previously. In (D), we show the pressure gesture of the muted bird and the output of the trigger/integrate algorithm.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002546.g003" xlink:type="simple"/>
          </fig>
          <p>In <xref ref-type="bibr" rid="pcbi.1002546-Perl1">[14]</xref>, values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e073" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e074" xlink:type="simple"/></inline-formula> reconstructed in this way were used to estimate the values of air sac pressure and activity of the ventral syringeal muscle (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e075" xlink:type="simple"/></inline-formula>), through a change of sign <xref ref-type="bibr" rid="pcbi.1002546-Sitt2">[13]</xref> and multiplication by a scaling factor. When the reconstructed gestures were compared against the actual, recorded values, the agreement was strong.</p>
        </sec>
        <sec id="s2c4">
          <title>Muting of the bird</title>
          <p>The bird is then muted via the insertion of an open cannula (<italic>Braintree Scientific</italic> MRE 05, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e076" xlink:type="simple"/></inline-formula> I.D., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e077" xlink:type="simple"/></inline-formula> O.D.) through the interclavicular air sac. Airflow is deviated away from the syringeal valve, and phonation is prevented. Muting achieved in this way introduces as a side effect a degradation of the registered pressure, in a degree that is not systematic (for example, it is not constant in time for each subject, it is not an obvious algebraic transformation of the intact pressure gesture and its similarity with it varies from bird to bird).</p>
          <p>The pressure pattern of the muted bird is then recorded as the bird attempts to produce song by producing the gesture corresponding to calls, introductory notes, and motifs.</p>
        </sec>
        <sec id="s2c5">
          <title>Control of the model via the degraded pressure gesture</title>
          <p>The recordings of the intact and degraded pressure gestures (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e078" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e079" xlink:type="simple"/></inline-formula> respectively) are used to tune the algorithm that will calculate the correlation of the muted pressure gesture with the intact pressure gesture to recognize the beginning of the first syllable of a motif. A test segment of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e080" xlink:type="simple"/></inline-formula> of the pressure gesture of the intact bird is selected, which contains the last part of the preceding introductory note and the first part of the first syllable of the motif. For each sample of the altered pressure gesture, a window of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e081" xlink:type="simple"/></inline-formula> (ending at that point) is taken and its cross-correlation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e082" xlink:type="simple"/></inline-formula> with the test segment of the intact gesture is computed.</p>
          <p>The length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e083" xlink:type="simple"/></inline-formula> of the test segment of the intact pressure gesture is kept fixed, once determined. To determine it, we do an off-line exploration to select the segment that best enables discrimination of the beginning of the song bout, while minimizing the delay in the detection. The test segment should include as few samples of the first part of the motif as possible. The reason for this is that, when dealing with the stream of data in real-time, those samples required for the detection introduce a delay between the moment the bird attempts to sing and the beginning of the synthesis. In this exploration we seek the segment that complies with this condition and that can be distinguished from the calls and the introductory notes. To do so, we observe about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e084" xlink:type="simple"/></inline-formula> calls and introductory notes, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e085" xlink:type="simple"/></inline-formula> motifs. We look for the test segment for which the mean cross-correlation at the beginning of a motif is at least <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e086" xlink:type="simple"/></inline-formula> times the mean cross-correlation at calls and introductory notes.</p>
          <p>This ensures that a correlation threshold can be set that leaves high probability of detection of the bout with low probability of false triggering (false inference of the onset of a motif from a call or any other note). Assuming that the cross-correlation values in the calls and introductory notes and the cross-correlation values in the beginning of a motif follow normal distributions with distinct mean and variance, these probabilities can be estimated. In the example shown here, the pressure gesture of a muted bird was recorded as it attempted to elicit <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e087" xlink:type="simple"/></inline-formula> calls or introductory notes and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e088" xlink:type="simple"/></inline-formula> motifs. With the selected test segment, the mean cross-correlation for introductory notes and calls was slightly higher than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e089" xlink:type="simple"/></inline-formula> times the mean cross-correlation for the motifs. In this case, a threshold could be set for which the estimated probability of missing the trigger (computed as the area of the distribution of the correlation for the motifs to the left of the threshold) was close to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e090" xlink:type="simple"/></inline-formula>. At the same time, the probability of false triggering (computed as the area of the distribution of the correlation for the calls and introductory notes to the right of the threshold) was close to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e091" xlink:type="simple"/></inline-formula>.</p>
          <p>The criterion for the selection of these quantities responds to the times of the procedure and the rate of success. The number of motifs used for calibration is the data available after one recording session of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e092" xlink:type="simple"/></inline-formula>, which takes place the day after the bird is muted. The proposed correlation threshold allows for the detection of the motif in most cases, while almost completely avoiding false triggering.</p>
          <p>In <xref ref-type="fig" rid="pcbi-1002546-g003">Fig. 3C</xref>, we show the cross-correlation along a segment of the pressure gesture of a muted bird. The segment includes two bouts, each made of one motif starting after a different number of introductory notes. When the correlation threshold is reached, integration of the model is triggered, as shown in <xref ref-type="fig" rid="pcbi-1002546-g003">Fig. 3D</xref>. In this example (bird R01), the test segment of the intact pressure gesture included only the first <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e093" xlink:type="simple"/></inline-formula> ms of the motif.</p>
          <p>We also tune an algorithm that detects the interruption of the song, comparing properties of the muted pressure gesture with thresholds on absolute value and variation of the intact gesture during the subsequent motif. The song bout of a Zebra finch is highly stereotyped. Once a motif starts, a fixed sequence of syllables is followed, and a bout consists of the repetition of this motif, with the eventual interleaving of an introductory note between one and the next <xref ref-type="bibr" rid="pcbi.1002546-Zeigler1">[1]</xref>. In order to control the electronic syrinx, it suffices to infer when a motif starts and when it is interrupted. Until the interruption is detected, the adequate vocalization is synthesized by integrating the model with the corresponding sequence of parameter values. The sequence is followed in segments of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e094" xlink:type="simple"/></inline-formula>, checking at the end of each window if integration of the next segment or silence corresponds. The end of phonation occurs when the pressure gesture remains below a certain threshold after registering a descending slope.</p>
          <p>The method used to trigger the integration of the model introduces a delay. A segment of the first syllable is used to detect the onset of a motif in the muted bird's pressure gesture. The length of this segment determines the time it takes the synthesis to begin after the bird has attempted to produce the syllable. The synthesis, which begins after the detection, produces the song that corresponds immediately after that segment. In this way, the delay does not introduce a shift in time in the feedback. Instead, the part of the motif used for detection is skipped.</p>
          <p>The model with the constructed parameter paths (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e095" xlink:type="simple"/></inline-formula>) and the detection algorithms are then programmed in a DSP developing platform (<italic>Texas Instruments DSK6713</italic>). This platform contains a memory, a set of AD/DA converters for input and output and a floating point DSP running at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e096" xlink:type="simple"/></inline-formula>. The muted bird is placed in the acoustic box and its pressure gesture is registered and fed to the DSP, where the detection algorithm runs in real-time. The recorded pressure gesture, sampled at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e097" xlink:type="simple"/></inline-formula>, goes through the detection algorithm. When the beginning of the motif is inferred from the muted pressure gesture, the model is numerically integrated with the corresponding values of the parameters. Numerical integration of the model is carried out via the Euler method at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e098" xlink:type="simple"/></inline-formula> steps, in a way that output can be generated at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e099" xlink:type="simple"/></inline-formula>. This output, corresponding to the sound produced by the syrinx, is used to compute the pressure at the output of the trachea and sent to another DSP that computes the dynamical equations that account for the vocal tract. The synthetic sound produced by these means is then converted into an analog signal, amplified and played through a speaker, placed in the acoustic box, about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e100" xlink:type="simple"/></inline-formula> from the bird. The setup is illustrated in <xref ref-type="fig" rid="pcbi-1002546-g004">Fig. 4</xref>.</p>
          <fig id="pcbi-1002546-g004" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002546.g004</object-id>
            <label>Figure 4</label>
            <caption>
              <title>Sketch of the experimental setup where the muted bird drives the electronic vocal organ.</title>
              <p>The pressure gesture of the bird is recorded simultaneously with its song. Then, the parameters driving the normal form to produce synthetic song are reconstructed, and the bird is muted. The bird is then connected to the electronic syrinx via its thoracic air sac pressure, which is digitized and fed to the DSP. In the DSP, an algorithm detects the onset of the first syllable of the motif in the degraded gesture of the muted bird. Upon detection, the model is integrated in real-time while the attempt of the bird to continue with the motif is inferred from the motor gesture. The computed pressure fluctuations at the output of the beak are converted to an analog signal and played through a speaker located <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e101" xlink:type="simple"/></inline-formula> away from the bird.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002546.g004" xlink:type="simple"/>
          </fig>
          <p>The sound registered in the box via the microphone, the direct analog output of the DSP system and the altered pressure gesture are digitized and recorded at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e102" xlink:type="simple"/></inline-formula>.</p>
        </sec>
      </sec>
    </sec>
    <sec id="s3">
      <title>Results</title>
      <sec id="s3a">
        <title>Subject-driven online synthesis</title>
        <p>The device succeeds in synthesizing song online when driven by the pressure gesture of a muted bird. From the altered motor gesture, the algorithm infers the segment of a motif intended by the bird and computes the model to produce the vocalizations. An example is illustrated in <xref ref-type="fig" rid="pcbi-1002546-g005">Fig. 5</xref>.</p>
        <fig id="pcbi-1002546-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002546.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Intact song and subject-driven, synthetic song.</title>
            <p>Intact pressure gesture and sonogram, with different colors and opacity of shading indicating the different syllables, and an arrow indicating the segment of the first syllable used for detection (upper panels). When the muted bird drives the syrinx, we see in the sonogram that synthetic sound is produced after the first syllable is detected and until recognition of the interruption of the motif (lower panels).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002546.g005" xlink:type="simple"/>
        </fig>
        <p>The upper panels of the figure display the recorded subsyringeal pressure and sonogram of a segment of a bout with its preceding introductory call. A song bout of this bird (B06) is composed of a number of introductory notes (O) and the repetition of a simple motif containing two syllables (A and B), indicated by different colors and opacity of shading in <xref ref-type="fig" rid="pcbi-1002546-g005">Fig. 5</xref>. An initial segment of syllable A (marked with clearer shading in the figure) is used to detect the intention to elicit a motif in the pressure gesture of the muted bird. The part that is produced upon triggering of the synthesis appears in a darker shade of the same color. These recordings were used to fit the parameters of the model to produce synthetic vocalizations showing a match in fundamental frequency and spectral content.</p>
        <p>The bird is then muted and placed in the setup to drive the electronic vocal organ with its pressure gesture. In the lower panel of <xref ref-type="fig" rid="pcbi-1002546-g005">Fig. 5</xref> we show the pressure gesture of the muted bird and the sonogram of the sound recorded by a microphone placed close (about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e103" xlink:type="simple"/></inline-formula>) to the bird and the speaker. It can be noted in the sonogram that no sound is produced during the bird's attempts to phonate an introductory note. When the pressure motor gesture corresponds to the first syllable in the bout (syllable A), the instruction is recognized and the corresponding song is synthesized and played through the speaker. In the first bout, the bird only elicits the gesture corresponding to the first syllable, and then stops. In this case the algorithm detects the interruption and turns off the integration. In the second bout, the bird continues with the second syllable and drives the electronic syrinx to the end of the motif.</p>
        <p>This example illustrates how this device works, and shows that it is successful in synthesizing the song motif as the bird drives it. We evaluate its success by counting the times the motif was properly detected and synthesized, and how many times a false trigger occurred. During a session of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e104" xlink:type="simple"/></inline-formula> hours (the second day after the muting took place), a muted bird elicited about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e105" xlink:type="simple"/></inline-formula> calls, out of which less than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e106" xlink:type="simple"/></inline-formula> generated false triggering of synthetic song. In most cases the false trigger event was recognized by the algorithm and silenced after less than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e107" xlink:type="simple"/></inline-formula>. The rate of success in detecting the beginning of a bout was of about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e108" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e109" xlink:type="simple"/></inline-formula> attempted bouts elicited by the bird.</p>
        <p>Despite the variability of the altered pressure gesture in the subsequent days (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e110" xlink:type="simple"/></inline-formula> days after the muting), a brief calibration before each daily session allowed the rates of success and false triggers to be maintained. To do this calibration, the pressure gesture was recorded during <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e111" xlink:type="simple"/></inline-formula> and these data were used to re-set the cross-correlation thresholds, while keeping the test segment of the intact pressure gesture previously selected.</p>
      </sec>
      <sec id="s3b">
        <title>Perspectives for BMI</title>
        <p>We show here that realistic vocal behavior is synthesized in real-time by our device, as it is controlled by the spontaneous behavior of a muted bird, a physiological signal (its air sac pressure) that is degraded in respect to the one recorded in the intact bird. The computing platform is a low cost, portable processor, and the initial rate of success is high. This is an encouraging example of the plausibility of a kind of interface between the central motor pattern generator and the synthetic, bio-mimetic behavior. DSP technology is being implemented in a variety of biologically inspired problems, and together with Field Programmable Gate Array technology (FPGA) is likely to become a standard solution for a variety of bio-mimetic applications <xref ref-type="bibr" rid="pcbi.1002546-Schwartz1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Darmanjian1">[35]</xref>.</p>
        <p>Brain computer and brain machine interfaces (BCI and BMI) typically read physiological data and attempt to decode motor instructions that drive peripheral devices in order to produce synthetic behavior <xref ref-type="bibr" rid="pcbi.1002546-Schwartz1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Carmena1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Darmanjian1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Chapin1">[36]</xref>. Because we have a physical model of the peripheral effector, the origins of the complexity of the behavior were linked to smooth paths in a low dimensional parameter space. Following the identification of the pertinent parameters and their physiological link to the pattern generator (activity of the ventral syringeal muscle and sub-syringeal air sac pressure), a further simplification of the system was carried out, on dynamical grounds, by eliminating irrelevant nonlinear terms (performing a reduction of the model to its normal form). This led to the possibility of implementing our bio-prosthetic device on a programmable electronic platform. Since the computing capabilities of the platform are greatly enhanced by the low costs of our implementation, technological advances in this front will have great impact on the complexity of the peripheral biomechanical system that can be emulated.</p>
      </sec>
    </sec>
    <sec id="s4">
      <title>Discussion</title>
      <sec id="s4a">
        <title>Model-based bio-prosthesis</title>
        <p>We have built a device that emulates complex motor behavior when driven by a subject by its actual (yet degraded) physiological motor gestures. It successfully reproduces the result of the stereotyped motor gesture that leads to the behavior, <italic>i.e.</italic>, the diverse and complex set of sounds comprising the bird's song bout. The realistic synthetic vocalizations are produced in real-time, by computing a mathematical model of the vocal organ on a portable Digital Signal Processor.</p>
        <p>The relative computational and technological simplicity of the device relies on the current level of understanding of the peripheral biomechanical effector <xref ref-type="bibr" rid="pcbi.1002546-Zeigler1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Mindlin1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Titze1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Fletcher1">[24]</xref>. We have been able to construct a physical model of the syrinx that presents the adequate level of description and converges to a low dimensional (as low as two dimensions) dynamical system. The deterministic model of the vocal organ on which our device is based is not a statistical attempt to capture causal relationships between motor commands and behavior; instead, it is a hierarchization of the interactions within the biomechanical periphery and with the pattern generator. It aims to identify the dynamical mechanisms by which the behavior is produced.</p>
        <p>Furthermore, exploration of the model leads to the finding that much of the diversity and complexity of the behavior can be explained in terms of the dynamical features of this nonlinear system <xref ref-type="bibr" rid="pcbi.1002546-Sitt1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002546-Amador1">[28]</xref>, requiring only simple instructions of the nervous system to produce a rich variety of vocalizations. Just as it is identified that a low dimensional system reproduces the main features of the complexity of the vocal organ, it can also be concluded that the control parameters are few and their behavior is simple (<italic>i.e.</italic>, the physiological motor gestures linked to the paths in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002546.e112" xlink:type="simple"/></inline-formula> space are smooth). In this way, the parameter space of the model not only suggests the pertinent physiological instructions determining the main properties of the output but also how they are expected to behave. Paths in parameter space reconstructed in order for the model to produce vocalizations matching experimental recordings are indeed effective in predicting the physiological motor gestures <xref ref-type="bibr" rid="pcbi.1002546-Perl1">[14]</xref>.</p>
        <p>In addition, knowledge of nonlinear dynamics allows us to find the simplest system with equivalent oscillatory behavior. The reduction of the low dimensional mathematical model for the syrinx to its normal form reduces the computational requirements and makes way for the implementation on a real-time computing solution, such as a DSP.</p>
      </sec>
      <sec id="s4b">
        <title>Conclusion</title>
        <p>Realistic vocal behavior is synthesized online, controlled by the motor gesture of a freely behaving muted bird, which is a physiological signal that is degraded respect to the one recorded in the intact bird. This was achieved by computing in real time a mathematical model describing the mechanisms of sound production in the interface between the motor pattern generator and the behavior, the highly nonlinear vocal organ. The computing platform is a low cost, portable processor. This successful avian vocal prosthesis is an encouraging example of the plausibility of a kind of interface between the central motor pattern generator and the synthetic, bio-mimetic behavior. An advance towards models in which certain complex features of the motor behavior are understood in terms of the underlying nonlinear mechanisms of the peripheral effectors has the potential to enhance solutions of brain-bio-mimetic effector interfaces in many ways.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank María de los Ángeles Suarez for her excellent technical assistance. EMA also thanks I. Vissani for useful comments.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002546-Zeigler1">
        <label>1</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zeigler</surname><given-names>P</given-names></name><name name-style="western"><surname>Marler</surname><given-names>P</given-names></name></person-group>             <year>2004</year>             <source>Neuroscience of birdsong</source>             <publisher-loc>Cambridge, Massachusetts</publisher-loc>             <publisher-name>Cambridge University press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Mindlin1">
        <label>2</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mindlin</surname><given-names>G</given-names></name><name name-style="western"><surname>Laje</surname><given-names>R</given-names></name></person-group>             <year>2005</year>             <source>The Physics of Birdsong</source>             <publisher-name>Springer</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Doupe1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Kuhl</surname><given-names>PK</given-names></name></person-group>             <year>1999</year>             <article-title>Birdsong and human speech: Common themes and mechanisms.</article-title>             <source>Annu Rev Neurosci</source>             <volume>22</volume>             <fpage>567</fpage>             <lpage>631</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Gardner1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gardner</surname><given-names>T</given-names></name><name name-style="western"><surname>Cecchi</surname><given-names>G</given-names></name><name name-style="western"><surname>Magnasco</surname><given-names>M</given-names></name><name name-style="western"><surname>Laje</surname><given-names>R</given-names></name><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name></person-group>             <year>2001</year>             <article-title>Simple motor gestures for birdsongs.</article-title>             <source>Phys Rev Lett</source>             <volume>87</volume>             <fpage>208101</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Sitt1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sitt</surname><given-names>JD</given-names></name><name name-style="western"><surname>Amador</surname><given-names>A</given-names></name><name name-style="western"><surname>Goller</surname><given-names>F</given-names></name><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name></person-group>             <year>2008</year>             <article-title>Dynamical origin of spectrally rich vocalizations in birdsong.</article-title>             <source>Phys Rev E Stat Nonlin Soft Matter Phys</source>             <volume>78</volume>             <fpage>011905</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Zollinger1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zollinger</surname><given-names>SA</given-names></name><name name-style="western"><surname>Suthers</surname><given-names>RA</given-names></name></person-group>             <year>2004</year>             <article-title>Motor mechanisms of a vocal mimic: implications for birdsong production.</article-title>             <source>Proc Biol Sci</source>             <volume>271</volume>             <fpage>483</fpage>             <lpage>491</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Zollinger2">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zollinger</surname><given-names>SA</given-names></name><name name-style="western"><surname>Riede</surname><given-names>T</given-names></name><name name-style="western"><surname>Suthers</surname><given-names>RA</given-names></name></person-group>             <year>2008</year>             <article-title>Two-voice complexity from a single side of the syrinx in northern mockingbird Mimus polyglottos vocalizations.</article-title>             <source>J Exp Biol</source>             <volume>211</volume>             <fpage>1978</fpage>             <lpage>1991</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Arneodo1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Arneodo</surname><given-names>EM</given-names></name><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name></person-group>             <year>2009</year>             <article-title>Source-tract coupling in birdsong production.</article-title>             <source>Phys Rev E</source>             <volume>79</volume>             <fpage>061921</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Arneodo2">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Arneodo</surname><given-names>EM</given-names></name><name name-style="western"><surname>Perl</surname><given-names>YS</given-names></name><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name></person-group>             <year>2011</year>             <article-title>Acoustic signatures of sound source-tract coupling.</article-title>             <source>Phys Rev E</source>             <volume>83</volume>             <fpage>041920</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Ishizaka1">
        <label>10</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ishizaka</surname><given-names>K</given-names></name><name name-style="western"><surname>Flanagan</surname><given-names>J</given-names></name></person-group>             <year>1973</year>             <source>Synthesis of voiced sounds from a two-mass model of the vocal cords</source>             <publisher-name>Dowden Hutchinson and Ross</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Laje1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Laje</surname><given-names>R</given-names></name><name name-style="western"><surname>Gardner</surname><given-names>TJ</given-names></name><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name></person-group>             <year>2002</year>             <article-title>Neuromuscular control of vocalizations in birdsong: A model.</article-title>             <source>Phys Rev E</source>             <volume>65</volume>             <fpage>051921</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Titze1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Titze</surname><given-names>IR</given-names></name></person-group>             <year>1988</year>             <article-title>The physics of small-amplitude oscillation of the vocal folds.</article-title>             <source>J Acoust Soc Am</source>             <volume>83</volume>             <fpage>1536</fpage>             <lpage>1552</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Sitt2">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sitt</surname><given-names>JD</given-names></name><name name-style="western"><surname>Arneodo</surname><given-names>EM</given-names></name><name name-style="western"><surname>Goller</surname><given-names>F</given-names></name><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name></person-group>             <year>2010</year>             <article-title>Physiologically driven avian vocal synthesizer.</article-title>             <source>Phys Rev E</source>             <volume>81</volume>             <fpage>031927</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Perl1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Perl</surname><given-names>YS</given-names></name><name name-style="western"><surname>Arneodo</surname><given-names>EM</given-names></name><name name-style="western"><surname>Amador</surname><given-names>A</given-names></name><name name-style="western"><surname>Goller</surname><given-names>F</given-names></name><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name></person-group>             <year>2011</year>             <article-title>Reconstruction of physiological instructions from zebra finch song.</article-title>             <source>Phys Rev E</source>             <volume>84</volume>             <fpage>051909</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Schwartz1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schwartz</surname><given-names>A</given-names></name><name name-style="western"><surname>Cui</surname><given-names>X</given-names></name><name name-style="western"><surname>Weber</surname><given-names>D</given-names></name><name name-style="western"><surname>Moran</surname><given-names>D</given-names></name></person-group>             <year>2006</year>             <article-title>Brain-controlled interfaces: movement restoration with neural prosthetics.</article-title>             <source>Neuron</source>             <volume>52</volume>             <fpage>205</fpage>             <lpage>220</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Cichocki1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cichocki</surname><given-names>A</given-names></name><name name-style="western"><surname>Washizawa</surname><given-names>Y</given-names></name><name name-style="western"><surname>Rutkowski</surname><given-names>T</given-names></name><name name-style="western"><surname>Bakardjian</surname><given-names>H</given-names></name><name name-style="western"><surname>Phan</surname><given-names>AH</given-names></name><etal/></person-group>             <year>2008</year>             <article-title>Noninvasive bcis: Multiway signal-processing array decompositions.</article-title>             <source>Computer</source>             <volume>41</volume>             <fpage>34</fpage>             <lpage>42</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Carmena1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Carmena</surname><given-names>J</given-names></name><name name-style="western"><surname>Lebedev</surname><given-names>M</given-names></name><name name-style="western"><surname>Crist</surname><given-names>R</given-names></name><name name-style="western"><surname>O'Doherty</surname><given-names>J</given-names></name><name name-style="western"><surname>Santucci</surname><given-names>D</given-names></name><etal/></person-group>             <year>2003</year>             <article-title>Learning to control a brain–machine interface for reaching and grasping by primates.</article-title>             <source>PLoS biol</source>             <volume>1</volume>             <fpage>e42</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Parra1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Parra</surname><given-names>L</given-names></name><name name-style="western"><surname>Spence</surname><given-names>C</given-names></name></person-group>             <year>2000</year>             <article-title>Convolutive blind separation of non-stationary sources.</article-title>             <source>IEEE T Speech Audi P</source>             <volume>8</volume>             <fpage>320</fpage>             <lpage>327</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Larsen1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Larsen</surname><given-names>ON</given-names></name><name name-style="western"><surname>Goller</surname><given-names>F</given-names></name></person-group>             <year>1999</year>             <article-title>Role of syringeal vibrations in bird vocalizations.</article-title>             <source>Proc Biol Sci</source>             <volume>266</volume>             <fpage>1609</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Goller1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Goller</surname><given-names>F</given-names></name><name name-style="western"><surname>Larsen</surname><given-names>ON</given-names></name></person-group>             <year>1997</year>             <article-title>A new mechanism of sound generation in songbirds.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>94</volume>             <fpage>14787</fpage>             <lpage>14791</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Nowicki1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nowicki</surname><given-names>S</given-names></name></person-group>             <year>1987</year>             <article-title>Vocal tract resonances in oscine bird sound production: evidence from birdsongs in a helium atmosphere.</article-title>             <source>Nature</source>             <volume>325</volume>             <fpage>53</fpage>             <lpage>5</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Riede1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Riede</surname><given-names>T</given-names></name><name name-style="western"><surname>Suthers</surname><given-names>RA</given-names></name><name name-style="western"><surname>Fletcher</surname><given-names>NH</given-names></name><name name-style="western"><surname>Blevins</surname><given-names>WE</given-names></name></person-group>             <year>2006</year>             <article-title>Songbirds tune their vocal tract to the fundamental frequency of their song.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>103</volume>             <fpage>5543</fpage>             <lpage>5548</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Assaneo1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Assaneo</surname><given-names>MF</given-names></name><name name-style="western"><surname>Trevisan</surname><given-names>MA</given-names></name></person-group>             <year>2010</year>             <article-title>Computational model for vocal tract dynamics in a suboscine bird.</article-title>             <source>Phys Rev E</source>             <volume>82</volume>             <fpage>031906</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Fletcher1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fletcher</surname><given-names>NH</given-names></name><name name-style="western"><surname>Riede</surname><given-names>T</given-names></name><name name-style="western"><surname>Suthers</surname><given-names>RA</given-names></name></person-group>             <year>2006</year>             <article-title>Model for vocalization by a bird with distensible vocal cavity and open beak.</article-title>             <source>J Acoust Soc Am</source>             <volume>119</volume>             <fpage>1005</fpage>             <lpage>1011</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Fee1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name><name name-style="western"><surname>Shraiman</surname><given-names>B</given-names></name><name name-style="western"><surname>Pesaran</surname><given-names>B</given-names></name><name name-style="western"><surname>Mitra</surname><given-names>PP</given-names></name></person-group>             <year>1998</year>             <article-title>The role of nonlinear dynamics of the syrinx in the vocalizations of a songbird.</article-title>             <source>Nature</source>             <volume>395</volume>             <fpage>67</fpage>             <lpage>71</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Titze2">
        <label>26</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Titze</surname><given-names>I</given-names></name></person-group>             <year>2000</year>             <source>Principles of voice production</source>             <publisher-name>National Center for Voice and Speech</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Coppens1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Coppens</surname><given-names>A</given-names></name><name name-style="western"><surname>Frey</surname><given-names>A</given-names></name><name name-style="western"><surname>Kinsler</surname><given-names>L</given-names></name><name name-style="western"><surname>Sanders</surname><given-names>J</given-names></name></person-group>             <year>1982</year>             <article-title>Fundamentals of acoustics.</article-title>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Amador1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Amador</surname><given-names>A</given-names></name><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name></person-group>             <year>2008</year>             <article-title>Beyond harmonic sounds in a simple model for birdsong production.</article-title>             <source>Chaos</source>             <volume>18</volume>             <fpage>043123</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-GOLLER1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>GOLLER</surname><given-names>F</given-names></name><name name-style="western"><surname>COOPER</surname><given-names>BG</given-names></name></person-group>             <year>2004</year>             <article-title>Peripheral motor dynamics of song production in the zebra finch.</article-title>             <source>Ann N Y Acad Sci</source>             <volume>1016</volume>             <fpage>130</fpage>             <lpage>152</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Strogatz1">
        <label>30</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Strogatz</surname><given-names>SH</given-names></name></person-group>             <year>2000</year>             <source>Nonlinear dynamics and chaos</source>             <publisher-loc>Cambridge, Massachusetts</publisher-loc>             <publisher-name>Perseus Publishing</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Guckenheimer1">
        <label>31</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Guckenheimer</surname><given-names>J</given-names></name><name name-style="western"><surname>Holmes</surname><given-names>P</given-names></name></person-group>             <year>1983</year>             <source>Nonlinear Oscilations, Dynamical Systems, and Bifurcations of Vector Fields</source>             <publisher-loc>New York Berlin Heidelberg Tokyo</publisher-loc>             <publisher-name>Springer-Verlag</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Brainard1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Brainard</surname><given-names>MS</given-names></name><name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name></person-group>             <year>2000</year>             <article-title>Auditory feedback in learning and maintenance of vocal behaviour.</article-title>             <source>Nat Rev Neurosci</source>             <volume>1</volume>             <fpage>31</fpage>             <lpage>40</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Cooper1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cooper</surname><given-names>BG</given-names></name><name name-style="western"><surname>Goller</surname><given-names>F</given-names></name></person-group>             <year>2004</year>             <article-title>Partial muting leads to age-dependent modi_cation of motor patterns underlying crystallized zebra finch song.</article-title>             <source>J Neurobiol</source>             <volume>61</volume>             <fpage>317</fpage>             <lpage>332</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Sakata1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sakata</surname><given-names>J</given-names></name><name name-style="western"><surname>Brainard</surname><given-names>M</given-names></name></person-group>             <year>2006</year>             <article-title>Real-time contributions of auditory feedback to avian vocal motor control.</article-title>             <source>J Neurosci</source>             <volume>26</volume>             <fpage>9619</fpage>             <lpage>9628</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Darmanjian1">
        <label>35</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Darmanjian</surname><given-names>S</given-names></name><name name-style="western"><surname>Cieslewski</surname><given-names>G</given-names></name><name name-style="western"><surname>Morrison</surname><given-names>S</given-names></name><name name-style="western"><surname>Dang</surname><given-names>B</given-names></name><name name-style="western"><surname>Gugel</surname><given-names>K</given-names></name><etal/></person-group>             <year>2006</year>             <article-title>A recon-figurable neural signal processor (nsp) for brain machine interfaces.</article-title>             <fpage>2502</fpage>             <lpage>2505</lpage>             <comment>In: Engineering in Medicine and Biology Society, 2006. EMBS'06. 28th Annual International Conference of the IEEE. IEEE</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002546-Chapin1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chapin</surname><given-names>J</given-names></name><name name-style="western"><surname>Moxon</surname><given-names>K</given-names></name><name name-style="western"><surname>Markowitz</surname><given-names>R</given-names></name><name name-style="western"><surname>Nicolelis</surname><given-names>M</given-names></name><etal/></person-group>             <year>1999</year>             <article-title>Real-time control of a robot arm using simultaneously recorded neurons in the motor cortex.</article-title>             <source>Nat Neurosci</source>             <volume>2</volume>             <fpage>664</fpage>             <lpage>670</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>