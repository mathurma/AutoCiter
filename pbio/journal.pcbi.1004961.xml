<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="editorial" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004961</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-00064</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Editorial</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical data</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical models</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Research assessment</subject><subj-group><subject>Reproducibility</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical methods</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Statistical signal processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Research design</subject><subj-group><subject>Experimental design</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Ten Simple Rules for Effective Statistical Practice</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kass</surname>
<given-names>Robert E.</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Caffo</surname>
<given-names>Brian S.</given-names>
</name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9202-6341</contrib-id>
<name name-style="western">
<surname>Davidian</surname>
<given-names>Marie</given-names>
</name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Meng</surname>
<given-names>Xiao-Li</given-names>
</name>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Yu</surname>
<given-names>Bin</given-names>
</name>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Reid</surname>
<given-names>Nancy</given-names>
</name>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Statistics, Machine Learning Department, and Center for the Neural Basis of Cognition, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Department of Biostatistics, Bloomberg School of Public Health, Johns Hopkins University, Baltimore, Maryland, United States of America</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Department of Statistics, North Carolina State University, Raleigh, North Carolina, United States of America</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Department of Statistics, Harvard University, Cambridge, Massachusetts, United States of America</addr-line></aff>
<aff id="aff005"><label>5</label> <addr-line>Department of Statistics and Department of Electrical Engineering and Computer Science, University of California Berkeley, Berkeley, California, United States of America</addr-line></aff>
<aff id="aff006"><label>6</label> <addr-line>Department of Statistical Sciences, University of Toronto, Toronto, Ontario, Canada</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Lewitter</surname>
<given-names>Fran</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Whitehead Institute, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">reid@utstat.utoronto.ca</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>9</day>
<month>6</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="collection">
<month>6</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>6</issue>
<elocation-id>e1004961</elocation-id>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Kass et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004961"/>
<funding-group>
<funding-statement>BSC's research is partially supported by the National Institutes of Health grant EB012547: <ext-link ext-link-type="uri" xlink:href="http://www.nibib.nih.gov" xlink:type="simple">www.nibib.nih.gov</ext-link>. MD's research is partially supported by the National Institutes of Health grant NIH P01 CA142538: <ext-link ext-link-type="uri" xlink:href="http://www.nih.gov" xlink:type="simple">www.nih.gov</ext-link>. REK's research is partially supported by the National Institute of Mental Health R01 MH064537: <ext-link ext-link-type="uri" xlink:href="http://www.nimh.nih.gov" xlink:type="simple">www.nimh.nih.gov</ext-link>. NR's research is partially supported by the Natural Sciences and Engineering Council of Canada grant RGPIN-2015-06390: <ext-link ext-link-type="uri" xlink:href="http://www.nserc.ca" xlink:type="simple">www.nserc.ca</ext-link>. BY's research is partially supported by the National Science Foundation grant CCF-0939370: <ext-link ext-link-type="uri" xlink:href="http://www.nsf.gov" xlink:type="simple">www.nsf.gov</ext-link>. XLM's research is partially supported by the National Science Foundation (<ext-link ext-link-type="uri" xlink:href="http://www.nsf.gov" xlink:type="simple">www.nsf.gov</ext-link>) DMS 1513492. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="0"/>
<table-count count="0"/>
<page-count count="8"/>
</counts>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Several months ago, Phil Bourne, the initiator and frequent author of the wildly successful and incredibly useful “Ten Simple Rules” series, suggested that some statisticians put together a Ten Simple Rules article related to statistics. (One of the rules for writing a PLOS Ten Simple Rules article is to be Phil Bourne [<xref ref-type="bibr" rid="pcbi.1004961.ref001">1</xref>]. In lieu of that, we hope effusive praise for Phil will suffice.)</p>
<p>Implicit in the guidelines for writing Ten Simple Rules [<xref ref-type="bibr" rid="pcbi.1004961.ref001">1</xref>] is “know your audience.” We developed our list of rules with researchers in mind: researchers having some knowledge of statistics, possibly with one or more statisticians available in their building, or possibly with a healthy do-it-yourself attitude and a handful of statistical packages on their laptops. We drew on our experience in both collaborative research and teaching, and, it must be said, from our frustration at being asked, more than once, to “take a quick look at my student’s thesis/my grant application/my referee’s report: it needs some input on the stats, but it should be pretty straightforward.”</p>
<p>There are some outstanding resources available that explain many of these concepts clearly and in much more detail than we have been able to do here: among our favorites are Cox and Donnelly [<xref ref-type="bibr" rid="pcbi.1004961.ref002">2</xref>], Leek [<xref ref-type="bibr" rid="pcbi.1004961.ref003">3</xref>], Peng [<xref ref-type="bibr" rid="pcbi.1004961.ref004">4</xref>], Kass et al. [<xref ref-type="bibr" rid="pcbi.1004961.ref005">5</xref>], Tukey [<xref ref-type="bibr" rid="pcbi.1004961.ref006">6</xref>], and Yu [<xref ref-type="bibr" rid="pcbi.1004961.ref007">7</xref>].</p>
<p>Every article on statistics requires at least one caveat. Here is ours: we refer in this article to “science” as a convenient shorthand for investigations using data to study questions of interest. This includes social science, engineering, digital humanities, finance, and so on. Statisticians are not shy about reminding administrators that statistical science has an impact on nearly every part of almost all organizations.</p>
</sec>
<sec id="sec002">
<title>Rule 1: Statistical Methods Should Enable Data to Answer Scientific Questions</title>
<p>A big difference between inexperienced users of statistics and expert statisticians appears as soon as they contemplate the uses of some data. While it is obvious that experiments generate data to answer scientific questions, inexperienced users of statistics tend to take for granted the link between data and scientific issues and, as a result, may jump directly to a technique based on data structure rather than scientific goal. For example, if the data were in a table, as for microarray gene expression data, they might look for a method by asking, “Which test should I use?” while a more experienced person would, instead, start with the underlying question, such as, “Where are the differentiated genes?” and, from there, would consider multiple ways the data might provide answers. Perhaps a formal statistical test would be useful, but other approaches might be applied as alternatives, such as heat maps or clustering techniques. Similarly, in neuroimaging, understanding brain activity under various experimental conditions is the main goal; illustrating this with nice images is secondary. This shift in perspective from statistical technique to scientific question may change the way one approaches data collection and analysis. After learning about the questions, statistical experts discuss with their scientific collaborators the ways that data might answer these questions and, thus, what kinds of studies might be most useful. Together, they try to identify potential sources of variability and what hidden realities could break the hypothesized links between data and scientific inferences; only then do they develop analytic goals and strategies. This is a major reason why collaborating with statisticians can be helpful, and also why the collaborative process works best when initiated early in an investigation. See <xref ref-type="sec" rid="sec004">Rule 3</xref>.</p>
</sec>
<sec id="sec003">
<title>Rule 2: Signals Always Come with Noise</title>
<p>Grappling with variability is central to the discipline of statistics. Variability comes in many forms. In some cases variability is good, because we need variability in predictors to explain variability in outcomes. For example, to determine if smoking is associated with lung cancer, we need variability in smoking habits; to find genetic associations with diseases, we need genetic variation. Other times variability may be annoying, such as when we get three different numbers when measuring the same thing three times. This latter variability is usually called “noise,” in the sense that it is either not understood or thought to be irrelevant. Statistical analyses aim to assess the signal provided by the data, the interesting variability, in the presence of noise, or irrelevant variability.</p>
<p>A starting point for many statistical procedures is to introduce a mathematical abstraction: outcomes, such as patients being diagnosed with specific diseases or receiving numerical scores on diagnostic tests, will vary across the set of individuals being studied, and statistical formalism describes such variation using probability distributions. Thus, for example, a data histogram might be replaced, in theory, by a probability distribution, thereby shifting attention from the raw data to the numerical parameters that determine the precise features of the probability distribution, such as its shape, its spread, or the location of its center. Probability distributions are used in statistical models, with the model specifying the way signal and noise get combined in producing the data we observe, or would like to observe. This fundamental step makes statistical inferences possible. Without it, every data value would be considered unique, and we would be left trying to figure out all the detailed processes that might cause an instrument to give different values when measuring the same thing several times. Conceptualizing signal and noise in terms of probability within statistical models has proven to be an extremely effective simplification, allowing us to capture the variability in data in order to express uncertainty about quantities we are trying to understand. The formalism can also help by directing us to look for likely sources of systematic error, known as bias.</p>
<p>Big data makes these issues more important, not less. For example, Google Flu Trends debuted to great excitement in 2008, but turned out to overestimate the prevalence of influenza by nearly 50%, largely due to bias caused by the way the data were collected; see Harford [<xref ref-type="bibr" rid="pcbi.1004961.ref008">8</xref>], for example.</p>
</sec>
<sec id="sec004">
<title>Rule 3: Plan Ahead, Really Ahead</title>
<p>When substantial effort will be involved in collecting data, statistical issues may not be captured in an isolated statistical question such as, “What should my <italic>n</italic> be?” As we suggested in Rule 1, rather than focusing on a specific detail in the design of the experiment, someone with a lot of statistical experience is likely to step back and consider many aspects of data collection in the context of overall goals and may start by asking, “What would be the ideal outcome of your experiment, and how would you interpret it?” In trying to determine whether observations of X and Y tend to vary together, as opposed to independently, key issues would involve the way X and Y are measured, the extent to which the measurements represent the underlying conceptual meanings of X and Y, the many factors that could affect the measurements, the ability to control those factors, and whether some of those factors might introduce systematic errors (bias).</p>
<p>In Rule 2 we pointed out that statistical models help link data to goals by shifting attention to theoretical quantities of interest. For example, in making electrophysiological measurements from a pair of neurons, a neurobiologist may take for granted a particular measurement methodology along with the supposition that these two neurons will represent a whole class of similar neurons under similar experimental conditions. On the other hand, a statistician will immediately wonder how the specific measurements get at the issue of co-variation; what the major influences on the measurements are, and whether some of them can be eliminated by clever experimental design; what causes variation among repeated measurements, and how quantitative knowledge about sources of variation might influence data collection; and whether these neurons may be considered to be sampled from a well-defined population, and how the process of picking that pair could influence subsequent statistical analyses. A conversation that covers such basic issues may reveal possibilities an experimenter has not yet considered.</p>
<p>Asking questions at the design stage can save headaches at the analysis stage: careful data collection can greatly simplify analysis and make it more rigorous. Or, as Sir Ronald Fisher put it: “To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of” [<xref ref-type="bibr" rid="pcbi.1004961.ref009">9</xref>]. As a good starting point for reading on planning of investigations, see Chapters 1 through 4 of [<xref ref-type="bibr" rid="pcbi.1004961.ref002">2</xref>].</p>
</sec>
<sec id="sec005">
<title>Rule 4: Worry about Data Quality</title>
<p>Well-trained experimenters understand instinctively that, when it comes to data analysis, “garbage in produces garbage out.” However, the complexity of modern data collection requires many assumptions about the function of technology, often including data pre-processing technology. It is highly advisable to approach pre-processing with care, as it can have profound effects that easily go unnoticed.</p>
<p>Even with pre-processed data, further considerable effort may be needed prior to analysis; this is variously called “data cleaning,” “data munging,” or “data carpentry.” Hands-on experience can be extremely useful, as data cleaning often reveals important concerns about data quality, in the best case confirming that what was measured is indeed what was intended to be measured and, in the worst case, ensuring that losses are cut early.</p>
<p>Units of measurement should be understood and recorded consistently. It is important that missing data values can be recognized as such by relevant software. For example, 999 may signify the number 999, or it could be code for “we have no clue.” There should be a defensible rule for handling situations such as “non-detects,” and data should be scanned for anomalies such as variable 27 having half its values equal to 0.00027. Try to understand as much as you can how these data arrived at your desk or disk. Why are some data missing or incomplete? Did they get lost through some substantively relevant mechanism? Understanding such mechanisms can help to avoid some seriously misleading results. For example, in a developmental imaging study of attention deficit hyperactivity disorder, might some data have been lost from children with the most severe hyperactivity because they could not sit still in the MR scanner?</p>
<p>Once the data have been wrestled into a convenient format, have a look! Tinkering around with the data, also known as exploratory data analysis, is often the most informative part of the analysis. Exploratory plots can reveal data quality issues and outliers. Simple summaries, such as means, standard deviations, and quantiles, can help refine thinking and offer face validity checks for hypotheses. Many studies, especially when going in completely new scientific directions, are exploratory by design; the area may be too novel to include clear a priori hypotheses. Working with the data informally can help generate new hypotheses and ideas. However, it is also important to acknowledge the specific ways data are selected prior to formal analyses and to consider how such selection might affect conclusions. And it is important to remember that using a single set of data to both generate and test hypotheses is problematic. See <xref ref-type="sec" rid="sec010">Rule 9</xref>.</p>
</sec>
<sec id="sec006">
<title>Rule 5: Statistical Analysis Is More Than a Set of Computations</title>
<p>Statistical software provides tools to assist analyses, not define them. The scientific context is critical, and the key to principled statistical analysis is to bring analytic methods into close correspondence with scientific questions. See <xref ref-type="sec" rid="sec002">Rule 1</xref>. While it can be helpful to include references to a specific algorithm or piece of software in the Methods section of a paper, this should not be a substitute for an explanation of the choice of statistical method in answering a question. A reader will likely want to consider the fundamental issue of whether the analytic technique is appropriately linked to the substantive questions being answered. Don’t make the reader puzzle over this: spell it out clearly.</p>
<p>At the same time, a structured algorithmic approach to the steps in your analysis can be very helpful in making this analysis reproducible by yourself at a later time, or by others with the same or similar data. See <xref ref-type="sec" rid="sec011">Rule 10</xref>.</p>
</sec>
<sec id="sec007">
<title>Rule 6: Keep it Simple</title>
<p>All else being equal, simplicity trumps complexity. This rule has been rediscovered and enshrined in operating procedures across many domains and variously described as “Occam’s razor,” “KISS,” “less is more,” and “simplicity is the ultimate sophistication.” The principle of parsimony can be a trusted guide: start with simple approaches and only add complexity as needed, and then only add as little as seems essential.</p>
<p>Having said this, scientific data have detailed structure, and simple models can’t always accommodate important intricacies. The common assumption of independence is often incorrect and nearly always needs careful examination. See <xref ref-type="sec" rid="sec009">Rule 8</xref>. Large numbers of measurements, interactions among explanatory variables, nonlinear mechanisms of action, missing data, confounding, sampling biases, and so on, can all require an increase in model complexity.</p>
<p>Keep in mind that good design, implemented well, can often allow simple methods of analysis to produce strong results. See <xref ref-type="sec" rid="sec004">Rule 3</xref>. Simple models help us to create order out of complex phenomena, and simple models are well suited for communication to our colleagues and the wider world.</p>
</sec>
<sec id="sec008">
<title>Rule 7: Provide Assessments of Variability</title>
<p>Nearly all biological measurements, when repeated, exhibit substantial variation, and this creates uncertainty in the result of every calculation based on the data. A basic purpose of statistical analysis is to help assess uncertainty, often in the form of a standard error or confidence interval, and one of the great successes of statistical modeling and inference is that it can provide estimates of standard errors from the same data that produce estimates of the quantity of interest. When reporting results, it is essential to supply some notion of statistical uncertainty. A common mistake is to calculate standard errors without taking into account the dependencies among data or variables, which usually means a substantial underestimate of the real uncertainty. See <xref ref-type="sec" rid="sec009">Rule 8</xref>.</p>
<p>Remember that every number obtained from the data by some computation would change somewhat, even if the measurements were repeated on the same biological material. If you are using new material, you can add to the measurement variability an increase due to the natural variability among samples. If you are collecting data on a different day, in a different lab, or under a slightly changed protocol, there are now three more potential sources of variability to be accounted for. In microarray analysis, batch effects are well known to introduce extra variability, and several methods are available to filter these. Extra variability means extra uncertainty in the conclusions, and this uncertainty needs to be reported. Such reporting is invaluable for planning the next investigation.</p>
<p>It is a very common feature of big data that uncertainty assessments tend to be overly optimistic (Cox [<xref ref-type="bibr" rid="pcbi.1004961.ref010">10</xref>], Meng [<xref ref-type="bibr" rid="pcbi.1004961.ref011">11</xref>]). For an instructive, and beguilingly simple, quantitative analysis most relevant to surveys, see the “data defect” section of [<xref ref-type="bibr" rid="pcbi.1004961.ref011">11</xref>]. Big data is not always as big as it looks: a large number of measurements on a small number of samples requires very careful estimation of the standard error, not least because these measurements are quite likely to be dependent.</p>
</sec>
<sec id="sec009">
<title>Rule 8: Check Your Assumptions</title>
<p>Every statistical inference involves assumptions, which are based on substantive knowledge and some probabilistic representation of data variation—this is what we call a statistical model. Even the so-called “model-free” techniques require assumptions, albeit less restrictive assumptions, so this terminology is somewhat misleading.</p>
<p>The most common statistical methods involve an assumption of linear relationships. For example, the ordinary correlation coefficient, also called the Pearson correlation, is a measure of linear association. Linearity often works well as a first approximation or as a depiction of a general trend, especially when the amount of noise in the data makes it difficult to distinguish between linear and nonlinear relationships. However, for any given set of data, the appropriateness of the linear model is an empirical issue and should be investigated.</p>
<p>In many ways, a more worrisome, and very common, assumption in statistical analysis is that multiple observations in the data are statistically independent. This is worrisome because relatively small deviations from this assumption can have drastic effects. When measurements are made across time, for example, the temporal sequencing may be important; if it is, specialized methods appropriate for time series need to be considered.</p>
<p>In addition to nonlinearity and statistical dependence, missing data, systematic biases in measurements, and a variety of other factors can cause violations of statistical modeling assumptions, even in the best experiments. Widely available statistical software makes it easy to perform analyses without careful attention to inherent assumptions, and this risks inaccurate, or even misleading, results. It is therefore important to understand the assumptions embodied in the methods you are using and to do whatever you can to understand and assess those assumptions. At a minimum, you will want to check how well your statistical model fits the data. Visual displays and plots of data and residuals from fitting are helpful for evaluating the relevance of assumptions and the fit of the model, and some basic techniques for assessing model fit are available in most statistical software. Remember, though, that several models can “pass the fit test” on the same data. See <xref ref-type="sec" rid="sec002">Rule 1</xref> and <xref ref-type="sec" rid="sec007">Rule 6</xref>.</p>
</sec>
<sec id="sec010">
<title>Rule 9: When Possible, Replicate!</title>
<p>Every good analyst examines the data at great length, looking for patterns of many types and searching for predicted and unpredicted results. This process often involves dozens of procedures, including many alternative visualizations and a host of numerical slices through the data. Eventually, some particular features of the data are deemed interesting and important, and these are often the results reported in the resulting publication.</p>
<p>When statistical inferences, such as <italic>p</italic>-values, follow extensive looks at the data, they no longer have their usual interpretation. Ignoring this reality is dishonest: it is like painting a bull’s eye around the landing spot of your arrow. This is known in some circles as <italic>p</italic>-hacking, and much has been written about its perils and pitfalls: see, for example, [<xref ref-type="bibr" rid="pcbi.1004961.ref012">12</xref>] and [<xref ref-type="bibr" rid="pcbi.1004961.ref013">13</xref>].</p>
<p>Recently there has been a great deal of criticism of the use of <italic>p</italic>-values in science, largely related to the misperception that results can’t be worthy of publication unless “<italic>p</italic> is less than 0.05.” The recent statement from the American Statistical Association (ASA) [<xref ref-type="bibr" rid="pcbi.1004961.ref014">14</xref>] presents a detailed view of the merits and limitations of the <italic>p</italic>-value.</p>
<p>Statisticians tend to be aware of the most obvious kinds of data snooping, such as choosing particular variables for a reported analysis, and there are methods that can help adjust results in these cases; the False Discovery Rate method of Benjamini and Hochberg [<xref ref-type="bibr" rid="pcbi.1004961.ref015">15</xref>] is the basis for several of these.</p>
<p>For some analyses, there may be a case that some kinds of preliminary data manipulation are likely to be innocuous. In other situations, analysts may build into their work an informal check by trusting only extremely small <italic>p</italic>-values. For example, in high energy physics, the requirement of a “5-sigma” result is at least partly an approximate correction for what is called the “look-elsewhere effect.”</p>
<p>The only truly reliable solution to the problem posed by data snooping is to record the statistical inference procedures that produced the key results, together with the features of the data to which they were applied, and then to replicate the same analysis using new data. Independent replications of this type often go a step further by introducing modifications to the experimental protocol, so that the replication will also provide some degree of robustness to experimental details.</p>
<p>Ideally, replication is performed by an independent investigator. The scientific results that stand the test of time are those that get confirmed across a variety of different, but closely related, situations. In the absence of experimental replications, appropriate forms of data perturbation can be helpful (Yu [<xref ref-type="bibr" rid="pcbi.1004961.ref016">16</xref>]). In many contexts, complete replication is very difficult or impossible, as in large-scale experiments such as multi-center clinical trials. In such cases, a minimum standard would be to follow Rule 10.</p>
</sec>
<sec id="sec011">
<title>Rule 10: Make Your Analysis Reproducible</title>
<p>In our current framework for publication of scientific results, the independent replication discussed in Rule 9 is not practical for most investigators. A different standard, which is easier to achieve, is reproducibility: given the same set of data, together with a complete description of the analysis, it should be possible to reproduce the tables, figures, and statistical inferences. However, even this lower standard can face multiple barriers, such as different computing architectures, software versions, and settings.</p>
<p>One can dramatically improve the ability to reproduce findings by being very systematic about the steps in the analysis (see <xref ref-type="sec" rid="sec006">Rule 5</xref>), by sharing the data and code used to produce the results, and by following Goodman et al. [<xref ref-type="bibr" rid="pcbi.1004961.ref017">17</xref>]. Modern reproducible research tools like Sweave [<xref ref-type="bibr" rid="pcbi.1004961.ref018">18</xref>], knitr [<xref ref-type="bibr" rid="pcbi.1004961.ref019">19</xref>], and iPython [<xref ref-type="bibr" rid="pcbi.1004961.ref020">20</xref>] notebooks take this a step further and combine the research report with the code. Reproducible research is itself an ongoing area of research and a very important area that we all need to pay attention to.</p>
</sec>
<sec id="sec012" sec-type="conclusions">
<title>Conclusion</title>
<p>Mark Twain popularized the saying, “There are three kinds of lies: lies, damned lies, and statistics.” It is true that data are frequently used selectively to give arguments a false sense of support. Knowingly misusing data or concealing important information about the way data and data summaries have been obtained is, of course, highly unethical. More insidious, however, are the widespread instances of claims made about scientific hypotheses based on well-intentioned yet faulty statistical reasoning. One of our chief aims here has been to emphasize succinctly many of the origins of such problems and ways to avoid the pitfalls.</p>
<p>A central and common task for us as research investigators is to decipher what our data are able to say about the problems we are trying to solve. Statistics is a language constructed to assist this process, with probability as its grammar. While rudimentary conversations are possible without good command of the language (and are conducted routinely), principled statistical analysis is critical in grappling with many subtle phenomena to ensure that nothing serious will be lost in translation and to increase the likelihood that your research findings will stand the test of time. To achieve full fluency in this mathematically sophisticated language requires years of training and practice, but we hope the Ten Simple Rules laid out here will provide some essential guidelines.</p>
<p>Among the many articles reporting on the ASA’s statement on <italic>p-</italic>values, we particularly liked a quote from biostatistician Andrew Vickers in [<xref ref-type="bibr" rid="pcbi.1004961.ref021">21</xref>]: “Treat statistics as a science, not a recipe.” This is a great candidate for Rule 0.</p>
</sec>
</body>
<back>
<ack>
<p>We consulted many colleagues informally about this article, but the opinions expressed here are unique to our small committee of authors. We’d like to give a shout out to <ext-link ext-link-type="uri" xlink:href="http://www.xkcd.com" xlink:type="simple">xkcd.com</ext-link> for conveying statistical ideas with humor, to the <ext-link ext-link-type="uri" xlink:href="http://simplystatistics.org" xlink:type="simple">Simply Statistics</ext-link> blog as a reliable source for thoughtful commentary, to <ext-link ext-link-type="uri" xlink:href="http://fivethirtyeight.com" xlink:type="simple">FiveThirtyEight</ext-link> for bringing statistics to the world (or at least to the media), to Phil Bourne for suggesting that we put together this article, and to Steve Pierson of the American Statistical Association for getting the effort started.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004961.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dashnow</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lonsdale</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bourne</surname> <given-names>PE</given-names></name> (<year>2014</year>) <article-title>Ten simple rules for writing a PLOS ten simple rules article</article-title>. <source>PLoS Comput Biol</source> <volume>10</volume>(<issue>10</issue>): <fpage>e1003858</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003858" xlink:type="simple">10.1371/journal.pcbi.1003858</ext-link></comment> <object-id pub-id-type="pmid">25340653</object-id></mixed-citation></ref>
<ref id="pcbi.1004961.ref002"><label>2</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Cox</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Donnelly</surname> <given-names>CA</given-names></name> (<year>2011</year>) <source>Principles of Applied Statistics</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref003"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Leek JT (2015) The Elements of Data Analytic Style. Leanpub, <ext-link ext-link-type="uri" xlink:href="https://leanpub.com/artofdatascience" xlink:type="simple">https://leanpub.com/artofdatascience</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref004"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Peng R (2014) The Art of Data Science. Leanpub, <ext-link ext-link-type="uri" xlink:href="https://leanpub.com/artofdatascience" xlink:type="simple">https://leanpub.com/artofdatascience</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref005"><label>5</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kass</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Eden</surname> <given-names>UT</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name> (<year>2014</year>) <source>Analysis of Neural Data</source>. <publisher-name>Springer</publisher-name>: <publisher-loc>New York</publisher-loc>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tukey</surname> <given-names>JW</given-names></name> (<year>1962</year>) <article-title>The future of data analysis</article-title>. <source>Ann Math Stat</source> <volume>33</volume>: <fpage>1</fpage>–<lpage>67</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname> <given-names>B</given-names></name> (<year>2013</year>) <article-title>Stability</article-title>. <source>Bernoulli</source>, <volume>19</volume>(<issue>4</issue>): <fpage>1484</fpage>–<lpage>1500</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harford</surname> <given-names>T</given-names></name> (<year>2015</year>) <article-title>Big Data: are we making a big mistake?</article-title> <source>Significance</source> <volume>11</volume>: <fpage>14</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fisher</surname> <given-names>RA</given-names></name> (<year>1938</year>) <article-title>Presidential address</article-title>. <source>Sankhyā</source> <volume>4</volume>: <fpage>14</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cox</surname> <given-names>DR</given-names></name> (<year>2015</year>) <article-title>Big data and precision</article-title>. <source>Biometrika</source> <volume>102</volume>: <fpage>712</fpage>–<lpage>716</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref011"><label>11</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Meng</surname> <given-names>XL</given-names></name> (<year>2014</year>) <chapter-title>A trio of inference problems that could win you a Nobel prize in statistics (if you help fund it)</chapter-title>. In: <name name-style="western"><surname>Lin</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Genest</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Molenberghs</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Scott</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>J-L</given-names></name>,editors. <source>Past, Present, and Future of Statistical Science</source>, <publisher-loc>Boca Raton</publisher-loc>: <publisher-name>CRC Press</publisher-name>. pp. <fpage>537</fpage>–<lpage>562</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Loken</surname> <given-names>E</given-names></name> (<year>2014</year>) <article-title>The statistical crisis in science</article-title>. <source>Am Sci</source> <volume>102</volume>: <fpage>460</fpage>–<lpage>465</lpage></mixed-citation></ref>
<ref id="pcbi.1004961.ref013"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Aschwanden C (2015) Science isn’t broken. August 11 2015 <ext-link ext-link-type="uri" xlink:href="http://fivethirtyeight.com/features/science-isnt-broken/" xlink:type="simple">http://fivethirtyeight.com/features/science-isnt-broken/</ext-link></mixed-citation></ref>
<ref id="pcbi.1004961.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wasserstein</surname> <given-names>RL</given-names></name>, <name name-style="western"><surname>Lazar</surname> <given-names>NA</given-names></name> (<year>2016</year>) <article-title>The ASA's statement on p-values: context, process, and purpose</article-title>, <source>The American Statistician</source> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/00031305.2016.1154108" xlink:type="simple">10.1080/00031305.2016.1154108</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1004961.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benjamini</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hochberg</surname> <given-names>Y</given-names></name> (<year>1995</year>) <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source>J R Statist Soc B</source> <volume>57</volume>: <fpage>289</fpage>–<lpage>300</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref016"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Yu, B (2015) Data wisdom for data science. April 13 2015 <ext-link ext-link-type="uri" xlink:href="http://www.odbms.org/2015/04/data-wisdom-for-data-science/" xlink:type="simple">http://www.odbms.org/2015/04/data-wisdom-for-data-science/</ext-link></mixed-citation></ref>
<ref id="pcbi.1004961.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goodman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pepe</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Blocker</surname> <given-names>AW</given-names></name>, <name name-style="western"><surname>Borgman</surname> <given-names>CL</given-names></name>, <name name-style="western"><surname>Cranmer</surname> <given-names>K</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>Ten simple rules for the care and feeding of scientific data</article-title>. <source>PLoS Comput Biol</source> <volume>10</volume>(<issue>4</issue>): <fpage>e1003542</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003858" xlink:type="simple">10.1371/journal.pcbi.1003858</ext-link></comment> <object-id pub-id-type="pmid">24763340</object-id></mixed-citation></ref>
<ref id="pcbi.1004961.ref018"><label>18</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Leisch</surname> <given-names>F</given-names></name> (<year>2002</year>) <chapter-title>Sweave: Dynamic generation of statistical reports using data analysis</chapter-title>. In <name name-style="western"><surname>Härdle</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Rönz</surname> <given-names>H</given-names></name>, editors. <source>Compstat: Proceedings in Computational Statistics</source>, <publisher-loc>Heidelberg</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>, pp. <fpage>575</fpage>–<lpage>580</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref019"><label>19</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Xie</surname> <given-names>Y</given-names></name> (<year>2014</year>) <source>Dynamic Documents with R and knitr</source>. <publisher-loc>Boca Raton</publisher-loc>: <publisher-name>CRC Press</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pérez</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Granger</surname> <given-names>BE</given-names></name> (<year>2007</year>) <article-title>IPython: A system for interactive scientific computing</article-title>. <source>Comput Sci Eng</source> <volume>9</volume> (<issue>3</issue>), <fpage>21</fpage>–<lpage>29</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004961.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baker</surname> <given-names>M</given-names></name> (<year>2016</year>) <article-title>Statisticians issue warning over misuse of <italic>P</italic> values</article-title>. <source>Nature</source> <volume>531</volume>, (<issue>151</issue>) <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature.2016.19503" xlink:type="simple">10.1038/nature.2016.19503</ext-link></comment></mixed-citation></ref>
</ref-list>
</back>
</article>