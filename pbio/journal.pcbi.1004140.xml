<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004140</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-00843</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Education</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Speeding Up Ecological and Evolutionary Computations in R; Essentials of High Performance Computing for Biologists</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Visser</surname>
<given-names>Marco D.</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
<xref rid="aff002" ref-type="aff"><sup>2</sup></xref>
<xref rid="cor001" ref-type="corresp">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>McMahon</surname>
<given-names>Sean M.</given-names>
</name>
<xref rid="aff003" ref-type="aff"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Merow</surname>
<given-names>Cory</given-names>
</name>
<xref rid="aff003" ref-type="aff"><sup>3</sup></xref>
<xref rid="aff004" ref-type="aff"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Dixon</surname>
<given-names>Philip M.</given-names>
</name>
<xref rid="aff005" ref-type="aff"><sup>5</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Record</surname>
<given-names>Sydne</given-names>
</name>
<xref rid="aff006" ref-type="aff"><sup>6</sup></xref>
<xref rid="aff007" ref-type="aff"><sup>7</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Jongejans</surname>
<given-names>Eelke</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Departments of Experimental Plant Ecology and Animal Ecology &amp; Ecophysiology, Radboud University Nijmegen, Nijmegen, The Netherlands</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Program for Applied Ecology, Centre for Tropical Forest Science, Smithsonian Tropical Research Institute, Balboa, Ancón, Panamá, Republic of Panamá</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Smithsonian Environmental Research Center, Edgewater, Maryland, United States of America</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Department of Ecology and Evolutionary Biology, University of Connecticut, Storrs, Connecticut, United States of America</addr-line></aff>
<aff id="aff005"><label>5</label> <addr-line>Department of Statistics, Iowa State University, Ames, Iowa, United States of America</addr-line></aff>
<aff id="aff006"><label>6</label> <addr-line>Harvard University, Harvard Forest, Petersham, Massachusetts, United States of America</addr-line></aff>
<aff id="aff007"><label>7</label> <addr-line>Bryn Mawr College, Bryn Mawr, Pennsylvania, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Ouellette</surname>
<given-names>Francis</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Ontario Institute for Cancer Research, CANADA</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">m.visser@science.ru.nl</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>26</day>
<month>3</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<month>3</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>3</issue>
<elocation-id>e1004140</elocation-id>
<permissions>
<license xlink:href="https://creativecommons.org/publicdomain/zero/1.0/" xlink:type="simple">
<license-p>This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/" xlink:type="simple">Creative Commons CC0</ext-link> public domain dedication</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004140" xlink:type="simple"/>
<abstract>
<p>Computation has become a critical component of research in biology. A risk has emerged that computational and programming challenges may limit research scope, depth, and quality. We review various solutions to common computational efficiency problems in ecological and evolutionary research. Our review pulls together material that is currently scattered across many sources and emphasizes those techniques that are especially effective for typical ecological and environmental problems. We demonstrate how straightforward it can be to write efficient code and implement techniques such as profiling or parallel computing. We supply a newly developed R package (<italic>aprof</italic>) that helps to identify computational bottlenecks in R code and determine whether optimization can be effective. Our review is complemented by a practical set of examples and detailed Supporting Information material (<xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1</xref>–<xref rid="pcbi.1004140.s003" ref-type="supplementary-material">S3</xref> Texts) that demonstrate large improvements in computational speed (ranging from 10.5 times to 14,000 times faster). By improving computational efficiency, biologists can feasibly solve more complex tasks, ask more ambitious questions, and include more sophisticated analyses in their research.</p>
</abstract>
<funding-group>
<funding-statement>We thank the Evolutionary Biodemography Laboratory and the Modelling the Evolution of Ageing Independent Group of the Max Planck Society for Demographic Research (<ext-link ext-link-type="uri" xlink:href="http://www.mpg.de" xlink:type="simple">http://www.mpg.de</ext-link>) in Rostock (Germany) and Odense (Denmark)) for supporting the working group where this paper was initiated. This study was supported by the Netherlands Foundation for Scientific Research (<ext-link ext-link-type="uri" xlink:href="http://www.nwo.nl" xlink:type="simple">www.nwo.nl</ext-link>, NWO-ALW 801-01-009 to MDV &amp; EJ; NWO-ALW 840.11.001 to EJ), the Smithsonian Tropical Research Institute (<ext-link ext-link-type="uri" xlink:href="http://www.stri.si.edu" xlink:type="simple">www.stri.si.edu</ext-link>, MDV) and the USA National Science Foundation (<ext-link ext-link-type="uri" xlink:href="http://www.nsf.gov" xlink:type="simple">www.nsf.gov</ext-link> NSF 640261 to SMM). The funders had no role in the preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="3"/>
<table-count count="0"/>
<page-count count="11"/>
</counts>
</article-meta>
</front>
<body>
<disp-quote>
<p><italic>This is part of the PLOS Computational Biology Education collection.</italic></p>
</disp-quote>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Emerging fields such as ecoinformatics and computational ecology [<xref rid="pcbi.1004140.ref001" ref-type="bibr">1</xref>,<xref rid="pcbi.1004140.ref002" ref-type="bibr">2</xref>] bear witness to the fact that biology is becoming more quantitative and interdisciplinary. Such research often requires intensive computing, which may be limited by inefficient code that confines the size of a simulation model or restricts the scope of data analysis. It is therefore increasingly necessary for biologists to become versed in efficient programming [<xref rid="pcbi.1004140.ref003" ref-type="bibr">3</xref>], as well as in mathematics and statistics [<xref rid="pcbi.1004140.ref004" ref-type="bibr">4</xref>].</p>
<p>Computer scientists have developed many optimization methods (e.g., [<xref rid="pcbi.1004140.ref005" ref-type="bibr">5</xref>]), however, the efficient translation of mathematical models to computer code has received very little attention in biology [<xref rid="pcbi.1004140.ref002" ref-type="bibr">2</xref>]. Here we present an overview of techniques to improve computational efficiency in a wide variety of settings. Much of the information we present is currently scattered throughout various textbooks, articles, or online sources, and our goal here is to provide a convenient summary for biologists interested in improving the efficiency of their computational methods. In short, we 1) highlight the processes that slow down computation; 2) introduce techniques, which, via an R package, help to decide whether and where optimization is needed; 3) give a step-by-step guide to implementing various basic, but powerful techniques for optimization; and 4) demonstrate the speed gains that can be achieved. We supplement this with more background information and detailed examples in the Supporting Information (<xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1</xref>–<xref rid="pcbi.1004140.s003" ref-type="supplementary-material">S3</xref> Texts). The widespread adoption in the biological sciences of the R programming language has motivated a focus on techniques that are directly applicable to R—although many principles hold for other platforms.</p>
</sec>
<sec id="sec002">
<title>Focus</title>
<p>Many analyses in biology are computationally demanding. Examples include large matrix operations [<xref rid="pcbi.1004140.ref006" ref-type="bibr">6</xref>], optimizing likelihood functions with complex functional forms [<xref rid="pcbi.1004140.ref007" ref-type="bibr">7</xref>], many applications of bootstrapping or other randomization-based inference, network analysis [<xref rid="pcbi.1004140.ref008" ref-type="bibr">8</xref>], and Markov Chain Monte Carlo fits of hierarchical Bayesian models (e.g., [<xref rid="pcbi.1004140.ref009" ref-type="bibr">9</xref>]). Here, we focus on common issues with large databases and stochastic simulation models, applying general approaches for optimizing code to two simple examples:
<list list-type="order">
<list-item><p>Bootstrapping mean values 10,000 times in a moderately large dataset of 750 million records. This example is highly suited for parallel computation and employs common data protocols: indexing and grouping, resampling and calculating means, and formatting and saving output (<xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1A–C</xref>).</p></list-item>
<list-item><p>A simple stochastic two-species Lotka-Volterra competition model, which utilizes basic mathematical operations, randomly sampling statistical distributions, and saving fairly large simulation results. Additionally, as change depends on the state of the population in a previous time step (a Markov process), a single run cannot be conducted in parallel (<xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1D–F</xref>).</p></list-item>
</list>
The optimization of these examples can be followed in detail in <xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>. In all cases, we obtained speed-ups of 10.5 to 14,000 times with benefits that increase with the amount of computation (<xref rid="pcbi.1004140.g002" ref-type="fig">Fig. 2</xref>). Finally we show the relevance of these techniques when applied to two previously published problems concerning spatial models [<xref rid="pcbi.1004140.ref010" ref-type="bibr">10</xref>] and the analysis of fitness landscapes [<xref rid="pcbi.1004140.ref011" ref-type="bibr">11</xref>] (documented in <xref rid="pcbi.1004140.s002" ref-type="supplementary-material">S2</xref> and <xref rid="pcbi.1004140.s003" ref-type="supplementary-material">S3</xref> Texts).</p>
<fig id="pcbi.1004140.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004140.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Visualization of profiling output using the <italic>aprof</italic> package for R code, where the amount of time spent in each line of code is indicated by the blue bars.</title>
<p>In A, B and C, a bootstrap algorithm is shown, and in D, E, and F, a stochastic Lotka-Volterra competition model is shown. The consecutive optimizations described in the text are indicated with the red lines in B, C, E, and F indicating the altered pieces of code. (A) An inefficiently coded bootstrap algorithm, with most time spent in lines 7–8. This algorithm shuffles the values of a large matrix (750,000 x 1000) stored in object "d", and then calculates columnwise the difference between the mean column values and the overall mean. (B) A slightly improved code where the overall mean calculation is stored in object "avg." (C) A further improved version of the code where column means are calculated by a specialized and vectorized function (<italic>colMeans</italic>). (D) A slow running stochastic Lotka-Volterra model of species coexistence that runs a simulation over T years where species have normally distributed intrinsic growth rates (r ∼ Norm(rm,rs)) and competition coefficients (a ∼ Norm(am,as)). (E) the Lotka-Volterra model is more efficient when the pre-allocation-and-fill method is applied. (F) Switching to a matrix to store results further decreases run time. A detailed description of each optimization step with profiling analysis is given online (<xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>, sections 2 and 6).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004140.g001" position="float" xlink:type="simple"/>
</fig>
<fig id="pcbi.1004140.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004140.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Execution time in minutes, required to complete various computational problems, using the optimization techniques discussed.</title>
<p>Panels A and B show the execution time as a function of problem size for 10,000 bootstrap resamples conducted on datasets varying in size (A) and time required to run a stochastic population model against the number of time steps (B). "Naive" R code, in which no optimizations are applied, uses most computing resources (solid lines in A and B). Optimized R code, with use of efficient functions and optimal data structures pre-allocated in memory (dashed lines in A and B), is faster. In both panels A and B, the largest speed-ups are obtained by using optimal R code (black lines). Subsequent use of parallelism causes further improvement (dot-dashed green line) in A. In panel B, using R's byte compiler improved execution time further above optimal R code (dotted lines in green) while the smallest execution times were achieved by refactoring code in C (red dot-dashed lines). Panels C and D give the computing time (in minutes) needed to conduct the calculations from (C) Merow et al. [<xref rid="pcbi.1004140.ref010" ref-type="bibr">10</xref>] and (D) the calculations represented by Fig. 3 in Visser et al. [<xref rid="pcbi.1004140.ref011" ref-type="bibr">11</xref>]. Bars in panel C represent the original unaltered code from [<xref rid="pcbi.1004140.ref010" ref-type="bibr">10</xref>] (I), the unaltered code run in parallel (II), the revised R code where we replaced a single data.frame with a matrix (III) and the revised code run in parallel (IV). Bars in panel (D) represent the original unaltered code [<xref rid="pcbi.1004140.ref011" ref-type="bibr">11</xref>] (I), original run in parallel (II), optimized R code (III), optimized R code using R's byte compiler (IV), optimized R code run in parallel (V), optimized R code using byte compiler run parallel (VI), code with key components refactored in C (VII), and parallel execution of refactored code (VIII). All parallel computations were run on 4 cores, and code is provided in <xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>, section 3, <xref rid="pcbi.1004140.s002" ref-type="supplementary-material">S2</xref> and <xref rid="pcbi.1004140.s003" ref-type="supplementary-material">S3</xref> Texts.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004140.g002" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec003">
<title>When (Not) to Optimize?</title>
<p>One should consider optimization only after the code works correctly and produces trustworthy results [<xref rid="pcbi.1004140.ref012" ref-type="bibr">12</xref>]. Correct code should be the primary goal in any analysis. Before optimizing, it is important to recall a fact that is recognized by programmers: “Everyone knows that debugging is twice as hard as writing a program in the first place. So if you're as clever as you can be when you write it, how will you ever debug it?” [<xref rid="pcbi.1004140.ref013" ref-type="bibr">13</xref>]. Optimized code may be faster but tends to lose robustness and generality, be more complex and less accessible, introduce new bugs, and have limited portability and maintainability. Loosely written code, in a high-level language, may be slow, but it will be faster to develop and easier to prototype. In concurrence, it is sensible to prioritize robust, general, and simple code above “fast code”—robust and general programs work in multiple situations (<xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>: examples 2.14 and 2.15), are reusable, and hence save development time, while clear simple code saves time when revisiting old code (or when sharing among peers). Clearly, slower code will lead to lower total project time if it is more generally applicable, or when additional development and debugging time exceeds what is saved in run time. Therefore, before attempting to optimize code, one should first determine if it will be worthwhile.</p>
</sec>
<sec id="sec004">
<title>What to Optimize?</title>
<p>Amdahl’s law (<xref rid="pcbi.1004140.g003" ref-type="fig">Fig. 3</xref>) [<xref rid="pcbi.1004140.ref014" ref-type="bibr">14</xref>] provides insight into the value of making a specific section of code more efficient: unless this code section uses a very large fraction of the overall execution time, the reduction in run time for the whole program may be modest. For example, consider code that requires 120 minutes to run, but one section can be sped up by a factor of 2. If that section consumes 95% of the original run time, optimization will improve total run time to 64 minutes. If that section consumes only 50% of the original run time, total run time will only improve to 90 minutes (<xref rid="pcbi.1004140.g003" ref-type="fig">Fig. 3A</xref>). Amdahl’s law also shows that increased effort in optimization has diminishing returns (<xref rid="pcbi.1004140.g003" ref-type="fig">Fig. 3B</xref>).</p>
<fig id="pcbi.1004140.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004140.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Projected improvements in total program run time using Amdahl's law.</title>
<p>(<bold>A</bold>) Realized total speed up when a section of code, taking up a fraction α of the total run time, is improved by a factor <italic>I</italic> (i.e., the expected program speed-up when the focal section runs <italic>I</italic> times faster). We see that optimization is only effective when the focal section of code consumes a large fraction of the total run time (α). (B) Total expected speed-up gain for different levels of α as a function of <italic>I</italic> (e.g., the number of parallel computations). Theoretical limits exist to the maximal improvement in speed, and this is crucially and asymptotically dependent on α—thus code optimization (and investment in computation hardware) are subject to the law of diminishing returns. All predictions here are subject to the scaling of the problem (<xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>, section 2).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004140.g003" position="float" xlink:type="simple"/>
</fig>
<p>Empirical studies in computer science show that small sections of code often consume large amounts of the total run time [<xref rid="pcbi.1004140.ref015" ref-type="bibr">15</xref>]. Identifying these code sections allows effective and targeted optimization. “Code profilers” are software engineering tools that measure the performance of different parts of a program as it executes [<xref rid="pcbi.1004140.ref016" ref-type="bibr">16</xref>]. When dealing with large data sets or large matrices, where memory storage is limiting, memory profilers (e.g., <italic>Rprofmem</italic>) provide statistics to gauge memory efficiency. We illustrate the value of profiling in <xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>, sections 1–3, using R’s profiler (<italic>Rprof</italic>) and a newly developed R package (<italic>aprof</italic>: “Amdahl's profiler”). This package helps to rapidly (and visually) identify code bottlenecks and potential optimization gains (as illustrated in <xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1</xref>).</p>
</sec>
<sec id="sec005">
<title>How to Optimize?</title>
<p>After bottlenecks have been identified, the precise nature of any optimization depends on the specific properties of the programming language. However, generally large gains can be achieved by avoiding common inefficiencies. (See <xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>, section 1.4 for more background information.)</p>
<sec id="sec006">
<title>1) Nonessential operations</title>
<p>Eliminating unnecessary function calls, printing statements, plotting, or memory references can increase efficiency. Many functions in high-level languages (see below) like R have default options enabled that may incur unnecessary cost. When profiling identifies a specific function as a bottleneck, check its inputs. For example, using unlist() on a list with named vectors can be sped up considerably with use.names = FALSE, while loading large datasets with read.table() or read.csv() is expedited by setting the colClasses input.</p>
</sec>
<sec id="sec007">
<title>2) Memoization</title>
<p>Store the results of expensive function calls that are used repeatedly. For instance, transpose a matrix or calculate a mean once prior to entering a loop rather than repeatedly within a loop. Replacing the repeatedly recalculated mean(d) in line 10 of <xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1A</xref>, with an object “avg” to store the mean of d, results in a drastic improvement in efficiency with a speedup of ∼ 28 times (red lines in <xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1B</xref>).</p>
</sec>
<sec id="sec008">
<title>3) Vectorized operations</title>
<p>Writing a loop to calculate elements of a vector or rows of a matrix is inefficient. In R, vectorized functions are faster because the actual loop has been pre-implemented in a lower-level, compiled language (in most cases C; [<xref rid="pcbi.1004140.ref017" ref-type="bibr">17</xref>]). Replacing the operation of calculating the mean differences over columns in lines 8–10 of <xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1B</xref> with its vectorized and highly specialized equivalent “colMeans(d[index,])-avg” (<xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1C</xref>, line 7), the overall execution speed is improved an additional 1.4 times. Note that very large vectors will be inefficient in R. In those cases chunk-based iteration is an effective compromise (see section on large data below).</p>
</sec>
<sec id="sec009">
<title>4) Growing data</title>
<p>“Growing data” refers to adding values incrementally to data frames, matrices, or vectors. When a new value is added and the object is lengthened, the new, longer, object must be written to free space in the memory. In the next iteration this process repeats itself, becoming ever more time-consuming. It is much faster to pre-allocate memory that is sufficiently large for the final object than to fill in new values as they are computed. Replacing line 16 from <xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1D</xref> with a pre-allocate-and-fill operation (lines 6, 7 and 17 in <xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1E</xref>) results in an ∼ 5 times speed-up.</p>
</sec>
<sec id="sec010">
<title>5) Dispatch overhead</title>
<p>Another potential speed-up strategy is to create custom functions to avoid overhead in base- or package-provided functions. The object-oriented philosophy of R encourages general purpose functions; these perform a large number of checks prior to doing the desired task. Custom-written functions perform only the desired task, without these checks, and can lead to significant speed-ups. Another strategy would be to use lower-level functions (see <xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>, section 1.4) instead of their default counterparts (e.g., <italic>lm</italic>.<italic>fit</italic> vs <italic>lm)</italic>. Note that custom and lower-level functions should be used cautiously as they provide speed at the cost of requiring much stricter compliance to input rules (e.g., <xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>: examples 2.14–2.15). For example, the Lotka-Volterra competition model code in <xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1F</xref> stores results in a data.frame. In R, data.frames are used for storing multiple types of data (e.g., integers, characters, factors etc.) however this functionality is not needed when only using numeric data. Switching to an efficient way of storing a single data type (a matrix) speeds up computation by a factor of ∼20 (compare <xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1E,F</xref>, <xref rid="pcbi.1004140.g002" ref-type="fig">Fig. 2B</xref>).</p>
<p>After each optimization step confirm that new code versions produce identical results compared to previous slower versions. Some simple functions for formal results checking in R include <italic>identical()</italic> and <italic>all</italic>.<italic>equal()</italic>.</p>
</sec>
<sec id="sec011">
<title>Parallelization</title>
<p>Parallel computing divides calculations into smaller problems and solves these simultaneously, using multiple computing elements (hereafter “workers”). In the biological sciences, many computationally intensive problems are “embarrassingly parallel” [<xref rid="pcbi.1004140.ref018" ref-type="bibr">18</xref>], where almost all calculations can be completed in parallel. Common examples are Monte-Carlo simulation and bootstrapping (<xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>, section 3). Popular parallel computing systems include computations on single multi-cored machines or “distributed computing” on clusters of workstations connected via a network. Our focus here is on modern multi-cored machines, where parallel computing has become relatively easy to implement, and which most people have access to—though we highlight where distributed computing will be particularly useful. Users should note before implementing a parallel algorithm that parallel code can be more challenging to debug. Accordingly, a handful of basic rules are worth reviewing (details in <xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>: section 3):
<list list-type="order">
<list-item><p>There is a start-up cost to initializing a collection of jobs to run in parallel, so a collection of small jobs may run faster sequentially (e.g., <xref rid="pcbi.1004140.g002" ref-type="fig">Fig. 2A</xref>), and more parallel processes do not necessarily lead to faster program execution [<xref rid="pcbi.1004140.ref005" ref-type="bibr">5</xref>] (i.e., parallel algorithms are also subject to Amdahl’s law, see <xref rid="pcbi.1004140.g003" ref-type="fig">Fig. 3B</xref>). When finalizing a parallel run, results need to be copied back to the parent process and collated from each worker; this can be expensive, especially when results are large.</p></list-item>
<list-item><p>In most computing devices, random access memory (RAM) is shared among parallel processes [<xref rid="pcbi.1004140.ref017" ref-type="bibr">17</xref>]. Ensure that enough memory is available for each worker, so parallel workers do not have to wait for memory to become available. Because shared memory decreases geometrically with each added worker, such systems are unsuited for big data. Parallel computing on a cluster, where memory is distributed (i.e., increases proportionately with the number of threads), or an algorithm that partitions the data proportionally to each worker, will be more feasible.</p></list-item>
<list-item><p>Independence of random number sequences must be ensured for valid scientific results (e.g., [<xref rid="pcbi.1004140.ref018" ref-type="bibr">18</xref>,<xref rid="pcbi.1004140.ref019" ref-type="bibr">19</xref>]). Ensure that random numbers sequences are unique, reproducible, and will not overlap (examples in <xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>, section 3.4).</p></list-item>
<list-item><p>Avoid load imbalances, where one processor has more work than the others causing them to wait. Attempt to split jobs equally. This is especially challenging on a cluster where jobs should match the available resources on each host machine.</p></list-item>
</list></p>
<p>Starting with the optimized but serial R-bootstrap code (<xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1C</xref>) we created a parallel algorithm for use on a single machine (<xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>, section 3), with which we achieved a speed-up by a factor of 2.5 with 4 cores (<xref rid="pcbi.1004140.g002" ref-type="fig">Fig. 2A</xref>).</p>
</sec>
<sec id="sec012">
<title>Calling Low-Level Languages</title>
<p>Parallel computing can reduce run time, but it essentially does not make code run any faster. In other cases parallel computing may not be possible (e.g., <xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1D–F</xref>). Substantial improvements in execution time can still be made by rewriting key sections of code in a “lower-level” or compiled language. Beginning R-programmers with limited familiarity with compiled languages are advised to pursue other “R-specific” routes of optimization first. These are more straightforward and lead to the greatest relative speed-ups (<xref rid="pcbi.1004140.g002" ref-type="fig">Fig. 2</xref>), while C is more complicated to develop and debug (requiring memory management and missing data (NA) handling).</p>
<p>In general, there are two types of programming languages: interpreted (R, MATLAB) and compiled (C, Fortran). In interpreted languages, like R, code is indirectly evaluated by an evaluation program (hereafter the R-interpreter; [<xref rid="pcbi.1004140.ref012" ref-type="bibr">12</xref>]). In compiled languages, like C, code is first translated to machine language (i.e., machine-specific instructions) by a compiler program and then directly executed on the central processing unit (CPU). The differences in the type of programming language used can have large effects on execution speed [<xref rid="pcbi.1004140.ref012" ref-type="bibr">12</xref>].</p>
<p>Compiled and interpreted languages exhibit a trade-off in run time versus programmer time, respectively. Interpreted languages have the benefits of being relatively easy to understand, debug, and alter. However, there is usually much higher CPU overhead as each line must be translated (i.e., “interpreted”) every time it is executed. Compiled languages tend to be more challenging to code and debug, but are highly efficient when executed, as “translation overhead” occurs just once, when the source code is compiled.</p>
<p>In the Lotka-Volterra code in <xref rid="pcbi.1004140.g001" ref-type="fig">Fig. 1F</xref>, we find no clear bottlenecks, with most time consumed by the repeated interpretation of mathematical operators (*, +, etc, <xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>, section 6.15) and random number generation (“rnorm”). We were able to remove such translation overhead by rewriting critical parts of the program in in C and calling the compiled code from R. With this we created a six-times–faster “vectorized” version of the model (<xref rid="pcbi.1004140.g002" ref-type="fig">Fig. 2B</xref>). In <xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref> (section 5) we give practical advice on extending R with C using the most common interfaces for extending R (through the .C and .Call interfaces; [<xref rid="pcbi.1004140.ref019" ref-type="bibr">19</xref>]). In <xref rid="pcbi.1004140.s003" ref-type="supplementary-material">S3 Text</xref>, our applied example, we use <italic>Rccp</italic> [<xref rid="pcbi.1004140.ref020" ref-type="bibr">20</xref>] and <italic>RccpArmadillo</italic> [<xref rid="pcbi.1004140.ref021" ref-type="bibr">21</xref>] to speed up a matrix-multiplication by a factor of 400.</p>
<p>Many interpreted languages also provide special compilers for finished programs, which are simple to use. These represent a compromise between a true compiler and an interpreter. In the R <italic>compiler</italic> package a byte-code compiler is used, which translates R code into more compact numeric codes. It does not produce machine-language code, but instruction sets designed for efficient execution by the interpreter. This may be a quick fix to speed up some code, but most functions are already distributed in byte-compiled form, so further speed gains using byte-compiling are modest. In our examples, we did find that using this compiler decreases execution time (<xref rid="pcbi.1004140.g002" ref-type="fig">Fig. 2B</xref> and <xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>, section 6.5.1).</p>
</sec>
<sec id="sec013">
<title>Large Data</title>
<p>R loads data into memory by default: datasets comparable in size to the amount of memory available will slow R to a crawl while datasets exceeding the memory space will fail altogether. In these cases researchers can either 1) use databases stored outside R, accessing these in R via languages like SQL (via, for example, RSQLite) or 2) use more memory-efficient algorithms. The latter usually involves sequential algorithms, which restrict memory usage to one block of data at a time. Many statistics can be calculated sequentially (e.g., [<xref rid="pcbi.1004140.ref022" ref-type="bibr">22</xref>]), but problems will take longer to solve as accessing data from a storage disk is slower than from memory. We provide a short example on how to do this for the bootstrap example in <xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref> (section 4), using the <italic>ff</italic> package [<xref rid="pcbi.1004140.ref023" ref-type="bibr">23</xref>].</p>
</sec>
<sec id="sec014">
<title>Using More Efficient Algorithms</title>
<p>A final method to speed up computations is to use a more efficient algorithm. These are mathematically equivalent, but computationally smaller, methods (i.e., they use fewer operations). Although this is highly problem specific, we nevertheless highlight this point, as it is worth scrutinizing the efficiency of the algorithm in use since substantial speed-up may be gained when alternatives exist [<xref rid="pcbi.1004140.ref002" ref-type="bibr">2</xref>]. For example, matching m values in a table of n elements requires on the order of m × n operations with a loop and on the order of m + n options when a hash table is constructed first [<xref rid="pcbi.1004140.ref012" ref-type="bibr">12</xref>]. Subsequent matches will be even faster if the hash table is stored, as in the <italic>fastmatch</italic> library [<xref rid="pcbi.1004140.ref024" ref-type="bibr">24</xref>]. Additional examples include using the turning bands algorithm [<xref rid="pcbi.1004140.ref025" ref-type="bibr">25</xref>,<xref rid="pcbi.1004140.ref026" ref-type="bibr">26</xref>] instead of a Cholesky (variance-covariance) decomposition when simulating a large spatially correlated random field or using an algorithm like Broyden-Fletcher-Goldfarb-Shanno in non-linear optimization, which requires fewer evaluations of the objective function because the Hessian matrix is built up from information about the first derivatives.</p>
</sec>
</sec>
<sec id="sec015">
<title>Recommendations</title>
<p>Optimizing code can provide efficiency gains of orders of magnitude, as our benchmark results show (e.g., <xref rid="pcbi.1004140.g002" ref-type="fig">Fig. 2</xref>). However, we do not recommend optimizing immediately. Realize that one will inevitably sacrifice clarity, generality, and robustness for speed. At the start of a project, the most productive approach (e.g., [<xref rid="pcbi.1004140.ref003" ref-type="bibr">3</xref>]) is often to write code in the highest-level language possible ensuring the program runs correctly. High-level languages enable rapid decision-making and prototyping, and correct code enables checking of more optimized versions. When a performance boost is deemed worthwhile, for example, through profiling, only optimize those parts identified as bottlenecks to avoid sacrificing development time in favour of optimization [<xref rid="pcbi.1004140.ref003" ref-type="bibr">3</xref>,<xref rid="pcbi.1004140.ref012" ref-type="bibr">12</xref>]. The primary route for optimization should be efficient R code which, as we show in <xref rid="pcbi.1004140.g002" ref-type="fig">Fig. 2</xref>, yields the largest gains for the least effort.</p>
<p>The fastest running code examples shown here are the instances where we called compiled code from R (<xref rid="pcbi.1004140.g002" ref-type="fig">Fig. 2</xref>, <xref rid="pcbi.1004140.s001" ref-type="supplementary-material">S1 Text</xref>: section 6). This technique is especially powerful when one can use the vast libraries of algorithms that already exist in C (and Fortran), which are often optimized and efficiently coded [<xref rid="pcbi.1004140.ref012" ref-type="bibr">12</xref>]. However, a programming language like C has a steeper learning curve and when learning C requires too much time, we encourage biologists to collaborate with computer scientists in their research or to include contracts for computational consultation in grant budgets [<xref rid="pcbi.1004140.ref003" ref-type="bibr">3</xref>].</p>
</sec>
<sec id="sec016" sec-type="conclusions">
<title>Conclusion</title>
<p>Learning how to program and efficiently use computational resources is not only convenient. Computing has become fundamental to the practice of science (e.g., [<xref rid="pcbi.1004140.ref001" ref-type="bibr">1</xref>–<xref rid="pcbi.1004140.ref003" ref-type="bibr">3</xref>,<xref rid="pcbi.1004140.ref027" ref-type="bibr">27</xref>]). In biology, research is striving toward ever more accurate projections to inform public leaders on nature management or make predictions regarding how ecosystems respond to change (e.g., [<xref rid="pcbi.1004140.ref028" ref-type="bibr">28</xref>–<xref rid="pcbi.1004140.ref030" ref-type="bibr">30</xref>]). More often than not, such accurate predictions will require high levels of detail as natural systems are variable and include intricate levels of biotic and abiotic interactions (e.g. [<xref rid="pcbi.1004140.ref031" ref-type="bibr">31</xref>–<xref rid="pcbi.1004140.ref032" ref-type="bibr">32</xref>]). With these challenges ahead, the use of computationally intensive analyses in the biological sciences should not be constrained by programming practices.</p>
</sec>
<sec id="sec017">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004140.s001" xlink:href="info:doi/10.1371/journal.pcbi.1004140.s001" mimetype="application/pdf" position="float" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Tutorial with background information and detailed examples on, e.g, profiling, optimal R coding, parallel computation, working with large datasets, and extending R with C.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004140.s002" xlink:href="info:doi/10.1371/journal.pcbi.1004140.s002" mimetype="application/pdf" position="float" xlink:type="simple">
<label>S2 Text</label>
<caption>
<title>Optimization of code from [<xref rid="pcbi.1004140.ref010" ref-type="bibr">10</xref>].</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004140.s003" xlink:href="info:doi/10.1371/journal.pcbi.1004140.s003" mimetype="application/pdf" position="float" xlink:type="simple">
<label>S3 Text</label>
<caption>
<title>Optimization of code from [<xref rid="pcbi.1004140.ref011" ref-type="bibr">11</xref>].</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Tyler Rinker for valuable comments on the examples in the Supporting Text files.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004140.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Michener</surname> <given-names>WK</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>MB</given-names></name>. <article-title>Ecoinformatics: supporting ecology as a data-intensive science</article-title>. <source>Trends Ecol Evol</source>. <year>2012</year>; <volume>27</volume>: <fpage>85</fpage>–<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tree.2011.11.016" xlink:type="simple">10.1016/j.tree.2011.11.016</ext-link></comment> <object-id pub-id-type="pmid">22240191</object-id></mixed-citation></ref>
<ref id="pcbi.1004140.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Petrovskii</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Petrovskaya</surname> <given-names>N</given-names></name>. <article-title>Computational ecology as an emerging science</article-title>. <source>Interface Focus</source>. <year>2012</year>; <volume>2</volume>: <fpage>241</fpage>–<lpage>254</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rsfs.2011.0083" xlink:type="simple">10.1098/rsfs.2011.0083</ext-link></comment> <object-id pub-id-type="pmid">23565336</object-id></mixed-citation></ref>
<ref id="pcbi.1004140.ref003"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Wilson G, Aruliah DA, Brown CT, Hong NPC, Davis M, Guy, RT, et al. Best practices for scientific computing; 2012. Preprint. Available: arXiv:1210.0530. Accessed 20 November 2012.</mixed-citation></ref>
<ref id="pcbi.1004140.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ellison</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Dennis</surname> <given-names>B</given-names></name>. <article-title>Paths to statistical fluency for ecologists</article-title>. <source>Front Ecol Environ</source>. <year>2010</year>;<volume>8</volume>: <fpage>362</fpage>–<lpage>370</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref005"><label>5</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hager</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wellein</surname> <given-names>G</given-names></name>. <source>Introduction to High Performance Computing for Scientists and Engineers</source>. <edition>1st ed.</edition> <publisher-loc>Boca Raton</publisher-loc>: <publisher-name>CRC Press</publisher-name>; <year>2010</year>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zuidema</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Jongejans</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Chien</surname> <given-names>PD</given-names></name>, <name name-style="western"><surname>During</surname> <given-names>HJ</given-names></name>, <name name-style="western"><surname>Schieving</surname> <given-names>F</given-names></name>. <article-title>Integral Projection Models for trees: a new parameterization method and a validation of model output</article-title>. <source>J Ecol</source>. <year>2010</year>; <volume>98</volume>: <fpage>345</fpage>–<lpage>355</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Putten</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Visser</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Muller-Landau</surname> <given-names>HC</given-names></name>, <name name-style="western"><surname>Jansen</surname> <given-names>PA</given-names></name>. <article-title>Distorted-distance models for directional dispersal: a general framework with application to a wind-dispersed tree</article-title>. <source>Methods Ecol Evol</source>. <year>2012</year>;<volume>3</volume>: <fpage>642</fpage>–<lpage>652</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref008"><label>8</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Nagarajan</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Scutari</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lèbre</surname> <given-names>S</given-names></name>. <chapter-title>Bayesian Networks in R with applications in systems biology</chapter-title>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2013</year>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Comita</surname> <given-names>LS</given-names></name>, <name name-style="western"><surname>Muller-Landau</surname> <given-names>HC</given-names></name>, <name name-style="western"><surname>Aguilar</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hubbell</surname> <given-names>SP</given-names></name>. <article-title>Asymmetric density dependence shapes species abundances in a tropical tree community</article-title>. <source>Science</source>. <year>2010</year>; <volume>329</volume>:<fpage>330</fpage>–<lpage>332</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1190772" xlink:type="simple">10.1126/science.1190772</ext-link></comment> <object-id pub-id-type="pmid">20576853</object-id></mixed-citation></ref>
<ref id="pcbi.1004140.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Merow</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>LaFleur</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Silander</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Rubega</surname> <given-names>M</given-names></name>. <article-title>Developing dynamic mechanistic species distribution models: predicting bird-mediated spread of invasive plants across Northeastern North America</article-title>. <source>Am Nat</source>. <year>2011</year>;<volume>178</volume>: <fpage>30</fpage>–<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1086/660295" xlink:type="simple">10.1086/660295</ext-link></comment> <object-id pub-id-type="pmid">21670575</object-id></mixed-citation></ref>
<ref id="pcbi.1004140.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Visser</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Jongejans</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Van Breugel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Zuidema</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>Y-Y</given-names></name>, <name name-style="western"><surname>Kassim</surname> <given-names>AR</given-names></name>, <etal>et al</etal>. <article-title>Strict mast fruiting for a tropical dipterocarp tree: a demographic cost-benefit analysis of delayed reproduction and seed predation</article-title>. <source>J Ecol</source>. <year>2011</year>;<volume>99</volume>: <fpage>1033</fpage>–<lpage>1044</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref012"><label>12</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Chambers</surname> <given-names>JM</given-names></name>. <source>Software for Data Analysis: Programming with R</source>. <publisher-name>Springer</publisher-name>: <publisher-loc>New York</publisher-loc>; <year>2009</year>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref013"><label>13</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kernighan</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Plauger</surname> <given-names>PJ</given-names></name>. <source>The Elements of Programming Style</source>. <edition>2nd ed.</edition> <publisher-name>McGraw Hill</publisher-name>: <publisher-loc>New York</publisher-loc>; <year>1978</year>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amdahl</surname> <given-names>G</given-names></name>. <article-title>Validity of the single processor approach to achieving large-scale computing capabilities</article-title>. <source>AFIPS Conference Proceedings</source>. <year>1967</year>; <volume>30</volume>: <fpage>483</fpage>–<lpage>485</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Porter</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Selby</surname> <given-names>RW</given-names></name>. <article-title>Evaluating techniques for generating metric-based classification trees</article-title>. <source>J Syst Softw</source>. <year>1990</year>; <volume>12</volume>: <fpage>209</fpage>–<lpage>218</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref016"><label>16</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bryant</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>O’Hallaron</surname> <given-names>DR</given-names></name>. <source>Computer Systems: A Programmer’s Perspective</source>. <publisher-name>Prentice Hall</publisher-name>: <publisher-loc>Upper Saddle River</publisher-loc>; <year>2010</year>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schmidberger</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Morgan</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Eddelbuettel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Tierney</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Mansmann</surname> <given-names>U</given-names></name>. <article-title>State-of-the-art in Parallel Computing with R</article-title>. <source>J Stat Softw</source>. <year>2009</year>; <volume>31</volume>:<fpage>1</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref018"><label>18</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Grama</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Karypis</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Kumar</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Gupta</surname> <given-names>A</given-names></name>. <source>Introduction to Parallel Computing</source>. <publisher-name>Pearson Education</publisher-name>; <year>2003</year>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref019"><label>19</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>L’Ecuyer</surname> <given-names>P</given-names></name>. <chapter-title>Random number generation</chapter-title>. In: <name name-style="western"><surname>Gentle</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Haerdle</surname> <given-names>w</given-names></name>, <name name-style="western"><surname>Mori</surname> <given-names>Y</given-names></name>, editors. <source>the Handbook of Computational Statistics</source>. <publisher-name>Springer-Verlag</publisher-name>; <year>2012</year>. pp. <fpage>35</fpage>–<lpage>71</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eddelbuettel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>François</surname> <given-names>R</given-names></name>. <article-title>Rcpp: Seamless R and C++ integration</article-title>. <source>J Stat Softw</source>. <year>2011</year>;<volume>40</volume>: <fpage>1</fpage>–<lpage>18</lpage>. <object-id pub-id-type="pmid">22523482</object-id></mixed-citation></ref>
<ref id="pcbi.1004140.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eddelbuettel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Sanderson</surname> <given-names>C</given-names></name>. <article-title>RcppArmadillo: Accelerating R with high-performance C++ linear algebra</article-title>. <source>Comput Stat Data Anal</source>. <year>2014</year>; <volume>71</volume>:<fpage>1054</fpage>–<lpage>1063</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Robbins</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Monro</surname> <given-names>S</given-names></name>. <article-title>A stochastic approximation method</article-title>. <source>Ann Math Stat</source>. <year>1951</year>; <volume>22</volume>: <fpage>400</fpage>–<lpage>407</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref023"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Adler D, Gläser C, Nenadic O, Oehlschlägel J, Zucchini W. ff: memory-efficient storage of large data on disk and fast access functions. 2014. Available: <ext-link ext-link-type="uri" xlink:href="http://cran.r-project.org/package=ff" xlink:type="simple">http://cran.r-project.org/package=ff</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref024"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Urbanek S. fastmatch: Fast match() function. 2012. Available: <ext-link ext-link-type="uri" xlink:href="http://CRAN.R-project.org/package=fastmatch" xlink:type="simple">http://CRAN.R-project.org/package=fastmatch</ext-link></mixed-citation></ref>
<ref id="pcbi.1004140.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mantoglou</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>JL</given-names></name>. <article-title>The turning bands method for simulation of random fields using line generation by a spectral method</article-title>. <source>Water Resour Res</source>. <year>1982</year>; <volume>18</volume>: <fpage>1379</fpage>–<lpage>1394</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Finley</surname> <given-names>AO</given-names></name>. <article-title>Comparing spatially-varying coefficients models for analysis of ecological data with non-stationary and anisotropic residual dependence</article-title>. <source>Methods Ecol Evol</source>. <year>2011</year>;<volume>2</volume>: <fpage>143</fpage>–<lpage>154</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Merali</surname> <given-names>Z</given-names></name>. <article-title>Computational science: Error, why scientific programming does not compute</article-title>. <source>Nature</source>. <year>2010</year>; <volume>467</volume>: <fpage>775</fpage>–<lpage>777</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/467775a" xlink:type="simple">10.1038/467775a</ext-link></comment> <object-id pub-id-type="pmid">20944712</object-id></mixed-citation></ref>
<ref id="pcbi.1004140.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guisan</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lehmann</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ferrier</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Austin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Overton</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Making better biogeographical predictions of species’ distributions</article-title>. <source>J Appl Ecol</source>. <year>2006</year>;<volume>43</volume>: <fpage>386</fpage>–<lpage>392</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004140.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brook</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>O’Grady</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Chapman</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Burgman</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Akçakaya</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Frankham</surname> <given-names>R</given-names></name>. <article-title>Predictive accuracy of population viability analysis in conservation biology</article-title>. <source>Nature</source>. <year>2000</year>;<volume>404</volume>: <fpage>385</fpage>–<lpage>387</lpage>. <object-id pub-id-type="pmid">10746724</object-id></mixed-citation></ref>
<ref id="pcbi.1004140.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Isbell</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Calcagno</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Hector</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Connolly</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Harpole</surname> <given-names>WS</given-names></name>, <name name-style="western"><surname>Reich</surname> <given-names>PB</given-names></name>, <etal>et al</etal>. <article-title>High plant diversity is needed to maintain ecosystem services</article-title>. <source>Nature</source>. <year>2011</year>; <volume>477</volume>: <fpage>199</fpage>–<lpage>202</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature10282" xlink:type="simple">10.1038/nature10282</ext-link></comment> <object-id pub-id-type="pmid">21832994</object-id></mixed-citation></ref>
<ref id="pcbi.1004140.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moran</surname> <given-names>E V</given-names></name>, <name name-style="western"><surname>Clark</surname> <given-names>JS</given-names></name>. <article-title>Estimating seed and pollen movement in a monoecious plant: a hierarchical Bayesian approach integrating genetic and ecological data</article-title>. <source>Mol Ecol</source>. <year>2011</year>;<volume>20</volume>: <fpage>1248</fpage>–<lpage>1262</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1365-294X.2011.05019.x" xlink:type="simple">10.1111/j.1365-294X.2011.05019.x</ext-link></comment> <object-id pub-id-type="pmid">21332584</object-id></mixed-citation></ref>
<ref id="pcbi.1004140.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bohrer</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Katul</surname> <given-names>GG</given-names></name>, <name name-style="western"><surname>Walko</surname> <given-names>RL</given-names></name>, <name name-style="western"><surname>Avissar</surname> <given-names>R</given-names></name>. <article-title>Exploring the effects of microscale structural heterogeneity of forest canopies using large-eddy simulations</article-title>. <source>Boundary-Layer Meteorol</source>. <year>2009</year>;<volume>132</volume>: <fpage>351</fpage>–<lpage>382</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>