<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005229</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-00513</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Phonology</subject><subj-group><subject>Syllables</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Geometry</subject><subj-group><subject>Ellipses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Technology development</subject><subj-group><subject>Prototypes</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A Causal Inference Model Explains Perception of the McGurk Effect and Other Incongruent Audiovisual Speech</article-title>
<alt-title alt-title-type="running-head">Causal Inference in the McGurk Effect</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Magnotti</surname>
<given-names>John F.</given-names>
</name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7599-9934</contrib-id>
<name name-style="western">
<surname>Beauchamp</surname>
<given-names>Michael S.</given-names>
</name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Department of Neurosurgery and Core for Advanced MRI, Baylor College of Medicine, Houston, Texas, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gershman</surname>
<given-names>Samuel J.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Harvard University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"> <list-item><p><bold>Conceived and designed the experiments:</bold> JFM MSB.</p></list-item> <list-item><p><bold>Performed the experiments:</bold> JFM MSB.</p></list-item> <list-item><p><bold>Analyzed the data:</bold> JFM.</p></list-item> <list-item><p><bold>Contributed reagents/materials/analysis tools:</bold> JFM.</p></list-item> <list-item><p><bold>Wrote the paper:</bold> JFM MSB.</p></list-item></list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">magnotti@bcm.edu</email> (JFM); <email xlink:type="simple">michael.beauchamp@bcm.edu</email> (MSB)</corresp>
</author-notes>
<pub-date pub-type="epub">
<day>16</day>
<month>2</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<month>2</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>2</issue>
<elocation-id>e1005229</elocation-id>
<history>
<date date-type="received">
<day>30</day>
<month>3</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>1</day>
<month>11</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Magnotti, Beauchamp</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005229"/>
<abstract>
<p>Audiovisual speech integration combines information from auditory speech (talker’s voice) and visual speech (talker’s mouth movements) to improve perceptual accuracy. However, if the auditory and visual speech emanate from different talkers, integration decreases accuracy. Therefore, a key step in audiovisual speech perception is deciding whether auditory and visual speech have the same source, a process known as causal inference. A well-known illusion, the McGurk Effect, consists of incongruent audiovisual syllables, such as auditory “ba” + visual “ga” (AbaVga), that are integrated to produce a fused percept (“da”). This illusion raises two fundamental questions: first, given the incongruence between the auditory and visual syllables in the McGurk stimulus, why are they integrated; and second, why does the McGurk effect not occur for other, very similar syllables (<italic>e</italic>.<italic>g</italic>., AgaVba). We describe a simplified model of causal inference in multisensory speech perception (CIMS) that predicts the perception of arbitrary combinations of auditory and visual speech. We applied this model to behavioral data collected from 60 subjects perceiving both McGurk and non-McGurk incongruent speech stimuli. The CIMS model successfully predicted both the audiovisual integration observed for McGurk stimuli and the lack of integration observed for non-McGurk stimuli. An identical model without causal inference failed to accurately predict perception for either form of incongruent speech. The CIMS model uses causal inference to provide a computational framework for studying how the brain performs one of its most important tasks, integrating auditory and visual speech cues to allow us to communicate with others.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>During face-to-face conversations, we seamlessly integrate information from the talker’s voice with information from the talker’s face. This multisensory integration increases speech perception accuracy and can be critical for understanding speech in noisy environments with many people talking simultaneously. A major challenge for models of multisensory speech perception is thus deciding which voices and faces should be integrated. Our solution to this problem is based on the idea of causal inference—given a particular pair of auditory and visual syllables, the brain calculates the likelihood they are from a single <italic>vs</italic>. multiple talkers and uses this likelihood to determine the final speech percept. We compared our model with an alternative model that is identical, except that it always integrated the available cues. Using behavioral speech perception data from a large number of subjects, the model with causal inference better predicted how humans would (or would not) integrate audiovisual speech syllables. Our results suggest a fundamental role for a causal inference type calculation in multisensory speech perception.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000065</institution-id>
<institution>National Institute of Neurological Disorders and Stroke</institution>
</institution-wrap>
</funding-source>
<award-id>R01NS065395</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7599-9934</contrib-id>
<name name-style="western">
<surname>Beauchamp</surname>
<given-names>Michael S.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000092</institution-id>
<institution>U.S. National Library of Medicine</institution>
</institution-wrap>
</funding-source>
<award-id>T15LM007093</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Magnotti</surname>
<given-names>John F.</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This research was supported by NIH R01NS065395 to MSB. JFM is supported by a training fellowship from the Gulf Coast Consortia, NLM Training Program in Biomedical Informatics (NLM Grant No. T15LM007093). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="3"/>
<table-count count="0"/>
<page-count count="15"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Data are available at the author's website: <ext-link ext-link-type="uri" xlink:href="http://openwetware.org/wiki/Beauchamp:DataSharing" xlink:type="simple">http://openwetware.org/wiki/Beauchamp:DataSharing</ext-link></meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Speech is the most important method of human communication and is fundamentally multisensory, with both auditory cues (the talker’s voice) and visual cues (the talker’s face) contributing to perception. Because auditory and visual speech cues can be corrupted by noise, integrating the cues allows subjects to more accurately perceive the speech content [<xref ref-type="bibr" rid="pcbi.1005229.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1005229.ref003">3</xref>]. However, integrating auditory and visual speech cues can also lead subjects to <italic>less</italic> accurately perceive speech if the speech cues are incongruent. For instance, a unisensory auditory “ba” paired with a unisensory visual “ga” (AbaVga) leads to the perception of “da”, a speech stimulus that is not physically present. The illusion was first described experimentally by McGurk and MacDonald in 1976 [<xref ref-type="bibr" rid="pcbi.1005229.ref004">4</xref>] and is commonly known as the McGurk effect.</p>
<p>The McGurk effect has become a staple of classroom demonstrations and television documentaries because it is both simple and powerful—simply closing and opening one’s eyes completely changes the speech percept—and is also an important tool for research, with over 3,000 citations to the original paper in the last ten years. The McGurk effect is surprising because the incongruent speech tokens are easy to identify as physically incompatible: it is impossible for an open-mouth velar as seen in visual “ga” to produce a closed-mouth bilabial sound as heard in auditory “ba”. The effect raises fundamental questions about the computations underlying multisensory speech perception: Why would the brain integrate two incompatible speech components to produce an illusory percept? If the illusion happens at all, why does it not happen more often?</p>
<p>We propose a comprehensive computational model of multisensory speech perception that can explain these properties of the McGurk effect, building on previous models [<xref ref-type="bibr" rid="pcbi.1005229.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005229.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1005229.ref009">9</xref>]. Our model is based on the principle of <italic>causal inference</italic> [<xref ref-type="bibr" rid="pcbi.1005229.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1005229.ref012">12</xref>]. Rather than integrating all available cues, observers should only integrate cues resulting from the same physical cause. In speech perception, humans often encounter environments with multiple faces and multiple voices and must decide whether to integrate information from a given face-voice pairing. More precisely, because observers can never be certain that a given face pairs with a given voice, they must infer the likelihood of each causal scenario (a single talker <italic>vs</italic>. separate talkers) and then combine the representations from each scenario, weighted by their likelihoods. For simple syllable perception, individuals often perceive the auditory component of speech when the face and voice are separate talkers [<xref ref-type="bibr" rid="pcbi.1005229.ref013">13</xref>]. The final result of causal inference is then the average of the integrated multisensory representation (the representation assuming a single talker) and the auditory representation (the representation assuming separate talkers), weighted by the likelihood that the face and voice arise from a single talker <italic>vs</italic>. separate talkers.</p>
<p>To test whether causal inference can account for the perception of incongruent multisensory speech, we created two similar models, one that <italic>did</italic> perform causal inference on multisensory speech (CIMS) and a model identical in every way, except that it <italic>did not</italic> perform causal inference (non-CIMS). We obtained predictions from the CIMS and non-CIMS models for a variety of audiovisual syllables and compared the model predictions with the percepts reported by human subjects presented with the same syllables.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Methods</title>
<p>In everyday environments, we encounter audiovisual speech and must decide whether the auditory and visual components of the speech emanate from a single talker (<italic>C</italic> = 1) or two separate talkers (<italic>C</italic> = 2; <xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1A</xref>). Most studies of multisensory integration assume that <italic>C</italic> = 1 and focus on the details of the inference used to produce a single multisensory representation that is then categorized as a particular percept [<xref ref-type="bibr" rid="pcbi.1005229.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005229.ref005">5</xref>]. To carry out causal inference, we must perform the additional steps of calculating the <italic>C</italic> = 2 representation and then combining the <italic>C</italic> = 1 and <italic>C</italic> = 2 representations. This combined representation is then categorized as a particular percept. Critically, identical stimuli can result in different percepts with (CIMS) and without (non-CIMS) causal inference.</p>
<fig id="pcbi.1005229.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005229.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Modeling of multisensory speech perception without causal inference.</title>
<p>(A) There are two possible causal structures for a given audiovisual speech stimulus. If there is a common cause (<italic>C</italic> = 1), a single talker generates the auditory and visual speech. Alternatively, if there is not a common cause (<italic>C</italic> = 2), two separate talkers generate the auditory and visual speech. (B) We generate multisensory representations in a two-dimensional representational space. The prototypes of the syllables “ba,” “da,” and “ga” (location of text labels) are mapped into the representational space with locations determined by pairwise confusability. The x-axis represents auditory features; the y-axis represents visual features. (C) Encoding the auditory “ba” + visual “ga” (AbaVga) McGurk stimulus. The unisensory components of the stimulus are encoded with noise that is independent across modalities. On three trials in which an identical AbaVga stimulus is presented (represented as 1, 2, 3) the encoded representations of the auditory and visual components differ because of sensory noise, although they are centered on the prototype (gray ellipses show 95% probability region across all presentations). Shapes of ellipses reflect reliability of each modality: for auditory “ba” (ellipse labeled A), the ellipse has its short axis along the auditory x-axis; visual “ga” (ellipse labeled V) has its short axis along the visual y-axis. (D) On each trial, the unisensory representations are integrated using Bayes’ rule to produce an integrated representation that is located between the unisensory components in representational space. Numbers show the actual location of the integrated unisensory representations from <bold><italic>C</italic></bold>. Because of reliability weighting, the integrated representations are closer to “ga” along the visual y-axis, but closer to “ba” along the auditory x-axis (ellipse shows 95% probability region across all presentations). (E) Without causal inference (non-CIMS), the AV representation is the final representation. On most trials, the representation lies in the “da” region of representational space (numbers and 95% probability ellipse from <bold>D</bold>). (F) A linear decision rule is applied, resulting in a model prediction of exclusively “da” percepts across trials. (G) Behavioral data from 60 subjects reporting their percept of auditory “ba” + visual “ga”. Across trials, subjects reported the “ba” percept for 57% of trials and “da” for 40% of trials. (H) Encoding the auditory “ga” + visual “ba” (AgaVba) incongruent non-McGurk stimulus. The unisensory components are encoded with modality-specific noise; the auditory “ga” ellipse has its short axis along the auditory axis, the visual “ba” ellipse has its short axis along the visual axis. (I) Across many trials, the integrated representation (AV) is closer to “ga” along the auditory x-axis, but closer to “ba” along the visual <italic>y</italic>-axis. (J) Over many trials, the integrated representation is found most often in the “da” region of perceptual space. (K) Across trials, the non-CIMS model predicts “da” for the non-McGurk stimulus. (L) Behavioral data from 60 subjects reporting their perception of AgaVba. Subjects reported “ga” on 96% of trials.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005229.g001" xlink:type="simple"/>
</fig>
<p>We begin by describing the common elements of the CIMS and non-CIMS models. We define a two-dimensional space as the minimum possible dimension for characterizing auditory and visual speech information (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1B</xref>). The x-axis represents auditory information in the stimulus while the y-axis represents visual information in the stimulus. For simplicity, we model a space containing only 3 speech token categories, “ba,” “da,” and “ga”. Based on behavioral confusability studies and previous modeling work [<xref ref-type="bibr" rid="pcbi.1005229.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1005229.ref014">14</xref>], “da” was placed intermediate to “ba” and “ga” on both the auditory and visual axes, slightly closer to “ba” on the auditory x-axis, and slightly closer to “ga” on the visual y-axis. These syllable locations can be thought of as prototypes, with different talkers (or different utterances from the same talker) differing from the prototype. To model these category distributions as simply as possible, we defined two-dimensional variance-covariance matrices (identical across syllables) with zero covariance (information in auditory and visual axes is uncorrelated) and equal variances along each axis. The axes of this representational space do not correspond in a simple way to physical properties of the stimulus; instead, they correspond to some internal neural representational space in which auditory and visual syllables are mapped into a single representational space that allows for integration.</p>
<p>A staple of Bayesian models of perception is the concept of sensory noise. Not only do individual exemplar stimuli vary from their prototype, the perceived stimulus varies from its actual physical properties due to sensory noise. We model this as two-dimensional variance-covariance matrices representing Gaussian noise in each modality (<italic>Σ</italic><sub>A</sub> and <italic>Σ</italic><sub>V</sub>, for auditory and visual encoding) with variances inversely proportional to the precision of the modality. We chose Gaussian noise both because of its use in previous computational models [<xref ref-type="bibr" rid="pcbi.1005229.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005229.ref005">5</xref>] and because Gaussian noise arises naturally in situations where a signal is corrupted by multiple independent sources. Modalities are encoded separately, but through extensive experience with audiovisual speech, encoding a unisensory speech stimulus provides some information about the other modality. For instance, hearing an unisensory auditory “ba” informs the observer that the mouth of the talker must have been in an initially lips-closed position. For such a unisensory cue, the information provided about the other sensory modality has higher variance. In our model, we assume that for auditory cues, the standard deviation along the visual axis is 1.5 times larger than the standard deviation along the auditory axis. For visual cues, the standard deviation along the auditory axis is 1.5 times larger than the standard deviation along the visual axis. This setup produces unisensory noise matrices that are rotations of one another (ellipses labelled <bold>A</bold> and <bold>V</bold> in <xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1C</xref>).</p>
<p>For each presentation of a given audiovisual stimulus, the model encodes each modality separately. For a single trial of a stimulus with auditory component <italic>S</italic><sub>A</sub> and visual component <italic>S</italic><sub>V</sub>, the model generates two vectors: the auditory representation <inline-formula id="pcbi.1005229.e001"><alternatives><graphic id="pcbi.1005229.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> and the visual representation <inline-formula id="pcbi.1005229.e002"><alternatives><graphic id="pcbi.1005229.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1005229.e003"><alternatives><graphic id="pcbi.1005229.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>Σ</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> is a normal distribution with mean <italic>μ</italic> and variance <italic>Σ</italic>. Across many trials, the values of <italic>X</italic><sub>A</sub> and <italic>X</italic><sub>V</sub> will cluster around the exemplar locations <italic>S</italic><sub>A</sub> and <italic>S</italic><sub>V</sub> with variance equal to the modality-specific encoding variances (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1C</xref>, where <italic>S</italic><sub>A</sub> and <italic>S</italic><sub>V</sub> are the set to the prototypical representations for “ba” and “ga”, respectively).</p>
<p>To form the <italic>C =</italic> 1 representation, the model assumes Bayesian inference (integration of auditory and visual speech cues according to their reliabilities). We use the two-dimensional analog of the common Bayesian cue-integration rules as described by [<xref ref-type="bibr" rid="pcbi.1005229.ref002">2</xref>]. On each trial, we calculate the integrated representation as <inline-formula id="pcbi.1005229.e004"><alternatives><graphic id="pcbi.1005229.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1005229.e005"><alternatives><graphic id="pcbi.1005229.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>. Across many trials, the distribution of the <italic>C</italic> = 1 representations will be the weighted average of the locations for <italic>S</italic><sub>A</sub> and <italic>S</italic><sub>V</sub> with the weighting controlled by the relative precision of the encoding matrices (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1D</xref>).</p>
<p>Without causal inference, the integrated representation, <italic>X</italic><sub>AV</sub>, is the final representation. Although the representational space is continuous, speech perception is categorical. Therefore, to produce a categorical percept, we determine the syllable that is most likely to have generated the integrated representation: <inline-formula id="pcbi.1005229.e006"><alternatives><graphic id="pcbi.1005229.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1005229.e007"><alternatives><graphic id="pcbi.1005229.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is the two-dimensional Gaussian density function, <inline-formula id="pcbi.1005229.e008"><alternatives><graphic id="pcbi.1005229.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is the two-dimensional location of a particular syllable category and <inline-formula id="pcbi.1005229.e009"><alternatives><graphic id="pcbi.1005229.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is the sum of the category’s variance-covariance matrix and the variance of <italic>X</italic><sub>AV</sub>. For simplicity, we have assumed that all syllables have equal prior probability, all locations within the representational space have equal prior probability, and that the category variance-covariance matrices are equal. The resulting linear decision boundaries are shown in <xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1E</xref>.</p>
<p>Across many trials, we can calculate the model responses for a given audiovisual stimulus. The frequency of each percept across simulated trials is tallied (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1F</xref>) and the percentage of each percept is compared with behavioral data from human subjects (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1G</xref>). All model simulations were done in R [<xref ref-type="bibr" rid="pcbi.1005229.ref015">15</xref>], multivariate probabilities were calculated using the <italic>mvtnorm</italic> package [<xref ref-type="bibr" rid="pcbi.1005229.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005229.ref017">17</xref>]. Model predictions were generated based on 10,000 samples from each audiovisual syllable tested.</p>
<p>In the CIMS model, rather than assuming that <italic>C</italic> = 1, we take both the <italic>C</italic> = 1 and <italic>C</italic> = 2 representations into consideration, weighting them by their likelihood. For <italic>C</italic> = 1, the representation is the same as for the non-CIMS model (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2A</xref>). For <italic>C</italic> = 2, the representation is simply the encoded representation of the auditory portion of the stimulus; this is reasonable because most incongruent pairings of auditory and visual speech result in perception of the auditory syllable [<xref ref-type="bibr" rid="pcbi.1005229.ref013">13</xref>]. Next, we calculate the log posterior ratio of <italic>C</italic> = 1 to <italic>C = 2</italic>:</p>
<fig id="pcbi.1005229.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005229.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Modeling of multisensory speech perception with causal inference.</title>
<p>(A) For the McGurk stimulus AbaVga, two causal structures are considered. If auditory and visual speech emanate from a single talker (<italic>C</italic> = 1), the integrated representation is calculated as in <xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1<bold><italic>D</italic></bold></xref>. If auditory and visual speech emanate from two talkers (<italic>C</italic> = 2) the representation is assumed to be the auditory component of the stimulus (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1<bold><italic>C</italic></bold></xref>). (B) The likelihood of each causal structure is calculated using knowledge about the natural statistics of audiovisual speech (red/blue color scale). Numbers (showing representation of an identical McGurk stimulus on three trials) are centered over the white region, indicating similar probability of <italic>C</italic> = 1 vs. <italic>C</italic> = 2 (ellipse shows 95% probability region across trials). (C) On each trial, a weighted average of the <italic>C</italic> = 1 (AV) and <italic>C</italic> = 2 (A) representations is calculated, using the probabilities from <bold>B</bold>. This shifts the representation so that it is intermediate between the AV and A representations (compare location of numbers with <bold>B</bold>). Red ellipse labelled CIMS shows 95% probability region across all trials. (D) A linear decision rule is applied to categorize the representation (same number and red ellipse as <bold>C</bold>). The CIMS representation is found most often in the “da” region of perceptual space, but sometimes lies in the “ba” region. (E) The model predicts “ba” for 51% of trials and “da” for 49% of trials. (F) Across trials, subjects reported the “ba” percept for 57% of trials and “da” for 40% of trials. (G) Causal inference for the non-McGurk auditory “ga” + visual “ba” (AgaVba) stimulus. The <italic>C</italic> = 1 representation (AV ellipse from <xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1<bold>I</bold></xref>) and the C = 2 representation (A ellipse from <xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1<bold>H</bold></xref>) are calculated. (H) The AV representations have a higher likelihood of originating from <italic>C</italic> = 2 (AV ellipse centered over red region). (I) Across many trials, the CIMS representations (red ellipse labelled CIMS is the 95% probability region) are shifted toward the <italic>C</italic> = 2 percept (A) away from the <italic>C</italic> = 1 representation (AV). (J) Over many trials, the CIMS representation is found most often in the “ga” region of perceptual space (same red ellipse as <bold>I</bold>). (K) The CIMS model primarily predicts “ga” (97%) for the non-McGurk AgaVba stimulus. (L) Behavioral data from 60 subjects reporting their perception of auditory “ga” + visual “ba.” Across trials, subjects reported “ga” for 96% of trials.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005229.g002" xlink:type="simple"/>
</fig>
<disp-formula id="pcbi.1005229.e010">
<alternatives>
<graphic id="pcbi.1005229.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e010" xlink:type="simple"/>
<mml:math display="block" id="M10">
<mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>The prior probability of a common cause, P(<italic>C</italic> = 1), is set to 0.50 (giving no prior bias toward one or two causes), resulting in P(<italic>C</italic> = 2) = 0.50. We calculate P(<italic>X</italic><sub>A</sub>,<italic>X</italic><sub><italic>V</italic></sub>|<italic>C</italic> = 1) by looking at each syllable, <italic>S</italic><sub><italic>i</italic></sub>, individually. These probabilities are then combined, weighted by their respective prior probabilities (assumed to be equal) to determine the overall conditional probability:
<disp-formula id="pcbi.1005229.e011">
<alternatives>
<graphic id="pcbi.1005229.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>We calculate P(<italic>X</italic><sub>A</sub>,<italic>X</italic><sub><italic>V</italic></sub>|<italic>C</italic> = 2) using a similar process for all possible incongruent syllable combinations. Their locations in representational space are determined as
<disp-formula id="pcbi.1005229.e012">
<alternatives>
<graphic id="pcbi.1005229.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e012" xlink:type="simple"/>
<mml:math display="block" id="M12">
<mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup>
</mml:math>
</alternatives>
</disp-formula>
<italic>X</italic><sub><italic>A</italic></sub> and <italic>X</italic><sub><italic>V</italic></sub> are the locations for the unisensory components. The matrices <italic>Σ</italic><sub>A′</sub> and <italic>Σ</italic><sub>V′</sub> are the original sensory noise matrices plus the variance of the syllable category that generated the exemplar. The probabilities are then calculated using the two-dimensional Gaussian density: <inline-formula id="pcbi.1005229.e013"><alternatives><graphic id="pcbi.1005229.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> and weighted by their prior probabilities (assumed to be equal).</p>
<p>After the decision variable <italic>d</italic> is computed, we convert it into the probability of each causal structure (shown for all positions in the representational space in <xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2B</xref>):
<disp-formula id="pcbi.1005229.e014">
<alternatives>
<graphic id="pcbi.1005229.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e014" xlink:type="simple"/>
<mml:math display="block" id="M14">
<mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
</disp-formula></p>
<p>The next step is to combine the <italic>C</italic> = 1 and <italic>C</italic> = 2 representations, weighted by their likelihood (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2C</xref>):
<disp-formula id="pcbi.1005229.e015">
<alternatives>
<graphic id="pcbi.1005229.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005229.e015" xlink:type="simple"/>
<mml:math display="block" id="M15">
<mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub>
</mml:math>
</alternatives>
</disp-formula>
This combination is done on a trial-by-trial basis, producing a non-linear combination of the original noisily-encoded exemplars (<italic>X</italic><sub>A</sub> and <italic>X</italic><sub>V</sub>). Across many trials the distribution of encoded exemplars is not multivariate Gaussian (an ellipse), but fitting a two-dimensional Gaussian to the exemplars provides a useful visual approximation for their distribution, depicting how the causal inference representations are intermediate between the <italic>C</italic> = 1 (AV) and <italic>C</italic> = 2 (A) representations (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2C</xref>). For each trial, this representation is categorized into a percept using the same linear decision rule as for the model without causal inference (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2D</xref>). The CIMS model responses across trials are computed (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2E</xref>) and compared with human behavioral data (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2F</xref>).</p>
<sec id="sec003">
<title>Behavioral data</title>
<p>To test the predictions of each model, we collected syllable recognition data from 60 participants (18 female; mean age = 33) for 9 stimuli: all possible combinations of auditory and visual “ba”, “da” and “ga” from a single talker. Participants were recruited using Amazon Mechanical Turk, using an identical design and procedure as a previous study of McGurk perception [<xref ref-type="bibr" rid="pcbi.1005229.ref018">18</xref>]. Each stimulus was repeated 10 times in a randomized order. Participants reported their percept of each audiovisual syllable by selecting from among: “ba”, “da/tha”, and “ga”. Performance on congruent syllables was at or near ceiling for all subjects (mean accuracy = 97%; range 87% to 100%).</p>
</sec>
</sec>
<sec id="sec004" sec-type="results">
<title>Results</title>
<p>We constructed two similar computational models of audiovisual speech perception that differed only in whether they did (CIMS) or did not (non-CIMS) incorporate causal inference. While the non-CIMS model assumes a common cause, the CIMS model estimates the likelihood of common <italic>vs</italic>. separate causes for each trial (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1A</xref>). We calculated the predictions of the models for two types of incongruent speech, McGurk syllables and non-McGurk syllables, and compared the predictions with behavioral data collected from human subjects.</p>
<p>First, we examine the non-CIMS model. A McGurk stimulus consisting of an auditory “ba” and visual “ga” is represented in the model by a two-dimensional representational space in which the x-axis represents visual information and the y-axis represents auditory information (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1B</xref>). The model assumes that the auditory “ba” and visual “ga” emanate from a single talker (<italic>C</italic> = 1) and integrates the unisensory cues according to Bayesian principles, with each cue weighted by its reliability. This results in an integrated audiovisual representation that is closer to “ba” along the auditory axis and closer to “ga” along the visual axis (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1D</xref>). After applying a linear categorization rule, the multisensory percept lies in the “da” region of perceptual space (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1E</xref>). Across multiple trials, identical physical stimuli are encoded differently because of sensory noise, resulting in a distribution of representations. Because speech is categorical, this variability can still lead to identical percepts; in this case, the model predicts a percept of “da” on &gt;99% of trials and “ba” or “ga” on &lt;1% of trials (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1F</xref>). Comparing this result to behavioral data collected from 60 human subjects (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1G</xref>) shows a poor correspondence. Subjects reported the “da” percept on only 40% of trials, and the “ba” percept on 57% of trials.</p>
<p>The non-CIMS model’s correspondence with human perception was even worse for a non-McGurk syllable consisting of an auditory “ga” and visual “ba” (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1K and 1L</xref>). This stimulus is the reverse of the McGurk syllable (auditory “ba” and visual “ga”) so that when the two unisensory cues were integrated according to Bayesian principles, the audiovisual representation was placed in a different region of perceptual space than the McGurk syllable (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1I</xref>). However, applying the linear categorization rule meant that the model still classified the percept as “da” (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1J</xref>) on &gt;99% of trials (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1K</xref>). This prediction was inaccurate, as human subjects reported “da” on 2% of trials, instead reporting “ga” on 96% of trials (<xref ref-type="fig" rid="pcbi.1005229.g001">Fig 1L</xref>).</p>
<p>Next, we examined the predictions of the CIMS model. The noisy encoding, Bayesian integration, and categorization modules were identical to the non-CIMS model, but an additional step of causal inference was performed. For any given speech stimulus, the true causal structure is unknown, meaning that the optimal strategy is to estimate the <italic>C</italic> = 1 and <italic>C</italic> = 2 representations (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2A</xref>) and combine them, weighted by their probabilities.</p>
<p>For a McGurk stimulus, the audiovisual representation lies in a region in which <italic>C</italic> = 1 and <italic>C</italic> = 2 are equally likely (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2B</xref>). This results in a CIMS representation located between the <italic>C</italic> = 1 (audiovisual, “da”) and <italic>C</italic> = 2 representations (auditory, “ba”) (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2C</xref>). Applying the categorization rule to the representations generated across many trials (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2D</xref>), the CIMS model predicts a mixture of “ba” percepts (51%) and “da” percepts (49%; <xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2E</xref>). This prediction is a good match to the behavioral data (57% “ba”, 40% “ba”; <xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2F</xref>).</p>
<p>For a non-McGurk stimulus (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2G</xref>), the audiovisual representation is much more likely to be obtained from the <italic>C</italic> = 2 than the <italic>C</italic> = 1 distribution (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2H</xref>). Therefore, the output of the causal inference step is much more strongly weighted towards the <italic>C</italic> = 2 representation (auditory, “ga”) than the <italic>C</italic> = 1 representation (audiovisual, “da”) (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2I</xref>). Applying the categorization rule to the causal inference representations generated across many trials (<xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2J</xref>), the model predicts predominantly “ga” percepts (97%; <xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2K</xref>) as was observed in the behavioral data (96% “ga”; <xref ref-type="fig" rid="pcbi.1005229.g002">Fig 2L</xref>).</p>
<sec id="sec005">
<title>Testing with other syllable combinations</title>
<p>To assess generalizability, we also tested both CIMS and non-CIMS models with other audiovisual syllables (<xref ref-type="fig" rid="pcbi.1005229.g003">Fig 3</xref>). For congruent syllables (AbaVba, AdaVda, AgaVga) human subjects show little variability, always reporting the syllable that is present in both modalities (<xref ref-type="fig" rid="pcbi.1005229.g003">Fig 3A</xref>). Both the non-CIMS (<xref ref-type="fig" rid="pcbi.1005229.g003">Fig 3B</xref>) and CIMS (<xref ref-type="fig" rid="pcbi.1005229.g003">Fig 3C</xref>) models predict this behavior with excellent fidelity (correlation between behavior and model prediction <italic>r</italic> ≈ 1 for both). However, differences between the predictions of the two models appear when they are tested with incongruent audiovisual syllables, with only the CIMS model able to accurately predict human perception. Across six different incongruent syllables, the CIMS model showed a significantly stronger correlation with human perception (correlation between behavioral and model: CIMS <italic>r</italic> = 0.95 <italic>vs</italic>. non-CIMS <italic>r</italic> = 0.21, <italic>z</italic> = 4.3, <italic>p</italic> = 10<sup>−5</sup> from Fisher r-to-z transformation.)</p>
<fig id="pcbi.1005229.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005229.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Generalizability of models tested with other audiovisual syllables.</title>
<p>(A) Behavior for congruent syllables. Each row represents a different congruent audiovisual syllable (AbaVba, AdaVda, AgaVga). Subjects made a three-alternative forced choice (ba, ga, da). The colors within each row show how often subjects reported each choice when presented with each syllable (<italic>e</italic>.<italic>g</italic>. for AbaVba, they always reported “ba”). (B) Non-CIMS model predictions for congruent syllables. Rows show syllables, colors across columns within each row show how often model predicted that percept (darker colors indicate higher percentages). (C) CIMS model predictions for congruent syllables. (D) Behavior for incongruent syllables. Each row represents a different incongruent audiovisual syllable. Subjects made a three-alternative forced choice (ba, ga, da). The colors within each row show how often subjects reported each choice when presented with each syllable (<italic>e</italic>.<italic>g</italic>. for AbaVda, they more often reported “ba”, less often reported “da”, never reported “ga”). (E) Non-CIMS model predictions for incongruent syllables. Rows show syllables, colors across columns within each row show how often model predicted that percept (darker colors indicate higher percentages). (F) CIMS model predictions for incongruent syllables.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005229.g003" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec006" sec-type="conclusions">
<title>Discussion</title>
<p>These results are important because speech is the most important form of human communication and is fundamentally multisensory, making use of both visual information from the talker’s face and the auditory information from the talker’s voice. In everyday situations we are frequently confronted with multiple talkers emitting auditory and visual speech cues, and the brain must decide whether or not to integrate a particular combination of voice and face. The best known laboratory example of this situation is the McGurk effect, in which an incongruent auditory “ba” and visual “ga” are fused to result in the percept “da”. This simple yet powerful illusion has been used in thousands of studies, ranging from developmental to clinical, to intercultural. However, there has been no clear theoretical understanding of why the McGurk effect occurs for some incongruent syllables (e.g. AbaVga) but not others (e.g. AgaVba). Some process must be operating that distinguishes between incongruent audiovisual speech that <italic>should</italic> and <italic>should not</italic> be integrated. A quantitative framework for this process is provided by causal inference.</p>
<p>We constructed two similar computational models of audiovisual speech perception. The CIMS and non-CIMS models, although identical in every respect except for the inclusion of causal inference, generated very different predictions about the perception of incongruent syllables. When tested with McGurk and inverse-McGurk incongruent syllables, the non-CIMS model predicted exclusively “da” responses. This was an inaccurate description of the perceptual reports of human subjects, who reported distinct percepts for the two types (mixture of “da” and “ba” for McGurk syllables <italic>vs</italic>. exclusively “ga” for inverse-McGurk syllables). In contrast, the CIMS model successfully reproduced this pattern of perceptual reports. In a test of generalizability, the CIMS model also was able to better predict perception than the non-CIMS model for six other incongruent audiovisual syllables.</p>
<p>The comparison between CIMS and non-CIMS models was fair because (except for causal inference) they were identical across all model steps, including the layout of the representational space, the amount of encoding noise, the rule used to integrate auditory and visual cues, and the rule used to categorize the final representations. We did not explicitly optimize the model parameters to the behavioral data for either model, simply choosing the values heuristically or setting them at plausible defaults (e.g., flat priors). Thus, any difference in performance between the CIMS and non-CIMS models is attributable to causal inference, not to over-fitting (the CIMS model has only one extra free parameter). There is no way for the non-CIMS model to accurately predict behavior for both McGurk and inverse McGurk stimuli without imposing arbitrary rules about integration, which is, after all, the rationale for considering causal inference in the first place.</p>
<p>Similarly, the precise stimuli and subjects used to generate the behavioral data was also not a key factor in the better performance of the CIMS model. Although there is variability in the frequency with which different subjects report the illusory McGurk percept and the efficacy of different stimuli in evoking it [<xref ref-type="bibr" rid="pcbi.1005229.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1005229.ref019">19</xref>], we are not aware of any reports of the inverse McGurk stimuli evoking an illusory percept, as predicted by the non-CIMS model. Without a way to predict integration for some combination and segregation for others, the non-CIMS model simply cannot replicate the observed pattern of human syllable recognition.</p>
<sec id="sec007">
<title>Model predictions</title>
<p>A key reason for creating models of cognitive processes is to generate testable predictions. The CIMS model successfully predicted perception for arbitrary combinations of three auditory and visual syllables (“ba”, “da”, and “ga”). By extending the representational space to consider other factors (e.g., voice onset time) the CIMS model could predict perception for any combination of auditory and visual syllables.</p>
<p>The CIMS model is also extensible to other cues that provides information about the causal structure of the stimulus. For the incongruent speech considered in the present paper, the main cue for causal inference is the content of the auditory and visual speech. However, there are other useful cues that can be used to estimate whether auditory and visual speech emanate from the same talker, especially the temporal disparity between auditory and visual speech [<xref ref-type="bibr" rid="pcbi.1005229.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1005229.ref023">23</xref>]. As the delay between auditory and visual speech is increased, observers are more likely to judge that they emanate from different talkers [<xref ref-type="bibr" rid="pcbi.1005229.ref024">24</xref>]. Causal inference predicts that observers should be less likely to integrate incongruent auditory and visual speech at high temporal disparity than at low disparity, and this is indeed the case: the McGurk percept of “da” for AbaVga stimuli is reported less frequently as temporal disparity increases [<xref ref-type="bibr" rid="pcbi.1005229.ref025">25</xref>–<xref ref-type="bibr" rid="pcbi.1005229.ref027">27</xref>]. This phenomenon could be incorporated into the CIMS model by adding an additional dimension to the common-cause computation, allowing for independent estimates of P(<italic>C</italic> = 1) for any given speech content disparity or temporal disparity. A similar extension would be possible for the different syllable exemplars of the same syllable combination generated from different talkers. One talker’s “ga” might provide more or less visual speech information than another talker’s, driving P(<italic>C</italic> = 1) and the frequency of the McGurk effect higher or lower. Some evidence for this idea is supported by data showing that detection of audiovisual incongruence is correlated with McGurk perception [<xref ref-type="bibr" rid="pcbi.1005229.ref028">28</xref>].</p>
<p>A key direction for future research will also be a better understanding of the neural mechanisms underlying causal inference in speech perception. The CIMS model predicts that the ultimate percept of multisensory speech results from a combination of the <italic>C</italic> = 1 (AV) and <italic>C</italic> = 2 (A) representations. This requires that the brain must contain distinct neural signatures of both <italic>C</italic> = 1 and <italic>C</italic> = 2 representations. In an audiovisual localization task, there is fMRI evidence for <italic>C</italic> = 2 representations in early sensory cortex and a <italic>C</italic> = 1 representation in the intraparietal sulcus [<xref ref-type="bibr" rid="pcbi.1005229.ref029">29</xref>]. For multisensory speech, the <italic>C</italic> = 1 (AV) representation is most likely represented in the superior temporal sulcus (STS) because interrupting activity in the STS interferes with perception of the McGurk effect [<xref ref-type="bibr" rid="pcbi.1005229.ref030">30</xref>] and the amplitude of STS activity measured with fMRI predicts McGurk perception in adults [<xref ref-type="bibr" rid="pcbi.1005229.ref031">31</xref>] and children [<xref ref-type="bibr" rid="pcbi.1005229.ref032">32</xref>].</p>
</sec>
<sec id="sec008">
<title>Relationship with other models</title>
<p>McGurk and MacDonald offered a descriptive word model of the illusion, stating that for AbaVga “there is visual information for [ga] and [da] and auditory information with features common to [da] and [ba]. By responding to the common information in both modalities, a subject would arrive at the unifying percept [da].” [<xref ref-type="bibr" rid="pcbi.1005229.ref004">4</xref>]. The CIMS model differs from this word model in several fundamental respects. Unlike the word model, the CIMS model is both quantitative, allowing for precise numerical predictions about behavior, and probabilistic, allowing percepts to vary from trial-to-trial of the identical stimuli as well as across different stimuli and observers. This allows it to more accurately describe actual human perception. For instance, the word model predicts that every AbaVga stimulus will be perceived as “da” while behavioral data show a wide range in efficacy for different AbaVga stimuli [<xref ref-type="bibr" rid="pcbi.1005229.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1005229.ref019">19</xref>].</p>
<p>The fuzzy logical model of perception (FLMP) as developed by Massaro [<xref ref-type="bibr" rid="pcbi.1005229.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005229.ref033">33</xref>] was an important advance because it was one of the first probabilistic models, allowing successive presentations of identical stimuli to produce different percepts. However, because the FLMP does not explicitly model the processes underlying perception, it has no way to separate stimulus variability (the location in representational space for CIMS) and sensory noise (the ellipse describing the distribution of encoded representations for CIMS).</p>
<p>Another important model of the McGurk effect uses predictive coding [<xref ref-type="bibr" rid="pcbi.1005229.ref008">8</xref>]. While the representational space in this model is somewhat similar to that of the CIMS model, the predictive coding model is a multi-level network model that allows for dynamic prediction of perception as evidence from different sensory modalities arrives asynchronously. However, because it does not incorporate sensory noise it cannot account for trial-to-trial differences in perception of identical stimuli.</p>
<p>The noisy encoding of disparity (NED) model used three parameters to account for trial-to-trial differences in perception [<xref ref-type="bibr" rid="pcbi.1005229.ref006">6</xref>]. However, the NED model predicts only one of two pre-specified percepts resulting from either the presence or absence of integration. The CIMS model is a significant advance over the NED model because it allows for a continuous variation along the axis from complete integration to complete segregation, and can thus produce the percept of any stimulus within the representational space.</p>
</sec>
<sec id="sec009">
<title>Other variables impacting causal inference</title>
<p>The role of causal inference in multisensory speech has been previously considered within the context of a synchrony judgment task [<xref ref-type="bibr" rid="pcbi.1005229.ref024">24</xref>]. Although this model is a Bayesian causal inference model, it has only superficial similarity to the current model. The earlier model focused on how an observer could use causal inference to decide if two signals produced at distinct points in time were generated from the same talker. The input to the model was thus a fixed asynchrony and the output a binary judgment of synchronous <italic>vs</italic>. asynchronous. In contrast, the current model does not consider the temporal relationship between the auditory and visual syllables, but rather is concerned only with their content and outputs a perceived syllable. In principle a more complicated that considers content and temporal disparity jointly.</p>
<p>Previous research has shown that not all kinds of disparity are used to determine how much to integrate auditory and visual speech streams. Previous researchers have created McGurk stimuli in which the auditory sounds and visual faces were from talkers with different genders [<xref ref-type="bibr" rid="pcbi.1005229.ref034">34</xref>]. Even though participants were able to identify this discrepancy, the McGurk effect was not diminished compared to stimuli with talkers from the same gender. This study highlights the important distinction between the perceived speech, and the judgment of a common cause <italic>per se</italic>.</p>
<p>There are at least two ways to square the idea that causal inference is important for integration but ignores information that would seemingly inform the calculation. First, even when the probability of a common cause is low, some weight is still given to the multisensory representation and thus a “da” percept is still possible. For instance, on a given trial where the probability of a common cause is 0.4, the optimal report from the participant is that there are 2 talkers. However, the final representation will still be given a substantial weight and thus the percept could be driven to a fusion response, rather than an auditory report. Second, there may be other, stronger indicators of a common cause that can override a noted discrepancy in a particular feature. For instance, the temporal simultaneity and the spatial compatibility of the auditory and visual tokens may together make the <italic>C</italic> = 1 scenario plausible, despite an apparent mismatch between the perceived gender of the auditory and visual speech. Well-studied phenomenon like the ventriloquist illusion [<xref ref-type="bibr" rid="pcbi.1005229.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1005229.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1005229.ref036">36</xref>] provide strong evidence that temporal cues elicit strong control of integration despite spatial disparity that would otherwise indicate separate talkers. These options are not mutually exclusive. A study of how temporal synchrony affect McGurk perception suggest both explanations may be at play. In one study, researchers measured perception of synchrony and perceived speech using both congruent and McGurk stimuli [<xref ref-type="bibr" rid="pcbi.1005229.ref027">27</xref>]. Reported synchrony was lower for McGurk stimuli than for congruent stimuli, consistent with the use of a combined disparity measure. In a separate study, the McGurk effect was perceived at asynchronies these same subjects judge to be asynchronous, and ostensibly not uttered by the same talker [<xref ref-type="bibr" rid="pcbi.1005229.ref025">25</xref>]. Taken together, these studies show that the general framework of causal inference can be fruitfully explored, and that a major issue for future studies is to determine the relative weighting of stimulus features in estimating the likelihood of a common cause.</p>
</sec>
<sec id="sec010">
<title>Generalized causal inference</title>
<p>The CIMS model is a simplification of a full Bayes-optimal causal inference model. For instance, the CIMS calculation of the final percept ignores differences in the prior for different locations within the representational space and the CIMS estimates of the likelihood of each causal structure are calculated using only integrated location (AV) rather than the joint distribution of the individual cues (A and V).</p>
<p>While our results suggest that causal inference is a key step in audiovisual speech perception, there are many possible solutions to the general problem of causal inference [<xref ref-type="bibr" rid="pcbi.1005229.ref037">37</xref>]. The CIMS model solves the problem by combining the <italic>C</italic> = 1 and <italic>C</italic> = 2 representations according to their probability (sometimes known as a “weighted average” or “model averaging” approach). A second option is to select the percept that is most likely on each single trial (“winner-take-all” or “model selection”). A third option is to distribute choices between <italic>C</italic> = 1 and <italic>C</italic> = 2 based on their probability (“probability matching”). The categorical nature of speech perception means that a very large stimulus set (much larger than that used in the present study) would be needed to determine which solution or solutions is used for audiovisual speech perception by human subjects. Here, we focused on model averaging because it was shown to be successful in previous studies of audiovisual perception in human subjects [<xref ref-type="bibr" rid="pcbi.1005229.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1005229.ref024">24</xref>].</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>The authors are grateful to Genevera Allen and Xaq Pitkow for helpful discussions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005229.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knill</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>The Bayesian brain: the role of uncertainty in neural coding and computation</article-title>. <source>Trends Neurosci</source>. <year>2004</year>;<volume>27</volume>(<issue>12</issue>):<fpage>712</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2004.10.007" xlink:type="simple">10.1016/j.tins.2004.10.007</ext-link></comment> <object-id pub-id-type="pmid">15541511</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Ross</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Foxe</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Parra</surname> <given-names>LC</given-names></name>. <article-title>Lip-reading aids word recognition most in moderate noise: a Bayesian explanation using high-dimensional feature space</article-title>. <source>PLoS One</source>. <year>2009</year>;<volume>4</volume>(<issue>3</issue>):<fpage>e4638</fpage>. Epub 2009/03/05. PubMed Central PMCID: PMC2645675. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0004638" xlink:type="simple">10.1371/journal.pone.0004638</ext-link></comment> <object-id pub-id-type="pmid">19259259</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sumby</surname> <given-names>WH</given-names></name>, <name name-style="western"><surname>Pollack</surname> <given-names>I</given-names></name>. <article-title>Visual contribution to speech intelligibility in noise</article-title>. <source>J Acoust Soc Am</source>. <year>1954</year>;<volume>26</volume>(<issue>2</issue>):<fpage>212</fpage>–<lpage>5</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005229.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McGurk</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>MacDonald</surname> <given-names>J</given-names></name>. <article-title>Hearing lips and seeing voices</article-title>. <source>Nature</source>. <year>1976</year>;<volume>264</volume>(<issue>5588</issue>):<fpage>746</fpage>–<lpage>8</lpage>. <object-id pub-id-type="pmid">1012311</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bejjanki</surname> <given-names>VR</given-names></name>, <name name-style="western"><surname>Clayards</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Knill</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Aslin</surname> <given-names>RN</given-names></name>. <article-title>Cue integration in categorical tasks: insights from audio-visual speech perception</article-title>. <source>PLoS One</source>. <year>2011</year>;<volume>6</volume>(<issue>5</issue>):<fpage>e19812</fpage>. Epub 2011/06/04. PubMed Central PMCID: PMC3102664. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0019812" xlink:type="simple">10.1371/journal.pone.0019812</ext-link></comment> <object-id pub-id-type="pmid">21637344</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Magnotti</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Beauchamp</surname> <given-names>MS</given-names></name>. <article-title>The noisy encoding of disparity model of the McGurk effect</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2015</year>;<volume>22</volume>(<issue>3</issue>):<fpage>701</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005229.ref007"><label>7</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Massaro</surname> <given-names>DW</given-names></name>. <chapter-title>Perceiving talking faces: from speech perception to a behavioral principle</chapter-title>. <publisher-loc>Cambridge, Mass.</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1998</year>. <fpage>xii</fpage>, 494 p. p.</mixed-citation></ref>
<ref id="pcbi.1005229.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olasagasti</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bouton</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Giraud</surname> <given-names>A-L</given-names></name>. <article-title>Prediction across sensory modalities: A neurocomputational model of the McGurk effect</article-title>. <source>Cortex</source>. <year>2015</year>;<volume>68</volume>:<fpage>61</fpage>–<lpage>75</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cortex.2015.04.008" xlink:type="simple">10.1016/j.cortex.2015.04.008</ext-link></comment> <object-id pub-id-type="pmid">26009260</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname> <given-names>JL</given-names></name>. <article-title>A reanalysis of McGurk data suggests that audiovisual fusion in speech perception is subject-dependent</article-title>. <source>J Acoust Soc Am</source>. <year>2010</year>;<volume>127</volume>(<issue>3</issue>):<fpage>1584</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1121/1.3293001" xlink:type="simple">10.1121/1.3293001</ext-link></comment> <object-id pub-id-type="pmid">20329858</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kording</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Quartz</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Causal inference in multisensory perception</article-title>. <source>PLoS One</source>. <year>2007</year>;<volume>2</volume>(<issue>9</issue>):<fpage>e943</fpage>. Epub 2007/09/27. PubMed Central PMCID: PMC1978520. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0000943" xlink:type="simple">10.1371/journal.pone.0000943</ext-link></comment> <object-id pub-id-type="pmid">17895984</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>UR</given-names></name>. <article-title>Causal inference in perception</article-title>. <source>Trends Cogn Sci</source>. <year>2010</year>;<volume>14</volume>(<issue>9</issue>):<fpage>425</fpage>–<lpage>32</lpage>. Epub 2010/08/14. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2010.07.001" xlink:type="simple">10.1016/j.tics.2010.07.001</ext-link></comment> <object-id pub-id-type="pmid">20705502</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schutz</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kubovy</surname> <given-names>M</given-names></name>. <article-title>Causality and cross-modal integration</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>2009</year>;<volume>35</volume>(<issue>6</issue>):<fpage>1791</fpage>–<lpage>810</lpage>. Epub 2009/12/09. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0016455" xlink:type="simple">10.1037/a0016455</ext-link></comment> <object-id pub-id-type="pmid">19968437</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>MacDonald</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>McGurk</surname> <given-names>H</given-names></name>. <article-title>Visual influences on speech perception processes</article-title>. <source>Percept Psychophys</source>. <year>1978</year>;<volume>24</volume>(<issue>3</issue>):<fpage>253</fpage>–<lpage>7</lpage>. <object-id pub-id-type="pmid">704285</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liberman</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Delattre</surname> <given-names>PC</given-names></name>, <name name-style="western"><surname>Cooper</surname> <given-names>FS</given-names></name>, <name name-style="western"><surname>Gerstman</surname> <given-names>LJ</given-names></name>. <article-title>The role of consonant-vowel transitions in the perception of the stop and nasal consonants</article-title>. <source>Psychological Monographs: General and Applied</source>. <year>1954</year>;<volume>68</volume>(<issue>8</issue>):<fpage>1</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005229.ref015"><label>15</label><mixed-citation publication-type="book" xlink:type="simple"><collab>R Core Team</collab>. <chapter-title>R: A language and environment for statistical computing</chapter-title>. <publisher-loc>Vienna, Austria</publisher-loc>: <publisher-name>R Foundation for Statistical Computing</publisher-name>; <year>2015</year>.</mixed-citation></ref>
<ref id="pcbi.1005229.ref016"><label>16</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Genz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bretz</surname> <given-names>F</given-names></name>. <chapter-title>Computation of Multivariate Normal and t Probabilities</chapter-title>. <source>Lecture Notes in Statistics</source>. <volume>195</volume>. <publisher-loc>Heidelberg</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>; <year>2009</year>.</mixed-citation></ref>
<ref id="pcbi.1005229.ref017"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Genz A, Bretz F, Miwa T, Mi X, Leisch F, Scheipl F, et al. mvtnorm: Multivariate Normal and t Distributions. R package version 1.0–5. 2016.</mixed-citation></ref>
<ref id="pcbi.1005229.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Basu Mallick</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Magnotti</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Beauchamp</surname> <given-names>MS</given-names></name>. <article-title>Variability and stability in the McGurk effect: contributions of participants, stimuli, time, and response type</article-title>. <source>Psychonomic bulletin &amp; review</source>. <year>2015</year>:<fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005229.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jiang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bernstein</surname> <given-names>LE</given-names></name>. <article-title>Psychophysics of the McGurk and other audiovisual speech integration effects</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>2011</year>;<volume>37</volume>(<issue>4</issue>):<fpage>1193</fpage>–<lpage>209</lpage>. PubMed Central PMCID: PMC3149717. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0023100" xlink:type="simple">10.1037/a0023100</ext-link></comment> <object-id pub-id-type="pmid">21574741</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chandrasekaran</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Trubanova</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Stillittano</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Caplier</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ghazanfar</surname> <given-names>AA</given-names></name>. <article-title>The natural statistics of audiovisual speech</article-title>. <source>PLoS Comput Biol</source>. <year>2009</year>;<volume>5</volume>(<issue>7</issue>):<fpage>e1000436</fpage>. Epub 2009/07/18. PubMed Central PMCID: PMC2700967. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000436" xlink:type="simple">10.1371/journal.pcbi.1000436</ext-link></comment> <object-id pub-id-type="pmid">19609344</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Conrey</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pisoni</surname> <given-names>DB</given-names></name>. <article-title>Auditory-visual speech perception and synchrony detection for speech and nonspeech signals</article-title>. <source>J Acoust Soc Am</source>. <year>2006</year>;<volume>119</volume>(<issue>6</issue>):<fpage>4065</fpage>–<lpage>73</lpage>. Epub 2006/07/15. PubMed Central PMCID: PMC3314884. <object-id pub-id-type="pmid">16838548</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dixon</surname> <given-names>NF</given-names></name>, <name name-style="western"><surname>Spitz</surname> <given-names>L</given-names></name>. <article-title>The detection of auditory visual desynchrony</article-title>. <source>Perception</source>. <year>1980</year>;<volume>9</volume>(<issue>6</issue>):<fpage>719</fpage>–<lpage>21</lpage>. <object-id pub-id-type="pmid">7220244</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vroomen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Keetels</surname> <given-names>M</given-names></name>. <article-title>Perception of intersensory synchrony: a tutorial review</article-title>. <source>Atten Percept Psychophys</source>. <year>2010</year>;<volume>72</volume>(<issue>4</issue>):<fpage>871</fpage>–<lpage>84</lpage>. Epub 2010/05/04. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/APP.72.4.871" xlink:type="simple">10.3758/APP.72.4.871</ext-link></comment> <object-id pub-id-type="pmid">20436185</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Magnotti</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Beauchamp</surname> <given-names>MS</given-names></name>. <article-title>Causal inference of asynchronous audiovisual speech</article-title>. <source>Frontiers in Psychology</source>. <year>2013</year>;<volume>4</volume>:<fpage>798</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fpsyg.2013.00798" xlink:type="simple">10.3389/fpsyg.2013.00798</ext-link></comment> <object-id pub-id-type="pmid">24294207</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Soto-Faraco</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Alsius</surname> <given-names>A</given-names></name>. <article-title>Deconstructing the McGurk–MacDonald illusion</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>2009</year>;<volume>35</volume>(<issue>2</issue>):<fpage>580</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0013483" xlink:type="simple">10.1037/a0013483</ext-link></comment> <object-id pub-id-type="pmid">19331510</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Munhall</surname> <given-names>KG</given-names></name>, <name name-style="western"><surname>Gribble</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sacco</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ward</surname> <given-names>M</given-names></name>. <article-title>Temporal constraints on the McGurk effect</article-title>. <source>Perception &amp; psychophysics</source>. <year>1996</year>;<volume>58</volume>(<issue>3</issue>):<fpage>351</fpage>–<lpage>62</lpage>. Epub 1996/04/01. <object-id pub-id-type="pmid">8935896</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Wassenhove</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Grant</surname> <given-names>KW</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>. <article-title>Temporal window of integration in auditory-visual speech perception</article-title>. <source>Neuropsychologia</source>. <year>2007</year>;<volume>45</volume>(<issue>3</issue>):<fpage>598</fpage>–<lpage>607</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuropsychologia.2006.01.001" xlink:type="simple">10.1016/j.neuropsychologia.2006.01.001</ext-link></comment> <object-id pub-id-type="pmid">16530232</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Strand</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Cooperman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rowe</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Simenstad</surname> <given-names>A</given-names></name>. <article-title>Individual Differences in Susceptibility to the McGurk Effect: Links With Lipreading and Detecting Audiovisual Incongruity</article-title>. <source>Journal of Speech, Language, and Hearing Research</source>. <year>2014</year>;<volume>57</volume>(<issue>6</issue>):<fpage>2322</fpage>–<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1044/2014_JSLHR-H-14-0059" xlink:type="simple">10.1044/2014_JSLHR-H-14-0059</ext-link></comment> <object-id pub-id-type="pmid">25296272</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rohe</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Noppeney</surname> <given-names>U</given-names></name>. <article-title>Cortical hierarchies perform Bayesian causal inference in multisensory perception</article-title>. <source>PLoS Biol</source>. <year>2015</year>;<volume>13</volume>(<issue>2</issue>):<fpage>e1002073</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.1002073" xlink:type="simple">10.1371/journal.pbio.1002073</ext-link></comment> <object-id pub-id-type="pmid">25710328</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beauchamp</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Nath</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Pasalar</surname> <given-names>S</given-names></name>. <article-title>fMRI-Guided transcranial magnetic stimulation reveals that the superior temporal sulcus is a cortical locus of the McGurk effect</article-title>. <source>J Neurosci</source>. <year>2010</year>;<volume>30</volume>(<issue>7</issue>):<fpage>2414</fpage>–<lpage>7</lpage>. Epub 2010/02/19. PubMed Central PMCID: PMC2844713. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4865-09.2010" xlink:type="simple">10.1523/JNEUROSCI.4865-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20164324</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nath</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Beauchamp</surname> <given-names>MS</given-names></name>. <article-title>A neural basis for interindividual differences in the McGurk effect, a multisensory speech illusion</article-title>. <source>NeuroImage</source>. <year>2012</year>;<volume>59</volume>(<issue>1</issue>):<fpage>781</fpage>–<lpage>7</lpage>. Epub 2011/07/27. PubMed Central PMCID: PMC3196040. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2011.07.024" xlink:type="simple">10.1016/j.neuroimage.2011.07.024</ext-link></comment> <object-id pub-id-type="pmid">21787869</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nath</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Fava</surname> <given-names>EE</given-names></name>, <name name-style="western"><surname>Beauchamp</surname> <given-names>MS</given-names></name>. <article-title>Neural correlates of interindividual differences in children's audiovisual speech perception</article-title>. <source>The Journal of neuroscience: the official journal of the Society for Neuroscience</source>. <year>2011</year>;<volume>31</volume>(<issue>39</issue>):<fpage>13963</fpage>–<lpage>71</lpage>. Epub 2011/10/01. PubMed Central PMCID: PMC3203203.</mixed-citation></ref>
<ref id="pcbi.1005229.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Massaro</surname> <given-names>DW</given-names></name>. <article-title>Testing between the TRACE model and the fuzzy logical model of speech perception</article-title>. <source>Cogn Psychol</source>. <year>1989</year>;<volume>21</volume>(<issue>3</issue>):<fpage>398</fpage>–<lpage>421</lpage>. Epub 1989/07/01. <object-id pub-id-type="pmid">2758786</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Green</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Kuhl</surname> <given-names>PK</given-names></name>, <name name-style="western"><surname>Meltzoff</surname> <given-names>AN</given-names></name>, <name name-style="western"><surname>Stevens</surname> <given-names>EB</given-names></name>. <article-title>Integrating speech information across talkers, gender, and sensory modality: female faces and male voices in the McGurk effect</article-title>. <source>Percept Psychophys</source>. <year>1991</year>;<volume>50</volume>(<issue>6</issue>):<fpage>524</fpage>–<lpage>36</lpage>. <object-id pub-id-type="pmid">1780200</object-id></mixed-citation></ref>
<ref id="pcbi.1005229.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jack</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Thurlow</surname> <given-names>WR</given-names></name>. <article-title>Effects of degree of visual association and angle of displacement on the" ventriloquism" effect</article-title>. <source>Perceptual and motor skills</source>. <year>1973</year>.</mixed-citation></ref>
<ref id="pcbi.1005229.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Warren</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Welch</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>McCarthy</surname> <given-names>TJ</given-names></name>. <article-title>The role of visual-auditory “compellingness” in the ventriloquism effect: Implications for transitivity among the spatial senses</article-title>. <source>Perception &amp; Psychophysics</source>. <year>1981</year>;<volume>30</volume>(<issue>6</issue>):<fpage>557</fpage>–<lpage>64</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005229.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wozny</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>UR</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Probability matching as a computational strategy used in perception</article-title>. <source>PLoS Comput Biol</source>. <year>2010</year>;<volume>6</volume>(<issue>8</issue>). Epub 2010/08/12. PubMed Central PMCID: PMC2916852. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000871" xlink:type="simple">10.1371/journal.pcbi.1000871</ext-link></comment> <object-id pub-id-type="pmid">20700493</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>