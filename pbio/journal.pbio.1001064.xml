<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id><journal-id journal-id-type="pmc">plosbiol</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Biology</journal-title></journal-title-group><issn pub-type="ppub">1544-9173</issn><issn pub-type="epub">1545-7885</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PBIOLOGY-D-11-00010</article-id><article-id pub-id-type="doi">10.1371/journal.pbio.1001064</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Medicine</subject>
          <subj-group>
            <subject>Neurology</subject>
            <subj-group>
              <subject>Neuroimaging</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Social and behavioral sciences</subject>
          <subj-group>
            <subject>Information science</subject>
            <subj-group>
              <subject>Information theory</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neurological Disorders</subject>
        </subj-group>
      </article-categories><title-group><article-title>Cracking the Code of Oscillatory Activity</article-title><alt-title alt-title-type="running-head">Cracking the Code of Oscillatory
                    Activity</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Schyns</surname>
            <given-names>Philippe G.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Thut</surname>
            <given-names>Gregor</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Gross</surname>
            <given-names>Joachim</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
      </contrib-group><aff id="aff1">
                <addr-line>Institute of Neuroscience and Psychology, University of Glasgow, Glasgow,
                    United Kingdom</addr-line>
            </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Fahle</surname>
            <given-names>Manfred</given-names>
          </name>
          <role>Academic Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Bremen University, Germany</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">philippe.schyns@glasgow.ac.uk</email></corresp>
        <fn fn-type="con">
          <p>The author(s) have made the following declarations about their contributions:
                        Conceived and designed the experiments: PGS. Performed the experiments: PGS.
                        Analyzed the data: PGS JG. Contributed reagents/materials/analysis tools:
                        PGS JG. Wrote the paper: PGS JG GT.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>5</month>
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>17</day>
        <month>5</month>
        <year>2011</year>
      </pub-date><volume>9</volume><issue>5</issue><elocation-id>e1001064</elocation-id><history>
        <date date-type="received">
          <day>30</day>
          <month>12</month>
          <year>2010</year>
        </date>
        <date date-type="accepted">
          <day>7</day>
          <month>4</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Schyns et al</copyright-holder><license><license-p>This is an open-access article distributed under the
                terms of the Creative Commons Attribution License, which permits unrestricted use,
                distribution, and reproduction in any medium, provided the original author and
                source are credited.</license-p></license></permissions><related-article id="RA1" issue="5" page="e1001063" related-article-type="companion" vol="9" xlink:href="info:doi/10.1371/journal.pbio.1001063" xlink:type="simple"><article-title/>A Rosetta Stone for Brain Waves</related-article><abstract>
        <p>Neural oscillations are ubiquitous measurements of cognitive processes and
                    dynamic routing and gating of information. The fundamental and so far unresolved
                    problem for neuroscience remains to understand how oscillatory activity in the
                    brain codes information for human cognition. In a biologically relevant
                    cognitive task, we instructed six human observers to categorize facial
                    expressions of emotion while we measured the observers' EEG. We combined
                    state-of-the-art stimulus control with statistical information theory analysis
                    to quantify how the three parameters of oscillations (i.e., power, phase, and
                    frequency) code the visual information relevant for behavior in a cognitive
                    task. We make three points: First, we demonstrate that phase codes considerably
                    more information (2.4 times) relating to the cognitive task than power. Second,
                    we show that the conjunction of power and phase coding reflects detailed visual
                    features relevant for behavioral response—that is, features of facial
                    expressions predicted by behavior. Third, we demonstrate, in analogy to
                    communication technology, that oscillatory frequencies in the brain multiplex
                    the coding of visual features, increasing coding capacity. Together, our
                    findings about the fundamental coding properties of neural oscillations will
                    redirect the research agenda in neuroscience by establishing the differential
                    role of frequency, phase, and amplitude in coding behaviorally relevant
                    information in the brain.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>To recognize visual information rapidly, the brain must continuously code
                    complex, high-dimensional information impinging on the retina, not all of which
                    is relevant, because a low-dimensional code can be sufficient for both
                    recognition and behavior (e.g. a fearful expression can be correctly recognized
                    only from the wide-opened eyes). The oscillatory networks of the brain
                    dynamically reduce the high-dimensional information into a low dimensional code,
                    but it remains unclear which aspects of these oscillations produce the low
                    dimensional code. Here, we measured the EEG of human observers while we
                    presented them with samples of visual information from expressive faces (happy,
                    sad, fear, etc.). Using statistical information theory, we extracted the
                    low-dimensional code that is most informative for correct recognition of each
                    expression (e.g. the opened mouth for “happy,” the wide opened eyes
                    for “fear”). Next, we measured how the three parameters of brain
                    oscillations (frequency, power and phase) code for low-dimensional features.
                    Surprisingly, we find that phase codes 2.4 times more task information than
                    power. We also show that the conjunction of power and phase sufficiently codes
                    the low-dimensional facial features across brain oscillations. These findings
                    offer a new way of thinking about the differential role of frequency, phase and
                    amplitude in coding behaviorally relevant information in the brain.</p>
      </abstract><funding-group><funding-statement>PGS, GT, and JG are supported by Biotechnology and Biological Science Research
                    Council grant BB/I006494/1. PGS is also supported by Economic and Social
                    Research Council and Medical Research Council grant ERSC/MRC-060-25-0010. The
                    funders had no role in study design, data collection and analysis, decision to
                    publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="8"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Invasive and noninvasive studies in humans under physiological and pathological
                conditions converged on the suggestion that the amplitude and phase of neural
                oscillations implement cognitive processes such as sensory representations,
                attentional selection, and dynamical routing/gating of information <xref ref-type="bibr" rid="pbio.1001064-Schnitzler1">[1]</xref>–<xref ref-type="bibr" rid="pbio.1001064-Salinas1">[4]</xref>. Surprisingly,
                most studies have ignored how the temporal dynamics of phase code the sensory
                stimulus, focusing instead on amplitude envelopes (but see <xref ref-type="bibr" rid="pbio.1001064-Busch1">[5]</xref>), relations between amplitude and
                frequency <xref ref-type="bibr" rid="pbio.1001064-Belitski1">[6]</xref>,
                or coupling between frequencies (<xref ref-type="bibr" rid="pbio.1001064-Canolty1">[7]</xref>–<xref ref-type="bibr" rid="pbio.1001064-Hanslmayr1">[10]</xref>; see <xref ref-type="bibr" rid="pbio.1001064-Jensen1">[11]</xref> for a review). But there is compelling evidence that
                phase dynamics of neural oscillations are functionally relevant <xref ref-type="bibr" rid="pbio.1001064-Huxter1">[12]</xref>–<xref ref-type="bibr" rid="pbio.1001064-Montemurro1">[16]</xref>.
                Furthermore, computational arguments suggest that if brain circuits performed
                efficient amplitude-to-phase conversion <xref ref-type="bibr" rid="pbio.1001064-Fries1">[17]</xref>,<xref ref-type="bibr" rid="pbio.1001064-Panzeri1">[18]</xref>, temporal phase coding could
                be advantageous in fundamental operations such as object representation and
                categorization by implementing efficient winner-takes-all algorithms <xref ref-type="bibr" rid="pbio.1001064-Fries1">[17]</xref>, by providing
                robust sensory representations in unreliable environments, and by lending themselves
                to multiplexing, an efficient mechanism to increase coding capacity <xref ref-type="bibr" rid="pbio.1001064-Panzeri1">[18]</xref>,<xref ref-type="bibr" rid="pbio.1001064-Smith1">[19]</xref>. To crack the
                code of oscillatory activity in human cognition, we must tease apart the relative
                contribution of frequency, amplitude, and phase to the coding of behaviorally
                relevant information.</p>
      <p>We instructed six observers to categorize faces according to six basic expressions of
                emotion (“happy,” “fear,” “surprise,”
                “disgust,” “anger,” “sad,” plus
                “neutral”). We controlled visual information, by presenting on each
                trial a random sample of face information—smoothly sampled from the image
                using Gaussian apertures at different spatial frequency bands. The Gaussian
                apertures randomly sampled face parts simultaneously across the two dimensions of
                the image and the third dimension of spatial frequency bands (<xref ref-type="supplementary-material" rid="pbio.1001064.s001">Figure S1</xref>
                illustrates the sampling process for one illustrative trial; <xref ref-type="bibr" rid="pbio.1001064-Gosselin1">[20]</xref>,<xref ref-type="bibr" rid="pbio.1001064-Schyns1">[21]</xref>). We recorded the
                observers' categorization and EEG responses to these samples (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>, Procedure).</p>
      <p>To quantify the relative coding properties of power, phase, and frequency, we used
                state-of-the-art information theoretic methods (Mutual Information,
                    <italic>MI</italic>, which measures the mutual dependence between two variables;
                    <xref ref-type="bibr" rid="pbio.1001064-Magri1">[22]</xref>) and
                computed three different <italic>MI</italic> measurements: between sampled pixel
                information and behavioral responses to each emotion category (correct versus
                incorrect), between EEG responses (for power, phase, and the conjunction of phase
                and power) and behavior, and finally between sampled pixel information and EEG
                response (see <xref ref-type="supplementary-material" rid="pbio.1001064.s002">Figure
                    S2</xref> for the mutual information analysis framework and Computation: Mutual
                Information).</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>First, to characterize the information that the brain processes in the cognitive
                task, for each observer and category, we computed <italic>MI</italic>(Pixel;
                Behavior), the <italic>MI</italic> between the distribution of grey-level values of
                each image pixel (arising from the summed Gaussian masks across spatial frequency
                bands, down-sampled from a 380×240 pixels image to a 38 to 24 image and
                gathered across trials) and equal numbers of correct versus incorrect categorization
                responses. <xref ref-type="fig" rid="pbio-1001064-g001">Figure 1</xref>,
                    <italic>MI</italic>(Pixel; Behavior) illustrates <italic>MI</italic> on a scale
                from 0 to 0.05 bits. High values indicate the face pixels (e.g., forming the mouth
                in “happy”) representing the visual information that the brain must
                process to correctly categorize the stimuli (see <xref ref-type="supplementary-material" rid="pbio.1001064.s003">Figure S3</xref> for a
                detailed example of the computation).</p>
      <fig id="pbio-1001064-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.1001064.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title><italic>MI</italic>(Pixel; Behavior).</title>
          <p>The top rows of faces illustrate, from top to bottom, each expression of the
                        experiment, the color-coded average <italic>MI</italic>
                        (<italic>n</italic> = 6 observers) for each expression
                            (<italic>p</italic>&lt;.01 = .0094 bits, corrected,
                        see * on the scale), an overlay of expression and <italic>MI</italic>
                        for ease of feature interpretation.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001064.g001" xlink:type="simple"/>
      </fig>
      <p>We now compare how the parameters of oscillatory frequency, power, and phase code
                this information in the brain. For each observer, expression, electrode of the
                standard 10–20 position system, and trial, we performed a Time ×
                Frequency decomposition of the signal sampled at 1,024 Hz, with a Morlet wavelet of
                size 5, between −500 and 500 ms around stimulus onset and every 2 Hz between 4
                and 96 Hz. We make three points:</p>
      <p><italic>(a) The conjunction of phase and power (phase&amp;power) codes more
                    information about complex categorization tasks than phase and power on their
                    own.</italic> In <xref ref-type="fig" rid="pbio-1001064-g002">Figure 2</xref>,
                    <italic>MI</italic>(EEG response; Behavior) measures the reduction of
                uncertainty of the brain response, when the behavioral variable correct versus
                incorrect categorization is known. We provide the measure for each electrode of the
                standard 10–20 position system over the Time × Frequency space. Pz, Oz,
                P8, and P7 had highest <italic>MI</italic> values of all electrodes, irrespective of
                whether the brain response considered was power (blue box), phase (green box), or
                the phase&amp;power (red box). The adjacent <italic>MI</italic> scales reveal that
                phase&amp;power was 1.25 times more informative of behavior than phase, itself 2.4
                times more informative than power. Phase&amp;power was 3 times more informative than
                power alone. Henceforth, the analyses focus on these four electrodes and on
                phase&amp;power, the most informative brain measurement for the cognitive task.</p>
      <fig id="pbio-1001064-g002" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.1001064.g002</object-id>
        <label>Figure 2</label>
        <caption>
          <title><italic>MI</italic>(EEG Response; Behavior).</title>
          <p><italic>MI</italic> between behavior and the EEG average response for power,
                        highlighted in the blue box for Pz, P8, P7, and Oz, phase (green box), and
                        phase&amp;power (red box), computed over the Time × Frequency space
                            (<italic>p</italic>&lt;.01  = .0013, see * on
                        the scale).</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001064.g002" xlink:type="simple"/>
      </fig>
      <p><italic>(b) Phase&amp;power codes detailed categorization-relevant features of
                    sensory stimuli. MI</italic>(Pixel; Behavior) revealed that the two eyes and the
                mouth are prominent features of expression discrimination (see <xref ref-type="fig" rid="pbio-1001064-g001">Figure 1</xref>). As explained, with Gaussian masks we
                sampled pixels from the face on each trial. Consequently, for all correct trials of
                an expression category (e.g., “happy”), we can measure at each pixel
                location the mutual information between the distribution of grey-level values of the
                Gaussian masks across trials and each cell of the Time × Frequency brain
                response. <xref ref-type="fig" rid="pbio-1001064-g003">Figure 3</xref> reports
                    <italic>MI</italic>(Pixel; Phase&amp;Power), focusing on Pz, Oz, P8, and P7. The
                red box represents, at 4 Hz and 156 ms, following stimulus onset (a time point
                chosen for its prominence in face coding <xref ref-type="bibr" rid="pbio.1001064-Schyns1">[21]</xref>), the color-coded
                    <italic>MI</italic> value of each face pixel—overlayed on a neutral face
                background for ease of feature interpretation (the yellow box presents mutual
                information at 12 Hz and 156 ms). The scale is the adjacent rainbow colors ranging
                from 0 to 0.03 bits. Electrodes P7 (over left occipito-temporal cortex) and P8 (over
                right occipital-temporal cortex) reveal the highest <italic>MI</italic> to the
                contra-lateral eye (i.e., left eye for P8; right eye for P7). At the same time on Pz
                and Oz, the highest <italic>MI</italic> is to both eyes and to the mouth.</p>
      <fig id="pbio-1001064-g003" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.1001064.g003</object-id>
        <label>Figure 3</label>
        <caption>
          <title>MI(Pixel; Phase&amp;Power).</title>
          <p>For electrode Pz, P8, P7, and Oz, the color-coded pixels overlayed on a
                        neutral face represent the average
                        (<italic>n</italic> = 6) <italic>MI</italic> values for
                        each face pixel and phase&amp;power brain responses (see adjacent scale), at
                        two different temporal frequencies (color-coded yellow and red), 156 ms
                        following stimulus onset
                        (<italic>p</italic>&lt;.0000001 = .01 bits,
                        uncorrected, see * on the scale). The underlying Time × Frequency
                        space generalizes this analysis to each cell, using feature masks (left eye,
                        mouth, right eye) and RGB coding to represent <italic>MI</italic> between
                        combinations of these features (see adjacent schematic faces) and the
                        phase&amp;power EEG response. On Oz, the 4 Hz green strip illustrates high
                            <italic>MI</italic> to the mouth, whereas the 8 to 24 Hz purple cloud
                        represents <italic>MI</italic> to two eyes, indicating multiplexing of
                        feature coding.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001064.g003" xlink:type="simple"/>
      </fig>
      <p>To generalize across Time × Frequency, for ease of presentation, we computed
                three masks extracting pixel locations from the left eye, right eye, and mouth. We
                averaged <italic>MI</italic> values within each mask, independently for each Time
                × Frequency cell. We then color-coded <italic>MI</italic> for each feature in
                RGB color space—red for “right eye,” green for
                “mouth,” and blue for “left eye”; see schematic colored
                faces adjacent to the Time × Frequency plot for complete color coding. The
                broad red (versus blue) cloud on electrode P7 (versus P8) denotes highest
                    <italic>MI</italic> to the right (versus left) eye in this Time ×
                Frequency region, whereas Pz and Oz demonstrate sensitivity to the two eyes (in
                purple) and to the mouth (in green). To conclude, phase&amp;power codes detailed
                categorization-relevant features of the sensory input.</p>
      <p><italic>(c) Phase&amp;power coding is multiplexed across oscillatory
                    frequencies.</italic> Theta (4 Hz) and low beta (12 Hz) on both Oz and Pz
                demonstrate the remarkable multiplexing property of phase&amp;power coding: the idea
                that the brain codes different information in different oscillatory bands. In <xref ref-type="fig" rid="pbio-1001064-g003">Figure 3</xref>, Oz and Pz reveal that
                beta encodes two eyes (see the purple RGB code and the yellow framed faces) when
                theta encodes the mouth (see the green RGB code and the red framed faces).
                Multiplexing is also present to a lesser degree on P8 and P7. <italic>MI</italic>
                values critically depend on the joint distribution of variables (see <xref ref-type="supplementary-material" rid="pbio.1001064.s003">Figure S3</xref>), and
                so we turn to <xref ref-type="fig" rid="pbio-1001064-g004">Figure 4</xref> to
                understand how the variables of phase and power jointly contribute to the coding of
                facial features. <xref ref-type="fig" rid="pbio-1001064-g004">Figure 4</xref>
                develops the red and yellow framed faces of <xref ref-type="fig" rid="pbio-1001064-g003">Figure 3</xref>, for electrode Pz. At 156 ms, at 4 and
                12 Hz, we discretized the distribution of power and phase neural responses in
                3×3 bins—represented in Cartesian coordinates as
                        <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001064.e001" xlink:type="simple"/></inline-formula>. In each bin, we averaged the pixel values leading to this
                range of imaginary numbers. At 12 Hz, what emerges is a phase&amp;power coding of
                the two eyes (in red, between 45 and 90 deg of phase) and an encoding of the mouth
                (in red, between 270 and 315 deg of phase). At 4 Hz, the encoding of mostly the
                mouth and the two eyes (in red) occurs between 90 and 135 deg of phase. The 4 and 12
                Hz colored boxes in <xref ref-type="fig" rid="pbio-1001064-g004">Figure 4</xref>
                therefore illustrate the prominence of phase coding for facial features.</p>
      <fig id="pbio-1001064-g004" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.1001064.g004</object-id>
        <label>Figure 4</label>
        <caption>
          <title>Mutual Information: The complex plane.</title>
          <p>For electrode Pz, the boxes develop the corresponding color-coded boxes in
                            <xref ref-type="fig" rid="pbio-1001064-g003">Figure 3</xref>. The red (4
                        Hz) and yellow (12 Hz) boxes represent the pixel mask values associated with
                        a 3×3 discretization of the distribution of complex numbers. For each
                        box, at 156 ms, for each correct trial we averaged the pixel values leading
                        to this range of imaginary numbers—coded on an arbitrary scale between
                        a low value of yellow (reflecting absence of this pixel in this range) and a
                        high value of red (reflecting presence of this pixel in this range). The
                        yellow box illustrates a phase&amp;power coding of the two eyes (in red)
                        between 45 and 90 deg of phase and a coding of the mouth (in red) between
                        270 and 315 deg of phase. The red box illustrates the coding of all three
                        features (in red) between 90 and 135 deg of phase.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001064.g004" xlink:type="simple"/>
      </fig>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>Here, using the concept of mutual information from Information Theory, we compared
                how the three parameters of neural oscillations (power, phase, and frequency)
                contribute to the coding of information in the biologically relevant cognitive task
                of categorizing facial expressions of emotion. We demonstrated that phase codes 2.4
                times more information about the task than power. The conjunction of power and phase
                (itself 3 times more informative than power) codes specific expressive features
                across different oscillatory bands, a multiplexing that increases coding capacity in
                the brain.</p>
      <p>In general, the relationship between our results on the frequency, power, and phase
                coding of neural oscillations cannot straightforwardly be related to the coding
                properties of more standard measures of the EEG such as event related potentials
                (ERP). However, an identical experimental protocol was run on the N170
                face-sensitive potential <xref ref-type="bibr" rid="pbio.1001064-Schyns1">[21]</xref>,<xref ref-type="bibr" rid="pbio.1001064-VanRijsbergen1">[23]</xref>, but using reverse correlation analyses, not MI. Sensor
                analyses revealed that the N170 ERP initially coded the eye contra-lateral to the
                sensor considered, for all expressions, followed at the N170 peak by a coding of the
                behaviorally relevant information <xref ref-type="bibr" rid="pbio.1001064-Schyns1">[21]</xref>, together with a more detailed coding of features (i.e.,
                with their Higher Spatial Frequencies) at the peak <xref ref-type="bibr" rid="pbio.1001064-VanRijsbergen1">[23]</xref>. Interestingly, distance
                of behaviorally relevant information (e.g., the wide-opened eyes in
                “fearful” versus the mouth in “happy”) to the initially
                coded eye determined the latency of the N170 peak (with the ERP to a
                “happy” face peaking later than to a “fearful” face). ERPs
                confer the advantage of precise timing, leading to precise time course of coding in
                the brain, including phase differences across visual categories. However, we do not
                know whether this coding occurs over one or multiple sources of a network that might
                oscillate at different temporal frequencies (as suggested here between theta and
                beta), for example to code features at different spatial resolutions (as suggested
                in <xref ref-type="bibr" rid="pbio.1001064-Smith1">[19]</xref> and <xref ref-type="bibr" rid="pbio.1001064-Romei1">[24]</xref>). In sum, the
                complex relations between EEG/MEG data, the underlying cortical networks of sources,
                their oscillatory behaviors, and the coding of behaviorally relevant features at
                different spatial resolutions open a new range of fundamental questions. Resolving
                these questions will require integration of existing methods, as none of them is
                singly sufficient.</p>
      <p>In these endeavors, the phase and frequency multiplexing coding properties of neural
                oscillations cannot be ignored.</p>
    </sec>
    <sec id="s4" sec-type="materials|methods">
      <title>Materials and Methods</title>
      <sec id="s4a">
        <title>Participants</title>
        <p>Six observers from Glasgow University, UK, were paid to take part in the
                    experiment. All had normal vision and gave informed consent prior to
                    involvement. Glasgow University Faculty of Information and Mathematical Sciences
                    Ethics Committee provided ethical approval.</p>
      </sec>
      <sec id="s4b">
        <title>Stimuli</title>
        <p>Original face stimuli were gray-scale images of five females and five males taken
                    under standardized illumination, each displaying seven facial expressions. All
                    70 stimuli (normalized for the location of the nose and mouth) complied with the
                    Facial Action Coding System (FACS, <xref ref-type="bibr" rid="pbio.1001064-Ekman1">[25]</xref>) and form part of the
                    California Facial Expressions (CAFE) database <xref ref-type="bibr" rid="pbio.1001064-Dailey1">[26]</xref>. As facial information is
                    represented at multiple spatial scales, on each trial we exposed the visual
                    system to a random subset of Spatial Frequency (SF) information contained within
                    the original face image. To this end, we first decomposed the original image
                    into five non-overlapping SF bands of one octave each (120–60,
                    60–30, 30–15, 15–7.5, and 7.5–3.8 cycles/face, see <xref ref-type="supplementary-material" rid="pbio.1001064.s001">Figure S1</xref>).
                    To each SF band, we then applied a mask punctured with Gaussian apertures to
                    sample SF face information with “bubbles.” These were positioned in
                    random locations trial by trial, approximating a uniform sampling of all face
                    regions across trials. The size of the apertures was adjusted for each SF band,
                    so as to reveal six cycles per face. In addition, the probability of a bubble in
                    each SF band was adjusted so as to maintain constant the total area of face
                    revealed (standard deviations of the bubbles were 0.36, 0.7, 1.4, 2.9, and 5.1
                    cycles/degree of visual angle from the fine to the coarse SF band). Calibration
                    of the sampling density (i.e., the number of bubbles) was performed online on a
                    trial-by-trial basis to maintain observer's performance at 75%
                    correct categorization independently for each expression. The stimulus presented
                    on each trial comprised the randomly sampled information from each SF band
                    summed together <xref ref-type="bibr" rid="pbio.1001064-Smith2">[27]</xref>.</p>
      </sec>
      <sec id="s4c">
        <title>Procedure</title>
        <p>Prior to testing, observers learned to categorize the 70 original images into the
                    seven expression categories. Upon achieving a 95% correct classification
                    criterion of the original images, observers performed a total of 15 sessions of
                    1,400 trials (for a total of 21,000 trials) of the facial expressions
                    categorization task (i.e., 3,000 trials per expression, happy, sad, fearful,
                    angry, surprised, disgusted, and neutral faces, randomly distributed across
                    sessions). Short breaks were permitted every 100 trials of the experiment.</p>
        <p>In each trial a 500 ms fixation cross (spanning 0.4° of visual angle) was
                    immediately followed by the sampled face information, as described before (see
                        <xref ref-type="supplementary-material" rid="pbio.1001064.s001">Figure
                        S1</xref>). Stimuli were presented on a light gray background in the centre
                    of a monitor; a chin-rest maintained a fixed viewing distance of 1 m (visual
                    angle 5.36°×3.7° forehead to base of chin). Stimuli remained on
                    screen until response. Observers were asked to respond as quickly and accurately
                    as possible by pressing expression-specific response keys (seven in total) on a
                    computer keyboard.</p>
      </sec>
      <sec id="s4d">
        <title>EEG Recording</title>
        <p>We recorded scalp electrical activity of the observers while they performed the
                    task. We used sintered Ag/AgCl electrodes mounted in a 62-electrode cap
                    (Easy-Cap) at scalp positions including the standard 10–20 system
                    positions along with intermediate positions and an additional row of low
                    occipital electrodes. Linked mastoids served as initial common reference and
                    electrode AFz as the ground. Vertical electro-oculogram (vEOG) was bipolarly
                    registered above and below the dominant eye and the horizontal electro-oculogram
                    (hEOG) at the outer canthi of both eyes. Electrode impedance was maintained
                    below 10 kΩ throughout recording. Electrical activity was continuously
                    sampled at 1,024 Hz. Analysis epochs were generated off-line, beginning 500 ms
                    prior to stimulus onset and lasting for 1,500 ms in total. We rejected EEG and
                    EOG artefacts using a [−30 µV; +30 µV]
                    deviation threshold over 200 ms intervals on all electrodes. The EOG rejection
                    procedure rejected rotations of the eyeball from 0.9 deg inward to 1.5 deg
                    downward of visual angle—the stimulus spanned 5.36°×3.7° of
                    visual angle on the screen. Artifact-free trials were sorted using EEProbe (ANT)
                    software, narrow-band notch filtered at 49–51 Hz, and re-referenced to
                    average reference.</p>
      </sec>
      <sec id="s4e">
        <title>Computation: Mutual Information</title>
        <p>In Information Theory <xref ref-type="bibr" rid="pbio.1001064-Shannon1">[28]</xref>,<xref ref-type="bibr" rid="pbio.1001064-Cover1">[29]</xref>, Mutual Information
                        <italic>MI</italic>(<italic>X</italic>;<italic>Y</italic> ) between random
                    variables <italic>X</italic> and <italic>Y</italic> measures their mutual
                    dependence. When logarithms to the base 2 are used in Equation 1, the unit of
                    mutual information is expressed in bits.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001064.e002" xlink:type="simple"/><label>(1)</label></disp-formula></p>
        <p>The critical term is <italic>p</italic>(<italic>x</italic>,<italic>y</italic>),
                    the joint probabilities between <italic>X</italic> and Y. When the variables are
                    independent, the logarithm term in Equation 1 becomes 0 and
                        <italic>MI</italic>(<italic>X</italic>;<italic>Y</italic>
                    ) = 0. In contrast, when <italic>X</italic> and
                        <italic>Y</italic> are dependent
                        <italic>MI</italic>(<italic>X</italic>;<italic>Y</italic> ) returns a value
                    in bits that quantifies the mutual dependence between <italic>X</italic> and
                        <italic>Y</italic>. Derived from the measure of uncertainty of a random
                    variable <italic>X</italic> expressed in Equation 2 and the conditional
                    uncertainty of two random variables <italic>X</italic> and <italic>Y</italic>
                    (Equation 3),<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001064.e003" xlink:type="simple"/><label>(2)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001064.e004" xlink:type="simple"/><label>(3)</label></disp-formula></p>
        <p>Mutual Information measures how much bits of information <italic>X</italic> and
                        <italic>Y</italic> share. It quantifies the reduction of uncertainty about
                    one variable that our knowledge of the other variable induces (Equation
                            4),<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001064.e005" xlink:type="simple"/><label>(4)</label></disp-formula></p>
        <p>Here, we use Mutual Information to measure the mutual dependence between the
                    sampling of input visual information from faces and the oscillatory brain
                    responses to these samples and between the same input information and behavior
                    (see <xref ref-type="supplementary-material" rid="pbio.1001064.s002">Figure
                        S2</xref> for an overall illustration of our framework; see <xref ref-type="supplementary-material" rid="pbio.1001064.s003">Figure S3</xref>
                    for a detailed development of the computations between face pixels and correct
                    versus incorrect behavioral responses). For all measures of MI, we used the
                    direct method with quadratic extrapolation for bias correction <xref ref-type="bibr" rid="pbio.1001064-Magri1">[22]</xref>. We
                    quantized data into four equi-populated bins, a distribution that maximizes
                    response entropy <xref ref-type="bibr" rid="pbio.1001064-Magri1">[22]</xref>. Results were qualitatively similar for a larger
                    number of bins (tested in the range of 4 to 16). Below, we provide details for
                    the computation of mutual information with behavioural and EEG responses,
                    including number of trials taken into consideration for the MI computations and
                    the determination of statistical thresholds of mutual information.</p>
      </sec>
      <sec id="s4f">
        <title>Behavioral Mutual Information, <italic>MI</italic>(Pixel; Behavior)</title>
        <p>On each of the 21,000 trials of a categorization task, the randomly located
                    Gaussian apertures make up a three-dimensional mask that reveals a sparse face.
                    Observers will tend to be correct when this sampled SF information is diagnostic
                    for the categorization of the considered expression. To identify the face
                    features used for each facial expression categorization, we computed mutual
                    information, per observer, between the grey levels of each face pixels and a
                    random sample of correct matching the number of incorrect trials (i.e., on
                    average 5,250 correct trials and 5,250 incorrect trials). For each expression,
                    we then averaged mutual information values across all six observers,
                    independently for each pixel. To establish statistical thresholds, we repeated
                    the computations 500 times for each pixel, after randomly shuffling the order of
                    response—to disrupt the association between pixel values and
                    categorization responses. For each of the 500 computations, we selected the
                    maximum mutual information value across all pixels. We then chose as statistical
                    threshold the 99th percentile of the distribution of maxima. This maximum
                    statistic implements a correction for multiple comparisons because the
                    permutation provides the null distribution of the maximum statistical value
                    across all considered dimensions <xref ref-type="bibr" rid="pbio.1001064-Nichols1">[30]</xref>. Behavioral mutual
                    information is reported as the top row of faces in <xref ref-type="fig" rid="pbio-1001064-g001">Figure 1</xref>.</p>
      </sec>
      <sec id="s4g">
        <title>EEG Mutual Information</title>
        <p>Here, we examined two different measures: <italic>MI</italic>(EEG Response;
                    Behavior) and <italic>MI</italic>(Pixel; EEG Response). <italic>MI</italic>(EEG
                    Response; Behavior) computed, for each electrode, subject, and expression, the
                    mutual information between correct and incorrect trials and the power, phase,
                    and phase&amp;power of the Time × Frequency EEG signal. For this
                    computation, we used the same number of trials as for Behavior MI (i.e., on
                    average 5,250 correct trials and 5,250 incorrect trials). As with behavior, for
                    each electrode and type of EEG measurement, we averaged the mutual information
                    values across subjects and expression. To establish statistical thresholds, we
                    repeated the computations 500 times, permuting the trial order of the EEG Time
                    × Frequency values and identified the 500 maxima each time across the
                    entire Time × Frequency space. We identified the statistical threshold as
                    the 99th percentile of the distribution of maxima (see <xref ref-type="fig" rid="pbio-1001064-g002">Figure 2</xref>).</p>
        <p><italic>MI</italic>(Pixel; Phase&amp;Power) computed, for each subject,
                    expression, and face pixel (down-sampled to 38×24 pixel maps), the mutual
                    information between the distribution of each face pixel grey-level value and the
                    most informative of the brain responses, phase&amp;power Time × Frequency
                    responses, for correct trials only. That is, an average of 15,750 trials per
                    subject. To establish statistical thresholds, given the magnitude of the
                    computation, we computed <italic>z</italic> scores using the pre-stimulus
                    presentation baseline (from −500 to 0 ms) to estimate mean and standard
                    deviation. In <xref ref-type="fig" rid="pbio-1001064-g003">Figure 3</xref>, .01
                    bits of mutual information correspond to a <italic>z</italic> score of 55.97, so
                    all mutual information values this number of bits (see the level marked with an
                    asterisk in <xref ref-type="fig" rid="pbio-1001064-g003">Figure 3</xref>) are
                    well above an uncorrected threshold of .0000001 (itself associated with a
                        <italic>z</italic> score of 5).</p>
        <p><xref ref-type="fig" rid="pbio-1001064-g002">Figure 2</xref> indicated two
                    clusters of maximal <italic>MI</italic> in all three measures (Power, Phase, and
                    Phase&amp;Power) at a latency of 140–250 ms in two frequency bands (4 Hz
                    and 12–14 Hz). We averaged the <italic>MI</italic> measures, for each
                    cluster, electrode, and subject, and subjected these <italic>MI</italic>
                    averages to a two-way ANOVA with factors electrode (P7, P8, Pz, and Oz) and
                    measure (Power, Phase, and Phase&amp;Power). Both clusters revealed a
                    significant main effect of electrode (<italic>F</italic>(1,
                    3) = 8.38, <italic>p</italic>&lt;0.001 for 4 Hz and
                        <italic>F</italic>(1, 3) = 79.34,
                    <italic>p</italic>&lt;0.001 for 12–14 Hz) and measure
                    (<italic>F</italic>(1, 2) = 44.24,
                    <italic>p</italic>&lt;0.001 for 4 Hz and <italic>F</italic>(1,
                    2) = 104.77, <italic>p</italic>&lt;0.001 for 12–14
                    Hz). Post hoc <italic>t</italic> test confirmed that
                    <italic>MI</italic>(Phase&amp;Power) is significantly higher than
                        <italic>MI</italic>(Phase)
                    (<italic>p</italic> = 0.013), which itself is significantly
                    higher than <italic>MI</italic>(Power)
                    (<italic>p</italic> = 0.003).</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pbio.1001064.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001064.s001" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <p>Illustration of the bubbles sampling procedure. The original stimulus is
                        decomposed into five non-overlapping bands of Spatial Frequencies (SF) of
                        one octave each (120–60; 60–30; 30–15; 15–7.5;
                        7.5–3.8 cycles per face). We sampled information from each SF band
                        using a mask punctured with Gaussian apertures. These were randomly
                        positioned trial by trial to approximate a uniform sampling distribution of
                        all face regions across trials. We adjusted the size of the apertures for
                        each SF band so as to maintain constant the total area of the face revealed
                        across trials (standard deviations of the bubbles were.36, .7, 1.4, 2.9, and
                        5.1 cycles/deg of visual angle from fine to coarse). We calibrated the
                        sampling density (i.e., the number of bubbles) on a trial-per-trial basis to
                        maintain a 75% correct categorization performance independently for
                        each expression. The stimulus presented on each trial comprised information
                        from each SF band summed together.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio.1001064.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001064.s002" xlink:type="simple">
        <label>Figure S2</label>
        <caption>
          <p>Mutual Information (<italic>MI</italic>) Framework. Pixel. Reduced 38 ×
                        24 pixels space used for analysis (see <xref ref-type="supplementary-material" rid="pbio.1001064.s001">Figure
                            S1</xref> for a full description of the information sampling used in the
                        actual experiment). <italic>EEG response</italic>. On each trial, we
                        recorded the observer's EEG response. With a size 5 Morlet wavelet, we
                        performed a Time × Frequency decomposition (with a 7.8 ms time step
                        between −500 to 500 ms around stimulus onset and with a 2 Hz step
                        between 4 and 96 Hz). <italic>Behavior</italic><underline>.</underline> On
                        each of the 3000 trials per expression (illustrated for
                        “happy”), we recorded the observer's correct versus
                        incorrect responses to the sampled information. <italic>Computation of
                            MI</italic>. Across the 3,000 trials per expression, for each pixel we
                        summed the Gaussian apertures across spatial frequency bands and collected
                        the distributions of resulting grey-level values associated with correct and
                        incorrect responses. We then computed <italic>MI</italic> between the pixel
                        values reflecting the Gaussian apertures and correct versus incorrect
                        responses, <italic>MI</italic>(Pixel; Behavior). We also computed
                            <italic>MI</italic> between behavior and the EEG response,
                            <italic>MI</italic>(EEG Response; Behavior), independently for power,
                        phase, and the conjunction of phase&amp;power. Finally, we computed
                            <italic>MI</italic> between the pixels values and the EEG response,
                            <italic>MI</italic>(EEG Response; Behavior).</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio.1001064.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001064.s003" xlink:type="simple">
        <label>Figure S3</label>
        <caption>
          <p>Detailed Illustration of the Computation of <italic>MI</italic>(Pixel;
                        Behavior). For one observer, expression “happy,” we provide the
                        full computation of mutual information using two face pixels (P1 and P2) and
                        an equal number of correct (c) and incorrect (i) categorization responses.
                        Note that if the computation had been between face pixels and EEG
                        parameters, we would have had four rows (one per bin of, e.g., amplitude or
                        phase) in the matrix of joint probabilities, not two (for correct and
                        incorrect).</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="pbio.1001064-Schnitzler1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Schnitzler</surname><given-names>A</given-names></name><name name-style="western"><surname>Gross</surname><given-names>J</given-names></name></person-group>
                    <year>2005</year>
                    <article-title>Normal and pathological oscillatory communication in the
                        brain.</article-title>
                    <source>Nat Rev Neuro</source>
                    <volume>6</volume>
                    <fpage>285</fpage>
                    <lpage>296</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Singer1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Singer</surname><given-names>W</given-names></name></person-group>
                    <year>1999</year>
                    <article-title>Neuronal synchrony: a versatile code for the definition of
                        relations?</article-title>
                    <source>Neuron</source>
                    <volume>24</volume>
                    <fpage>49</fpage>
                    <lpage>65, 111-125</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Buzsaki1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Buzsaki</surname><given-names>G</given-names></name><name name-style="western"><surname>Draguhn</surname></name></person-group>
                    <year>2004</year>
                    <article-title>A neuronal oscillations in cortical networks.</article-title>
                    <source>Science</source>
                    <volume>304</volume>
                    <fpage>1926</fpage>
                    <lpage>1929</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Salinas1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Salinas</surname><given-names>E</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>T. J</given-names></name></person-group>
                    <year>2001</year>
                    <article-title>Correlated neuronal activity and the flow of neural
                        information.</article-title>
                    <source>Nat Rev Neuro</source>
                    <volume>2</volume>
                    <fpage>539</fpage>
                    <lpage>550</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Busch1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Busch</surname><given-names>N. A</given-names></name><name name-style="western"><surname>VanRullen</surname><given-names>R</given-names></name></person-group>
                    <year>2010</year>
                    <article-title>Spontaneous EEG oscillations reveal periodic sampling of visual
                        attention.</article-title>
                    <source>Proc Natl Acad Sci U S A</source>
                    <volume>37</volume>
                    <fpage>16048</fpage>
                    <lpage>16053</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Belitski1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Belitski</surname><given-names>A</given-names></name><name name-style="western"><surname>Gretton</surname><given-names>A</given-names></name><name name-style="western"><surname>Magri</surname><given-names>C</given-names></name><name name-style="western"><surname>Murayama</surname><given-names>Y</given-names></name><name name-style="western"><surname>Montemurro</surname><given-names>M. A</given-names></name><name name-style="western"><surname>Logothetis</surname><given-names>N. K</given-names></name><name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name></person-group>
                    <year>2008</year>
                    <article-title>Low-frequency local field potentials and spikes in primary visual
                        cortex convey independent visual information.</article-title>
                    <source>J Neurosci</source>
                    <volume>28</volume>
                    <fpage>5696</fpage>
                    <lpage>5709</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Canolty1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Canolty</surname><given-names>R. T</given-names></name><name name-style="western"><surname>Edwards</surname><given-names>E</given-names></name><name name-style="western"><surname>Dalal</surname><given-names>S. S</given-names></name><name name-style="western"><surname>Soltani</surname><given-names>M</given-names></name><name name-style="western"><surname>Nagarajan</surname><given-names>S. S</given-names></name><etal/></person-group>
                    <year>2006</year>
                    <article-title>High gamma power is phase-locked to theta oscillations in human
                        neocortex.</article-title>
                    <source>Science</source>
                    <volume>313</volume>
                    <fpage>1626</fpage>
                    <lpage>1628</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Demiralp1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Demiralp</surname><given-names>T</given-names></name><name name-style="western"><surname>Bayraktaroglu</surname><given-names>Z</given-names></name><name name-style="western"><surname>Lenz</surname><given-names>D</given-names></name><name name-style="western"><surname>Junge</surname><given-names>S</given-names></name><name name-style="western"><surname>Busch</surname><given-names>N. A</given-names></name><etal/></person-group>
                    <year>2007</year>
                    <article-title>Gamma amplitudes are coupled to theta phase in human EEG during
                        visual perception.</article-title>
                    <source>Int J Psychophysiol</source>
                    <volume>64</volume>
                    <fpage>24</fpage>
                    <lpage>30</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Frnd1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Fründ</surname><given-names>I</given-names></name><name name-style="western"><surname>Busch</surname><given-names>N. A</given-names></name><name name-style="western"><surname>Schadow</surname><given-names>J</given-names></name><name name-style="western"><surname>Körner</surname><given-names>U</given-names></name><name name-style="western"><surname>Herrmann</surname><given-names>C. S</given-names></name></person-group>
                    <year>2007</year>
                    <article-title>From perception to action: phase-locked gamma oscillations
                        correlate with reaction times in a speeded response task.</article-title>
                    <source>BMC Neurosci</source>
                    <volume>8</volume>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Hanslmayr1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Hanslmayr</surname><given-names>S</given-names></name><name name-style="western"><surname>Asian</surname><given-names>A</given-names></name><name name-style="western"><surname>Staudigi</surname><given-names>T</given-names></name><name name-style="western"><surname>Kilmesch</surname><given-names>W</given-names></name><name name-style="western"><surname>Herrmann</surname><given-names>C. S</given-names></name><name name-style="western"><surname>Bäumi</surname><given-names>K. H</given-names></name></person-group>
                    <year>2007</year>
                    <article-title>Prestimulus oscillations predict visual perception performance
                        between and within subjects.</article-title>
                    <source>Neuroimage</source>
                    <volume>37</volume>
                    <fpage>1465</fpage>
                    <lpage>1473</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Jensen1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Jensen</surname><given-names>O</given-names></name><name name-style="western"><surname>Colgin</surname><given-names>L. L</given-names></name></person-group>
                    <year>2007</year>
                    <article-title>Cross-frequency coupling between neural
                        oscillations.</article-title>
                    <source>Trends Cog Sci</source>
                    <volume>11</volume>
                    <fpage>267</fpage>
                    <lpage>269</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Huxter1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Huxter</surname><given-names>J</given-names></name><name name-style="western"><surname>Burgess</surname><given-names>N</given-names></name><name name-style="western"><surname>O'Keefe</surname><given-names>J</given-names></name></person-group>
                    <year>2003</year>
                    <article-title>Independent rate and temporal coding in hippocampal pyramidal
                        cells.</article-title>
                    <source>Nature</source>
                    <volume>425</volume>
                    <fpage>828</fpage>
                    <lpage>832</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Mehta1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Mehta</surname><given-names>M. R</given-names></name><name name-style="western"><surname>Lee</surname><given-names>A. K</given-names></name><name name-style="western"><surname>Wilson</surname><given-names>M. A</given-names></name></person-group>
                    <year>2002</year>
                    <article-title>Role of experience and oscillations in transforming a rate code
                        into a temporal code.</article-title>
                    <source>Nature</source>
                    <volume>417</volume>
                    <fpage>741</fpage>
                    <lpage>746</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Lakatos1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name><name name-style="western"><surname>Karmos</surname><given-names>G</given-names></name><name name-style="western"><surname>Mehta</surname><given-names>M. D</given-names></name><name name-style="western"><surname>Ulbert</surname><given-names>I</given-names></name><name name-style="western"><surname>Schroeder</surname><given-names>C. E</given-names></name></person-group>
                    <year>2008</year>
                    <article-title>Entrainment of neuronal oscillations as a mechanism of
                        attentional selection.</article-title>
                    <source>Science</source>
                    <volume>320</volume>
                    <fpage>110</fpage>
                    <lpage>113</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Schroeder1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Schroeder</surname><given-names>C. E</given-names></name><name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name></person-group>
                    <year>2009</year>
                    <article-title>Low-frequency neuronal oscillations as instruments of sensory
                        selection.</article-title>
                    <source>Trends Neurosci</source>
                    <volume>32</volume>
                    <fpage>9</fpage>
                    <lpage>18</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Montemurro1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Montemurro</surname><given-names>M. A</given-names></name><name name-style="western"><surname>Rasch</surname><given-names>M. J</given-names></name><name name-style="western"><surname>Murayama</surname><given-names>Y</given-names></name><name name-style="western"><surname>Logothetis</surname><given-names>N. K</given-names></name><name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name><etal/></person-group>
                    <year>2008</year>
                    <article-title>Phase-of-firing coding of natural visual stimuli in primary
                        visual cortex.</article-title>
                    <source>Current Biology</source>
                    <volume>18</volume>
                    <fpage>375</fpage>
                    <lpage>380</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Fries1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Fries</surname><given-names>P</given-names></name><name name-style="western"><surname>Nikolić</surname><given-names>D</given-names></name><name name-style="western"><surname>Singer</surname><given-names>W</given-names></name></person-group>
                    <year>2007</year>
                    <article-title>The gamma cycle.</article-title>
                    <source>Trends Neurosci</source>
                    <volume>30</volume>
                    <fpage>309</fpage>
                    <lpage>316</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Panzeri1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name><name name-style="western"><surname>Logothetis</surname><given-names>N. K</given-names></name><name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name></person-group>
                    <year>2010</year>
                    <article-title>Sensory neural codes using multiplexed temporal
                        scales.</article-title>
                    <source>Trends Neurosci</source>
                    <volume>33</volume>
                    <fpage>111</fpage>
                    <lpage>120</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Smith1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>M. L</given-names></name><name name-style="western"><surname>Gosselin</surname><given-names>F</given-names></name><name name-style="western"><surname>Schyns</surname><given-names>P. G</given-names></name></person-group>
                    <year>2005</year>
                    <article-title>Perceptual moments of conscious visual experience inferred from
                        oscillatory brain activity.</article-title>
                    <source>Proc Natl Acad Sci U S A</source>
                    <volume>103</volume>
                    <fpage>5626</fpage>
                    <lpage>5631</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Gosselin1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Gosselin</surname><given-names>F</given-names></name><name name-style="western"><surname>Schyns</surname><given-names>P. G</given-names></name></person-group>
                    <year>2001</year>
                    <article-title>Bubbles: a new technique to reveal the use of visual information
                        in recognition tasks.</article-title>
                    <source>Vis Res</source>
                    <volume>41</volume>
                    <fpage>2261</fpage>
                    <lpage>2271</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Schyns1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Schyns</surname><given-names>P. G</given-names></name><name name-style="western"><surname>Petro</surname><given-names>L</given-names></name><name name-style="western"><surname>Smith</surname><given-names>M. L</given-names></name></person-group>
                    <year>2007</year>
                    <article-title>Dynamics of visual information integration in the brain to
                        categorize facial expressions.</article-title>
                    <source>Curr Biol</source>
                    <volume>17</volume>
                    <fpage>1580</fpage>
                    <lpage>1585</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Magri1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Magri</surname><given-names>C</given-names></name><name name-style="western"><surname>Whittingstall</surname><given-names>K</given-names></name><name name-style="western"><surname>Singh</surname><given-names>V</given-names></name><name name-style="western"><surname>Logothetis</surname><given-names>N. K</given-names></name><name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name><etal/></person-group>
                    <year>2009</year>
                    <article-title>A toolbox for the fast information analysis of multiple-site LFP,
                        EEG and spike train recordings.</article-title>
                    <source>BMC Neuroscience</source>
                    <volume>10</volume>
                    <fpage>81</fpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-VanRijsbergen1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Van Rijsbergen</surname><given-names>N</given-names></name><name name-style="western"><surname>Schyns</surname><given-names>P. G</given-names></name></person-group>
                    <year>2010</year>
                    <article-title>Dynamics of trimming the content of face representations for
                        categorization in the brain.</article-title>
                    <source>PLoS Comp Biol</source>
                    <volume>5</volume>
                    <fpage>e1000561</fpage>
                    <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000561" xlink:type="simple">10.1371/journal.pcbi.1000561</ext-link></comment>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Romei1">
        <label>24</label>
        <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Romei</surname><given-names>V</given-names></name><name name-style="western"><surname>Driver</surname><given-names>J</given-names></name><name name-style="western"><surname>Schyns</surname><given-names>P. G</given-names></name><name name-style="western"><surname>Thut</surname><given-names>G</given-names></name></person-group>
                    <year>2011</year>
                    <article-title>Rhythmic TMS over parietal cortex link distinct brain frequencies
                        to global versus local visual processing.</article-title>
                    <source>Curr Biol</source>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Ekman1">
        <label>25</label>
        <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Ekman</surname><given-names>P</given-names></name><name name-style="western"><surname>Friesen</surname><given-names>W. V</given-names></name></person-group>
                    <year>1978</year>
                    <article-title>The facial action coding system (FACS): A technique for the
                        measurement of facial action.</article-title>
                    <publisher-loc>Palo Alto, CA</publisher-loc>
                    <publisher-name>Consulting Psychologists Press</publisher-name>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Dailey1">
        <label>26</label>
        <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Dailey</surname><given-names>M</given-names></name><name name-style="western"><surname>Cottrell</surname><given-names>G. W</given-names></name><name name-style="western"><surname>Reilly</surname><given-names>J</given-names></name></person-group>
                    <year>2001</year>
                    <comment>California Facial Expressions, CAFE, unpublished digital images, UCSD
                        Computer Science and Engineering Department</comment>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Smith2">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>M. L</given-names></name><name name-style="western"><surname>Cottrell</surname><given-names>G. W</given-names></name><name name-style="western"><surname>Gosselin</surname><given-names>F</given-names></name><name name-style="western"><surname>Schyns</surname><given-names>P. G</given-names></name></person-group>
                    <year>2005</year>
                    <article-title>Transmitting and decoding facial expressions.</article-title>
                    <source>Psych Sci</source>
                    <volume>16</volume>
                    <fpage>184</fpage>
                    <lpage>189</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Shannon1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Shannon</surname><given-names>C. E</given-names></name></person-group>
                    <year>1948</year>
                    <article-title>A mathematical theory of communication.</article-title>
                    <source>Bell Sys Tech Journal</source>
                    <volume>27</volume>
                    <fpage>379</fpage>
                    <lpage>423, 623-656</lpage>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Cover1">
        <label>29</label>
        <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Cover</surname><given-names>T. M</given-names></name><name name-style="western"><surname>Thomas</surname><given-names>J. A</given-names></name></person-group>
                    <year>1991</year>
                    <article-title>Elements of information theory.</article-title>
                    <publisher-loc>New York</publisher-loc>
                    <publisher-name>John Wiley &amp; Sons</publisher-name>
                </element-citation>
      </ref>
      <ref id="pbio.1001064-Nichols1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author"><name name-style="western"><surname>Nichols</surname><given-names>T. E</given-names></name><name name-style="western"><surname>Holmes</surname><given-names>A. P</given-names></name></person-group>
                    <year>2002</year>
                    <article-title>Nonparametric permutation tests for functional neuroimaging: a
                        primer with examples.</article-title>
                    <source>Hum Brain Mapp</source>
                    <volume>15</volume>
                    <fpage>1</fpage>
                    <lpage>25</lpage>
                </element-citation>
      </ref>
    </ref-list>
    <glossary>
      <title>Abbreviations</title>
      <def-list>
        <def-item>
          <term>CAFÉ</term>
          <def>
            <p>California Facial Expressions</p>
          </def>
        </def-item>
        <def-item>
          <term>ERP</term>
          <def>
            <p>event related potentials</p>
          </def>
        </def-item>
        <def-item>
          <term>cortical oscillations</term>
        </def-item>
        <def-item>
          <term>neural coding</term>
        </def-item>
        <def-item>
          <term>FACS</term>
          <def>
            <p>Facial Action Coding System</p>
          </def>
        </def-item>
        <def-item>
          <term>hEOG</term>
          <def>
            <p>horizontal electro-oculogram</p>
          </def>
        </def-item>
        <def-item>
          <term>
            <italic>MI</italic>
          </term>
          <def>
            <p>Mutual Information</p>
          </def>
        </def-item>
        <def-item>
          <term>SF</term>
          <def>
            <p>spatial frequency</p>
          </def>
        </def-item>
        <def-item>
          <term>vEOG</term>
          <def>
            <p>vertical electro-oculogram</p>
          </def>
        </def-item>
      </def-list>
    </glossary>
    
  </back>
</article>