<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-01062</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002249</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Coding mechanisms</subject>
                <subject>Sensory systems</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Sensory systems</subject>
              <subj-group>
                <subject>Visual system</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Computer science</subject>
          <subj-group>
            <subject>Algorithms</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
          <subject>Computer Science</subject>
        </subj-group>
      </article-categories><title-group><article-title>Second Order Dimensionality Reduction Using Minimum and Maximum Mutual Information Models</article-title><alt-title alt-title-type="running-head">Second Order Dimensionality Reduction</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Fitzgerald</surname>
            <given-names>Jeffrey D.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Rowekamp</surname>
            <given-names>Ryan J.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Sincich</surname>
            <given-names>Lawrence C.</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Sharpee</surname>
            <given-names>Tatyana O.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Computational Neurobiology Laboratory, The Salk Institute for Biological Studies, La Jolla, California, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Center for Theoretical Biological Physics and Department of Physics, University of California, San Diego, California, United States of America</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Department of Vision Sciences, University of Alabama at Birmingham, Birmingham, Alabama, United States of America</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">sharpee@salk.edu</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: JDF RJR LCS TOS. Performed the experiments: LCS. Analyzed the data: JDF RJR LCS TOS. Wrote the paper: JDF RJR LCS TOS.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>10</month>
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>27</day>
        <month>10</month>
        <year>2011</year>
      </pub-date><volume>7</volume><issue>10</issue><elocation-id>e1002249</elocation-id><history>
        <date date-type="received">
          <day>21</day>
          <month>7</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>7</day>
          <month>9</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Fitzgerald et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>Conventional methods used to characterize multidimensional neural feature selectivity, such as spike-triggered covariance (STC) or maximally informative dimensions (MID), are limited to Gaussian stimuli or are only able to identify a small number of features due to the curse of dimensionality. To overcome these issues, we propose two new dimensionality reduction methods that use minimum and maximum information models. These methods are information theoretic extensions of STC that can be used with non-Gaussian stimulus distributions to find relevant linear subspaces of arbitrary dimensionality. We compare these new methods to the conventional methods in two ways: with biologically-inspired simulated neurons responding to natural images and with recordings from macaque retinal and thalamic cells responding to naturalistic time-varying stimuli. With non-Gaussian stimuli, the minimum and maximum information methods significantly outperform STC in all cases, whereas MID performs best in the regime of low dimensional feature spaces.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Neurons are capable of simultaneously encoding information about multiple features of sensory stimuli in their spikes. The dimensionality reduction methods that currently exist to extract those relevant features are either biased for non-Gaussian stimuli or fall victim to the curse of dimensionality. In this paper we introduce two information theoretic extensions of the spike-triggered covariance method. These new methods use the concepts of minimum and maximum mutual information to identify the stimulus features encoded in the spikes of a neuron. Using simulated and experimental neural data, these methods are shown to perform well both in situations where conventional approaches are appropriate and where they fail. These new techniques should improve the characterization of neural feature selectivity in areas of the brain where the application of currently available approaches is restricted.</p>
      </abstract><funding-group><funding-statement>This work was funded by NIH Grant EY019493; NSF Grants IIS-0712852 and PHY-0822283 and the Searle Funds; the Alfred P. Sloan Fellowship; the McKnight Scholarship; W.M. Keck Research Excellence Award; and the Ray Thomas Edwards Career Development Award in Biomedical Sciences. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="9"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>In recent years it has become apparent that many types of sensory neurons simultaneously encode information about more than one stimulus feature in their spiking activity. Examples can be found across a wide variety of modalities, including the visual <xref ref-type="bibr" rid="pcbi.1002249-Brenner1">[1]</xref>â€“<xref ref-type="bibr" rid="pcbi.1002249-Tanabe1">[12]</xref>, auditory <xref ref-type="bibr" rid="pcbi.1002249-Atencio1">[13]</xref>, olfactory <xref ref-type="bibr" rid="pcbi.1002249-Kim1">[14]</xref>, somatosensory <xref ref-type="bibr" rid="pcbi.1002249-Maravall1">[15]</xref> and mechanosensory <xref ref-type="bibr" rid="pcbi.1002249-Fox1">[16]</xref> systems. This discovery was facilitated by the development of dimensionality reduction techniques like spike-triggered covariance (STC) <xref ref-type="bibr" rid="pcbi.1002249-deRuytervanSteveninck1">[17]</xref>â€“<xref ref-type="bibr" rid="pcbi.1002249-Schwartz2">[22]</xref> and maximally informative dimensions (MID) <xref ref-type="bibr" rid="pcbi.1002249-Sharpee1">[23]</xref>. These two methods exhibit complementary advantages and disadvantages. For instance, STC can identify many relevant features for stimuli whose parameters are distributed in a Gaussian manner but can fail when natural stimuli are used, whereas MID works well for arbitrary stimuli but requires exponentially larger data sets to find more than a few features. Therefore, there is need for a method that can find relevant features from arbitrary stimulus distributions while bypassing the curse of dimensionality. Here we propose two novel techniques based on minimum and maximum mutual information; these new approaches can be seen as an extension of STC to arbitrary stimuli.</p>
      <p>Neural coding of multiple stimulus features is typically modeled as a linear-nonlinear Poisson (LNP) process <xref ref-type="bibr" rid="pcbi.1002249-deBoer1">[24]</xref>â€“<xref ref-type="bibr" rid="pcbi.1002249-Dayan1">[28]</xref>. A stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e001" xlink:type="simple"/></inline-formula>, such as an image with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e002" xlink:type="simple"/></inline-formula> pixels, as well as each of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e003" xlink:type="simple"/></inline-formula> features <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e004" xlink:type="simple"/></inline-formula> for which a neuron is selective are represented by vectors in a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e005" xlink:type="simple"/></inline-formula> dimensional space. The neuron extracts information about the stimulus by projecting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e006" xlink:type="simple"/></inline-formula> onto the linear subspace spanned by the feature vectors. The result is a stimulus of reduced dimensionality <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e007" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e008" xlink:type="simple"/></inline-formula>; this input is then passed through an nonlinear firing rate function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e009" xlink:type="simple"/></inline-formula>. Spikes are then assumed to be generated by a Poisson process with a rate equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e010" xlink:type="simple"/></inline-formula>, which only depends on the relevant dimensions of the stimulus space.</p>
      <p>Given a set of stimuli <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e011" xlink:type="simple"/></inline-formula>, for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e012" xlink:type="simple"/></inline-formula> and the corresponding observed neural responses <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e013" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e014" xlink:type="simple"/></inline-formula> is number of spikes, there are a few commonly used methods available to extract the stimulus features relevant to the neuron. In the STC method, the stimulus covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e015" xlink:type="simple"/></inline-formula> and the covariance of the spike-triggered ensemble,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e016" xlink:type="simple"/></disp-formula> are compared to discover the dimensions along which the stimulus variance conditional on a spike is significantly different from the stimulus variance overall. This comparison is done by diagonalizing the matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e017" xlink:type="simple"/></inline-formula>. The relevant features can be identified by the eigenvectors that have nonzero eigenvalues. If the stimuli are drawn from a distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e018" xlink:type="simple"/></inline-formula> which is Gaussian, then the only limitation to finding the features is having a large enough set of spike data. In practice, the STC procedure can be extended to Gaussian stimuli containing correlations by adding a whitening step <xref ref-type="bibr" rid="pcbi.1002249-deRuytervanSteveninck1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002249-Bialek1">[18]</xref>, and can also include a regularization term to smooth the results (see <xref ref-type="sec" rid="s4">Methods</xref>). On the other hand, if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e019" xlink:type="simple"/></inline-formula> is non-Gaussian, as is the case for natural images, then higher order stimulus correlations can greatly affect the results <xref ref-type="bibr" rid="pcbi.1002249-Sharpee1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1002249-Paninski2">29</xref>.</p>
      <p>The use of Gaussian stimuli makes it possible to find many relevant dimensions using STC, but fully sampling the dynamic range of responses often requires a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e020" xlink:type="simple"/></inline-formula> more similar to the non-Gaussian distributions found in nature <xref ref-type="bibr" rid="pcbi.1002249-Rieke1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1002249-Simoncelli1">[30]</xref>. It has also been suggested that neural representations of stimuli may be optimized in some way <xref ref-type="bibr" rid="pcbi.1002249-Barlow1">[31]</xref>â€“<xref ref-type="bibr" rid="pcbi.1002249-vonderTwer1">[33]</xref> to the statistics of the natural environment. With this in mind, it is important that multidimensional feature extraction methods be extended to stimulus distributions with non-Gaussian statistics.</p>
      <p>The MID method is an information theoretic dimensionality reduction technique that identifies relevant features based on how much information a linear subspace contains about the observed spikes (see <xref ref-type="sec" rid="s4">Methods</xref>). Unlike STC, the dimensionality of the relevant subspace to be found using MID must be specified <italic>a priori</italic>, and thus to discover the number of relevant features one must search for additional dimensions until the subspace accounts for a sufficient fraction of the information carried in the neural response. The objective function in MID relies on an empirical construction of the reduced stimulus distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e021" xlink:type="simple"/></inline-formula> and the corresponding conditional distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e022" xlink:type="simple"/></inline-formula>, and thus suffers from the curse of dimensionality <xref ref-type="bibr" rid="pcbi.1002249-Bellman1">[34]</xref>. A related problem that occurs equally for Gaussian and non-Gaussian stimuli, and affects both the STC and MID methods, is that even if one is able to find many relevant dimensions, it is usually not possible to sample the nonlinear gain function simultaneously along all of these dimensions.</p>
      <p>Here we put forth two new dimensionality reduction techniques applicable to arbitrary stimulus distributions. These methods, much like STC, make use of pairwise correlations between stimulus dimensions and are not hindered by the curse of dimensionality in the same manner as MID. To demonstrate the usefulness of the proposed methods, we apply them to simulated neural data for two biologically inspired model cells, and to physiological recordings of the response of macaque retina and thalamus cells to time-varying stimuli.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>Dimensionality reduction using minimal models</title>
        <p>If the spiking activity of a neuron is encoding certain aspects of the stimulus, then the corresponding stimulus features must be correlated in some way with the neural response. From an experiment one can estimate specific stimulus/response correlations, such as the spike-triggered average (STA), the spike-triggered covariance (STC), or the mutual information <xref ref-type="bibr" rid="pcbi.1002249-Cover1">[35]</xref>,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e023" xlink:type="simple"/><label>(1)</label></disp-formula>which provides a full measure of the degree of dependence between stimulus and response. These estimates can then be used to construct a model of the conditional response probability by constraining <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e024" xlink:type="simple"/></inline-formula> to match a given set of observed correlations, as in the STA and STC methods. As there are an infinite number of models that match any given set of experimentally estimated correlations, the values of the unconstrained correlations are necessarily determined by the specific choice of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e025" xlink:type="simple"/></inline-formula>.</p>
        <p>The minimal model of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e026" xlink:type="simple"/></inline-formula> is the one that is consistent with the chosen set of correlations but is otherwise as random as possible, making it minimally biased with respect to unconstrained correlations <xref ref-type="bibr" rid="pcbi.1002249-Fitzgerald1">[36]</xref>. This model can be obtained by maximizing the noise entropy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e027" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e028" xlink:type="simple"/></inline-formula> denotes an average over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e029" xlink:type="simple"/></inline-formula>. For a binary spike/no spike neuron consistent with an observed mean firing rate, as well as the correlation of the neural response with linear and quadratic moments of the stimulus, the minimal model is a logistic function <xref ref-type="bibr" rid="pcbi.1002249-Fitzgerald1">[36]</xref><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e030" xlink:type="simple"/><label>(2)</label></disp-formula>where the parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e031" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e032" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e033" xlink:type="simple"/></inline-formula> are chosen such that the mean firing rate, STA and STC of the model match the experimentally observed values (see <xref ref-type="sec" rid="s4">Methods</xref>). If correlations between a spike and higher order moments of the stimulus are measured, the argument of the logistic function would include higher powers of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e034" xlink:type="simple"/></inline-formula>. In addition to being as unbiased as possible, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e035" xlink:type="simple"/></inline-formula> also minimizes the mutual information <xref ref-type="bibr" rid="pcbi.1002249-Fitzgerald1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1002249-Globerson1">[37]</xref>, which only includes the contribution of the chosen constraints. We note that previously we used this minimal model framework to characterize the computation performed within the reduced relevant subspace <xref ref-type="bibr" rid="pcbi.1002249-Fitzgerald1">[36]</xref>, and in particular to quantify in information-theoretic terms the contribution of higher-than-second powers of relevant stimulus features to neural firing. Here, we study whether analysis of the second-order minimal models constructed in the full stimulus space can be used to find the relevant feature subspace itself.</p>
        <p>The contours of constant probability of the minimal second order models are quadric surfaces, defined by the quadratic polynomial <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e036" xlink:type="simple"/></inline-formula>. The diagonalization of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e037" xlink:type="simple"/></inline-formula> involves a change of coordinates such that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e038" xlink:type="simple"/><label>(3)</label></disp-formula>This is accomplished through the diagonalization of the matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e039" xlink:type="simple"/></inline-formula>, yielding <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e040" xlink:type="simple"/></inline-formula> eigenvectors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e041" xlink:type="simple"/></inline-formula> with corresponding eigenvalues <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e042" xlink:type="simple"/></inline-formula>. These eigenvectors are the principal axes of the constant probability surfaces, and as such the magnitude of the eigenvalue along a particular direction is indicative of the curvature, and hence the selectivity, of the surface in that dimension. This point is illustrated in <xref ref-type="fig" rid="pcbi-1002249-g001">Fig. 1</xref>.</p>
        <fig id="pcbi-1002249-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002249.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Eigenvector analysis of quadratic probability surfaces.</title>
            <p>The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e043" xlink:type="simple"/></inline-formula> surfaces are shown for two simple second order minimal models in a three dimensional space. For the surface on the left all three eigenvalues are nonzero; the surface curves in all three dimensions and the neuron is selective for three features. For the surface on the right one of the eigenvalues is equal to zero; the surface only curves in two dimensions and the neuron is selective for only two features.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002249.g001" xlink:type="simple"/>
        </fig>
        <p>The linear term in Eq. (3) may also contain a significant feature. Subtracting off the relevant dimensions found from diagonalizing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e044" xlink:type="simple"/></inline-formula> leaves an orthogonal vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e045" xlink:type="simple"/></inline-formula>. The magnitude of this vector can be directly compared to the eigenvalue spectrum to determine its relative strength.</p>
      </sec>
      <sec id="s2b">
        <title>Dimensionality reduction using nonlinear MID</title>
        <p>The minimal models of binary response systems take the form of logistic functions. This restriction can be eliminated if we look for a maximally informative second order model. To accomplish this, we extend the MID algorithm to second order in the stimulus by assuming the firing rate is a function of a quadratic polynomial, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e046" xlink:type="simple"/></inline-formula>. The nonlinear MID (nMID) algorithm is then run exactly as linear MID in the expanded <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e047" xlink:type="simple"/></inline-formula> dimensional space.</p>
        <p>Once the maximally informative parameters are found, the matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e048" xlink:type="simple"/></inline-formula> can be diagonalized to reveal the relevant features, and the linear term can be analyzed in the same manner as for the minimal sigmoidal model. The ability to construct an arbitrary nonlinearity allows nonlinear MID to include information contained in higher order stimulus/response correlations and to find the linear combination that captures the most information about the neural response. Unlike multidimensional linear MID, nonlinear MID is one-dimensional in the quadratic stimulus space and therefore avoids the curse of dimensionality in the calculation of the objective function.</p>
      </sec>
      <sec id="s2c">
        <title>Application to simulated neurons</title>
        <p>To test and compare the two proposed methods, both to each other and to the established methods such as STC and MID, we created two model cells designed to mimic properties of neurons in primary visual cortex (V1). The first model cell was designed to have two relevant dimensions, which places it in the regime where the linear MID method should work. The second model was designed to have six relevant dimensions and serves as an example of a case that would be difficult to characterize with linear MID. Using the van Hateren <xref ref-type="bibr" rid="pcbi.1002249-vanHateren1">[38]</xref> natural image database, a different set of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e049" xlink:type="simple"/></inline-formula> patches of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e050" xlink:type="simple"/></inline-formula> pixels were randomly selected as stimuli for each cell; 100 repetitions of these image sequences were presented during the course of the simulated experiment.</p>
        <p>To quantify the performance of a given dimensionality reduction method, we calculate the subspace projection <xref ref-type="bibr" rid="pcbi.1002249-Rowekamp1">[39]</xref><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e051" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e052" xlink:type="simple"/></inline-formula> is an <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e053" xlink:type="simple"/></inline-formula> matrix whose rows are the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e054" xlink:type="simple"/></inline-formula> most significant dimensions found from either <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e055" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e056" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e057" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e058" xlink:type="simple"/></inline-formula> is a matrix containing the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e059" xlink:type="simple"/></inline-formula> model cell features. This quantity is the intersection of the volumes spanned by the two sets of vectors. It is bounded between 0 and 1, with 0 meaning the two subspaces have no overlap and 1 meaning they are identical, and is invariant to a change of basis or rescaling of the vectors in either subspace.</p>
        <p>The first model cell was constructed to respond to the two Gabor features shown in <xref ref-type="fig" rid="pcbi-1002249-g002">Fig. 2A</xref> in a phase invariant manner. This cell approximates a complex cell in area V1 by responding to the square of the stimulus projections onto the Gabor features, with a firing rate proportional to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e060" xlink:type="simple"/></inline-formula>, as in the energy model <xref ref-type="bibr" rid="pcbi.1002249-Rust1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002249-Movshon1">[40]</xref>â€“<xref ref-type="bibr" rid="pcbi.1002249-Heeger2">[45]</xref>. Although the firing rate was low for this model cell, there was occasionally more than one spike per stimulus frame. These instances were rare and to simplify the analysis the neural response was binarized by setting all multiple spiking events equal to one.</p>
        <fig id="pcbi-1002249-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002249.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Model complex cell.</title>
            <p><bold>A</bold>) The two excitatory features of the model are Gabor filters 90 degrees out of phase. The quadratic nonlinearity ensures that the responses are invariant to phase. <bold>B</bold>) Subspace projections for the STC, minimal model (MM), and nonlinear and linear MID models. The normalized eigenvectors (left) corresponding to the two largest magnitude eigenvalues (right) for <bold>C</bold>) STC, <bold>D</bold>) minimal model and <bold>E</bold>) nonlinear MID method.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002249.g002" xlink:type="simple"/>
        </fig>
        <p>As expected, the STC method performed poorly due to the strong non-Gaussian properties of natural stimuli <xref ref-type="bibr" rid="pcbi.1002249-Simoncelli1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002249-Ruderman1">[46]</xref>. The STC method found a subspace with an overlap of 0.77, whereas the nonlinear MID result had an overlap of 0.87 and the minimal model subspace had an overlap of 0.90, as shown in <xref ref-type="fig" rid="pcbi-1002249-g002">Fig. 2B</xref>. For comparison, the conventional MID method searched for the two most informative dimensions and was able to recover a subspace that almost perfectly reproduced the ground truth, with an overlap of 0.98. The feature vectors found by the different methods and the corresponding eigenvalue spectra are shown in <xref ref-type="fig" rid="pcbi-1002249-g002">Fig. 2Câ€“E</xref>.</p>
        <p>A second model cell was also created to resemble a V1 complex cell, but with a divisive normalization based on inhibitory features with orthogonal orientation in the center and parallel orientation in the surround <xref ref-type="bibr" rid="pcbi.1002249-Rust1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002249-Movshon1">[40]</xref>â€“<xref ref-type="bibr" rid="pcbi.1002249-Heeger2">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1002249-Karklin1">[47]</xref>, as shown in <xref ref-type="fig" rid="pcbi-1002249-g003">Fig. 3A</xref>. The two excitatory features in the center of the receptive field have a specific orientation. The two inhibitory features in the center of the receptive field have an orientation orthogonal to that of the excitatory features, while the two suppressive features in the surround have the same orientation as the excitatory ones in the center. The nonlinear gain function for this cell is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e061" xlink:type="simple"/><label>(5)</label></disp-formula>scaled such that the average spike probability over the stimulus set was approximately 0.15. Spiking responses were binarized as for the first model cell.</p>
        <fig id="pcbi-1002249-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002249.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Model complex cell with inhibitory features.</title>
            <p><bold>A</bold>) The first two panels show the excitatory fields: two Gabor filters 90 degrees out of phase located only in the center region of the receptive field (RF). The middle two panels show two inhibitory Gabor features, also in the middle of the RF and rotated to have an orientation perpendicular to that of the excitatory features. The right two panels show two inhibitory surround features aligned in orientation to the excitatory features. A quadratic nonlinearity applied to the projection of the stimulus onto these six features ensures phase invariance. <bold>B</bold>) The subspace projections for the STC, minimal model (MM) and nonlinear MID models. The eigenvectors (left) corresponding to the six largest magnitude eigenvalues (right) using the <bold>C</bold>) STC, <bold>D</bold>) minimal models and <bold>E</bold>) nonlinear MID method.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002249.g003" xlink:type="simple"/>
        </fig>
        <p>The performance of the various dimensionality reduction methods is shown in <xref ref-type="fig" rid="pcbi-1002249-g003">Fig. 3B</xref>. The spike-triggered covariance approach finds features (<xref ref-type="fig" rid="pcbi-1002249-g003">Fig. 3C</xref>) that bear some resemblance to the model features, but have a low overlap of 0.29. In contrast, nonlinear MID and the minimal model find features with much larger overlaps: 0.84 and 0.85, respectively. Note that the linear MID was not implemented for this model cell, as the algorithm cannot recover a 6 dimensional feature space.</p>
      </sec>
      <sec id="s2d">
        <title>Feature selectivity of real neurons</title>
        <p>To demonstrate the usefulness of the new approaches proposed here for the analysis of real neural data, we analyzed the responses of 9 macaque retina ganglion cells (RGC) and 9 cells from the lateral geniculate nucleus (LGN) under naturalistic stimulus conditions <xref ref-type="bibr" rid="pcbi.1002249-Sincich2">[48]</xref> (see <xref ref-type="sec" rid="s4">Methods</xref>). In this case, the stimulus was a spot of light filling the center of the RGC or LGN receptive field with non-Gaussian intensity fluctuations.</p>
        <p>While we cannot know the true features of these neurons as we can for the model cells, this data was previously analyzed using MID <xref ref-type="bibr" rid="pcbi.1002249-Sincich1">[3]</xref> and it was found that two stimulus features explain nearly all of the information in the neural response (an average of 85% information explained across the 18 cells analyzed). We can therefore use the two linear MID features as a benchmark for comparing the features recovered with the new algorithms, using the subspace projection quantity in Eq. (4). Moreover, the veracity of these new algorithms can be tested by comparison with other studies that have used Gaussian stimuli and STC to investigate feature selectivity of retinal cells. For instance, it was previously shown that salamander RGCs are selective to 2 to 6 significant stimulus features <xref ref-type="bibr" rid="pcbi.1002249-Fairhall1">[2]</xref>. Here we examine if the new algorithms can find a similar number of features in macaque RGCs.</p>
        <p>We show the result of fitting the minimal model to one of the RGCs. The parameters are shown in <xref ref-type="fig" rid="pcbi-1002249-g004">Fig. 4A</xref>; the 50 dimensional linear term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e062" xlink:type="simple"/></inline-formula> is plotted as a function of time before a spike and the matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e063" xlink:type="simple"/></inline-formula> is shown in the inset. The eigenvalue spectrum of this cell is shown in <xref ref-type="fig" rid="pcbi-1002249-g004">Fig. 4B</xref>. The eigenvectors corresponding to the two largest eigenvalues are shown in <xref ref-type="fig" rid="pcbi-1002249-g004">Fig. 4C</xref> (solid curves); the MID features (dashed curves), shown for comparison, captured 92% of the information. These two subspaces are very similar, with an overlap of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e064" xlink:type="simple"/></inline-formula>, demonstrating that the minimal model method is able to accurately identify the two features of this cell.</p>
        <fig id="pcbi-1002249-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002249.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Minimal model of retinal feature selectivity in a retinal ganglion cell.</title>
            <p>A second order minimal model was fit to the spike train of a RGC. <bold>A</bold>) The feature <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e065" xlink:type="simple"/></inline-formula> that controls the linear term in the argument of the logistic nonlinearity, plotted as a function of time before the neural response. The matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e066" xlink:type="simple"/></inline-formula> that controls the quadratic term is shown as an inset. <bold>B</bold>) The eigenvalue spectrum for this cell has two significant features. The inset shows a histogram of the number of significant features across the population of 9 retinal cells and 9 thalamic cells. All cells fell in the range of 2 to 5 features. <bold>C</bold>) The minimal model eigenvectors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e067" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e068" xlink:type="simple"/></inline-formula> corresponding to the two largest eigenvalues (solid) along with the two most informative features (dashed). The most informative dimensions and these eigenvectors had a subspace projection of 0.93. This analysis thus validates the minimal model algorithm by applying it to neural data in a case where the relevant dimensions can be obtained by an existing and well established method.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002249.g004" xlink:type="simple"/>
        </fig>
        <p>Although the two most informative dimensions captured a very large percentage of the information in the neural response <xref ref-type="bibr" rid="pcbi.1002249-Sincich1">[3]</xref>, the number of significant features found using the minimal model approach ranged from 2 to 5, echoing the previous work <xref ref-type="bibr" rid="pcbi.1002249-Fairhall1">[2]</xref> in salamander retina using white noise stimuli and STC. The number of cells with a given number of significant features is shown in the histogram in <xref ref-type="fig" rid="pcbi-1002249-g004">Fig. 4B</xref>. Most of the cells were dominated by one or two features, with additional weakly influential dimensions having significant curvature, in agreement with previous findings <xref ref-type="bibr" rid="pcbi.1002249-Fairhall1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002249-Sincich1">[3]</xref>.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>Both of the methods proposed here find relevant subspaces using second order stimulus statistics and can therefore be seen as extensions of the STC method. The minimal model is forced to have a logistic function nonlinearity, which has the benefit of removing unwanted model bias regarding higher than second order stimulus moments. In contrast, nonlinear MID uses an arbitrary nonlinear gain function and is therefore able to make use of higher order statistics to maximize information. Although both methods yield models consistent with first and second order stimulus/response correlations, neither method is guaranteed to work if the underlying neural computation does not match the structure of the model or the assumptions that underlie the estimation of relevant features.</p>
      <p>In principle, the flexibility in the nonlinear MID gain function means it should perform at least as well as the minimal model. However, what we have observed is that the nonlinear MID subspace projection with these two model cells is slightly smaller than the minimal model subspace. This may be due to the differences in the nature of the optimization problems being solved in the two methods. Maximizing noise entropy under constraints is a convex optimization problem <xref ref-type="bibr" rid="pcbi.1002249-Malouf1">[49]</xref>, whereas maximizing mutual information is not convex. This means that the parameter space for nonlinear MID may contain many local maxima. Although the MID algorithm uses simulated annealing to overcome this issue, the number of iterations required to outperform the minimal model may be large. We have observed (data not shown) that minimal models can find feature spaces with extremely high dimensionality <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e069" xlink:type="simple"/></inline-formula>, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e070" xlink:type="simple"/></inline-formula>, which corresponds to finding on the order of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e071" xlink:type="simple"/></inline-formula> values of the covariance matrix.</p>
      <p>Neurons with selectivity for only a few features that are probed with non-Gaussian stimuli, such as the model cell shown in <xref ref-type="fig" rid="pcbi-1002249-g002">Fig. 2</xref> or the RGC in <xref ref-type="fig" rid="pcbi-1002249-g004">Fig. 4</xref>, can be characterized very well with MID, as previously shown <xref ref-type="bibr" rid="pcbi.1002249-Sharpee1">[23]</xref>. Thus, in such cases MID is a useful tool for estimating the relevant features. We have found that for both real and model neurons with a small number of relevant features, the minimum and maximum information models performed quite well, despite the large number of parameters that need to be estimated. In particular, both methods were able to outperform STC in the recovery of the relevant stimulus subspace. On the other hand, when the dimensionality of the feature space is larger, as for the 6 dimensional cell in <xref ref-type="fig" rid="pcbi-1002249-g003">Fig. 3</xref>, linear MID cannot be used reliably due to the massive amount of data needed to construct a 6 dimensional empirical spike-conditional probability distribution. Because in the case of model cells the relevant features are known, we can verify that the minimal models and nonlinear MID approaches are able to find all of the features, whereas STC performs significantly worse. Furthermore, the fact that the second-order minimal models yielded a similar number (2â€“5) of relevant dimensions across the neural population as was previously described with Gaussian stimuli can be viewed as a further validation of the new method. It is our hope that these new techniques will advance the characterization of neural feature selectivity under a variety of stimulus conditions.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Ethics statement</title>
        <p>Experimental data were collected as part of a previous study using procedures approved by the UCSF Institutional Animal Care and Use Committee, and in accordance with National Institutes of Health guidelines.</p>
      </sec>
      <sec id="s4b">
        <title>Spike-triggered covariance</title>
        <p>When applied to stimuli with correlations, a whitening procedure can be used to correct for them <xref ref-type="bibr" rid="pcbi.1002249-Bialek1">[18]</xref>. This procedure can still be used if stimuli are non-Gaussian, but the results are biased <xref ref-type="bibr" rid="pcbi.1002249-Paninski2">[29]</xref>. The whitening operation can be performed after diagonalization of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e072" xlink:type="simple"/></inline-formula> by multiplying the eigenvectors by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e073" xlink:type="simple"/></inline-formula>, the inverse of the prior covariance matrix.</p>
        <p>Whitening has the consequence of amplifying noise along poorly sampled dimensions. To combat this effect, we regularize using a technique called ridge regression <xref ref-type="bibr" rid="pcbi.1002249-Hastie1">[50]</xref> in our analysis, in which <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e074" xlink:type="simple"/></inline-formula> instead of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e075" xlink:type="simple"/></inline-formula> is used in the whitening step. Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e076" xlink:type="simple"/></inline-formula> is the identity matrix and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e077" xlink:type="simple"/></inline-formula> is a regularization parameter that was varied for both model cells to identify the value which gave the largest overlap. This value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e078" xlink:type="simple"/></inline-formula> was used to give a best case estimate of STC performance. We note that this procedure gives more credit to STC compared to the other methods used here because it is not possible to evaluate a cross-validation metric such as percent information explained when many dimensions are involved.</p>
      </sec>
      <sec id="s4c">
        <title>Maximally informative dimensions</title>
        <p>Maximally informative dimensions <xref ref-type="bibr" rid="pcbi.1002249-Sharpee1">[23]</xref> is an algorithm that finds one or more linear combinations of the stimulus dimensions, i.e. a reduced stimulus vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e079" xlink:type="simple"/></inline-formula>, that maximizes the information per spike <xref ref-type="bibr" rid="pcbi.1002249-Brenner2">[51]</xref><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e080" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e081" xlink:type="simple"/></inline-formula> is the total number of stimuli. The mutual information between the stimulus features and the neural response (the presence of a spike, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e082" xlink:type="simple"/></inline-formula>, or its absence, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e083" xlink:type="simple"/></inline-formula>) is a sum of contributions from both types of responses: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e084" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e085" xlink:type="simple"/></inline-formula> defined by replacing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e086" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e087" xlink:type="simple"/></inline-formula> in Eq. (6). However, in the limit of small time bins where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e088" xlink:type="simple"/></inline-formula> in most of the bins, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e089" xlink:type="simple"/></inline-formula>, which leads to vanishing contributions from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e090" xlink:type="simple"/></inline-formula>. In this case, one can optimize either <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e091" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e092" xlink:type="simple"/></inline-formula> to find the relevant features <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e093" xlink:type="simple"/></inline-formula> along which the probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e094" xlink:type="simple"/></inline-formula> is most different from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e095" xlink:type="simple"/></inline-formula> according to the Kullback-Leibler distance, cf. Eq. (6). We note that this optimization is not convex and therefore a standard gradient ascent algorithm may not find the global maximum. An algorithm that combines stochastic gradient ascent with simulated annealing is publicly available at <ext-link ext-link-type="uri" xlink:href="http://cnl-t.salk.edu" xlink:type="simple">http://cnl-t.salk.edu</ext-link>.</p>
        <p>To extend the MID algorithm to nonlinear MID (nMID), the stimulus is simply transformed by a nonlinear operation. For the second order nonlinear transformation considered in this paper, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e096" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e097" xlink:type="simple"/></inline-formula> is a vector whose first <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e098" xlink:type="simple"/></inline-formula> components are the components of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e099" xlink:type="simple"/></inline-formula> and the remaining components are the elements of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e100" xlink:type="simple"/></inline-formula>. Due to the symmetry of the outer product matrix, this transformed stimulus dimensionality is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e101" xlink:type="simple"/></inline-formula>. In this new space, the MID algorithm works as before, finding a linear combination of these dimensions, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e102" xlink:type="simple"/></inline-formula>, such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e103" xlink:type="simple"/></inline-formula> is maximized. To improve performance and cut down on runtime, the search was started from the minimal model estimate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e104" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e105" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e106" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e107" xlink:type="simple"/></inline-formula>.</p>
        <p>To prevent overfitting of the parameters, an early stopping mechanism was used whereby the data was broken into two sets: one set was used for training and the other used for testing. The training set was used to search the parameter space, while the test set was used to evaluate the parameters on independent data. The best linear combination for both data sets was returned by the algorithm. This procedure was done four times, using four different quarters of the complete data set as the test set. The resulting parameters found from these four fittings were averaged before diagonalizing and finding the relevant features. Unlike the regularization of STC models, this procedure can be used when analyzing experimental data.</p>
      </sec>
      <sec id="s4d">
        <title>Minimal models</title>
        <p>The model of the neural response that matches experimental observations in terms of the mean response probability, as well as correlations between the neural response with linear and quadratic moments of stimuli can be obtained by enforcing<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e108" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e109" xlink:type="simple"/></inline-formula> is an average over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e110" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e111" xlink:type="simple"/></inline-formula> is an average over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e112" xlink:type="simple"/></inline-formula>. Because <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e113" xlink:type="simple"/></inline-formula>, this reduces to a set of<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e114" xlink:type="simple"/><label>(8)</label></disp-formula>equations. Simultaneously satisfying these equations is analytically equivalent to maximizing the log likelihood of the data <xref ref-type="bibr" rid="pcbi.1002249-Malouf1">[49]</xref>, which is convex and can therefore be maximized using a conjugate gradient ascent algorithm.</p>
        <p>To prevent overfitting of the parameters, an early stopping procedure was implemented similar to that used in the MID algorithm. Each step of the algorithm increased the likelihood of the training set, but at some point began decreasing the likelihood of the test set, indicating the fitting of noise within the training set. The algorithm then returned the parameters found at the maximum likelihood of the test set. As described above, this was done four times with different quarters of the data serving as the test set and the resulting parameter vectors were averaged before diagonalizing the matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e115" xlink:type="simple"/></inline-formula>.</p>
        <p>Significance testing of the eigenvalues was done by creating 500 Gaussian distributed random matrices with the same variance as that of the set of elements of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e116" xlink:type="simple"/></inline-formula>. These random matrices were each diagonalized to create a random eigenvalue distribution. Eigenvalues of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e117" xlink:type="simple"/></inline-formula> were then said to be significant if they fell below the lower 2.5<sup>th</sup> percentile or above the 97.5<sup>th</sup> percentile.</p>
      </sec>
      <sec id="s4e">
        <title>Physiology experiments</title>
        <p>The data analyzed in this paper were collected in a previous study <xref ref-type="bibr" rid="pcbi.1002249-Sincich2">[48]</xref> and the details are found therein. The stimulus was a spot of light covering a cell's receptive field center, flickering with non-Gaussian statistics that mimic those of light intensity fluctuations found in natural environments <xref ref-type="bibr" rid="pcbi.1002249-Simoncelli1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002249-vanHateren1">[38]</xref>. The values of light intensities were updated every <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e118" xlink:type="simple"/></inline-formula> (update rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002249.e119" xlink:type="simple"/></inline-formula>). The spikes were recorded extracellularly in the LGN with high signal-to-noise ratio, allowing for excitatory post-synaptic potentials generated by the RGC inputs to be recorded. From such data, the complete spike trains of the RGCs could be reconstructed.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank Jonathan C. Horton for sharing the data collected in his laboratory and the Sharpee group for helpful conversations.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002249-Brenner1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Brenner</surname><given-names>N</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name></person-group>             <year>2000</year>             <article-title>Adaptive rescaling maximizes information transmission.</article-title>             <source>Neuron</source>             <volume>26</volume>             <fpage>695</fpage>             <lpage>702</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Fairhall1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fairhall</surname><given-names>AL</given-names></name><name name-style="western"><surname>Burlingame</surname><given-names>CA</given-names></name><name name-style="western"><surname>Narasimhan</surname><given-names>R</given-names></name><name name-style="western"><surname>Harris</surname><given-names>RA</given-names></name><name name-style="western"><surname>Puchalla</surname><given-names>JL</given-names></name><etal/></person-group>             <year>2006</year>             <article-title>Selectivity for multiple stimulus features in retinal ganglion cells.</article-title>             <source>J Neurophysiol</source>             <volume>96</volume>             <fpage>2724</fpage>             <lpage>2738</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Sincich1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sincich</surname><given-names>LC</given-names></name><name name-style="western"><surname>Horton</surname><given-names>JC</given-names></name><name name-style="western"><surname>Sharpee</surname><given-names>TO</given-names></name></person-group>             <year>2009</year>             <article-title>Preserving information in neural transmission.</article-title>             <source>J Neurosci</source>             <volume>29</volume>             <fpage>6207</fpage>             <lpage>6216</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Touryan1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Touryan</surname><given-names>J</given-names></name><name name-style="western"><surname>Lau</surname><given-names>B</given-names></name><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name></person-group>             <year>2002</year>             <article-title>Isolation of relevant visual features from random stimuli for cortical complex cells.</article-title>             <source>J Neurosci</source>             <volume>22</volume>             <fpage>10811</fpage>             <lpage>10818</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Touryan2">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Touryan</surname><given-names>J</given-names></name><name name-style="western"><surname>Felsen</surname><given-names>G</given-names></name><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name></person-group>             <year>2005</year>             <article-title>Spatial structure of complex cell receptive fields measured with natural images.</article-title>             <source>Neuron</source>             <volume>45</volume>             <fpage>781</fpage>             <lpage>791</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Felsen1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Felsen</surname><given-names>G</given-names></name><name name-style="western"><surname>Touryan</surname><given-names>J</given-names></name><name name-style="western"><surname>Han</surname><given-names>F</given-names></name><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name></person-group>             <year>2005</year>             <article-title>Cortical sensitivity to visual features in natural scenes.</article-title>             <source>PLoS Biol</source>             <volume>3</volume>             <fpage>e342</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Rust1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name><name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name><name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group>             <year>2005</year>             <article-title>Spatiotemporal elements of macaque V1 receptive fields.</article-title>             <source>Neuron</source>             <volume>46</volume>             <fpage>945</fpage>             <lpage>956</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Chen1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X</given-names></name><name name-style="western"><surname>Han</surname><given-names>F</given-names></name><name name-style="western"><surname>Poo</surname><given-names>MM</given-names></name><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name></person-group>             <year>2007</year>             <article-title>Excitatory and suppressive receptive field subunits in awake monkey primary visual cortex (V1).</article-title>             <source>Proc Natl Acad Sci USA</source>             <volume>104</volume>             <fpage>19120</fpage>             <lpage>5</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Cantrell1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cantrell</surname><given-names>DR</given-names></name><name name-style="western"><surname>Cang</surname><given-names>J</given-names></name><name name-style="western"><surname>Troy</surname><given-names>JB</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X</given-names></name></person-group>             <year>2010</year>             <article-title>Non-centered spike-triggered covariance analysis reveals neurotrophin-3 as a developmental regulator of receptive field properties of on-off retinal ganglion cells.</article-title>             <source>PLoS Comput Biol</source>             <volume>6</volume>             <fpage>e1000967</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Horwitz1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Horwitz</surname><given-names>GD</given-names></name><name name-style="western"><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name name-style="western"><surname>Albright</surname><given-names>TD</given-names></name></person-group>             <year>2005</year>             <article-title>Blue-yellow signals are enhanced by spatiotem- poral luminance contrast in macaque V1.</article-title>             <source>J Neurophys</source>             <volume>93</volume>             <fpage>2263</fpage>             <lpage>2278</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Horwitz2">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Horwitz</surname><given-names>GD</given-names></name><name name-style="western"><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name name-style="western"><surname>Albright</surname><given-names>TD</given-names></name></person-group>             <year>2007</year>             <article-title>Cone inputs to simple and complex cells in V1 of awake macaque.</article-title>             <source>J Neurophys</source>             <volume>97</volume>             <fpage>3070</fpage>             <lpage>3081</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Tanabe1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tanabe</surname><given-names>S</given-names></name><name name-style="western"><surname>Haefner</surname><given-names>RM</given-names></name><name name-style="western"><surname>Cumming</surname><given-names>BG</given-names></name></person-group>             <year>2011</year>             <article-title>Suppressive mechanisms in monkey V1 help to solve the stereo correspondence problem.</article-title>             <source>J Neurosci</source>             <volume>31</volume>             <fpage>8295</fpage>             <lpage>8305</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Atencio1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Atencio</surname><given-names>CA</given-names></name><name name-style="western"><surname>Sharpee</surname><given-names>TO</given-names></name><name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name></person-group>             <year>2008</year>             <article-title>Cooperative nonlinearities in auditory cortical neurons.</article-title>             <source>Neuron</source>             <volume>58</volume>             <fpage>956</fpage>             <lpage>966</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Kim1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Lazar</surname><given-names>AA</given-names></name><name name-style="western"><surname>Slutskiy</surname><given-names>YB</given-names></name></person-group>             <year>2011</year>             <article-title>System identification of drosophila olfactory sensory neurons.</article-title>             <source>J Comput Neurosci</source>             <volume>30</volume>             <fpage>143</fpage>             <lpage>161</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Maravall1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Maravall</surname><given-names>M</given-names></name><name name-style="western"><surname>Petersen</surname><given-names>RS</given-names></name><name name-style="western"><surname>Fairhall</surname><given-names>A</given-names></name><name name-style="western"><surname>Arabzadeh</surname><given-names>E</given-names></name><name name-style="western"><surname>Diamond</surname><given-names>M</given-names></name></person-group>             <year>2007</year>             <article-title>Shifts in coding properties and maintenance of information transmission during adaptation in barrel cortex.</article-title>             <source>PLoS Biol</source>             <volume>5</volume>             <fpage>e19</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Fox1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fox</surname><given-names>JL</given-names></name><name name-style="western"><surname>Fairhall</surname><given-names>AL</given-names></name><name name-style="western"><surname>Daniel</surname><given-names>TL</given-names></name></person-group>             <year>2010</year>             <article-title>Encoding properties of haltere neurons enable motion feature detection in a biological gyroscope.</article-title>             <source>Proc Natl Acad Sci USA</source>             <volume>107</volume>             <fpage>3840</fpage>             <lpage>3845</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-deRuytervanSteveninck1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>1988</year>             <article-title>Real-time performance of a movement-sensitive neuron in the blowfly visual system: coding and information transfer in short spike sequences.</article-title>             <source>Proc R Soc Lond B</source>             <volume>265</volume>             <fpage>259</fpage>             <lpage>265</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Bialek1">
        <label>18</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name></person-group>             <year>2005</year>             <article-title>Features and dimensions: Motion estimation in fly vision.</article-title>             <comment>Q-bio/2005;0505003</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Schwartz1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name><name name-style="western"><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>E</given-names></name></person-group>             <year>2002</year>             <article-title>Characterizing neural gain control using spike-triggered covariance.</article-title>             <source>Adv Neural Inf Process Syst</source>             <volume>14</volume>             <fpage>269</fpage>             <lpage>276</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Paninski1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name></person-group>             <year>2003</year>             <article-title>Convergence properties of some spike-triggered analysis techniques.</article-title>             <source>Advances in Neural Information Processing</source>             <volume>15</volume>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Pillow1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pillow</surname><given-names>J</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group>             <year>2006</year>             <article-title>Dimensionality reduction in neural models: An information-theoretic generalization of spike-triggered average and covariance analysis.</article-title>             <source>J Vis</source>             <volume>6</volume>             <fpage>414</fpage>             <lpage>428</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Schwartz2">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name><name name-style="western"><surname>Pillow</surname><given-names>J</given-names></name><name name-style="western"><surname>Rust</surname><given-names>N</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group>             <year>2006</year>             <article-title>Spike-triggered neural characterization.</article-title>             <source>J Vis</source>             <volume>176</volume>             <fpage>484</fpage>             <lpage>507</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Sharpee1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sharpee</surname><given-names>T</given-names></name><name name-style="western"><surname>Rust</surname><given-names>N</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>2004</year>             <article-title>Analyzing neural responses to natural signals: Maximally informative dimensions.</article-title>             <source>Neural Comput</source>             <volume>16</volume>             <fpage>223</fpage>             <lpage>250</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-deBoer1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>de Boer</surname><given-names>E</given-names></name><name name-style="western"><surname>Kuyper</surname><given-names>P</given-names></name></person-group>             <year>1968</year>             <article-title>Triggered correlation.</article-title>             <source>IEEE Trans Biomed Eng</source>             <volume>15</volume>             <fpage>169</fpage>             <lpage>179</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Shapley1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shapley</surname><given-names>RM</given-names></name><name name-style="western"><surname>Victor</surname><given-names>JD</given-names></name></person-group>             <year>1978</year>             <article-title>The effect of contrast on the transfer properties of cat retinal ganglion cells.</article-title>             <source>J Physiol</source>             <volume>285</volume>             <fpage>275</fpage>             <lpage>298</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Meister1">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Meister</surname><given-names>M</given-names></name><name name-style="western"><surname>Berry</surname><given-names>MJ</given-names></name></person-group>             <year>1999</year>             <article-title>The neural code of the retina.</article-title>             <source>Neuron</source>             <volume>22</volume>             <fpage>435</fpage>             <lpage>450</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Rieke1">
        <label>27</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rieke</surname><given-names>F</given-names></name><name name-style="western"><surname>Warland</surname><given-names>D</given-names></name><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>1997</year>             <source>Spikes: Exploring the neural code</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Dayan1">
        <label>28</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>             <year>2001</year>             <source>Theoretical neuroscience: computational and mathematical modeling of neural systems</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Paninski2">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name></person-group>             <year>2003</year>             <article-title>Convergence properties of three spike-triggered average techniques.</article-title>             <source>Network</source>             <volume>14</volume>             <fpage>437</fpage>             <lpage>464</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Simoncelli1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name></person-group>             <year>2001</year>             <article-title>Natural image statistics and neural representation.</article-title>             <source>Annu Rev Neurosci</source>             <volume>24</volume>             <fpage>1193</fpage>             <lpage>1216</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Barlow1">
        <label>31</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barlow</surname><given-names>H</given-names></name></person-group>             <year>1961</year>             <article-title>Possible principles underlying the transformations of sensory images.</article-title>             <fpage>217</fpage>             <lpage>234</lpage>             <comment>Sensory Communication, MIT Press, Cambridge</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Barlow2">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barlow</surname><given-names>H</given-names></name></person-group>             <year>2001</year>             <article-title>Redundancy reduction revisited.</article-title>             <source>Network</source>             <volume>12</volume>             <fpage>241</fpage>             <lpage>253</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-vonderTwer1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>von der Twer</surname><given-names>T</given-names></name><name name-style="western"><surname>I A Macleod</surname><given-names>D</given-names></name></person-group>             <year>2001</year>             <article-title>Optimal nonlinear codes for the perception of natural colours.</article-title>             <source>Network</source>             <volume>12</volume>             <fpage>395</fpage>             <lpage>407</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Bellman1">
        <label>34</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bellman</surname><given-names>RE</given-names></name></person-group>             <year>1961</year>             <source>Adaptive processes - A guided tour</source>             <publisher-loc>Princeton, NJ</publisher-loc>             <publisher-name>Princeton University Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Cover1">
        <label>35</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cover</surname><given-names>TM</given-names></name><name name-style="western"><surname>Thomas</surname><given-names>JA</given-names></name></person-group>             <year>1991</year>             <source>Information theory</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>John Wiley &amp; Sons, Inc</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Fitzgerald1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fitzgerald</surname><given-names>JD</given-names></name><name name-style="western"><surname>Sincich</surname><given-names>LC</given-names></name><name name-style="western"><surname>Sharpee</surname><given-names>TO</given-names></name></person-group>             <year>2011</year>             <article-title>Minimal models of multidimensional computations.</article-title>             <source>PLoS Comput Biol</source>             <volume>7</volume>             <fpage>e1001111</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Globerson1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Globerson</surname><given-names>A</given-names></name><name name-style="western"><surname>Stark</surname><given-names>E</given-names></name><name name-style="western"><surname>Vaadia</surname><given-names>E</given-names></name><name name-style="western"><surname>Tishby</surname><given-names>N</given-names></name></person-group>             <year>2009</year>             <article-title>The minimum information principle and its application to neural code analysis.</article-title>             <source>Proc Natl Acad Sci USA</source>             <volume>106</volume>             <fpage>3490</fpage>             <lpage>3495</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-vanHateren1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>van Hateren</surname><given-names>JH</given-names></name></person-group>             <year>1997</year>             <article-title>Processing of natural time series of intensities by the visual system of the blowfly.</article-title>             <source>Vision Res</source>             <volume>37</volume>             <fpage>3407</fpage>             <lpage>3416</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Rowekamp1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rowekamp</surname><given-names>RJ</given-names></name><name name-style="western"><surname>Sharpee</surname><given-names>TO</given-names></name></person-group>             <year>2011</year>             <article-title>Analyzing multicomponent receptive fields from neural responses to natural stimuli.</article-title>             <source>Network</source>             <volume>22</volume>             <fpage>1</fpage>             <lpage>29</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Movshon1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name><name name-style="western"><surname>Thompson</surname><given-names>ID</given-names></name><name name-style="western"><surname>Tolhurst</surname><given-names>DJ</given-names></name></person-group>             <year>1978</year>             <article-title>Receptive field organization of complex cells in the cat's striate cortex.</article-title>             <source>J Physiol (Lond)</source>             <volume>283</volume>             <fpage>79</fpage>             <lpage>99</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Adelson1">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Adelson</surname><given-names>EH</given-names></name><name name-style="western"><surname>Bergen</surname><given-names>JR</given-names></name></person-group>             <year>1985</year>             <article-title>Spatio-temporal energy models for the perception of motion.</article-title>             <source>J Opt Soc Am</source>             <volume>2</volume>             <fpage>284</fpage>             <lpage>299</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Carandini1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name><name name-style="western"><surname>Heeger</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name></person-group>             <year>1997</year>             <article-title>Linearity and normalization in simple cells of the macaque primary visual cortex.</article-title>             <source>J Neurosci</source>             <volume>17</volume>             <fpage>8621</fpage>             <lpage>8644</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Cavanaugh1">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cavanaugh</surname><given-names>JR</given-names></name><name name-style="western"><surname>Bair</surname><given-names>W</given-names></name><name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name></person-group>             <year>2002</year>             <article-title>Nature and interaction of signals from the receptive field center and surround in macaque V1 neurons.</article-title>             <source>J Neurophysiol</source>             <volume>88</volume>             <fpage>2530</fpage>             <lpage>2546</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Heeger1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Heeger</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name><name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name></person-group>             <year>1996</year>             <article-title>Computational models of cortical visual processing.</article-title>             <source>Proc Natl Acad Sci USA</source>             <volume>93</volume>             <fpage>623</fpage>             <lpage>627</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Heeger2">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Heeger</surname><given-names>DJ</given-names></name></person-group>             <year>1992</year>             <article-title>Normalization of cell responses in cat striate cortex.</article-title>             <source>Vis Neurosci</source>             <volume>9</volume>             <fpage>181</fpage>             <lpage>197</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Ruderman1">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ruderman</surname><given-names>DL</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>1994</year>             <article-title>Statistics of natural images: scaling in the woods.</article-title>             <source>Phys Rev Lett</source>             <volume>73</volume>             <fpage>814</fpage>             <lpage>817</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Karklin1">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Karklin</surname><given-names>Y</given-names></name><name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name></person-group>             <year>2009</year>             <article-title>Emergence of complex cell properties by learning to generalize in natural scenes.</article-title>             <source>Nature</source>             <volume>457</volume>             <fpage>83</fpage>             <lpage>86</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Sincich2">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sincich</surname><given-names>LC</given-names></name><name name-style="western"><surname>Adams</surname><given-names>DL</given-names></name><name name-style="western"><surname>Economides</surname><given-names>JR</given-names></name><name name-style="western"><surname>Horton</surname><given-names>JC</given-names></name></person-group>             <year>2007</year>             <article-title>Transmission of spike trains at the retinogeniculate synapse.</article-title>             <source>J Neurosci</source>             <volume>27</volume>             <fpage>2683</fpage>             <lpage>2692</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Malouf1">
        <label>49</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Malouf</surname><given-names>R</given-names></name></person-group>             <year>2002</year>             <article-title>A comparison of algorithms for maximum entropy parameter estimation.</article-title>             <fpage>49</fpage>             <lpage>55</lpage>             <comment>Proceedings of the Conference on Natural Language Learning</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Hastie1">
        <label>50</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name><name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name><name name-style="western"><surname>Friedman</surname><given-names>J</given-names></name></person-group>             <year>2001</year>             <source>The elements of statistical learning: data mining, inference, and prediction</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Springer-Verlag</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002249-Brenner2">
        <label>51</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Brenner</surname><given-names>N</given-names></name><name name-style="western"><surname>Strong</surname><given-names>SP</given-names></name><name name-style="western"><surname>Koberle</surname><given-names>R</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name></person-group>             <year>2000</year>             <article-title>Synergy in a neural code.</article-title>             <source>Neural Comput</source>             <volume>12</volume>             <fpage>1531</fpage>             <lpage>1552</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>