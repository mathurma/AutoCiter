<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006690</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00510</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Clinical medicine</subject><subj-group><subject>Clinical neurophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Diagnostic medicine</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Radiology and imaging</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Clinical medicine</subject><subj-group><subject>Clinical neurophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Reaction time</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Reaction time</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Computer vision</subject><subj-group><subject>Target detection</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Scene complexity modulates degree of feedback activity during object detection in natural scenes</article-title>
<alt-title alt-title-type="running-head">Scene complexity affects object detection</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5536-6128</contrib-id>
<name name-style="western">
<surname>Groen</surname>
<given-names>Iris I. A.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1979-589X</contrib-id>
<name name-style="western">
<surname>Jahfari</surname>
<given-names>Sara</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8785-8135</contrib-id>
<name name-style="western">
<surname>Seijdel</surname>
<given-names>Noor</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ghebreab</surname>
<given-names>Sennay</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4013-3687</contrib-id>
<name name-style="western">
<surname>Lamme</surname>
<given-names>Victor A. F.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9839-1788</contrib-id>
<name name-style="western">
<surname>Scholte</surname>
<given-names>H. Steven</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>New York University, Department of Psychology, New York, New York, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Spinoza Centre for Neuroimaging, Royal Netherlands Academy of Arts and Sciences (KNAW), Amsterdam, The Netherlands</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>University of Amsterdam, Department of Psychology, Section Brain and Cognition, Amsterdam, The Netherlands</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>University of Amsterdam, Department of Informatics, Intelligent Systems Lab, Amsterdam, The Netherlands</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gershman</surname>
<given-names>Samuel J.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Harvard University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">iris.groen@nyu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>31</day>
<month>12</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>12</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>12</issue>
<elocation-id>e1006690</elocation-id>
<history>
<date date-type="received">
<day>1</day>
<month>4</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>1</day>
<month>12</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Groen et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006690"/>
<abstract>
<p>Selective brain responses to objects arise within a few hundreds of milliseconds of neural processing, suggesting that visual object recognition is mediated by rapid feed-forward activations. Yet disruption of neural responses in early visual cortex beyond feed-forward processing stages affects object recognition performance. Here, we unite these discrepant findings by reporting that object recognition involves enhanced feedback activity (recurrent processing within early visual cortex) when target objects are embedded in natural scenes that are characterized by high complexity. Human participants performed an animal target detection task on natural scenes with low, medium or high complexity as determined by a computational model of low-level contrast statistics. Three converging lines of evidence indicate that feedback was selectively enhanced for high complexity scenes. First, functional magnetic resonance imaging (fMRI) activity in early visual cortex (V1) was enhanced for target objects in scenes with high, but not low or medium complexity. Second, event-related potentials (ERPs) evoked by target objects were selectively enhanced at feedback stages of visual processing (from ~220 ms onwards) for high complexity scenes only. Third, behavioral performance for high complexity scenes deteriorated when participants were pressed for time and thus less able to incorporate the feedback activity. Modeling of the reaction time distributions using drift diffusion revealed that object information accumulated more slowly for high complexity scenes, with evidence accumulation being coupled to trial-to-trial variation in the EEG feedback response. Together, these results suggest that while feed-forward activity may suffice to recognize isolated objects, the brain employs recurrent processing more adaptively in naturalistic settings, using minimal feedback for simple scenes and increasing feedback for complex scenes.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>How much neural processing is required to detect objects of interest in natural scenes? The speed and efficiency of object recognition suggests that fast feed-forward buildup of perceptual activity is sufficient. However, there is also evidence that disruption of visual processing beyond feed-forward stages leads to decreased object detection performance. Our study resolves this discrepancy by identifying scene complexity as a driver of recurrent activity. We show that recurrent activity is enhanced for complex, cluttered scenes compared to simple, well-organized scenes. Moreover, recurrent activity reflects the amount of accumulated evidence for target object presence. These findings elucidate the neural processes underlying perceptual decision-making by demonstrating that the brain dynamically directs neural resources based on the complexity of real-world visual inputs.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek (NL)</institution>
</funding-source>
<award-id>Rubicon Fellowship</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5536-6128</contrib-id>
<name name-style="western">
<surname>Groen</surname>
<given-names>Iris I. A.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>European Research Council ()</institution>
</funding-source>
<award-id>Advanced Investigator Grant</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4013-3687</contrib-id>
<name name-style="western">
<surname>Lamme</surname>
<given-names>Victor A. F.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>Dutch public-private research program</institution>
</funding-source>
<award-id>COMMIT</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Ghebreab</surname>
<given-names>Sennay</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work is part of the Research Priority Program ‘Brain and Cognition’ at the University of Amsterdam. IIAG was supported by a Rubicon Fellowship from the Netherlands Organization for Scientific Research (NWO: <ext-link ext-link-type="uri" xlink:href="https://www.nwo.nl/en" xlink:type="simple">https://www.nwo.nl/en</ext-link>) and VAFL was supported by an Advanced Investigator Grant from the European Research Council (ERC: <ext-link ext-link-type="uri" xlink:href="http://erc.europa.eu/" xlink:type="simple">http://erc.europa.eu/</ext-link>). SG was supported by the Dutch public-private research program COMMIT (<ext-link ext-link-type="uri" xlink:href="http://www.commit-nl.nl" xlink:type="simple">http://www.commit-nl.nl</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="1"/>
<page-count count="28"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-01-11</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data reported in this manuscript have been made available on the Open Science Framework database (DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/QTBU2" xlink:type="simple">10.17605/OSF.IO/QTBU2</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://osf.io/qtbu2/" xlink:type="simple">https://osf.io/qtbu2/</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Object recognition is often regarded as a task that is solved in the first wave of visual processing [<xref ref-type="bibr" rid="pcbi.1006690.ref001">1</xref>]. The human brain indeed recognizes objects at astonishing speed, with single neurons exhibiting object-selectivity from 100 ms after stimulus onset [<xref ref-type="bibr" rid="pcbi.1006690.ref002">2</xref>], and global brain signals diverging within 100–200 ms [<xref ref-type="bibr" rid="pcbi.1006690.ref003">3</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref004">4</xref>]. Furthermore, hierarchical feed-forward models can emulate human object recognition performance [<xref ref-type="bibr" rid="pcbi.1006690.ref005">5</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref006">6</xref>], and neural representations in human and non-human primate brains match those in feed-forward neural networks [<xref ref-type="bibr" rid="pcbi.1006690.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref009">9</xref>].</p>
<p>However, the visual system is not a strict feed-forward hierarchy: it also contains an abundance of feedback connections [<xref ref-type="bibr" rid="pcbi.1006690.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref013">13</xref>]. Visual response modulations that occur within visual cortex after the initial feed-forward sweep has passed (i.e. beyond ~150 ms after stimulus onset) are thought to reflect recurrent interactions (‘feedback’) between e.g. V1-IT that aid segmentation of figures from backgrounds [<xref ref-type="bibr" rid="pcbi.1006690.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref019">19</xref>] and perceptual grouping [<xref ref-type="bibr" rid="pcbi.1006690.ref020">20</xref>]. One way in which feedback is thought to facilitate object recognition is through <italic>visual routines</italic> such as curve tracing and texture segmentation, to integrate line segments and other low-level features encoded in early visual areas [<xref ref-type="bibr" rid="pcbi.1006690.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref023">23</xref>]. Supporting this model of visual processing, transcranial stimulation evidence shows that detection of target objects in natural scenes deteriorates when neural activity in early visual cortex is disrupted not just at feed-forward processing stages (e.g., 100 ms after stimulus onset), but also at feedback stages (e.g., 220 ms after stimulus onset [<xref ref-type="bibr" rid="pcbi.1006690.ref024">24</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref025">25</xref>]).</p>
<p>How can we reconcile the speed of object recognition with an important role for feedback? Two lines of evidence suggest that feedback may be employed <italic>adaptively</italic> depending on the complexity of the visual input. First, computer simulations indicate that disrupting feedback activity has stronger effects for occluded or degraded target objects [<xref ref-type="bibr" rid="pcbi.1006690.ref026">26</xref>]. Second, backward masking, which interrupts recurrent processing [<xref ref-type="bibr" rid="pcbi.1006690.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref029">29</xref>], has weaker effects for scenes with target objects that are “easily segregated” compared to “more demanding backgrounds”, as assessed through behavioral ratings of independent observers [<xref ref-type="bibr" rid="pcbi.1006690.ref030">30</xref>]. But how does the visual system determine the complexity of a visual input scene? Computer vision and scene perception research shows that computational summary statistics of low-level image features are diagnostic of scene complexity [<xref ref-type="bibr" rid="pcbi.1006690.ref031">31</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref033">33</xref>]. For example, contrast distributions can be summarized by two statistics that reflect a scene’s contrast energy (CE, average contrast) and spatial coherence (SC, variability in contrast across the scene) [<xref ref-type="bibr" rid="pcbi.1006690.ref034">34</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref035">35</xref>]. Computing these statistics for a large set of scenes results in a two-dimensional space in which sparse scenes with just a few scene elements separate from complex scenes with a lot of clutter and a high degree of fragmentation (<xref ref-type="fig" rid="pcbi.1006690.g001">Fig 1<italic>A</italic></xref>). Since CE and SC appear to provide information about the ‘segmentability’ of a scene, we hypothesized that the visual system computes these scene statistics as a measure of overall scene complexity, with the goal to determine a need for enhanced visual processing mediated by feedback.</p>
<fig id="pcbi.1006690.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006690.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Stimuli and experimental paradigms.</title>
<p><bold>A)</bold> Stimulus space described by two parameters derived from the distribution of local contrast, contrast energy (CE) and spatial coherence (SC). In this space, simple images containing one or a few easily segmentable objects are on the lower left, while complex images with a high degree of fragmentation are on the upper right. Thumbnails show 100 images (50 animal, 50 non-animal) randomly drawn from the larger image set from which the stimuli were selected. <bold>B)</bold> Image statistics of the stimuli: each point represents a scene sampled from the image space described in A). Scenes had either low (red), medium (green) or high (blue) CE and SC values. Within these conditions, CE and SC values were matched between scenes with target objects (animals: “A”, filled dots) and without target objects (non-animals: “NA”, open dots). <bold>C)</bold> Exemplars from each stimulus complexity condition. <bold>D)</bold> Experimental design of Experiment 1 (fMRI). On GO trials, subjects indicated whether the scene contained a target object or not. On STOP trials, an auditory signal followed stimulus presentation after a variable inter-trial-interval (ITI), signaling that subjects had to withhold their response. Only GO trials were analyzed. <bold>E)</bold> Experimental design of Experiment 2 (EEG). Participants were instructed to respond as fast as possible on speed trials and as accurate as possible on accuracy trials and received feedback on each trial.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006690.g001" xlink:type="simple"/>
</fig>
<p>Importantly, for this hypothesis to be biologically realistic, CE and SC need to be a) plausibly computable in the visual system and b) available early in visual processing. The contrast distribution of a scene can in theory be deduced from the population response of contrast-sensitive neurons, e.g. neurons in early visual areas such as LGN and V1, which respond to local scene elements (edges). Since these responses constitute the first stage of the feedforward processing cascade, this information can be made available very early on in visual processing through a ‘read-out’ of the early population response across the visual scene within the feed-forward sweep. Consistent with this idea, CE and SC values computed from simulated early visual contrast responses [<xref ref-type="bibr" rid="pcbi.1006690.ref035">35</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref036">36</xref>] have been shown to modulate the magnitude of single-trial evoked responses to natural scenes as early as 100 ms after stimulus onset [<xref ref-type="bibr" rid="pcbi.1006690.ref035">35</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref037">37</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref042">42</xref>].</p>
<p>Here, we tested whether scene complexity predicts the degree of feedback activity by measuring brain responses while participants performed a target object detection task in scenes that were systematically sampled to contain either low, medium or high CE and SC values (see <xref ref-type="fig" rid="pcbi.1006690.g001">Fig 1<italic>B</italic> and 1<italic>C</italic></xref>). First, we measured whole-brain fMRI responses to target objects in scenes with low, medium or high complexity (Experiment 1). Next, we measured EEG responses to the same stimuli to examine the time-course of visually evoked activity (Experiment 2). Importantly, complexity was matched for target and non-target scenes <italic>within</italic> each complexity condition, allowing us to disambiguate any feedforward response differences due to scene complexity from subsequent feedback modulations by examining differential responses to target and non-targets. In addition, scene complexity was varied on a trial-by-trial basis, such that participants could not form an expectation of scene complexity beforehand, allowing us to measure responses with unbiased feed-forward processing (i.e. without a difference in top-down task set or attentional state).</p>
<p>Together, these experiments show that successful detection of target objects in high, but not low or medium complexity scenes is associated with enhanced activity in early visual areas (in fMRI) which emerges at feedback time-points in visual processing (in EEG). Moreover, behavioral performance for high complexity scenes was associated with decreased accuracy and slower response times, reflecting a slower rate of evidence accumulation as formalized by the drift rate parameter in the drift-diffusion model [<xref ref-type="bibr" rid="pcbi.1006690.ref043">43</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref045">45</xref>]. In addition, trial-by-trial variations in drift rate within the high complexity condition were coupled to EEG feedback responses on those trials. Together, these results demonstrate a contribution of feedback to object detection in complex natural scenes.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Experiment 1 (fMRI)</title>
<sec id="sec004">
<title>Behavior</title>
<p>In Experiment 1, participants viewed images of real-world scenes with low, medium or high complexity in the MRI scanner. On each trial, they indicated via a button press whether the scene contained a target object (animal) or not (<xref ref-type="fig" rid="pcbi.1006690.g001">Fig 1D</xref>). This detection task was embedded in a stop-signal paradigm, but for the purpose of this study, only the trials without a stop signal (‘go trials’) were analyzed (see <xref ref-type="sec" rid="sec019">Materials and methods</xref>). Behavioral performance for each condition is presented in <xref ref-type="fig" rid="pcbi.1006690.g002">Fig 2</xref>. Reaction times were found to increase gradually with scene complexity (F(2,44) = 4.96, p = 0.011, η<sup>2</sup> = 0.18). Planned post-hoc comparisons showed that RTs increased for the HIGH complexity condition compared to the LOW (t(22) = 2.7, p(Sidák-corrected) = 0.035), and the MEDIUM condition (t(22) = 2.5, p(Sidák-corrected) = 0.057), with no significant difference between the LOW and MEDIUM conditions (t(22) = 0.7, p(Sidák-corrected) = 0.85; <xref ref-type="fig" rid="pcbi.1006690.g002">Fig 2A</xref>).</p>
<fig id="pcbi.1006690.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006690.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Behavioral results of the fMRI experiment (Experiment 1).</title>
<p><bold>A)</bold> Average reaction times (RT) for the animal/non-animal categorization task per condition. <bold>B)</bold> Accuracy (percentage correct) per condition. Horizontal black lines indicate the statistical outcome of one-way repeated-measures ANOVAs; gray lines indicate the results of pairwise tests corrected for multiple comparisons using a Sidák correction. Error bars represent S.E.M. * = p &lt; 0.05, # = p &lt; 0.10.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006690.g002" xlink:type="simple"/>
</fig>
<p>Response accuracy was also modulated by scene complexity (F(2,44) = 8.37, p = 0.001, η<sup>2</sup> = 0.28). Post-hoc comparisons showed the best performance for the MEDIUM condition compared to the LOW (t(22) = 3.5, p(Sidák-corrected) = 0.007) and HIGH condition (t(22) = 3.2, p(Sidák-corrected) = 0.012), with no differences between the LOW and HIGH conditions (t(22) = 0.8, p(Sidák-corrected) = 0.77; <xref ref-type="fig" rid="pcbi.1006690.g002">Fig 2<italic>B</italic></xref>).</p>
<p>Together, these results indicate that the high scene complexity leads to significant slowing in object detection. Although response accuracy in both the LOW and HIGH condition was worse than for the MEDIUM condition, responses were only slowed for HIGH complexity scenes, suggesting that the HIGH condition was experienced as most difficult.</p>
</sec>
<sec id="sec005">
<title>fMRI results: Whole brain analysis</title>
<p>Whole-brain comparisons of target-present (animal) vs. target-absent (non-animal) scenes revealed significant clusters in lateral and ventral high-level visual cortex (<xref ref-type="fig" rid="pcbi.1006690.g003">Fig 3</xref>). For the animal &gt; non-animal contrast (<xref ref-type="fig" rid="pcbi.1006690.g003">Fig 3<italic>A</italic></xref>), bilateral clusters overlying lateral occipital cortex were found in all conditions. Critically, in the HIGH condition, additional differential activity was found in low-level visual areas. Indeed, contrasting these statistical maps between conditions revealed a large cluster in several early visual areas (<xref ref-type="fig" rid="pcbi.1006690.g003">Fig 3<italic>B</italic></xref>) and smaller clusters in inferior parietal regions. For the non-animal &gt; animal contrast (<xref ref-type="fig" rid="pcbi.1006690.g003">Fig 3<italic>C</italic></xref>), bilateral clusters in parahippocampal cortex were found in the MEDIUM condition, while in the LOW and HIGH condition, only right-lateralized clusters survived whole-brain cluster-correction. Contrasting the difference between non-animal &gt; animal scenes between conditions resulted in no significant clusters. Cluster coordinates for all contrasts are reported in <xref ref-type="table" rid="pcbi.1006690.t001">Table 1</xref>.</p>
<fig id="pcbi.1006690.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006690.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Whole brain fMRI results.</title>
<p><bold>A)</bold> Statistical parametric maps for the animal (A) &gt; non-animal (NA) contrast for each condition. From left to right, MNI coordinates for each transversal slice are z = [10, 2, –6]. <bold>B)</bold> Statistical parametric maps for the non-animal &gt; animal contrast for each condition. From left to right, MNI coordinates are z = [–2, –8, –14]. <bold>C)</bold> Differences in the differential animal vs. non-animal activity between conditions. MNI-coordinates: [x = 6, y = -88, z = 6]. Maps were cluster-corrected and thresholded at z = 2.3. Color scales range from z = 2.5 to 5 (A and B) and z = 2.3 to 3.5 (C).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006690.g003" xlink:type="simple"/>
</fig>
<table-wrap id="pcbi.1006690.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006690.t001</object-id>
<label>Table 1</label> <caption><title>Whole brain fMRI analysis cluster coordinates for the significant contrasts.</title> <p>COG = center of gravity, p = p-value, lat. = lateral, occ. = occipital, temp. = temporal, inf. = inferior, post. = posterior, L = left, R = right. Coordinates are reported in MNI space. Areas included in the clusters were determined by determining overlap of local maxima within clusters with the probability maps of the Juelich Histological Atlas and the Harvard-Oxford Cortical Structural Atlas implemented in FSL. Note that for the contrasts HIGH (animal—non-animal), MED (non-animal—animal) and [HIGH (animal-non-animal)—MED (animal- non-animal)], the clusters are bilateral, forming one cluster across hemispheres (see also <xref ref-type="fig" rid="pcbi.1006690.g003">Fig 3</xref>).</p></caption>
<alternatives>
<graphic id="pcbi.1006690.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006690.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" style="background-color:#A0A0A0">contrast</th>
<th align="left" style="background-color:#A0A0A0">size (mm<sup>3</sup>)</th>
<th align="left" style="background-color:#A0A0A0">log10 (p)</th>
<th align="left" style="background-color:#A0A0A0">x</th>
<th align="left" style="background-color:#A0A0A0">y</th>
<th align="left" style="background-color:#A0A0A0">z</th>
<th align="left" style="background-color:#A0A0A0">areas included</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="7" style="background-color:#E0E0E0">animal &gt; non-animal</td>
</tr>
<tr>
<td align="left">LOW</td>
<td align="left">43656</td>
<td align="char" char=".">15.6</td>
<td align="left">45</td>
<td align="left">-66</td>
<td align="left">-9</td>
<td align="left">V4, V5, lat. occ. cortex. inf. division, inf. temp. gyrus, temp. occ. fusiform. gyrus (R)</td>
</tr>
<tr>
<td align="left"/>
<td align="left">43472</td>
<td align="char" char=".">15.6</td>
<td align="left">-41</td>
<td align="left">-71</td>
<td align="left">-8</td>
<td align="left">V4, V5, lat. occ. cortex. inf. division, inf. temp. gyrus, temp. occ. fusiform. gyrus (L)</td>
</tr>
<tr>
<td align="left">MED</td>
<td align="left">39296</td>
<td align="char" char=".">15.7</td>
<td align="left">45</td>
<td align="left">-70</td>
<td align="left">-6</td>
<td align="left">V4, V3v, lat. occ. cortex. inf. division, inf. temp. gyrus, temp. occ. fusiform. gyrus, occ. fusiform gyrus (R)</td>
</tr>
<tr>
<td align="left"/>
<td align="left">39976</td>
<td align="char" char=".">15.9</td>
<td align="left">-42</td>
<td align="left">-74</td>
<td align="left">-6</td>
<td align="left">V4, V5, lat. occ. cortex. inf. division, inf. temp. gyrus, temp. occ. fusiform. gyrus. (L)</td>
</tr>
<tr>
<td align="left">HIGH</td>
<td align="left">142168</td>
<td align="char" char=".">36.1</td>
<td align="left">3</td>
<td align="left">-74</td>
<td align="left">-3</td>
<td align="left">V1, V2, V3, V4, V5, lat. occ. cortex. inf. division, inf. temp. gyrus, temp. occ. fusiform. gyrus. (L+R).</td>
</tr>
<tr>
<td align="left" colspan="7" style="background-color:#E0E0E0">non-animal &gt; animal</td>
</tr>
<tr>
<td align="left">LOW</td>
<td align="left">4585</td>
<td align="char" char=".">1.96</td>
<td align="left">27</td>
<td align="left">-45</td>
<td align="left">-9</td>
<td align="left">parahippocampal gyrus post. div., lingual gyrus, temp. occ fusiform cortex (R)</td>
</tr>
<tr>
<td align="left">MED</td>
<td align="left">29872</td>
<td align="char" char=".">5.78</td>
<td align="left">4</td>
<td align="left">49</td>
<td align="left">2</td>
<td align="left">parahippocampal gyrus post. div., lingual gyrus, temp. occ. fusiform cortex, supracalcarine cortex, precuneus (L+R)</td>
</tr>
<tr>
<td align="left">HIGH</td>
<td align="left">4392</td>
<td align="char" char=".">1.81</td>
<td align="left">26</td>
<td align="left">-47</td>
<td align="left">-1</td>
<td align="left">parahippocampal gyrus post. div. lingual gyrus, temp. occ. fusiform cortex. supracalcarine cortex, precuneus (R)</td>
</tr>
<tr>
<td align="left" colspan="7" style="background-color:#E0E0E0">animal &gt; non-animal, between conditions</td>
</tr>
<tr>
<td align="left">HIGH &gt; MED</td>
<td align="left">14536</td>
<td align="char" char=".">6.32</td>
<td align="left">7</td>
<td align="left">-85</td>
<td align="left">-2</td>
<td align="left">V1, V2, V3V, V4, occ. fusiform gyrus, lingual gyrus (R).</td>
</tr>
<tr>
<td align="left"/>
<td align="left">7992</td>
<td align="char" char=".">3.61</td>
<td align="left">-15</td>
<td align="left">-95</td>
<td align="left">19</td>
<td align="left">V1, V2, V3V, inf. parietal lobule, occ. fusiform gyrus, lingual gyrus (R)</td>
</tr>
<tr>
<td align="left">HIGH &gt; LOW</td>
<td align="left">21016</td>
<td align="char" char=".">8.94</td>
<td align="left">2</td>
<td align="left">-90</td>
<td align="left">4</td>
<td align="left">V1, V2, V3V, inf. parietal lobule, lingual gyrus, (L+R)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec006">
<title>fMRI results: ROI analysis</title>
<p>Following the whole brain analysis, the difference in BOLD activity for animal vs. non-animal scenes in each condition was computed in four <italic>a priori</italic>, independently defined regions of interest and compared across conditions using repeated-measures ANOVAs. In line with the whole brain results, scene complexity was found to modulate activity in early visual cortex (V1: (F(2,44) = 4.9, p = 0.01, η<sup>2</sup> = 0.18; <xref ref-type="fig" rid="pcbi.1006690.g003">Fig 3<italic>A</italic></xref>). Planned post-hoc comparisons indicated that this effect was driven by increased activity for animal vs. non-animal scenes in the HIGH compared with the MEDIUM condition (t(22) = 2.6, p(Sidák-corrected) = 0.05) and a trend towards increased differential activity in the HIGH versus the LOW condition (t(22) = 2.3, p(Sidák-corrected) = 0.09), with no difference between LOW and MEDIUM conditions (t(22) = 1.2, p(Sidák-corrected) = 0.54).</p>
<p>In addition, one-sample t-tests conducted for each of the three conditions indicated that the differential activity in V1 significantly deviated from zero only in the HIGH condition (HIGH: t(22) = 2.8, p(Sidák-corrected) = 0.03; LOW: t(22) = 0.19, p(Sidák-corrected) = 0.99; MEDIUM: t(22) = -1.28, p(Sidák-corrected) = 0.52).</p>
<p>We also observed a main effect of scene complexity in place-selective PPA (F(2,44) = 3.6, p = 0.04, η<sup>2</sup> = 0.14 (<xref ref-type="fig" rid="pcbi.1006690.g004">Fig 4<italic>B</italic></xref>), which exhibited stronger responses for non-animal than animal scenes. Planned post-hoc comparisons indicated a trend towards less differential activity for the HIGH compared with the MEDIUM (t(22) = 2.4, p(Sidák-corrected) = 0.07), but not the LOW condition (t(22) = 1.9, p(Sidák-corrected) = 0.19), and no difference between the LOW and HIGH conditions (t t(22) = 0.7, p(Sidák-corrected) = 0.89). In contrast, object-selective LOC and face-selective FFA both responded more positively to scenes with animals than to scenes without animals (<xref ref-type="fig" rid="pcbi.1006690.g004">Fig 4<italic>C</italic> and 4<italic>D</italic></xref>), but these responses did not differ across conditions (FFA: F(2,44) = 1.7, p = 0.19, η<sup>2</sup> = 0.07; LOC: F(2,42) = 1.9, p = 0.14, η<sup>2</sup> = 0.08).</p>
<fig id="pcbi.1006690.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006690.g004</object-id>
<label>Fig 4</label>
<caption>
<title>fMRI ROI analysis results.</title>
<p>Differential activity for animal vs. non-animal scenes for each condition in <bold>A)</bold> V1, <bold>B)</bold> PPA, <bold>C)</bold> LOC and <bold>D)</bold> FFA. Horizontal black lines indicate the statistical outcome of repeated-measures ANOVAs; gray lines indicate the results of pairwise tests corrected for multiple comparisons using a Sidák correction. * = p &lt; 0.05, # = p &lt; 0.10. Error bars represent S.E.M.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006690.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Summary Experiment 1</title>
<p>In sum, behavioral detection of target objects (animals) in natural scenes was slower and less accurate for scenes of increased complexity. The presence of a target object was associated with an enhanced fMRI response in known object- and face-selective regions, but these responses were not significantly modulated by scene complexity. In contrast, scene complexity modulated activity levels in V1 and scene-selective PPA. In PPA, differential activity related to target-presence was least pronounced for the high complexity scenes compared to the medium complexity scenes, while V1 activity was only modulated by target presence for high complexity scenes. Taken together, these results indicate that scenes with high complexity induced the least difference in PPA while giving rise to additional differential activity in early visual areas.</p>
<p>These fMRI results suggest that information encoded in early visual cortex may be selectively recruited during target detection for the most difficult, high complexity scenes. Since the scenes were carefully matched in CE and SC <italic>within</italic> complexity levels, it is unlikely that this activity was driven by differences in low-level properties between target and non-target scenes evoked by feed-forward processing. Therefore, we hypothesized that the differential V1 early visual activity results from increased recurrent activity elicited by a need for additional visual processing to detect the target object in high complexity scenes.</p>
<p>However, two limitations preclude a strong confirmation of this hypothesis. First, due to the poor temporal resolution of the fMRI signal, we cannot exclude the possibility that early visual activity differences were present as a result of other differences between the target and non-target scenes than those captured by CE and SC. Second, the categorization task was embedded in a stop-signal paradigm (see <xref ref-type="sec" rid="sec019">Materials and methods</xref>). Although all results reported above were based on GO trials only (i.e. omitting all trials in which a stop signal was presented), the observed effects could be specific to, or amplified by, an expectation of a stop instruction, and might not generalize to a task in which participants are never asked to withhold their response.</p>
<p>To overcome these two limitations, we conducted a second experiment, in which we recorded EEG while participants performed the same animal detection task on the same set of scenes without any stop-signal manipulation. First, to test our hypothesis that the activity differences in early visual areas were feedback-related, we computed event-related potentials (ERPs) for target and non-target scenes separately in each condition, to infer when in time neural responses started to differ on electrodes overlying early visual cortex. Since scenes were matched in image statistics, we predicted that there would be no differences between animal and non-animal scenes in feedforward responses, i.e. before ~150 ms after stimulus onset, after which ERP responses are thought to start reflecting recurrent activity [<xref ref-type="bibr" rid="pcbi.1006690.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref028">28</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref046">46</xref>]. Second, because behavioral performance for high complexity scenes in Experiment 1 was characterized by slowed RTs with no substantial improvements in detection accuracy, the behavioral task in the EEG experiment was divided into blocks with a task instruction to emphasize either ‘speed’ or ‘accuracy’ (<xref ref-type="fig" rid="pcbi.1006690.g001">Fig 1<italic>E</italic></xref>). We hypothesized that if target detection in high complexity scenes was indeed associated with increased feedback, performance in the HIGH condition should be most affected for speeded trials, in which extensive visual processing is limited by time constraints.</p>
</sec>
</sec>
<sec id="sec008">
<title>Experiment 2 (EEG)</title>
<sec id="sec009">
<title>Behavior and HDDM parameters</title>
<p><xref ref-type="fig" rid="pcbi.1006690.g005">Fig 5</xref> shows an overview of the behavioral results in Experiment 2. Analysis of the reaction times showed a significant main effect for instruction (speed or accurate; F(1,25) = 87.6, p &lt; 0.001, η<sup>2par</sup> = 0.78), and scene complexity (LOW, MEDIUM, or HIGH; F(2,50) = 29.1, p &lt; 0.001, η<sup>2par</sup> = 0.54). As expected, participants responded faster under speed instructions, and response times were again longest for the high complexity scenes. Planned post-hoc comparisons further showed that responses were significantly slower for high compared with medium complexity scenes under both instructions (speed: t(25) = 5.0, p &lt; 0.001; accurate: t(25) = 5.1, p = &lt; 0.001; all Sidák-corrected; <xref ref-type="fig" rid="pcbi.1006690.g005">Fig 5<italic>A</italic></xref>). Compared to low complexity scenes, responses in the high complexity condition were significantly slower for accurate task instructions (t(25) = 4.9, p &gt; 0.001), while showing a similar trend for the speed instruction (t(25) = 2.7, p = 0.065; all Sidák-corrected). Response times did not differ between low and medium conditions (speed: t(25) = 2.4, p = 0.13; accurate: t(25) = 0.72, p = 0.98; all Sidák-corrected).</p>
<fig id="pcbi.1006690.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006690.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Behavioral and HDDM modeling results of the EEG experiment (Experiment 2).</title>
<p><bold>A)</bold> Average reaction times (RT) per condition and task instruction. <bold>B)</bold> Accuracy (percentage correct) per condition and task instruction. <bold>C)</bold> Estimates of drift rates per condition, indicating slower evidence accumulation for high complexity scenes. <bold>D)</bold> Estimates of the amount of evidence required (boundary) per condition and task instruction. Gray lines indicate the results of pairwise post-hoc tests between conditions corrected for multiple comparisons using a Sidák correction. Error bars represent S.E.M. * = p &lt; 0.05.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006690.g005" xlink:type="simple"/>
</fig>
<p>Accuracy scores showed a significant interaction between scene complexity and task-instruction (F(2,50) = 5.6, p = 0.002, η<sup>2par</sup> = 0.18). That is, detection accuracy was selectively decreased for highly complex scenes when participants were motivated to respond as fast as possible (HIGH vs. LOW, t(25) = 3.2, p = 0.024; HIGH vs. MEDIUM, t(25) = 4.4, p = 0.001; LOW vs. MEDIUM, t(25) = 1.98, p = 0.38, all Sidák-corrected). This effect was not found for trials in which participants were asked to be as accurate as possible (HIGH vs. LOW; t(25) = 1.5, p = 0.60; HIGH vs. MEDIUM, t(25) = 1.8, p = 0.42; LOW vs. MEDIUM: t(25) = 0.1, p = 0.99, all Sidák-corrected; <xref ref-type="fig" rid="pcbi.1006690.g005">Fig 5<italic>B</italic></xref>).</p>
<p>In summary, these results indicate increased reaction times for high complexity scenes both when participants are pressed for time (speed trials), or when asked to make the most accurate response (accurate trials). However, detection accuracy is decreased only when participants are pressed for time (<xref ref-type="fig" rid="pcbi.1006690.g005">Fig 5B</xref>). This suggests that for high complexity scenes, visual information processing might be too slow to benefit detection, especially when participants are motivated to respond quickly and thus should have lower evidence requirements in comparison to accurate instruction trials.</p>
<p>To formally support this assumption, we modeled the RT and detection accuracy data with a hierarchical implementation of the drift diffusion model (HDDM), which details how latent parameters describing the speed of sensory information accumulation (drift rate) and evidence requirements (boundary) each contribute to reaction times and response accuracy [<xref ref-type="bibr" rid="pcbi.1006690.ref044">44</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref045">45</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref047">47</xref>]. We found that drift rate was signficantly modulated across the three scene complexity conditions (F(2,50) = 12.5, p&lt;0.001, η<sup>2par</sup> = 0.33), with the slowest rate of information accumulation for high complexity scenes (HIGH vs. LOW; t(25) = 2.97, p = 0.02; HIGH vs. MEDIUM, t(25) = 5.04, p &lt; 0.001; LOW vs. MEDIUM: t(25) = 2.05, p = 0.15, all Sidák-corrected; <xref ref-type="fig" rid="pcbi.1006690.g005">Fig 5<italic>C</italic></xref>). In contrast, the boundary only varied as a function of task instructions (F(1,25) = 93.5, p&lt;0.001, η<sup>2par</sup> = 0.79), and was not modulated by scene complexity (F(2,50) = 1.3, p = 0.22, η<sup>2par</sup> = 0.05), and there was no interaction (F(2,50) = 2.5, p = 0.10, η<sup>2par</sup> = 0.09; <xref ref-type="fig" rid="pcbi.1006690.g005">Fig 5<italic>D</italic></xref>). This suggests that RTs and error rates are increased in the HIGH condition because information accumulation is slowed, but not because the requirements to reach a decision are changed.</p>
<p>Together, the behavioral results of Experiment 2 again indicated increased reaction times for high complexity scenes relative to less complex scenes. In addition, they revealed a slower rate of information processing (drift rate) for high complexity scenes, resulting in a selective decrease in performance when the decision is speeded.</p>
</sec>
<sec id="sec010">
<title>Grand-average ERP results</title>
<p>To investigate the time-course of object detection in visual cortex, evoked responses to the target and non-target scenes were pooled across a set of occipital and peri-occipital electrodes (see <xref ref-type="sec" rid="sec019">Materials and methods</xref>). The results in <xref ref-type="fig" rid="pcbi.1006690.g006">Fig 6</xref> show that target and non-target ERPs before 150 ms were highly overlapping in all three conditions (although the ERPs diverged for a few time-points before 150 ms in the LOW condition). Critically, target and non-target ERPs in the HIGH condition only started to diverge from ~220 ms, suggesting a contribution of recurrent processing to target detection in high complexity scenes. These effects were consistent across the speed (<xref ref-type="fig" rid="pcbi.1006690.g006">Fig 6A</xref>) and accuracy trials (<xref ref-type="fig" rid="pcbi.1006690.g006">Fig 6B</xref>).</p>
<fig id="pcbi.1006690.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006690.g006</object-id>
<label>Fig 6</label>
<caption>
<title>ERP results from the EEG experiment (Experiment 2).</title>
<p>Average ERP amplitude for animal (solid lines) and non-animal scenes (dashed lines) per condition for an occipital-peri-occipital cluster of EEG channels (Oz, POz, O1, O2, PO3, PO4, PO7, PO8) for the <bold>A)</bold> speed and <bold>B)</bold> accuracy trials. Shaded regions indicate SEM across participants. Significant differences between animal and non-animal ERPs (FDR-corrected) are indicated with thick black lines (also in C/D). <bold>C-D</bold> Animal vs. non-animal difference wave for each condition overlaid and statistically compared (FDR-corrected) for the speed <bold>A)</bold> and accuracy <bold>B)</bold> trials. Thick lines at the bottom of the graphs indicate significant difference wave deflections from zero, and shadings indicate S.E.M. Symbol markers indicate significant differences in difference wave amplitude between conditions, blue asterisk: HIGH vs. LOW; blue plus: HIGH vs. MEDIUM; green plus: MEDIUM vs. LOW.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006690.g006" xlink:type="simple"/>
</fig>
<p>Statistical comparisons (FDR-corrected across all time-points, task-instructions and conditions) of the animal vs non-animal difference waves indicated that for speeded trials, the difference wave in the HIGH condition did not diverge from zero before prior to 220 ms (<xref ref-type="fig" rid="pcbi.1006690.g006">Fig 6<italic>C</italic></xref>, blue solid line); in the accuracy trials, there was a brief interval of significant deflection before 200 ms (<xref ref-type="fig" rid="pcbi.1006690.g006">Fig 6<italic>D</italic></xref>), but this reflected a reversed difference wave compared to the other two conditions. Beyond 220 ms, however, this pattern reversed: direct comparison of the difference wave across conditions indicated that the difference wave for the HIGH condition after 220 ms was significantly enhanced compared to the LOW (blue asterisks; top) and MEDIUM (blue plusses; top) conditions.</p>
<p>To exclude the possibility that the differential effects in HIGH simply reflected the absence of any object information in non-target trials, we repeated these analyses while restricting the non-animal trials to only those images that contained large, non-animal distractor objects (vehicles, humans and man-made objects). The results of this analysis were qualitatively similar (see <xref ref-type="supplementary-material" rid="pcbi.1006690.s001">S1 Fig</xref>), suggesting that these differential responses were not driven by the absence of distractor objects for non-animal trials.</p>
<p>The deviating shape of the difference wave in the HIGH condition relative to the other two conditions has two important implications. First, the lack of a reliable early (&lt; 150 ms) difference wave in the HIGH condition indicates that it is unlikely that the differential fMRI activity in early visual areas found in Experiment 1 was driven by uncontrolled low-level image properties. Such properties are expected to elicit a difference in feed-forward responses, and thus an early difference in ERP responses. Instead, the strongly enhanced ERP difference beyond 220 ms in the HIGH condition suggests this to be the electrophysiological correlate of the differences in the early visual fMRI response.</p>
<p>Second, the late onset of the target vs. non-target distinction for HIGH provides a potential neural explanation for the decreased behavioral performance for these scenes. As shown in <xref ref-type="fig" rid="pcbi.1006690.g005">Fig 5</xref>, high scene complexity was associated with slower reaction times and lower drift rates, as well as decreased accuracy during speeded trials, because participants commit to a choice with less evidence on those trials (lower boundary; <xref ref-type="fig" rid="pcbi.1006690.g005">Fig 5D</xref>). The ERP results suggest that in the HIGH condition, participants relied on differential activity emerging at recurrent stages of visual processing (beyond 220 ms), which may be less accessible under time pressure (speeded trials). Thus, these observations suggest the rate of evidence accumulation for highly complex scenes was slowed due to enhanced recurrent processing. We examined this relation more closely in the next section, by comparing ERPs and behavioral performance at the single-trial level.</p>
</sec>
<sec id="sec011">
<title>Relating ERPs to behavior</title>
<p>To assess whether EEG activity beyond 200 ms was indeed indicative of the accumulation of evidence for the target/non-target decision, we investigated the relation between ERP responses and drift rate using trial-by-trial analysis. We again fitted a HDDM model [<xref ref-type="bibr" rid="pcbi.1006690.ref045">45</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref048">48</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref049">49</xref>] to the RT distributions for target and non-target decisions in LOW, MED and HIGH, now including the single-trial ERP amplitude as a predictor (HDDM regression; see <xref ref-type="sec" rid="sec019">Materials and methods</xref>). Instead of estimating one drift rate per subject across trials, this model assumes the drift rate to vary on each trial according to a linear relation with their measured EEG activity. Since our behavioral analysis did not indicate an effect of task instruction on drift rates, we estimated this model across all trials (i.e., both speed and accurate) simultaneously. The results show that drift rates were indeed modulated significantly by trial-by-trial differences in late ERP amplitude (220–325 ms; <xref ref-type="fig" rid="pcbi.1006690.g007">Fig 7</xref>). The regression weight is negative, indicating that higher ERP amplitude in that interval reflects lower drift rates, consistent with our interpretation that slower evidence accumulation is associated with increased feedback processing. The regression weight was significantly different from zero in all conditions (all p = 0.0); however, it was significantly larger for HIGH relative to both LOW (p = 0.0) and MED (p = 0.0008) trials, whose regression weights did not differ from one another (p = 0.11). These results thus provide further support that late ERP activity (presumably reflecting recurrent interactions) reflects a relatively slowed accumulation of evidence for the target/non-target decision for scenes with high complexity.</p>
<fig id="pcbi.1006690.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006690.g007</object-id>
<label>Fig 7</label>
<caption>
<title>HDDM regression results.</title>
<p>Posterior probabilities of the regression weight for drift rate (v) on the single-trial ERP amplitude averaged between 220–325 ms after stimulus onset for LOW, MED and HIGH complexity.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006690.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec012">
<title>Summary Experiment 2</title>
<p>The differential ERP response indicating target presence in the HIGH, but not the LOW and MEDIUM conditions emerged after 200 ms, suggesting that it does not reflect a feed-forward signal, but recurrent processing. Moreover, this feedback response reflected a slower rate of evidence accumulation at the single-trial level. While feedback activity was present for all trials (speeded or accurate), behavioral performance was only impaired during speeded trials. Because evidence requirements were lower during speed trials (<xref ref-type="fig" rid="pcbi.1006690.g005">Fig 5D</xref>), we reason that the feedback activity during those trials, while present, did not benefit task performance, because participants lacked sufficient time to incorporate the accumulating evidence into a behavioral response.</p>
</sec>
</sec>
</sec>
<sec id="sec013" sec-type="conclusions">
<title>Discussion</title>
<p>We investigated the influence of scene complexity on the detection of target objects in natural scenes using fMRI, EEG and behavior. Behaviorally, we observed prolonged detection times when participants viewed high compared with low or medium complexity scenes. Using fMRI, we found these slower responses to be accompanied by a selective increase in activity for target objects in early visual regions including V1, while target activity in higher-level object-selective cortex was not affected by scene complexity. EEG recordings showed that the differential response to targets and non-targets in early visual cortex arose after ~220 ms, suggesting it is likely driven by feedback/recurrent interactions, rather than feed-forward activity differences. While increased feedback for target objects in high complexity scenes was observed for both fast and accurate task instructions, behavioral performance only profited from this activity when the instructions emphasized accuracy, and so encouraged participants to wait for evidence accumulated via feedback.</p>
<p>We interpret these findings as showing a selective need for increased detailed visual analysis of high complexity scenes. For simple scenes, the initial, coarse representation provided by the feed-forward sweep is enough to obtain information about the presence of a target object. For complex scenes, however, the feed-forward sweep is not sufficiently informative: it merely signals that there is a lot of ‘stuff’ in the scene, and detecting the target object requires additional processing.</p>
<sec id="sec014">
<title>Global-to-local processing and attention</title>
<p>This interpretation is consistent with the global-to-local processing frameworks which suggest that detailed scene analysis takes place via reentrant processing [<xref ref-type="bibr" rid="pcbi.1006690.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref050">50</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref052">52</xref>]. These frameworks propose that the feed-forward sweep provides visual cortex with a <italic>base representation</italic>. If the visual task can be solved based on this representation alone, no further operations are necessary and action selection can be initiated. If it is not sufficiently informative, <italic>elemental operations</italic> are applied such as contour grouping and texture segmentation, which integrate individual line segments and other features encoded in low-level areas via incremental grouping [<xref ref-type="bibr" rid="pcbi.1006690.ref021">21</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref022">22</xref>]. We believe that the differential neural response observed in our fMRI and EEG data for targets relative to non-target scenes reflects the implementation of these elemental operations. A psychological correlate of this feedback-driven process could be termed ‘attentive processing’ while the feed-forward stage could be considered pre-attentive [<xref ref-type="bibr" rid="pcbi.1006690.ref053">53</xref>]; see [<xref ref-type="bibr" rid="pcbi.1006690.ref021">21</xref>] for a detailed discussion. Our observation of an increased fMRI response to targets is clearly consistent with numerous observations that attentional modulations are reflected in response enhancements in visual cortex [<xref ref-type="bibr" rid="pcbi.1006690.ref054">54</xref>]. Importantly, however, our results suggest that this response enhancement is <italic>selectively applied</italic> on a trial-by-trial basis to scenes which have high complexity, as determined by scene statistics.</p>
</sec>
<sec id="sec015">
<title>The impact of recurrent processing on behavior</title>
<p>Our behavioral observations support the presence of selectively increased feedback for complex scenes in two ways. First, in Experiment 1 we observed that scene complexity influenced response selection, but not response suppression [<xref ref-type="bibr" rid="pcbi.1006690.ref055">55</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref058">58</xref>]. Specifically, we observed that the decision time was prolonged for complex scenes, suggesting a slower rate of information accumulation. In the Experiment 2 we examined this hypothesis through the manipulation of time restrictions. Behaviorally, we showed that while RTs were always increased for highly complex scenes, accuracy only declined when the instructions emphasized a speeded response. The drift diffusion model [<xref ref-type="bibr" rid="pcbi.1006690.ref043">43</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref044">44</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref059">59</xref>] was then used to show that drift rate (the latent variable capturing the rate of information accumulation) was indeed slower for highly complex scenes [<xref ref-type="bibr" rid="pcbi.1006690.ref060">60</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref062">62</xref>]. In addition, we observed that drift rate was related to single-trial feedback-driven ERP responses for high, but not low or medium complexity scenes. This reliance on feedback provides an explanation for the difference in behavioral performance when participants emphasized speed above accuracy (and thus were not able to incorporate the information provided by the feedback response). Moreover, this ERP-drift rate relation is consistent with reports of a ‘discriminating component’ around 300 ms reflecting the amount of sensory evidence for perceptual decisions on objects in phase noise [<xref ref-type="bibr" rid="pcbi.1006690.ref061">61</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref063">63</xref>]. Taken together, these observations underline the importance of feedback activity for processing complex scenes to optimize behavioral performance.</p>
</sec>
<sec id="sec016">
<title>A role for scene statistics in signaling the need for recurrent processing</title>
<p>We propose that the complexity of the visual input can be estimated using image statistics derived from contrast distributions, in particular contrast energy and spatial coherence. These statistics are potentially suitable computational substrates for such a representation because they can be computed directly from local contrast responses in e.g. LGN [<xref ref-type="bibr" rid="pcbi.1006690.ref035">35</xref>]. These parameters strongly affect the amplitude of evoked EEG activity early in time in visual processing [<xref ref-type="bibr" rid="pcbi.1006690.ref035">35</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref037">37</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref064">64</xref>] suggesting they are indeed available at early stages of visual computation and could therefore serve as ‘markers’ to determine whether further visual operations are necessary. This idea is reminiscent of previous proposals suggesting that scenes are summarized as coarse ‘blobs’ that precede detailed analysis at smaller spatial scales depending on their diagnostic value [<xref ref-type="bibr" rid="pcbi.1006690.ref065">65</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref066">66</xref>], or via low spatial frequencies that are used to direct top-down facilitation [<xref ref-type="bibr" rid="pcbi.1006690.ref067">67</xref>]. It is also consistent with results showing that feedback is necessary to model categorization of degraded scenes [<xref ref-type="bibr" rid="pcbi.1006690.ref026">26</xref>] and with results reported by [<xref ref-type="bibr" rid="pcbi.1006690.ref030">30</xref>], who found that masking effects were stronger for scenes that were ‘less easily segmented’.</p>
<p>Here, rather than filtering, degrading or otherwise manipulating the scenes, we used scene statistics to sample variation in scene complexity, providing a quantitative computational approach to estimate this property. We sampled scenes with low, intermediate or high CE and SC values, assuming a linear relation between these values and scene complexity. We note, however, that several of our results, in particular the fMRI responses in higher-order regions and behavioral accuracy in Experiment 1, appeared to show a U-shaped, rather than linear modulation across conditions, suggesting that CE and SC are not a straightforward parametric index of scene complexity. In particular, while the high condition consistently deviated from the medium condition in terms of both behavioral (higher RT, lower accuracy) and neural (increased differential V1 fMRI and late ERP responses) effects, the low condition sometimes also showed a decrease in performance, but without a clear neural correlate. Given the clear neural effects for the high condition, the main focus of this paper is on that condition. However, a separate investigation of the U-shaped effects based on an additional set of behavioral studies [<xref ref-type="bibr" rid="pcbi.1006690.ref068">68</xref>] shows that behavioral performance is indeed often optimal for scenes with intermediate, rather than low, scene complexity. One potential explanation for this benefit is that feed-forward processing for intermediate CE/SC scenes is faster or more efficient due to, for example, increased familiarity with such scenes. However, intermediate scenes could also contain more or better contextual cues regarding the presence of target objects, resulting in increased interaction between scene and object processing pathways [<xref ref-type="bibr" rid="pcbi.1006690.ref069">69</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref071">71</xref>]. In sum, our results do not provide a clear explanation for this pattern, and future research is needed to determine the neural underpinnings of the improved performance for intermediate relative to low complexity images as defined by the CE/SC parameterization.</p>
</sec>
<sec id="sec017">
<title>How does the visual system determine whether recurrent processing is needed?</title>
<p>A question that remains unresolved in this study is which, if any, brain areas might be involved in computing scene complexity, i.e. which region ‘reads out’ the scene statistics from the population response in early visual regions. Consistent with previous studies [<xref ref-type="bibr" rid="pcbi.1006690.ref072">72</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref073">73</xref>], we found a strong distinction between animal- and non-animal images in object-selective LOC and face-selective FFA. However, this differential activity was not strongly modulated by whether the object was embedded in a complex scene or a simple scene. We did observe, however, an effect of scene complexity in PPA. While the scene-selectivity of PPA is commonly attributed to coding of 3D spatial layout [<xref ref-type="bibr" rid="pcbi.1006690.ref074">74</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref076">76</xref>] it is also sensitive to object information [<xref ref-type="bibr" rid="pcbi.1006690.ref077">77</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref079">79</xref>], as well as low-level features such as spatial frequency, contrast, rectilinearity and texture [<xref ref-type="bibr" rid="pcbi.1006690.ref079">79</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref084">84</xref>], suggesting that PPA may be a suitable region to demonstrate an influence of the broader scene context on object-related activity [<xref ref-type="bibr" rid="pcbi.1006690.ref069">69</xref>]. Importantly, PPA is biased towards the visual periphery [<xref ref-type="bibr" rid="pcbi.1006690.ref085">85</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref086">86</xref>], containing relatively large receptive fields [<xref ref-type="bibr" rid="pcbi.1006690.ref087">87</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref088">88</xref>] making it suited for computation of larger-scale summary statistics of the input [<xref ref-type="bibr" rid="pcbi.1006690.ref089">89</xref>]. Consistently, a recent study found that PPA was sensitive to difference in scene complexity as defined by various image-computable computational measures such as image compression and self-similarity [<xref ref-type="bibr" rid="pcbi.1006690.ref090">90</xref>].</p>
<p>Based on this prior literature, we speculate that the enhanced recurrent processing in early visual regions is initiated based on a feed-forward, summary statistics based computation of scene complexity in PPA. However, future research using for example time-resolved measurements of PPA activity will be necessary to confirm or deny the presence of such a representation in PPA during object detection in natural scenes.</p>
</sec>
<sec id="sec018">
<title>Influence of task on target detection in complex scenes</title>
<p>Perceiving real-world scenes involves more than detection of objects: for example, we can recognize a scene as a specific place, or determine its navigability [<xref ref-type="bibr" rid="pcbi.1006690.ref091">91</xref>]. It is unclear whether feedback is ‘intrinsically’ enhanced upon perceiving a scene of high complexity regardless of observer goal, or whether it is selectively enhanced when a participant is actively searching for a target object in a complex scene. One way to test this is to compare the current results to a situation in which participants see scenes of varying levels of complexity while performing a task without any object detection requirement (e.g. an orthogonal visual task at fixation, or detecting a concurrent auditory signal). Such top-down task manipulations have been shown to affect ERP responses to natural scenes in recurrent processing time windows [<xref ref-type="bibr" rid="pcbi.1006690.ref064">64</xref>], and feedback is thought to be involved in the application of attentional templates [<xref ref-type="bibr" rid="pcbi.1006690.ref092">92</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref093">93</xref>], which may differ between different tasks [<xref ref-type="bibr" rid="pcbi.1006690.ref094">94</xref>] or the level of detail necessary to solve the task [<xref ref-type="bibr" rid="pcbi.1006690.ref066">66</xref>] and therefore in the amount of feedback required. While we varied scene complexity on a trial-by-trial basis, making it difficult for participants to use a top-down strategy to ‘predict’ how much attention they would need to direct to solve that trial, they still needed to apply a search target in order to solve the task. This search target was the same throughout the entire experiment (animal), but our whole-brain fMRI analysis (<xref ref-type="fig" rid="pcbi.1006690.g003">Fig 3</xref>) did indicate some target-related activity differences for high complexity scenes in posterior parietal regions, which have been associated both with representing attentional templates as well as outcomes of attentional selection [<xref ref-type="bibr" rid="pcbi.1006690.ref092">92</xref>]. A deeper understanding of the interaction between scene complexity and top-down task requirements and their respective representation in cortical regions requires future experimental study. So far, however, our results suggest that although object recognition based on feed-forward information [<xref ref-type="bibr" rid="pcbi.1006690.ref005">5</xref>] may be possible for simple stimuli, detection of objects in complex real-world scenes additionally involves feedback processing.</p>
</sec>
</sec>
<sec id="sec019" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec020">
<title>Experiment 1 (fMRI)</title>
<sec id="sec021">
<title>Ethics statement</title>
<p>All participants provided written informed consent and were financially compensated. The experiment was approved by the ethics committee of the University of Amsterdam.</p>
</sec>
<sec id="sec022">
<title>Subjects</title>
<p>Twenty-five participants (7 males, age 19–26 years; mean = 21.6, SD = 1.7, normal or corrected-to-normal vision) took part in the fMRI experiment. One participant had a median reaction time of 2 standard deviations above average as well as 7.5% non-responses and was therefore excluded from further data analysis. Another participant was excluded because of excessive head movement (absolute displacement of twice the voxel size caused by multiple movements across a single experimental run).</p>
</sec>
<sec id="sec023">
<title>Stimuli</title>
<p>Scenes were selected from a larger set of 4800 scenes used in a previous EEG study [<xref ref-type="bibr" rid="pcbi.1006690.ref095">95</xref>]. The larger dataset contained scenes from several online databases, including the INRIA holiday database [<xref ref-type="bibr" rid="pcbi.1006690.ref096">96</xref>], the GRAZ dataset [<xref ref-type="bibr" rid="pcbi.1006690.ref097">97</xref>], ImageNet [<xref ref-type="bibr" rid="pcbi.1006690.ref098">98</xref>], and the McGill Calibrated Color Image Database [<xref ref-type="bibr" rid="pcbi.1006690.ref099">99</xref>]. For each scene, one CE and one SC value was computed by simulating the output of contrast-sensitive receptive fields and integrating these responses across the scene by averaging (CE) and divisive normalization (SC). In natural scenes, CE and SC correlate strongly with parameters of a Weibull function fitted to the distribution of contrast values [<xref ref-type="bibr" rid="pcbi.1006690.ref035">35</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref036">36</xref>], which is informative about the degree of scene fragmentation [<xref ref-type="bibr" rid="pcbi.1006690.ref034">34</xref>]. CE is an approximation of the distribution width (the scale parameter of the function), informing about the overall presence of edges in an image, whereas SC is an approximation of its shape (the degree to which the function describes a power law or a Gaussian distribution), capturing higher-order correlations between edges. As a result, images with low CE/SC values have strong ‘Inherent’ figure-ground segmentation, often containing a large (central) object surrounded by a homogenous background (and therefore containing few edges that are spatially correlated because they belong to the same object), whereas images with high CE/SC values are more complex, and typically textured or cluttered (with many edges distributed in a Gaussian manner that are uncorrelated); while they can contain large objects, they tend to have a background consisting of uncorrelated edge ‘noise’ (see also De Cesarei et al., (2017), footnote 4). The model is described in more detail in [<xref ref-type="bibr" rid="pcbi.1006690.ref037">37</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref064">64</xref>].</p>
<p>Here, we used these image statistics to selectively sample scenes with various levels of complexity. We created three conditions: LOW, MEDIUM and HIGH (<xref ref-type="fig" rid="pcbi.1006690.g001">Fig 1<italic>B</italic></xref>), whereby each condition was defined by its CE/SC values. For the LOW and HIGH condition, we selected the lower and upper 25% percentiles of the distribution of CE/SC values in the full stimulus set, respectively; for the MEDIUM condition, the middle 35% percentile. Each condition consisted of 160 images, half of which contained an animal. Importantly, target and non-target images were matched <italic>within condition</italic> in their CE and SC values such that animal and non-animal images did not differ from each other in their mean (all t(158) &lt; 0.13, all p &gt; 0.89) or median values (Wilcoxon rank sum test all z &lt; 0.16, all p &gt; 0.87). Images were randomly selected from the larger set of scenes solely based on their image statistics and annotations (animal/non-animal). Animal images contained a wide variety of animals including pets such as dogs and cats but also wildlife, reptiles and fish. Non-animal images consisted of urban, landscape and indoor scenes and contained a variety of objects, ranging from vehicles to household items. Several representative exemplars from each condition are provided in <xref ref-type="fig" rid="pcbi.1006690.g001">Fig 1<italic>C</italic></xref>.</p>
</sec>
<sec id="sec024" sec-type="materials|methods">
<title>Experimental design</title>
<p>Participants performed the animal vs. non-animal categorization task in a stop signal paradigm (<xref ref-type="fig" rid="pcbi.1006690.g001">Fig 1<italic>D</italic></xref>). Our motivation for using this paradigm was based on a separate research question inspired by a previous set of studies [<xref ref-type="bibr" rid="pcbi.1006690.ref056">56</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref100">100</xref>], which asked whether the quality of visual input affects decision-making and response inhibition. However, analysis of the behavioral data showed scene complexity to only modulate decision-making on trials without stop-signals [<xref ref-type="bibr" rid="pcbi.1006690.ref055">55</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref058">58</xref>]. Given that the focus of the current study is on detection performance rather than response inhibition, trials with a stop-signal presentation were excluded from analysis. Each participant performed 480 trials in total, divided over 2 separate runs. Each trial lasted 2000 ms and started with a fixation cross of variable duration (500–750 ms jittered with 50 ms intervals), after which a scene (640x480 pixels) was presented for 100 ms. The scene stimuli were back-projected on a 61x36 cm LCD screen that was viewed through a mirror attached to the head coil at ~120 cm viewing distance using Presentation software (Neurobehavioral Systems, Albany, CA, USA). There were two trial types: GO and STOP trials. On GO trials, participants had to indicate whether the stimulus was an animal or non-animal scene before the trial ended (i.e. at maximum within 1250 ms) by pressing one of two buttons. They indicated their response using a hand-held button box with the index or middle finger. If they did not respond in time, a screen displaying the word ‘miss’ appeared for 2000 ms. On STOP trials a beep signal was presented over the headphones indicating that the participant had to withhold their response. At the start of the experiment, the time interval between the stimulus and beep (stop signal delay) was initialized at 250 ms and was adjusted in a staircase procedure based on the stopping performance [<xref ref-type="bibr" rid="pcbi.1006690.ref101">101</xref>]). Trials were presented in two randomized sequences that were counterbalanced across participants. Overall, 25% of the scenes (60 animal, 60 non-animal) were shown in STOP trials. The same set of scenes was used in the STOP trials for all participants, and these scenes were excluded from analysis. Thus, all analyses reported here included only the GO trials. For these trials, animal and non-animal scenes were still matched in their CE and SC values per condition (means: all t(118) &lt; 1.13, all p &gt; 0.26; medians: all z &lt; 1.10, all p &gt; 0.28).</p>
</sec>
<sec id="sec025">
<title>fMRI data acquisition</title>
<p>BOLD-MRI data was acquired in a single scanning session, over the course of two runs on a Philips Achieva XT 3T scanner with a 32-channel head-coil located at the University of Amsterdam, The Netherlands. In each run 255 T2*-weighted GE-EPI recordings (TR = 2000 ms, TE = 27.6 ms, FA = 76.1°, SENSE = 2, FOV = 240 mm<sup>2</sup>, matrix size = 80<sup>2</sup>, 37 slices, slice thickness 3 mm, slice gap = 0.3 mm) were made. Breathing rate and heartbeat (using the pulse-oxidization signal recorded from the tip of one of the participant’s fingers) was measured during fMRI acquisition. In addition, a separate functional localizer scan was recorded (317 T2*weighted echo-planar images; TR = 1500 ms, TE = 27.6, FA = 70°, FOV = 240*79.5*240, matrix size = 96<sup>2</sup>, 29 slices, slice thickness 2.5mm, slice gap = 0.25) in which participants viewed a series of houses, faces, objects as well as phase-scrambled scenes while pressing a button when an image was directly repeated (12.5% likelihood). These separate localizer scans were used to identify the following regions-of-interest (ROIs): the fusiform face area (FFA), the parahippocampal place area (PPA) and lateral occipital complex (LOC). Finally, a 3D-T1 weighted scan (TE = 3.8 ms, TR = 8.2 ms, FA = 8°, FOV = 256<sup>2</sup>, matrix size = 256<sup>2</sup>, 160 sagittal slices, slice thickness = 1mm) was acquired after the functional runs. This scan was used to register the functional volumes of each run to the structural brain, after which they were registered to standard MNI (Montreal Neurological Institute) space.</p>
</sec>
<sec id="sec026">
<title>fMRI analysis: Animal/non-animal categorization</title>
<p>Analysis was performed using FEAT (FMRI Expert Analysis Tool) Version 6.00, part of FSL (FMRIB’s Software LIbrary, <ext-link ext-link-type="uri" xlink:href="http://www.fmrib.ox.ac.uk/fsl" xlink:type="simple">www.fmrib.ox.ac.uk/fsl</ext-link>) and custom Matlab code. The functional data were motion- [<xref ref-type="bibr" rid="pcbi.1006690.ref102">102</xref>] and slice-time corrected. A temporal median filter was applied to remove low frequencies, after which the data was spatially smoothed with Gaussian kernel at a 5 mm FWHM. The preprocessed scans were subjected to voxel-wise event-related GLM analysis using FILM [<xref ref-type="bibr" rid="pcbi.1006690.ref103">103</xref>] by convolving the onset times of each trial with a double gamma function to model the hemodynamic response function. We generated explanatory variables (EVs) according to the following conditions: LOW animal, LOW non-animal, MEDIUM animal, MEDIUM non-animal, HIGH animal, and HIGH non-animal. For these EVs, only correct GO trials were included; STOP, omission, and error trials were modeled as separate EVs. In addition, heartbeat and breathing measurements were included in the GLM as nuisance variables. This resulted in an estimate of the BOLD signal for each EV in each voxel, based on which the following contrasts of interest were computed: LOW (animal &gt; non-animal), MED (animal &gt; non-animal) and HIGH (animal &gt; non-animal). As the trial design contained insufficient spacing between trials to estimate a baseline condition, all analyses were conducted on these differential activity measures.</p>
</sec>
<sec id="sec027">
<title>fMRI analysis: Localizer scans</title>
<p>The localizer scans were preprocessed for the purpose of another study conducted within the same experimental session [<xref ref-type="bibr" rid="pcbi.1006690.ref056">56</xref>]. Again, the data were motion- and slicetime corrected and prewhitened. In addition, they were spatially smoothed using a 4mm Gaussian filter and were temporally filtered by means of a high-pass filter (sigma = 50s). A GLM was fitted with following EVs: for FFA, faces &gt; (houses and objects), for PPA, houses &gt; (faces and objects) and for LOC, intact images &gt; scrambled images. The resulting statistical maps were masked with anatomically-defined regions from the Harvard-Oxford cortical-structural atlas implemented in FSL. For FFA, these were the temporal occipital and occipital fusiform gyrus; for PPA, the parahippocampal gyrus and lingual gyrus, allowing activity to extend posteriorly up to MNI coordinate y = -74; for LOC, the lateral occipital cortex inferior division. Significant voxels within these masks were thresholded at z = 2.3 (FFA and PPA) or z = 3.0 (LOC). For one participant, neither left nor right LOC could be reliably identified: the ROI results for LOC are thus based on 22 instead of 23 participants.</p>
</sec>
<sec id="sec028">
<title>Statistical analysis: Behavioral data</title>
<p>Mean RT and accuracy (percentage correct) was computed for each participant based on the GO trials. Differences in accuracy and RT between the three conditions (LOW, MEDIUM, HIGH) were statistically evaluated using repeated-measures ANOVAs. Significant main effects were followed up by two-tailed, post-hoc pairwise comparisons using a Sidák correction (a variant of Bonferroni correction; [<xref ref-type="bibr" rid="pcbi.1006690.ref104">104</xref>–<xref ref-type="bibr" rid="pcbi.1006690.ref106">106</xref>]) at α = 0.05, and the p-values reported in the main text are the Sidák adjusted p-values. Data were analyzed in Matlab (Mathworks, Natick, MA, USA) and SPSS 22.0 (IBM, Armonk, USA).</p>
</sec>
<sec id="sec029">
<title>Statistical analysis: Whole brain</title>
<p>Whole brain maps were computed by contrasting animal and non-animal responses for each of the three conditions (LOW, MEDIUM, and HIGH). The resulting maps were first pooled across runs (fixed effects) and then across subjects (mixed effects using FLAME1 [<xref ref-type="bibr" rid="pcbi.1006690.ref107">107</xref>];. after which the following contrasts were run: HIGH (animal &gt; non-animal) &gt; LOW (animal &gt; non-animal); HIGH (animal &gt; non-animal) &gt; MEDIUM (animal &gt; non-animal); and MEDIUM (animal &gt; non-animal) &gt; LOW (animal &gt; non-animal). Results were corrected for multiple comparisons using cluster correction implemented in FSL (z = 2.3, p &lt; 0.05; [<xref ref-type="bibr" rid="pcbi.1006690.ref108">108</xref>]).</p>
</sec>
<sec id="sec030">
<title>Statistical analysis: ROIs</title>
<p>We additionally examined the contrasts of interest within <italic>a priori</italic> defined regions-of-interest (ROIs) derived from the functional localizer scans (see above) as well as an anatomical mask of V1 derived from the Jülich histological atlas implemented in FSL [<xref ref-type="bibr" rid="pcbi.1006690.ref109">109</xref>]. Because initial inspection of the results did not indicate major differences between hemispheres, all results are reported for bilateral ROIs. T-values obtained for the contrast of interest were averaged over voxels within each ROI. The resulting averaged activity values were compared across conditions using one-way repeated-measures ANOVAs. Significant main effects were followed up by post-hoc pairwise comparisons between conditions with a Sidák correction at α = 0.05. Data were analyzed in Matlab (Mathworks, Natick, MA, USA) and SPSS 22.0 (IBM, Armonk, USA).</p>
</sec>
</sec>
<sec id="sec031">
<title>Experiment 2 (EEG)</title>
<sec id="sec032">
<title>Subjects</title>
<p>Twenty-eight participants (8 males, 19–25 years old, mean = 21.9, SD = 1.9) took part in the EEG experiment. All participants had normal or corrected-to-normal vision, provided written informed consent and received financial compensation. The ethics committee of the University of Amsterdam approved the experiment. Two participants were excluded in preprocessing: one based on behavior (average RT 2 standard deviations from the group average) as well as bad EEG quality (excessive muscle tension reflected in the EEG signal as high frequency noise), and another due to a technical issue which led to duplicate markers being written into the EEG signal.</p>
</sec>
<sec id="sec033" sec-type="materials|methods">
<title>Experimental design</title>
<p>Participants viewed the 480 scene stimuli in randomized orders while performing an animal / non-animal speed-accuracy categorization task on the same images used for the fMRI experiment (<xref ref-type="fig" rid="pcbi.1006690.g001">Fig 1<italic>E</italic></xref>). Stimuli were presented on a 19-inch ASUS monitor with a frame rate of 60 Hz and a screen resolution of 1920 x 1080 pixels. Participants were seated 90 cm from the monitor such that stimuli subtended ~14x10° of visual angle. On each trial, one image was randomly selected and presented in the center of the screen on a grey background for 100 ms. Between trials, a fixation-cross was presented with a semi-randomly chosen duration of either 350, 400, 450, 500 or 550 ms, averaging to 450 ms. Participants searched for animals under either speed or accuracy instructions in randomly alternating blocks that each consisted of 20 trials. Each mini block started with the presentation of an instruction screen displaying either the words ‘QUICK!' for speeded blocks, or ‘ACCURATE!’ (in Dutch) for accuracy blocks for a duration of 5000 ms. In addition, before every trial, the instruction appeared again for 100 ms. Every image was presented twice, once under a speed instruction, and once under an accuracy instruction. After every 120 trials, participants took a short break. To exclude effects of response preparation, keyboard buttons were switched half-way in the experiment. Choices and RTs with respect to the start of the presentation of the image were recorded. For both instruction types, participants received feedback on their performance. On the speed trials, participants were presented with “too slow” feedback in case they failed to respond in time (&lt;500 ms), and “on time” when they were quick enough. On the accuracy trials, participants were presented with “correct” and “incorrect” feedback. Stimuli were presented using Presentation software (version 17.0, Neurobehavioral Systems, Inc).</p>
</sec>
<sec id="sec034">
<title>EEG data acquisition</title>
<p>EEG was recorded with a 64-channel Active Two EEG system (Biosemi Instrumentation, Amsterdam, The Netherlands, <ext-link ext-link-type="uri" xlink:href="http://www.biosemi.com" xlink:type="simple">www.biosemi.com</ext-link>) at a sample rate of 2048 Hz. The EEG setup was similar to that of our previous studies [<xref ref-type="bibr" rid="pcbi.1006690.ref037">37</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref039">39</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref064">64</xref>]. In short, we used caps with an extended 10–20 layout modified with 2 additional occipital electrodes (I1 and I2, which replaced F5 and F6). Eye movements were recorded with additional electrooculograms (EOG). Preprocessing was done in Brain Vision Analyzer 2 (BVA2) and included the following steps: 1) offline referencing to the average of two external electrodes placed on the earlobes; 2) a high-pass filter at 0.1 Hz (12 dB/octave), a low-pass filter at 30 Hz (24 dB/octave), and a notch filter at 50 Hz; 3) automatic removal of deflections larger than 250 mV (however, after visual inspection of the removed data segments, this threshold was raised for some participants for which data had erroneously been marked for removal due to the presence of high amplitude blinks, which were corrected for at step 5 below); 4) down sampling to 256 Hz; 5) ocular correction using semi-automatic independent component analysis (ICA) followed by visual inspection to identify the components related to eye blinks; 6) segmentation into epochs from -100 to 500 ms from stimulus onset; 7) baseline correction between -200 and 0 ms; 8) automated artifact rejection (maximal voltage 50 μV, minimal/maximal amplitudes -75/75 μV, lowest activity 0.50 μV); 9), conversion of the obtained ERPs to current source density responses [<xref ref-type="bibr" rid="pcbi.1006690.ref110">110</xref>]. For the temporal filtering, phase shift-free Butterworth filters implemented in BVA2 were used. Median rejection rate across subjects was 94.5 out of 960 ERP trials (min 8 trials, max 486 trials); in total, 13.6% of the data was rejected. After preprocessing, the ERPs were imported into MatLab (Mathworks, Natick, MA, USA) for statistical analysis.</p>
</sec>
<sec id="sec035">
<title>Statistical analysis: Behavioral data</title>
<p>Choice accuracy and reaction times (RTs) were computed separately for the speed and accurate blocks. Fast guesses (RTs &lt; 150 ms) or RTs &gt; 3 standard deviations from the mean were removed before analysis (mean = 2.2%, SD = 0.9%, min = 0.8%, max = 4.2%). Differences between the LOW, MEDIUM and HIGH condition were tested using two-factor repeated-measures ANOVAs. Significant main effects were followed up by post-hoc pairwise comparisons between conditions or tasks using Sidák multiple comparisons correction at α = 0.05. Data were analyzed in Matlab (Mathworks, Natick, MA, USA) and SPSS 22.0 (IBM, Armonk, USA).</p>
</sec>
<sec id="sec036">
<title>Drift diffusion modeling</title>
<p>Based on go trial RT distributions of both correct responses and errors, the formal Ratcliff drift diffusion model (DDM) estimates the speed of evidence accumulation, ‘drift rate’, the variability of evidence accumulation ƞ, the amount of evidence needed for a decision (a), the starting point of evidence accumulation (z), and the variability of this starting point (S<sub>z</sub>). Together these parameters generate a distribution of decision times (DT). However, observed reaction times (RT) are also thought to contain non-stimulus specific components such as response preparation and motor execution, which combine in the parameters non-decision time (T<sub>er</sub>), and non-decision time variability (S<sub>t</sub>). In general, DDM assumes that (T<sub>er</sub>) simply shifts the distribution of DT such that: RT = DT+ T<sub>er</sub> [<xref ref-type="bibr" rid="pcbi.1006690.ref044">44</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref111">111</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref112">112</xref>].</p>
<p>To analyze the RT data of the EEG experiment with the drift diffusion model we used a recently developed hierarchical Bayesian estimation of DDM parameters (HDDM) implemented in Python (version 0.6.0, see <ext-link ext-link-type="uri" xlink:href="http://ski.clps.brown.edu/hddm_docs/" xlink:type="simple">http://ski.clps.brown.edu/hddm_docs/</ext-link>), which allows the simultaneous estimation of subject and group parameters and thus requires less data per subject [<xref ref-type="bibr" rid="pcbi.1006690.ref045">45</xref>]. To gain a deeper insight into how scene complexity affects choice RT, we investigated a model where the speed of information accumulation (v) and the amount of evidence required to reach a choice (a) were both allowed to vary across the three natural scene conditions (LOW, MED, HIGH). Moreover, because previous work has consistently shown that participants require more evidence to reach a choice during accurate instruction trials (as compared to speed trials), evidence requirements (a) was additionally allowed to vary across ‘speed’ or ‘accurate’ instruction trials [<xref ref-type="bibr" rid="pcbi.1006690.ref113">113</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref114">114</xref>]. For this model, three chains of 20,000 samples were generated from the posteriors. In order to assure chain convergence, the first 5,000 samples were discarded (burn), resulting in a trace of 15000 samples for each chain. These chains were then tested for convergence using the Gelman-Rubin statistic, which compares the intra-chain variance of the model to the intra-chain variance of different runs of the same model. All chains were converged and all Rhats were close to 1 [<xref ref-type="bibr" rid="pcbi.1006690.ref115">115</xref>].</p>
</sec>
<sec id="sec037">
<title>Statistical analysis: EEG</title>
<p>For each individual subject we computed the average ERP to animal and non-animal scenes in each of the three conditions. Following previous EEG studies on figure-ground segmentation [<xref ref-type="bibr" rid="pcbi.1006690.ref018">18</xref>,<xref ref-type="bibr" rid="pcbi.1006690.ref046">46</xref>], ERPs were pooled across a set of occipital and peri-occipital electrodes overlying visual cortex (O1, O2, Oz, POz, PO3, PO4, PO6, PO7, PO8). Per condition, difference waves between animal and non-animal scenes were tested against zero using two-tailed, one-sample t-tests. Difference waves were compared across conditions using the following two-tailed, paired-samples t-tests: HIGH (animal &gt; non-animal) vs. LOW (animal &gt; non-animal); HIGH (animal &gt; non-animal) vs. MEDIUM (animal &gt; non-animal); and MEDIUM (animal &gt; non-animal) vs. LOW (animal &gt; non-animal). Given the large number of statistical comparisons, the results were corrected for multiple comparisons across the two task-instructions, conditions and time-points by means of FDR correction at α = 0.01 using an empirically derived FDR-corrected threshold of <italic>q</italic> = 0.0014).</p>
<p>To assess the trial-by-trial linear relationship between the EEG amplitudes and drift rate, we fitted a HDDM regression model [<xref ref-type="bibr" rid="pcbi.1006690.ref045">45</xref>] to the RT distributions for target and non-target decisions (referred to as ‘response-coded’ in HDDM), across both speeded and accurate trials, with LOW, MED and HIGH coded as categorical variables. As the ERP predictor per trial, we took the average ERP amplitude between the first significant deflection until the peak amplitude of the average HIGH difference wave for speed and accurate trials (220–325 ms). We ran four separate chains with 20,000 samples. The first 7500 samples were discarded (burn) and every 10th sample (thin) was concatenated, resulting in a trace of 5000 samples. The model was tested for convergence using the Gelman-Rubin statistic, which compares the intra-chain variance of the model to the intra-chain variance of the different runs. All chains were converged and all values were close to 1. Since HDDM uses Bayesian estimation to obtain the posterior probability densities of the regression weights, the p-values for the distributions in <xref ref-type="fig" rid="pcbi.1006690.g007">Fig 7</xref> were analyzed directly for hypothesis testing. Specifically, the p-value for the significance of each regression weight reflects the percentage of the posterior probability distribution that crosses the zero point. The p-value for the test between distribution reflects the percentage of distribution 1 (e.g., LOW) that crosses the mean of distribution 2 (e.g., MED), see <ext-link ext-link-type="uri" xlink:href="http://ski.clps.brown.edu/hddm_docs/howto.html#hypothesis-testing" xlink:type="simple">http://ski.clps.brown.edu/hddm_docs/howto.html#hypothesis-testing</ext-link>.</p>
</sec>
</sec>
</sec>
<sec id="sec038">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006690.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006690.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Influence of non-animal image content on behavioral and EEG results from Experiment 2.</title>
<p>A) Behavioral reaction time and accuracy as a function of complexity (LOW, MED, HIGH) and task instruction (speeded or accurate) when only including (left) or excluding (right) non-animal scenes with vehicles, humans and man-made objects (manually annotated). B-C) Differences in ERP amplitude for animal and non-animal scenes for LOW, MED, and HIGH complexity scenes, computed with (left) or without (right) scenes with vehicles, humans and man-made objects, separately for speed (B) and accurate (C) instructions. The ‘with vehicles, humans, objects’ analysis included 51 non-animal trials for LOW, 58 scenes for MED, and 57 scenes for HIGH; the ‘without vehicles, humans, objects’ analysis included 29 trials for LOW, 22 trials for MED, and 23 trials for HIGH.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Anne Urai for helpful discussions on the HDDM analyses.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006690.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>VanRullen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Thorpe</surname> <given-names>SJ</given-names></name>. <article-title>Surfing a spike wave down the ventral stream</article-title>. <source>Vision Res</source>. <year>2002</year>;<volume>42</volume>: <fpage>2593</fpage>–<lpage>615</lpage>. <object-id pub-id-type="pmid">12446033</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Agam</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Madsen</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Kreiman</surname> <given-names>G</given-names></name>. <article-title>Timing, timing, timing: fast decoding of object information from intracranial field potentials in human visual cortex</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>62</volume>: <fpage>281</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.02.025" xlink:type="simple">10.1016/j.neuron.2009.02.025</ext-link></comment> <object-id pub-id-type="pmid">19409272</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorpe</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fize</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Marlot</surname> <given-names>C</given-names></name>. <article-title>Speed of processing in the human visual system</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>: <fpage>520</fpage>–<lpage>522</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/381520a0" xlink:type="simple">10.1038/381520a0</ext-link></comment> <object-id pub-id-type="pmid">8632824</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>VanRullen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Thorpe</surname> <given-names>S</given-names></name>. <article-title>The time course of visual processing: from early perception to decision-making</article-title>. <source>J Cogn Neurosci</source>. <year>2001</year>;<volume>13</volume>: <fpage>454</fpage>–<lpage>61</lpage>. <object-id pub-id-type="pmid">11388919</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serre</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>A feedforward architecture accounts for rapid categorization</article-title>. <source>Proc Natl Acad Sci</source>. <year>2007</year>;<volume>104</volume>: <fpage>6424</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0700622104" xlink:type="simple">10.1073/pnas.0700622104</ext-link></comment> <object-id pub-id-type="pmid">17404214</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Serre</surname> <given-names>T</given-names></name>. <article-title>Models of visual cortex</article-title>. <source>Scholarpedia</source>. <year>2013</year>;<volume>8</volume>: <fpage>3516</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamins</surname> <given-names>DLK</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Dicarlo</surname> <given-names>JJ</given-names></name>. <article-title>Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</article-title>. <source>Adv Neural Inf Process Syst</source>. <year>2013</year>;<volume>26</volume>: <fpage>3093</fpage>–<lpage>3101</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003915" xlink:type="simple">10.1371/journal.pcbi.1003915</ext-link></comment> <object-id pub-id-type="pmid">25375136</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Güçlü</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>van Gerven</surname> <given-names>MAJ</given-names></name>. <article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>: <fpage>10005</fpage>–<lpage>10014</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5023-14.2015" xlink:type="simple">10.1523/JNEUROSCI.5023-14.2015</ext-link></comment> <object-id pub-id-type="pmid">26157000</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rockland</surname> <given-names>KS</given-names></name>, <name name-style="western"><surname>Pandya</surname> <given-names>DN</given-names></name>. <article-title>Laminar origins and terminations of cortical connections of the occipital lobe in the rhesus monkey</article-title>. <source>Brain Res</source>. <year>1979</year>;<volume>179</volume>: <fpage>3</fpage>–<lpage>20</lpage>. <object-id pub-id-type="pmid">116716</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Salin</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Bullier</surname> <given-names>J</given-names></name>. <article-title>Corticocortical connections in the visual system: structure and function</article-title>. <source>Physiol Rev</source>. <year>1995</year>;<volume>75</volume>: <fpage>107</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/physrev.1995.75.1.107" xlink:type="simple">10.1152/physrev.1995.75.1.107</ext-link></comment> <object-id pub-id-type="pmid">7831395</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Essen</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Maunsell</surname> <given-names>JHR</given-names></name>. <article-title>Hierarchical organization and functional streams in the visual cortex</article-title>. <source>Trends Neurosci</source>. <year>1983</year>;<volume>6</volume>: <fpage>370</fpage>–<lpage>375</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0166-2236(83)90167-4" xlink:type="simple">10.1016/0166-2236(83)90167-4</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006690.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kravitz</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Saleem</surname> <given-names>KS</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CI</given-names></name>, <name name-style="western"><surname>Ungerleider</surname> <given-names>LG</given-names></name>, <name name-style="western"><surname>Mishkin</surname> <given-names>M</given-names></name>. <article-title>The ventral visual pathway: an expanded neural framework for the processing of object quality</article-title>. <source>Trends Cogn Sci. Elsevier Ltd</source>; <year>2013</year>;<volume>17</volume>: <fpage>26</fpage>–<lpage>49</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2012.10.011" xlink:type="simple">10.1016/j.tics.2012.10.011</ext-link></comment> <object-id pub-id-type="pmid">23265839</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>. <article-title>The neurophysiology of figure-ground segregation in primary visual cortex</article-title>. <source>J Neurosci</source>. <year>1995</year>;<volume>15</volume>: <fpage>1605</fpage>–<lpage>1615</lpage>. <object-id pub-id-type="pmid">7869121</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zipser</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>PH</given-names></name>. <article-title>Contextual modulation in primary visual cortex</article-title>. <source>J Neurosci</source>. <year>1996</year>;<volume>16</volume>: <fpage>7376</fpage>–<lpage>89</lpage>. <object-id pub-id-type="pmid">8929444</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Self</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>van Kerkoerle</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Supèr</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>. <article-title>Distinct Roles of the Cortical Layers of Area V1 in Figure-Ground Segregation</article-title>. <source>Curr Biol</source>. <year>2013</year>;<volume>23</volume>: <fpage>2121</fpage>–<lpage>2129</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2013.09.013" xlink:type="simple">10.1016/j.cub.2013.09.013</ext-link></comment> <object-id pub-id-type="pmid">24139742</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poort</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Self</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>van Vugt</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Malkki</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>. <article-title>Texture Segregation Causes Early Figure Enhancement and Later Ground Suppression in Areas V1 and V4 of Visual Cortex</article-title>. <source>Cereb cortex</source>. <year>2016</year>;<volume>26</volume>: <fpage>3964</fpage>–<lpage>3976</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhw235" xlink:type="simple">10.1093/cercor/bhw235</ext-link></comment> <object-id pub-id-type="pmid">27522074</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wokke</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Sligte</surname> <given-names>IG</given-names></name>, <name name-style="western"><surname>Steven Scholte</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>. <article-title>Two critical periods in early visual cortex during figure-ground segregation</article-title>. <source>Brain Behav</source>. <year>2012</year>;<volume>2</volume>: <fpage>763</fpage>–<lpage>777</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/brb3.91" xlink:type="simple">10.1002/brb3.91</ext-link></comment> <object-id pub-id-type="pmid">23170239</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Heinen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Jolij</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>. <article-title>Figure-ground segregation requires two distinct periods of activity in V1: a transcranial magnetic stimulation study</article-title>. <source>Neuroreport</source>. <year>2005</year>;<volume>16</volume>: <fpage>1483</fpage>–<lpage>1487</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1097/01.wnr.0000246328.96932.4c" xlink:type="simple">10.1097/01.wnr.0000246328.96932.4c</ext-link></comment> <object-id pub-id-type="pmid">16110276</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wokke</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Vandenbroucke</surname> <given-names>ARE</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>. <article-title>Confuse your illusion: feedback to early visual cortex contributes to perceptual completion</article-title>. <source>Psychol Sci</source>. <year>2013</year>;<volume>24</volume>: <fpage>63</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0956797612449175" xlink:type="simple">10.1177/0956797612449175</ext-link></comment> <object-id pub-id-type="pmid">23228938</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>. <article-title>Cortical algorithms for perceptual grouping</article-title>. <source>Annu Rev Neurosci</source>. <year>2006</year>;<volume>29</volume>: <fpage>203</fpage>–<lpage>27</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.neuro.29.051605.112939" xlink:type="simple">10.1146/annurev.neuro.29.051605.112939</ext-link></comment> <object-id pub-id-type="pmid">16776584</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>, <name name-style="western"><surname>Spekreijse</surname> <given-names>H</given-names></name>. <article-title>The implementation of visual routines</article-title>. <source>Vision Res</source>. <year>2000</year>;<volume>40</volume>: <fpage>1385</fpage>–<lpage>411</lpage>. <object-id pub-id-type="pmid">10788648</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lamme</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>P</given-names></name>. <article-title>The distinct modes of vision offered by feedforward and recurrent processing</article-title>. <source>Trends Neurosci</source>. <year>2000</year>;<volume>23</volume>: <fpage>571</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">11074267</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koivisto</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Railo</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Revonsuo</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vanni</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Salminen-Vaparanta</surname> <given-names>N</given-names></name>. <article-title>Recurrent processing in V1/V2 contributes to categorization of natural scenes</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>: <fpage>2488</fpage>–<lpage>92</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3074-10.2011" xlink:type="simple">10.1523/JNEUROSCI.3074-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21325516</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Camprodon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Zohary</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Brodbeck</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Pascual-Leone</surname> <given-names>A</given-names></name>. <article-title>Two phases of V1 activity for visual recognition of natural images</article-title>. <source>J Cogn Neurosci</source>. <year>2010</year>;<volume>22</volume>: <fpage>1262</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2009.21253" xlink:type="simple">10.1162/jocn.2009.21253</ext-link></comment> <object-id pub-id-type="pmid">19413482</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wyatte</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Curran</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>O’Reilly</surname> <given-names>R</given-names></name>. <article-title>The limits of feedforward vision: recurrent processing promotes robust object recognition when objects are degraded</article-title>. <source>J Cogn Neurosci</source>. <year>2012</year>;<volume>24</volume>: <fpage>2248</fpage>–<lpage>61</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_00282" xlink:type="simple">10.1162/jocn_a_00282</ext-link></comment> <object-id pub-id-type="pmid">22905822</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>, <name name-style="western"><surname>Zipser</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Spekreijse</surname> <given-names>H</given-names></name>. <article-title>Masking interrupts figure-ground signals in V1</article-title>. <source>J Cogn Neurosci</source>. <year>2002</year>;<volume>14</volume>: <fpage>1044</fpage>–<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089892902320474490" xlink:type="simple">10.1162/089892902320474490</ext-link></comment> <object-id pub-id-type="pmid">12419127</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fahrenfort</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>. <article-title>Masking disrupts reentrant processing in human visual cortex</article-title>. <source>J Cogn Neurosci</source>. <year>2007</year>;<volume>19</volume>: <fpage>1488</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2007.19.9.1488" xlink:type="simple">10.1162/jocn.2007.19.9.1488</ext-link></comment> <object-id pub-id-type="pmid">17714010</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Loon</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Van Gaal</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>van der Hoort</surname> <given-names>BJJ</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>. <article-title>GABA A agonist reduces visual awareness: A masking—EEG experiment</article-title>. <source>J Cogn Neurosci</source>. <year>2012</year>;<volume>24</volume>: <fpage>965</fpage>–<lpage>974</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_00197" xlink:type="simple">10.1162/jocn_a_00197</ext-link></comment> <object-id pub-id-type="pmid">22264199</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koivisto</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kastrati</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Revonsuo</surname> <given-names>A</given-names></name>. <article-title>Recurrent processing enhances visual awareness but is not necessary for fast categorization of natural scenes</article-title>. <source>J Cogn Neurosci</source>. <year>2013</year>;<volume>26</volume>: <fpage>223</fpage>–<lpage>231</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_00486" xlink:type="simple">10.1162/jocn_a_00486</ext-link></comment> <object-id pub-id-type="pmid">24047378</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Cesarei</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Loftus</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Mastria</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Codispoti</surname> <given-names>M</given-names></name>. <article-title>Understanding natural scenes: Contributions of image statistics</article-title>. <source>Neurosci Biobehav Rev. Elsevier Ltd</source>; <year>2017</year>;<volume>74</volume>: <fpage>44</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neubiorev.2017.01.012" xlink:type="simple">10.1016/j.neubiorev.2017.01.012</ext-link></comment> <object-id pub-id-type="pmid">28089884</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>Statistics of natural image categories</article-title>. <source>Netw Comput Neural Syst</source>. <year>2003</year>;<volume>14</volume>: <fpage>391</fpage>–<lpage>412</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/0954-898X/14/3/302" xlink:type="simple">10.1088/0954-898X/14/3/302</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006690.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>Gist of the scene</article-title>. <source>Neurobiology of Attention</source>. <year>2005</year>. pp. <fpage>251</fpage>–<lpage>257</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geusebroek</surname> <given-names>J-M</given-names></name>, <name name-style="western"><surname>Smeulders</surname> <given-names>AWM</given-names></name>. <article-title>Fragmentation in the vision of scenes</article-title>. <source>Proc Ninth IEEE Int Conf Comput Vis. Ieee</source>; <year>2003</year>; <fpage>130</fpage>–<lpage>135</lpage> <volume>vol.1</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/ICCV.2003.1238326" xlink:type="simple">10.1109/ICCV.2003.1238326</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006690.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Ghebreab</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Waldorp</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Smeulders</surname> <given-names>AWM</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>. <article-title>Brain responses strongly correlate with Weibull image statistics when processing natural images</article-title>. <source>J Vis</source>. <year>2009</year>;<volume>9</volume>: <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghebreab</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Smeulders</surname> <given-names>AWM</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>. <article-title>A biologically plausible model for rapid natural image identification</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2009</year>. pp. <fpage>629</fpage>–<lpage>637</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Groen</surname> <given-names>IIA</given-names></name>, <name name-style="western"><surname>Ghebreab</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Prins</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>, <name name-style="western"><surname>Steven Scholte</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>. <article-title>From image statistics to scene gist: evoked neural activity reveals transition from low-level natural image structure to scene category</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>: <fpage>18814</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3128-13.2013" xlink:type="simple">10.1523/JNEUROSCI.3128-13.2013</ext-link></comment> <object-id pub-id-type="pmid">24285888</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Groen</surname> <given-names>IIA</given-names></name>, <name name-style="western"><surname>Ghebreab</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>. <article-title>Low-level contrast statistics are diagnostic of invariance of natural textures</article-title>. <source>Front Comput Neurosci</source>. <year>2012</year>;<volume>6</volume>: <fpage>34</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2012.00034" xlink:type="simple">10.3389/fncom.2012.00034</ext-link></comment> <object-id pub-id-type="pmid">22701419</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Groen</surname> <given-names>IIA</given-names></name>, <name name-style="western"><surname>Ghebreab</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>. <article-title>Spatially pooled contrast responses predict neural and perceptual similarity of naturalistic image categories</article-title>. <source>PLoS Comput Biol. Public Library of Science</source>; <year>2012</year>;<volume>8</volume>: <fpage>e1002726</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002726" xlink:type="simple">10.1371/journal.pcbi.1002726</ext-link></comment> <object-id pub-id-type="pmid">23093921</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Groen</surname> <given-names>IIA</given-names></name>, <name name-style="western"><surname>Kravitz</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Deouell</surname> <given-names>LY</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CI</given-names></name>. <article-title>The time course of scene processing: A multi-faceted EEG investigation</article-title>. <source>eNeuro</source>. <year>2016</year>;<volume>3</volume>: <fpage>e0139</fpage>–<lpage>16</lpage>.2016. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/ENEURO.0139-16.2016" xlink:type="simple">http://dx.doi.org/10.1523/ENEURO.0139-16.2016</ext-link></mixed-citation></ref>
<ref id="pcbi.1006690.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghodrati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ghodousi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Yoonessi</surname> <given-names>A</given-names></name>. <article-title>Low-Level Contrast Statistics of Natural Images Can Modulate the Frequency of Event-Related Potentials (ERP) in Humans</article-title>. <source>Front Hum Neurosci</source>. <year>2016</year>;<volume>10</volume>: <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Cesarei</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Cavicchi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Micucci</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Codispoti</surname> <given-names>M</given-names></name>. <article-title>Categorization Goals Modulate the Use of Natural Scene Statistics</article-title>. <source>J Cogn Neurosci</source>. <year>2018</year>;<volume>xx</volume>: <fpage>1</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_01333" xlink:type="simple">10.1162/jocn_a_01333</ext-link></comment> <object-id pub-id-type="pmid">30188778</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ratcliff</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>PL</given-names></name>. <article-title>Perceptual discrimination in static and dynamic noise: the temporal relation between perceptual encoding and decision making</article-title>. <source>J Exp Psychol Gen</source>. <year>2010</year>;<volume>139</volume>: <fpage>70</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0018128" xlink:type="simple">10.1037/a0018128</ext-link></comment> <object-id pub-id-type="pmid">20121313</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ratcliff</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>McKoon</surname> <given-names>G</given-names></name>. <article-title>The diffusion decision model: theory and data for two-choice decision tasks</article-title>. <source>Neural Comput</source>. <year>2008</year>;<volume>20</volume>: <fpage>873</fpage>–<lpage>922</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2008.12-06-420" xlink:type="simple">10.1162/neco.2008.12-06-420</ext-link></comment> <object-id pub-id-type="pmid">18085991</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wiecki</surname> <given-names>T V</given-names></name>, <name name-style="western"><surname>Sofer</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>. <article-title>HDDM: Hierarchical Bayesian estimation of the Drift-Diffusion Model in Python</article-title>. <source>Front Neuroinform</source>. <year>2013</year>;<volume>7</volume>: <fpage>14</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fninf.2013.00014" xlink:type="simple">10.3389/fninf.2013.00014</ext-link></comment> <object-id pub-id-type="pmid">23935581</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Jolij</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Fahrenfort</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>. <article-title>Feedforward and recurrent processing in scene segmentation: electroencephalography and functional magnetic resonance imaging</article-title>. <source>J Cogn Neurosci</source>. <year>2008</year>;<volume>20</volume>: <fpage>2097</fpage>–<lpage>109</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2008.20142" xlink:type="simple">10.1162/jocn.2008.20142</ext-link></comment> <object-id pub-id-type="pmid">18416684</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ratcliff</surname> <given-names>R</given-names></name>. <article-title>A theory of memory retrieval</article-title>. <source>Psychol Rev</source>. <year>1978</year>;<volume>85</volume>: <fpage>59</fpage>–<lpage>108</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cavanagh</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Wiecki</surname> <given-names>T V.</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>MX</given-names></name>, <name name-style="western"><surname>Figueroa</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Samanta</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sherman</surname> <given-names>SJ</given-names></name>, <etal>et al</etal>. <article-title>Subthalamic nucleus stimulation reverses mediofrontal influence over decision threshold</article-title>. <source>Nat Neurosci. Nature Publishing Group</source>; <year>2011</year>;<volume>14</volume>: <fpage>1462</fpage>–<lpage>1467</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2925" xlink:type="simple">10.1038/nn.2925</ext-link></comment> <object-id pub-id-type="pmid">21946325</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Delis</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Dmochowski</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Sajda</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Q</given-names></name>. <article-title>Correlation of neural activity with behavioral kinematics reveals distinct sensory encoding and evidence accumulation processes during active tactile sensing</article-title>. <source>Neuroimage</source>. <year>2018</year>;<volume>175</volume>: <fpage>12</fpage>–<lpage>21</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2018.03.035" xlink:type="simple">https://doi.org/10.1016/j.neuroimage.2018.03.035</ext-link> <object-id pub-id-type="pmid">29580968</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hochstein</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ahissar</surname> <given-names>M</given-names></name>. <article-title>View from the top: Hierarchies and reverse hierarchies in the visual system</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>36</volume>: <fpage>791</fpage>–<lpage>804</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0896-6273(02)01091-7" xlink:type="simple">10.1016/S0896-6273(02)01091-7</ext-link></comment> <object-id pub-id-type="pmid">12467584</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Petro</surname> <given-names>LS</given-names></name>, <name name-style="western"><surname>Vizioli</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Muckli</surname> <given-names>L</given-names></name>. <article-title>Contributions of cortical feedback to sensory processing in primary visual cortex</article-title>. <source>Front Psychol</source>. <year>2014</year>;<volume>5</volume>: <fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Muckli</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>De Martino</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Vizioli</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Petro</surname> <given-names>LS</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>FW</given-names></name>, <name name-style="western"><surname>Ugurbil</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Contextual Feedback to Superficial Layers of V1</article-title>. <source>Curr Biol</source>. The Authors; <year>2015</year>;<volume>25</volume>: <fpage>2690</fpage>–<lpage>2695</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2015.08.057" xlink:type="simple">10.1016/j.cub.2015.08.057</ext-link></comment> <object-id pub-id-type="pmid">26441356</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Treisman</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Gelade</surname> <given-names>G</given-names></name>. <article-title>A feature-integration theory of attention</article-title>. <source>Cogn Psychol</source>. <year>1980</year>;<volume>136</volume>: <fpage>97</fpage>–<lpage>136</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kastner</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ungerleider</surname> <given-names>LG</given-names></name>. <article-title>Mechanisms of visual attention in the human cortex</article-title>. <source>Annu Rev Neurosci</source>. <year>2000</year>;<volume>23</volume>: <fpage>315</fpage>–<lpage>341</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.neuro.23.1.315" xlink:type="simple">10.1146/annurev.neuro.23.1.315</ext-link></comment> <object-id pub-id-type="pmid">10845067</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Logan</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Burkell</surname> <given-names>J</given-names></name>. <article-title>Dependence and independence in responding to double stimulation: A comparison of stop, change, and dual-task paradigms</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>1986</year>;<volume>12</volume>: <fpage>549</fpage>–<lpage>563</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0096-1523.12.4.549" xlink:type="simple">10.1037/0096-1523.12.4.549</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006690.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jahfari</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Waldorp</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ridderinkhof</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>. <article-title>Visual information shapes the dynamics of corticobasal ganglia pathways during response selection and inhibition</article-title>. <source>J Cogn Neurosci</source>. <year>2015</year>;<volume>27</volume>: <fpage>1344</fpage>–<lpage>1359</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_00792" xlink:type="simple">10.1162/jocn_a_00792</ext-link></comment> <object-id pub-id-type="pmid">25647338</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Verbruggen</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Logan</surname> <given-names>GD</given-names></name>. <article-title>Proactive adjustments of response strategies in the stop-signal paradigm</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>2009</year>;<volume>35</volume>: <fpage>835</fpage>–<lpage>854</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0012726" xlink:type="simple">10.1037/a0012726</ext-link></comment> <object-id pub-id-type="pmid">19485695</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jahfari</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Verbruggen</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Waldorp</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Colzato</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ridderinkhof</surname> <given-names>KR</given-names></name>, <etal>et al</etal>. <article-title>How Preparation Changes the Need for Top-Down Control of the Basal Ganglia When Inhibiting Premature Actions</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>: <fpage>10870</fpage>–<lpage>10878</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0902-12.2012" xlink:type="simple">10.1523/JNEUROSCI.0902-12.2012</ext-link></comment> <object-id pub-id-type="pmid">22875921</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wiecki</surname> <given-names>T V</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>. <article-title>A computational model of inhibitory control in frontal cortex and basal ganglia</article-title>. <source>Psychol Rev</source>. <year>2013</year>;<volume>120</volume>: <fpage>329</fpage>–<lpage>355</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0031542" xlink:type="simple">10.1037/a0031542</ext-link></comment> <object-id pub-id-type="pmid">23586447</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref060"><label>60</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sajda</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Philiastides</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Heekeren</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Ratcliff</surname> <given-names>R</given-names></name>. <chapter-title>Linking Neuronal Variability to Perceptual Decision Making via Neuroimaging</chapter-title>. In: <name name-style="western"><surname>Ding</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Glanzman</surname> <given-names>D</given-names></name>, editors. <source>The Dynamic Brain: an Exploration of Neuronal Variability and Its Functional Significance</source>. <publisher-name>Oxford scholarship online</publisher-name>; <year>2011</year>. pp. <fpage>214</fpage>–<lpage>231</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Philiastides</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Heekeren</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Sajda</surname> <given-names>P</given-names></name>. <article-title>Human Scalp Potentials Reflect a Mixture of Decision-Related Signals during Perceptual Choices</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>: <fpage>16877</fpage>–<lpage>16889</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3012-14.2014" xlink:type="simple">10.1523/JNEUROSCI.3012-14.2014</ext-link></comment> <object-id pub-id-type="pmid">25505339</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Heekeren</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Marrett</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ungerleider</surname> <given-names>LG</given-names></name>. <article-title>The neural systems that mediate human perceptual decision making</article-title>. <source>Nat Rev Neurosci</source>. <year>2008</year>;<volume>9</volume>: <fpage>467</fpage>–<lpage>479</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn2374" xlink:type="simple">10.1038/nrn2374</ext-link></comment> <object-id pub-id-type="pmid">18464792</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Philiastides</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Ratcliff</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sajda</surname> <given-names>P</given-names></name>. <article-title>Neural representation of task difficulty and decision making during perceptual categorization: a timing diagram</article-title>. <source>J Neurosci</source>. <year>2006</year>;<volume>26</volume>: <fpage>8965</fpage>–<lpage>75</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1655-06.2006" xlink:type="simple">10.1523/JNEUROSCI.1655-06.2006</ext-link></comment> <object-id pub-id-type="pmid">16943552</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Groen</surname> <given-names>IIA</given-names></name>, <name name-style="western"><surname>Ghebreab</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>. <article-title>The time course of natural scene perception with reduced attention</article-title>. <source>J Neurophysiol</source>. <year>2016</year>;<volume>115</volume>: <fpage>931</fpage>–<lpage>946</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00896.2015" xlink:type="simple">10.1152/jn.00896.2015</ext-link></comment> <object-id pub-id-type="pmid">26609116</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schyns</surname> <given-names>PG</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>From blobs to boundary edges: Evidence for time- and spatial-scale-dependent scene recognition</article-title>. <source>Psychol Sci</source>. <year>1994</year>;<volume>5</volume>: <fpage>195</fpage>–<lpage>200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-9280.1994.tb00500.x" xlink:type="simple">10.1111/j.1467-9280.1994.tb00500.x</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006690.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schyns</surname> <given-names>PG</given-names></name>. <article-title>Coarse blobs or fine edges? Evidence that information diagnosticity changes the perception of complex visual stimuli</article-title>. <source>Cogn Psychol</source>. <year>1997</year>;<volume>34</volume>: <fpage>72</fpage>–<lpage>107</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1006/cogp.1997.0667" xlink:type="simple">10.1006/cogp.1997.0667</ext-link></comment> <object-id pub-id-type="pmid">9325010</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bar</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kassam</surname> <given-names>KS</given-names></name>, <name name-style="western"><surname>Ghuman</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Boshyan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schmid</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Dale</surname> <given-names>AM</given-names></name>, <etal>et al</etal>. <article-title>Top-down facilitation of visual recognition</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2006</year>;<volume>103</volume>: <fpage>449</fpage>–<lpage>454</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0507062103" xlink:type="simple">10.1073/pnas.0507062103</ext-link></comment> <object-id pub-id-type="pmid">16407167</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seijdel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Jahfari</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Groen</surname> <given-names>IIA</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>. <source>Low-level image statistics in natural scenes influence perceptual decision-making</source>. <year>2018</year>; <fpage>1</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/P3R8A" xlink:type="simple">10.17605/OSF.IO/P3R8A</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006690.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brandman</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Vincent Peelen</surname> <given-names>M</given-names></name>. <article-title>Interaction between scene and object processing revealed by human fMRI and MEG decoding</article-title>. <source>J Neurosci</source>. <year>2017</year>; <fpage>0582</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0582-17.2017" xlink:type="simple">10.1523/JNEUROSCI.0582-17.2017</ext-link></comment> <object-id pub-id-type="pmid">28687603</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aminoff</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Kveraga</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Bar</surname> <given-names>M</given-names></name>. <article-title>The role of the parahippocampal cortex in cognition</article-title>. <source>Trends Cogn Sci. Elsevier Ltd</source>; <year>2013</year>;<volume>17</volume>: <fpage>379</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2013.06.009" xlink:type="simple">10.1016/j.tics.2013.06.009</ext-link></comment> <object-id pub-id-type="pmid">23850264</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kveraga</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ghuman</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Kassam</surname> <given-names>KS</given-names></name>, <name name-style="western"><surname>Aminoff</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Hamalainen</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Chaumon</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Early onset of neural synchronization in the contextual associations network</article-title>. <source>Proc Natl Acad Sci</source>. <year>2011</year>;<volume>108</volume>: <fpage>3389</fpage>–<lpage>3394</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1013760108" xlink:type="simple">10.1073/pnas.1013760108</ext-link></comment> <object-id pub-id-type="pmid">21300869</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ruff</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bodurka</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Esteky</surname> <given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title>. <source>Neuron. Elsevier Ltd</source>; <year>2008</year>;<volume>60</volume>: <fpage>1126</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2008.10.043" xlink:type="simple">10.1016/j.neuron.2008.10.043</ext-link></comment> <object-id pub-id-type="pmid">19109916</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Naselaris</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Stansbury</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Cortical representation of animate and inanimate objects in complex natural scenes</article-title>. <source>J Physiol Paris. Elsevier Ltd</source>; <year>2012</year>;<volume>106</volume>: <fpage>239</fpage>–<lpage>49</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jphysparis.2012.02.001" xlink:type="simple">10.1016/j.jphysparis.2012.02.001</ext-link></comment> <object-id pub-id-type="pmid">22472178</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Epstein</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>. <article-title>A cortical representation of the local visual environment</article-title>. <source>Nature</source>. <year>1998</year>;<volume>392</volume>: <fpage>598</fpage>–<lpage>601</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/33402" xlink:type="simple">10.1038/33402</ext-link></comment> <object-id pub-id-type="pmid">9560155</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kravitz</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Peng</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CI</given-names></name>. <article-title>Real-world scene representations in high-level visual cortex: it’s the spaces more than the places</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>: <fpage>7322</fpage>–<lpage>7333</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4588-10.2011" xlink:type="simple">10.1523/JNEUROSCI.4588-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21593316</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Park</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Brady</surname> <given-names>TF</given-names></name>, <name name-style="western"><surname>Greene</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>Disentangling scene content from spatial boundary: complementary roles for the parahippocampal place area and lateral occipital complex in representing real-world scenes</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>: <fpage>1333</fpage>–<lpage>40</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3885-10.2011" xlink:type="simple">10.1523/JNEUROSCI.3885-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21273418</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Troiani</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Stigliani</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Epstein</surname> <given-names>RA</given-names></name>. <article-title>Multiple object properties drive scene-selective regions</article-title>. <source>Cereb Cortex</source>. <year>2014</year>;<volume>24</volume>: <fpage>883</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhs364" xlink:type="simple">10.1093/cercor/bhs364</ext-link></comment> <object-id pub-id-type="pmid">23211209</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kravitz</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CI</given-names></name>. <article-title>Deconstructing visual scenes in cortex: gradients of object and spatial layout Information</article-title>. <source>Cereb Cortex</source>. <year>2012</year>;<volume>23</volume>: <fpage>947</fpage>–<lpage>957</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhs091" xlink:type="simple">10.1093/cercor/bhs091</ext-link></comment> <object-id pub-id-type="pmid">22473894</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cant</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>Y</given-names></name>. <article-title>Object ensemble processing in human anterior-medial ventral visual cortex</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>: <fpage>7685</fpage>–<lpage>700</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3325-11.2012" xlink:type="simple">10.1523/JNEUROSCI.3325-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22649247</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rajimehr</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Devaney</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Bilenko</surname> <given-names>NY</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Tootell</surname> <given-names>RBH</given-names></name>. <article-title>The “parahippocampal place area” responds preferentially to high spatial frequencies in humans and monkeys</article-title>. <source>PLoS Biol</source>. <year>2011</year>;<volume>9</volume>: <fpage>e1000608</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1000608" xlink:type="simple">10.1371/journal.pbio.1000608</ext-link></comment> <object-id pub-id-type="pmid">21483719</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nasr</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tootell</surname> <given-names>RBH</given-names></name>. <article-title>A cardinal orientation bias in scene-selective visual cortex</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>: <fpage>14921</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2036-12.2012" xlink:type="simple">10.1523/JNEUROSCI.2036-12.2012</ext-link></comment> <object-id pub-id-type="pmid">23100415</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watson</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Hartley</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Andrews</surname> <given-names>TJ</given-names></name>. <article-title>Patterns of response to visual scenes are linked to the low-level properties of the image</article-title>. <source>Neuroimage. Elsevier Inc.</source>; <year>2014</year>;<volume>99</volume>: <fpage>402</fpage>–<lpage>410</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2014.05.045" xlink:type="simple">10.1016/j.neuroimage.2014.05.045</ext-link></comment> <object-id pub-id-type="pmid">24862072</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kauffmann</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ramanoël</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Guyader</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Chauvin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Peyrin</surname> <given-names>C</given-names></name>. <article-title>Spatial frequency processing in scene-selective cortical regions</article-title>. <source>Neuroimage. Elsevier Inc.</source>; <year>2015</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2015.02.058" xlink:type="simple">10.1016/j.neuroimage.2015.02.058</ext-link></comment> <object-id pub-id-type="pmid">25754068</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lowe</surname> <given-names>MX</given-names></name>, <name name-style="western"><surname>Gallivan</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Ferber</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cant</surname> <given-names>JS</given-names></name>. <article-title>Feature diagnosticity and task context shape activity in human scene-selective cortex</article-title>. <source>Neuroimage. Elsevier Inc.</source>; <year>2016</year>;<volume>125</volume>: <fpage>681</fpage>–<lpage>692</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2015.10.089" xlink:type="simple">10.1016/j.neuroimage.2015.10.089</ext-link></comment> <object-id pub-id-type="pmid">26541082</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arcaro</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>McMains</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Singer</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>Kastner</surname> <given-names>S</given-names></name>. <article-title>Retinotopic organization of human ventral visual cortex</article-title>. <source>J Neurosci</source>. <year>2009</year>;<volume>29</volume>: <fpage>10638</fpage>–<lpage>10652</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2807-09.2009" xlink:type="simple">10.1523/JNEUROSCI.2807-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19710316</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Levy</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hasson</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Avidan</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Hendler</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Malach</surname> <given-names>R</given-names></name>. <article-title>Center-periphery organization of human object areas</article-title>. <source>Nat Neurosci</source>. <year>2001</year>;<volume>4</volume>: <fpage>533</fpage>–<lpage>539</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/87490" xlink:type="simple">10.1038/87490</ext-link></comment> <object-id pub-id-type="pmid">11319563</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Silson</surname> <given-names>EH</given-names></name>, <name name-style="western"><surname>Chan</surname> <given-names>AW-Y</given-names></name>, <name name-style="western"><surname>Reynolds</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Kravitz</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CI</given-names></name>. <article-title>A Retinotopic Basis for the Division of High-Level Scene Processing between Lateral and Ventral Human Occipitotemporal Cortex</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>: <fpage>11921</fpage>–<lpage>11935</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0137-15.2015" xlink:type="simple">10.1523/JNEUROSCI.0137-15.2015</ext-link></comment> <object-id pub-id-type="pmid">26311774</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Silson</surname> <given-names>EH</given-names></name>, <name name-style="western"><surname>Groen</surname> <given-names>IIA</given-names></name>, <name name-style="western"><surname>Kravitz</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CI</given-names></name>. <article-title>Evaluating the correspondence between face-, scene-, and object-selectivity and retinotopic organization within lateral occipitotemporal cortex</article-title>. <source>J Vis</source>. <year>2016</year>;<volume>16</volume>: <fpage>1</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/16.6.14" xlink:type="simple">10.1167/16.6.14</ext-link></comment> <object-id pub-id-type="pmid">27105060</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Groen</surname> <given-names>IIA</given-names></name>, <name name-style="western"><surname>Silson</surname> <given-names>EH</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CI</given-names></name>. <article-title>Contributions of low- and high-level properties to neural processing of visual scenes in the human brain</article-title>. <source>Philos Trans R Soc B</source>. <year>2017</year>;<volume>372</volume>: <fpage>1</fpage>–<lpage>11</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2016.0102" xlink:type="simple">10.1098/rstb.2016.0102</ext-link></comment> <object-id pub-id-type="pmid">28044013</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Güçlütürk</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Güçlü</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>van Gerven</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>van Lier</surname> <given-names>R</given-names></name>. <article-title>Representations of naturalistic stimulus complexity in early and associative visual and auditory cortices</article-title>. <source>Sci Rep</source>. <year>2018</year>;<volume>8</volume>: <fpage>3439</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-018-21636-y" xlink:type="simple">10.1038/s41598-018-21636-y</ext-link></comment> <object-id pub-id-type="pmid">29467495</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Malcolm</surname> <given-names>GL</given-names></name>, <name name-style="western"><surname>Groen</surname> <given-names>IIA</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CI</given-names></name>. <article-title>Making sense of real-world scenes</article-title>. <source>Trends Cogn Sci</source>. <year>2016</year>;<volume>20</volume>: <fpage>843</fpage>–<lpage>856</lpage>. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2016.09.003" xlink:type="simple">http://dx.doi.org/10.1016/j.tics.2016.09.003</ext-link> <object-id pub-id-type="pmid">27769727</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peelen</surname> <given-names>M V</given-names></name>, <name name-style="western"><surname>Kastner</surname> <given-names>S</given-names></name>. <article-title>Attention in the real world: toward understanding its neural basis</article-title>. <source>Trends Cogn Sci. Elsevier Ltd</source>; <year>2014</year>; <fpage>1</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2014.02.004" xlink:type="simple">10.1016/j.tics.2014.02.004</ext-link></comment> <object-id pub-id-type="pmid">24630872</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kaiser</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Oosterhof</surname> <given-names>NN</given-names></name>, <name name-style="western"><surname>Peelen</surname> <given-names>M V</given-names></name>. <article-title>The Neural Dynamics of Attentional Selection in Natural Scenes</article-title>. <source>J Neurosci</source>. <year>2016</year>;<volume>36</volume>: <fpage>10522</fpage>–<lpage>10528</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1385-16.2016" xlink:type="simple">10.1523/JNEUROSCI.1385-16.2016</ext-link></comment> <object-id pub-id-type="pmid">27733605</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Malcolm</surname> <given-names>GL</given-names></name>, <name name-style="western"><surname>Henderson</surname> <given-names>JM</given-names></name>. <article-title>The effects of target template specificity on visual search in real-world scenes: Evidence from eye movements</article-title>. <source>J Vis</source>. <year>2009</year>;<volume>9</volume>: <fpage>1</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/9.11.8" xlink:type="simple">10.1167/9.11.8</ext-link></comment> <object-id pub-id-type="pmid">20053071</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref095"><label>95</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Groen</surname> <given-names>IIA</given-names></name>, <name name-style="western"><surname>Ghebreab</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>. <article-title>The role of Weibull statistics in rapid object detection in natural scenes</article-title>. <source>J Vis</source>. <year>2010</year>;<volume>10</volume>: <fpage>992</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/10.7.992" xlink:type="simple">10.1167/10.7.992</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006690.ref096"><label>96</label><mixed-citation publication-type="other" xlink:type="simple">Jegou H, Douze M, Schmid C. Hamming embedding and weak geometric consistency for large scale image search. Proceedings of the 10th European conference on Computer Vision. 2008.</mixed-citation></ref>
<ref id="pcbi.1006690.ref097"><label>97</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Opelt</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pinz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fussenegger</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Auer</surname> <given-names>P</given-names></name>. <article-title>Generic object recognition with boosting</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2004</year>;<volume>28</volume>: <fpage>416</fpage>–<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TPAMI.2006.54" xlink:type="simple">10.1109/TPAMI.2006.54</ext-link></comment> <object-id pub-id-type="pmid">16526427</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref098"><label>98</label><mixed-citation publication-type="other" xlink:type="simple">Deng J, Dong W, Socher R, Li L-J, Li K, Fei-Fei L. ImageNet: A large-scale hierarchical image database. 2009 IEEE Conf Comput Vis Pattern Recognit. Ieee; 2009; 248–255.</mixed-citation></ref>
<ref id="pcbi.1006690.ref099"><label>99</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olmos</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kingdom</surname> <given-names>FAA</given-names></name>. <article-title>A biologically inspired algorithm for the recovery of shading and reflectance images</article-title>. <source>Perception</source>. <year>2004</year>;<volume>33</volume>: <fpage>1463</fpage>–<lpage>1473</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1068/p5321" xlink:type="simple">10.1068/p5321</ext-link></comment> <object-id pub-id-type="pmid">15729913</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref100"><label>100</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jahfari</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ridderinkhof</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>. <article-title>Spatial frequency information modulates response inhibition and decision-making processes</article-title>. <source>PLoS One</source>. <year>2013</year>;<volume>8</volume>: <fpage>e76467</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0076467" xlink:type="simple">10.1371/journal.pone.0076467</ext-link></comment> <object-id pub-id-type="pmid">24204630</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref101"><label>101</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jahfari</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Waldorp</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>van den Wildenberg</surname> <given-names>WPM</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Ridderinkhof</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Forstmann</surname> <given-names>BU</given-names></name>. <article-title>Effective connectivity reveals important roles for both the hyperdirect (fronto-subthalamic) and the indirect (fronto-striatal-pallidal) fronto-basal ganglia pathways during response inhibition</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>: <fpage>6891</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5253-10.2011" xlink:type="simple">10.1523/JNEUROSCI.5253-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21543619</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref102"><label>102</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jenkinson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bannister</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Brady</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>S</given-names></name>. <article-title>Improved optimisation for the robust and accurate linear registration and motion correction of brain images</article-title>. <source>Neuroimage</source>. <year>2002</year>;<volume>17</volume>: <fpage>825</fpage>–<lpage>841</lpage>. <object-id pub-id-type="pmid">12377157</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref103"><label>103</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Woolrich</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Ripley</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>Brady</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>SM</given-names></name>. <article-title>Temporal autocorrelation in univariate linear modeling of fMRI data</article-title>. <source>Neuroimage</source>. <year>2001</year>;<volume>14</volume>: <fpage>1370</fpage>–<lpage>1386</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1006/nimg.2001.0931" xlink:type="simple">10.1006/nimg.2001.0931</ext-link></comment> <object-id pub-id-type="pmid">11707093</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref104"><label>104</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sidak</surname> <given-names>Z</given-names></name>. <article-title>Rectangular Confidence Regions for the Means of Multivariate Normal Distributions</article-title>. <source>J Am Stat Assoc</source>. <year>1967</year>;<volume>62</volume>: <fpage>626</fpage>–<lpage>633</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref105"><label>105</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ludbrook</surname> <given-names>J</given-names></name>. <article-title>On Making Multiple Comparisons in Clinical and Experimental Pharmacology and Physiology</article-title>. <source>Clin Exp Pharmacol Physiol</source>. <year>1991</year>;<volume>18</volume>: <fpage>379</fpage>–<lpage>392</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1440-1681.1991.tb01468.x" xlink:type="simple">10.1111/j.1440-1681.1991.tb01468.x</ext-link></comment> <object-id pub-id-type="pmid">1914240</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref106"><label>106</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abdi</surname> <given-names>H</given-names></name>. <article-title>The Bonferonni and Šidák corrections for multiple comparisons</article-title>. <source>Encyclopedia of Measurement and Statistics</source>. <year>2007</year>. pp. <fpage>103</fpage>–<lpage>107</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref107"><label>107</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Woolrich</surname> <given-names>MW</given-names></name>. <article-title>Robust group analysis using outlier inference</article-title>. <source>Neuroimage</source>. <year>2008</year>;<volume>41</volume>: <fpage>286</fpage>–<lpage>301</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2008.02.042" xlink:type="simple">10.1016/j.neuroimage.2008.02.042</ext-link></comment> <object-id pub-id-type="pmid">18407525</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref108"><label>108</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Worsley</surname> <given-names>KJ</given-names></name>. <chapter-title>Statistical analysis of activation images</chapter-title>. In: <name name-style="western"><surname>Jezzard</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Matthews</surname> <given-names>PM</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>SM</given-names></name>, editors. <source>Functional MRI: An introduction to methods</source>. <publisher-name>Oxford University Press</publisher-name>; <year>2001</year>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref109"><label>109</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eickhoff</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Mohlberg</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Grefkes</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fink</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Amunts</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data</article-title>. <source>Neuroimage</source>. <year>2005</year>;<volume>25</volume>: <fpage>1325</fpage>–<lpage>35</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2004.12.034" xlink:type="simple">10.1016/j.neuroimage.2004.12.034</ext-link></comment> <object-id pub-id-type="pmid">15850749</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref110"><label>110</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perrin</surname> <given-names>F</given-names></name>. <article-title>Spherical splines for scalp potential and current density mapping</article-title>. <source>Electroencephalogr Clin Neurophysiol</source>. <year>1989</year>;<volume>72</volume>: <fpage>184</fpage>–<lpage>187</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0013-4694(89)90180-6" xlink:type="simple">10.1016/0013-4694(89)90180-6</ext-link></comment> <object-id pub-id-type="pmid">2464490</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref111"><label>111</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ratcliff</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Tuerlinckx</surname> <given-names>F</given-names></name>. <article-title>Estimating parameters of the diffusion model: approaches to dealing with contaminant reaction times and parameter variability</article-title>. <source>Psychon Bull Rev</source>. <year>2002</year>;<volume>9</volume>: <fpage>438</fpage>–<lpage>481</lpage>. <object-id pub-id-type="pmid">12412886</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref112"><label>112</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Ravenzwaaij</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Oberauer</surname> <given-names>K</given-names></name>. <article-title>How to use the diffusion model: parameter recovery of three methods: {EZ}, fast-dm, and {DMAT}</article-title>. <source>J Math Psychol</source>. <year>2009</year>;<volume>53</volume>: <fpage>463</fpage>–<lpage>473</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006690.ref113"><label>113</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Forstmann</surname> <given-names>BU</given-names></name>, <name name-style="western"><surname>Dutilh</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Neumann</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>von Cramon</surname> <given-names>DY</given-names></name>, <name name-style="western"><surname>Ridderinkhof</surname> <given-names>KR</given-names></name>, <etal>et al</etal>. <article-title>Striatum and pre-SMA facilitate decision-making under time pressure</article-title>. <source>Proc Natl Acad Sci</source>. <year>2008</year>;<volume>105</volume>: <fpage>17538</fpage>–<lpage>17542</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0805903105" xlink:type="simple">10.1073/pnas.0805903105</ext-link></comment> <object-id pub-id-type="pmid">18981414</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref114"><label>114</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mulder</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Bos</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Weusten</surname> <given-names>JMH</given-names></name>, <name name-style="western"><surname>van Belle</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>van Dijk</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Simen</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Basic impairments in regulating the speed-accuracy tradeoff predict symptoms of attention-deficit/hyperactivity disorder</article-title>. <source>Biol Psychiatry</source>. <year>2010</year>;<volume>68</volume>: <fpage>1114</fpage>–<lpage>1119</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.biopsych.2010.07.031" xlink:type="simple">10.1016/j.biopsych.2010.07.031</ext-link></comment> <object-id pub-id-type="pmid">20926067</object-id></mixed-citation></ref>
<ref id="pcbi.1006690.ref115"><label>115</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Carlin</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Stern</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>DB</given-names></name>. <source>Bayesian data analysis</source>. <publisher-name>Chapman &amp; Hall/CRC</publisher-name>; <year>2003</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>