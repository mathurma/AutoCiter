<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006122</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-01689</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Behavioral ecology</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Behavioral ecology</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Ecology</subject><subj-group><subject>Behavioral ecology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Ecology and environmental sciences</subject><subj-group><subject>Ecology</subject><subj-group><subject>Behavioral ecology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Experimental organism systems</subject><subj-group><subject>Model organisms</subject><subj-group><subject>Caenorhabditis elegans</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Model organisms</subject><subj-group><subject>Caenorhabditis elegans</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Experimental organism systems</subject><subj-group><subject>Animal models</subject><subj-group><subject>Caenorhabditis elegans</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Nematoda</subject><subj-group><subject>Caenorhabditis</subject><subj-group><subject>Caenorhabditis elegans</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal migration</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal migration</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Statistical distributions</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Identification of animal behavioral strategies by inverse reinforcement learning</article-title>
<alt-title alt-title-type="running-head">Inverse reinforcement learning and behavioral strategies</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Yamaguchi</surname>
<given-names>Shoichiro</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6816-9126</contrib-id>
<name name-style="western">
<surname>Naoki</surname>
<given-names>Honda</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ikeda</surname>
<given-names>Muneki</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Tsukada</surname>
<given-names>Yuki</given-names>
</name>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Nakano</surname>
<given-names>Shunji</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Mori</surname>
<given-names>Ikue</given-names>
</name>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ishii</surname>
<given-names>Shin</given-names>
</name>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Integrated Systems Biology Laboratory, Graduate School of Informatics, Kyoto University, Sakyo, Kyoto, Japan</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Laboratory of Theoretical Biology, Graduate School of Biostudies, Kyoto University, Yoshidakonoecho, Sakyo, Kyoto, Japan</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Data-driven Modeling Team, Research Center for Dynamic Living Systems, Graduate School of Biostudies, Kyoto University, Yoshidakonoecho, Sakyo, Kyoto, Japan</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Group of Molecular Neurobiology, Graduate School of Science, Nagoya University, Furoucho, Chikusa, Nagoya, Aichi, Japan</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Brown</surname>
<given-names>Andre</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>UNITED KINGDOM</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">honda.naoki.4v@kyoto-u.ac.jp</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>2</day>
<month>5</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>5</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>5</issue>
<elocation-id>e1006122</elocation-id>
<history>
<date date-type="received">
<day>10</day>
<month>10</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>3</day>
<month>4</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Yamaguchi et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006122"/>
<abstract>
<p>Animals are able to reach a desired state in an environment by controlling various behavioral patterns. Identification of the behavioral strategy used for this control is important for understanding animals’ decision-making and is fundamental to dissect information processing done by the nervous system. However, methods for quantifying such behavioral strategies have not been fully established. In this study, we developed an inverse reinforcement-learning (IRL) framework to identify an animal’s behavioral strategy from behavioral time-series data. We applied this framework to <italic>C</italic>. <italic>elegans</italic> thermotactic behavior; after cultivation at a constant temperature with or without food, fed worms prefer, while starved worms avoid the cultivation temperature on a thermal gradient. Our IRL approach revealed that the fed worms used both the absolute temperature and its temporal derivative and that their behavior involved two strategies: directed migration (DM) and isothermal migration (IM). With DM, worms efficiently reached specific temperatures, which explains their thermotactic behavior when fed. With IM, worms moved along a constant temperature, which reflects isothermal tracking, well-observed in previous studies. In contrast to fed animals, starved worms escaped the cultivation temperature using only the absolute, but not the temporal derivative of temperature. We also investigated the neural basis underlying these strategies, by applying our method to thermosensory neuron-deficient worms. Thus, our IRL-based approach is useful in identifying animal strategies from behavioral time-series data and could be applied to a wide range of behavioral studies, including decision-making, in other organisms.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Understanding animal decision-making has been a fundamental problem in neuroscience and behavioral ecology. Many studies have analyzed the actions representing decision-making in behavioral tasks, in which rewards are artificially designed with specific objectives. However, it is impossible to extend this artificially designed experiment to a natural environment, as in the latter, the rewards for freely-behaving animals cannot be clearly defined. To this end, we sought to reverse the current paradigm so that rewards could be identified from behavioral data. Here, we propose a new reverse-engineering approach (inverse reinforcement learning), which can estimate a behavioral strategy from time-series data of freely-behaving animals. By applying this technique on <italic>C</italic>. <italic>elegans</italic> thermotaxis, we successfully identified the respective reward-based behavioral strategy.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Japan Society for the Promotion of Science (JP)</institution>
</funding-source>
<award-id>16K16147</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6816-9126</contrib-id>
<name name-style="western">
<surname>Naoki</surname>
<given-names>Honda</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This research was mainly supported by Grant-in-Aids for Young Scientists (B) (No. 16K16147) from the Ministry of Education, Culture, Sports, Science and Technology (MEXT), Japan (author HN). It was also supported partially by the Platform Project for Supporting in Drug Discovery and Life Science Research (Platform for Dynamic Approaches to Living System) (authors HN and SI) from the Japan Agency for Medical Research and Development (AMED), the Brain Mapping by Integrated Neurotechnologies for Disease Studies (Brain/MINDS) (author SI) from AMED and the Strategic Research Program for Brain Sciences (authors HN, SN, YT, IM, and SI) from MEXT. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="0"/>
<page-count count="20"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-05-14</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Animals develop behavioral strategies, a set of sequential decisions necessary for organizing appropriate actions in response to environmental stimuli, to ensure their survival and reproduction. Such strategies lead animals to their preferred states and provide them with effective solutions to overcome difficulties in a given environment. For example, foraging animals are known to optimize their strategy to most efficiently exploit food sources [<xref ref-type="bibr" rid="pcbi.1006122.ref001">1</xref>]. Therefore, understanding behavioral strategies of biological organisms is important from biological, ethological, and engineering point of views.</p>
<p>A number of studies have recorded the behavioral sequences reflecting the overall animal strategies. However, mechanistic descriptions are different from phenomenological descriptions of recorded behaviors [<xref ref-type="bibr" rid="pcbi.1006122.ref002">2</xref>], and there is no well-established method that can objectively identify behavioral strategies, a mechanistic component of behavior. From a theoretical viewpoint, this mechanistic component corresponds to an algorithmic/representational level of understanding of information processing systems [<xref ref-type="bibr" rid="pcbi.1006122.ref003">3</xref>]. To derive behavioral strategies from quantitative time-series behavioral data, we propose a new computational framework based on the concept of reinforcement learning (RL).</p>
<p>RL is a mathematical paradigm that represents how animals adapt their behavior to maximize cumulative rewards via trial and error [<xref ref-type="bibr" rid="pcbi.1006122.ref004">4</xref>] (blue arrow in <bold><xref ref-type="fig" rid="pcbi.1006122.g001">Fig 1A</xref></bold>). A previous study indicated that dopamine activity reflects the reward prediction error [<xref ref-type="bibr" rid="pcbi.1006122.ref005">5</xref>], similar to temporal difference learning in RL [<xref ref-type="bibr" rid="pcbi.1006122.ref006">6</xref>], suggesting that RL-based regulation underlies animal’s behavioral learning. Even in the simple neural circuits of <italic>Caenorhabditis elegans</italic> (<italic>C</italic>. <italic>elegans</italic>), dopamine-dependent activity, involved in explorative behavior, is reminiscent of RL [<xref ref-type="bibr" rid="pcbi.1006122.ref007">7</xref>]. Thus some behavioral strategies are likely associated with the reward system.</p>
<fig id="pcbi.1006122.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006122.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Concept and procedure of the inverse reinforcement learning (IRL)-based approach.</title>
<p><bold>(A)</bold> Reinforcement learning represents a forward problem, in which a behavioral strategy is optimized to maximize the cumulative reward given as a series of states and rewards. IRL represents an inverse problem, in which a behavioral strategy, or its underlying value and reward functions, is estimated in order to reproduce an observed series of behaviors. The behavioral strategy is evaluated by the profiles of the identified functions. <bold>(B)</bold> Examples of passive and controlled dynamics. An animal migrates upwards, while the food (reward) is placed to its right. In this situation, if the animal continues to migrate upwards, the distance to the food increases. If the animal exercises harder body control, that is, changes its migrating direction towards the food, the distance to the food decreases. The animal should therefore make decisions based on balancing these two dynamics. <bold>(C)</bold> The agent-environment interaction. The agent autonomously acts in the environment, observes the resultant state-transition through its sensory system, and receives not only a state reward but also a body control cost. The behavioral strategy is optimized to maximize the accumulation of the net reward, which is given as the state reward minus the body control cost. <bold>(D)</bold> IRL framework for the identification of animal behavioral strategies. If a certain behavioral strategy is under investigation, a behavioral experiment is initially performed (<bold>phase 1</bold>), which can either involve a free-movement task or a conditional task. Subsequently, the states and passive dynamics, based on which the animal develops its strategy, are selected and modelled (<bold>phase 2 and 3</bold>). For these phases, prior knowledge on the type of sensory information an animal processes is useful for appropriately selecting the states and passive dynamics. <bold>Phases 4 and 5</bold> involve the quantification of the time-series of the selected states and the implementation of the linearly-solvable Markov decision process-based IRL, respectively, in order to estimate the value function. The behavioral strategy can be then identified.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006122.g001" xlink:type="simple"/>
</fig>
<p>Inverse reinforcement learning (IRL) is a recently developed machine-learning framework that can solve the inverse problem of RL (blue arrow in <bold><xref ref-type="fig" rid="pcbi.1006122.g001">Fig 1A</xref></bold>) and estimate reward-based strategies from behavioral time-series data [<xref ref-type="bibr" rid="pcbi.1006122.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006122.ref009">9</xref>]. One engineering application of IRL is apprenticeship learning. For example, seminal studies on IRL employed a radio-controlled helicopter, for which the state-dependent rewards of an expert were estimated by using time-series observations of both a human expert’s manipulation and the helicopter’s state. Consequently, autonomous control of the helicopter was achieved by (forward) RL, by utilizing the estimated rewards [<xref ref-type="bibr" rid="pcbi.1006122.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1006122.ref011">11</xref>]. This engineering application prompted studies on animal behavioral strategies by using IRL. Recently, IRL application studies have emerged, mostly regarding human behavior, with a particular interest in constructing artificially intelligent systems that mimic such behavior [<xref ref-type="bibr" rid="pcbi.1006122.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1006122.ref015">15</xref>]. In these studies, the behavioral tasks were designed with specific objectives, thus the observed behavioral strategies were usually expected. However, IRL applications involving freely behaving animals, in a more natural environment, are far from established.</p>
<p>In an effort to apply IRL to freely behaving animals, we chose thermotaxis in <italic>C</italic>. <italic>elegans</italic> as a model for a behavior that is regulated by specific strategies. When worms are cultivated at a constant temperature with plenty of food and then placed on a thermal gradient without food, they show an appetitive response to the cultivation temperature [<xref ref-type="bibr" rid="pcbi.1006122.ref016">16</xref>,<xref ref-type="bibr" rid="pcbi.1006122.ref017">17</xref>]. In contrast, if they are first cultivated at a constant temperature without food and then transferred on the thermal gradient, they show an aversive behavior towards the cultivation temperature [<xref ref-type="bibr" rid="pcbi.1006122.ref018">18</xref>,<xref ref-type="bibr" rid="pcbi.1006122.ref019">19</xref>]. Although the worms are not aware of the spatial temperature profile or their current location, it is obvious that they somehow make rational decisions, depending on their feeding status. Although there are multiple potential strategies that can theoretically lead animals to their goals, the actual ones they utilize in each condition are largely unknown due to the stochastic nature of behavioral sequences, which conceals the principles of behavioral regulation, as in the case of many other animal behaviors.</p>
<p>In this study, we developed a new IRL framework to identify the behavioral strategy as a value function. The value function represented the benefit of each state, namely, how much future rewards were expected starting from a given state. By applying this IRL framework to time-series behavioral data of freely migrating <italic>C</italic>. <italic>elegans</italic>, we identified the value functions underlying thermotactic strategies. Fed animals behaved based on sensory information of both the absolute and temporal derivative of temperature, and their behavior involved two modes; directed migration (DM) towards the cultivation temperature and isothermal migration (IM) along contour at constant temperature. Starved worms, in contrast, used only the absolute temperature but not its temporal derivative for escaping the cultivation temperature. By further applying the IRL to thermosensory neuron-impaired worms, we found that the so-called “AFD” neurons are fundamental for the DM exhibited by the fed worms. Thus, our framework can reveal the most preferable/optimal state for the animals and, more importantly, how animals reach that state, thereby providing clues for understanding the computational principles in the nervous system.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>IRL framework</title>
<p>To identify animal behavioral strategies based on IRL, we initially made the assumption that they are the result of the balance between two factors: passive dynamics (blue worm in <bold><xref ref-type="fig" rid="pcbi.1006122.g001">Fig 1B</xref></bold>) and reward-maximizing dynamics (red worm in <bold><xref ref-type="fig" rid="pcbi.1006122.g001">Fig 1B</xref></bold>), which correspond to inertia-based and purpose-driven body movements, respectively. For example, even if a worm moving in a straight line wants to make a purpose-driven turn towards a reward, it cannot turn suddenly, due to the inertia of its already moving body. Thus, it is reasonable to consider that the animal’s behavior is optimized by taking the above two factors into account, i.e., by minimizing the resistance to passive dynamics and maximizing approach to the destination (reward). Such a behavioral strategy has recently been modeled by using a linearly-solvable Markov decision process (LMDP) [<xref ref-type="bibr" rid="pcbi.1006122.ref020">20</xref>], in which the agent requires not only a state-dependent reward, but also a control cost for quantifying resistance to passive dynamics (<bold><xref ref-type="fig" rid="pcbi.1006122.g001">Fig 1C</xref></bold>). Importantly, the optimal strategy in the LMDP is analytically obtained as the probability of controlled state transition [<xref ref-type="bibr" rid="pcbi.1006122.ref020">20</xref>]:
<disp-formula id="pcbi.1006122.e001">
<alternatives>
<graphic id="pcbi.1006122.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006122.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <italic>s</italic><sub><italic>t</italic></sub> indicates the animal’s state at time step <italic>t</italic>; <italic>v</italic>(<italic>s</italic>) is the value function and is defined as the expected sum of state-dependent rewards, <italic>r</italic>(<italic>s</italic>), and negative control cost, <italic>KL</italic>[<italic>π</italic>(⋅|<italic>s</italic>)||<italic>p</italic>(⋅|<italic>s</italic>)], from state <italic>s</italic> towards the future; and <italic>P</italic>(<italic>s</italic><sub><italic>t+</italic>1</sub>|<italic>s</italic><sub><italic>t</italic></sub>) represents the probability of uncontrolled state transition, indicating the passive dynamics from <italic>s</italic><sub><italic>t</italic></sub> to <italic>s</italic><sub><italic>t+</italic>1</sub>. In this equation, the entire set of <italic>v</italic>(<italic>s</italic>) represents the behavioral strategy. Thus, the identification of a behavioral strategy is equivalent to the estimation of the value function <italic>v</italic>(<italic>s</italic>), based on the observed behavioral data (<italic>s</italic><sub><italic>1</italic></sub>, <italic>s</italic><sub><italic>2</italic></sub>,…<italic>s</italic><sub><italic>t</italic></sub>,…<italic>s</italic><sub><italic>T</italic></sub>; red arrow in <bold><xref ref-type="fig" rid="pcbi.1006122.g001">Fig 1A</xref></bold>). For this purpose, we used the maximum likelihood estimation (MLE) method [<xref ref-type="bibr" rid="pcbi.1006122.ref021">21</xref>]. Notably, in this estimation, we introduced a constraint to make the value function smooth, since animals generate similar actions in similar states. This constraint was essential to stably estimate the behavioral strategy of animals. The different phases of the IRL framework are depicted in the flowchart of <bold><xref ref-type="fig" rid="pcbi.1006122.g001">Fig 1D</xref></bold>. Following this flowchart, we applied the IRL framework to freely-migrating <italic>C</italic>. <italic>elegans</italic> under a thermal gradient.</p>
</sec>
<sec id="sec004">
<title>Phase 1: Monitoring animal behaviors</title>
<p>To identify the behavioral strategy underlying the thermotactic behavior of <italic>C</italic>. <italic>elegans</italic>, we performed population thermotaxis assays, in which 80–150 worms, which had been cultivated at 20°C, were placed on the surface of an agar plate with controlled thermal gradients (<bold><xref ref-type="fig" rid="pcbi.1006122.g002">Fig 2A</xref></bold>). Since the rate of physical contact is low at this worm density, behavioral crosstalk was negligible. To collect behavioral data, we prepared three different thermal gradients of 14–20, 17–23, and 20–26°C, centered at 17, 20, and 23°C, respectively; we expected that the first gradient would encourage ascent up the gradient, the second movement around the center, and the third descent down the gradient. Indeed, the fed worms aggregated around the standard cultivation temperature (20°C) in all gradients (<bold><xref ref-type="fig" rid="pcbi.1006122.g002">Fig 2B</xref></bold>).</p>
<fig id="pcbi.1006122.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006122.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Thermotactic behavior in <italic>C</italic>. <italic>elegans</italic>.</title>
<p><bold>(A)</bold> Thermotaxis assays including a thermal gradient. In each assay, a linear temperature gradient was set along the agar surface, whose center was set at either 17, 20, or 23°C. At the onset of the assay, fed or starved worms were placed at the center of the agar surface. <bold>(B)</bold> Temporal changes in the spatial distribution of the fed worms under the 17°C-, 20°C- and 23°C-centered thermal gradients. <bold>(C)</bold> Passive dynamics of persistent migration on a linear thermal gradient. <bold>(D)</bold> Representative trajectories of worms extracted by the multi-worm tracking system (n = 33 in this panel). Different colors indicate individual worms. <bold>(E)</bold> Time series of the temperature and its derivative experienced by the migrating worms shown in C (colors correspond to those in D).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006122.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Phase 2: Selection of states</title>
<p>We first defined the worms’ state, signified by <italic>s</italic> in Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e001">1</xref>), taking into account that it should represent the sensory information that the worms process during thermotaxis. Previous studies have shown that thermosensory AFD neurons encode the temporal derivative of temperature [<xref ref-type="bibr" rid="pcbi.1006122.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006122.ref023">23</xref>]; therefore, we assumed that worms select appropriate actions based not only on temperature, but also on its temporal derivative. We thus represented state by a two-dimensional (2-D) sensory space: <italic>s</italic> = (<italic>T</italic>, <italic>dT</italic>), where <italic>T</italic> and <italic>dT</italic> denote temperature and its temporal derivative, respectively. This means that the value function in Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e001">1</xref>) represents a function of <italic>T</italic> and <italic>dT</italic>, i.e., <italic>v</italic>(<italic>s</italic>) = <italic>v</italic>(<italic>T</italic>, <italic>dT</italic>). Notably, we did not select the spatial coordinates on the assay plate for state, since the worms cannot recognize the spatial temperature profile or their current position on the plate.</p>
</sec>
<sec id="sec006">
<title>Phase 3: Modeling passive dynamics</title>
<p>Next, we defined passive dynamics, signified by <italic>P</italic>(<italic>s</italic><sub><italic>t+</italic>1</sub>|<italic>s</italic><sub><italic>t</italic></sub>) in Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e001">1</xref>). Passive dynamics are the result of state transitions as a consequence of uncontrolled behavior. We assumed that a worm likely migrates in a persistent direction, but in a sometimes fluctuating manner. During state transition in a short time interval, the local thermal gradient can be considered as linear (<bold><xref ref-type="fig" rid="pcbi.1006122.g002">Fig 2C</xref></bold>). Thus, we modelled the passive transition from state <italic>s</italic><sub><italic>t</italic></sub> = (<italic>T</italic><sub><italic>t</italic></sub>, <italic>dT</italic><sub><italic>t</italic></sub>), at time <italic>t</italic>, to the next state, <italic>s</italic><sub><italic>t+</italic>1</sub> = (<italic>T</italic><sub><italic>t+</italic>1</sub>, <italic>dT</italic><sub><italic>t+</italic>1</sub>), at time <italic>t</italic> + 1, where <italic>dT</italic><sub><italic>t+</italic>1</sub> maintains <italic>dT</italic><sub><italic>t</italic></sub> with noise perturbation, while <italic>T</italic><sub><italic>t+</italic>1</sub> is updated as <italic>T</italic><sub><italic>t</italic></sub><italic>+dT</italic><sub><italic>t</italic></sub> with noise perturbation. Accordingly, <italic>P</italic>(<italic>s</italic><sub><italic>t+</italic>1</sub>|<italic>s</italic><sub><italic>t</italic></sub>) was simply expressed by a normal distribution (please note the distinction between <italic>T</italic> and <italic>t</italic> throughout this paper).</p>
</sec>
<sec id="sec007">
<title>Phase 4: Quantification of state time-series</title>
<p>To quantify thermosensory states selected in phase 2, we tracked the trajectories of individual worms over 60 min within each gradient, by using a multi-worm tracking software [<xref ref-type="bibr" rid="pcbi.1006122.ref024">24</xref>] (<bold><xref ref-type="fig" rid="pcbi.1006122.g002">Fig 2D</xref></bold>). We then recorded the temperature that each individual worm experienced at each time-point (upper panel in <bold><xref ref-type="fig" rid="pcbi.1006122.g002">Fig 2E</xref></bold>) and calculated the temporal derivative of temperature by using a Savitzky-Golay filter [<xref ref-type="bibr" rid="pcbi.1006122.ref025">25</xref>] (lower panel in <bold><xref ref-type="fig" rid="pcbi.1006122.g002">Fig 2E</xref></bold>). State trajectories in the <italic>T</italic>-<italic>dT</italic> space were also plotted (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s002">S2A Fig</xref></bold>).</p>
</sec>
<sec id="sec008">
<title>Phase 5: Identification of behavioral strategy by IRL</title>
<p>Using the collected state time-series data, <italic>s</italic> = (<italic>T</italic>, <italic>dT</italic>), and passive dynamics, <italic>P</italic>(<italic>s</italic><sub><italic>t+</italic>1</sub>|<italic>s</italic><sub><italic>t</italic></sub>), we performed IRL, i.e., we estimated the value function, <italic>v</italic>(<italic>s</italic>). We modified an existing estimation method called OptV [<xref ref-type="bibr" rid="pcbi.1006122.ref021">21</xref>], by introducing a smoothness constraint, and confirmed that this constraint was indeed effective in accurately estimating the value function, when applied to artificial data simulated by Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e001">1</xref>) (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s001">S1 Fig</xref></bold>). Since this method could powerfully estimate a behavioral strategy based on artificial data, we next applied it to the behavioral data of the fed worms.</p>
<p>Our method successfully estimated the value function (<bold><xref ref-type="fig" rid="pcbi.1006122.g003">Fig 3A</xref></bold>) and visualized the desirability function, expressed by exp(<italic>v</italic>(<italic>T</italic>, <italic>dT</italic>)) [<xref ref-type="bibr" rid="pcbi.1006122.ref021">21</xref>] (<bold><xref ref-type="fig" rid="pcbi.1006122.g003">Fig 3B</xref></bold>). Furthermore, we could calculate the reward function from the identified desirability function using Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e009">8</xref>) (<bold><xref ref-type="fig" rid="pcbi.1006122.g003">Fig 3C</xref></bold>). The reward function primarily represents the worms’ preference, while the desirability function represents the behavioral strategy and is thus a result of optimizing the cumulative sum of rewards and negative control costs. Therefore, our method quantitatively clarified the behavioral strategy of fed <italic>C</italic>. <italic>elegans</italic>.</p>
<fig id="pcbi.1006122.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006122.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Behavioral strategy identified for fed WT worms.</title>
<p>The behavioral strategies of the fed WT worms, as represented by the value <bold>(A)</bold>, desirability <bold>(B)</bold>, and reward <bold>(C)</bold> functions. The worms prefer and avoid the red- and blue-colored states, respectively.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006122.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec009">
<title>Interpretation of the identified strategy</title>
<p>Since both the value and desirability functions essentially represented the same thermotactic strategy, we focus on the results only for the desirability function. We found that the identified desirability function peaked at <italic>T</italic> = 20°C and <italic>dT</italic> = 0°C/s, encouraging the worms to reach and stay close to the cultivation temperature. Moreover, we recognized both diagonal and horizontal components (<bold><xref ref-type="fig" rid="pcbi.1006122.g003">Fig 3B</xref></bold>), though the latter one was partially truncated by data limitation and data inhomogeneity (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s002">S2B Fig</xref></bold>). The diagonal component represented directed migration (DM), a strategy that enables worms to efficiently reach the cultivation temperature. At lower temperatures than the cultivation temperature a more positive <italic>dT</italic> is favored, whereas at higher temperatures a more negative <italic>dT</italic> is favored. This DM strategy is consistent with the observation that worms migrate toward the cultivation temperature, and also clarifies how they control their thermosensory state throughout migration. On the other hand, the horizontal component represented isothermal migration (IM), which explains a well-known characteristic of worms, called isothermal tracking; worms typically exhibit circular migration under a concentric thermal gradient [<xref ref-type="bibr" rid="pcbi.1006122.ref017">17</xref>]. Although we used a linear, not a concentric gradient in our thermotaxis assay, our IRL algorithm successfully extracted the isothermal tracking-related migration strategy, which worked both at the cultivation temperature and at other temperatures. The desirability function (<bold><xref ref-type="fig" rid="pcbi.1006122.g003">Fig 3B</xref></bold>) described the strategy of state transition (Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e001">1</xref>)), while the state distribution of <italic>T</italic> and <italic>dT</italic> (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s002">S2B Fig</xref></bold>) was an outcome of the strategy; therefore, the desirability function was not equivalent to the actual state distribution.</p>
<p>During thermotaxis, worms alternate between ‘runs’ and ‘sharp turns’, which correspond to persistent migration with slight changes in direction, during long intervals, and intermittent directional changes with large angle, during short intervals, respectively [<xref ref-type="bibr" rid="pcbi.1006122.ref026">26</xref>]. Because the number of data points obtained during the runs is much larger than those during the sharp turns in total, our IRL framework could recapitulate the strategy for shallow but not for sharp turns. Indeed, we could not find a relationship between the desirability function and the rate of sharp turns (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s002">S2C and S2D Fig</xref></bold>).</p>
</sec>
<sec id="sec010">
<title>Reliability of the identified strategy</title>
<p>We verified the reliability of the identified strategies with the following four ways. First, we examined the dimension of the strategy. We performed IRL based on a one-dimensional (1-D) state representation, i.e., <italic>s</italic> = (<italic>T</italic>). Comparing 1-D and 2-D cases, we used cross-validation to confirm that the prediction ability for a future state transition was significantly higher in the 2-D than in the 1-D behavioral strategy (<italic>p =</italic> 0.0002; Mann-Whitney U test) (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s003">S3 Fig</xref></bold>). This result indicates that fed worms utilized sensory information of both the absolute temperature and its temporal derivative for their behavioral strategy. Second, we confirmed that our IRL approach recapitulated the nature of thermotactic behaviors. We simulated temperature trajectories starting from 15, 20, and 25°C, by sampling the state transition from Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e001">1</xref>), using the identified value function. The simulated worm population converged around the cultivation temperature (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s004">S4 Fig</xref></bold>), showing that the identified strategy indeed represented the thermotactic property of the fed worms. Third, we statistically tested the identified DM and IM strategies. As a null hypothesis, we assumed that the worms randomly migrated under a thermal gradient with no behavioral strategy. By means of surrogate method-based statistical testing, we showed that the DM and IM strategies could not be obtained by chance, indicating that both strategies reflected an actual strategy of thermotaxis (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s005">S5 Fig</xref></bold>). Finally, we cross-checked the DM and IM strategies by repeating our IRL protocol on another <italic>C</italic>. <italic>elegans</italic> strain. To this end, we used worms in which the chemosensory ASI neurons were genetically ablated via cell-specific expression of caspases [<xref ref-type="bibr" rid="pcbi.1006122.ref027">27</xref>]. This ASI-deficient strain appeared to show normal thermotaxis (<bold><xref ref-type="fig" rid="pcbi.1006122.g004">Fig 4Aa</xref></bold>), suggesting that the ASI neurons were not responsible for thermotaxis in our assay. We found clear diagonal and horizontal components in the desirability function, supporting the existence of the DM and IM strategies (<bold><xref ref-type="fig" rid="pcbi.1006122.g004">Fig 4Ab</xref></bold>).</p>
<fig id="pcbi.1006122.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006122.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Inverse reinforcement learning analyses of ASI-, AWC-, and AFD-neuron deficient worms and starved worms.</title>
<p>Temporal changes in distributions of ASI-, AWC-, and AFD-neuron deficient worms, as well as of starved worms in the 17°C-, 20°C- and 23°C-centered thermal gradients after behavior onset are presented in column <bold>a</bold> of A, B, C, and D, respectively. The corresponding desirability functions are shown in column <bold>b</bold> of A, B, C, and D, respectively. Starved worms disperse under a thermal gradient, while ASI- and AWC-deficient worms migrate to the cultivation temperature, similarly to fed WT worms; AFD-deficient worms show cryophilic thermotaxis.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006122.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec011">
<title>Strategies of thermosensory neuron-deficient worms</title>
<p>To examine the role of the thermosensory circuit in the observed behavioral strategy, we created two worm strains in which one of the two types of thermosensory neurons, AWC or AFD, [<xref ref-type="bibr" rid="pcbi.1006122.ref016">16</xref>,<xref ref-type="bibr" rid="pcbi.1006122.ref017">17</xref>,<xref ref-type="bibr" rid="pcbi.1006122.ref028">28</xref>] had been genetically ablated via cell-specific expression of caspases. The AWC-deficient worms appeared to show normal thermotaxis (<bold><xref ref-type="fig" rid="pcbi.1006122.g004">Fig 4Ba</xref></bold>). The desirability function, obtained as for wild type (WT) animals (<bold><xref ref-type="fig" rid="pcbi.1006122.g004">Fig 4Bb</xref></bold>), suggested that AWC neurons did not play an essential role in thermotaxis. In contrast, AFD-deficient worms demonstrated cryophilic thermotaxis (<bold><xref ref-type="fig" rid="pcbi.1006122.g004">Fig 4Ca</xref></bold>). The desirability function consistently increased as temperature decreased (<bold><xref ref-type="fig" rid="pcbi.1006122.g004">Fig 4Cb</xref></bold>) but lacked the <italic>dT-</italic>dependent structure, indicating that the DM strategy observed in WT worms had disappeared. Moreover, the fact that AFD neurons encode the temporal derivative of temperature [<xref ref-type="bibr" rid="pcbi.1006122.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006122.ref023">23</xref>] further corroborates the loss of the <italic>dT-</italic>dependent structure. Thus, AFD-deficient worms inefficiently aimed for lower temperatures by a strategy primarily depending on the absolute temperature but not on its temporal derivative (<bold><xref ref-type="fig" rid="pcbi.1006122.g004">Fig 4Cb</xref></bold>). Taken together, these findings demonstrate that AFD and not AWC neurons are essential for efficiently navigating towards the desired/cultivation temperature.</p>
</sec>
<sec id="sec012">
<title>Strategy of starved worms</title>
<p>Further, we performed IRL on behavioral data from starved worms, which were cultivated at 20°C without food and then placed on the thermal gradient. The starved worms dispersed in the low-temperature region and avoided the high-temperature one (<bold><xref ref-type="fig" rid="pcbi.1006122.g004">Fig 4Da</xref></bold>). Regarding the desirability function, we found that, compared with the fed worms (<bold><xref ref-type="fig" rid="pcbi.1006122.g003">Fig 3B</xref></bold>), the diagonal structure was not present in the starved worms (<bold><xref ref-type="fig" rid="pcbi.1006122.g004">Fig 4Db</xref></bold>), suggesting that they did not use DM. In contrast, we could still observe IM (<bold><xref ref-type="fig" rid="pcbi.1006122.g005">Fig 5Ab</xref></bold>), indicating that the starved worms retained the ability to perform isothermal tracking. Most importantly, the desirability function was lower at the cultivation temperature than at surrounding temperatures, suggesting that, unlike the fed worms, the starved ones escaped the cultivation temperature region based on sensory information of only the absolute temperature, but not of its temporal derivative. These results indicate that our method could distinguish between strategies of normally fed and starved worms.</p>
<fig id="pcbi.1006122.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006122.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Possible strategies involved in preference and avoidance of the cultivation temperature.</title>
<p>Each panel represents the desirability function of a possible strategy (fed worms: <bold>A-C</bold>, starved worms: <bold>D-F</bold>). The prior knowledge that fed worms navigate to the cultivation temperature and starved worms escape the cultivation temperature suggests several possible strategies, but does not identify the actual strategy exhibited by the animals. The inverse reinforcement learning approach identified that the fed worms use the proportional-derivative (PD) control-like DM strategy shown in (A), while the starved worms use the proportional (P) control-like strategy shown in (D).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006122.g005" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec013" sec-type="conclusions">
<title>Discussion</title>
<p>In this study, we proposed an IRL framework to identify animal behavioral strategies based on collected behavioral time-series data. We validated the framework using artificial data, and then applied it to behavioral data collected during <italic>C</italic>. <italic>elegans</italic> thermotaxis experiments. We quantitatively identified the thermotactic strategies and discovered that fed worms use both the absolute temperature and its temporal derivative, whereas starved worms only use the absolute temperature. We then visualized the properties of this thermotactic strategy, by means of the desirability function, and successfully identified which states are pleasant and unpleasant for <italic>C</italic>. <italic>elegans</italic>. Finally, we demonstrated the ability of this technique to discriminate alterations in components within a strategy, by using it to compare the desirability functions of two strains of worms with impaired thermosensory neuron function; we found that AFD, but not AWC, neurons are fundamental for the worms to efficiently navigated towards the cultivation temperature.</p>
<sec id="sec014">
<title>Advantages of the IRL approach</title>
<p>Our approach has three advantages. First, it is generally applicable to behavioral data of any organism, not just <italic>C</italic>. <italic>elegans</italic>. Second, it can be applied independently of the experimental conditions. Our approach is especially suitable for analyzing behavior in natural conditions where target animals are behaving freely. To the best of our knowledge, this is the first study to identify the behavioral strategy of a freely-behaving animal by using IRL. Third, this approach estimates the strategy that generates natural behaviors, by introducing passive dynamics in the LMDP. Animal movements are usually restricted by external constraints, including inertia and gravity, as well as by internal (musculoskeletal) constraints; therefore, animals prefer entering a natural unrestricted state-transition. Thus, the LMDP-based IRL is suitable for modeling animal behavioral strategies. Although there are several studies on IRL application to human behaviors [<xref ref-type="bibr" rid="pcbi.1006122.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1006122.ref015">15</xref>], none of these have considered passive dynamics. Since high-throughput experiments produce massive amounts of behavioral data, our IRL approach could be a fundamental tool for their analysis, with applicability in behavioral sciences, in general, including ecology and ethology.</p>
</sec>
<sec id="sec015">
<title>Validity of the identified strategies</title>
<p>We applied our IRL approach to worms of different genetic backgrounds (WT and three mutant strains) and confirmed that the identified behavioral strategies undertaken by the animals, as expressed by the desirability function, showed no discrepancy in thermotactic behaviors. The fact that fed WT worms aggregated at the cultivation temperature, while starved WT worms dispersed around it can be explained by the increased and decreased amplitude, respectively, of the desirability function at the cultivation temperature. We found that ASI- and AWC-deficient worms exhibit normal thermotaxis, and their desirability functions were similar to that of WT animals. However, AFD-deficient worms demonstrate cryophilic thermotaxis, consistent with the increased amplitude of the desirability function at lower temperatures. Taken together, these results demonstrate the validity of our approach, as well as its potential to determine changes in behavioral strategies.</p>
</sec>
<sec id="sec016">
<title>Alternative behavioral strategies</title>
<p>Our approach provides novel insight into how the <italic>C</italic>. <italic>elegans</italic> reaches a target temperature on a thermal gradient. In theory, the strategy we identified is not the sole solution for the animals in order to reach the target state; several alternative solutions could have allowed animals to navigate to their behavioral goals. The strategies undertaken by fed or starved animals and the possible alternative ones are discussed below in terms of control theory [<xref ref-type="bibr" rid="pcbi.1006122.ref029">29</xref>].</p>
<p>In the case of the fed worms (<bold><xref ref-type="fig" rid="pcbi.1006122.g005">Fig 5A–5C</xref></bold>), several alternative strategies might have enabled the animals in their DM towards the goal (cultivation temperature). The DM strategy is shown in <bold><xref ref-type="fig" rid="pcbi.1006122.g005">Fig 5A</xref></bold>. <bold><xref ref-type="fig" rid="pcbi.1006122.g005">Fig 5B</xref></bold> shows the desirability function for worms switching their preference between a positive and a negative temperature gradient, lower or higher than the goal temperature, representing the so called “bang-bang control”. A previous computational study modeled <italic>C</italic>. <italic>elegans</italic> thermotaxis based on the bang-bang control [<xref ref-type="bibr" rid="pcbi.1006122.ref030">30</xref>], in which straight runs and random turnings (corresponding to omega and reversal turns) alternate, while the run duration is regulated by the temperature, its temporal derivative and the cultivation temperature. <bold><xref ref-type="fig" rid="pcbi.1006122.g005">Fig 5C</xref></bold> shows the resulting desirability function when worms simply prefer the goal temperature, regardless of its temporal derivative. This might be interpreted as “proportional (P) control”. However, the identified DM strategy is based on both the absolute temperature and its temporal derivative, suggesting that the worms in fact perform “proportional-derivative (PD) control”, which is more sophisticated than the bang-bang control.</p>
<p>Regarding the strategy of the starved worms, similar alternatives exist, as discussed above. The worms could escape the cultivation temperature by performing “bang-bang control” or “PD control”, as shown in <bold><xref ref-type="fig" rid="pcbi.1006122.g005">Fig 5E and 5F</xref></bold>. The identified starved strategy however is closer to “P control”, which only uses the absolute temperature. Our IRL-based approach is therefore able to clarify how the worms control their thermosensory state throughout migration, which was not understood until now.</p>
</sec>
<sec id="sec017">
<title>Functional significance of DM and IM strategies</title>
<p>We found that the WT worms use a thermotactic strategy consisting of two components; a diagonal, representing DM; and a horizontal, representing IM. What is the functional meaning of these two strategies? We propose that they might be necessary for balancing exploration and exploitation. Exploitation is the use of pre-acquired knowledge in an effort to obtain rewards, while exploration is the effort of searching for possible greater rewards. For example, worms know that food is associated with the cultivation temperature and can exploit this association. Alternatively, they can explore different temperatures to search for more food than that available at the cultivation temperature. In an uncertain environment, animals usually face an “exploration-exploitation dilemma” [<xref ref-type="bibr" rid="pcbi.1006122.ref031">31</xref>]; exploitative behaviors reduce the chance to explore for greater rewards, whereas exploratory behaviors disrupt the collection of the already-available reward. Therefore, an appropriate balance between exploration and exploitation is important for controlling behavioral strategies. We propose that DM and IM generate exploitative and explorative behaviors, respectively: the worms, via DM, exploit the cultivation temperature, and at the same time explore possible alternative rewards (food) in different temperatures through IM.</p>
<p>We found that in the case of starved worms, temperature and feeding are dissociated, and worms do not exhibit DM; instead they still exhibit IM. According to these findings, we hypothesize that DM emerges as a consequence of associative learning (association between the cultivation temperature and food access); the IM strategy, however, could be innate. Further investigation regarding these hypotheses should be expected in the future.</p>
<p>In the case of thermosensory neuron-deficient worms, we found that AWC-neuron ablation does not affect the desirability function, whereas AFD-neuron depletion abolishes the DM diagonal component, as well as any bias along the <italic>dT</italic> axis. The AWC and AFD neurons are both known to sense the temporal derivative of temperature, <italic>dT</italic> [<xref ref-type="bibr" rid="pcbi.1006122.ref016">16</xref>,<xref ref-type="bibr" rid="pcbi.1006122.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006122.ref023">23</xref>]. Thus, we can assume that AFD-neuron loss might prevent worms from deciding whether an increase or decrease in temperature is favorable, which could lead to inefficient thermotactic migration. Thus, the AFD, but not AWC neurons, are involved in the DM based on temporal changes in temperature.</p>
</sec>
<sec id="sec018">
<title>Future perspectives for neuroscience research</title>
<p>Finally, it is worth discussing future perspective of our IRL approach in neuroscience research focusing on higher-order animals beyond <italic>C</italic>. <italic>elegans</italic>. Over the last two decades, several reports have demonstrated that dopaminergic activity in the ventral tegmental area (VTA) encodes for reward prediction error [<xref ref-type="bibr" rid="pcbi.1006122.ref005">5</xref>], similar to temporal difference (TD) learning in RL [<xref ref-type="bibr" rid="pcbi.1006122.ref006">6</xref>], suggesting that animal behavioral strategies are associated with reward-based representation. In addition, it is widely believed that RL-like algorithms are processed within functionally connected cortical and subcortical areas, especially within the basal ganglia [<xref ref-type="bibr" rid="pcbi.1006122.ref032">32</xref>–<xref ref-type="bibr" rid="pcbi.1006122.ref035">35</xref>] and amygdala [<xref ref-type="bibr" rid="pcbi.1006122.ref036">36</xref>,<xref ref-type="bibr" rid="pcbi.1006122.ref037">37</xref>], brain areas that heavily innervated by VTA dopaminergic neurons. Recent advances in neural recording technology have enabled researchers to monitor the activity of neuronal populations related to the reward-based representation of a given strategy in freely-behaving animals. However, the actual rewards for freely-behaving animals, especially those internally-represented in the brain, rather than the primitive ones, like food, are difficult to recognize. Our study shows that the presented IRL framework can identify the reward-based representation of animal strategies, thus allowing the analysis of neural correlates, such as comparing neural activities in freely-behaving animals with strategy-related variables, calculated by using IRL. Therefore, a combination of neuroscience experiments and the IRL technology could contribute in discovering behavioral neural substrates and their computational principles.</p>
</sec>
</sec>
<sec id="sec019" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec020">
<title>Reinforcement learning</title>
<p>RL is a machine learning model that describes how agents learn to obtain an optimal policy, that is, a behavioral strategy, in a given environment [<xref ref-type="bibr" rid="pcbi.1006122.ref004">4</xref>]. RL consists of several components: an agent, an environment, and a reward function. The agent learns and makes decisions, and the environment is defined by everything else. The agent continuously interacts with the environment, in which the state of the agent changes based on its actions (behavior), and the agent gets a reward at the new state according to the reward function. The aim of the agent is to identify an optimal strategy (policy) that maximizes cumulative rewards in the long term.</p>
<p>In this study, the environment and the agent’s behavioral strategy were modeled as an LMDP, one of settings of RL [<xref ref-type="bibr" rid="pcbi.1006122.ref020">20</xref>]. The LMDP included the passive dynamics of the environment, in the absence of control, and the controlled dynamics that reflect a behavioral strategy. Passive and controlled dynamics were each defined by transition probabilities from state <italic>s</italic> to <italic>s</italic>’, namely, <italic>p</italic>(<italic>s</italic>’|<italic>s</italic>) and <italic>π</italic>(<italic>s</italic>’|<italic>s</italic>), respectively. In each state, the agent not only acquires a reward, but also receives resistance to passive dynamics (<bold><xref ref-type="fig" rid="pcbi.1006122.g001">Fig 1C</xref></bold>). Thus, the net reward is described as
<disp-formula id="pcbi.1006122.e002">
<alternatives>
<graphic id="pcbi.1006122.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006122.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mrow><mml:mi mathvariant="script">l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where <italic>r</italic>(<italic>s</italic>) denotes a state reward and <italic>KL</italic>[<italic>π</italic>(.|<italic>s</italic>)||<italic>p</italic>(.|<italic>s</italic>)] indicates the Kullback–Leibler (KL) divergence between <italic>π</italic>(.|<italic>s</italic>) and <italic>p</italic>(.|<italic>s</italic>), which represents the resistance to passive dynamics. The optimal policy that maximizes the cumulative net reward has been analytically obtained [<xref ref-type="bibr" rid="pcbi.1006122.ref020">20</xref>] as
<disp-formula id="pcbi.1006122.e003">
<alternatives>
<graphic id="pcbi.1006122.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006122.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>y</mml:mi></mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where the asterisk indicates optimal, and <italic>v</italic>(<italic>s</italic>) is the value function, i.e., the cumulative net reward expected from state <italic>s</italic> toward the future:
<disp-formula id="pcbi.1006122.e004">
<alternatives>
<graphic id="pcbi.1006122.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006122.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mi mathvariant="script">l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula></p>
<p>Here, we briefly show how to derive Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e003">3</xref>). First, the controlled dynamics were defined as
<disp-formula id="pcbi.1006122.e005">
<alternatives>
<graphic id="pcbi.1006122.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006122.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="bold">u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
where the elements <italic>u</italic><sub><italic>s</italic></sub> of a vector <bold>u</bold> directly modulate the transition probability of passive dynamics. Note that <italic>π</italic>(<italic>s</italic>’|<italic>s</italic>, <bold>0</bold>) = <italic>p</italic>(<italic>s’</italic>|<italic>s</italic>). Because of probability, Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e005">5</xref>) must satisfy
<disp-formula id="pcbi.1006122.e006">
<alternatives>
<graphic id="pcbi.1006122.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006122.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="bold">u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
The value function can be rewritten by the Bellman equation:
<disp-formula id="pcbi.1006122.e007">
<alternatives>
<graphic id="pcbi.1006122.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006122.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi mathvariant="script">l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="bold">u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
where <inline-formula id="pcbi.1006122.e008"><alternatives><graphic id="pcbi.1006122.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006122.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mi mathvariant="script">l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="script">l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="bold">u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The maximization in Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e007">7</xref>), subjected to Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e006">6</xref>) by the method of Lagrange multipliers, yields <bold>u</bold><sup><bold>*</bold></sup>, which represents the optimal strategy. Substituting <bold>u</bold><sup><bold>*</bold></sup> in Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e005">5</xref>) gives Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e003">3</xref>). In addition, substituting the optimal strategy [Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e003">3</xref>)] in the Bellman Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e007">7</xref>) and dropping the max operator lead to
<disp-formula id="pcbi.1006122.e009">
<alternatives>
<graphic id="pcbi.1006122.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006122.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:mrow><mml:mi mathvariant="normal">exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
which satisfies Bellman’s self-consistency. Using this equation, <italic>v</italic>(<italic>s</italic>) can be calculated from the reward function <italic>r</italic>(<italic>s</italic>), and vice versa. The full derivation is described in [<xref ref-type="bibr" rid="pcbi.1006122.ref020">20</xref>].</p>
</sec>
<sec id="sec021">
<title>Inverse reinforcement learning (estimation of the value function)</title>
<p>To estimate <italic>v</italic>(<italic>s</italic>), we assumed that the observed sequential state transitions {<italic>s</italic><sub><italic>t</italic></sub>, <italic>s</italic><sub><italic>t</italic>+1</sub>}<sub><italic>t</italic> = 1:<italic>T</italic></sub> are generated by the stationary optimal policy <italic>π</italic><sup><italic>*</italic></sup>. We then maximized the likelihood of the sequential state transition:
<disp-formula id="pcbi.1006122.e010">
<alternatives>
<graphic id="pcbi.1006122.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006122.e010" xlink:type="simple"/>
<mml:math display="block" id="M10">
<mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∏</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula>
where <italic>π</italic><sup><italic>*</italic></sup>(<italic>s</italic><sub><italic>t</italic>+1</sub>|<italic>s</italic><sub><italic>t</italic></sub>; <italic>v</italic>(<italic>s</italic>)) corresponds to Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e003">3</xref>). This estimation is called OptV [<xref ref-type="bibr" rid="pcbi.1006122.ref021">21</xref>]. Based on the estimated <italic>v</italic>(<italic>s</italic>), the primary reward function, <italic>r</italic>(<italic>s</italic>), can be calculated by using Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e009">8</xref>).</p>
<p>In our implementation, states were represented by a tabular format, in which 2-D space (temperature and its temporal derivative) was divided as a mesh grid. Thus, our IRL required a number of state trajectory data, spanning the entire mesh grid. In order to compensate for data limitation and noisy sensory systems, we assumed that animals have value functions that are smooth in their state space. To obtain smooth value functions, we regularized MLE as
<disp-formula id="pcbi.1006122.e011">
<alternatives>
<graphic id="pcbi.1006122.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006122.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:mrow><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">arg</mml:mi><mml:mspace width="0.15em"/><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">log</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mi>χ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
where the first term represents the log-likelihood and the second term represents a smoothness constraint introduced to the value function; a positive constant <italic>λ</italic> indicates the strength of the constraint, and <italic>χ</italic>(<italic>s</italic>) indicates a set of neighboring states of <italic>s</italic> in the state space. The evaluation function, i.e., the regularized log-likelihood, is convex with respect to <italic>v</italic>(<italic>s</italic>), which means there are no local minima in its optimization procedure.</p>
</sec>
<sec id="sec022">
<title>Passive dynamics of thermotaxis in <italic>C. elegans</italic></title>
<p>To apply the LMDP-based IRL to the thermotactic behaviors of <italic>C</italic>. <italic>elegans</italic>, state <italic>s</italic> and passive dynamics <italic>p</italic>(<italic>s</italic>’|<italic>s</italic>) were defined (phase 2 and 3 in <bold><xref ref-type="fig" rid="pcbi.1006122.g001">Fig 1D</xref></bold>). We previously found that the thermosensory AFD neurons encode the temporal derivative of the environmental temperature [<xref ref-type="bibr" rid="pcbi.1006122.ref022">22</xref>] and thus assumed that worms can sense not only the absolute temperature, <italic>T</italic>, but also its temporal derivative, <italic>dT/dt</italic>. We therefore set a 2-D state representation as (<italic>T</italic>, <italic>dT</italic>). For simplicity <italic>dT/dt</italic> is simply denoted as <italic>dT</italic>.</p>
<p>The passive dynamics were described by the transition probability of a state (<italic>T</italic>, <italic>dT</italic>) as
<disp-formula id="pcbi.1006122.e012">
<alternatives>
<graphic id="pcbi.1006122.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006122.e012" xlink:type="simple"/>
<mml:math display="block" id="M12">
<mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mi>T</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>T</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mi>T</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula>
where <italic>N</italic>(<italic>x</italic>|<italic>μ</italic>, σ) indicates a Gaussian distribution of variable <italic>x</italic> with mean <italic>μ</italic> and variance σ, and <italic>Δt</italic> indicates the time interval of monitoring during behavioral experiments. The passive-dynamics aspect can be loosely interpreted as if the worms inertially migrate in a short time interval under a thermal gradient, and may be perturbed by white noise. The distribution of passive dynamics can be arbitrary selected, and the choice of Gaussian was not due to mathematical necessity for the IRL.</p>
</sec>
<sec id="sec023">
<title>Artificial data</title>
<p>To confirm that our regularized version of OptV (Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e006">6</xref>)) provided a good estimation of the value function, we used simulation data. First, we designed the value function of <italic>T</italic> and <italic>dT</italic> as the ground truth (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s001">S1A Fig</xref></bold>), and passive dynamics through Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e007">7</xref>). Thus, the optimal policy was defined by Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e003">3</xref>). Second, we generated a time-series of state transitions based on the optimal policy and separated these time series into training and test datasets. Next, we estimated <italic>v</italic>(<italic>s</italic>) from the training dataset, varying the regularization parameter <italic>λ</italic> in Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e006">6</xref>) (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s001">S1B Fig</xref></bold>). We then evaluated the squared error between the behavioral strategy, based on the ground truth, and the estimated <italic>v</italic>(<italic>s</italic>), using the test dataset. Since the squared error on the test data was substantially reduced (by 88.1%) due to regularization, we deemed it effective for avoiding overfitting (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s001">S1C Fig</xref></bold>).</p>
</sec>
<sec id="sec024">
<title>Cross-validation</title>
<p>For estimating <italic>v</italic>(<italic>s</italic>), we performed cross-validation to determine <italic>λ</italic> in Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e011">10</xref>), and <italic>σ</italic><sub><italic>T</italic></sub> and <italic>σ</italic><sub><italic>dT</italic></sub> in Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e012">11</xref>), with which the prediction ability is maximized. We divided the time-series behavioral data equally into nine parts. We then independently performed estimation of <italic>v</italic>(<italic>s</italic>) nine times; for each estimation, eight of the nine parts of the data were used for estimation, while the remaining part was used to evaluate the prediction ability of the estimated value function by the likelihood [Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e010">9</xref>)]. We then optimized those parameters at which we obtained the highest log-likelihood, as averaged from the nine estimations.</p>
</sec>
<sec id="sec025">
<title>Surrogate method-based statistical testing</title>
<p>To check whether the DM and IM strategies were not obtained by chance, surrogate method-based statistical testing was performed under a null hypothesis that the worms randomly migrated under a thermal gradient with no behavioral strategy. We first constructed a set of artificial temperature time-series, which could be observed under the null hypothesis. By using the iterated amplitude adjusted Fourier transform method [<xref ref-type="bibr" rid="pcbi.1006122.ref038">38</xref>], we prepared 1000 surrogate datasets by shuffling the observed temperature time-series (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s005">S5A Fig</xref></bold>), while preserving the autocorrelation of the original time-series (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s005">S5B Fig</xref></bold>). We then applied our IRL algorithm to this surrogate dataset to estimate the desirability function (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s005">S5C Fig</xref></bold>). To assess the significance of the DM and IM strategies, we calculated the sums of the estimated desirability functions within the previously described horizontal and diagonal regions, respectively (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s005">S5D Fig</xref></bold>). Empirical distributions of these test statistics for the surrogate datasets could then serve as null distributions (<bold><xref ref-type="supplementary-material" rid="pcbi.1006122.s005">S5E Fig</xref></bold>). For both DM and IM, the test statistic derived using the original desirability function was located above the empirical null distribution (<italic>p</italic> &lt;0.001 for the DM strategy; <italic>p</italic> &lt;0.001 for the IM strategy), indicating that both strategies were not obtained by chance but reflected an actual strategy of thermotaxis.</p>
</sec>
<sec id="sec026">
<title><italic>C. elegans</italic> preparation</title>
<p>All worms were hermaphrodites and cultivated on OP50 as bacterial food using standard techniques [<xref ref-type="bibr" rid="pcbi.1006122.ref039">39</xref>]. The following strains were used: N2 wild-type Bristol strain, PY7505 <italic>oyIs84[gcy-27p</italic>::<italic>cz</italic>::<italic>caspase-3(p17)</italic>, <italic>gpa-4p</italic>::<italic>caspase-3(p12)</italic>::<italic>nz</italic>, <italic>gcy-27p</italic>::<italic>GFP</italic>, <italic>unc-122p</italic>::<italic>dsRed]</italic>, IK2808 <italic>njIs79[ceh-36p</italic>::<italic>cz</italic>::<italic>caspase-3(p17)</italic>, <italic>ceh-36p</italic>::<italic>caspase-3(p12)</italic>::<italic>nz</italic>, <italic>ges-1p</italic>::<italic>NLS</italic>::<italic>GFP]</italic> and IK2809 <italic>njIs80[gcy-8p</italic>::<italic>cz</italic>::<italic>caspase-3(p17)</italic>, <italic>gcy-8p</italic>::<italic>caspase3(p12)</italic>::<italic>nz</italic>, <italic>ges-1p</italic>::<italic>NLS</italic>::<italic>GFP]</italic>. The ASI-ablated strain (PY7505) was a kind gift from Dr. Piali Sengupta [<xref ref-type="bibr" rid="pcbi.1006122.ref027">27</xref>]. The AFD-ablated strain (IK2809) and the AWC-ablated strain (IK2808) were generated by the expression of reconstituted caspases [<xref ref-type="bibr" rid="pcbi.1006122.ref040">40</xref>]. Plasmids carrying the reconstituted caspases were injected at 25 ng/μl with the injection marker pKDK66 (<italic>ges-1p</italic>::<italic>NLS</italic>::<italic>GFP</italic>) (50 ng/μl). Extrachromosomal arrays were integrated into the genome by gamma irradiation, and the resulting strains were outcrossed four times before analysis. To assess the efficiency of cell killing by the caspase transgenes, the integrated transgenes were crossed into integrated reporters that expressed GFPs in several neurons, including the neuron of interest, as follows: IK0673 <italic>njIs2</italic>[<italic>nhr-38p</italic>::<italic>GFP</italic>, <italic>AIYp</italic>::<italic>GFP</italic>] for AFD and IK2811 <italic>njIs82</italic>[<italic>ceh-36p</italic>::<italic>GFP</italic>, <italic>glr-3p</italic>::<italic>GFP</italic>] for AWC. Neuronal loss was confirmed by the disappearance of fluorescence; 100% of <italic>njIs80</italic> animals displayed a loss of AFD and 98.4% of the <italic>njIs79</italic> animals displayed a loss of AWC neurons.</p>
</sec>
<sec id="sec027">
<title>Thermotaxis assay</title>
<p>Thermotaxis assays were performed as previously described [<xref ref-type="bibr" rid="pcbi.1006122.ref041">41</xref>]. Animals were first cultivated at 20°C and then placed on the center of an assay plate (14 cm × 10 cm, 1.45 cm height) containing 18 ml of thermotaxis medium, supplemented with 2% agar, and were allowed to move freely for 60 min. The center of the plate was adjusted to 17, 20, or 23°C, to create three different gradient conditions, and the plates were then maintained at a linear thermal gradient of approximately 0.45°C/cm.</p>
</sec>
<sec id="sec028">
<title>Behavioral recording</title>
<p>Worm behaviors were recorded using a CMOS sensor camera-link camera (8 bits, 4,096 × 3,072 pixels; CSC12M25BMP19-01B; Toshiba-Teli), a Line-Scan Lens (35 mm, f/2.8; YF3528; PENTAX), and a camera-link frame grabber (PCIe-1433; National Instruments). The camera was mounted at a distance above the assay plate and consistently produced an image with 33.2 μm per pixel. The frame rate of recordings was approximately 13.5 Hz. Images were captured and processed by a multi-worm Tracker [<xref ref-type="bibr" rid="pcbi.1006122.ref024">24</xref>], to detect worm bodies and measure the position of the centroid.</p>
</sec>
</sec>
<sec id="sec029">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006122.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006122.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Validation of the regularized (OptV) estimation method by using artificial data.</title>
<p><bold>(A)</bold> The desirability function corresponding to the ground truth value function used for generation of artificial data. Time-series data were artificially generated as training and test data sets by sampling Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e001">1</xref>), based on the ground truth of the value function. <bold>(B)</bold> The desirability functions under three different regularization parameters (<italic>λ</italic>) were visualized from the estimated value functions. <bold>(C)</bold> Squared error between the behavioral strategies based on the ground truth and estimated value functions using the test data set. The presence of an optimal <italic>λ</italic>, at which the minimal squared error is obtained, indicates that the regularization was effective for accurately estimating the value function.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006122.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006122.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Behaviors in the <italic>T</italic>-<italic>dT</italic> space.</title>
<p><bold>(A)</bold> <italic>T-dT</italic> trajectories of fed WT worms. This is another representation of <xref ref-type="fig" rid="pcbi.1006122.g002">Fig 2E</xref>. <bold>(B)</bold> Distributions of <italic>T</italic> and <italic>dT</italic> in all trajectories of fed WT worms. Notice that the distribution is substantially different from the desirability function (see <xref ref-type="fig" rid="pcbi.1006122.g003">Fig 3B</xref>). <bold>(C)</bold> Scatter plot of <italic>T</italic> and <italic>dT</italic> at 5 seconds before the moment of sharp turns. Correlation coefficient was 3.6e-10. Note that <italic>dT</italic> is 0 at the moment of a sharp turn, because the worm stops in order to make large directional changes. <bold>(D)</bold> Histogram of the scatter plot in C.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006122.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006122.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Inverse reinforcement learning analysis using one-dimensional state representation.</title>
<p>IRL was performed with one-dimensional state representation (<italic>s</italic> = (<italic>T</italic>)). <bold>(A)</bold> The desirability function was calculated using the estimated value function. In the estimation, the regularization parameter, <italic>λ</italic>, in Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e006">6</xref>), was optimized by cross-validation. <bold>(B)</bold> The prediction ability was compared between IRLs with <italic>s</italic> = (<italic>T</italic>, <italic>dT</italic>) and <italic>s</italic> = (<italic>T</italic>) using a cross-validation dataset. The negative log-likelihood of behavioral strategies (Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e001">1</xref>)) when estimating the value function of both <italic>T</italic> and <italic>dT</italic> (see <xref ref-type="fig" rid="pcbi.1006122.g003">Fig 3B</xref>), was significantly smaller than when estimating the value function of <italic>T</italic> alone (A; <italic>p =</italic> 0.0002; Mann-Whitney U test). Thus, the behavioral strategy with <italic>s</italic> = (<italic>T</italic>, <italic>dT</italic>) was more appropriate than that with <italic>s</italic> = (<italic>T</italic>). <bold>(C)</bold> The desirability function became smoother as <italic>λ</italic> increased, with a peak around the cultivation temperature (20°C).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006122.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006122.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Reproduction of thermotaxis by simulating the identified strategy.</title>
<p><bold>(A)</bold> The identified desirability function of the fed WT worms. This is identical to <xref ref-type="fig" rid="pcbi.1006122.g003">Fig 3B</xref>. <bold>(B)</bold> Temperature time-series of simulated worms started from 15, 20, or 25°C with 0°C/s. In the simulation, the state transition was sampled from Eq (<xref ref-type="disp-formula" rid="pcbi.1006122.e003">3</xref>) using the identified desirability function in (A). Different colored lines correspond to different simulation runs. <bold>(C)</bold> Temporal changes in distributions of 100 simulated worms. Notice that most worms converged around the cultivation temperature, i.e., 20°C.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006122.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006122.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Statistical test of the behavioral strategy reliability using the surrogate method.</title>
<p>The reliability of the directed migration (DM) and isothermal migration (IM) strategies (see <xref ref-type="fig" rid="pcbi.1006122.g003">Fig 3</xref>) was assessed by means of statistical testing with the null hypothesis that worms randomly migrate with no behavioral strategy. <bold>(A)</bold> To generate time-series data under this null hypothesis, original time-series data of temperature (left panel) were surrogated by the iterated amplitude adjusted Fourier transform method (right panel). <bold>(B)</bold> Before and after the surrogation, the autocorrelations were almost preserved. <bold>(C)</bold> The desirability functions estimated from the surrogate datasets. <bold>(D)</bold> The DM and IM strategies correspond to the red-highlighted diagonal and horizontal regions of the desirability function, respectively. Within these regions, sums of the estimated desirability functions were calculated as test statistics. <bold>(E)</bold> Histograms of the empirical null distributions of the test statistics for the DM and IM strategies. Test statistics derived by the original desirability function (red arrows) are located above the empirical null distributions (<italic>p</italic> &lt;0.001 for the PT strategy; <italic>p</italic> &lt;0.001 for the IT strategy).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006122.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006122.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Estimated value/reward functions and state distributions.</title>
<p>The estimated value functions, reward functions, and state distributions are depicted for the ASI- <bold>(A)</bold>, AWC- <bold>(B)</bold>, and AFD-deficient worms <bold>(C)</bold>, as well as for the starved WT worms <bold>(D)</bold>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Drs. Eiji Uchibe, Masataka Yamao, and Shin-ichi Maeda for their valuable comments. We are also grateful to Dr. Shigeyuki Oba for giving advice on statistical testing.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006122.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Iwasa</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Higashi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Yamamura</surname> <given-names>N</given-names></name>. <article-title>Prey Distribution as a Factor Determining the Choice of Optimal Foraging Strategy</article-title>. <source>Am Nat</source>. <year>1981</year>;<volume>117</volume>: <fpage>710</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1086/283754" xlink:type="simple">10.1086/283754</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006122.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Perona</surname> <given-names>P</given-names></name>. <article-title>Toward a science of computational ethology</article-title>. <source>Neuron</source>. <year>2014</year>. pp. <fpage>18</fpage>–<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.09.005" xlink:type="simple">10.1016/j.neuron.2014.09.005</ext-link></comment> <object-id pub-id-type="pmid">25277452</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref003"><label>3</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Marr</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <source>From Understanding Computation to Understanding Neural Circuitry</source>. <publisher-name>Massachusetts Institute of Technology Artificial Intelligence Laboratory</publisher-name>. <year>1976</year>.</mixed-citation></ref>
<ref id="pcbi.1006122.ref004"><label>4</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <chapter-title>Sutton &amp; Barto Book: Reinforcement Learning: An Introduction</chapter-title>. In: <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, MA</publisher-loc>, <source>A Bradford Book</source> [Internet]. <year>1998</year>. Available: <ext-link ext-link-type="uri" xlink:href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" xlink:type="simple">http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html</ext-link></mixed-citation></ref>
<ref id="pcbi.1006122.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>. <article-title>A Neural Substrate of Prediction and Reward</article-title>. <source>Science</source> (80-). <year>1997</year>;<volume>275</volume>: <fpage>1593</fpage>–<lpage>1599</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.275.5306.1593" xlink:type="simple">10.1126/science.275.5306.1593</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006122.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>A framework for mesencephalic dopamine systems based on predictive Hebbian learning</article-title>. <source>J Neurosci</source>. <year>1996</year>;<volume>16</volume>: <fpage>1936</fpage>–<lpage>1947</lpage>. doi:10.1.1.156.635 <object-id pub-id-type="pmid">8774460</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Calhoun</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Tong</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pokala</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Fitzpatrick</surname> <given-names>JAJ</given-names></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name>, <name name-style="western"><surname>Chalasani</surname> <given-names>SH</given-names></name>. <article-title>Neural Mechanisms for Evaluating Environmental Variability in Caenorhabditis elegans</article-title>. <source>Neuron</source>. Elsevier; <year>2015</year>;<volume>86</volume>: <fpage>428</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2015.03.026" xlink:type="simple">10.1016/j.neuron.2015.03.026</ext-link></comment> <object-id pub-id-type="pmid">25864633</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Russell</surname> <given-names>S</given-names></name>. <article-title>Learning agents for uncertain environments (extended abstract)</article-title>. <source>Proc 11th Annu Conf Comput Learn Theory</source>. <year>1998</year>; <fpage>101</fpage>–<lpage>103</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/279943.279964" xlink:type="simple">10.1145/279943.279964</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006122.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ng</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Russell</surname> <given-names>S</given-names></name>. <article-title>Algorithms for inverse reinforcement learning</article-title>. <source>Proc Seventeenth Int Conf Mach Learn</source>. <year>2000</year>;0: <fpage>663</fpage>–<lpage>670</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2460/ajvr.67.2.323" xlink:type="simple">10.2460/ajvr.67.2.323</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006122.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbeel</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Coates</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ng</surname> <given-names>AY</given-names></name>. <article-title>Autonomous Helicopter Aerobatics through Apprenticeship Learning</article-title>. <source>Int J Rob Res</source>. <year>2010</year>;<volume>29</volume>: <fpage>1608</fpage>–<lpage>1639</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0278364910371999" xlink:type="simple">10.1177/0278364910371999</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006122.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbeel</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Coates</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Quigley</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ng</surname> <given-names>AY</given-names></name>. <article-title>An application of reinforcement learning to aerobatic helicopter flight</article-title>. <source>Adv Neural Inf Process Syst</source>. <year>2007</year>. p. <fpage>1</fpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://heli.stanford.edu/papers/nips06-aerobatichelicopter.pdf" xlink:type="simple">http://heli.stanford.edu/papers/nips06-aerobatichelicopter.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1006122.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vu</surname> <given-names>VH</given-names></name>, <name name-style="western"><surname>Isableu</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Berret</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Uno</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Gomi</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yoshioka</surname> <given-names>T</given-names></name>, <etal>et al</etal>. <article-title>Adaptive use of interaction torque during arm reaching movement from the optimal control viewpoint</article-title>. <source>Sci Rep</source>. Nature Publishing Group; <year>2016</year>;<volume>6</volume>: <fpage>38845</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep38845" xlink:type="simple">10.1038/srep38845</ext-link></comment> <object-id pub-id-type="pmid">27941920</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Muelling</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Boularias</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mohler</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Schölkopf</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Peters</surname> <given-names>J</given-names></name>. <article-title>Learning strategies in table tennis using inverse reinforcement learning</article-title>. <source>Biol Cybern</source>. <year>2014</year>;<volume>108</volume>: <fpage>603</fpage>–<lpage>619</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00422-014-0599-1" xlink:type="simple">10.1007/s00422-014-0599-1</ext-link></comment> <object-id pub-id-type="pmid">24756167</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mohammed</surname> <given-names>RAA</given-names></name>, <name name-style="western"><surname>Staadt</surname> <given-names>O</given-names></name>. <article-title>Learning eye movements strategies on tiled Large High-Resolution Displays using inverse reinforcement learning</article-title>. <source>2015 International Joint Conference on Neural Networks (IJCNN)</source>. IEEE; <year>2015</year>. pp. <fpage>1</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/IJCNN.2015.7280675" xlink:type="simple">10.1109/IJCNN.2015.7280675</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006122.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rothkopf</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name>. <article-title>Modular inverse reinforcement learning for visuomotor behavior</article-title>. <source>Biol Cybern</source>. <year>2013</year>;<volume>107</volume>: <fpage>477</fpage>–<lpage>490</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00422-013-0562-6" xlink:type="simple">10.1007/s00422-013-0562-6</ext-link></comment> <object-id pub-id-type="pmid">23832417</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kuhara</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Okumura</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kimata</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tanizawa</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Takano</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Kimura</surname> <given-names>KD</given-names></name>, <etal>et al</etal>. <article-title>Temperature sensing by an olfactory neuron in a circuit controlling behavior of C-elegans</article-title>. <source>Science</source> (80-). <year>2008</year>;<volume>320</volume>: <fpage>803</fpage>–<lpage>807</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1148922" xlink:type="simple">10.1126/science.1148922</ext-link></comment> <object-id pub-id-type="pmid">18403676</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mori</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Ohshima</surname> <given-names>Y</given-names></name>. <article-title>Neural regulation of thermotaxis in Caenorhabditis elegans</article-title>. <source>Nature</source>. <year>1995</year>. pp. <fpage>344</fpage>–<lpage>348</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/376344a0" xlink:type="simple">10.1038/376344a0</ext-link></comment> <object-id pub-id-type="pmid">7630402</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hedgecock</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Russell</surname> <given-names>RL</given-names></name>. <article-title>Normal and mutant thermotaxis in the nematode Caenorhabditis elegans</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>1975</year>;<volume>72</volume>: <fpage>4061</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.72.10.4061" xlink:type="simple">10.1073/pnas.72.10.4061</ext-link></comment> <object-id pub-id-type="pmid">1060088</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mohri</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kodama</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Kimura</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Koike</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Mizuno</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Mori</surname> <given-names>I</given-names></name>. <article-title>Genetic control of temperature preference in the nematode Caenorhabditis elegans</article-title>. <source>Genetics</source>. <year>2005</year>;<volume>169</volume>: <fpage>1437</fpage>–<lpage>1450</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1534/genetics.104.036111" xlink:type="simple">10.1534/genetics.104.036111</ext-link></comment> <object-id pub-id-type="pmid">15654086</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Todorov</surname> <given-names>E</given-names></name>. <article-title>Linearly-solvable Markov decision problems</article-title>. <source>Adv Neural Inf Process Syst</source>. <year>2006</year>; <volume>8</volume>. Available: <ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/3002-linearly-solvable-markov-decision-problems" xlink:type="simple">https://papers.nips.cc/paper/3002-linearly-solvable-markov-decision-problems</ext-link></mixed-citation></ref>
<ref id="pcbi.1006122.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dvijotham</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Todorov</surname> <given-names>E</given-names></name>. <article-title>Inverse Optimal Control with Linearly-Solvable MDPs</article-title>. <source>Int Conf Machine Learning</source>. <year>2010</year>. pp. <fpage>335</fpage>–<lpage>342</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="https://homes.cs.washington.edu/~todorov/papers/DvijothamICML10.pdf" xlink:type="simple">https://homes.cs.washington.edu/~todorov/papers/DvijothamICML10.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1006122.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsukada</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Yamao</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Naoki</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Shimowada</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ohnishi</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kuhara</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Reconstruction of Spatial Thermal Gradient Encoded in Thermosensory Neuron AFD in Caenorhabditis elegans</article-title>. <source>J Neurosci</source>. <year>2016</year>;<volume>36</volume>: <fpage>2571</fpage>–<lpage>81</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2837-15.2016" xlink:type="simple">10.1523/JNEUROSCI.2837-15.2016</ext-link></comment> <object-id pub-id-type="pmid">26936999</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ramot</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>MacInnis</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Goodman</surname> <given-names>MB</given-names></name>. <article-title>Bidirectional temperature-sensing by a single thermosensory neuron in C. elegans</article-title>. <source>Nat Neurosci</source>. <year>2008</year>;<volume>11</volume>: <fpage>908</fpage>–<lpage>915</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2157" xlink:type="simple">10.1038/nn.2157</ext-link></comment> <object-id pub-id-type="pmid">18660808</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Swierczek</surname> <given-names>NA</given-names></name>, <name name-style="western"><surname>Giles</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Rankin</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Kerr</surname> <given-names>RA</given-names></name>. <article-title>High-throughput behavioral analysis in C. elegans</article-title>. <source>Nat Methods</source>. <year>2011</year>;<volume>8</volume>: <fpage>592</fpage>–<lpage>U112</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nmeth.1625" xlink:type="simple">10.1038/nmeth.1625</ext-link></comment> <object-id pub-id-type="pmid">21642964</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Savitzky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Golay</surname> <given-names>MJE</given-names></name>. <article-title>Smoothing and Differentiation of Data by Simplified Least Squares Procedures</article-title>. <source>Anal Chem</source>. <year>1964</year>;<volume>36</volume>: <fpage>1627</fpage>–<lpage>1639</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1021/ac60214a047" xlink:type="simple">10.1021/ac60214a047</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006122.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pierce-Shimomura</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Morse</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Lockery</surname> <given-names>SR</given-names></name>. <article-title>The fundamental role of pirouettes in Caenorhabditis elegans chemotaxis</article-title>. <source>J Neurosci</source>. <year>1999</year>;<volume>19</volume>: <fpage>9557</fpage>–<lpage>9569</lpage>. <object-id pub-id-type="pmid">10531458</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beverly</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Anbil</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sengupta</surname> <given-names>P</given-names></name>. <article-title>Degeneracy and Neuromodulation among Thermosensory Neurons Contribute to Robust Thermosensory Behaviors in Caenorhabditis elegans</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>: <fpage>11718</fpage>–<lpage>11727</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1098-11.2011" xlink:type="simple">10.1523/JNEUROSCI.1098-11.2011</ext-link></comment> <object-id pub-id-type="pmid">21832201</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biron</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Wasserman</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Thomas</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Samuel</surname> <given-names>ADT</given-names></name>, <name name-style="western"><surname>Sengupta</surname> <given-names>P</given-names></name>. <article-title>An olfactory neuron responds stochastically to temperature and modulates Caenorhabditis elegans thermotactic behavior</article-title>. <source>Proc Natl Acad Sci</source>. <year>2008</year>;<volume>105</volume>: <fpage>11002</fpage>–<lpage>11007</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0805004105" xlink:type="simple">10.1073/pnas.0805004105</ext-link></comment> <object-id pub-id-type="pmid">18667708</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Franklin</surname> <given-names>GF</given-names></name>, <name name-style="western"><surname>Powell</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Emami-Naeini</surname> <given-names>A</given-names></name>. <article-title>Feedback Control of Dynamic Systems</article-title> [Internet]. <source>Sound And Vibration</source>. <year>2002</year>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.pearsonhighered.com/educator/product/Feedback-Control-of-Dynamic-Systems-6E/9780136019695.page" xlink:type="simple">http://www.pearsonhighered.com/educator/product/Feedback-Control-of-Dynamic-Systems-6E/9780136019695.page</ext-link></mixed-citation></ref>
<ref id="pcbi.1006122.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ramot</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>MacInnis</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>H-C</given-names></name>, <name name-style="western"><surname>Goodman</surname> <given-names>MB</given-names></name>. <article-title>Thermotaxis is a Robust Mechanism for Thermoregulation in Caenorhabditis elegans Nematodes</article-title>. <source>J Neurosci</source>. <year>2008</year>;<volume>28</volume>: <fpage>12546</fpage>–<lpage>12557</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2857-08.2008" xlink:type="simple">10.1523/JNEUROSCI.2857-08.2008</ext-link></comment> <object-id pub-id-type="pmid">19020047</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ishii</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Yoshida</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Yoshimoto</surname> <given-names>J</given-names></name>. <article-title>Control of exploitation-exploration meta-parameter in reinforcement learning</article-title> [Internet]. <source>Neural Networks</source>. <year>2002</year>. pp. <fpage>665</fpage>–<lpage>687</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0893-6080(02)00056-4" xlink:type="simple">10.1016/S0893-6080(02)00056-4</ext-link></comment> <object-id pub-id-type="pmid">12371519</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Okada</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ueda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Okamoto</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Yamawaki</surname> <given-names>S</given-names></name>. <article-title>Prediction of immediate and future rewards differentially recruits cortico-basal ganglia loops</article-title>. <source>Behavioral Economics of Preferences, Choices, and Happiness</source>. <year>2016</year>. pp. <fpage>593</fpage>–<lpage>616</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-4-431-55402-8_22" xlink:type="simple">10.1007/978-4-431-55402-8_22</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006122.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Samejima</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ueda</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kimura</surname> <given-names>M</given-names></name>. <article-title>Representation of action-specific reward values in the striatum</article-title>. <source>Science</source>. <year>2005</year>;<volume>310</volume>: <fpage>1337</fpage>–<lpage>40</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1115270" xlink:type="simple">10.1126/science.1115270</ext-link></comment> <object-id pub-id-type="pmid">16311337</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>Modulators of decision making</article-title>. <source>Nat Neurosci</source>. <year>2008</year>;<volume>11</volume>: <fpage>410</fpage>–<lpage>416</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn2077" xlink:type="simple">10.1038/nn2077</ext-link></comment> <object-id pub-id-type="pmid">18368048</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yagishita</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hayashi-Takagi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ellis-Davies</surname> <given-names>GCR</given-names></name>, <name name-style="western"><surname>Urakubo</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Ishii</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kasai</surname> <given-names>H</given-names></name>. <article-title>A critical time window for dopamine actions on the structural plasticity of dendritic spines</article-title>. <source>Science</source> (80-). <year>2014</year>;<volume>345</volume>: <fpage>1616</fpage>–<lpage>1620</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1255514" xlink:type="simple">10.1126/science.1255514</ext-link></comment> <object-id pub-id-type="pmid">25258080</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Nakae</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ishii</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Naoki</surname> <given-names>H</given-names></name>. <article-title>Uncertainty-Dependent Extinction of Fear Memory in an Amygdala-mPFC Neural Circuit Model</article-title>. <source>PLoS Comput Biol</source>. <year>2016</year>;<volume>12</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005099" xlink:type="simple">10.1371/journal.pcbi.1005099</ext-link></comment> <object-id pub-id-type="pmid">27617747</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yokoyama</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Suzuki</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Sato</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Maruta</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Watanabe</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Miyaoka</surname> <given-names>H</given-names></name>. <article-title>Amygdalic levels of dopamine and serotonin rise upon exposure to conditioned fear stress without elevation of glutamate</article-title>. <source>Neurosci Lett</source>. <year>2005</year>;<volume>379</volume>: <fpage>37</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neulet.2004.12.047" xlink:type="simple">10.1016/j.neulet.2004.12.047</ext-link></comment> <object-id pub-id-type="pmid">15814195</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schreiber</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Schmitz</surname> <given-names>A</given-names></name>. <article-title>Improved surrogate data for nonlinearity tests</article-title>. <source>Phys Rev Lett</source>. <year>1999</year>;<volume>77</volume>: <fpage>635</fpage>–<lpage>638</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.77.635" xlink:type="simple">10.1103/PhysRevLett.77.635</ext-link></comment> <object-id pub-id-type="pmid">10062864</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brenner</surname> <given-names>S</given-names></name>. <article-title>The genetics of Caenorhabditis elegans</article-title>. <source>Genetics</source>. <year>1974</year>;<volume>77</volume>: <fpage>71</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/cbic.200300625" xlink:type="simple">10.1002/cbic.200300625</ext-link></comment> <object-id pub-id-type="pmid">4366476</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chelur</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Chalfie</surname> <given-names>M</given-names></name>. <article-title>Targeted cell killing by reconstituted caspases</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2007</year>;<volume>104</volume>: <fpage>2283</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0610877104" xlink:type="simple">10.1073/pnas.0610877104</ext-link></comment> <object-id pub-id-type="pmid">17283333</object-id></mixed-citation></ref>
<ref id="pcbi.1006122.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ito</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Inada</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Mori</surname> <given-names>I</given-names></name>. <article-title>Quantitative analysis of thermotaxis in the nematode Caenorhabditis elegans</article-title>. <source>J Neurosci Methods</source>. <year>2006</year>;<volume>154</volume>: <fpage>45</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2005.11.011" xlink:type="simple">10.1016/j.jneumeth.2005.11.011</ext-link></comment> <object-id pub-id-type="pmid">16417923</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>