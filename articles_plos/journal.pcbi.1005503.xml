<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-01602</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005503</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal system</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal system</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Markov processes</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Markov processes</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Markov models</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Predicting explorative motor learning using decision-making and motor noise</article-title>
<alt-title alt-title-type="running-head">Decision-making during motor learning</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1549-4352</contrib-id>
<name name-style="western">
<surname>Chen</surname> <given-names>Xiuli</given-names></name>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6165-7810</contrib-id>
<name name-style="western">
<surname>Mohr</surname> <given-names>Kieran</given-names></name>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="fn" rid="currentaff001"><sup>Â¤</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Galea</surname> <given-names>Joseph M.</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>School of Psychology, University of Birmingham, Birmingham, United Kingdom</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Haith</surname> <given-names>Adrian M</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Johns Hopkins University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p>
<list list-type="simple">
<list-item>
<p><bold>Conceptualization:</bold> JMG XC.</p>
</list-item>
<list-item>
<p><bold>Data curation:</bold> XC.</p>
</list-item>
<list-item>
<p><bold>Formal analysis:</bold> XC.</p>
</list-item>
<list-item>
<p><bold>Funding acquisition:</bold> JMG.</p>
</list-item>
<list-item>
<p><bold>Investigation:</bold> XC KM.</p>
</list-item>
<list-item>
<p><bold>Methodology:</bold> JMG XC.</p>
</list-item>
<list-item>
<p><bold>Project administration:</bold> JMG XC.</p>
</list-item>
<list-item>
<p><bold>Resources:</bold> JMG.</p>
</list-item>
<list-item>
<p><bold>Software:</bold> JMG XC KM.</p>
</list-item>
<list-item>
<p><bold>Supervision:</bold> JMG.</p>
</list-item>
<list-item>
<p><bold>Validation:</bold> JMG XC.</p>
</list-item>
<list-item>
<p><bold>Visualization:</bold> XC.</p>
</list-item>
<list-item>
<p><bold>Writing â original draft:</bold> XC.</p>
</list-item>
<list-item>
<p><bold>Writing â review &amp; editing:</bold> JMG XC KM.</p>
</list-item>
</list>
</p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<p>Â¤Current address: School of Electronic and Electrical Engineering, University College Dublin, Belfield, Ireland</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">chenxy@bham.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>4</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>24</day>
<month>4</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>4</issue>
<elocation-id>e1005503</elocation-id>
<history>
<date date-type="received">
<day>30</day>
<month>9</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>6</day>
<month>4</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Chen et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005503"/>
<abstract>
<p>A fundamental problem faced by humans is learning to select motor actions based on noisy sensory information and incomplete knowledge of the world. Recently, a number of authors have asked whether this type of motor learning problem might be very similar to a range of higher-level decision-making problems. If so, participant behaviour on a high-level decision-making task could be predictive of their performance during a motor learning task. To investigate this question, we studied performance during an explorative motor learning task and a decision-making task which had a similar underlying structure with the exception that it was not subject to motor (execution) noise. We also collected an independent measurement of each participantâs level of motor noise. Our analysis showed that explorative motor learning and decision-making could be modelled as the (approximately) optimal solution to a Partially Observable Markov Decision Process bounded by noisy neural information processing. The model was able to predict participant performance in motor learning by using parameters estimated from the decision-making task and the separate motor noise measurement. This suggests that explorative motor learning can be formalised as a sequential decision-making process that is adjusted for motor noise, and raises interesting questions regarding the neural origin of explorative motor learning.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Until recently, motor learning was viewed as an automatic process that was independent, and even in conflict with higher-level cognitive processes such as decision-making. However, it is now thought that decision-making forms an integral part of motor learning. To further examine the relationship between decision-making and motor learning, we asked whether explorative motor learning could be considered a decision-making task that was adjusted for motor noise. We studied human performance in an explorative motor learning task and a decision-making task which had a similar underlying structure with the exception that it was not subject to motor (execution) noise. In addition, we independently measured each participantâs level of motor noise. Crucially, with a computational model, we were able to predict participant explorative motor learning by using parameters estimated from the decision-making task and the separate motor noise task. This suggests that explorative motor learning can be formalised as a sequential decision-making process that is adjusted for motor noise, and reinforces the view that the mechanisms which control decision-making and motor behaviour are highly integrated.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000781</institution-id>
<institution>European Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>MotMotLearn: 637488</award-id>
</award-group>
<funding-statement>This work was supported by an European Research Council starter grant to JMG (MotMotLearn: 637488), <ext-link ext-link-type="uri" xlink:href="https://erc.europa.eu" xlink:type="simple">https://erc.europa.eu</ext-link>. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="12"/>
<table-count count="2"/>
<page-count count="33"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2017-05-08</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Data and model code can be found on Open Science Framework at <ext-link ext-link-type="uri" xlink:href="https://osf.io/bafms/" xlink:type="simple">https://osf.io/bafms/</ext-link> (Project title: Predicting Explorative Motor Learning Using Decision-Making and Motor Noise).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Previously, human motor learning has mainly been examined through motor adaptation tasks in which participants are exposed to a novel perturbation during reaching movements [<xref ref-type="bibr" rid="pcbi.1005503.ref001">1</xref>â<xref ref-type="bibr" rid="pcbi.1005503.ref004">4</xref>]. The error reduction observed during these tasks has been conceptualised as a cerebellar-dependent supervised-learning process in which they learn through a sensory prediction error [<xref ref-type="bibr" rid="pcbi.1005503.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref006">6</xref>]. However, recent work has shown that motor learning is a far more complex process that can involve multiple mechanisms, including decision-making processes, taking place simultaneously [<xref ref-type="bibr" rid="pcbi.1005503.ref007">7</xref>â<xref ref-type="bibr" rid="pcbi.1005503.ref010">10</xref>].</p>
<p>One example of these motor-learning processes is reinforcement learning. This learning mechanism requires participants to explore their motor behaviour in order to identify actions that maximise expected future success/reward (in contrast with minimising the sensory prediction error). Despite being significantly slower and more variable than learning through a sensory prediction error, recent work has shown that participants are able to identify and adjust specific features of a movement, such as the curvature of a trajectory, simply through a reinforcement signal [<xref ref-type="bibr" rid="pcbi.1005503.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref011">11</xref>â<xref ref-type="bibr" rid="pcbi.1005503.ref015">15</xref>]. Such explorative motor learning has been explained using reinforcement models in which learning is driven by a reward prediction error. This enables actions to be selected based on the probability of yielding future rewards [<xref ref-type="bibr" rid="pcbi.1005503.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref016">16</xref>].</p>
<p>Arguably, it follows that explorative motor learning is simply a sequential decision task where the goal is to optimise reward in the face of task and sensory uncertainty. If so, participant behaviour on a matched high-level decision-making task should be predictive of performance during an explorative motor learning task. Previous work has compared high-level (economic) decision-making tasks with an equivalent motor lottery task [<xref ref-type="bibr" rid="pcbi.1005503.ref017">17</xref>, a review]. Some found that, in contrast to the well-documented sub-optimality in high-level (economic) decision-making [<xref ref-type="bibr" rid="pcbi.1005503.ref018">18</xref>], participants were able to perform near optimal decisions in a motor lottery task [<xref ref-type="bibr" rid="pcbi.1005503.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref020">20</xref>]. For example, during simple pointing movements, participants hold an internal representation of motor noise uncertainty and compensate for this variability when planning a movement [<xref ref-type="bibr" rid="pcbi.1005503.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref020">20</xref>]. However, others found that participants in a motor lottery task (where the uncertainty of outcomes were primarily due to motor noise) exhibited significant suboptimal choice patterns [<xref ref-type="bibr" rid="pcbi.1005503.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref021">21</xref>]. Yet, the patterns of deviation from optimal choice were markedly different from those shown in high-level (economic) decision-making. Previous work highlights that one of the unique features that affect motor performance is a noisy motor system (motor noise). To our knowledge, most of these previous studies focused on binary or one-shot decision-making and its motor analogue. In contrast, here we ask if explorative motor learning is a sequential decision task that optimises reward in the face of task uncertainty, sensory uncertainty and motor noise uncertainty [<xref ref-type="bibr" rid="pcbi.1005503.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref022">22</xref>]?</p>
<p>To explore this question, we investigated learning performance in an explorative motor learning task [<xref ref-type="bibr" rid="pcbi.1005503.ref013">13</xref>] and a decision-making task with a similar underlying structure with the exception that it was not subject to motor noise. We also took an independent measurement of each participantâs motor noise. We formulated the learning problem as a Partially Observable Markov Decision Process (POMDP) and built a computational model to solve the defined POMDP. The question we asked was whether we could predict participant explorative motor learning performance by fitting the model to the decision task performance and then adding each participantâs measured level of motor noise.</p>
<p>In addition, we were interested in whether we could predict motor learning performance as a function of gains and lossesâone of the key concepts in the decision-making literature. In Prospect Theory [<xref ref-type="bibr" rid="pcbi.1005503.ref018">18</xref>], a theory of human decision-making, gains and losses are defined relative to a reference point that shifts with the decision context. For example [<xref ref-type="bibr" rid="pcbi.1005503.ref018">18</xref>], imagine a situation where a participant has just lost Â£2000 and is now facing a choice between a 100% chance of winning Â£1000 and a 50% chance of winning Â£2000 or nothing. If the participantâs reference frame had shifted to account for their recent loss, then they are likely to code the decision as choice between a 100% chance of losing Â£1000 and a 50% chance of losing Â£2000 or nothing. Understanding how people interpret gains and losses is important, because, for example, it has been shown that people are more adventurous in the latter representation (i.e., loss aversion, [<xref ref-type="bibr" rid="pcbi.1005503.ref018">18</xref>]). In the motor learning domain, research has shown that reward (positive feedback) and punishment (negative feedback) have multifaceted effects on motor learning [<xref ref-type="bibr" rid="pcbi.1005503.ref023">23</xref>]. Therefore, we were interested in understanding whether the ideas regarding gains and losses in decision-making were relevant to explorative motor learning.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Behavioural analysis: Learning performance</title>
<p>We investigated performance during an explorative motor learning (reaching) task adopted from [<xref ref-type="bibr" rid="pcbi.1005503.ref013">13</xref>] and a novel decision-making (DM) task which had a similar underlying structure. In the reaching (MO) task (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1A</xref>), participants were seated at a desk, looking down at a horizontal mirror that reflected task-related stimuli from a computer screen. The mirror blocked direct observation of the index finger, which was instead represented on the mirror via a circular green cursor. Participants were asked to draw trajectories by sliding their index finger from a central start position across the surface of the desk towards a target line (thick black line in <xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1B</xref>) positioned in front of the start position. Participants made 25 attempts (green dashed lines in <xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1B</xref>) to approximate each hidden target trajectory (red line). Each attempted trajectory resulted in a score that indicated the proximity of the attempted trajectory to the target trajectory. Both the target and the attempted trajectory were characterised by two parameters: direction and curvature (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1C</xref>; <xref ref-type="disp-formula" rid="pcbi.1005503.e002">Eq 1</xref>). The score for each attempt was calculated based on the errors between target and attempt in these two dimensions (<xref ref-type="disp-formula" rid="pcbi.1005503.e003">Eq 2</xref>). The participants were instructed to adjust their movementsâ direction and curvature based on the feedback to produce movements that were as close to the target trajectory as possible. Each participant attempted to match 24 different, invisible target trajectories that varied in both direction and curvature (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1C</xref>).</p>
<fig id="pcbi.1005503.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Experimental design for the motor learning (reaching) task and the decision-making task.</title>
<p><bold>(A)</bold> Illustration of the motor reaching task settings. <bold>(B)</bold> An example of the explorative motor learning in which a participant matched the reaching trajectory with a hidden target trajectory across 25 attempts. The red line represents the hidden target, while the green dashed lines represent the attempts. A score (points) was given after each attempt. <bold>(C)</bold> The 24 target trajectories used in the reaching task. The title of each panel includes the direction and curvature parameters for one target trajectory ([<italic>Î±</italic>, <italic>Î²</italic>]). These 24 targets were generated to be evenly distributed across the workspace. <bold>(D)</bold> Illustration of the decision-making task. Participants explored the cells (green) within the grid, defined by <italic>Î±</italic> and <italic>Î²</italic>, to find a hidden target cell (red asterisk). After each attempt (cell selection), a score (points) was provided.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.g001" xlink:type="simple"/>
</fig>
<p>We also designed a novel decision-making task. The objective was to capture the structure of the motor learning (reaching) task within a decision-making context that was uncontaminated by motor noise. The effect of motor noise on an aimed movement is that the outcome location is a probability density function centred on the goal [<xref ref-type="bibr" rid="pcbi.1005503.ref024">24</xref>]. In the decision-making task, participants interacted with an interface using a computer mouse. The interface consisted of a two-dimensional grid with cells (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1D</xref>). The horizontal and vertical dimension reflected two parameters: <italic>Î±</italic> and <italic>Î²</italic> respectively, akin to the direction and curvature parameters in the reaching task. The parameter values were assigned to the cells in a spatially ordered manner. Each cell of the grid therefore corresponded to a unique combination of the two parameters. When one of the cells (i.e., one parameter pair) was chosen as a target cell, the score associated with each of the cells was then calculated using the same score function (<xref ref-type="disp-formula" rid="pcbi.1005503.e003">Eq 2</xref>) as in the reaching task. Once a cell was chosen (mouse-clicked), an associated score would appear in the feedback window at the top of the screen. Similar to the reaching task, participants were required to explore different cells (parameter pairs) based on the feedback to find the cell that was as close to the target cell as possible. Participants were asked to search for a series of 24 hidden target cells.</p>
<p>In both tasks, the 24 target trajectories/cells were randomly divided into two feedback conditions (12 of each): a positive feedback condition and a negative feedback condition. In the positive feedback condition, points ranged from 0 to 50 (<xref ref-type="disp-formula" rid="pcbi.1005503.e003">Eq 2</xref>), with greater magnitude indicating greater similarity between the attempted and target trajectory (50 for the target). In the negative feedback condition, points ranged from -50 to 0 (<xref ref-type="disp-formula" rid="pcbi.1005503.e003">Eq 2</xref>), with greater magnitude indicating reduced similarity between the attempted and target trajectory (0 for the target). Hence, the goal for the positive feedback condition was to achieve 50 points, whereas for the negative feedback condition it was to achieve 0 points (i.e., avoiding losing points). Participants were told which of the two feedback conditions they were in at the beginning of each target search.</p>
<p>Analysis of the points achieved, across both tasks, showed that participants were able to update their behaviour, based on the feedback, and produce actions that were close to the target trajectory/cell (<xref ref-type="fig" rid="pcbi.1005503.g002">Fig 2A and 2B</xref>). First we examined whether participant performance was different between the positive and negative feedback conditions within both tasks. To do so, we averaged each participantâs performance across all target trajectories/cells that were experienced with either positive or negative feedback (<xref ref-type="fig" rid="pcbi.1005503.g002">Fig 2B</xref>). We fitted the exponential function, <italic>y</italic> = <italic>ae</italic><sup>â<italic>bx</italic></sup> + <italic>c</italic>, to each participantâs average learning curve in each condition (across 12 targets) (Decision-Making: <italic>R</italic><sup>2</sup> = 0.97 Â± 0.02; reaching: <italic>R</italic><sup>2</sup> = 0.89 Â± 0.10). Paired t-tests on the three parameters (a, b, c) revealed no significant differences between positive and negative feedback conditions in either the decision-making or reaching task (<xref ref-type="table" rid="pcbi.1005503.t001">Table 1</xref>).</p>
<table-wrap id="pcbi.1005503.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.t001</object-id>
<label>Table 1</label>
<caption>
<title>Comparison of learning performance between the positive and negative feedback conditions.</title>
<p>Paired t-test results on the three parameters (a,b and c in <italic>y</italic> = <italic>ae</italic><sup>â<italic>bx</italic></sup> + <italic>c</italic>) between the positive and negative feedback conditions within each of the tasks.</p>
</caption>
<alternatives>
<graphic id="pcbi.1005503.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" style="border-bottom:thick"/>
<th align="left" style="border-bottom:thick">a (Positive vs Negative)</th>
<th align="left" style="border-bottom:thick">b (Positive vs Negative)</th>
<th align="left" style="border-bottom:thick">c (Positive vs Negative)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Decision-Making</td>
<td align="left"><bold>P</bold>: M = -46.86; SD = 4.02<break/><bold>N</bold>: M = -47.22; SD = 2.75<break/>t(23) = 0.36; p = 0.72; d = 0.07</td>
<td align="left"><bold>P</bold>: M = 0.23; SD = 0.07<break/><bold>N</bold>: M = 0.23; SD = 0.06<break/>t(23) = -0.26; p = 0.80; d= -0.05</td>
<td align="left"><bold>P</bold>: M = 50.62; SD = 1.61<break/><bold>N</bold>: M = 50.18; SD = 1.47<break/>t(23) = 1.11; p = 0.28; d = 0.23</td>
</tr>
<tr>
<td align="left">Reaching</td>
<td align="left"><bold>P</bold>: M = -29.37; SD = 8.10<break/><bold>N</bold>: M = -30.89; SD = 9.79<break/>t(23) = 0.60; p = 0.55; d = 0.12</td>
<td align="left"><bold>P</bold>: M = 0.30; SD = 0.18<break/><bold>N</bold>: M = 0.24; SD = 0.11<break/>t(23) = 1.06; p = 0.30; d = 0.21</td>
<td align="left"><bold>P</bold>: M = 39.44; SD = 5.58<break/><bold>N</bold>: M = 40.96; SD = 7.13<break/>t(23) = -0.84; p = 0.41; d = -0.17</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<fig id="pcbi.1005503.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Behavioural learning performance for the decision-making and the reaching task.</title>
<p><bold>(A)</bold> Representative participant data showing how a reaching trajectory is gradually updated to match the hidden target trajectory (red). The colours of the lines indicate the sequence of attempts (ranging from green to blue), with later attempts being closer to the target trajectory. <bold>(B)</bold> Learning curves (positive and negative feedback conditions) for the participants in the decision-making task (DM) and the motor learning/reaching task (MO). Points achieved (y-axis) are plotted against the number of attempts (1-25). The dark red-triangle line and dark green-circle line represent the negative conditions in the DM and MO task respectively, while the light red-triangle line and light green-circle line represent the positive conditions. Error bars indicate 95% confidence intervals (CI) across 24 participants. <bold>(C-D)</bold> Two representative participants in terms of their curvature exploration. The curvature parameter (x-axis) ranges from â1 to 1, where â1 = âcurve to the leftâ, 1 = âcurve to the rightâ, and 0 = âstraight movementâ. The participant in (C) evenly explored the curvature dimension, while the participant in (D) concentrated on straight movements with little curvature. <bold>(E-F)</bold> Two representative participants in terms of their error reduction in both the direction (open circle) and curvature (solid circle) dimensions plotted against the number of attempts. For the participant in (E), the error in both dimensions was reduced to a relatively low level, while for the participant in (F) the error in curvature remained high. The latter was due to the lack of exploration in the curvature dimension as shown in panel (D). <bold>(G)</bold> Each participantâs mean curvature across all movements during the reaching task (blue circles; the absolute values were used for the movements with negative curvature). Four participants (10,16,18,22) were identified as outliers (red crosses). The blue circles (mean curvature values) were counted as outliers if they were larger than <italic>q</italic>3 + 0.15(<italic>q</italic>3 â <italic>q</italic>1) or smaller than <italic>q</italic>1 â 0.15(<italic>q</italic>3 â <italic>q</italic>1), where <italic>q</italic>1 and <italic>q</italic>3 were the 25<italic><sup>th</sup></italic> and 75<italic><sup>th</sup></italic> percentiles respectively.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.g002" xlink:type="simple"/>
</fig>
<p>Further analysis regarding the effect of positive and negative feedback is provided at the end of the results section. However, for the following analysis, we pooled data from the positive and negative feedback conditions by simply defining a negative score as its positive equivalent. For example, a score of -40 (10 points above the minimum point -50) in the negative condition was equivalent to 10 (10 points above the minimum point 0) in the positive condition (<xref ref-type="fig" rid="pcbi.1005503.g002">Fig 2B</xref>). Therefore, we then had one average learning curve (across 24 targets) for each participant in each of the tasks. Next we compared the learning performance across tasks (<xref ref-type="fig" rid="pcbi.1005503.g002">Fig 2</xref>). In the decision-making task, starting from 12.08 Â± 6.05, the average points achieved for each target was 49.98 Â± 0.31. For the reaching task, starting from 15.92 Â± 4.42, the average points achieved for each target was 40.96 Â± 4.67. Although participants began with a similar score across tasks, they achieved significantly more points in the decision-making task (<italic>t</italic>(23) = 9.49, <italic>p</italic> &lt; 0.001, <italic>d</italic> = 2.74).</p>
<p>We also noticed that some of the participants failed to explore the curvature dimension in the reaching task. Specifically, a small subset of participants produced straight movements with little curvature (<xref ref-type="fig" rid="pcbi.1005503.g002">Fig 2D</xref>). This resulted in significantly greater error remaining in the curvature dimension (<xref ref-type="fig" rid="pcbi.1005503.g002">Fig 2F</xref>), and thus substantially lower points being achieved. Having quantified the amount of curvature explored during the reaching task, 4 out of the 24 participants (10, 16, 18, 22) could be considered as outliers (<xref ref-type="fig" rid="pcbi.1005503.g002">Fig 2G</xref>). For the following analysis, we removed these 4 participants unless stated otherwise.</p>
</sec>
<sec id="sec004">
<title>Behavioural analysis: Action change and error reduction</title>
<p>The aim of both tasks was to find the target by exploring a range of actions. As the proximity of an action to the target was indicated by the number of points, the exploration process may have been performed by avoiding the actions with bad outcomes (low reward, high punishment) and reinforcing the actions with good outcomes (high reward, low punishment). Thus we expected to see participants make larger action changes after receiving lower points and smaller action changes after receiving higher points. Using the <italic>Î±</italic> and <italic>Î²</italic> parameters from each action, we determined action change, â<italic>A</italic>, between two successive actions: <italic>a</italic><sub><italic>t</italic></sub> = [<italic>Î±</italic><sub><italic>t</italic></sub>, <italic>Î²</italic><sub><italic>t</italic></sub>] and <italic>a</italic><sub><italic>t</italic>+1</sub> = [<italic>Î±</italic><sub><italic>t</italic>+1</sub>, <italic>Î²</italic><sub><italic>t</italic>+1</sub>] as Euclidean distance between two points, i.e., <inline-formula id="pcbi.1005503.e001"><alternatives><graphic id="pcbi.1005503.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mrow><mml:mo>â</mml:mo> <mml:mi>A</mml:mi> <mml:mo>=</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Î±</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>Î±</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Î²</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>Î²</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt> <mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. As shown in <xref ref-type="fig" rid="pcbi.1005503.g003">Fig 3A</xref>, the action change decreased as a function of score in both tasks. Interestingly, although the actions were in different forms across the tasks, the amount of action change (in terms of the Euclidean distance measurement) given the levels of score was quantitatively similar across the tasks. Paired t-tests revealed no significant difference in the average action changes between the DM and MO tasks (t(19) = 1.33, p = 0.20, d = 0.42; Bars in <xref ref-type="fig" rid="pcbi.1005503.g003">Fig 3A</xref>).</p>
<fig id="pcbi.1005503.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.g003</object-id>
<label>Fig 3</label>
<caption>
<title>The score-effect on action selection and error reduction in the decision-making task (DM) and the reaching task (MO).</title>
<p><bold>(A)</bold> Action change on attempt t+1 (y-axis) following a score (points) received on attempt t (x-axis) in the DM task (red) and MO task (green). Bar plot represents average across points. <bold>(B-C)</bold> Error in <italic>Î±</italic> and <italic>Î²</italic> (y-axis) plotted against the number of attempts (x-axis) in the DM task (B) and MO task (C). Error bars in all panels represent 95%CI across 20 participants.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.g003" xlink:type="simple"/>
</fig>
<p>One pressing question is how the action change looked in terms of <italic>Î±</italic> and <italic>Î²</italic> within and across tasks. To examine this, we first fitted the exponential function, <italic>y</italic> = <italic>ae</italic><sup>â<italic>bx</italic></sup> + <italic>c</italic>, to each participantâs error reduction learning curves (examples shown in <xref ref-type="fig" rid="pcbi.1005503.g002">Fig 2E and 2F</xref>; DM: <italic>R</italic><sup>2</sup> = 0.96 Â± 0.02 [<italic>Î±</italic>], <italic>R</italic><sup>2</sup> = 0.92 Â± 0.20 [<italic>Î²</italic>]; MO: <italic>R</italic><sup>2</sup> = 0.84 Â± 0.20 [<italic>Î±</italic>], <italic>R</italic><sup>2</sup> = 0.80 Â± 0.26 [<italic>Î²</italic>]). Secondly, three two-way (IV1:task = DM vs MO; IV2:dimension =<italic>Î±</italic> vs <italic>Î²</italic>) repeated measures ANOVA were performed for the parameters a, b and c respectively. The results showed that the error reduction rate (b) and plateau (c) were not significantly different across <italic>Î±</italic> and <italic>Î²</italic> within each of the tasks (<xref ref-type="supplementary-material" rid="pcbi.1005503.s005">S1 Table</xref>). In both tasks, the errors in both dimensions were equally weighted to determine the feedback score (<xref ref-type="disp-formula" rid="pcbi.1005503.e003">Eq 2</xref>). Hence, the participants learnt to treat these two dimensions equally in order to achieve maximal points. On the other hand, the error reduction rate (b) and plateau (c) were different across the tasks (<xref ref-type="supplementary-material" rid="pcbi.1005503.s005">S1 Table</xref>). We postulate that this difference was primarily due to the fact that the reaching task required participants to overcome uncertainty involved in the execution of the planned trajectories (motor noise) and the lack of visual information of the executed action that was associated with the feedback score.</p>
</sec>
<sec id="sec005">
<title>Behavioural analysis: Motor noise measurement</title>
<p>To examine the role of motor noise in the explorative motor learning task, we obtained a measure of motor noise for each participant. In the motor noise measurement task, unlike the main motor learning task where the target trajectories were hidden, a series of trajectories was displayed on the screen (red lines in <xref ref-type="fig" rid="pcbi.1005503.g004">Fig 4A</xref>). For each displayed trajectory, the participants were asked to trace it within a specific time window (&gt; 700ms and &lt; 1500ms). Five traces were performed for each trajectory (black lines in <xref ref-type="fig" rid="pcbi.1005503.g004">Fig 4A</xref>).</p>
<fig id="pcbi.1005503.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Motor noise measurement task.</title>
<p><bold>(A)</bold> Data for one representative participant during the motor noise measurement task. The red lines indicate the target trajectories that the participants were asked to trace within a certain time window. The black lines in each panel represent the 5 attempts made by one participant. <bold>(B)</bold> Histograms of the errors in direction and curvature for one representative participant. <bold>(C)</bold> Motor noise in direction and curvature across 24 participants. *p = 0.049. <bold>(D)</bold> The absolute error in the direction and curvature dimensions due to motor noise.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.g004" xlink:type="simple"/>
</fig>
<p>By comparing the direction and curvature parameters of each trace with the target parameters, we obtained one direction error and one curvature error for each trace. Therefore, we had 5 pairs of errors for each target trajectory (5 traces). Each participant was asked to trace 10 target trajectories. Hence, we collected 50 errors in the direction and 50 errors in the curvature (<xref ref-type="fig" rid="pcbi.1005503.g004">Fig 4B</xref>). For each participant, we calculated the standard deviation across the errors in the direction and curvature parameters and used these two standard deviations as our measure of their motor noise in the direction and curvature dimensions, respectively (<xref ref-type="fig" rid="pcbi.1005503.g004">Fig 4B</xref>). As shown in <xref ref-type="fig" rid="pcbi.1005503.g004">Fig 4D</xref>, although participants were encouraged to replicate the trajectories displayed on the screen, the average errors made in both dimensions were significantly larger than zero (Dir: 0.10 Â± 0.05; Cur: 0.12 Â± 0.04).</p>
</sec>
<sec id="sec006">
<title>Behavioural analysis: Exploration and motor noise</title>
<p>Next, we examined how each participantâs level of motor noise correlated with their ability to âfindâ the hidden target trajectory. First, the measure of variance from the motor noise task provided an estimate of how accurate a participant could replicate a planned movement trajectory. During the reaching task, movement variance was initially relatively high as participants explored the space of possible trajectories (including both the exploration variance and motor noise variance) (<xref ref-type="fig" rid="pcbi.1005503.g005">Fig 5A</xref>). However, by the end of each target search movement variance had decreased toward to a level observed in the motor noise task (although still higher than the variance purely due to motor noise). More importantly, we found that the level of variance observed in the motor noise task was negatively associated with motor learning performance across participants. Specifically, we fitted an exponential function, <italic>y</italic> = <italic>ae</italic><sup>â<italic>bx</italic></sup> + <italic>c</italic>, to each participantâs average learning curve across all the targets in the reaching task (<italic>R</italic><sup>2</sup> = 0.94 Â± 0.05). A Pearson correlation indicated that there was a negative correlation between motor noise and the learning rate parameter <italic>b</italic> (r = -0.47, n = 20, p = 0.022; <xref ref-type="fig" rid="pcbi.1005503.g005">Fig 5B</xref>), and maximal points achieved (r = -0.49, n = 20, <italic>p</italic> = 0.015; <xref ref-type="fig" rid="pcbi.1005503.g005">Fig 5C</xref>).</p>
<fig id="pcbi.1005503.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Exploration and motor noise.</title>
<p><bold>(A)</bold> How movement variance in the movements changes during the course of the learning process, compared with the variance observed in the motor noise task. <bold>(B)</bold> Learning rate and <bold>(C)</bold> maximal points achieved plotted against the variance observed in the motor noise task across participants (x-axis). Each dot represents one participant, indexed with the participant ID; Red crosses in (C) are the participants who failed to explore the curvature dimension (concentrated on straight movements with little curvature) and were identified as outliers. The least-squares line (blue dash line) is with the outliers removed.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Model analysis</title>
<p>The main purpose of this study was to test whether explorative motor learning and decision-making could both be understood as a sequential decision process that optimises reward given task, sensory and/or motor uncertainty. To this end, we framed the learning problem as a Partially Observable Markov Decision Process (POMDP) [<xref ref-type="bibr" rid="pcbi.1005503.ref025">25</xref>] and built a computational model to solve (approximately) the defined POMDP. The POMDP framework has been proposed to model a variety of real-world sequential decision problems [<xref ref-type="bibr" rid="pcbi.1005503.ref025">25</xref>â<xref ref-type="bibr" rid="pcbi.1005503.ref030">30</xref>], and provides a general mathematical framework that captures the interaction between an agent and a stochastic environment (<xref ref-type="fig" rid="pcbi.1005503.g006">Fig 6</xref>). It suggests an interpretation of participant behaviour in terms of maximising total expected future reward.</p>
<fig id="pcbi.1005503.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.g006</object-id>
<label>Fig 6</label>
<caption>
<title>An illustration of the model for the decision-making task and the explorative motor learning task.</title>
<p>On each trial, a hidden target is chosen (Environment). That is, the environment is in a state, which is not directly observable. The model starts with an initial uniformly distributed belief state (illustrated with the red arrow on the top right). On each time step, given an belief, the model then chooses an action based on the belief-action value function (Action selection). Subsequently, the action is executed (Execution). Decision-making task actions are performed without motor noise; the model is able to choose the selected action accurately. Reaching actions are performed with motor noise; there is uncertainty between the selected and executed action. Once the action is executed, the environment gives observable feedback (<italic>o</italic><sub><italic>t</italic>â1</sub> = 35 in the figure). The action and observation are then used to update the belief (Bayesian belief update). The update is constrained by the fact that participants were naÃ¯ve to the score function used. We modelled this uncertainty using the likelihood uncertainty parameter (Î; <xref ref-type="disp-formula" rid="pcbi.1005503.e016">Eq 3</xref>). A new cycle then starts with the new belief state (Bt).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.g006" xlink:type="simple"/>
</fig>
<p>An informal description of the decision-making task as a POMDP is given in what follows (for a formal description see the <xref ref-type="sec" rid="sec022">Methods</xref>). There is a set of states, each of which corresponds to an event in which the target is one of the cells in the grid (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1D</xref>). As in the experiment, the task is divided into episodes; each episode consists of 25 time steps (attempts) to find a hidden target cell. On each episode, one of the cells is randomly chosen as the hidden target cell. That is, the environment is in one of the states (Environment; <xref ref-type="fig" rid="pcbi.1005503.g006">Fig 6</xref>); and the state is not directly observable. On each time step within one episode, the model chooses an action (i.e., which cell to click) based on a control strategy so as to maximise the expected future reward (Action selection; <xref ref-type="fig" rid="pcbi.1005503.g006">Fig 6</xref>). After taking an action, the model receives two signals from the environment: an observation and a reward (cost if the value is negative). In our case, the observation and reward are equal, which is the feedback score (points).</p>
<p>Given the defined POMDP, an algorithm can then be used to acquire the optimal control strategy for action selection. In our model, an approximated optimal control strategy was acquired (more details in <xref ref-type="sec" rid="sec022">Methods</xref>). Framing the model as a POMDP allows for the calculation of the optimal policy given the theoretical constraints [<xref ref-type="bibr" rid="pcbi.1005503.ref031">31</xref>]. Constraints include the uncertainty in the sensory input and the uncertain effect of executing an action. The behaviour predicted by the optimal policy is therefore the rational behaviour given the constraints. The POMDP framing thereby serves the goal of drawing a causal relationship between the theoretical constraints and the behaviour (assuming rationality, [<xref ref-type="bibr" rid="pcbi.1005503.ref032">32</xref>]). For the decision-making task, we assumed that participant performance was constrained by the fact they were naÃ¯ve to the underlying equation used to generate the score. In other words, participants were unsure how the current score (received by selecting a certain cell) related to the position of the target cell. This uncertainty was represented in our model by a likelihood uncertainty parameter (Î; <xref ref-type="disp-formula" rid="pcbi.1005503.e016">Eq 3</xref>). Crucially, this was the only free model parameter for the decision-making task. Initially, we ran the model and examined the effect of increasing likelihood uncertainty on learning rate. As shown in <xref ref-type="fig" rid="pcbi.1005503.g007">Fig 7</xref>, a model with a likelihood uncertainty of 1 would find the target after approximately 7 attempts, with increasing uncertainty causing a gradual decline in the speed at which the target was found.</p>
<fig id="pcbi.1005503.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.g007</object-id>
<label>Fig 7</label>
<caption>
<title>The effects of likelihood uncertainty parameter (Î) on the speed at which the target is found.</title>
<p>The models, with a range of likelihood uncertainty values (Î: 1-15), were given the same set of the target cells as the participants. Model predictions show that an increasing amount of likelihood uncertainty caused learning (ability to locate the hidden target and achieve 50 points) to be slower and often incomplete after 25 attempts. Error bars represent model performance variance (95% CI) across the 24 targets. The modelâs performance for each target was averaged over 100 runs.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.g007" xlink:type="simple"/>
</fig>
<sec id="sec008">
<title>Model performance: Learning curve</title>
<p>The best-fitting likelihood uncertainty parameter was found for each participant based on their individual learning curve averaged over 24 targets in the DM task. Specifically, we found the likelihood uncertainty parameter Î which produced a learning curve that best-fit (maximum <italic>R</italic><sup>2</sup>) each participantâs average learning curve (<xref ref-type="fig" rid="pcbi.1005503.g007">Fig 7</xref>). The search range was from 1 to 15; none of the best fits had values at the extreme of this range. <xref ref-type="fig" rid="pcbi.1005503.g008">Fig 8</xref> shows that it was possible to fit all 24 participants learning behaviour in the DM task. Across 24 participants the average <italic>R</italic><sup>2</sup> between the modelâs fitted decision-making learning curves and the participantsâ actual data was 0.95 Â± 0.03. Across participants, the best-fitted likelihood uncertainty value was 8.63 Â± 2.46 (<xref ref-type="supplementary-material" rid="pcbi.1005503.s003">S3</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005503.s004">S4</xref> Figs provide an example of a representative participantâs attempt-by-attempt performance to each target along with the modelâs prediction).</p>
<fig id="pcbi.1005503.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Model predictions for each participantâs learning curves in the DM and MO tasks.</title>
<p>Each panel represents a single participant. Red lines represent the DM data. Green lines represent the MO data. The black and black dashed lines are model predictions for the DM task and the MO task respectively. Each title includes <italic>R</italic><sup>2</sup> for the DM task between the model and data (left), and <italic>R</italic><sup>2</sup> for the MO task between the model prediction and human data (right). The error bars represent 95% CI across 24 targets.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.g008" xlink:type="simple"/>
</fig>
<p>We then built a model to predict each participantâs performance in the MO task. The model used each individualâs likelihood uncertainty (estimated from their DM task performance), and each individualâs noise parameters for direction and curvature (estimated from the motor noise measurement task). Therefore, this model was not fitted to the MO data but predicted it based on parameters derived from the decision-making and motor noise tasks. This predictive model was able to explain 0.63 Â± 0.34 of the variance across the 24 participants (<xref ref-type="fig" rid="pcbi.1005503.g008">Fig 8</xref>). As mentioned, 4 of the participants were considered to be outliers. Having removed these 4 outliers, the model was able to explain 0.76 Â± 0.19 of the remaining 20 participantsâ variance within the MO task.</p>
</sec>
<sec id="sec009">
<title>Model performance: Action change and error reduction</title>
<p>Next we examined model performance in terms of predicting the score-effect on action selection and error reduction across attempts. Similar to participant performance, the model predicted action change to decrease as a function of score in both the DM task (<italic>R</italic><sup>2</sup> = 0.88, <xref ref-type="fig" rid="pcbi.1005503.g009">Fig 9A</xref> left) and the MO task (<italic>R</italic><sup>2</sup> = 0.92, <xref ref-type="fig" rid="pcbi.1005503.g009">Fig 9A</xref> right). The model was also able to predict the error reduction observed across attempts in both dimensions and tasks (<xref ref-type="fig" rid="pcbi.1005503.g009">Fig 9B</xref>, DM: <italic>Î±</italic>: <italic>R</italic><sup>2</sup> = 0.95, <italic>Î²</italic>: <italic>R</italic><sup>2</sup> = 0.93, MO: <italic>Î±</italic>: <italic>R</italic><sup>2</sup> = 0.83, <italic>Î²</italic>: <italic>R</italic><sup>2</sup> = 0.39).</p>
<fig id="pcbi.1005503.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Model performance: The score-effect on action selection and error reduction in the DM and MO tasks.</title>
<p><bold>(A)</bold> Action change on attempt <italic>t</italic> + 1 (related to the action at step <italic>t</italic>, y-axis) following a score (points) received on attempt <italic>t</italic> (x-axis) in the DM (red) task and the MO task (green). The model predictions are also provided (black). <bold>(B)</bold> Error in <italic>Î±</italic> or <italic>Î²</italic> (y-axis) plotted against the number of attempts (x-axis) in the DM task and the MO task. Error bars in all panels represent 95%CI across 20 participants.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.g009" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec010">
<title>Model performance: Predictions for each target</title>
<p>We noticed that participant performance was systematically different across the different targets. For the DM task, we noticed that differences in target cell location appeared to be associated with differences in starting position and learning rate (<xref ref-type="supplementary-material" rid="pcbi.1005503.s001">S1 Fig</xref>). However, plateau performance was similar across target cell locations, perhaps owing to the fact that most participants found most targets within 25 attempts. We examined whether the model captured the variance in performance across the targets. <xref ref-type="supplementary-material" rid="pcbi.1005503.s001">S1 Fig</xref> shows the modelâs predicted learning curves against participantsâ for all 24 target cells and highlights that the model was able to capture the variance across targets (<italic>R</italic><sup>2</sup> = 0.903 Â± 0.05). For the reaching task, participant performance also varied depending on the shape of the target trajectory. <xref ref-type="supplementary-material" rid="pcbi.1005503.s002">S2 Fig</xref> shows that performance varied in most dimensions, including start points, plateaus and the learning rate. The model was able to explain 0.57 Â± 0.3 of the variance across targets. Interestingly, the model predicted faster learning rates and higher plateaus than what was achieved by the participants for a number of trajectories. These trajectories appear harder for participants than for the model. These trajectories had higher curvature than other trajectories and also began on one side of the central line and finished on the opposite side (e.g., trajectory 6,12,13,19 in <xref ref-type="supplementary-material" rid="pcbi.1005503.s002">S2 Fig</xref>). This may indicate that the theoretical assumptions of the model are under constrained.</p>
</sec>
<sec id="sec011">
<title>Model performance: Alternative models for the reaching task</title>
<p>In order to understand the benefits of modelling individual motor noise and individual Gamma (Î, estimated from the DM task) for predicting participant performance in the MO task, we compared our main model (i.e., a model with individual Gamma and individual motor noise) with two alternatives: a model with individual Gamma and average group motor noise (alternative Model 1) and a model with average group Gamma and individual motor noise (alternative Model 2). We used the mean square error (MSE) between each of the modelâs predicted learning curves and the participantsâ actual learning curves to measure model performance. A one-way ANOVA showed that there was a significant difference in MSE across these three models (<italic>F</italic>(2) = 7.47, <italic>p</italic> = 0.002, <italic>Î·</italic><sup>2</sup> = 28.22). Post hoc (2-tailed) paired t-tests indicated that the model using individual Gamma and individual motor noise (MSE = 11.29 Â± 13.09) explained significantly more variance than the alternative Model 1 (MSE = 20.41 Â± 21.18, t(19) = 3.56, p = 0.002, d = 0.80), and the alternative Model 2 (MSE = 26.88 Â± 32.32, t(19) = 2.96, p = 0.008, d = 0.66). However, these two alternative models were not significantly different from one another (t(19) = 1.67, p = 0.11, d = 0.37). This indicates that both the Gamma (Î) and motor noise parameters were important for the model to best predict participant behaviour in the MO task.</p>
</sec>
</sec>
<sec id="sec012">
<title>Behavioural analysis: Decision-making task with âmotor noiseâ</title>
<p>The previous modelling showed that individual performance in the decision-making task (parameter Î) and motor noise task were both critical for predicting individual performance in the reaching task. Next, we examined whether participant performance in the decision-making task would become similar to their performance in the reaching task if their individual âmotor noiseâ was added to the feedback they received during decision-making. We recruited a further 6 participants for Experiment 2. In this experiment, we asked each participant to complete the same reaching and motor noise task as in the previous experiment. However, for the decision-making task, the feedback score provided after each attempt (i.e., clicking on a cell) now included noise parameters that were equivalent to the level of noise/uncertainty observed in the motor noise task for each participant (DM+noise). For example, when a cell [<italic>Î±</italic><sub>1</sub>, <italic>Î²</italic><sub>1</sub>] is selected and the target is [<italic>Î±</italic><sub><italic>T</italic></sub>, <italic>Î²</italic><sub><italic>T</italic></sub>], the feedback score is determined by two errors: |<italic>Î±</italic><sub>1</sub> â <italic>Î±</italic><sub><italic>T</italic></sub>| + <italic>noise</italic><sub><italic>Î±</italic></sub> and |<italic>Î²</italic><sub>1</sub> â <italic>Î²</italic><sub><italic>T</italic></sub>| + <italic>noise</italic><sub><italic>Î²</italic></sub>, instead of |<italic>Î±</italic><sub>1</sub> â <italic>Î±</italic><sub><italic>T</italic></sub>| and <italic>Î²</italic><sub>1</sub> â <italic>Î²</italic><sub><italic>T</italic></sub>| as in the previous experiment. Two motor noise parameters: <italic>noise</italic><sub><italic>Î±</italic></sub> and <italic>noise</italic><sub><italic>Î²</italic></sub> were measured in the motor noise task. <xref ref-type="fig" rid="pcbi.1005503.g010">Fig 10</xref> shows that participant learning in the decision-making task with âmotor noiseâ (DM+noise) and the reaching task (MO) was now identical (<italic>R</italic><sup>2</sup> = 0.88, rmse = 2.64, <xref ref-type="fig" rid="pcbi.1005503.g010">Fig 10B</xref>). Once again, three two-way repeated measures ANOVAs were conducted on fitted exponential parameters a, b and c. Unlike Experiment 1, we found that the error reduction was not significantly different either across <italic>Î±</italic> and <italic>Î²</italic> or across tasks (<xref ref-type="fig" rid="pcbi.1005503.g010">Fig 10C</xref>; <xref ref-type="supplementary-material" rid="pcbi.1005503.s006">S2 Table</xref>)</p>
<fig id="pcbi.1005503.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Experiment 2: Decision-making with motor noise.</title>
<p><bold>(A)</bold> Learning curves for the DM+noise (red) and MO (green) tasks for each participant. The <italic>R</italic><sup>2</sup> between the DM+noise and MO task is provided. <bold>(B)</bold> Average learning curves across 6 participants. <bold>(C)</bold> Error in <italic>Î±</italic> and <italic>Î²</italic> (y-axis) plotted against the number of attempts (x-axis) in the DM+noise and the MO task. Error bars in all panels represent 95%CI across participants.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.g010" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec013">
<title>Gains and losses</title>
<p>As we said in the introduction, we were also interested in whether trial-by-trial motor performance could be predicted as a function of gains and losses. It has been suggested by a number of authors that the effects of gains and losses maybe be elucidated via trial-by-trial analysis of choice behaviour, as the outcomes of previous choices have been shown to affect subsequent decisions [<xref ref-type="bibr" rid="pcbi.1005503.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref034">34</xref>]. For example, in a sequential tree search task, it has been shown that participants are more likely to curtail any further evaluation of a branch as soon as they encountered a large loss [<xref ref-type="bibr" rid="pcbi.1005503.ref035">35</xref>]. Another example of these local influences on choice behaviour is a tendency to repeat the same behaviour following a gain, coupled with a bias to switching behaviour after a loss [<xref ref-type="bibr" rid="pcbi.1005503.ref036">36</xref>].</p>
<p>In Experiment 1, gains and losses are operationalised as positive and negative feedback. Here, we examine the degree of action change on attempt <italic>t</italic> + 1 after receiving a certain score on attempt <italic>t</italic>. As mentioned (page 6), the action change was defined as the Euclidean distance between two actions. First, we compared action changes between positive and negative feedback conditions. The action change following a score of 10 (10 points above the minimum point 0) in the positive condition was compared to the action change following -40 (10 points above the minimum point -50) in the negative condition. Paired t-tests revealed no significant difference between positive and negative conditions for either the DM task (t(23) = -1.00, p = 0.32, d = -0.21; Bars in <xref ref-type="fig" rid="pcbi.1005503.g011">Fig 11</xref>) or the MO task (t(23) = -0.26, p = 0.79, d = -0.05; Bars in <xref ref-type="fig" rid="pcbi.1005503.g011">Fig 11</xref>). Model predictions were given in <xref ref-type="fig" rid="pcbi.1005503.g009">Fig 9</xref>.</p>
<fig id="pcbi.1005503.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Action change following positive and negative feedback.</title>
<p>Action change on attempt (<italic>t</italic> + 1) following a score on attempt <italic>t</italic> in the positive (pink circle) and negative (blue cross) feedback conditions for the DM (left) and MO task (right). Mean action change for each participant for the positive condition (pink bar with circle) and the negative condition (blue bar with cross) in the DM task (left) and the MO task (right).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.g011" xlink:type="simple"/>
</fig>
<p>Next, we considered whether gains and losses are better measured relative to a reference point. Prospect theory suggests that gains and losses are measured relative to a reference point that may shift with recent experience [<xref ref-type="bibr" rid="pcbi.1005503.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref038">38</xref>]. It follows that within the current study, participants may have thought of gains and losses relative to the best score achieved so far while searching for the current target. For example, a participant who received a score of 22 on their 8th attempt might see this as a loss of 13 given that their best score so far (on attempt 4) was 35. Therefore, during the 25 attempts, the current best score could be thought of as the current reference point. A score that was better than the reference point can be defined as a gain, and a score worse than this reference point a loss.</p>
<p>Participant data was pooled across the positive and negative feedback conditions by transforming a negative score into its positive equivalent. We investigated action change on attempt <italic>t</italic> + 1 as a function of the maximum points achieved up to <italic>t</italic> â 1 (the reference point). A gain was a score that was better than the reference point on attempt <italic>t</italic>, and a loss was a score that was worse or equal to the reference point on attempt <italic>t</italic> (<xref ref-type="fig" rid="pcbi.1005503.g012">Fig 12</xref>). Paired t-tests indicated that the action change following a loss was statistically greater than the action change following a gain in both the DM task (<italic>t</italic>(23) = 11.39, <italic>p</italic> &lt; 0.001, <italic>d</italic> = 2.32; Bars in <xref ref-type="fig" rid="pcbi.1005503.g012">Fig 12</xref> Left) and MO task (<italic>t</italic>(23) = 12.18, <italic>p</italic> &lt; 0.001, <italic>d</italic> = 2.49; Bars in <xref ref-type="fig" rid="pcbi.1005503.g012">Fig 12</xref> Right). The model predicted this behaviour in both the DM task (<italic>R</italic><sup>2</sup> = 0.91, RMSE = 0.13; <xref ref-type="fig" rid="pcbi.1005503.g012">Fig 12</xref>) and MO task (<italic>R</italic><sup>2</sup> = 0.96, RMSE = 0.10; <xref ref-type="fig" rid="pcbi.1005503.g012">Fig 12</xref>).</p>
<fig id="pcbi.1005503.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Action change relative to the current highest score (the reference point).</title>
<p>Action change on attempt <italic>t</italic> + 1 was plotted as a function of the maximum points achieved up to <italic>t</italic> â 1 (the reference point) for the DM task (left panel) and MO task (right panel). If the score on attempt <italic>t</italic> was greater than the reference point, then the action change at <italic>t</italic> + 1 was considered as an action after a gain (black circle). If the score on attempt <italic>t</italic> was smaller than the reference point, then the action change after this score (<italic>t</italic> + 1) was considered as an action after a loss (black cross). Model (green) predictions are also provided. The bars represent the mean action change in the DM task (left panel) and the MO task (right panel). Error bars represent 95% CI across participants.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.g012" xlink:type="simple"/>
</fig>
<p>This suggests that participant sensitivity to gains and losses was possibly independent of the positive and negative feedback conditions but in fact related to a shifting reference frame determined by their current best score.</p>
</sec>
</sec>
<sec id="sec014" sec-type="conclusions">
<title>Discussion</title>
<sec id="sec015">
<title>Summary</title>
<p>Our goal was to examine whether explorative motor learning [<xref ref-type="bibr" rid="pcbi.1005503.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref014">14</xref>] and decision-making could be modelled as the (approximately) optimal solution to a Partially Observable Markov Decision Process [<xref ref-type="bibr" rid="pcbi.1005503.ref025">25</xref>] bounded by noisy neural information processing. To achieve this, we studied performance during an explorative motor learning task [<xref ref-type="bibr" rid="pcbi.1005503.ref013">13</xref>] and a decision-making task which had a similar underlying structure with the exception that it was not subject to motor noise. The solution to the defined POMDP explained 0.94 of the variance in the decision-making task, and 0.76 of the variance in the explorative motor learning task. Importantly, we did not fit the model to the motor learning data but predicted it based on parameters derived from the decision-making task and a separate motor noise task. In addition, the model was also able to explain (1) varying performance across different target trajectories, (2) the magnitude of action change after different scores, and (3) the differences in the magnitude of action change between gains and losses.</p>
</sec>
<sec id="sec016">
<title>Explorative motor learning and decision-making</title>
<p>A key contribution of the work reported here is to furthering our understanding of the relationship between motor learning and decision making. In the reported studies, the decision-making task allowed measurement of a participantâs ability to make use of information in previous attempts. Participants with a high likelihood uncertainty were less able to integrate this information and were slower learners. Participants with a low likelihood uncertainty were more able to integrate information and were faster learners. Given an equivalent level of motor noise, participants who were faster learners in the decision-making task were also faster learners in the reaching task.</p>
<p>We can draw this conclusion because of the modelling approach that we used. We used the likelihood uncertainty parameter estimated from the decision-making task, and the individual motor noise estimated from the motor noise task, to predict motor learning behaviour. Importantly, we found that the model performed significantly worse when averaged parameters (across all participants) were used rather than parameters derived from each individualâs behaviour. This suggests that taking into account individual performance during both the decision-making and motor noise tasks was important for explaining behaviour during the explorative motor learning task. Finally, we showed that performance during the decision-making task was similar to performance in the reaching task if motor noise was added to the decision-making taskâs feedback. This provides strong empirical evidence for the predicted relationship between explorative motor learning, decision-making and motor noise within our model.</p>
<p>Although the decision-making task was designed to have a similar underlying structure to the reaching task there were still differences. For example, unlike the explicit visual cues of orthogonally organised actions in the decision-making task, the relationship between the two parameters was less intuitive in the reaching task. It is possible that this could have led to these parameters being treated more dependently in the reaching task. For instance, the errors of these two parameters were correlated during the âmotor noiseâ task (<italic>r</italic> = 0.52, <italic>p</italic> = 0.008). However, Dam et al., (2013) [<xref ref-type="bibr" rid="pcbi.1005503.ref013">13</xref>], who used a near identical motor learning task, showed that participants were able to isolate direction and curvature so that they only altered the parameter being currently rewarded. We believe our results suggest that participants treated the parameters in a similar fashion within the reaching task and decision-making task. For example, the rate of error reduction for the two dimensions was similar within each task, indicating that participants explored both parameters simultaneously, while also implying a comparable strategy across both tasks. Another potential difference was that there were clearly defined discrete action options (grid-design) in the decision-making task. It has previously been shown that there are limits to the sensory and motor systemâs ability to distinguish endless continuous options [<xref ref-type="bibr" rid="pcbi.1005503.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref043">43</xref>]. For example, our ability to distinguish two shades of grey is limited rather than continuous. This suggests that the motor learning task may also have involved a set of discrete action options. However, our tasks were not designed to measure participant ability to distinguish between trajectories that varied in direction and curvature. Therefore, it was not possible to define what these discrete action options could have been during the motor learning task. Future work could examine whether the ability of the decision-making task to predict explorative motor learning is improved by creating a grid-size which directly reflected participantâs ability to distinguish trajectories with different curvature and directions.</p>
<p>It was also clear that our model did not fully explain motor learning behaviour. For example, the model predicted faster learning rates and higher plateaus than what was achieved by the participants for a number of trajectories in the reaching task. These trajectories had large amounts of curvature and also began on one side of the central line and finished on the opposite side. These elements appeared to make the trajectories more difficult for the participants than the model. One might argue that these types of trajectories are less likely to be performed in everyday life and therefore are more difficult to find through exploration [<xref ref-type="bibr" rid="pcbi.1005503.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref045">45</xref>]. Alternatively, such âtwo-directionâ movements may be more difficult to execute. To improve the modelâs performance, future work could utilise a more sensitive measure of motor noise by obtaining a curvature and direction noise measurement for each of the trajectories examined.</p>
</sec>
<sec id="sec017">
<title>Exploration and motor noise for motor learning</title>
<p>Variability in movement is a fundamental component in motor behaviour. It is caused by numerous factors including planning, sensory and neuromuscular noise [<xref ref-type="bibr" rid="pcbi.1005503.ref046">46</xref>]. Researchers often categorise variability into two sources: exploration and motor noise. Exploration represents the variability which results from âintentionalâ exploration of different actions [<xref ref-type="bibr" rid="pcbi.1005503.ref015">15</xref>]. While motor noise represents the variability observed when attempting to repeat a single action [<xref ref-type="bibr" rid="pcbi.1005503.ref024">24</xref>]. Previous work has examined the differential role of exploration and motor noise in motor learning [<xref ref-type="bibr" rid="pcbi.1005503.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref046">46</xref>]. For example, Wu et al., (2014) found a positive relationship between motor noise and motor learning [<xref ref-type="bibr" rid="pcbi.1005503.ref014">14</xref>]. This is in contrast to our, and others [<xref ref-type="bibr" rid="pcbi.1005503.ref015">15</xref>], results which showed a strong negative relationship between motor noise and the rate of motor learning. However, this difference in findings might be explained by differences in experimental design. Whereas Wu et al.âs (2014) participants were provided with visual guidance for a trajectory (which differed from the reward trajectory), our participants were provided with a reward signal but not provided with visual guidance. In Wu et al.âs (2014) task success was based on the similarity between a participantâs attempted path and a reward shape that was similar but independent from the guided shape. Therefore, participants would have initially attempted to execute the guided shape; individuals with higher motor noise would deviate from this shape to a greater degree and thus be more likely to find the underlying rewarded shape. In contrast, in our study learning was achieved by determining the relationship between attempted actions and their associated points where the points were based on the executed action not the intended action. As motor noise represented the gap between the intended and executed action, the greater the motor noise, the more difficult and slower the process of learning the relationship between actions and points was likely to be. He et al., (2016) have shown that motor variability can have a complicated relationship with learning: positive, negative and neutral [<xref ref-type="bibr" rid="pcbi.1005503.ref046">46</xref>]. They emphasised that it is important to consider the relationship between motor noise and learning in a task-specific manner [<xref ref-type="bibr" rid="pcbi.1005503.ref046">46</xref>].</p>
</sec>
<sec id="sec018">
<title>Neural basis of explorative learning</title>
<p>Defining explorative motor learning as a sequential decision-making task suggests that this form of motor learning could be dependent on brain areas more commonly associated with cognitive decision-making such as the frontal cortex and basal ganglia [<xref ref-type="bibr" rid="pcbi.1005503.ref047">47</xref>â<xref ref-type="bibr" rid="pcbi.1005503.ref054">54</xref>]. However, it is unclear how varying levels of motor noise alter this type of âcognitiveâ learning. Although with a very different task, it has recently been shown that explorative motor learning is impaired in patients with cerebellar damage who show increased levels of motor noise [<xref ref-type="bibr" rid="pcbi.1005503.ref015">15</xref>]. One suggestion is that the cerebellum predicts the sensory state of an action and feeds it to the basal ganglia [<xref ref-type="bibr" rid="pcbi.1005503.ref055">55</xref>] or frontal cortex [<xref ref-type="bibr" rid="pcbi.1005503.ref056">56</xref>], which in turn estimates the value of the new state through reinforcement processes. Without the cerebellum, predicted action outcomes may be poorly represented, or even unknown, and so linking them to reward values would be more difficult. This increased (motor) noise in predicting movement outcomes could lead to greater uncertainty with respect to reward based predictions and thus a reduced ability, or reluctance, to adapt behaviour [<xref ref-type="bibr" rid="pcbi.1005503.ref057">57</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref058">58</xref>]. Although we are not suggesting that participants in our study, who displayed increased motor noise had a damaged cerebellum, such a neural mechanism could readily explain our results.</p>
</sec>
<sec id="sec019">
<title>Gains and losses</title>
<p>Although a great deal of research has investigated cognitive decision-making [<xref ref-type="bibr" rid="pcbi.1005503.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref059">59</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref060">60</xref>], only recently have researchers begun to examine motor-based decision-making [<xref ref-type="bibr" rid="pcbi.1005503.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref019">19</xref>]. Some studies have shown humans perform optimally when making motor decisions, in contrast with markedly sub-optimal and biased performance in economic decision-making [<xref ref-type="bibr" rid="pcbi.1005503.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref061">61</xref>]. In contrast, other studies have revealed similar sub-optimal behaviour across motor and cognitive decision-making tasks [<xref ref-type="bibr" rid="pcbi.1005503.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref062">62</xref>]. One of the most influential findings in decision-making is that people behave asymmetrically with gains and losses (e.g., loss aversion, [<xref ref-type="bibr" rid="pcbi.1005503.ref018">18</xref>]). When initially comparing performance with positive or negative feedback, we observed no observable differences. However, one of the difficulties in defining gains and losses during a dynamic learning process is that the definition of gains and losses is highly dependent on previous experience [<xref ref-type="bibr" rid="pcbi.1005503.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref037">37</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref063">63</xref>]. This is known as a reference point in economic decision-making tasks, with the value and importance of the reference point being altered by task instructions and feedback [<xref ref-type="bibr" rid="pcbi.1005503.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref063">63</xref>]. For example, providing a running total on the screen causes participants to make choices based on this reference point rather than making an independent decision based on the current trial [<xref ref-type="bibr" rid="pcbi.1005503.ref037">37</xref>]. Therefore, we decided to collapse the data across positive and negative feedback and instead look at participant behaviour in terms of whether a trial was better (gain) or worse (loss) than the maximum (best) achieved so far. This reference point was chosen as it reflected the instructions of the task. By defining the score relative to this dynamic reference point, we found participants made substantially larger changes in behaviour following a loss compared to a gain. This suggests that when comparing positive (reward) and negative (punishment) feedback in motor learning [<xref ref-type="bibr" rid="pcbi.1005503.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref064">64</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref065">65</xref>] a clear understanding of the reference point being used by the participants is crucial. In other words, it should not be assumed that positive feedback is always a gain and negative feedback is always a loss [<xref ref-type="bibr" rid="pcbi.1005503.ref037">37</xref>]. Interestingly, our model shows that the asymmetric response of participants to gains and losses is not irrational. Rather, participants are responding optimally given their likelihood and motor uncertainty. Essentially, what the model does is discover the bounded optimality of the loss-shift/win-stay heuristic.</p>
</sec>
<sec id="sec020">
<title>Partially Observable Markov Decision Process</title>
<p>The reported model builds on preceding work that has explored the use of POMDP, and related models, for explaining various aspects of human decision-making [<xref ref-type="bibr" rid="pcbi.1005503.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref039">39</xref>â<xref ref-type="bibr" rid="pcbi.1005503.ref041">41</xref>]. Framing the model as a POMDP allowed us to calculate the (approximately) optimal strategy given certain pre-defined constraints. It is this calculation that allows the model to make predictions of behaviour in the reaching task given parameters set to the decision-making task and the motor noise task. In other words, the strategy is determined by the optimisation and not by the theorist picking a strategy that fits the data. The behaviour predicted by the optimal policy is therefore the rational behaviour given the constraints. The POMDP framing thereby serves the goal of drawing a causal relationship between the theoretical constraints and the behaviour (assuming rationality, [<xref ref-type="bibr" rid="pcbi.1005503.ref032">32</xref>]). If theorists are to progress in explaining human behaviour then they must move away from fitting models to the data that they are trying to explain [<xref ref-type="bibr" rid="pcbi.1005503.ref031">31</xref>]. The POMDP framing supports such a move.</p>
<p>One advantage of a POMDP framing is that models framed in this way are readily falsifiable. The modelling involves specifying the states, actions, observations and rewards (a POMDP problem), computing the optimal policy, and comparing the predicted behaviour to the observed human behaviour. If there are discrepancies between the predictions and the observed human behaviour, alternate theories of the constraints can be explored, the model can be refined, and the process repeated. This iterative process leads to the assertion of a set of theoretical constraints that would lead a rational human generating the observed behaviours [<xref ref-type="bibr" rid="pcbi.1005503.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref066">66</xref>].</p>
<p>However, in practice, POMDPs are often computationally intractable to solve exactly. Computer scientists have developed methods that approximate solutions for POMDPs. More recent work has made use of sampling techniques, generalisation techniques and the exploitation of the problemâs structure to extend POMDP into large domains with millions of states [<xref ref-type="bibr" rid="pcbi.1005503.ref067">67</xref>]. Importantly, this work provides a basis to attempt more comprehensive techniques in order to better approximate the optimal solution for this task.</p>
</sec>
<sec id="sec021">
<title>Conclusion</title>
<p>In conclusion, we modelled behaviour during an explorative motor learning task and a decision-making task with similar underlying structure using a Partially Observable Markov Decision Process (POMDP). The model was able to predict performance in motor learning by using parameters estimated from the decision-making task and a separate motor noise task. This suggests that explorative motor learning could be considered as a sequential decision-making process that is adjusted for motor noise. This work reinforces the view that the mechanisms which control decision-making and motor behaviour are highly integrated and raises interesting questions regarding the neural origin of explorative motor learning.</p>
</sec>
</sec>
<sec id="sec022">
<title>Methods and models</title>
<sec id="sec023">
<title>Ethics statement and subjects</title>
<p>The study was approved by Ethical Review Committee of the University of Birmingham, UK, and was in accordance with the declaration of Helsinki. Written informed consent was obtained from all participants. Participants were recruited through online advertising and received monetary compensation upon completion of the study. Thirty-two healthy individuals were recruited for the two experiments. All were naÃ¯ve to the task, had normal/corrected vision, and reported to have no history of any neurological condition (Mean age: 26.46 Â± 5.96; 17 females; 25 right handed). Twenty-six participants participated in Experiment 1 (however, one withdrew during the experiment due to personal reasons; another one did not finish the experiment due to equipment malfunction). Six new individuals participated in Experiment 2.</p>
</sec>
<sec id="sec024">
<title>Experiment 1</title>
<sec id="sec025">
<title>Procedure</title>
<p>All participants completed both the motor learning (reaching) task and the decision-making task; the order was counterbalanced across the participants. In addition, all participants completed the motor noise measurement task prior to the main motor learning task.</p>
</sec>
<sec id="sec026">
<title>Explorative motor learning task</title>
<p>Participants were seated with their heads supported by a chin-rest (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1A</xref>), looking down at a horizontal (65cm Ã 40cm) mirror, which reflected task-related stimuli from a computer screen. The mirror blocked direct observation of the index finger, which was instead represented on the mirror via a circular green cursor (0.25cm diameter). Index finger position was recorded at a sampling rate of 120Hz by a Fastrak motion tracking system (Polhemus, USA) through a custom Matlab (Mathworks, USA) program. Participants were asked to draw trajectories (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1B</xref>) by sliding their index finger from a central start position (1cm<sup>2</sup> square) across the surface of a desk towards a target line positioned 15cm (Y-direction) in front of the start position (the thick black line in <xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1B</xref>). This target line had a length of 50cm (X-direction), with the end of movement being defined as the point at which the index finger (represented by the green cursor) hit this line. Each participant attempted to match 24 different, invisible target trajectories that varied in both direction and curvature [<xref ref-type="bibr" rid="pcbi.1005503.ref013">13</xref>].</p>
<p>All participants experienced the same set of 24 target trajectories that were given in a random order (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1C</xref>). The shapes of the trajectories were defined by:
<disp-formula id="pcbi.1005503.e002"><alternatives><graphic id="pcbi.1005503.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mrow><mml:mi>x</mml:mi> <mml:mo>=</mml:mo> <mml:mi>Î±</mml:mi> <mml:mi>y</mml:mi> <mml:mo>+</mml:mo> <mml:mi>Î²</mml:mi> <mml:mi>s</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mo>(</mml:mo> <mml:mi>Ï</mml:mi> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
where the y-coordinates (Y) represented the reaching depth, which was 15 cm, and the x-coordinates (X) of the trajectory were determined by two parameters, direction (<italic>Î±</italic>) and curvature (<italic>Î²</italic>). The 24 target trajectories were formed by a Cartesian product of <italic>A</italic> = [-0.8,-0.4,0.4,0.8] and <italic>B</italic> = [-0.9,-0.6,-0.3,0.3,0.6,0.9]. The Cartesian product includes a set of 24 ordered pairs (<italic>Î±</italic>, <italic>Î²</italic>), where <italic>Î±</italic> â <italic>A</italic>, <italic>Î²</italic> â <italic>B</italic>, each of which is a target parameter pair. All target trajectories were confined within a quadrangular table-top space of 46.5cm in width and 15cm in depth (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1C</xref>). Participants were permitted to generate trajectories within a space of 50cm in width and 15cm in depth, with this being defined by an outer white square displayed on the mirror (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1A and 1B</xref>). Importantly, these 24 targets were generated to be evenly distributed across the workspace, so that learning was minimally affected by a target location bias (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1C</xref>).</p>
<p>Participants made 25 attempts to approximate each desired target trajectory (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1B</xref>). Each attempted trajectory resulted in a score that indicated the proximity of the attempted trajectory to the target trajectory. The 24 target trajectories were randomly divided into two feedback conditions (12 of each): a positive feedback condition and a negative feedback condition. In the positive feedback condition, points ranged from 0 to 50, with greater magnitude indicating greater similarity between the attempted and target trajectory. In the negative feedback condition, points ranged from â50 to 0, with greater magnitude indicating reduced similarity between the attempted and target trajectory. Hence, the goal for the positive feedback condition was to achieve 50 points, whereas for the negative feedback condition it was to achieve 0 points (i.e., avoiding losing points). These points were directly related to monetary incentive (2 points were equivalent to 1 pence). Participants were told which of the two feedback conditions they were in at any time. Participants were informed that they would receive the highest reward or lowest punishment that they achieved from the trajectory attempts for each target trajectory. Each target search was terminated by either finding the hidden trajectory, and so obtaining a maximal score, or by reaching attempts. Movement duration was defined as the time between the cursor leaving the start position and it hitting the target line (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1B</xref>). If the movement duration was &lt; 700ms or &gt; 1500ms, the attempt was deemed invalid, leading to the score being withheld and the trial being repeated. Hence, a total of 25 valid attempts were permitted per target trajectory. After each valid attempt, participants were presented with the feedback. Positive feedback was presented in yellow text as âYou won xx points.â, while negative feedback was presented in red text as âYou lost xx pointsâ. For invalid trials, no score feedback was provided and instead the home square turned from white to either red (if the movement was too slow) or green (if the movement was too fast).</p>
<p>To determine the score, each attempted trajectory was fitted to <xref ref-type="disp-formula" rid="pcbi.1005503.e002">Eq 1</xref> in order to obtain an estimate of its direction and curvature parameters. The error in the direction was: Î<italic>Î±</italic> = |<italic>Î±</italic><sub><italic>target</italic></sub> â <italic>Î±</italic><sub><italic>attempt</italic></sub>|, and the error in curvature was: Î<italic>Î²</italic> = |<italic>Î²</italic><sub><italic>target</italic></sub> â <italic>Î²</italic><sub><italic>attempt</italic></sub>|. The feedback score (<xref ref-type="disp-formula" rid="pcbi.1005503.e003">Eq 2</xref>) was determined by the total error (<italic>Îµ</italic>) of the attempt along both dimensions: <italic>Îµ</italic> = 1 â (0.5 Ã Î<italic>Î±</italic> + 0.5 Ã Î<italic>Î²</italic>) [<xref ref-type="bibr" rid="pcbi.1005503.ref013">13</xref>]. Therefore, the score (points) in the positive feedback condition were integers in [0, 50]. The score (points) in the negative feedback condition were integers in [-50,0].
<disp-formula id="pcbi.1005503.e003"><alternatives><graphic id="pcbi.1005503.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mrow><mml:mi>S</mml:mi> <mml:mo>=</mml:mo> <mml:mfenced close="" open="{" separators=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mn>50</mml:mn> <mml:mo>Ã</mml:mo> <mml:mi>Îµ</mml:mi> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>condition = positive</mml:mtext></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>-</mml:mo> <mml:mo>(</mml:mo> <mml:mn>50</mml:mn> <mml:mo>-</mml:mo> <mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mn>50</mml:mn> <mml:mo>Ã</mml:mo> <mml:mi>Îµ</mml:mi> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>condition = negative</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula></p>
</sec>
<sec id="sec027">
<title>Motor noise measurement task</title>
<p>Unlike the main learning task where the target trajectories were hidden, a series of trajectories was displayed on the screen. All participants experienced the same set of 10 trajectories which were given in a random order. As in the main motor learning task, each trajectory was defined by a parameter pair (<italic>Î±</italic>, <italic>Î²</italic>) as in <xref ref-type="disp-formula" rid="pcbi.1005503.e002">Eq 1</xref>. These trajectories were formed by a Cartesian product of <italic>A</italic> = [-0.7,0.7] and <italic>B</italic> = [-0.8,-0.4,0,0.4,0.8], which contains a set of 10 ordered parameter pairs (<italic>Î±</italic>, <italic>Î²</italic>), where <italic>Î±</italic> â <italic>A</italic>, <italic>Î²</italic> â <italic>B</italic>. These trajectories were not repeated during the main experiment. During this task, the participants could see a circular green cursor that tracked the index finger.</p>
<p>For each displayed trajectory, the participants were asked to trace it within a time window that was identical to the one used in the main experiment (&gt; 700ms and &lt; 1500ms). Only the movements that were within this time window were deemed valid traces. No score feedback was provided during the motor noise task. For each invalid trace, the home square turned from white to either red (if the movement was too slow) or green (if the movement was too fast). Five valid traces were performed for each trajectory (<xref ref-type="fig" rid="pcbi.1005503.g004">Fig 4A</xref>).</p>
<p>To measure the execution error due to motor noise, each valid trace was fitted to <xref ref-type="disp-formula" rid="pcbi.1005503.e002">Eq 1</xref> to obtain a pair of estimated direction and curvature parameters. By comparing the estimated direction and curvature parameters with the target parameters, we obtained one direction error and one curvature error for each valid trace. Therefore, we had 5 pairs of errors for each target (5 valid traces). Each participant was asked to trace 10 target trajectories. Hence, we collected 50 errors in the direction and 50 errors in the curvature. For each participant, we calculated the standard deviation across the errors in the direction and curvature parameters and used these two standard deviations as our measure of their motor noise in the direction and curvature dimensions, respectively.</p>
</sec>
<sec id="sec028">
<title>Decision-making task</title>
<p>Participants in this task were interacting with an interface using a computer mouse. The interface was designed using Matlab and displayed on a desktop PC. The interface consisted of a two-dimensional grid, in which there were 21 Ã 21 cells (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1D</xref>). The horizontal and vertical dimension were defined with two parameters: <italic>Î±</italic> and <italic>Î²</italic> respectively, akin to the direction and curvature parameters in the reaching task. Both parameters ranged from -1 to 1 with 0.1 increments. The parameter values were assigned to the cells in a spatially ordered manner. Specifically, the cells in the same row had the same <italic>Î²</italic> values, but with <italic>Î±</italic> values ordered from -1 to 1 with an 0.1 increment (from left to right, <xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1D</xref>); the cells in the same column had the same <italic>Î±</italic> values, but with <italic>Î²</italic> values ordered from -1 to 1 with an 0.1 increment (from bottom to top, <xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1D</xref>). Therefore, each cell of the grid corresponded to a unique combination of the two parameters. When one of the cells (i.e., one parameter pair) was chosen as a target cell, the score associated with each of the cells was then calculated based on <xref ref-type="disp-formula" rid="pcbi.1005503.e003">Eq 2</xref>. That is, the score for decision-making task was calculated using the same function as in the reaching task. Once a cell was chosen (mouse-clicked), an associated score would appear in the feedback window at the top of the screen.</p>
<p>Participants were asked to search for a series of 24 hidden target cells by exploring the cells on the grid. The participants were told that the rows and columns represented different values in an ordinal space. Similar to the reaching task, the 24 target cells were randomly divided into two feedback conditions (12 of each): a positive feedback condition and a negative feedback condition. Again participants were informed that they would receive the highest reward or lowest punishment that they achieved from the 25 attempts for each target cell. Each target search was terminated by either finding the hidden target cell, and so obtaining a maximal score, or by reaching 25 attempts. The 24 target cells were formed by the ordered pairs [<italic>Î±</italic>, <italic>Î²</italic>], where <italic>Î±</italic> â <italic>A</italic>, <italic>A</italic> = [-1,-0.5,0.5,1], <italic>Î²</italic> â <italic>B</italic>, <italic>B</italic> = [-1,-0.6,-0.2,0.2,0.6,1]. These pairs were intentionally different to the reaching task however covered the same workspace where both the two parameters ranged from -1 to 1 with 0.1 increments.</p>
<p>Therefore, a target cell in the decision-making task could be considered identical to a target trajectory in the reaching task as they were both defined by a combination of <italic>Î±</italic> and <italic>Î²</italic> within a similar workspace. In addition, the score for the decision-making task was calculated using the same function as in the reaching task (<xref ref-type="disp-formula" rid="pcbi.1005503.e003">Eq 2</xref>). Therefore, we believe this provided two explorative learning tasks that were analogous except that the decision-making task did not involve motor uncertainty/noise between the planned and executed behaviours.</p>
</sec>
<sec id="sec029">
<title>Payoff scheme</title>
<p>At the onset of the experiment, participants were given Â£5 in cash. They were informed that their final payoff would be increased by the positive points obtained across both tasks, and decreased by the negative points across both tasks. Although participants were informed of their best score at the end of each target trajectory/cell, they were not given a running total until the end of the experiment. The experiment lasted around 90 minutes. Their average points achieved for the decision-making task was 595 Â± 8.71, and 469 Â± 81.0 in the reaching task. The average total payment was Â£10.32 Â± Â£0.42.</p>
</sec>
</sec>
<sec id="sec030">
<title>Experiment 2</title>
<p>All participants first completed the motor noise measurement task. This task was the same as in Experiment 1. Participants then completed the motor learning (reaching) task, followed by a different version of the decision-making task (i.e., with motor noise added into the feedback score). The reaching task was the same as in Experiment 1 except that the participants were asked to find half of the target trajectories (i.e. 12) with positive feedback (points ranged from 0 to 50). That is, only the positive condition in Experiment 1 was replicated in Experiment 2. These 12 target trajectories were formed by [<italic>Î±</italic>, <italic>Î²</italic>] pairs shown in <xref ref-type="table" rid="pcbi.1005503.t002">Table 2</xref>.</p>
<table-wrap id="pcbi.1005503.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005503.t002</object-id>
<label>Table 2</label>
<caption>
<title>Target parameters used in the MO and the DM+noise task in Experiment 2.</title>
</caption>
<alternatives>
<graphic id="pcbi.1005503.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" style="border-bottom:thick">Targets</th>
<th align="left" style="border-bottom:thick">MO</th>
<th align="left" style="border-bottom:thick">DM+noise</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[<italic>Î±</italic>, <italic>Î²</italic>]</td>
<td align="left">[-0.8,-0.6], [-0.8,-0.3], [-0.8, 0.3]<break/>[-0.4,-0.6], [-0.4,-0.3], [-0.4, 0.3]<break/>[0.4,-0.3], [0.4, 0.3], [0.4, 0.6]<break/>[0.8,-0.3], [0.8, 0.3], [0.8, 0.6]</td>
<td align="left">[-0.8,-0.6], [-0.8,-0.3], [-0.8, 0.3]<break/>[-0.4,-0.6], [-0.4,-0.3], [-0.4, 0.3]<break/>[0.4,-0.6], [0.4,-0.3], [0.4, 0.3]<break/>[0.8,-0.6], [0.8,-0.3], [0.8, 0.3],</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>In the decision-making task, participants were asked to find 12 hidden target cells (<xref ref-type="table" rid="pcbi.1005503.t002">Table 2</xref>) with positive feedback (points ranged from 0 to 50). Unlike the decision-making task in Experiment 1, the âmotor noiseâ was added to the feedback score as follows. Recall that, for each participant, we calculated the standard deviations of the direction errors and the curvature errors during the noise measurement task. We used these two standard deviations, <italic>Ï</italic><sub><italic>dir</italic></sub> and <italic>Ï</italic><sub><italic>cur</italic></sub>, as the measure of their direction motor noise and curvature motor noise respectively. For a clicked cell with parameters [<italic>Î±</italic><sub><italic>attempt</italic></sub>, <italic>Î²</italic><sub><italic>attempt</italic></sub>], the feedback score was based on <inline-formula id="pcbi.1005503.e004"><alternatives><graphic id="pcbi.1005503.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mo>[</mml:mo> <mml:msubsup><mml:mi>Î±</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>m</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>â²</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>Î²</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>m</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>â²</mml:mo></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1005503.e005"><alternatives><graphic id="pcbi.1005503.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:msubsup><mml:mi>Î±</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>m</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>â²</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mi>Î±</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>m</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:msub><mml:mi>Ï</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pcbi.1005503.e006"><alternatives><graphic id="pcbi.1005503.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:msubsup><mml:mi>Î²</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>m</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>â²</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mi>Î²</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>m</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:msub><mml:mi>Ï</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>u</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. That is, the feedback score for the cell: [<italic>Î±</italic><sub><italic>attempt</italic></sub>, <italic>Î²</italic><sub><italic>attempt</italic></sub>] was calculated using the same score function as in Experiment 1 (<xref ref-type="disp-formula" rid="pcbi.1005503.e003">Eq 2</xref>) given the cell <inline-formula id="pcbi.1005503.e007"><alternatives><graphic id="pcbi.1005503.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:mo>[</mml:mo> <mml:msubsup><mml:mi>Î±</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>m</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>â²</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>Î²</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>m</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>â²</mml:mo></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The instructions given in this experiment were the same as in Experiment 1.</p>
</sec>
<sec id="sec031">
<title>Model for the decision-making task</title>
<p>The decision-making task is formulated as a POMDP as follows [<xref ref-type="bibr" rid="pcbi.1005503.ref025">25</xref>]. There is a set of states <inline-formula id="pcbi.1005503.e008"><alternatives><graphic id="pcbi.1005503.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula>, each of which corresponds to an event in which the target is one of the cells in the grid (<xref ref-type="fig" rid="pcbi.1005503.g001">Fig 1D</xref>). At any time step <italic>t</italic>, the environment is in a state <inline-formula id="pcbi.1005503.e009"><alternatives><graphic id="pcbi.1005503.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>â</mml:mo> <mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>i</italic> and <italic>j</italic> indicate the target location in the grid, <italic>i</italic> â [1: 21], <italic>j</italic> â [1: 21]. Therefore, there are 441 (21 Ã 21) states in the state space <inline-formula id="pcbi.1005503.e010"><alternatives><graphic id="pcbi.1005503.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula>. As the experiment to be modelled, the task is divided into episodes; one of the cells is randomly chosen as the hidden target on each episode; each episode consists of 25 time steps (attempts) to find the hidden target cell. That is, the environment is in one of the states; and the state is not directly observable. On each time step within one episode, the model chooses an action. Each action represents an event of clicking one cell in the grid, <inline-formula id="pcbi.1005503.e011"><alternatives><graphic id="pcbi.1005503.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mi>a</mml:mi> <mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo> <mml:mo>â</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, <italic>i</italic> â [1: 21], <italic>j</italic> â [1: 21]. Therefore, there are 441 (21 Ã 21) actions available on each time step. After taking an action <italic>a</italic>, the environment transitions from state <italic>s</italic> to a new state <italic>s</italic>â² according to the transition function <inline-formula id="pcbi.1005503.e012"><alternatives><graphic id="pcbi.1005503.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:mi mathvariant="script">T</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>,</mml:mo> <mml:msup><mml:mi>s</mml:mi> <mml:mo>â²</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>s</mml:mi> <mml:mo>â²</mml:mo></mml:msup> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Note that the underlying state of the environment (i.e. the target cell) remains unchanged within each episode. Therefore, the transition function <inline-formula id="pcbi.1005503.e013"><alternatives><graphic id="pcbi.1005503.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mi mathvariant="script">T</mml:mi></mml:math></alternatives></inline-formula> equals to 1 only when <italic>s</italic>â² = <italic>s</italic>; it equals 0 otherwise. That is, the state transition matrix is the identity matrix. After taking the action <italic>a</italic>, the model also receives two signals from the environment: an observation <inline-formula id="pcbi.1005503.e014"><alternatives><graphic id="pcbi.1005503.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:mi>o</mml:mi> <mml:mo>â</mml:mo> <mml:mi mathvariant="script">O</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and a reward <inline-formula id="pcbi.1005503.e015"><alternatives><graphic id="pcbi.1005503.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:mi>r</mml:mi> <mml:mo>â</mml:mo> <mml:mi mathvariant="script">R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> (cost if the value is negative). In our task, the observation and reward are equal, which is the feedback score (points between 0 and 50 (or [-50:0]). The feedback score is calculated based on the hidden target location (i.e. state) and the clicked cell (i.e. action) as in the experiment (<xref ref-type="disp-formula" rid="pcbi.1005503.e003">Eq 2</xref>).</p>
<p>For the decision-making task, one assumption is that participant performance was constrained by the fact that they were naÃ¯ve to the underlying equation used to generate the score. We model this with a likelihood uncertainty parameter Î (âGammaâ), which represents the uncertainty the score would receive for the current action if a target cell is in a certain location. Specifically, the observation function is the conditional density of the observation given the true state of the environment and the action, <italic>p</italic>(<italic>o</italic><sub><italic>t</italic></sub>|<italic>s</italic>, <italic>a</italic><sub><italic>t</italic></sub>). This function is normally distributed around the true score (based on <xref ref-type="disp-formula" rid="pcbi.1005503.e003">Eq 2</xref>); the standard deviation of this normal distribution is the likelihood uncertainty parameter Î:
<disp-formula id="pcbi.1005503.e016"><alternatives><graphic id="pcbi.1005503.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>o</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>â¼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mi>r</mml:mi> <mml:mi>u</mml:mi> <mml:mi>e</mml:mi> <mml:mi>S</mml:mi> <mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mo>Î</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula></p>
<p>Given the defined POMDP, an algorithm is then used to acquire the optimal control policy (an approximate solution was used in our model). This is the control policy that maximises the expected sum of rewards over 25 steps:
<disp-formula id="pcbi.1005503.e017"><alternatives><graphic id="pcbi.1005503.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mrow><mml:mi>E</mml:mi> <mml:mo>[</mml:mo> <mml:munderover><mml:mo>â</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>25</mml:mn></mml:munderover> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>r</italic>(<italic>t</italic>) is the reward on time step <italic>t</italic>, <italic>E</italic> represents the expected value over all the uncertainty in the task performance. Hence, for example, if the model is certain about the state (i.e., target cell), then the control policy (for action selection) becomes trivial (i.e., clicking the target cell on each of the 25 steps), thus the expected reward in this situation would be: <inline-formula id="pcbi.1005503.e018"><alternatives><graphic id="pcbi.1005503.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:mi>E</mml:mi> <mml:mo>[</mml:mo> <mml:msubsup><mml:mo>â</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>25</mml:mn></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>50</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:mn>50</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, given that the reward (points) obtained for clicking on the target cell is 50. As the model is unsure of the target location (i.e., the model does not directly observe the underlying state of the environment), it must rely on its history of actions and observations. This history is used to estimate the current (unobserved) state (i.e. to estimate where the target is given the action/observation history). This history information is succinctly captured by the <italic>belief state</italic>. The belief state is a posterior probability distribution over the state space given past observations and actions. The action selection is thus then based on the belief state. Our approach involves a Bayesian belief update for state estimate and a control part for action selection (<xref ref-type="fig" rid="pcbi.1005503.g006">Fig 6</xref>). The control part is to select actions, so as to maximise the expected reward (approximately).</p>
<sec id="sec032">
<title>Bayesian belief update</title>
<p>Specifically, the states are discrete in our task, <inline-formula id="pcbi.1005503.e019"><alternatives><graphic id="pcbi.1005503.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:mi mathvariant="script">S</mml:mi> <mml:mo>=</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi> <mml:mo>â</mml:mo> <mml:mo>[</mml:mo> <mml:mn>1</mml:mn> <mml:mo>:</mml:mo> <mml:mn>21</mml:mn> <mml:mo>]</mml:mo> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>â</mml:mo> <mml:mo>[</mml:mo> <mml:mn>1</mml:mn> <mml:mo>:</mml:mo> <mml:mn>21</mml:mn> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. A belief state is therefore represented as a matrix of probabilities whose size is the same as the state space. The belief state at time step <italic>t</italic> is: <inline-formula id="pcbi.1005503.e020"><alternatives><graphic id="pcbi.1005503.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:msub><mml:mi>b</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>â</mml:mo> <mml:mi mathvariant="script">B</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, <italic>i</italic> â [1: 21], <italic>j</italic> â [1: 21]. Each element <italic>b</italic><sub><italic>t</italic></sub>(<italic>i</italic>, <italic>j</italic>) represents the posterior probability of the state <italic>s</italic>(<italic>i</italic>, <italic>j</italic>) after the history of actions, <italic>a</italic><sub>1,2,3,â¦,<italic>t</italic>â1</sub>, and observations <italic>o</italic><sub>1,2,3,â¦,<italic>t</italic>â1</sub>. The initial belief state <italic>b</italic><sub>0</sub> (red arrow in the right of <xref ref-type="fig" rid="pcbi.1005503.g006">Fig 6</xref>) was assumed to be an uniform distribution across the state space. That is, without any evidence, the model believes that the environment is equally possible to be in one of the states.</p>
<p>The update process is as follows. At <italic>t</italic>, an action, <italic>a</italic><sub><italic>t</italic></sub>, is taken, which causes the environment to transition from state <italic>s</italic> to state <italic>s</italic>â² with probability <italic>T</italic>(<italic>s</italic>â² â£ <italic>s</italic>, <italic>a</italic>) (the transition function). After reaching <italic>s</italic>â², one observation, <italic>o</italic><sub><italic>t</italic></sub>, is received with probability <italic>p</italic>(<italic>o</italic><sub><italic>t</italic></sub> â£ <italic>s</italic>â²,<italic>a</italic><sub><italic>t</italic></sub>) (the observation function). The belief state, <italic>b</italic><sub><italic>t</italic></sub>, is obtained given the action <italic>a</italic><sub><italic>t</italic></sub>, the observation <italic>o</italic><sub><italic>t</italic></sub>, and the previous belief <italic>b</italic><sub><italic>t</italic>â1</sub>, as in <xref ref-type="disp-formula" rid="pcbi.1005503.e021">Eq (5)</xref> below.
<disp-formula id="pcbi.1005503.e021"><alternatives><graphic id="pcbi.1005503.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mrow><mml:msub><mml:mi>b</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>s</mml:mi> <mml:mo>â²</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mo>â</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>â</mml:mo> <mml:mi>S</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>b</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Ã</mml:mo> <mml:mi>T</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>s</mml:mi> <mml:mo>â²</mml:mo></mml:msup> <mml:mo>â£</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Ã</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>o</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>â£</mml:mo> <mml:msup><mml:mi>s</mml:mi> <mml:mo>â²</mml:mo></mml:msup> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mo>â</mml:mo> <mml:mrow><mml:msup><mml:mi>s</mml:mi> <mml:mo>â²</mml:mo></mml:msup> <mml:mo>â</mml:mo> <mml:mi>S</mml:mi></mml:mrow></mml:msub> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>o</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>â£</mml:mo> <mml:msup><mml:mi>s</mml:mi> <mml:mo>â²</mml:mo></mml:msup> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo>â</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>â</mml:mo> <mml:mi>S</mml:mi></mml:mrow></mml:msub> <mml:mi>T</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>s</mml:mi> <mml:mo>â²</mml:mo></mml:msup> <mml:mo>â£</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>b</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula></p>
<p>As mentioned above <italic>T</italic>(<italic>s</italic>â²|<italic>s</italic>, <italic>a</italic><sub>1</sub>) = 1 only if <italic>s</italic>â² = <italic>s</italic>, and 0 otherwise, <xref ref-type="disp-formula" rid="pcbi.1005503.e021">Eq (5)</xref> can be simplified as <xref ref-type="disp-formula" rid="pcbi.1005503.e022">Eq (6)</xref>:
<disp-formula id="pcbi.1005503.e022"><alternatives><graphic id="pcbi.1005503.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>b</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>b</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Ã</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>o</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>â£</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mo>â</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>â</mml:mo> <mml:mi>S</mml:mi></mml:mrow></mml:msub> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>o</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>â£</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>b</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>â</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Ã</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>o</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
where the likelihood <italic>p</italic>(<italic>o</italic><sub><italic>t</italic></sub>|<italic>s</italic>, <italic>a</italic><sub><italic>t</italic></sub>) denotes the conditional density of the observation given the true state of the environment and the action. As mentioned, <italic>p</italic>(<italic>o</italic><sub><italic>t</italic></sub>|<italic>s</italic>, <italic>a</italic><sub><italic>t</italic></sub>) is normally distributed around the true score with the standard deviation Î (<xref ref-type="disp-formula" rid="pcbi.1005503.e016">Eq 3</xref>) (The implementation details can be found in <xref ref-type="supplementary-material" rid="pcbi.1005503.s007">S1 Text</xref> and also in online code).</p>
</sec>
<sec id="sec033">
<title>Action selection</title>
<p>The belief state is the best estimate of the current state given the observation/action history, and the action is chosen by the optimal control policy (<xref ref-type="fig" rid="pcbi.1005503.g006">Fig 6</xref>). That is, the POMDP is now a belief-state MDP. Any solution that solves MDP could be, theoretically, used to solve this problem, including Q-learning [<xref ref-type="bibr" rid="pcbi.1005503.ref068">68</xref>], Value/Policy Iteration [<xref ref-type="bibr" rid="pcbi.1005503.ref016">16</xref>]. However, in practice, POMDPs are often computationally intractable to solve exactly, so computer scientists have developed methods that approximate solutions for POMDPs. We used one of the approximated solutions called QMDP [<xref ref-type="bibr" rid="pcbi.1005503.ref069">69</xref>, <xref ref-type="bibr" rid="pcbi.1005503.ref070">70</xref>]. The logic is that the value of each action given a belief, <inline-formula id="pcbi.1005503.e023"><alternatives><graphic id="pcbi.1005503.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mi>Q</mml:mi> <mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mi>b</mml:mi> <mml:mo>â</mml:mo> <mml:mi mathvariant="script">B</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>â</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, is equal to the sum of the expected reward after taking this action in each state, Q(s,a), multiplied by the probability of the agent being in that state, b(s):
<disp-formula id="pcbi.1005503.e024"><alternatives><graphic id="pcbi.1005503.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mrow><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>â</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>â</mml:mo> <mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:munder> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula></p>
<p>We have the belief state <italic>b</italic>(<italic>s</italic>) from the Bayesian inference. The state-action value function, Q(s,a), is formally defined as the sum of the expected reward after taking an action <italic>a</italic> in a state <italic>s</italic> [<xref ref-type="bibr" rid="pcbi.1005503.ref016">16</xref>]:
<disp-formula id="pcbi.1005503.e025"><alternatives><graphic id="pcbi.1005503.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mrow><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>E</mml:mi> <mml:mo>[</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mo>â²</mml:mo></mml:msup></mml:munder> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mo>â²</mml:mo></mml:msup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula>
where <italic>t</italic> is the time step within one episode. The state-action value function Q(S,A) could be derived using any standard solution for MDP (Markov Decision Process) [<xref ref-type="bibr" rid="pcbi.1005503.ref016">16</xref>]. Due to the simplicity of our task, Q(S,A) is relatively straightforward to calculate. As mentioned, the underlying state (i.e. the target cell) remained the same within 25 steps (<italic>t</italic> = 1: 25) of each episode. Therefore <italic>s</italic><sub><italic>t</italic>+1</sub> = <italic>s</italic><sub><italic>t</italic></sub> = <italic>s</italic> (<xref ref-type="disp-formula" rid="pcbi.1005503.e025">Eq 8</xref>). Recall, the state means where the target cell is, so knowing the state means knowing where the target cell is. Therefore after the action at time step <italic>t</italic> (<italic>a</italic><sub><italic>t</italic></sub> = <italic>a</italic>), the best action is clicking the target cell (denoted as <italic>a</italic><sup><italic>s</italic></sup>) as it gives the highest reward. Therefore, <xref ref-type="disp-formula" rid="pcbi.1005503.e025">Eq 8</xref> could be expanded as:
<disp-formula id="pcbi.1005503.e026"><alternatives><graphic id="pcbi.1005503.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mrow><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>E</mml:mi> <mml:mo>[</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mi>s</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mi>s</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo>â¦</mml:mo> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>T</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mi>s</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula>
where <italic>T</italic> indicates the last time step of the episode, <italic>r</italic><sub><italic>t</italic>+1</sub> is the reward of taking action <italic>a</italic> at state <italic>s</italic> at time step <italic>t</italic>. The term after <italic>r</italic><sub><italic>t</italic>+1</sub> is identical for all the actions at time step <italic>t</italic>, <xref ref-type="disp-formula" rid="pcbi.1005503.e025">Eq 8</xref> can therefore be further simplified as:
<disp-formula id="pcbi.1005503.e027"><alternatives><graphic id="pcbi.1005503.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mrow><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>â</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula>
As mentioned, one of the constraints was that participants were uncertain about what score they would get given a state and an action, and we modelled this with a likelihood uncertainty parameter Î. Therefore,
<disp-formula id="pcbi.1005503.e028"><alternatives><graphic id="pcbi.1005503.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>t</mml:mi> <mml:mi>r</mml:mi> <mml:mi>u</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi> <mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mo>Î</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula>
where <italic>truescore</italic>(<italic>s</italic>, <italic>a</italic>) is calculated based on <xref ref-type="disp-formula" rid="pcbi.1005503.e003">Eq 2</xref>. In summary, we now have an action value of each action given the belief state b based on:
<disp-formula id="pcbi.1005503.e029"><alternatives><graphic id="pcbi.1005503.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mrow><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>â</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>â</mml:mo> <mml:mi>S</mml:mi></mml:mrow></mml:munder> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>â</mml:mo> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>â</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>â</mml:mo> <mml:mi>S</mml:mi></mml:mrow></mml:munder> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula></p>
<p>The original QMDP algorithm [<xref ref-type="bibr" rid="pcbi.1005503.ref069">69</xref>] then selects the action that yields the highest value on the belief state. One known disadvantage of the QMDP method is that it lacks exploration. To remedy this shortcoming [<xref ref-type="bibr" rid="pcbi.1005503.ref071">71</xref>], the action selection in our model was based on the value of each action relative to the value of all the actions (i.e., soft-max action selection). The probability of choosing an action a was calculated as:
<disp-formula id="pcbi.1005503.e030"><alternatives><graphic id="pcbi.1005503.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e030" xlink:type="simple"/><mml:math display="block" id="M30"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>Ï</mml:mi> <mml:mo>Ã</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>[</mml:mo> <mml:mi>b</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:msubsup><mml:mo>â</mml:mo> <mml:mrow><mml:mi>u</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>v</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mn>21</mml:mn> <mml:mo>,</mml:mo> <mml:mn>21</mml:mn></mml:mrow></mml:msubsup> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>Ï</mml:mi> <mml:mo>Ã</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>[</mml:mo> <mml:mi>b</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>,</mml:mo> <mml:mi>v</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula></p>
<p>With <italic>Ï</italic> = 0, action selection is totally random. As <italic>Ï</italic> increased, the action selection becomes more dependent on the value function <inline-formula id="pcbi.1005503.e031"><alternatives><graphic id="pcbi.1005503.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mrow><mml:mi>Q</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">B</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>). Therefore, the exploration was inversely proportional to the uncertainty in the value function. It has been shown that a temperature schedule that begins with a high level of exploration but then leads to exploration gradually decreasing as learning progresses generally leads to higher expected reward [<xref ref-type="bibr" rid="pcbi.1005503.ref072">72</xref>]. However, there is no consensus regarding which decreasing schedule is optimal [<xref ref-type="bibr" rid="pcbi.1005503.ref072">72</xref>]. In our model, <italic>Ï</italic> was set to <italic>Ï</italic><sub>0</sub>/<italic>t</italic>, where <italic>Ï</italic><sub>0</sub> is a constant and <italic>t</italic> is the number of attempts. The <italic>Ï</italic><sub>0</sub> (between 1 to 15) that generated the highest mean points across 25 attempts was chosen for each Î (<xref ref-type="fig" rid="pcbi.1005503.g007">Fig 7</xref>). The modelâs performance reported in the main results is based on an average over 100 runs.</p>
</sec>
</sec>
<sec id="sec034">
<title>Model for the reaching task</title>
<p>Next, a model for the reaching task is introduced by adding motor noise to the model for the decision-making task.</p>
<p>In the decision-making task model, the action space is <inline-formula id="pcbi.1005503.e032"><alternatives><graphic id="pcbi.1005503.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:mi mathvariant="script">A</mml:mi> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>i</italic> â [1: 21], <italic>j</italic> â [1: 21]), <italic>a</italic>(<italic>i</italic>, <italic>j</italic>) represents the event of clicking the cell [<italic>i</italic>, <italic>j</italic>] in the grid. That is, each action has two properties <italic>i</italic> and <italic>j</italic>, which represent the cell location in the decision-making task. Given that the participants were able to click the exact cells they wanted during the task, there was no noise/uncertainty between the planned actions and executed actions. In the reaching task, these two properties of an action were the direction and curvature of the movement. Due to motor noise, and a lack of informative visual feedback, there was uncertainty between the planned and executed action.</p>
<p>The model for the reaching task was identical to the decision-making model, except that motor noise was now added (Execution, <xref ref-type="fig" rid="pcbi.1005503.g006">Fig 6</xref>). Recall that, for each participant, we calculated the standard deviations of the direction errors and the curvature errors during the noise measurement task. We used these two standard deviations, <italic>Ï</italic><sub><italic>dir</italic></sub> and <italic>Ï</italic><sub><italic>cur</italic></sub>, as the measure of their direction motor noise and curvature motor noise respectively. The motor noise was added to the planned action as follows. The planned action with direction <italic>i</italic> and curvature <italic>j</italic>, <italic>a</italic>[<italic>i</italic>, <italic>j</italic>] became <italic>a</italic>[<italic>i</italic>â², <italic>j</italic>â²] due to the motor noise [<italic>Ï</italic><sub><italic>dir</italic></sub>, <italic>Ï</italic><sub><italic>cur</italic></sub>]:
<disp-formula id="pcbi.1005503.e033"><alternatives><graphic id="pcbi.1005503.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e033" xlink:type="simple"/><mml:math display="block" id="M33"><mml:mrow><mml:mi>a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>i</mml:mi> <mml:mo>â²</mml:mo></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>j</mml:mi> <mml:mo>â²</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>i</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:msub><mml:mi>Ï</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:msub><mml:mi>Ï</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>u</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula></p>
<p>The motor noise also affects the belief-action value function estimate. Specifically, for the reaching task, the score received after taking action <italic>a</italic> at state <italic>s</italic> (i.e. <italic>r</italic><sub><italic>t</italic>+1</sub>|<italic>s</italic><sub><italic>t</italic></sub> = <italic>s</italic>, <italic>a</italic><sub><italic>t</italic></sub> = <italic>a</italic>) is also determined by motor noise. That is, even if the target trajectory is known, the reward of executing the planed action is affected by motor noise. Therefore <xref ref-type="disp-formula" rid="pcbi.1005503.e028">Eq 10</xref> is modified as <xref ref-type="disp-formula" rid="pcbi.1005503.e034">Eq 14</xref> below.
<disp-formula id="pcbi.1005503.e034"><alternatives><graphic id="pcbi.1005503.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005503.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>t</mml:mi> <mml:mi>r</mml:mi> <mml:mi>u</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi> <mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>o</mml:mi> <mml:mi>i</mml:mi> <mml:mi>s</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mo>Î</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
where <italic>a</italic><sub><italic>noise</italic></sub> is the action <italic>a</italic> contaminated by motor noise as in <xref ref-type="disp-formula" rid="pcbi.1005503.e033">Eq 13</xref>.</p>
</sec>
</sec>
<sec id="sec035">
<title>Supporting information</title>
<supplementary-material id="pcbi.1005503.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Learning curve predictions for different target cells (the DM task).</title>
<p>The modelâs predictions of the learning curves (black) for all the 24 targets used in the experiment, against participant performance (red). Each panel is for a specific target, indicated by blue asterisk plotted against the rectangle in the bottom right of each panel. Red error bars represent 95% CI across 20 participants.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005503.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Learning curve predictions for different target trajectories (the MO task).</title>
<p>The modelâs predictions of the learning curves (black) for all the 24 targets used in the experiment, against participant performance (green). Each panel is for a specific target trajectory, indicated by blue trajectory plotted against the rectangle in the bottom right of each panel. Green error bars represent 95% CI across 20 participants.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005503.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Representative participant learning curves for each target with model prediction (average over 100 runs).</title>
<p>One participantâs learning curves for all 24 targets in both the DM (red) and the MO task (green), against model predictions (black; average over 100 runs). Each panel represents a specific target.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005503.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Representative participant learning curves for each target with model prediction (one single run).</title>
<p>One participantâs learning curves for all 24 targets in both the DM (red) and the MO task (green), against model predictions (black; one single run). Each panel represents a specific target.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005503.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.s005" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Comparison of the error reduction (Experiment 1).</title>
<p>Two-way repeated measures ANOVA results on the three parameters (a,b and c in <italic>y</italic> = <italic>ae</italic><sup>â<italic>bx</italic></sup> + <italic>c</italic>).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005503.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.s006" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Comparison of the error reduction (Experiment 2).</title>
<p>Two-way repeated measures ANOVA results on the three parameters (a,b and c in <italic>y</italic> = <italic>ae</italic><sup>â<italic>bx</italic></sup> + <italic>c</italic>).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005503.s007" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005503.s007" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Likelihood implementation details.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Jeremy Wyatt for valuable comments on a previous version.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005503.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lackner</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Dizio</surname> <given-names>P</given-names></name>. <article-title>Rapid adaptation to Coriolis force perturbations of arm trajectory</article-title>. <source>Journal of neurophysiology</source>. <year>1994</year>;<volume>72</volume>(<issue>1</issue>):<fpage>299</fpage>â<lpage>313</lpage>. <object-id pub-id-type="pmid">7965013</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Mussa-Ivaldi</surname> <given-names>F</given-names></name>. <article-title>Adaptive representation of dynamics during learning of a motor task</article-title>. <source>Journal of Neuroscience</source>. <year>1994</year>;<volume>14</volume>:<fpage>3208</fpage>â<lpage>3224</lpage>. <object-id pub-id-type="pmid">8182467</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Martin</surname> <given-names>Ta</given-names></name>, <name name-style="western"><surname>Keating</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Goodkin</surname> <given-names>HP</given-names></name>, <name name-style="western"><surname>Bastian</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Thach</surname> <given-names>WT</given-names></name>. <article-title>Throwing while looking through prisms. I. Focal olivocerebellar lesions impair adaptation</article-title>. <source>Brain</source>. <year>1996</year>;<volume>119</volume>:<fpage>1183</fpage>â<lpage>1198</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/brain/119.4.1183" xlink:type="simple">10.1093/brain/119.4.1183</ext-link></comment> <object-id pub-id-type="pmid">8813282</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miall</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Jenkinson</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kulkarni</surname> <given-names>K</given-names></name>. <article-title>Adaptation to rotated visual feedback: A re-examination of motor interference</article-title>. <source>Experimental Brain Research</source>. <year>2004</year>;<volume>154</volume>(<issue>2</issue>):<fpage>201</fpage>â<lpage>210</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-003-1630-2" xlink:type="simple">10.1007/s00221-003-1630-2</ext-link></comment> <object-id pub-id-type="pmid">14608451</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tseng</surname> <given-names>Yw</given-names></name>, <name name-style="western"><surname>Diedrichsen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bastian</surname> <given-names>AJ</given-names></name>. <article-title>Sensory prediction errors drive cerebellum-dependent adaptation of reaching</article-title>. <source>Journal of neurophysiology</source>. <year>2007</year>;<volume>98</volume>(<issue>1</issue>):<fpage>54</fpage>â<lpage>62</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00266.2007" xlink:type="simple">10.1152/jn.00266.2007</ext-link></comment> <object-id pub-id-type="pmid">17507504</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rabe</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Livne</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Gizewski</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Aurich</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Timmann</surname> <given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Adaptation to visuomotor rotation and force field perturbation is correlated to different brain areas in patients with cerebellar degeneration</article-title>. <source>Journal of neurophysiology</source>. <year>2009</year>;<volume>101</volume>(<issue>4</issue>):<fpage>1961</fpage>â<lpage>1971</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.91069.2008" xlink:type="simple">10.1152/jn.91069.2008</ext-link></comment> <object-id pub-id-type="pmid">19176608</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Diedrichsen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>White</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Newman</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Lally</surname> <given-names>N</given-names></name>. <article-title>Use-Dependent and Error-Based Learning of Motor Behaviors</article-title>. <source>Journal of Neuroscience</source>. <year>2010</year>;<volume>30</volume>(<issue>15</issue>):<fpage>5159</fpage>â<lpage>5166</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5406-09.2010" xlink:type="simple">10.1523/JNEUROSCI.5406-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20392938</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huang</surname> <given-names>VS</given-names></name>, <name name-style="western"><surname>Haith</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mazzoni</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>JW</given-names></name>. <article-title>Rethinking Motor Learning and Savings in Adaptation Paradigms: Model-Free Memory for Successful Actions Combines with Internal Models</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>70</volume>(<issue>4</issue>):<fpage>787</fpage>â<lpage>801</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.04.012" xlink:type="simple">10.1016/j.neuron.2011.04.012</ext-link></comment> <object-id pub-id-type="pmid">21609832</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref009">
<label>9</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Haith</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>JW</given-names></name>. <chapter-title>Theoretical models of motor control and motor learning</chapter-title>. <source>The Routledge Handbook of Motor Control and Motor Learning</source>. <year>2013</year>; p. <fpage>7</fpage>â<lpage>28.</lpage></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Taylor</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Ivry</surname> <given-names>RB</given-names></name>. <article-title>Cerebellar and Prefrontal Cortex Contributions to Adaptation, Strategies, and Reinforcement Learning</article-title>. <source>Progress in Brain Research</source>. <year>2014</year>;<volume>210</volume>:<fpage>217</fpage>â<lpage>253</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/B978-0-444-63356-9.00009-1" xlink:type="simple">10.1016/B978-0-444-63356-9.00009-1</ext-link></comment> <object-id pub-id-type="pmid">24916295</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Izawa</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>Learning from sensory and reward prediction errors during motor adaptation</article-title>. <source>PLoS Computational Biology</source>. <year>2011</year>;<volume>7</volume>(<issue>3</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002012" xlink:type="simple">10.1371/journal.pcbi.1002012</ext-link></comment> <object-id pub-id-type="pmid">21423711</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shmuelof</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>VS</given-names></name>, <name name-style="western"><surname>Haith</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Delnicki</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Mazzoni</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>JW</given-names></name>. <article-title>Overcoming Motor âForgettingâ Through Reinforcement Of Learned Actions</article-title>. <source>Journal of Neuroscience</source>. <year>2012</year>;<volume>32</volume>(<issue>42</issue>):<fpage>14617</fpage>â<lpage>14621</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2184-12.2012" xlink:type="simple">10.1523/JNEUROSCI.2184-12.2012</ext-link></comment> <object-id pub-id-type="pmid">23077047</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dam</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Kording</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Wei</surname> <given-names>K</given-names></name>. <article-title>Credit Assignment during Movement Reinforcement Learning</article-title>. <source>PLoS ONE</source>. <year>2013</year>;<volume>8</volume>(<issue>2</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0055352" xlink:type="simple">10.1371/journal.pone.0055352</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wu</surname> <given-names>HG</given-names></name>, <name name-style="western"><surname>Miyamoto</surname> <given-names>YR</given-names></name>, <name name-style="western"><surname>Gonzales Castro</surname> <given-names>LN</given-names></name>, <name name-style="western"><surname>Ãlveczky</surname> <given-names>BP</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>MA</given-names></name>. <article-title>Temporal structure of motor vriability is dynamically regulated and predicts motor learning ability</article-title>. <source>Nature Neuroscience</source>. <year>2014</year>;<volume>17</volume>(<issue>2</issue>):<fpage>312</fpage>â<lpage>321</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3616" xlink:type="simple">10.1038/nn.3616</ext-link></comment> <object-id pub-id-type="pmid">24413700</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Therrien</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Bastian</surname> <given-names>AJ</given-names></name>. <article-title>Effective Reinforcement learning following cerebellar damage requires a balance between exploration and motor noise</article-title>. <source>Brain</source>. <year>2016</year>;<volume>139</volume>(<issue>1</issue>):<fpage>101</fpage>â<lpage>114</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/brain/awv329" xlink:type="simple">10.1093/brain/awv329</ext-link></comment> <object-id pub-id-type="pmid">26626368</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <article-title>Reinforcement Learning: An Introduction</article-title>. <source>IEEE Transactions on Neural Networks</source>. <year>1998</year>;<volume>9</volume>(<issue>5</issue>):<fpage>1054</fpage>â<lpage>1054</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TNN.1998.712192" xlink:type="simple">10.1109/TNN.1998.712192</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref017">
<label>17</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Wu</surname> <given-names>SW</given-names></name>, <name name-style="western"><surname>Delgado</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>. <chapter-title>Motor Decision-Making</chapter-title>. In: <source>Brain Mapping: An Encyclopedic Reference</source>. <volume>vol. 3</volume>. <publisher-name>Elsevier Inc</publisher-name>.; <year>2015</year>. p. <fpage>417</fpage>â<lpage>427</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kahneman</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Tversky</surname> <given-names>A</given-names></name>. <article-title>Prospect theory: An analysis of decision under risk</article-title>. <source>Econometrica: Journal of the Econometric Society</source>. <year>1979</year>; p. <fpage>263</fpage>â<lpage>291</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/1914185" xlink:type="simple">10.2307/1914185</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>TrommershÃ¤user</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>. <article-title>Statistical decision theory and trade-offs in the control of motor response</article-title>. <source>Spatial vision</source>. <year>2003</year>;<volume>16</volume>(<issue>3-4</issue>):<fpage>255</fpage>â<lpage>275</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1163/156856803322467527" xlink:type="simple">10.1163/156856803322467527</ext-link></comment> <object-id pub-id-type="pmid">12858951</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>TrommershÃ¤user</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>. <article-title>Decision making, movement planning and statistical decision theory</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2008</year>;<volume>12</volume>(<issue>8</issue>):<fpage>291</fpage>â<lpage>297</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2008.04.010" xlink:type="simple">10.1016/j.tics.2008.04.010</ext-link></comment> <object-id pub-id-type="pmid">18614390</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wu</surname> <given-names>SW</given-names></name>, <name name-style="western"><surname>Delgado</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>. <article-title>Economic decision-making compared with an equivalent motor task</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2009</year>;<volume>106</volume>(<issue>15</issue>):<fpage>6088</fpage>â<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0900102106" xlink:type="simple">10.1073/pnas.0900102106</ext-link></comment> <object-id pub-id-type="pmid">19332799</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>. <article-title>Motor control is decision-making</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2012</year>;<volume>22</volume>(<issue>6</issue>):<fpage>996</fpage>â<lpage>1003</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2012.05.003" xlink:type="simple">10.1016/j.conb.2012.05.003</ext-link></comment> <object-id pub-id-type="pmid">22647641</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Galea</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Mallia</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Rothwell</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Diedrichsen</surname> <given-names>J</given-names></name>. <article-title>The dissociable effects of punishment and reward on motor learning</article-title>. <source>Nature Neuroscience</source>. <year>2015</year>;<volume>18</volume>(<issue>4</issue>):<fpage>597</fpage>â<lpage>602</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3956" xlink:type="simple">10.1038/nn.3956</ext-link></comment> <object-id pub-id-type="pmid">25706473</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van Beers</surname> <given-names>RJ</given-names></name>. <article-title>Motor Learning Is Optimally Tuned to the Properties of Motor Noise</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>63</volume>(<issue>3</issue>):<fpage>406</fpage>â<lpage>417</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.06.025" xlink:type="simple">10.1016/j.neuron.2009.06.025</ext-link></comment> <object-id pub-id-type="pmid">19679079</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kaelbling</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Littman</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Cassandra</surname> <given-names>A</given-names></name>. <article-title>Planning and Acting in Partially Observable Stochastic Domains</article-title>. <source>Artificial Intelligence</source>. <year>1998</year>;<volume>101</volume>(<issue>1-2</issue>):<fpage>99</fpage>â<lpage>134</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0004-3702(98)00023-X" xlink:type="simple">10.1016/S0004-3702(98)00023-X</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref026">
<label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Butko NJ, Movellan JR. I-POMDP: An infomax model of eye movement. In: 2008 IEEE 7th International Conference on Development and Learning, ICDL; 2008. p. 139â144.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rao</surname> <given-names>RPN</given-names></name>. <article-title>Decision making under uncertainty: a neural model based on partially observable markov decision processes</article-title>. <source>Frontiers in computational neuroscience</source>. <year>2010</year>;<volume>4</volume>(<month>November</month>):<fpage>146</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2010.00146" xlink:type="simple">10.3389/fncom.2010.00146</ext-link></comment> <object-id pub-id-type="pmid">21152255</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref028">
<label>28</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Chen</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Lewis</surname> <given-names>RL</given-names></name>, <name name-style="western"><surname>Myers</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Houpt</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Howes</surname> <given-names>A</given-names></name>. <chapter-title>Discovering Computationally Rational Eye Movements in the Distractor Ratio Task</chapter-title>. In: <source>Reinforcement Learning and Decision Making</source>. <publisher-loc>Princeton</publisher-loc>; <year>2013</year>. p. <fpage>106</fpage>â<lpage>110</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref029">
<label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Chen X, Bailly G, Brumby DP, Oulasvirta A, Howes A. The Emergence of Interactive Behavior: A Model of Rational Menu Search. Proceedings of the ACM CHIâ15 Conference on Human Factors in Computing Systems. 2015;1:4217â4226.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref030">
<label>30</label>
<mixed-citation publication-type="other" xlink:type="simple">Chen X, Starke S, Baber C, Howes A. A Cognitive Model of How People Make Decisions Through Interaction with Visual Displays. In: Proceedings of the ACM CHIâ17 Conference on Human Factors in Computing Systems; 2017.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lewis</surname> <given-names>RL</given-names></name>, <name name-style="western"><surname>Howes</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>S</given-names></name>. <article-title>Computational rationality: linking mechanism and behavior through bounded utility maximization</article-title>. <source>Topics in Cognitive Science</source>. <year>2014</year>;<volume>6</volume>(<issue>2</issue>):<fpage>279</fpage>â<lpage>311</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/tops.12086" xlink:type="simple">10.1111/tops.12086</ext-link></comment> <object-id pub-id-type="pmid">24648415</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Howes</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lewis</surname> <given-names>RL</given-names></name>, <name name-style="western"><surname>Vera</surname> <given-names>A</given-names></name>. <article-title>Rational adaptation under task and processing constraints: implications for testing theories of cognition and action</article-title>. <source>Psychological review</source>. <year>2009</year>;<volume>116</volume>(<issue>4</issue>):<fpage>717</fpage>â<lpage>751</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0017187" xlink:type="simple">10.1037/a0017187</ext-link></comment> <object-id pub-id-type="pmid">19839682</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dukas</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Real</surname> <given-names>LA</given-names></name>. <article-title>Effects of recent experience on foraging decisions by Bumble Bees</article-title>. <source>Oecologia</source>. <year>1993</year>;<volume>94</volume>(<issue>2</issue>):<fpage>244</fpage>â<lpage>246</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00341323" xlink:type="simple">10.1007/BF00341323</ext-link></comment> <object-id pub-id-type="pmid">28314038</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Marshall</surname> <given-names>AT</given-names></name>, <name name-style="western"><surname>Kirkpatrick</surname> <given-names>K</given-names></name>. <article-title>Relative gains, losses, and reference points in probabilistic choice in rats</article-title>. <source>PLoS ONE</source>. <year>2015</year>;<volume>10</volume>(<issue>2</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0117697" xlink:type="simple">10.1371/journal.pone.0117697</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huys</surname> <given-names>QJM</given-names></name>, <name name-style="western"><surname>Eshel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>OâNions</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Sheridan</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Roiser</surname> <given-names>JP</given-names></name>. <article-title>Bonsai trees in your head: How the pavlovian system sculpts goal-directed choices by pruning decision trees</article-title>. <source>PLoS Computational Biology</source>. <year>2012</year>;<volume>8</volume>(<issue>3</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002410" xlink:type="simple">10.1371/journal.pcbi.1002410</ext-link></comment> <object-id pub-id-type="pmid">22412360</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Person</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Bee foraging in uncertain environments using predictive hebbian learning</article-title>. <source>Nature</source>. <year>1995</year>;<volume>377</volume>(<issue>6551</issue>):<fpage>725</fpage>â<lpage>728</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/377725a0" xlink:type="simple">10.1038/377725a0</ext-link></comment> <object-id pub-id-type="pmid">7477260</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Maruyama</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>De Martino</surname> <given-names>B</given-names></name>. <article-title>When is a loss a loss? Excitatory and inhibitory processes in loss-related decision-making</article-title>. <source>Current Opinion in Behavioral Sciences</source>. <year>2015</year>;<volume>5</volume>:<fpage>122</fpage>â<lpage>127</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cobeha.2015.09.003" xlink:type="simple">10.1016/j.cobeha.2015.09.003</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Koszegi</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Rabin</surname> <given-names>M</given-names></name>. <article-title>Reference-dependent risk attitudes</article-title>. <source>American Economic Review</source>. <year>2007</year>;<volume>97</volume>(<issue>4</issue>):<fpage>1047</fpage>â<lpage>1073</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1257/aer.97.4.1047" xlink:type="simple">10.1257/aer.97.4.1047</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Courville</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Tourtezky</surname> <given-names>DS</given-names></name>. <article-title>Representation and timing in theories of the dopamine system</article-title>. <source>Neural computation</source>. <year>2006</year>;<volume>18</volume>(<issue>7</issue>):<fpage>1637</fpage>â<lpage>77</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2006.18.7.1637" xlink:type="simple">10.1162/neco.2006.18.7.1637</ext-link></comment> <object-id pub-id-type="pmid">16764517</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Decision theory, reinforcement learning, and the brain</article-title>. <source>Cognitive, affective &amp; behavioral neuroscience</source>. <year>2008</year>;<volume>8</volume>(<issue>4</issue>):<fpage>429</fpage>â<lpage>453</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/CABN.8.4.429" xlink:type="simple">10.3758/CABN.8.4.429</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref041">
<label>41</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Frazier</surname> <given-names>PI</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>AJ</given-names></name>. <article-title>Sequential hypothesis testing under stochastic deadlines</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2008</year>; p. <fpage>1</fpage>â<lpage>8</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tseng</surname> <given-names>FY</given-names></name>, <name name-style="western"><surname>Chao</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Feng</surname> <given-names>WY</given-names></name>, <name name-style="western"><surname>Hwang</surname> <given-names>SL</given-names></name>. <article-title>Assessment of human color discrimination based on illuminant color, ambient illumination and screen background color for visual display terminal workers</article-title>. <source>Industrial health</source>. <year>2010</year>;<volume>48</volume>(<issue>4</issue>):<fpage>438</fpage>â<lpage>46</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2486/indhealth.MS1009" xlink:type="simple">10.2486/indhealth.MS1009</ext-link></comment> <object-id pub-id-type="pmid">20720335</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Johnson</surname> <given-names>KO</given-names></name>, <name name-style="western"><surname>Phillips</surname> <given-names>JR</given-names></name>. <article-title>Tactile spatial resolution. I. two-point discrimination, gap detection, grating resolution, and letter recognition</article-title>. <source>Journal of neurophysiology</source>. <year>1981</year>;<volume>46</volume>(<issue>6</issue>):<fpage>1177</fpage>â<lpage>1192</lpage>. <object-id pub-id-type="pmid">7320742</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schmidt</surname> <given-names>Ra</given-names></name>, <name name-style="western"><surname>Bjork</surname> <given-names>Ra</given-names></name>. <article-title>New Conceptualizations of Practice: Common Principles in Three Paradigms Suggest New Concepts for Training</article-title>. <source>Psychological Science</source>. <year>1992</year>;<volume>3</volume>(<issue>4</issue>):<fpage>207</fpage>â<lpage>217</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-9280.1992.tb00029.x" xlink:type="simple">10.1111/j.1467-9280.1992.tb00029.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shea</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Morgan</surname> <given-names>RL</given-names></name>. <article-title>Contextual interference effects on the acquisition, retention, and transfer of a motor skill</article-title>. <source>Journal of Experimental Psychology: Human Learning &amp; Memory</source>. <year>1979</year>;<volume>5</volume>(<issue>2</issue>):<fpage>179</fpage>â<lpage>187</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>He</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Liang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Abdollahi</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Fisher Bittmann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kording</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Wei</surname> <given-names>K</given-names></name>. <article-title>The Statistical Determinants of the Speed of Motor Learning</article-title>. <source>PLoS Computational Biology</source>. <year>2016</year>;<volume>12</volume>(<issue>9</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1005023" xlink:type="simple">10.1371/journal.pcbi.1005023</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Claus</surname> <given-names>ED</given-names></name>. <article-title>Anatomy of a decision: striato-orbitofrontal interactions in reinforcement learning, decision making, and reversal</article-title>. <source>Psychological review</source>. <year>2006</year>;<volume>113</volume>(<issue>2</issue>):<fpage>300</fpage>â<lpage>326</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.113.2.300" xlink:type="simple">10.1037/0033-295X.113.2.300</ext-link></comment> <object-id pub-id-type="pmid">16637763</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Beiser</surname> <given-names>DG</given-names></name>, <name name-style="western"><surname>Hua</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Houk</surname> <given-names>J</given-names></name>. <article-title>Network models of the basal ganglia</article-title>. <source>Current opinion in neurobiology</source>. <year>1997</year>;<volume>7</volume>(<issue>2</issue>):<fpage>185</fpage>â<lpage>190</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0959-4388(97)80006-2" xlink:type="simple">10.1016/S0959-4388(97)80006-2</ext-link></comment> <object-id pub-id-type="pmid">9142759</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gurney</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Prescott</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Redgrave</surname> <given-names>P</given-names></name>. <article-title>A computational model of action selection in the basal ganglia. I. A new functional anatomy</article-title>. <source>Biological cybernetics</source>. <year>2001</year>;<volume>84</volume>(<issue>6</issue>):<fpage>401</fpage>â<lpage>410</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/PL00007984" xlink:type="simple">10.1007/PL00007984</ext-link></comment> <object-id pub-id-type="pmid">11417052</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Seeberger</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>OâReilly</surname> <given-names>RC</given-names></name>. <article-title>By carrot or by stick: cognitive reinforcement learning in parkinsonism</article-title>. <source>Science</source>. <year>2004</year>;<volume>306</volume>:<fpage>1940</fpage>â<lpage>1943</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1102941" xlink:type="simple">10.1126/science.1102941</ext-link></comment> <object-id pub-id-type="pmid">15528409</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mink</surname> <given-names>JW</given-names></name>. <article-title>The basal ganglia: Focused selection and inhibition of competing motor programs</article-title>. <source>Progress in Neurobiology</source>. <year>1996</year>;<volume>50</volume>(<issue>4</issue>):<fpage>381</fpage>â<lpage>425</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0301-0082(96)00042-1" xlink:type="simple">10.1016/S0301-0082(96)00042-1</ext-link></comment> <object-id pub-id-type="pmid">9004351</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>OâReilly</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>. <article-title>Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia</article-title>. <source>Neural computation</source>. <year>2006</year>;<volume>18</volume>(<issue>2</issue>):<fpage>283</fpage>â<lpage>328</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976606775093909" xlink:type="simple">10.1162/089976606775093909</ext-link></comment> <object-id pub-id-type="pmid">16378516</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kringelbach</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Rolls</surname> <given-names>ET</given-names></name>. <article-title>The functional neuroanatomy of the human orbitofrontal cortex: Evidence from neuroimaging and neuropsychology</article-title>. <source>Progress in Neurobiology</source>. <year>2004</year>;<volume>72</volume>(<issue>5</issue>):<fpage>341</fpage>â<lpage>372</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.pneurobio.2004.03.006" xlink:type="simple">10.1016/j.pneurobio.2004.03.006</ext-link></comment> <object-id pub-id-type="pmid">15157726</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Tremblay</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Hollerman</surname> <given-names>JR</given-names></name>. <article-title>Reward processing in primate orbitofrontal cortex and basal ganglia</article-title>. <source>Cerebral cortex (New York, NY: 1991)</source>. <year>2000</year>;<volume>10</volume>(<issue>3</issue>):<fpage>272</fpage>â<lpage>84</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bostan</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Strick</surname> <given-names>PL</given-names></name>. <article-title>The cerebellum and basal ganglia are interconnected</article-title>. <source>Neuropsychology Review</source>. <year>2010</year>;<volume>20</volume>(<issue>3</issue>):<fpage>261</fpage>â<lpage>270</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s11065-010-9143-9" xlink:type="simple">10.1007/s11065-010-9143-9</ext-link></comment> <object-id pub-id-type="pmid">20811947</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>OâReilly</surname> <given-names>JX</given-names></name>, <name name-style="western"><surname>Beckmann</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Tomassini</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Ramnani</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Johansen-Berg</surname> <given-names>H</given-names></name>. <article-title>Distinct and overlapping functional zones in the cerebellum defined by resting state functional connectivity</article-title>. <source>Cerebral Cortex</source>. <year>2010</year>;<volume>20</volume>(<issue>4</issue>):<fpage>953</fpage>â<lpage>965</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhp157" xlink:type="simple">10.1093/cercor/bhp157</ext-link></comment> <object-id pub-id-type="pmid">19684249</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miall</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Galea</surname> <given-names>JM</given-names></name>. <article-title>Cerebellar damage limits reinforcement learning. Commentary on Therrien et al: Effective reinforcement learning following cerebellar damage requires a balance between exploration and motor noise</article-title>. <source>Brain</source>. <year>2016</year>;<volume>139</volume>(<issue>1</issue>):<fpage>4</fpage>â<lpage>7</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Caligiore</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Baldassarre</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Bostan</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Strick</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Consensus Paper: Towards a Systems-Level View of Cerebellar Function: the Interplay Between Cerebellum, Basal Ganglia, and Cortex</article-title>. <source>Cerebellum</source>. <year>2016</year>; p. <fpage>1</fpage>â<lpage>27</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Savage</surname> <given-names>LJ</given-names></name>. <article-title>The Theory of Statistical Decision</article-title>. <source>Journal of the American Statistical Association</source>. <year>1951</year>;<volume>46</volume>(<issue>253</issue>):<fpage>55</fpage>â<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/01621459.1951.10500768" xlink:type="simple">10.1080/01621459.1951.10500768</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref060">
<label>60</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Von Neumann</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Morgenstern</surname> <given-names>O</given-names></name>. <source>Theory of Games and Economic Behavior</source>. <publisher-name>Princeton University Press</publisher-name>. <year>1944</year>; p. <fpage>625</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>KÃ¶rding</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>Bayesian decision theory in sensorimotor control</article-title>. <source>Trends in cognitive sciences</source>. <year>2006</year>;<volume>10</volume>(<issue>7</issue>):<fpage>319</fpage>â<lpage>26</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2006.05.003" xlink:type="simple">10.1016/j.tics.2006.05.003</ext-link></comment> <object-id pub-id-type="pmid">16807063</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nagengast</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Braun</surname> <given-names>Da</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>Risk-sensitivity and the mean-variance trade-off: decision making in sensorimotor control</article-title>. <source>Proceedings Biological sciences / The Royal Society</source>. <year>2011</year>;<volume>278</volume>(<issue>1716</issue>):<fpage>2325</fpage>â<lpage>2332</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rspb.2010.2518" xlink:type="simple">10.1098/rspb.2010.2518</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barberis</surname> <given-names>NC</given-names></name>. <article-title>Thirty Years of Prospect Theory in Economics: A Review and Assessment</article-title>. <source>Journal of Economic Perspectives</source>. <year>2013</year>;<volume>27</volume>:<fpage>173</fpage>â<lpage>196</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1257/jep.27.1.173" xlink:type="simple">10.1257/jep.27.1.173</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Abe</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schambra</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wassermann</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Luckenbaugh</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Schweighofer</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>LG</given-names></name>. <article-title>Reward improves long-term retention of a motor memory through induction of offline memory gains</article-title>. <source>Current Biology</source>. <year>2011</year>;<volume>21</volume>(<issue>7</issue>):<fpage>557</fpage>â<lpage>562</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2011.02.030" xlink:type="simple">10.1016/j.cub.2011.02.030</ext-link></comment> <object-id pub-id-type="pmid">21419628</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>WÃ¤chter</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Lungu</surname> <given-names>OV</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Willingham</surname> <given-names>DT</given-names></name>, <name name-style="western"><surname>Ashe</surname> <given-names>J</given-names></name>. <article-title>Differential effect of reward and punishment on procedural learning</article-title>. <source>The Journal of neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>2</issue>):<fpage>436</fpage>â<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4132-08.2009" xlink:type="simple">10.1523/JNEUROSCI.4132-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19144843</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jarvstad</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hahn</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Rushton</surname> <given-names>SK</given-names></name>, <name name-style="western"><surname>Warren</surname> <given-names>Pa</given-names></name>. <article-title>Perceptuo-motor, cognitive, and description-based decision-making seem equally good</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2013</year>;<volume>110</volume>(<issue>40</issue>):<fpage>16271</fpage>â<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1300239110" xlink:type="simple">10.1073/pnas.1300239110</ext-link></comment> <object-id pub-id-type="pmid">24048030</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref067">
<label>67</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Silver</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Veness</surname> <given-names>J</given-names></name>. <article-title>Monte-Carlo Planning in Large POMDPs</article-title>. <source>Advances in neural information processing systems (NIPS)</source>. <year>2010</year>; p. <fpage>1</fpage>â<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref068">
<label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Watkins</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Q-Learning</article-title>. <source>Machine Learning</source>. <year>1992</year>;<volume>8</volume>:<fpage>279</fpage>â<lpage>292</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1022676722315" xlink:type="simple">10.1023/A:1022676722315</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref069">
<label>69</label>
<mixed-citation publication-type="other" xlink:type="simple">Littman ML, Cassandra A, Kaelbling L. Learning policies for partially observable environments: Scaling up. In: Proceedings of the Twelfth International Conference on Machine Learning. February 1970. California: Morgan Kaufmann.; 1995. p. 1â59.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hauskrecht</surname> <given-names>M</given-names></name>. <article-title>Value-function Approximations for Partially Observable Markov Decision Processes</article-title>. <source>J Artif Int Res</source>. <year>2000</year>;<volume>13</volume>(<issue>1</issue>):<fpage>33</fpage>â<lpage>94</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005503.ref071">
<label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Apostolikas</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Tzafestas</surname> <given-names>S</given-names></name>. <article-title>Improved Qmdp Policy for Partially Observable Markov Decision Processes in Large Domains: Embedding Exploration</article-title>. <source>Intelligent Automation and Soft Computing</source>. <year>2004</year>;<volume>10</volume>(<issue>3</issue>):<fpage>209</fpage>â<lpage>220</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/10798587.2004.10642878" xlink:type="simple">10.1080/10798587.2004.10642878</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005503.ref072">
<label>72</label>
<mixed-citation publication-type="other" xlink:type="simple">Vermorel J, Mohri M. Multi-armed bandit algorithms and empirical evaluation. In: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). vol. 3720 LNAI; 2005. p. 437â448.</mixed-citation>
</ref>
</ref-list>
</back>
</article>